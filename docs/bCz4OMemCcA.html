<html><head><title>Attention is all you need (Transformer) - Model explanation (including math), Inference and Training</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Attention is all you need (Transformer) - Model explanation (including math), Inference and Training</h2><a href="https://www.youtube.com/watch?v=bCz4OMemCcA"><img src="https://i.ytimg.com/vi/bCz4OMemCcA/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=70">1:10</a> RNN and their problems<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=484">8:4</a> Transformer Model<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=542">9:2</a> Maths background and notations<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=740">12:20</a> Encoder (overview)<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=751">12:31</a> Input Embeddings<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=904">15:4</a> Positional Encoding<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1208">20:8</a> Single Head Self-Attention<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1710">28:30</a> Multi-Head Attention<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2139">35:39</a> Query, Key, Value<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2275">37:55</a> Layer Normalization<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2413">40:13</a> Decoder (overview)<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2544">42:24</a> Masked Multi-Head Attention<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2699">44:59</a> Training<br><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3129">52:9</a> Inference<br><br><div style="text-align: left;"><a href="./bCz4OMemCcA.html">Whisper Transcript</a> | <a href="./transcript_bCz4OMemCcA.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys, welcome to my video about the transformer and this is actually the version 2.0 of my series on the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=8" target="_blank">00:00:08.500</a></span> | <span class="t">I had a previous video in which I talked about the transformer but the audio quality was not good and as suggested by my viewers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=15" target="_blank">00:00:15.520</a></span> | <span class="t">as the video had a huge success, the viewers suggested me to improve the audio quality, so this is why I'm doing this video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=24" target="_blank">00:00:24.120</a></span> | <span class="t">You don't have to watch the previous series because I would be doing basically the same things but with some improvements,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=30" target="_blank">00:00:30.080</a></span> | <span class="t">so I'm actually compensating from some mistakes I made or from some improvements that I could add.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=35" target="_blank">00:00:35.580</a></span> | <span class="t">After watching this video, I suggest watching my other video about how to code a transformer model from scratch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=43" target="_blank">00:00:43.500</a></span> | <span class="t">so how to code the model itself, how to train it on a data and how to inference it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=49" target="_blank">00:00:49.220</a></span> | <span class="t">Stick with me because it's gonna be a little long journey but for sure worth it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=54" target="_blank">00:00:54.160</a></span> | <span class="t">Now, before we talk about the transformer, I want to first talk about recurrent neural networks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=60" target="_blank">00:01:00.320</a></span> | <span class="t">so the networks that were used before they introduced the transformer for most of the sequence-to-sequence tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=68" target="_blank">00:01:08.500</a></span> | <span class="t">So let's review them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=71" target="_blank">00:01:11.160</a></span> | <span class="t">Recurrent neural networks existed a long time before the transformer and they allowed to map one sequence of input to another sequence of output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=81" target="_blank">00:01:21.400</a></span> | <span class="t">In this case, our input is X and we want an input sequence Y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=86" target="_blank">00:01:26.680</a></span> | <span class="t">What we did before is that we split the sequence into single items, so we gave the recurrent neural network the first item as input, so X1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=96" target="_blank">00:01:36.760</a></span> | <span class="t">along with an initial state, usually made up of only zeros, and the recurrent neural network produced an output, let's call it Y1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=106" target="_blank">00:01:46.540</a></span> | <span class="t">And this happened at the first time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=109" target="_blank">00:01:49.740</a></span> | <span class="t">Then we took the hidden state, this is called the hidden state of the network of the previous time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=116" target="_blank">00:01:56.760</a></span> | <span class="t">along with the next input token, so X2, and the network had to produce the second output token, Y2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=126" target="_blank">00:02:06.360</a></span> | <span class="t">And then we did the same procedure at the third time step, in which we took the hidden state of the previous time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=133" target="_blank">00:02:13.500</a></span> | <span class="t">along with the input token at the time step 3, and the network had to produce the next output token, which is Y3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=143" target="_blank">00:02:23.520</a></span> | <span class="t">If you have n tokens, you need n time steps to map an n-sequence input into an n-sequence output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=153" target="_blank">00:02:33.320</a></span> | <span class="t">This worked fine for a lot of tasks, but had some problems. Let's review them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=160" target="_blank">00:02:40.760</a></span> | <span class="t">The problems with recurrent neural networks, first of all, are that they are slow for long sequences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=166" target="_blank">00:02:46.800</a></span> | <span class="t">because think of the process we did before, we have kind of like a for loop in which we do the same operation for every token in the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=177" target="_blank">00:02:57.080</a></span> | <span class="t">So if you have the longer the sequence, the longer this computation, and this made the network not easy to train for long sequences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=187" target="_blank">00:03:07.120</a></span> | <span class="t">The second problem was the vanishing or the exploding gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=191" target="_blank">00:03:11.280</a></span> | <span class="t">Now, you may have heard these terms or expression on the Internet or from other videos,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=196" target="_blank">00:03:16.160</a></span> | <span class="t">but I will try to give you a brief insight on what do they mean on a practical level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=203" target="_blank">00:03:23.080</a></span> | <span class="t">So as you know, frameworks like PyTorch, they convert our networks into a computation graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=211" target="_blank">00:03:31.200</a></span> | <span class="t">So basically, suppose we have a computation graph. This is not a neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=216" target="_blank">00:03:36.320</a></span> | <span class="t">I will be making a computational graph that is very simple, has nothing to do with neural networks, but will show you the problems that we have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=225" target="_blank">00:03:45.120</a></span> | <span class="t">So imagine we have two inputs X and another input, let's call it Y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=231" target="_blank">00:03:51.880</a></span> | <span class="t">Our computational graph first, let's say, multiplies these two numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=235" target="_blank">00:03:55.880</a></span> | <span class="t">So we have a first function, let's call it f of X and Y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=242" target="_blank">00:04:02.560</a></span> | <span class="t">That is X multiplied by Y, I mean, multiplied.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=248" target="_blank">00:04:08.000</a></span> | <span class="t">And the result, let's call it Z, is given to another function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=254" target="_blank">00:04:14.440</a></span> | <span class="t">Let's call this function g of Z is equal to, let's say, Z squared.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=262" target="_blank">00:04:22.400</a></span> | <span class="t">What our PyTorch, for example, does, it's that PyTorch want to calculate, usually we have a loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=270" target="_blank">00:04:30.160</a></span> | <span class="t">PyTorch calculates the derivative of the loss function with respect to each weight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=275" target="_blank">00:04:35.840</a></span> | <span class="t">In this case, we just calculate the derivative of the g function, so the output function with respect to all of its inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=282" target="_blank">00:04:42.520</a></span> | <span class="t">So derivative of g with respect to X, let's say, is equal to the derivative of g with respect to f.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=295" target="_blank">00:04:55.680</a></span> | <span class="t">And multiplied by the derivative of f with respect to X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=302" target="_blank">00:05:02.640</a></span> | <span class="t">These two should kind of cancel out, this is called the chain rule.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=307" target="_blank">00:05:07.040</a></span> | <span class="t">Now, as you can see, the longer the chain of computation, so if we have many nodes, one after another,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=314" target="_blank">00:05:14.440</a></span> | <span class="t">the longer this multiplication chain, so here we have two because the distance from this node and this is two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=321" target="_blank">00:05:21.720</a></span> | <span class="t">but imagine you have 100 or 1000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=325" target="_blank">00:05:25.960</a></span> | <span class="t">Now imagine this number is 0.5 and this number is 0.5 also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=333" target="_blank">00:05:33.000</a></span> | <span class="t">The resulting numbers, when multiplied together, is a number that is smaller than the two initial numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=338" target="_blank">00:05:38.920</a></span> | <span class="t">It's going to be 0.25 because it's 1/2 multiplied by 1/2 is 1/4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=346" target="_blank">00:05:46.480</a></span> | <span class="t">So if we have two numbers that are smaller than one and we multiply them together, they will produce an even smaller number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=353" target="_blank">00:05:53.240</a></span> | <span class="t">And if we have two numbers that are bigger than one and we multiply them together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=357" target="_blank">00:05:57.400</a></span> | <span class="t">they will produce a number that is bigger than both of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=360" target="_blank">00:06:00.480</a></span> | <span class="t">So if we have a very long chain of computation, it eventually will either become a very big number or a very small number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=368" target="_blank">00:06:08.520</a></span> | <span class="t">And this is not desirable, first of all, because our CPU of our GPU can only represent numbers up to a certain precision,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=377" target="_blank">00:06:17.680</a></span> | <span class="t">let's say 32 bit or 64 bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=380" target="_blank">00:06:20.200</a></span> | <span class="t">And if the number becomes too small, the contribution of this number to the output will become very small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=386" target="_blank">00:06:26.760</a></span> | <span class="t">So when the PyTorch or our automatic, let's say our framework, will calculate how to adjust the weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=395" target="_blank">00:06:35.560</a></span> | <span class="t">the weight will move very, very, very slowly because the contribution of this product will be a very small number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=404" target="_blank">00:06:44.400</a></span> | <span class="t">And this means that we have the gradient is vanishing or in the other case, it can explode, become very big numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=413" target="_blank">00:06:53.920</a></span> | <span class="t">And this is a problem. The next problem is difficulty in accessing information from a long time ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=419" target="_blank">00:06:59.720</a></span> | <span class="t">What does it mean? It means that, as you remember from the previous slide,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=423" target="_blank">00:07:03.360</a></span> | <span class="t">we saw that the first input token is given to the recurrent neural network along with the first state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=430" target="_blank">00:07:10.440</a></span> | <span class="t">Now, we need to think that the recurrent neural network is a long graph of computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=434" target="_blank">00:07:14.480</a></span> | <span class="t">It will produce a new hidden state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=436" target="_blank">00:07:16.680</a></span> | <span class="t">Then we will use the new hidden state along with the next token to produce the next output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=442" target="_blank">00:07:22.880</a></span> | <span class="t">If we have a very long sequence of input sequence, the last token will have a hidden state whose contribution from the first token has nearly gone because of this long chain of multiplication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=457" target="_blank">00:07:37.200</a></span> | <span class="t">So actually, the last token will not depend much on the first token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=463" target="_blank">00:07:43.000</a></span> | <span class="t">And this is also not good because, for example, we know as humans that in a text, in a quite long text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=469" target="_blank">00:07:49.600</a></span> | <span class="t">the context that we saw, let's say 200 words before, is still relevant to the context of the current words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=477" target="_blank">00:07:57.480</a></span> | <span class="t">And this is something that the RNN could not map. And this is why we have the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=485" target="_blank">00:08:05.680</a></span> | <span class="t">So the transformer solves these problems with the recurrent neural networks, and we will see how.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=491" target="_blank">00:08:11.640</a></span> | <span class="t">The structure of the transformer, we can divide into two macro blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=497" target="_blank">00:08:17.440</a></span> | <span class="t">The first macro block is called the encoder, and it's this part here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=502" target="_blank">00:08:22.560</a></span> | <span class="t">The second macro block is called the decoder, and it's the second part here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=508" target="_blank">00:08:28.000</a></span> | <span class="t">The third part here you see on the top, it's just a linear layer, and we will see why it's there and what its function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=515" target="_blank">00:08:35.720</a></span> | <span class="t">So and the two layers, so the encoder and the decoder, are connected by this connection you can see here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=523" target="_blank">00:08:43.680</a></span> | <span class="t">in which some output of the encoder is sent as input to the decoder. And we will also see how.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=530" target="_blank">00:08:50.320</a></span> | <span class="t">Let's start, first of all, with some notations that I will be using during my explanation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=537" target="_blank">00:08:57.640</a></span> | <span class="t">And you should be familiar with this notation. Also to review some maths.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=542" target="_blank">00:09:02.200</a></span> | <span class="t">So the first thing we should be familiar with is matrix multiplication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=546" target="_blank">00:09:06.480</a></span> | <span class="t">So imagine we have an input matrix, which is a sequence of, let's say, words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=552" target="_blank">00:09:12.840</a></span> | <span class="t">So sequence by d model, and we will see why it's called sequence by d model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=557" target="_blank">00:09:17.400</a></span> | <span class="t">So imagine we have a matrix that is 6 by 512, in which each row is a word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=567" target="_blank">00:09:27.120</a></span> | <span class="t">And this word is not made of characters, but by 512 numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=571" target="_blank">00:09:31.600</a></span> | <span class="t">So each word is represented by 512 numbers, OK, like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=577" target="_blank">00:09:37.040</a></span> | <span class="t">Imagine you have 512 of them along this row, 512 along this other row, etc, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=583" target="_blank">00:09:43.520</a></span> | <span class="t">1, 2, 3, 4, 5, so we need another one here, OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=587" target="_blank">00:09:47.720</a></span> | <span class="t">The first word we will call it A, the second B, the C, D, E, and F.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=594" target="_blank">00:09:54.920</a></span> | <span class="t">If we multiply this matrix by another matrix, let's say the transpose of this matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=601" target="_blank">00:10:01.320</a></span> | <span class="t">so it's a matrix where the rows become columns, so 3, 4, 5, and 6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=615" target="_blank">00:10:15.960</a></span> | <span class="t">This word will be here, B, C, D, E, and F.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=621" target="_blank">00:10:21.920</a></span> | <span class="t">And then we have 512 numbers along each column, because before we had them on the rows, now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=633" target="_blank">00:10:33.480</a></span> | <span class="t">they will become on the columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=635" target="_blank">00:10:35.160</a></span> | <span class="t">So here we have the 512th number, etc, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=640" target="_blank">00:10:40.240</a></span> | <span class="t">This is a matrix that is 512 by 6, so let me add some brackets here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=647" target="_blank">00:10:47.440</a></span> | <span class="t">If we multiply them, we will get a new matrix that is, we cancel the inner dimensions and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=654" target="_blank">00:10:54.320</a></span> | <span class="t">we get the outer dimensions, so it will become 6 by 6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=658" target="_blank">00:10:58.300</a></span> | <span class="t">So it will be 6 rows by 6 rows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=660" target="_blank">00:11:00.820</a></span> | <span class="t">So let's draw it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=662" target="_blank">00:11:02.720</a></span> | <span class="t">How do we calculate the values of this output matrix?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=665" target="_blank">00:11:05.720</a></span> | <span class="t">This is 6 by 6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=668" target="_blank">00:11:08.840</a></span> | <span class="t">This is the dot product of the first row with the first column.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=673" target="_blank">00:11:13.680</a></span> | <span class="t">So this is A multiplied by A. The second value is the first row with the second column.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=681" target="_blank">00:11:21.160</a></span> | <span class="t">The third value is the first row with the third column until the last column, so A multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=689" target="_blank">00:11:29.360</a></span> | <span class="t">by F, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=691" target="_blank">00:11:31.240</a></span> | <span class="t">What is the dot product?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=692" target="_blank">00:11:32.400</a></span> | <span class="t">It's basically you take the first number of the first row, so here we have 512 numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=699" target="_blank">00:11:39.760</a></span> | <span class="t">here we have 512 numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=701" target="_blank">00:11:41.620</a></span> | <span class="t">So you take the first number of the first row and the first number of the first column,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=706" target="_blank">00:11:46.480</a></span> | <span class="t">you multiply them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=708" target="_blank">00:11:48.640</a></span> | <span class="t">Second value of the first row, second value of the first column, you multiply them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=714" target="_blank">00:11:54.880</a></span> | <span class="t">And then you add all these numbers together, so it will be, let's say, this number multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=722" target="_blank">00:12:02.120</a></span> | <span class="t">by this plus this number multiplied by this plus this number multiplied by this plus this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=728" target="_blank">00:12:08.260</a></span> | <span class="t">number multiplied by this plus you sum all these numbers together and this is the A dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=733" target="_blank">00:12:13.940</a></span> | <span class="t">product A. So we should be familiar with this notation because I will be using it a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=739" target="_blank">00:12:19.120</a></span> | <span class="t">in the next slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=740" target="_blank">00:12:20.920</a></span> | <span class="t">Let's start our journey of the transformer by looking at the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=746" target="_blank">00:12:26.820</a></span> | <span class="t">So the encoder starts with the input embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=751" target="_blank">00:12:31.160</a></span> | <span class="t">So what is an input embedding?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=753" target="_blank">00:12:33.800</a></span> | <span class="t">First of all, let's start with our sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=756" target="_blank">00:12:36.400</a></span> | <span class="t">We have a sentence of, in this case, six words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=760" target="_blank">00:12:40.460</a></span> | <span class="t">What we do is we tokenize it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=762" target="_blank">00:12:42.220</a></span> | <span class="t">We transform the sentence into tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=764" target="_blank">00:12:44.880</a></span> | <span class="t">What does it mean to tokenize?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=766" target="_blank">00:12:46.080</a></span> | <span class="t">We split them into single words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=769" target="_blank">00:12:49.240</a></span> | <span class="t">It is not necessary to always split the sentence using single words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=774" target="_blank">00:12:54.440</a></span> | <span class="t">We can even split the sentence in smaller parts that are even smaller than a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=780" target="_blank">00:13:00.440</a></span> | <span class="t">word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=781" target="_blank">00:13:01.440</a></span> | <span class="t">For example, we can split this sentence into, let's say, 20 tokens by splitting each word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=788" target="_blank">00:13:08.600</a></span> | <span class="t">into multiple words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=790" target="_blank">00:13:10.480</a></span> | <span class="t">This is usually done in most modern transformer models, but we will not be doing it, otherwise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=798" target="_blank">00:13:18.120</a></span> | <span class="t">it's really difficult to visualize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=800" target="_blank">00:13:20.400</a></span> | <span class="t">So let's suppose we have this input sentence and we split it into tokens and each token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=806" target="_blank">00:13:26.160</a></span> | <span class="t">is a single word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=807" target="_blank">00:13:27.940</a></span> | <span class="t">The next step we do is we map these words into numbers and these numbers represent the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=815" target="_blank">00:13:35.160</a></span> | <span class="t">position of these words in our vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=818" target="_blank">00:13:38.320</a></span> | <span class="t">So imagine we have a vocabulary of all the possible words that appear in our training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=823" target="_blank">00:13:43.200</a></span> | <span class="t">set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=824" target="_blank">00:13:44.600</a></span> | <span class="t">Each word will occupy a position in this vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=827" target="_blank">00:13:47.880</a></span> | <span class="t">So for example, the word "your" will occupy the position 105, the word "cat" will occupy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=832" target="_blank">00:13:52.680</a></span> | <span class="t">the position 6,500, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=836" target="_blank">00:13:56.800</a></span> | <span class="t">And as you can see, this cat here has the same number as this cat here because they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=841" target="_blank">00:14:01.120</a></span> | <span class="t">occupy the same position in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=844" target="_blank">00:14:04.200</a></span> | <span class="t">We take these numbers, which are called input IDs, and we map them into a vector of size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=850" target="_blank">00:14:10.160</a></span> | <span class="t">512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=852" target="_blank">00:14:12.840</a></span> | <span class="t">This vector is a vector made of 512 numbers and we always map the same word to always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=859" target="_blank">00:14:19.880</a></span> | <span class="t">the same embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=862" target="_blank">00:14:22.000</a></span> | <span class="t">However, this number is not fixed, it's a parameter for our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=868" target="_blank">00:14:28.320</a></span> | <span class="t">So our model will learn to change these numbers in such a way that it represents the meaning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=874" target="_blank">00:14:34.280</a></span> | <span class="t">of the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=875" target="_blank">00:14:35.360</a></span> | <span class="t">So the input IDs never change because our vocabulary is fixed, but the embedding will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=880" target="_blank">00:14:40.240</a></span> | <span class="t">change along with the training process of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=883" target="_blank">00:14:43.240</a></span> | <span class="t">So the embedding numbers will change according to the needs of the loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=888" target="_blank">00:14:48.680</a></span> | <span class="t">So the input embedding are basically mapping our single word into an embedding of size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=893" target="_blank">00:14:53.680</a></span> | <span class="t">512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=895" target="_blank">00:14:55.120</a></span> | <span class="t">And we call this quantity 512D_MODEL because it's the same name that is also used in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=900" target="_blank">00:15:00.920</a></span> | <span class="t">paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=901" target="_blank">00:15:01.920</a></span> | <span class="t">Attention is all you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=905" target="_blank">00:15:05.320</a></span> | <span class="t">Let's look at the next layer of the encoder, which is the positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=910" target="_blank">00:15:10.680</a></span> | <span class="t">So what is positional encoding?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=913" target="_blank">00:15:13.600</a></span> | <span class="t">What we want is that each word should carry some information about its position in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=919" target="_blank">00:15:19.920</a></span> | <span class="t">sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=921" target="_blank">00:15:21.260</a></span> | <span class="t">Because now we built a matrix of words that are embeddings, but they don't convey any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=927" target="_blank">00:15:27.200</a></span> | <span class="t">information about where that particular word is inside the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=932" target="_blank">00:15:32.920</a></span> | <span class="t">And this is the job of the positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=935" target="_blank">00:15:35.700</a></span> | <span class="t">So what we do, we want the model to treat words that appear close to each other as close</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=942" target="_blank">00:15:42.080</a></span> | <span class="t">and words that are distant as distant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=944" target="_blank">00:15:44.900</a></span> | <span class="t">So we want the model to see this information about the spatial information that we see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=949" target="_blank">00:15:49.680</a></span> | <span class="t">with our eyes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=950" target="_blank">00:15:50.680</a></span> | <span class="t">So for example, when we see this sentence, what is positional encoding, we know that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=954" target="_blank">00:15:54.840</a></span> | <span class="t">the word "what" is more far from the word "is" compared to encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=962" target="_blank">00:16:02.520</a></span> | <span class="t">Because we have this spatial information given by our eyes, but the model cannot see this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=967" target="_blank">00:16:07.340</a></span> | <span class="t">So we need to give some information to the model about how the words are spatially distributed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=973" target="_blank">00:16:13.380</a></span> | <span class="t">inside of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=975" target="_blank">00:16:15.960</a></span> | <span class="t">And we want the positional encoding to represent a pattern that the model can learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=981" target="_blank">00:16:21.780</a></span> | <span class="t">And we will see how.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=984" target="_blank">00:16:24.980</a></span> | <span class="t">Imagine we have our original sentence, "Your cat is a lovely cat".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=988" target="_blank">00:16:28.940</a></span> | <span class="t">What we do is we first convert into embeddings using the previous layer, so the input embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=995" target="_blank">00:16:35.540</a></span> | <span class="t">And these are embeddings of size 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=998" target="_blank">00:16:38.740</a></span> | <span class="t">Then we create some special vectors called the positional encoding vectors that we add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1003" target="_blank">00:16:43.900</a></span> | <span class="t">to these embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1005" target="_blank">00:16:45.340</a></span> | <span class="t">So this vector we see here in red is a vector of size 512, which is not learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1013" target="_blank">00:16:53.220</a></span> | <span class="t">It's computed once and not learned along with the training process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1017" target="_blank">00:16:57.340</a></span> | <span class="t">It's fixed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1018" target="_blank">00:16:58.800</a></span> | <span class="t">And this word, this vector represents the position of the word inside of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1024" target="_blank">00:17:04.740</a></span> | <span class="t">And this should give us an output that is a vector of size, again, 512, because we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1032" target="_blank">00:17:12.220</a></span> | <span class="t">summing this number with this number, this number with this number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1037" target="_blank">00:17:17.580</a></span> | <span class="t">So the first dimension with the first dimension, the second dimension with the second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1041" target="_blank">00:17:21.020</a></span> | <span class="t">So we will get a new vector of the same size of the input vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1046" target="_blank">00:17:26.380</a></span> | <span class="t">How are these positional embeddings calculated?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1049" target="_blank">00:17:29.140</a></span> | <span class="t">Let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1051" target="_blank">00:17:31.220</a></span> | <span class="t">Again we have a smaller sentence, let's say "your cat is".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1055" target="_blank">00:17:35.140</a></span> | <span class="t">And you may have seen the following expressions from the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1059" target="_blank">00:17:39.500</a></span> | <span class="t">What we do is we create a vector of size D model, so 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1067" target="_blank">00:17:47.040</a></span> | <span class="t">And for each position in this vector, we calculate the value using these two expressions, using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1074" target="_blank">00:17:54.900</a></span> | <span class="t">these arguments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1076" target="_blank">00:17:56.020</a></span> | <span class="t">So the first argument indicates the position of the word inside of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1081" target="_blank">00:18:01.020</a></span> | <span class="t">So the word "your" occupies the position zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1084" target="_blank">00:18:04.740</a></span> | <span class="t">And we use the, for the even dimension, so the zero, the two, the four, the 510, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1093" target="_blank">00:18:13.580</a></span> | <span class="t">We use the first expression, so the sign.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1096" target="_blank">00:18:16.740</a></span> | <span class="t">And for the odd positions of this vector, we use the second expression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1102" target="_blank">00:18:22.620</a></span> | <span class="t">And we do this for all the words inside of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1105" target="_blank">00:18:25.820</a></span> | <span class="t">So this particular embedding is calculated PE of 10, because it's the first word embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1112" target="_blank">00:18:32.900</a></span> | <span class="t">zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1113" target="_blank">00:18:33.940</a></span> | <span class="t">So this one represents the argument "pause".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1117" target="_blank">00:18:37.940</a></span> | <span class="t">And this zero represents the argument "2i".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1122" target="_blank">00:18:42.180</a></span> | <span class="t">And PE of 11 means that the first word dimension one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1128" target="_blank">00:18:48.980</a></span> | <span class="t">So we will use the cosine, given the position one, and the 2i will be equal to, 2i + 1 will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1136" target="_blank">00:18:56.140</a></span> | <span class="t">be equal to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1139" target="_blank">00:18:59.060</a></span> | <span class="t">And we do this for this third word, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1143" target="_blank">00:19:03.320</a></span> | <span class="t">If we have another sentence, we will not have different positional encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1148" target="_blank">00:19:08.780</a></span> | <span class="t">We will have the same vectors, even for different sentences, because the positional encoding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1155" target="_blank">00:19:15.140</a></span> | <span class="t">are computed once and reused for every sentence that our model will see, during inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1161" target="_blank">00:19:21.580</a></span> | <span class="t">or training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1163" target="_blank">00:19:23.000</a></span> | <span class="t">So we only compute the positional encoding once, when we create the model, we save them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1168" target="_blank">00:19:28.140</a></span> | <span class="t">and then we reuse them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1169" target="_blank">00:19:29.140</a></span> | <span class="t">We don't need to compute it every time we feed a sentence to the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1176" target="_blank">00:19:36.520</a></span> | <span class="t">So why the authors chose the cosine and the sine functions to represent positional encodings?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1182" target="_blank">00:19:42.620</a></span> | <span class="t">Because let's watch the plot of these two functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1186" target="_blank">00:19:46.760</a></span> | <span class="t">You can see the plot is by position, so the position of the word inside of the sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1191" target="_blank">00:19:51.260</a></span> | <span class="t">and this depth is the dimension along the vector, so the 2i that you saw before in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1197" target="_blank">00:19:57.240</a></span> | <span class="t">previous expressions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1199" target="_blank">00:19:59.460</a></span> | <span class="t">And if we plot them, we can see, as humans, a pattern here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1202" target="_blank">00:20:02.960</a></span> | <span class="t">And we hope that the model can also see this pattern.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1207" target="_blank">00:20:07.100</a></span> | <span class="t">Okay, the next layer of the encoder is the multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1212" target="_blank">00:20:12.420</a></span> | <span class="t">We will not go inside of the multi-head attention first, we will first visualize the single-head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1219" target="_blank">00:20:19.260</a></span> | <span class="t">attention, so the self-attention with a single head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1222" target="_blank">00:20:22.920</a></span> | <span class="t">And let's do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1224" target="_blank">00:20:24.820</a></span> | <span class="t">So what is self-attention?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1226" target="_blank">00:20:26.980</a></span> | <span class="t">Self-attention is a mechanism that existed before they introduced the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1232" target="_blank">00:20:32.060</a></span> | <span class="t">The authors of the transformer just changed it into a multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1237" target="_blank">00:20:37.660</a></span> | <span class="t">So how did the self-attention work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1241" target="_blank">00:20:41.140</a></span> | <span class="t">The self-attention allows the model to relate words to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1245" target="_blank">00:20:45.940</a></span> | <span class="t">Okay, so we had the input embeddings that capture the meaning of the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1252" target="_blank">00:20:52.060</a></span> | <span class="t">Then we had the positional encoding that gives the information about the position of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1257" target="_blank">00:20:57.620</a></span> | <span class="t">word inside of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1259" target="_blank">00:20:59.400</a></span> | <span class="t">Now we want this self-attention to relate words to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1264" target="_blank">00:21:04.220</a></span> | <span class="t">Now imagine we have an input sequence of 6 words with a d-model of size 512, which can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1273" target="_blank">00:21:13.340</a></span> | <span class="t">be represented as a matrix that we will call Q, K, and V.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1278" target="_blank">00:21:18.140</a></span> | <span class="t">So our Q, K, and V are the same matrix representing the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1285" target="_blank">00:21:25.220</a></span> | <span class="t">So the input of 6 words with the dimension of 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1289" target="_blank">00:21:29.940</a></span> | <span class="t">So each word is represented by a vector of size 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1294" target="_blank">00:21:34.860</a></span> | <span class="t">We basically apply this formula we saw here from the paper to calculate the attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1300" target="_blank">00:21:40.180</a></span> | <span class="t">the self-attention in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1301" target="_blank">00:21:41.700</a></span> | <span class="t">Why self-attention?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1302" target="_blank">00:21:42.780</a></span> | <span class="t">Because it's each word in the sentence related to other words in the same sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1308" target="_blank">00:21:48.660</a></span> | <span class="t">So it's self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1311" target="_blank">00:21:51.940</a></span> | <span class="t">So we start with our Q matrix, which is the input sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1316" target="_blank">00:21:56.300</a></span> | <span class="t">So let's visualize it, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1317" target="_blank">00:21:57.780</a></span> | <span class="t">So we have 6 rows, and on the columns we have 512 columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1323" target="_blank">00:22:03.300</a></span> | <span class="t">Now they are really difficult to draw, but let's say we have 512 columns, and here we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1328" target="_blank">00:22:08.860</a></span> | <span class="t">have 6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1331" target="_blank">00:22:11.700</a></span> | <span class="t">Now what we do, according to this formula, we multiply it by the same sentence but transposed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1337" target="_blank">00:22:17.860</a></span> | <span class="t">So the transposed of the K, which is again the same input sequence, we divide it by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1343" target="_blank">00:22:23.940</a></span> | <span class="t">square root of 512, and then we apply the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1349" target="_blank">00:22:29.100</a></span> | <span class="t">The output of this, as we saw before in the initial matrix notations, we saw that when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1356" target="_blank">00:22:36.060</a></span> | <span class="t">we multiply 6 by 512 with another matrix that is 512 by 6, we obtain a new matrix that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1364" target="_blank">00:22:44.820</a></span> | <span class="t">6 by 6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1366" target="_blank">00:22:46.300</a></span> | <span class="t">And each value in this matrix represents the dot product of the first row with the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1372" target="_blank">00:22:52.300</a></span> | <span class="t">column.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1373" target="_blank">00:22:53.300</a></span> | <span class="t">Each row represents the dot product of the first row with the second column, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1378" target="_blank">00:22:58.740</a></span> | <span class="t">The values here are actually randomly generated, so don't concentrate on the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1383" target="_blank">00:23:03.180</a></span> | <span class="t">What you should notice is that the softmax makes all these values in such a way that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1388" target="_blank">00:23:08.380</a></span> | <span class="t">they sum up to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1390" target="_blank">00:23:10.160</a></span> | <span class="t">So this row, for example, here, sums up to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1394" target="_blank">00:23:14.620</a></span> | <span class="t">This other row also sums up to 1, etc., etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1398" target="_blank">00:23:18.220</a></span> | <span class="t">And this value we see here is the dot product of the first word with the embedding of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1405" target="_blank">00:23:25.660</a></span> | <span class="t">word itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1407" target="_blank">00:23:27.020</a></span> | <span class="t">This value here is the dot product of the embedding of the word "your" with the embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1413" target="_blank">00:23:33.420</a></span> | <span class="t">of the word "cat".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1415" target="_blank">00:23:35.460</a></span> | <span class="t">And this value here is the dot product of the embedding of the word "your" with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1420" target="_blank">00:23:40.740</a></span> | <span class="t">embedding of the word "is".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1424" target="_blank">00:23:44.500</a></span> | <span class="t">And this value represents somehow a score, that how intense is the relationship between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1430" target="_blank">00:23:50.860</a></span> | <span class="t">one word and another.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1433" target="_blank">00:23:53.060</a></span> | <span class="t">Let's go ahead with the formula.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1435" target="_blank">00:23:55.100</a></span> | <span class="t">So for now we just multiplied Q by K, divided by the square root of dK, applied to the softmax,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1442" target="_blank">00:24:02.020</a></span> | <span class="t">but we didn't multiply by V.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1444" target="_blank">00:24:04.760</a></span> | <span class="t">So let's go forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1446" target="_blank">00:24:06.500</a></span> | <span class="t">We multiply this matrix by V, and we obtain a new matrix, which is 6 by 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1452" target="_blank">00:24:12.740</a></span> | <span class="t">So if we multiply a matrix that is 6 by 6 with another that is 6 by 512, we get a new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1458" target="_blank">00:24:18.860</a></span> | <span class="t">matrix that is 6 by 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1461" target="_blank">00:24:21.780</a></span> | <span class="t">And one thing you should notice is that the dimension of this matrix is exactly the dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1466" target="_blank">00:24:26.380</a></span> | <span class="t">of the initial matrix from which we started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1469" target="_blank">00:24:29.820</a></span> | <span class="t">This, what does it mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1472" target="_blank">00:24:32.420</a></span> | <span class="t">That we obtain a new matrix that is 6 rows, so let's say 6 rows, with 512 columns, in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1480" target="_blank">00:24:40.780</a></span> | <span class="t">which each, these are our words, so we have 6 words, and each word has an embedding of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1486" target="_blank">00:24:46.420</a></span> | <span class="t">dimension 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1488" target="_blank">00:24:48.020</a></span> | <span class="t">So now this embedding here represents not only the meaning of the word, which was given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1495" target="_blank">00:24:55.020</a></span> | <span class="t">by the input embedding, not only the position of the word, which was added by the positional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1500" target="_blank">00:25:00.460</a></span> | <span class="t">encoding, but now somehow this special embedding, so these values represent a special embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1506" target="_blank">00:25:06.740</a></span> | <span class="t">that also captures the relationship of this particular word with all the other words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1514" target="_blank">00:25:14.300</a></span> | <span class="t">And this particular embedding of this word here also captures not only its meaning, not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1520" target="_blank">00:25:20.060</a></span> | <span class="t">only its position inside of the sentence, but also the relationship of this word with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1525" target="_blank">00:25:25.360</a></span> | <span class="t">all the other words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1527" target="_blank">00:25:27.420</a></span> | <span class="t">I want to remind you that this is not the multi-head attention, we are just watching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1531" target="_blank">00:25:31.240</a></span> | <span class="t">the self-attention, so one head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1534" target="_blank">00:25:34.060</a></span> | <span class="t">We will see later how this becomes the multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1541" target="_blank">00:25:41.780</a></span> | <span class="t">Self-attention has some properties that are very desirable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1545" target="_blank">00:25:45.180</a></span> | <span class="t">First of all, it's permutation invariant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1547" target="_blank">00:25:47.860</a></span> | <span class="t">What does it mean to be permutation invariant?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1549" target="_blank">00:25:49.980</a></span> | <span class="t">It means that if we have a matrix, let's say, first we have a matrix of 6 words, in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1557" target="_blank">00:25:57.420</a></span> | <span class="t">case let's say just 4 words, so A, B, C, and D, and suppose by applying the formula before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1564" target="_blank">00:26:04.700</a></span> | <span class="t">this produces this particular matrix in which there is new special embedding for the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1572" target="_blank">00:26:12.220</a></span> | <span class="t">A, a new special embedding for the word B, a new special embedding for the word C and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1576" target="_blank">00:26:16.060</a></span> | <span class="t">D, so let's call it A', B', C', D'.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1578" target="_blank">00:26:18.900</a></span> | <span class="t">If we change the position of these two rows, the values will not change, the position of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1585" target="_blank">00:26:25.180</a></span> | <span class="t">the output will change accordingly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1586" target="_blank">00:26:26.860</a></span> | <span class="t">So the values of B' will not change, it will just change in the position, and also the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1592" target="_blank">00:26:32.160</a></span> | <span class="t">C will also change position, but the values in each vector will not change, and this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1596" target="_blank">00:26:36.620</a></span> | <span class="t">a desirable property.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1599" target="_blank">00:26:39.220</a></span> | <span class="t">Self-attention as of now requires no parameters, I mean, I didn't introduce any parameter that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1604" target="_blank">00:26:44.500</a></span> | <span class="t">is learned by the model, I just took the initial sentence of, in this case, 6 words, we multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1612" target="_blank">00:26:52.220</a></span> | <span class="t">it by itself, we divide it by a fixed quantity which is the square root of 512 and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1618" target="_blank">00:26:58.860</a></span> | <span class="t">apply the softmax which is not introducing any parameter, so for now the self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1624" target="_blank">00:27:04.100</a></span> | <span class="t">didn't require any parameter except for the embedding of the words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1630" target="_blank">00:27:10.100</a></span> | <span class="t">This will change later when we introduce the multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1634" target="_blank">00:27:14.860</a></span> | <span class="t">So we expect, because each value in the self-attention, in the softmax matrix, is a dot product of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1643" target="_blank">00:27:23.020</a></span> | <span class="t">the word embedding with itself and the other words, we expect the values along the diagonal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1648" target="_blank">00:27:28.220</a></span> | <span class="t">to be the maximum, because it's the dot product of each word with itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1655" target="_blank">00:27:35.420</a></span> | <span class="t">And there is another property of this matrix, that is, before we apply the softmax, if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1663" target="_blank">00:27:43.620</a></span> | <span class="t">replace the value in this matrix, suppose we don't want the word your and cat to interact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1670" target="_blank">00:27:50.100</a></span> | <span class="t">with each other, or we don't want the word, let's say, is and the lovely to interact with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1674" target="_blank">00:27:54.820</a></span> | <span class="t">each other, what we can do is, before we apply the softmax, we can replace this value with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1679" target="_blank">00:27:59.980</a></span> | <span class="t">-infinity and also this value with -infinity, and when we apply the softmax, the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1688" target="_blank">00:28:08.900</a></span> | <span class="t">will replace -infinity with 0, because as you remember the softmax is e to the power</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1695" target="_blank">00:28:15.300</a></span> | <span class="t">of x, if x is going to -infinity, e to the power of -infinity will become very very close</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1701" target="_blank">00:28:21.700</a></span> | <span class="t">to 0, so basically 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1705" target="_blank">00:28:25.580</a></span> | <span class="t">This is a desirable property that we will use in the decoder of the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1710" target="_blank">00:28:30.660</a></span> | <span class="t">Now let's have a look at what is a multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1714" target="_blank">00:28:34.140</a></span> | <span class="t">So what we just saw was the self attention, and we want to convert it into a multi-head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1719" target="_blank">00:28:39.420</a></span> | <span class="t">attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1720" target="_blank">00:28:40.420</a></span> | <span class="t">You may have seen these expressions from the paper, but don't worry, I will explain them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1724" target="_blank">00:28:44.500</a></span> | <span class="t">one by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1725" target="_blank">00:28:45.500</a></span> | <span class="t">So let's go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1727" target="_blank">00:28:47.540</a></span> | <span class="t">Imagine we have our encoder, so we are on the encoder side of the transformer, and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1732" target="_blank">00:28:52.900</a></span> | <span class="t">have our input sentence, which is, let's say, 6 by 512, so 6 word by 512 is the size of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1741" target="_blank">00:29:01.300</a></span> | <span class="t">the embedding of each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1743" target="_blank">00:29:03.280</a></span> | <span class="t">In this case I call it sequence by dmodel, so sequence is the sequence length, as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1747" target="_blank">00:29:07.980</a></span> | <span class="t">can see on the legend in the bottom left of the slide, and the dmodel is the size of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1754" target="_blank">00:29:14.300</a></span> | <span class="t">embedding vector, which is 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1757" target="_blank">00:29:17.740</a></span> | <span class="t">What we do, just like the picture shows, we take this input and we make 4 copies of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1765" target="_blank">00:29:25.100</a></span> | <span class="t">One will be sent along this connection we can see here, and 3 will be sent to the multi-head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1774" target="_blank">00:29:34.380</a></span> | <span class="t">attention with 3 respective names, so it's the same input that becomes 3 matrices that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1781" target="_blank">00:29:41.360</a></span> | <span class="t">are equal to input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1783" target="_blank">00:29:43.160</a></span> | <span class="t">One is called query, one is called key, and one is called value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1787" target="_blank">00:29:47.160</a></span> | <span class="t">So basically we are taking this input and making 3 copies of it, one we call q, k, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1791" target="_blank">00:29:51.880</a></span> | <span class="t">b.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1792" target="_blank">00:29:52.880</a></span> | <span class="t">They have, of course, the same dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1795" target="_blank">00:29:55.240</a></span> | <span class="t">What does the multi-head attention do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1797" target="_blank">00:29:57.080</a></span> | <span class="t">First of all it multiplies these 3 matrices by 3 parameter matrices called wq, wk, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1804" target="_blank">00:30:04.800</a></span> | <span class="t">wv.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1807" target="_blank">00:30:07.120</a></span> | <span class="t">These matrices have dimension dmodel by dmodel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1810" target="_blank">00:30:10.640</a></span> | <span class="t">So if we multiply a matrix that is sequence by dmodel with another one that is dmodel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1815" target="_blank">00:30:15.520</a></span> | <span class="t">by dmodel, we get a new matrix as output that is sequence by dmodel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1821" target="_blank">00:30:21.420</a></span> | <span class="t">So basically the same dimension as the starting matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1826" target="_blank">00:30:26.140</a></span> | <span class="t">And we will call them q', k' and v'.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1830" target="_blank">00:30:30.560</a></span> | <span class="t">Our next step is to split these matrices into smaller matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1835" target="_blank">00:30:35.560</a></span> | <span class="t">Let's see how.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1837" target="_blank">00:30:37.160</a></span> | <span class="t">We can split this matrix q' by the sequence dimension or by the dmodel dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1844" target="_blank">00:30:44.720</a></span> | <span class="t">In the multi-head attention we always split by the dmodel dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1848" target="_blank">00:30:48.780</a></span> | <span class="t">So every head will see the full sentence but a smaller part of the embedding of each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1857" target="_blank">00:30:57.280</a></span> | <span class="t">So if we have an embedding of let's say 512, it will become smaller embeddings of 512 divided</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1865" target="_blank">00:31:05.280</a></span> | <span class="t">by 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1866" target="_blank">00:31:06.860</a></span> | <span class="t">And we call this quantity dk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1869" target="_blank">00:31:09.020</a></span> | <span class="t">So dk is dmodel divided by h, where h is the number of heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1873" target="_blank">00:31:13.600</a></span> | <span class="t">In our case we have h equal to 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1877" target="_blank">00:31:17.560</a></span> | <span class="t">We can calculate the attention between these smaller matrices, so q1, k1 and v1 using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1883" target="_blank">00:31:23.860</a></span> | <span class="t">expression taken from the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1888" target="_blank">00:31:28.600</a></span> | <span class="t">And this will result into a small matrix called head1, head2, head3 and head4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1896" target="_blank">00:31:36.460</a></span> | <span class="t">The dimension of head1 up to head4 is sequence by dv.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1903" target="_blank">00:31:43.200</a></span> | <span class="t">What is dv is basically it's equal to dk, it's just called dv because the last multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1909" target="_blank">00:31:49.740</a></span> | <span class="t">is done by v and in the paper they call it dv so I'm also sticking to the same names.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1916" target="_blank">00:31:56.140</a></span> | <span class="t">Our next step is to combine these matrices, these small heads, by concatenating them along</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1925" target="_blank">00:32:05.560</a></span> | <span class="t">the dv dimension, just like the paper says.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1929" target="_blank">00:32:09.660</a></span> | <span class="t">So we concat all these heads together and we get a new matrix that is sequence by h</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1936" target="_blank">00:32:16.380</a></span> | <span class="t">multiplied by dv, where h multiplied by dv, as we know dv is equal to dk, so h multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1944" target="_blank">00:32:24.500</a></span> | <span class="t">by dv is equal to dmodel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1946" target="_blank">00:32:26.820</a></span> | <span class="t">So we get back the initial shape, so it's sequence by dmodel here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1955" target="_blank">00:32:35.040</a></span> | <span class="t">The next step is to multiply the result of this concatenation by wo and wo is a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1962" target="_blank">00:32:42.160</a></span> | <span class="t">that is h multiplied by dv, so dmodel, with the other dimension being dmodel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1968" target="_blank">00:32:48.960</a></span> | <span class="t">And the result of this is a new matrix that is the result of the multi-head attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1973" target="_blank">00:32:53.700</a></span> | <span class="t">which is sequence by dmodel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1976" target="_blank">00:32:56.920</a></span> | <span class="t">So the multi-head attention, instead of calculating the attention between these matrices here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1984" target="_blank">00:33:04.000</a></span> | <span class="t">so q', k' and v', splits them along the dmodel dimension into smaller matrices and calculates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1992" target="_blank">00:33:12.720</a></span> | <span class="t">the attention between these smaller matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=1995" target="_blank">00:33:15.720</a></span> | <span class="t">So each head is watching the full sentence but a different aspect of the embedding of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2002" target="_blank">00:33:22.160</a></span> | <span class="t">each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2003" target="_blank">00:33:23.640</a></span> | <span class="t">Why we want this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2005" target="_blank">00:33:25.100</a></span> | <span class="t">Because we want each head to watch different aspect of the same word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2010" target="_blank">00:33:30.480</a></span> | <span class="t">For example, in the Chinese language, but also in other languages, one word may be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2015" target="_blank">00:33:35.640</a></span> | <span class="t">noun in some cases, may be a verb in some other cases, may be an adverb in some other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2020" target="_blank">00:33:40.400</a></span> | <span class="t">cases, depending on the context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2023" target="_blank">00:33:43.380</a></span> | <span class="t">So what we want is that one head maybe learns to relate that word as a noun, another head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2030" target="_blank">00:33:50.480</a></span> | <span class="t">maybe learns to relate that word as a verb, and another head learns to relate that verb</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2036" target="_blank">00:33:56.120</a></span> | <span class="t">as an adjective or an adverb.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2039" target="_blank">00:33:59.000</a></span> | <span class="t">So this is why we want multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2043" target="_blank">00:34:03.160</a></span> | <span class="t">Now you may also have seen online that the attention can be visualized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2049" target="_blank">00:34:09.720</a></span> | <span class="t">And I will show you how.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2052" target="_blank">00:34:12.040</a></span> | <span class="t">When we calculate the attention between the q and the k matrices, so when we do this operation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2058" target="_blank">00:34:18.800</a></span> | <span class="t">so the softmax of q multiplied by the k divided by the square root of dk, we get a new matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2066" target="_blank">00:34:26.240</a></span> | <span class="t">just like we saw before, which is sequence by sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2069" target="_blank">00:34:29.760</a></span> | <span class="t">And this represents a score that represents the intensity of the relationship between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2075" target="_blank">00:34:35.120</a></span> | <span class="t">the two words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2077" target="_blank">00:34:37.160</a></span> | <span class="t">We can visualize this, and this will produce a visualization similar to this one, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2084" target="_blank">00:34:44.240</a></span> | <span class="t">I took from the paper, in which we see how all the heads work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2088" target="_blank">00:34:48.360</a></span> | <span class="t">So for example, if we concentrate on this word "making", this word here, we can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2093" target="_blank">00:34:53.560</a></span> | <span class="t">that "making" is related to the word "difficult", so this word here, by different heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2099" target="_blank">00:34:59.300</a></span> | <span class="t">So the blue head, the red head, and the green head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2103" target="_blank">00:35:03.560</a></span> | <span class="t">But let's say the violet head is not relating these two words together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2109" target="_blank">00:35:09.500</a></span> | <span class="t">So "making" and "difficult" is not related by the violet or the pink head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2114" target="_blank">00:35:14.920</a></span> | <span class="t">The violet head or the pink head, they are relating the word "making" to other words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2120" target="_blank">00:35:20.300</a></span> | <span class="t">for example to this word "2009".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2125" target="_blank">00:35:25.080</a></span> | <span class="t">Why this is the case?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2126" target="_blank">00:35:26.480</a></span> | <span class="t">Because maybe this pink head could see the part of the embedding that these other heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2132" target="_blank">00:35:32.080</a></span> | <span class="t">could not see, that made this interaction possible between these two words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2141" target="_blank">00:35:41.120</a></span> | <span class="t">You may be also wondering why these three matrices are called "query keys" and "values".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2147" target="_blank">00:35:47.080</a></span> | <span class="t">Okay, the terms come from the database terminology, or from the Python-like dictionaries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2153" target="_blank">00:35:53.520</a></span> | <span class="t">But I would also like to give an interpretation of my own, making a very simple example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2157" target="_blank">00:35:57.960</a></span> | <span class="t">I think it's quite easy to understand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2163" target="_blank">00:36:03.540</a></span> | <span class="t">So imagine we have a Python-like dictionary, or a database, in which we have keys and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2170" target="_blank">00:36:10.180</a></span> | <span class="t">The keys are the category of movies, and the values are the movies belonging to that category.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2176" target="_blank">00:36:16.360</a></span> | <span class="t">In my case, I just put one value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2179" target="_blank">00:36:19.800</a></span> | <span class="t">So we have Romantics category, which includes Titanic, we have action movies that include</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2185" target="_blank">00:36:25.160</a></span> | <span class="t">The Dark Knight, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2187" target="_blank">00:36:27.280</a></span> | <span class="t">Imagine we also have a user that makes a query, and the query is "love".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2192" target="_blank">00:36:32.980</a></span> | <span class="t">Because we are in the transformer world, all these words actually are represented by embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2197" target="_blank">00:36:37.880</a></span> | <span class="t">of size 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2200" target="_blank">00:36:40.440</a></span> | <span class="t">So what our transformer will do, he will convert this word "love" into an embedding of 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2206" target="_blank">00:36:46.680</a></span> | <span class="t">All these queries and values are already embeddings of 512, and it will calculate the dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2213" target="_blank">00:36:53.480</a></span> | <span class="t">between the query and all the keys, just like the formula.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2217" target="_blank">00:36:57.960</a></span> | <span class="t">So as you remember, the formula is a softmax of query multiplied by the transpose of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2222" target="_blank">00:37:02.780</a></span> | <span class="t">keys, divided by the square root of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2226" target="_blank">00:37:06.320</a></span> | <span class="t">So we are doing the dot product of all the queries with all the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2231" target="_blank">00:37:11.140</a></span> | <span class="t">In this case, the word "love" with all the keys, one by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2235" target="_blank">00:37:15.520</a></span> | <span class="t">And this will result in a score that will amplify some values or not amplify other values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2245" target="_blank">00:37:25.920</a></span> | <span class="t">In this case, our embedding may be in such a way that the word "love" and "romantic"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2251" target="_blank">00:37:31.560</a></span> | <span class="t">are related to each other, the word "love" and "comedy" are also related to each other,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2257" target="_blank">00:37:37.000</a></span> | <span class="t">but not so intensively like the word "love" and "romantic".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2261" target="_blank">00:37:41.280</a></span> | <span class="t">So it's more, how to say, less strong relationship.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2266" target="_blank">00:37:46.000</a></span> | <span class="t">But maybe the word "horror" and "love" are not related at all, so maybe their softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2270" target="_blank">00:37:50.700</a></span> | <span class="t">score is very close to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2276" target="_blank">00:37:56.600</a></span> | <span class="t">Our next layer in the encoder is the add and norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2282" target="_blank">00:38:02.520</a></span> | <span class="t">And to introduce the add and norm, we need the layer normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2285" target="_blank">00:38:05.820</a></span> | <span class="t">So let's see what is the layer normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2288" target="_blank">00:38:08.120</a></span> | <span class="t">Layer normalization is a layer that, okay, let's make a practical example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2295" target="_blank">00:38:15.480</a></span> | <span class="t">Imagine we have a batch of n items, in this case n is equal to 3, item 1, item 2, item</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2304" target="_blank">00:38:24.240</a></span> | <span class="t">3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2305" target="_blank">00:38:25.240</a></span> | <span class="t">Each of these items will have some features, it could be an embedding, so for example it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2310" target="_blank">00:38:30.360</a></span> | <span class="t">could be a feature of a vector of size 512, but it could be a very big matrix of thousands</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2316" target="_blank">00:38:36.100</a></span> | <span class="t">of features, doesn't matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2318" target="_blank">00:38:38.320</a></span> | <span class="t">What we do is we calculate the mean and the variance of each of these items independently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2323" target="_blank">00:38:43.320</a></span> | <span class="t">from each other, and we replace each value with another value that is given by this expression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2329" target="_blank">00:38:49.820</a></span> | <span class="t">So basically we are normalizing so that the new values are all in the range 0 to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2337" target="_blank">00:38:57.160</a></span> | <span class="t">Actually we also multiply this new value with a parameter called gamma, and then we add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2343" target="_blank">00:39:03.900</a></span> | <span class="t">another parameter called beta, and this gamma and beta are learnable parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2350" target="_blank">00:39:10.280</a></span> | <span class="t">And the model should learn to multiply and add these parameters so as to amplify the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2357" target="_blank">00:39:17.180</a></span> | <span class="t">value that it wants to be amplified and not amplify the value that it doesn't want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2362" target="_blank">00:39:22.320</a></span> | <span class="t">be amplified.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2365" target="_blank">00:39:25.260</a></span> | <span class="t">So we don't just normalize, we actually introduce some parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2370" target="_blank">00:39:30.360</a></span> | <span class="t">And I found a really nice visualization from paperswithcode.com in which we see the difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2376" target="_blank">00:39:36.420</a></span> | <span class="t">between batch norm and layer norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2379" target="_blank">00:39:39.060</a></span> | <span class="t">So as we can see in the layer normalization we are calculating if n is the batch dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2385" target="_blank">00:39:45.720</a></span> | <span class="t">we are calculating all the values belonging to one item in the batch, while in the batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2391" target="_blank">00:39:51.880</a></span> | <span class="t">norm we are calculating the same feature for all the batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2397" target="_blank">00:39:57.440</a></span> | <span class="t">So for all the items in the batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2399" target="_blank">00:39:59.800</a></span> | <span class="t">So we are mixing, let's say, values from different items of the batch, while in the layer normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2406" target="_blank">00:40:06.200</a></span> | <span class="t">we are treating each item in the batch independently, which will have its own mean and its own variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2415" target="_blank">00:40:15.000</a></span> | <span class="t">Let's look at the decoder now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2417" target="_blank">00:40:17.440</a></span> | <span class="t">Now, in the encoder we saw the input embeddings, in this case they are called output embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2424" target="_blank">00:40:24.600</a></span> | <span class="t">but the underlying working is the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2427" target="_blank">00:40:27.960</a></span> | <span class="t">Here also we have the positional encoding, and they are also the same as the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2435" target="_blank">00:40:35.880</a></span> | <span class="t">The next layer is the masked multihead attention, and we will see it now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2441" target="_blank">00:40:41.160</a></span> | <span class="t">We also have the multihead attention here, here we should see that there is the encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2449" target="_blank">00:40:49.980</a></span> | <span class="t">here that produces the output and is sent to the decoder in the forms of keys and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2462" target="_blank">00:41:02.240</a></span> | <span class="t">While the query, so this connection here is the query coming from the decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2468" target="_blank">00:41:08.120</a></span> | <span class="t">So in this multihead attention, it's not a self-attention anymore, it's a cross-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2474" target="_blank">00:41:14.040</a></span> | <span class="t">because we are taking two sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2476" target="_blank">00:41:16.420</a></span> | <span class="t">One is sent from the encoder side, so let's write encoder, in which we provide the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2482" target="_blank">00:41:22.480</a></span> | <span class="t">of the encoder and we use it as keys and values, while the output of the masked multihead attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2490" target="_blank">00:41:30.160</a></span> | <span class="t">is used as the query in this multihead attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2494" target="_blank">00:41:34.840</a></span> | <span class="t">The masked multihead attention is a self-attention of the input sentence of the decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2500" target="_blank">00:41:40.760</a></span> | <span class="t">So we take the input sentence of the decoder, we transform into embeddings, we add the positional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2506" target="_blank">00:41:46.880</a></span> | <span class="t">encoding, we give it to this multihead attention in which the query key and values are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2511" target="_blank">00:41:51.520</a></span> | <span class="t">same input sequence, we do the add and norm, then we send this as the queries of the multihead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2520" target="_blank">00:42:00.080</a></span> | <span class="t">attention, while the keys and the values are coming from the encoder, then we do the add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2524" target="_blank">00:42:04.680</a></span> | <span class="t">and norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2525" target="_blank">00:42:05.680</a></span> | <span class="t">I will not be showing the feedforward, which is just a fully connected layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2530" target="_blank">00:42:10.840</a></span> | <span class="t">We then send the output of the feedforward to the add and norm, and finally to the linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2536" target="_blank">00:42:16.040</a></span> | <span class="t">layer, which we will see later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2538" target="_blank">00:42:18.640</a></span> | <span class="t">So let's have a look at the masked multihead attention and how it differs from a normal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2543" target="_blank">00:42:23.300</a></span> | <span class="t">multihead attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2546" target="_blank">00:42:26.360</a></span> | <span class="t">What we want, our goal, is that we want to make the model causal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2551" target="_blank">00:42:31.080</a></span> | <span class="t">It means that the output at a certain position can only depend on the words on the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2556" target="_blank">00:42:36.880</a></span> | <span class="t">position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2557" target="_blank">00:42:37.880</a></span> | <span class="t">So the model must not be able to see future words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2561" target="_blank">00:42:41.400</a></span> | <span class="t">How can we achieve that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2564" target="_blank">00:42:44.120</a></span> | <span class="t">As you saw, the output of the softmax in the attention calculation formula is this metric,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2571" target="_blank">00:42:51.240</a></span> | <span class="t">sequence by sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2572" target="_blank">00:42:52.240</a></span> | <span class="t">If we want to hide the interaction of some words with other words, we delete this value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2578" target="_blank">00:42:58.320</a></span> | <span class="t">and we replace it with minus infinity before we apply the softmax, so that the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2584" target="_blank">00:43:04.540</a></span> | <span class="t">will replace this value with zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2588" target="_blank">00:43:08.520</a></span> | <span class="t">And we do this for all the interaction that we don't want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2592" target="_blank">00:43:12.100</a></span> | <span class="t">So we don't want your to watch future words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2595" target="_blank">00:43:15.000</a></span> | <span class="t">So we don't want your to watch cat is a lovely cat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2599" target="_blank">00:43:19.080</a></span> | <span class="t">And we don't want the word cat to watch future words, but only all the words that come before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2603" target="_blank">00:43:23.880</a></span> | <span class="t">it or the word itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2605" target="_blank">00:43:25.920</a></span> | <span class="t">So we don't want this, this, this, this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2609" target="_blank">00:43:29.160</a></span> | <span class="t">Also the same for the other words, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2612" target="_blank">00:43:32.880</a></span> | <span class="t">So we can see that we are replacing all the word, all these values here that are above</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2620" target="_blank">00:43:40.000</a></span> | <span class="t">this diagonal here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2621" target="_blank">00:43:41.840</a></span> | <span class="t">So this is the principal diagonal of the matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2624" target="_blank">00:43:44.480</a></span> | <span class="t">And we want all the values that are above this diagonal to be replaced with minus infinity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2630" target="_blank">00:43:50.160</a></span> | <span class="t">so that, so that the softmax will replace them with zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2634" target="_blank">00:43:54.840</a></span> | <span class="t">Let's see in which stage of the multi-head attention this mechanism is introduced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2640" target="_blank">00:44:00.960</a></span> | <span class="t">So when we calculate the attention between these molar matrices, so Q1, K1, and V1, before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2649" target="_blank">00:44:09.720</a></span> | <span class="t">we apply the softmax, we replace these values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2652" target="_blank">00:44:12.960</a></span> | <span class="t">So this one, this one, this one, this one, this one, etc. with minus infinity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2658" target="_blank">00:44:18.720</a></span> | <span class="t">Then we apply the softmax and then the softmax will take care of transforming these values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2665" target="_blank">00:44:25.040</a></span> | <span class="t">into zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2666" target="_blank">00:44:26.200</a></span> | <span class="t">So basically we don't want these words to interact with each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2671" target="_blank">00:44:31.440</a></span> | <span class="t">And if we don't want this interaction, the model will learn to not make them interact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2675" target="_blank">00:44:35.480</a></span> | <span class="t">because the model will not get any information from this interaction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2679" target="_blank">00:44:39.040</a></span> | <span class="t">So it's like this word cannot interact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2681" target="_blank">00:44:41.620</a></span> | <span class="t">Now let's look at how the inference and training works for a transformer model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2686" target="_blank">00:44:46.780</a></span> | <span class="t">As I said previously, we are dealing with it, we will be dealing with a translation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2692" target="_blank">00:44:52.360</a></span> | <span class="t">task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2693" target="_blank">00:44:53.360</a></span> | <span class="t">So because it's easy to visualize and it's easy to understand all the steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2698" target="_blank">00:44:58.200</a></span> | <span class="t">Let's start with the training of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2701" target="_blank">00:45:01.040</a></span> | <span class="t">We will go from an English sentence "I love you very much" into an Italian sentence "Ti</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2705" target="_blank">00:45:05.960</a></span> | <span class="t">amo molto".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2706" target="_blank">00:45:06.960</a></span> | <span class="t">It's a very simple sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2707" target="_blank">00:45:07.960</a></span> | <span class="t">It's easy to describe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2711" target="_blank">00:45:11.440</a></span> | <span class="t">Let's go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2712" target="_blank">00:45:12.800</a></span> | <span class="t">We start with a description of the transformer model and we start with our English sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2720" target="_blank">00:45:20.240</a></span> | <span class="t">which is sent to the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2722" target="_blank">00:45:22.460</a></span> | <span class="t">So our English sentence here on which we prepend and append two special tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2729" target="_blank">00:45:29.200</a></span> | <span class="t">One is called start of sentence and one is called end of sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2733" target="_blank">00:45:33.520</a></span> | <span class="t">These two tokens are taken from the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2736" target="_blank">00:45:36.960</a></span> | <span class="t">So they are special tokens in our vocabulary that tells the model what is the start position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2743" target="_blank">00:45:43.120</a></span> | <span class="t">of a sentence and what is the end of a sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2746" target="_blank">00:45:46.480</a></span> | <span class="t">We will see later why we need them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2749" target="_blank">00:45:49.240</a></span> | <span class="t">For now just think that we take our sentence, we prepend a special token and we append a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2753" target="_blank">00:45:53.920</a></span> | <span class="t">special token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2756" target="_blank">00:45:56.160</a></span> | <span class="t">Then what we do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2757" target="_blank">00:45:57.160</a></span> | <span class="t">As you can see from the picture, we take our inputs, we transform into input embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2762" target="_blank">00:46:02.000</a></span> | <span class="t">we add the positional encoding and then we send it to the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2766" target="_blank">00:46:06.560</a></span> | <span class="t">So this is our encoder input, sequence by D model, we send it to the encoder, it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2771" target="_blank">00:46:11.040</a></span> | <span class="t">produce an output which is sequence by D model and it's called encoder output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2777" target="_blank">00:46:17.600</a></span> | <span class="t">So as we saw previously, the output of the encoder is another matrix that has the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2783" target="_blank">00:46:23.560</a></span> | <span class="t">dimension as the input matrix in which the embedding, we can see it as a sequence of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2791" target="_blank">00:46:31.120</a></span> | <span class="t">embeddings in which this embedding is special because it captures not only the meaning of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2796" target="_blank">00:46:36.160</a></span> | <span class="t">the word which was given by the input embedding we saw here, so by this, not only the position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2802" target="_blank">00:46:42.480</a></span> | <span class="t">which was given by the positional encoding, but also the interaction of every word with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2808" target="_blank">00:46:48.420</a></span> | <span class="t">every other word in the same sentence because this is the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2812" target="_blank">00:46:52.520</a></span> | <span class="t">So we are talking about self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2815" target="_blank">00:46:55.080</a></span> | <span class="t">So it's the interaction of each word in the sentence with all the other words in the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2820" target="_blank">00:47:00.240</a></span> | <span class="t">sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2823" target="_blank">00:47:03.040</a></span> | <span class="t">We want to convert this sentence into Italian, so we prepare the input of the decoder, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2828" target="_blank">00:47:08.440</a></span> | <span class="t">is start of sentence "ti amo molto".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2832" target="_blank">00:47:12.180</a></span> | <span class="t">As you can see from the picture of the transformer, the outputs here you can see shifted right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2839" target="_blank">00:47:19.060</a></span> | <span class="t">What does it mean to shift right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2840" target="_blank">00:47:20.060</a></span> | <span class="t">Basically, it means we prepend a special token called SOS, start of sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2846" target="_blank">00:47:26.880</a></span> | <span class="t">You should also notice that these two sequences actually, when we code the transformer, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2855" target="_blank">00:47:35.300</a></span> | <span class="t">if you watch my other video on how to code a transformer, you will see that we make this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2859" target="_blank">00:47:39.920</a></span> | <span class="t">sequence of fixed length so that if we have a sentence that is "ti amo molto" or a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2864" target="_blank">00:47:44.840</a></span> | <span class="t">long sequence, actually when we feed them to the transformer, they all become of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2870" target="_blank">00:47:50.680</a></span> | <span class="t">same length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2873" target="_blank">00:47:53.360</a></span> | <span class="t">How to do this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2874" target="_blank">00:47:54.360</a></span> | <span class="t">We add padding words to reach the desired length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2878" target="_blank">00:47:58.360</a></span> | <span class="t">So if our model can support, let's say, a sequence length of 1000, in this case we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2883" target="_blank">00:48:03.800</a></span> | <span class="t">4 tokens, we will add 996 tokens of padding to make this sentence long enough to reach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2892" target="_blank">00:48:12.280</a></span> | <span class="t">the sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2893" target="_blank">00:48:13.640</a></span> | <span class="t">Of course, I'm not doing it here because it's not easy to visualize otherwise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2897" target="_blank">00:48:17.760</a></span> | <span class="t">Okay, we prepare this input for the decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2901" target="_blank">00:48:21.640</a></span> | <span class="t">We add transform into embeddings, we add the positional encoding, then we send it first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2908" target="_blank">00:48:28.360</a></span> | <span class="t">to the multi-head attention, to the masked multi-head attention, so along with the causal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2912" target="_blank">00:48:32.680</a></span> | <span class="t">mask and then we take the output of the encoder and we send it to the decoder as keys and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2921" target="_blank">00:48:41.360</a></span> | <span class="t">values while the queries are coming from the mask, so the queries are coming from this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2927" target="_blank">00:48:47.760</a></span> | <span class="t">layer and the keys and the values are the output of the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2933" target="_blank">00:48:53.200</a></span> | <span class="t">The output of all this block here, so all this big block here, will be a matrix that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2940" target="_blank">00:49:00.400</a></span> | <span class="t">is sequenced by the model, just like for the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2944" target="_blank">00:49:04.720</a></span> | <span class="t">However, we can see that this is still an embedding because it's a D model, it's a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2951" target="_blank">00:49:11.280</a></span> | <span class="t">of size 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2952" target="_blank">00:49:12.920</a></span> | <span class="t">How can we relate this embedding back into our dictionary?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2957" target="_blank">00:49:17.840</a></span> | <span class="t">How can we understand what is this word in our vocabulary?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2963" target="_blank">00:49:23.520</a></span> | <span class="t">That's why we need a linear layer that will map sequence by D model into another sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2969" target="_blank">00:49:29.960</a></span> | <span class="t">by vocabulary size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2971" target="_blank">00:49:31.720</a></span> | <span class="t">So it will tell for every embedding that it sees what is the position of that word in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2977" target="_blank">00:49:37.560</a></span> | <span class="t">our vocabulary, so that we can understand what is the actual token that is output by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2982" target="_blank">00:49:42.860</a></span> | <span class="t">the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2986" target="_blank">00:49:46.180</a></span> | <span class="t">After that we apply the softmax and then we have our label, what we expect the model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2994" target="_blank">00:49:54.000</a></span> | <span class="t">output given this English sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=2999" target="_blank">00:49:59.980</a></span> | <span class="t">We expect the model to output this "ti amo molto" end of sentence and this is called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3006" target="_blank">00:50:06.480</a></span> | <span class="t">the label or the target.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3008" target="_blank">00:50:08.680</a></span> | <span class="t">What we do when we have the output of the model and the corresponding label?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3013" target="_blank">00:50:13.080</a></span> | <span class="t">We calculate the loss, in this case is the cross entropy loss, and then we backpropagate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3018" target="_blank">00:50:18.640</a></span> | <span class="t">the loss to all the weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3021" target="_blank">00:50:21.360</a></span> | <span class="t">Now let's understand why we have these special tokens called SOS and EOS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3027" target="_blank">00:50:27.320</a></span> | <span class="t">Basically you can see that here the sequence length is 4, actually it's 1000 because we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3032" target="_blank">00:50:32.000</a></span> | <span class="t">have the embedding, but let's say we don't have any embedding, so it's 4 tokens, start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3036" target="_blank">00:50:36.160</a></span> | <span class="t">of sentence "ti amo molto" and what we want is "ti amo molto" end of sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3042" target="_blank">00:50:42.100</a></span> | <span class="t">So our model, when it will see the start of sentence token, it will output the first token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3049" target="_blank">00:50:49.840</a></span> | <span class="t">as output "ti".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3051" target="_blank">00:50:51.920</a></span> | <span class="t">When it will see "ti" it will output "amo", when it will see "amo" it will output "molto"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3059" target="_blank">00:50:59.400</a></span> | <span class="t">and when it will see "molto" it will output end of sentence, which will indicate that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3065" target="_blank">00:51:05.240</a></span> | <span class="t">ok, the translation is done, and we will see this mechanism in the inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3071" target="_blank">00:51:11.920</a></span> | <span class="t">Ah, this all happens in one time step, just like I promised at the beginning of the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3079" target="_blank">00:51:19.400</a></span> | <span class="t">I said that with recurrent neural networks we have n time steps to map n input sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3087" target="_blank">00:51:27.160</a></span> | <span class="t">into n output sequence, but this problem would be solved with the transformer, yes, it has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3093" target="_blank">00:51:33.880</a></span> | <span class="t">been solved, because you can see here we didn't do any for loop, we just did all in one pass,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3100" target="_blank">00:51:40.160</a></span> | <span class="t">we give an input sequence to the encoder, an input sequence to the decoder, we produced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3106" target="_blank">00:51:46.240</a></span> | <span class="t">some outputs, we calculated the cross entropy loss with the label and that's it, it all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3112" target="_blank">00:51:52.240</a></span> | <span class="t">happens in one time step, and this is the power of the transformer, because it made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3117" target="_blank">00:51:57.200</a></span> | <span class="t">it very easy and very fast to train very long sequences and with very very nice performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3124" target="_blank">00:52:04.480</a></span> | <span class="t">that you can see in chatGPT, you can see in GPT, in BERT, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3130" target="_blank">00:52:10.400</a></span> | <span class="t">Let's have a look at how inference works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3134" target="_blank">00:52:14.560</a></span> | <span class="t">Again we have our English sentence "I love you very much", we want to map it into an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3138" target="_blank">00:52:18.740</a></span> | <span class="t">Italian sentence "ti amo molto".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3142" target="_blank">00:52:22.720</a></span> | <span class="t">We have our usual transformer, we prepare the input for the encoder, which is start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3148" target="_blank">00:52:28.320</a></span> | <span class="t">of sentence "I love you very much", end of sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3151" target="_blank">00:52:31.920</a></span> | <span class="t">We convert into input embeddings, then we add the positional encoding, we prepare the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3155" target="_blank">00:52:35.680</a></span> | <span class="t">input for the encoder and we send it to the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3159" target="_blank">00:52:39.000</a></span> | <span class="t">The encoder will produce an output, which is sequenced by the model, and we saw it before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3163" target="_blank">00:52:43.320</a></span> | <span class="t">that it's a sequence of special embeddings that capture the meaning, the position, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3168" target="_blank">00:52:48.040</a></span> | <span class="t">also the interaction of all the words with other words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3172" target="_blank">00:52:52.720</a></span> | <span class="t">What we do is, for the decoder, we give him just the start of sentence, and of course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3179" target="_blank">00:52:59.280</a></span> | <span class="t">we add enough embedding tokens to reach our sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3184" target="_blank">00:53:04.680</a></span> | <span class="t">We just give the model the start of sentence token, and again, for this single token we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3191" target="_blank">00:53:11.520</a></span> | <span class="t">convert into embeddings, we add the positional encoding and we send it to the decoder as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3196" target="_blank">00:53:16.880</a></span> | <span class="t">decoder input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3198" target="_blank">00:53:18.440</a></span> | <span class="t">The decoder will take this, his input as a query, and the key and the values coming from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3205" target="_blank">00:53:25.160</a></span> | <span class="t">the encoder, and it will produce an output, which is sequenced by the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3211" target="_blank">00:53:31.400</a></span> | <span class="t">Again, we want the linear layer to project it back to our vocabulary, and this projection</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3216" target="_blank">00:53:36.960</a></span> | <span class="t">is called logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3219" target="_blank">00:53:39.600</a></span> | <span class="t">What we do is, we apply the softmax, which will select, given the logits, the position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3227" target="_blank">00:53:47.440</a></span> | <span class="t">of the output word will have the maximum score with the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3232" target="_blank">00:53:52.780</a></span> | <span class="t">This is how we know what words to select from the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3237" target="_blank">00:53:57.480</a></span> | <span class="t">And this, hopefully, should produce the first output token, which is T, if the model has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3243" target="_blank">00:54:03.540</a></span> | <span class="t">been trained correctly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3246" target="_blank">00:54:06.060</a></span> | <span class="t">This, however, happens at time step 1, so when we train the model, the transformer model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3251" target="_blank">00:54:11.780</a></span> | <span class="t">it happens in one pass, so we have one input sequence, one output sequence, we give it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3256" target="_blank">00:54:16.400</a></span> | <span class="t">to the model, we do it one time step, and the model will learn it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3260" target="_blank">00:54:20.340</a></span> | <span class="t">When we inference, however, we need to do it token by token, and we will also see why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3264" target="_blank">00:54:24.380</a></span> | <span class="t">this is the case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3267" target="_blank">00:54:27.460</a></span> | <span class="t">At time step 2, we don't need to recompute the encoder output again, because our English</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3276" target="_blank">00:54:36.740</a></span> | <span class="t">sentence didn't change, so we hope the encoder should produce the same output for it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3284" target="_blank">00:54:44.340</a></span> | <span class="t">And then, what we do is, we take the output of the previous sentence, so T, we append</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3292" target="_blank">00:54:52.740</a></span> | <span class="t">it to the input of the decoder, and then we feed it to the decoder, again with the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3300" target="_blank">00:55:00.420</a></span> | <span class="t">of the encoder from the previous step, which will produce an output sequence from the decoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3305" target="_blank">00:55:05.860</a></span> | <span class="t">side, which we again project back into our vocabulary, and we get the next token, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3313" target="_blank">00:55:13.340</a></span> | <span class="t">is AMO.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3314" target="_blank">00:55:14.780</a></span> | <span class="t">So, as I said before, we are not recalculating the output of the encoder for every time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3323" target="_blank">00:55:23.700</a></span> | <span class="t">because our English sentence didn't change at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3326" target="_blank">00:55:26.780</a></span> | <span class="t">What is changing is the input of the decoder, because at every time step, we are appending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3331" target="_blank">00:55:31.060</a></span> | <span class="t">the output of the previous step to the input of the decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3335" target="_blank">00:55:35.220</a></span> | <span class="t">We do the same for the time step 3, and we do the same for the time step 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3342" target="_blank">00:55:42.140</a></span> | <span class="t">And hopefully, we will stop when we see the end of sentence token, because that's how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3348" target="_blank">00:55:48.300</a></span> | <span class="t">the model tells us to stop inferencing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3352" target="_blank">00:55:52.020</a></span> | <span class="t">And this is how the inference works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3354" target="_blank">00:55:54.320</a></span> | <span class="t">Why we needed four time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3357" target="_blank">00:55:57.460</a></span> | <span class="t">When we inference a model, like in this case the translation model, there are many strategies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3364" target="_blank">00:56:04.060</a></span> | <span class="t">for inferencing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3365" target="_blank">00:56:05.300</a></span> | <span class="t">What we used is called greedy strategy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3367" target="_blank">00:56:07.740</a></span> | <span class="t">So for every step, we get the word with the maximum softmax value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3374" target="_blank">00:56:14.500</a></span> | <span class="t">And however this strategy works, usually not bad, but there are better strategies, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3382" target="_blank">00:56:22.700</a></span> | <span class="t">one of them is called beam search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3385" target="_blank">00:56:25.100</a></span> | <span class="t">In beam search, instead of always greedily, so that's why it's called greedy, instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3390" target="_blank">00:56:30.780</a></span> | <span class="t">of greedily taking the maximum soft value, we take the top B values, and then for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3398" target="_blank">00:56:38.780</a></span> | <span class="t">of these choices, we inference what are the next possible tokens for each of the top B</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3405" target="_blank">00:56:45.860</a></span> | <span class="t">values at every step, and we keep only the one with the B most probable sequences, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3413" target="_blank">00:56:53.380</a></span> | <span class="t">we delete the others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3415" target="_blank">00:56:55.440</a></span> | <span class="t">This is called beam search, and generally it performs better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3420" target="_blank">00:57:00.880</a></span> | <span class="t">So thank you guys for watching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3423" target="_blank">00:57:03.560</a></span> | <span class="t">I know it was a long video, but it was really worth it to go through each aspect of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3429" target="_blank">00:57:09.220</a></span> | <span class="t">transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3430" target="_blank">00:57:10.220</a></span> | <span class="t">I hope you enjoyed this journey with me, so please subscribe to the channel, and don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3434" target="_blank">00:57:14.680</a></span> | <span class="t">forget to watch my other video on how to code a transformer model from scratch, in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3439" target="_blank">00:57:19.860</a></span> | <span class="t">I describe not only again the structure of the transformer model while coding it, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3446" target="_blank">00:57:26.040</a></span> | <span class="t">I also show you how to train it on a dataset of your choice, how to inference it, and I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3453" target="_blank">00:57:33.680</a></span> | <span class="t">also provided the code on GitHub, and a Colab notebook to train the model directly on Colab.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3464" target="_blank">00:57:44.480</a></span> | <span class="t">Please subscribe to the channel, and let me know what you didn't understand, so that I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3469" target="_blank">00:57:49.480</a></span> | <span class="t">can give more explanation, and please tell me what are the problems in this kind of videos,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3475" target="_blank">00:57:55.520</a></span> | <span class="t">or in this particular video, that I can improve for the next videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bCz4OMemCcA&t=3480" target="_blank">00:58:00.320</a></span> | <span class="t">Thank you very much, and have a great rest of the day!</span></div></div></body></html>