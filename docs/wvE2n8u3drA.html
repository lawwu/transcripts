<html><head><title>Stanford CS25: V1 I Audio Research: Transformers for Applications in Audio, Speech, Music</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V1 I Audio Research: Transformers for Applications in Audio, Speech, Music</h2><a href="https://www.youtube.com/watch?v=wvE2n8u3drA"><img src="https://i.ytimg.com/vi/wvE2n8u3drA/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=6">0:6</a> Transformers for Music and Audio: Language Modelling to Understanding to Synthesis<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=95">1:35</a> The Transformer Revolution<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=302">5:2</a> Models getting bigger ...<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=463">7:43</a> What are spectograms<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=870">14:30</a> Raw Audio Synthesis: Difficulty Classical FM synthesis Karplus Strong<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1034">17:14</a> Baseline : Classic WaveNet<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1204">20:4</a> Improving Transformer Baseline • Major bottleneck of Transformers<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1262">21:2</a> Results & Unconditioned Setup • Evaluation Criterion o Comparing Wavenet, Transformers on next sample prediction Top-5 accuracy, out of 256 possible states as a error metric Why this setup 7 1. Application agnostic 2. Suits training setup<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1331">22:11</a> A Framework for Generative and Contrastive Learning of Audio Representations<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1358">22:38</a> Acoustic Scene Understanding<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1474">24:34</a> Recipe of doing<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1560">26:0</a> Turbocharging best of two worlds Vector Quantization: A powerful and under-uilized algorithm Combining VQwih auto-encoders and Transformers<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2004">33:24</a> Turbocharging best of two worlds Leaming clusters from vector quantization Use long term dependency kaming with that cluster based representation for markovian assumption Better we become in prediction, the better the summarization is<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2226">37:6</a> Audio Transformers: Transformer Architectures for Large Scale Audio Understanding - Adieu Convolutions Stanford University March 2021<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2325">38:45</a> Wavelets on Transformer Embeddings<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2480">41:20</a> Methodology + Results<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2644">44:4</a> What does it learn -- the front end<br><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2838">47:18</a> Final Thoughts<br><br><div style="text-align: left;"><a href="./wvE2n8u3drA.html">Whisper Transcript</a> | <a href="./transcript_wvE2n8u3drA.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">[AUDIO OUT]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=4" target="_blank">00:00:04.920</a></span> | <span class="t">Thanks for inviting me for the talk today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=7" target="_blank">00:00:07.480</a></span> | <span class="t">And I'll be just talking about transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=10" target="_blank">00:00:10.320</a></span> | <span class="t">for music and audio, which is very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=12" target="_blank">00:00:12.400</a></span> | <span class="t">different than what all of us were doing in this past course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=16" target="_blank">00:00:16.880</a></span> | <span class="t">I'm also the only speaker from Stanford,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=18" target="_blank">00:00:18.840</a></span> | <span class="t">so I have to do a good job.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=20" target="_blank">00:00:20.200</a></span> | <span class="t">So you'll see very good slides, because I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=23" target="_blank">00:00:23.280</a></span> | <span class="t">representing the university in some sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=26" target="_blank">00:00:26.520</a></span> | <span class="t">So yeah, so the flow of the talk for today</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=28" target="_blank">00:00:28.240</a></span> | <span class="t">is basically like I'll be throwing a lot of stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=30" target="_blank">00:00:30.960</a></span> | <span class="t">It's kind of like a buffet style,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=32" target="_blank">00:00:32.440</a></span> | <span class="t">and then you feel free to like or dislike whatever you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=36" target="_blank">00:00:36.640</a></span> | <span class="t">And I'll be talking mostly about three papers of what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=39" target="_blank">00:00:39.560</a></span> | <span class="t">I've been working on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=42" target="_blank">00:00:42.120</a></span> | <span class="t">I'll start with introducing what transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=44" target="_blank">00:00:44.600</a></span> | <span class="t">are from a different perspective, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=46" target="_blank">00:00:46.480</a></span> | <span class="t">audio representations are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=49" target="_blank">00:00:49.240</a></span> | <span class="t">Talk about a generative model for audio,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=52" target="_blank">00:00:52.200</a></span> | <span class="t">which is just doing language modeling on sample level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=56" target="_blank">00:00:56.080</a></span> | <span class="t">Then I'll talk about how can one do like language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=59" target="_blank">00:00:59.080</a></span> | <span class="t">modeling for speech and audio, which is different than what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=62" target="_blank">00:01:02.480</a></span> | <span class="t">people do for text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=64" target="_blank">00:01:04.280</a></span> | <span class="t">What are the current trends in the literature?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=67" target="_blank">00:01:07.360</a></span> | <span class="t">Finally, I'll briefly mention similar stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=71" target="_blank">00:01:11.000</a></span> | <span class="t">as to what was happening in computer vision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=73" target="_blank">00:01:13.080</a></span> | <span class="t">with regard to vision transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=74" target="_blank">00:01:14.920</a></span> | <span class="t">How can we adapt similar ideas for audio transformers?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=77" target="_blank">00:01:17.920</a></span> | <span class="t">And throw in a bit of signal processing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=80" target="_blank">00:01:20.040</a></span> | <span class="t">to improve the performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=82" target="_blank">00:01:22.120</a></span> | <span class="t">Having told that the talk is about 35 to 40 minutes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=84" target="_blank">00:01:24.840</a></span> | <span class="t">with about 15 minutes of Q&A, I should also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=87" target="_blank">00:01:27.600</a></span> | <span class="t">say that all of the opinions are mine,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=89" target="_blank">00:01:29.600</a></span> | <span class="t">and Stanford or any other professor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=91" target="_blank">00:01:31.160</a></span> | <span class="t">is not responsible for any of the mistake which I do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=95" target="_blank">00:01:35.880</a></span> | <span class="t">So transformers have kind of revolutionized in a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=100" target="_blank">00:01:40.240</a></span> | <span class="t">like everyone was approaching deep learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=102" target="_blank">00:01:42.960</a></span> | <span class="t">Before that, it was all about CNNs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=105" target="_blank">00:01:45.240</a></span> | <span class="t">And mostly, all of these prominent models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=108" target="_blank">00:01:48.360</a></span> | <span class="t">have been coming in waves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=109" target="_blank">00:01:49.440</a></span> | <span class="t">So there was a time when everyone was just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=111" target="_blank">00:01:51.200</a></span> | <span class="t">applying CNNs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=113" target="_blank">00:01:53.360</a></span> | <span class="t">Then came a time where people started</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=115" target="_blank">00:01:55.600</a></span> | <span class="t">adapting CNNs in some sort of diluted convolutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=118" target="_blank">00:01:58.480</a></span> | <span class="t">And slowly, the recurrent networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=120" target="_blank">00:02:00.400</a></span> | <span class="t">were getting out of fashion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=122" target="_blank">00:02:02.480</a></span> | <span class="t">Now, it seems like transformers are in fashion all the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=125" target="_blank">00:02:05.960</a></span> | <span class="t">So it seems to be solving almost every single problem which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=128" target="_blank">00:02:08.840</a></span> | <span class="t">is being thrown at them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=132" target="_blank">00:02:12.640</a></span> | <span class="t">So what's special about them?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=135" target="_blank">00:02:15.000</a></span> | <span class="t">One of the fact which struck me was their simplicity, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=139" target="_blank">00:02:19.160</a></span> | <span class="t">is, if you think about it, this--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=143" target="_blank">00:02:23.680</a></span> | <span class="t">and it has been hugely popular also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=145" target="_blank">00:02:25.720</a></span> | <span class="t">So it was just released in 2018.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=148" target="_blank">00:02:28.640</a></span> | <span class="t">And within three years, it has about 30,000 citations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=151" target="_blank">00:02:31.280</a></span> | <span class="t">And it is kind of solving every single problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=153" target="_blank">00:02:33.200</a></span> | <span class="t">in every single domain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=155" target="_blank">00:02:35.400</a></span> | <span class="t">It has its limitations, though, also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=158" target="_blank">00:02:38.560</a></span> | <span class="t">But if you think about it, in a way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=160" target="_blank">00:02:40.720</a></span> | <span class="t">transformers are basically a way of just cascading self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=165" target="_blank">00:02:45.800</a></span> | <span class="t">with feature learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=166" target="_blank">00:02:46.960</a></span> | <span class="t">And if you keep on doing it over and over again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=169" target="_blank">00:02:49.280</a></span> | <span class="t">then the model, in a way, learns which parts of the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=172" target="_blank">00:02:52.160</a></span> | <span class="t">are important and keep on transforming them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=175" target="_blank">00:02:55.120</a></span> | <span class="t">removing the contents which are not important,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=177" target="_blank">00:02:57.360</a></span> | <span class="t">and just have the limited information which is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=180" target="_blank">00:03:00.480</a></span> | <span class="t">responsible for a particular task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=184" target="_blank">00:03:04.120</a></span> | <span class="t">And it has been very, very difficult to keep up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=186" target="_blank">00:03:06.160</a></span> | <span class="t">with the literature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=189" target="_blank">00:03:09.000</a></span> | <span class="t">I have put it as a joke here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=190" target="_blank">00:03:10.760</a></span> | <span class="t">But then even Twitter's recommendation engine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=193" target="_blank">00:03:13.160</a></span> | <span class="t">were kind of just getting out of--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=196" target="_blank">00:03:16.120</a></span> | <span class="t">they were getting haywire as to why is Chris Manning just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=199" target="_blank">00:03:19.360</a></span> | <span class="t">searching over transformers?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=201" target="_blank">00:03:21.480</a></span> | <span class="t">And that was way back in 2020.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=204" target="_blank">00:03:24.480</a></span> | <span class="t">So it has been difficult for researchers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=206" target="_blank">00:03:26.440</a></span> | <span class="t">also to keep up with the pace of what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=209" target="_blank">00:03:29.800</a></span> | <span class="t">Just before transformers, all of the NLP community</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=213" target="_blank">00:03:33.000</a></span> | <span class="t">was just doing gaga about bidirectional LSTMs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=215" target="_blank">00:03:35.760</a></span> | <span class="t">with attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=216" target="_blank">00:03:36.760</a></span> | <span class="t">So every single paper before 2017</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=218" target="_blank">00:03:38.840</a></span> | <span class="t">was just like you have encoded LSTM layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=222" target="_blank">00:03:42.320</a></span> | <span class="t">You keep on adding multiple layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=226" target="_blank">00:03:46.000</a></span> | <span class="t">And then after that, you have attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=228" target="_blank">00:03:48.440</a></span> | <span class="t">which just learns that what's important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=230" target="_blank">00:03:50.600</a></span> | <span class="t">and then just keeps on decoding sequentially one at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=234" target="_blank">00:03:54.760</a></span> | <span class="t">But this was not kind of like an ideal way to do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=238" target="_blank">00:03:58.040</a></span> | <span class="t">Because what turns out is when we start throwing longer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=241" target="_blank">00:04:01.560</a></span> | <span class="t">sequences, the connections are no longer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=247" target="_blank">00:04:07.120</a></span> | <span class="t">storing the gradient updates in a way it should be doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=249" target="_blank">00:04:09.840</a></span> | <span class="t">So what the researchers from Google said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=253" target="_blank">00:04:13.360</a></span> | <span class="t">instead of having just an attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=255" target="_blank">00:04:15.040</a></span> | <span class="t">layer at the very last encoding, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=259" target="_blank">00:04:19.240</a></span> | <span class="t">would just have these attention mechanisms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=261" target="_blank">00:04:21.280</a></span> | <span class="t">at every single layer, which in a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=263" target="_blank">00:04:23.640</a></span> | <span class="t">would just learn what's important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=265" target="_blank">00:04:25.280</a></span> | <span class="t">for a particular problem at that particular layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=268" target="_blank">00:04:28.240</a></span> | <span class="t">And we keep on doing it over and over again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=271" target="_blank">00:04:31.920</a></span> | <span class="t">So then the whole idea of transformers and attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=275" target="_blank">00:04:35.920</a></span> | <span class="t">mechanism cascaded one after the other came.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=279" target="_blank">00:04:39.520</a></span> | <span class="t">And I'll not go into the details,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=280" target="_blank">00:04:40.880</a></span> | <span class="t">because this is the last class of the course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=283" target="_blank">00:04:43.400</a></span> | <span class="t">But then usual tricks do help across the neural net</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=286" target="_blank">00:04:46.600</a></span> | <span class="t">literature, which is like having multi-header tensions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=290" target="_blank">00:04:50.680</a></span> | <span class="t">having skip connection and layer norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=292" target="_blank">00:04:52.280</a></span> | <span class="t">So all of these things, they are not only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=294" target="_blank">00:04:54.440</a></span> | <span class="t">like giving gains for transformers themselves,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=297" target="_blank">00:04:57.800</a></span> | <span class="t">but they can be just applied to any single other architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=301" target="_blank">00:05:01.520</a></span> | <span class="t">also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=303" target="_blank">00:05:03.440</a></span> | <span class="t">The other thing which is helping this research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=305" target="_blank">00:05:05.960</a></span> | <span class="t">is basically the compute bar is getting better and better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=309" target="_blank">00:05:09.360</a></span> | <span class="t">So all of these big companies are just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=312" target="_blank">00:05:12.640</a></span> | <span class="t">throwing massive amounts of computing resources</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=314" target="_blank">00:05:14.880</a></span> | <span class="t">at solving very, very simple and trivial tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=318" target="_blank">00:05:18.920</a></span> | <span class="t">The top of the hill being the switch transformer, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=321" target="_blank">00:05:21.560</a></span> | <span class="t">was discussed in the course also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=323" target="_blank">00:05:23.000</a></span> | <span class="t">But one other thing which I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=328" target="_blank">00:05:28.240</a></span> | <span class="t">started all of this trend was ELMo,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=330" target="_blank">00:05:30.080</a></span> | <span class="t">which was just learning these contextualized representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=333" target="_blank">00:05:33.200</a></span> | <span class="t">for natural language processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=334" target="_blank">00:05:34.920</a></span> | <span class="t">And that model right here was perhaps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=337" target="_blank">00:05:37.400</a></span> | <span class="t">one of the first kind of like model 0.0 or something,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=345" target="_blank">00:05:45.640</a></span> | <span class="t">or 0.1 in terms of bringing and ushering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=348" target="_blank">00:05:48.320</a></span> | <span class="t">in the whole revolution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=350" target="_blank">00:05:50.760</a></span> | <span class="t">You can see that how similar these kind of models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=353" target="_blank">00:05:53.520</a></span> | <span class="t">look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=354" target="_blank">00:05:54.560</a></span> | <span class="t">BERT was basically inspired heavily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=357" target="_blank">00:05:57.280</a></span> | <span class="t">from ELMo, in which they just replaced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=360" target="_blank">00:06:00.200</a></span> | <span class="t">some of the LSTM layers with transformer modules.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=363" target="_blank">00:06:03.800</a></span> | <span class="t">So a point to note also is irrespective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=368" target="_blank">00:06:08.360</a></span> | <span class="t">of natural language processing or other domain,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=370" target="_blank">00:06:10.760</a></span> | <span class="t">these can be adopted in a variety of domains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=373" target="_blank">00:06:13.000</a></span> | <span class="t">And for today's talk, I'll be just adopting them to audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=377" target="_blank">00:06:17.720</a></span> | <span class="t">So I'll basically start with introducing people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=381" target="_blank">00:06:21.240</a></span> | <span class="t">what audio representations are, and just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=383" target="_blank">00:06:23.400</a></span> | <span class="t">for the sake of completeness, talk about spectrograms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=388" target="_blank">00:06:28.240</a></span> | <span class="t">So you can take any time domain signal,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=391" target="_blank">00:06:31.400</a></span> | <span class="t">and you can decompose that signal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=394" target="_blank">00:06:34.560</a></span> | <span class="t">into a variety of basis functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=399" target="_blank">00:06:39.600</a></span> | <span class="t">And if you take up a Fourier transform,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=401" target="_blank">00:06:41.680</a></span> | <span class="t">you're kind of like decomposing the actual time domain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=406" target="_blank">00:06:46.040</a></span> | <span class="t">signal into its sinusoidal basis components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=409" target="_blank">00:06:49.400</a></span> | <span class="t">So if you have like a waveform here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=411" target="_blank">00:06:51.880</a></span> | <span class="t">like this, which is a sum of three pure sinusoids,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=415" target="_blank">00:06:55.240</a></span> | <span class="t">then their sum basically is this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=417" target="_blank">00:06:57.440</a></span> | <span class="t">And you can see that when you take a Fourier transform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=420" target="_blank">00:07:00.120</a></span> | <span class="t">and its magnitude, you kind of have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=423" target="_blank">00:07:03.280</a></span> | <span class="t">the strength of the individual components shown here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=428" target="_blank">00:07:08.800</a></span> | <span class="t">So you can take up another waveform,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=430" target="_blank">00:07:10.960</a></span> | <span class="t">let's say a square wave, and what you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=433" target="_blank">00:07:13.720</a></span> | <span class="t">is basically a much richer sinusoidal decomposition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=437" target="_blank">00:07:17.680</a></span> | <span class="t">because it is kind of a discontinuous signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=439" target="_blank">00:07:19.960</a></span> | <span class="t">So you need like many more sinusoids</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=441" target="_blank">00:07:21.640</a></span> | <span class="t">to represent that particular signal as close</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=444" target="_blank">00:07:24.400</a></span> | <span class="t">to the actual signal as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=446" target="_blank">00:07:26.960</a></span> | <span class="t">And here also you can see that, OK, if this was a square wave,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=449" target="_blank">00:07:29.880</a></span> | <span class="t">then it is actually made up of a lot of sinusoids</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=455" target="_blank">00:07:35.960</a></span> | <span class="t">where each of the bar here represents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=459" target="_blank">00:07:39.200</a></span> | <span class="t">the strength of the particular sinusoid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=462" target="_blank">00:07:42.280</a></span> | <span class="t">From an optimization perspective,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=464" target="_blank">00:07:44.120</a></span> | <span class="t">I mean, this right away is suboptimal, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=467" target="_blank">00:07:47.040</a></span> | <span class="t">Because you're kind of fixing up the number of sinusoids</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=471" target="_blank">00:07:51.080</a></span> | <span class="t">you're using for representing a square wave.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=473" target="_blank">00:07:53.240</a></span> | <span class="t">I would have rather used a basis function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=476" target="_blank">00:07:56.080</a></span> | <span class="t">which was a square wave itself than a sinusoidal signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=481" target="_blank">00:08:01.360</a></span> | <span class="t">The second thing is even if you are taking a sinusoidal signal,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=485" target="_blank">00:08:05.320</a></span> | <span class="t">we kind of are just putting them in an equidistant space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=489" target="_blank">00:08:09.440</a></span> | <span class="t">So you're kind of dividing the whole frequency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=491" target="_blank">00:08:11.760</a></span> | <span class="t">axis into equidistant bins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=494" target="_blank">00:08:14.440</a></span> | <span class="t">And each of the bins is responsible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=496" target="_blank">00:08:16.040</a></span> | <span class="t">for a particular sinusoid a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=499" target="_blank">00:08:19.400</a></span> | <span class="t">So that is like a traditional Fourier representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=502" target="_blank">00:08:22.360</a></span> | <span class="t">for representing any signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=505" target="_blank">00:08:25.840</a></span> | <span class="t">What we do for--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=508" target="_blank">00:08:28.840</a></span> | <span class="t">what are spectrograms?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=510" target="_blank">00:08:30.480</a></span> | <span class="t">But in reality, all of these signals are discontinuous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=514" target="_blank">00:08:34.880</a></span> | <span class="t">All of these signals vary quite a bit, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=517" target="_blank">00:08:37.360</a></span> | <span class="t">So you can have a signal while I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=519" target="_blank">00:08:39.960</a></span> | <span class="t">speaking which is like a square wave for a certain period</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=522" target="_blank">00:08:42.680</a></span> | <span class="t">of time, and then it gets sinusoidal,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=524" target="_blank">00:08:44.720</a></span> | <span class="t">and then it becomes something else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=526" target="_blank">00:08:46.680</a></span> | <span class="t">So what we really need is in a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=528" target="_blank">00:08:48.880</a></span> | <span class="t">to kind of take batches of input signal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=532" target="_blank">00:08:52.840</a></span> | <span class="t">and take Fourier transform of these individual batches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=536" target="_blank">00:08:56.320</a></span> | <span class="t">I'm deliberately using the word batches,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=538" target="_blank">00:08:58.000</a></span> | <span class="t">but you can-- in traditional terms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=539" target="_blank">00:08:59.520</a></span> | <span class="t">you are windowing the signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=542" target="_blank">00:09:02.240</a></span> | <span class="t">So right here, you can see that you have a continuous signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=545" target="_blank">00:09:05.120</a></span> | <span class="t">You keep on windowing it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=547" target="_blank">00:09:07.320</a></span> | <span class="t">You apply the Fourier transform, and what you get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=550" target="_blank">00:09:10.280</a></span> | <span class="t">is basically like a spectrogram representation of the signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=554" target="_blank">00:09:14.320</a></span> | <span class="t">So right here, what you're seeing basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=556" target="_blank">00:09:16.360</a></span> | <span class="t">is for each of the slices, the signal kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=559" target="_blank">00:09:19.520</a></span> | <span class="t">look like this after taking the Fourier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=561" target="_blank">00:09:21.560</a></span> | <span class="t">transform with the waveform which is there below.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=564" target="_blank">00:09:24.840</a></span> | <span class="t">And what you do is for spectrogram representation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=566" target="_blank">00:09:26.920</a></span> | <span class="t">you keep on stacking these Fourier transform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=569" target="_blank">00:09:29.160</a></span> | <span class="t">slice, the magnitude of the Fourier transform slices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=571" target="_blank">00:09:31.760</a></span> | <span class="t">And in this way, you kind of get like a 2D representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=574" target="_blank">00:09:34.760</a></span> | <span class="t">of audio signals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=576" target="_blank">00:09:36.720</a></span> | <span class="t">And if you're coming from a vision background,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=578" target="_blank">00:09:38.640</a></span> | <span class="t">it is basically all of the things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=580" target="_blank">00:09:40.160</a></span> | <span class="t">which you're doing in vision would just work well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=582" target="_blank">00:09:42.240</a></span> | <span class="t">if you just apply them to these 2D spectra representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=587" target="_blank">00:09:47.240</a></span> | <span class="t">I'll quickly play how these spectrograms look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=589" target="_blank">00:09:49.960</a></span> | <span class="t">like for a wide area of common sounds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=593" target="_blank">00:09:53.400</a></span> | <span class="t">[VIDEO PLAYBACK]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=596" target="_blank">00:09:56.400</a></span> | <span class="t">[MUSIC PLAYING]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=600" target="_blank">00:10:00.400</a></span> | <span class="t">[BIRDS CHIRPING]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=603" target="_blank">00:10:03.400</a></span> | <span class="t">[MUSIC PLAYING]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=607" target="_blank">00:10:07.400</a></span> | <span class="t">[WHISTLING]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=625" target="_blank">00:10:25.400</a></span> | <span class="t">[MUSIC PLAYING]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=628" target="_blank">00:10:28.400</a></span> | <span class="t">[WHISTLING]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=631" target="_blank">00:10:31.400</a></span> | <span class="t">[MUSIC PLAYING]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=634" target="_blank">00:10:34.400</a></span> | <span class="t">[WHISTLING]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=635" target="_blank">00:10:35.400</a></span> | <span class="t">So you can see like for spectrograms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=638" target="_blank">00:10:38.120</a></span> | <span class="t">you have kind of like a time axis on your x-axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=640" target="_blank">00:10:40.880</a></span> | <span class="t">And then you have a frequency axis on y-axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=643" target="_blank">00:10:43.680</a></span> | <span class="t">And then for whatever is your signal of interest,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=646" target="_blank">00:10:46.120</a></span> | <span class="t">you're basically like putting these slices together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=648" target="_blank">00:10:48.880</a></span> | <span class="t">And different sound gives you like different spectra</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=650" target="_blank">00:10:50.960</a></span> | <span class="t">representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=651" target="_blank">00:10:51.760</a></span> | <span class="t">So it's kind of a vision problem just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=654" target="_blank">00:10:54.080</a></span> | <span class="t">in this sort of like Fourier space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=658" target="_blank">00:10:58.040</a></span> | <span class="t">So there can be like different kinds of representations also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=661" target="_blank">00:11:01.200</a></span> | <span class="t">So one, you could just take these slices of Fourier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=666" target="_blank">00:11:06.320</a></span> | <span class="t">transform and then do like a linear mapping to them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=669" target="_blank">00:11:09.320</a></span> | <span class="t">so that you're kind of in a way making these as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=673" target="_blank">00:11:13.400</a></span> | <span class="t">close to how humans hear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=674" target="_blank">00:11:14.680</a></span> | <span class="t">So you can have like log of the frequency on the y-axis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=677" target="_blank">00:11:17.960</a></span> | <span class="t">instead of common frequency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=679" target="_blank">00:11:19.240</a></span> | <span class="t">And then you get like a constant Q-like representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=682" target="_blank">00:11:22.040</a></span> | <span class="t">The advantage of this being like you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=683" target="_blank">00:11:23.640</a></span> | <span class="t">can see that for different frequencies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=685" target="_blank">00:11:25.880</a></span> | <span class="t">the spacing between the harmonics kind of remains same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=688" target="_blank">00:11:28.960</a></span> | <span class="t">So if you're like training convolutional filters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=691" target="_blank">00:11:31.040</a></span> | <span class="t">then that's of a huge advantage because the signal,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=693" target="_blank">00:11:33.680</a></span> | <span class="t">like one component of the invariance is gone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=695" target="_blank">00:11:35.800</a></span> | <span class="t">And you can just learn these filters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=697" target="_blank">00:11:37.280</a></span> | <span class="t">which are catching onto these constant templates of Fourier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=701" target="_blank">00:11:41.880</a></span> | <span class="t">slices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=703" target="_blank">00:11:43.200</a></span> | <span class="t">You can have melt filter bank coefficients,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=705" target="_blank">00:11:45.080</a></span> | <span class="t">or you can have like the raw waveform also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=708" target="_blank">00:11:48.960</a></span> | <span class="t">For raw waveforms, basically there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=710" target="_blank">00:11:50.520</a></span> | <span class="t">are two things which we have to keep in mind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=713" target="_blank">00:11:53.200</a></span> | <span class="t">One is the sampling rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=714" target="_blank">00:11:54.320</a></span> | <span class="t">So we kind of like take the continuous signal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=717" target="_blank">00:11:57.200</a></span> | <span class="t">and then we discretize the continuous signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=719" target="_blank">00:11:59.400</a></span> | <span class="t">So one parameter is like how fast we are sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=723" target="_blank">00:12:03.360</a></span> | <span class="t">the continuous signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=724" target="_blank">00:12:04.280</a></span> | <span class="t">So that's typically on the order of like 16,000 or 8,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=727" target="_blank">00:12:07.720</a></span> | <span class="t">times a second if you're on telephonic speech.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=730" target="_blank">00:12:10.440</a></span> | <span class="t">The other thing which we also is like how many levels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=733" target="_blank">00:12:13.720</a></span> | <span class="t">we are dividing your vertical axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=735" target="_blank">00:12:15.480</a></span> | <span class="t">So in this case, you can see that each of the dots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=737" target="_blank">00:12:17.760</a></span> | <span class="t">is basically one level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=739" target="_blank">00:12:19.400</a></span> | <span class="t">And typically, people use 8-bit quantizers or 16-bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=742" target="_blank">00:12:22.280</a></span> | <span class="t">quantizers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=743" target="_blank">00:12:23.280</a></span> | <span class="t">So in a way, you can think about that for every one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=745" target="_blank">00:12:25.400</a></span> | <span class="t">second of audio which we would hear,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=747" target="_blank">00:12:27.480</a></span> | <span class="t">you would have like 16,000 samples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=749" target="_blank">00:12:29.720</a></span> | <span class="t">And then in each of the 16,000 samples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=752" target="_blank">00:12:32.720</a></span> | <span class="t">are allowed to take one of the levels between 0 to 55.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=756" target="_blank">00:12:36.800</a></span> | <span class="t">And that's like if I can take the problem of continuous audio</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=761" target="_blank">00:12:41.560</a></span> | <span class="t">and just have it in terms of this sort of discrete space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=765" target="_blank">00:12:45.320</a></span> | <span class="t">then basically I'm just going to the territory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=767" target="_blank">00:12:47.520</a></span> | <span class="t">of doing language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=770" target="_blank">00:12:50.600</a></span> | <span class="t">So the first papers I discuss is how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=773" target="_blank">00:12:53.480</a></span> | <span class="t">can we do generative modeling for raw audio, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=777" target="_blank">00:12:57.880</a></span> | <span class="t">is similar to WaveNets using transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=781" target="_blank">00:13:01.480</a></span> | <span class="t">I'll be putting QR codes if you like the stuff what I'm doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=785" target="_blank">00:13:05.560</a></span> | <span class="t">And if you think that this is relevant to you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=787" target="_blank">00:13:07.800</a></span> | <span class="t">please cite or please have a look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=790" target="_blank">00:13:10.080</a></span> | <span class="t">in terms of the QR codes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=793" target="_blank">00:13:13.000</a></span> | <span class="t">So yeah, so I'll start with the first subtopic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=796" target="_blank">00:13:16.400</a></span> | <span class="t">of today's talk, which is like what are WaveNets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=802" target="_blank">00:13:22.640</a></span> | <span class="t">and how do we do this generative modeling over raw audio?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=806" target="_blank">00:13:26.960</a></span> | <span class="t">So in a single word, you can think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=808" target="_blank">00:13:28.360</a></span> | <span class="t">about this as doing language modeling over these 255</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=811" target="_blank">00:13:31.360</a></span> | <span class="t">states of audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=813" target="_blank">00:13:33.120</a></span> | <span class="t">So you can throw in your favorite transformer model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=815" target="_blank">00:13:35.760</a></span> | <span class="t">like transformer XL or GPT or whatever you want to call it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=821" target="_blank">00:13:41.520</a></span> | <span class="t">And just treat the problem as if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=823" target="_blank">00:13:43.160</a></span> | <span class="t">are trying to predict one of the levels out of 255.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=825" target="_blank">00:13:45.800</a></span> | <span class="t">And you have to predict the next level given a certain context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=829" target="_blank">00:13:49.480</a></span> | <span class="t">That's what WaveNet was doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=830" target="_blank">00:13:50.760</a></span> | <span class="t">So the way you are modeling the probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=835" target="_blank">00:13:55.320</a></span> | <span class="t">of a continuous space is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=837" target="_blank">00:13:57.320</a></span> | <span class="t">you're trying to predict what's the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=839" target="_blank">00:13:59.360</a></span> | <span class="t">of the next sample given some parsed context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=842" target="_blank">00:14:02.560</a></span> | <span class="t">And WaveNet has been hugely popular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=844" target="_blank">00:14:04.720</a></span> | <span class="t">because it has over 3,000 citations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=847" target="_blank">00:14:07.080</a></span> | <span class="t">and it has been a core building block for almost all speech</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=851" target="_blank">00:14:11.040</a></span> | <span class="t">and audio related problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=853" target="_blank">00:14:13.000</a></span> | <span class="t">You can think about speech to text, text to speech synthesis,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=856" target="_blank">00:14:16.600</a></span> | <span class="t">instrument conversion, packet loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=859" target="_blank">00:14:19.040</a></span> | <span class="t">concealment over the internet, speech denoising.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=861" target="_blank">00:14:21.680</a></span> | <span class="t">So wherever there's some sort of element of modifying audio,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=865" target="_blank">00:14:25.560</a></span> | <span class="t">people have been using WaveNet as a core building block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=870" target="_blank">00:14:30.440</a></span> | <span class="t">And raw waveform synthesis has been difficult</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=872" target="_blank">00:14:32.880</a></span> | <span class="t">because just the magnitude of the problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=875" target="_blank">00:14:35.920</a></span> | <span class="t">if I'm just trying to synthesize 10 seconds of audio,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=878" target="_blank">00:14:38.880</a></span> | <span class="t">it would just amount to me having a probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=881" target="_blank">00:14:41.680</a></span> | <span class="t">distribution over 160,000 samples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=885" target="_blank">00:14:45.760</a></span> | <span class="t">And that itself is tough because our ears are very, very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=889" target="_blank">00:14:49.120</a></span> | <span class="t">sensitive to subtle changes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=891" target="_blank">00:14:51.000</a></span> | <span class="t">If I'm off by one pixel in an image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=895" target="_blank">00:14:55.600</a></span> | <span class="t">my eyes would not be as susceptible to noticing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=899" target="_blank">00:14:59.200</a></span> | <span class="t">that effect versus if I'm off by, say, a few samples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=904" target="_blank">00:15:04.240</a></span> | <span class="t">in an audio, it would just catch our ears pretty quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=908" target="_blank">00:15:08.600</a></span> | <span class="t">People have been trying raw audio synthesis a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=910" target="_blank">00:15:10.960</a></span> | <span class="t">in the past.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=912" target="_blank">00:15:12.440</a></span> | <span class="t">And before all of the WaveNet and transformer-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=915" target="_blank">00:15:15.720</a></span> | <span class="t">approaches, WaveRNNs and SampleRNNs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=920" target="_blank">00:15:20.280</a></span> | <span class="t">were kind of like state-of-the-art models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=925" target="_blank">00:15:25.600</a></span> | <span class="t">On the right, I've shown a SampleRNN model, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=928" target="_blank">00:15:28.600</a></span> | <span class="t">kind of models the probability distribution of what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=933" target="_blank">00:15:33.280</a></span> | <span class="t">going to come next given the past at multiple levels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=936" target="_blank">00:15:36.400</a></span> | <span class="t">And this was work done by Yoshua Bengio at Mila.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=940" target="_blank">00:15:40.200</a></span> | <span class="t">But you can closely see, if you just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=942" target="_blank">00:15:42.560</a></span> | <span class="t">see this architecture versus a transformer architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=945" target="_blank">00:15:45.520</a></span> | <span class="t">in a way, these are starting to get very, very similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=948" target="_blank">00:15:48.680</a></span> | <span class="t">Because what you're trying to do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=950" target="_blank">00:15:50.200</a></span> | <span class="t">that for the probability distribution here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=952" target="_blank">00:15:52.320</a></span> | <span class="t">you're trying to see a lot of local substructures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=956" target="_blank">00:15:56.040</a></span> | <span class="t">And then you keep on doing it over and over again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=958" target="_blank">00:15:58.120</a></span> | <span class="t">And you can draw parallels, like attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=960" target="_blank">00:16:00.840</a></span> | <span class="t">should also kind of be doing the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=964" target="_blank">00:16:04.000</a></span> | <span class="t">So this was kind of like the literature in the past.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=969" target="_blank">00:16:09.280</a></span> | <span class="t">What we tried to do was we just had the WaveNet model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=973" target="_blank">00:16:13.120</a></span> | <span class="t">And we tried to see whether transformers can beat them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=975" target="_blank">00:16:15.800</a></span> | <span class="t">And our intuition was it should be able to beat them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=978" target="_blank">00:16:18.160</a></span> | <span class="t">because they are successful all over the other domains,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=983" target="_blank">00:16:23.000</a></span> | <span class="t">like in language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=984" target="_blank">00:16:24.120</a></span> | <span class="t">So it should do that for raw waveforms also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=988" target="_blank">00:16:28.720</a></span> | <span class="t">We also tried to see whether we can circumvent the order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=991" target="_blank">00:16:31.400</a></span> | <span class="t">n squared constraint by conditioning of the context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=995" target="_blank">00:16:35.560</a></span> | <span class="t">itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=997" target="_blank">00:16:37.160</a></span> | <span class="t">And we did not go for specific applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1000" target="_blank">00:16:40.560</a></span> | <span class="t">And we just said, OK, just in terms like modeling behavior,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1003" target="_blank">00:16:43.000</a></span> | <span class="t">how will they do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1005" target="_blank">00:16:45.440</a></span> | <span class="t">So the data set for this was just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1007" target="_blank">00:16:47.200</a></span> | <span class="t">like real-world kind of recording.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1008" target="_blank">00:16:48.760</a></span> | <span class="t">So actual sound should not matter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1013" target="_blank">00:16:53.440</a></span> | <span class="t">because the model is agnostic to what it is being thrown in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1017" target="_blank">00:16:57.880</a></span> | <span class="t">And the setup was exactly the same,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1019" target="_blank">00:16:59.520</a></span> | <span class="t">like you are giving a certain context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1021" target="_blank">00:17:01.360</a></span> | <span class="t">And I have to predict the next sample.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1023" target="_blank">00:17:03.600</a></span> | <span class="t">You do the same thing with WaveNets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1025" target="_blank">00:17:05.520</a></span> | <span class="t">You do the exact same thing with transform-based,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1029" target="_blank">00:17:09.240</a></span> | <span class="t">like GPT kind of model and see how well they do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1034" target="_blank">00:17:14.000</a></span> | <span class="t">I'll briefly chat about what WaveNet models are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1038" target="_blank">00:17:18.040</a></span> | <span class="t">So WaveNet was kind of like a convolution-based model, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1041" target="_blank">00:17:21.360</a></span> | <span class="t">was getting rid of all of the vanishing gradient problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1045" target="_blank">00:17:25.360</a></span> | <span class="t">by just treating a sequential problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1048" target="_blank">00:17:28.680</a></span> | <span class="t">as being learned by a convolutional model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1051" target="_blank">00:17:31.120</a></span> | <span class="t">So what they did was basically have this sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1053" target="_blank">00:17:33.880</a></span> | <span class="t">of dilation layers, or convolution with dilations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1058" target="_blank">00:17:38.320</a></span> | <span class="t">which is basically I kind of skip in every subsequent layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1062" target="_blank">00:17:42.000</a></span> | <span class="t">by one sample.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1063" target="_blank">00:17:43.240</a></span> | <span class="t">So you can see if I have a dilation factor of 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1067" target="_blank">00:17:47.080</a></span> | <span class="t">with a kernel size of 2, I would get this kind of a topology</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1070" target="_blank">00:17:50.400</a></span> | <span class="t">where my convolution filters in the very first layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1072" target="_blank">00:17:52.720</a></span> | <span class="t">are just combining the first two samples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1075" target="_blank">00:17:55.120</a></span> | <span class="t">Then I skip by one in the next layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1077" target="_blank">00:17:57.520</a></span> | <span class="t">And then I skip by three, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1079" target="_blank">00:17:59.360</a></span> | <span class="t">like I look at the fourth one in the next layer and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1083" target="_blank">00:18:03.400</a></span> | <span class="t">The loss is still the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1084" target="_blank">00:18:04.600</a></span> | <span class="t">So I have this network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1086" target="_blank">00:18:06.640</a></span> | <span class="t">I learn a latent space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1088" target="_blank">00:18:08.000</a></span> | <span class="t">And then I have a categorical cross-entropy loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1091" target="_blank">00:18:11.200</a></span> | <span class="t">which is basically I have to predict the next sample given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1093" target="_blank">00:18:13.960</a></span> | <span class="t">the previous one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1096" target="_blank">00:18:16.640</a></span> | <span class="t">And I just do the exact same thing with transformers also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1100" target="_blank">00:18:20.800</a></span> | <span class="t">But then I have to make sure that I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1102" target="_blank">00:18:22.760</a></span> | <span class="t">do it in a causal manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1104" target="_blank">00:18:24.120</a></span> | <span class="t">So I have something which is very similar to GPT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1106" target="_blank">00:18:26.840</a></span> | <span class="t">in which I have causal masks in my attention mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1110" target="_blank">00:18:30.320</a></span> | <span class="t">And I keep doing it over and over again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1112" target="_blank">00:18:32.360</a></span> | <span class="t">So you have self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1115" target="_blank">00:18:35.520</a></span> | <span class="t">After that, you have feedforward layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1117" target="_blank">00:18:37.400</a></span> | <span class="t">You just have a stack of these transformer blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1120" target="_blank">00:18:40.400</a></span> | <span class="t">and see how they do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1123" target="_blank">00:18:43.320</a></span> | <span class="t">So I said intuitively it should work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1125" target="_blank">00:18:45.320</a></span> | <span class="t">So it should be doing better than our base wave net models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1132" target="_blank">00:18:52.360</a></span> | <span class="t">Because if you look at the topology,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1135" target="_blank">00:18:55.120</a></span> | <span class="t">we are kind of defining a topology on our own, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1137" target="_blank">00:18:57.640</a></span> | <span class="t">So what if the current prediction at, say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1141" target="_blank">00:19:01.520</a></span> | <span class="t">layer one were to depend on very way back sample, say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1147" target="_blank">00:19:07.760</a></span> | <span class="t">instead of the second sample, the 10th sample?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1149" target="_blank">00:19:09.640</a></span> | <span class="t">So we are kind of ignoring all of that topology, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1152" target="_blank">00:19:12.800</a></span> | <span class="t">would have been important for prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1154" target="_blank">00:19:14.480</a></span> | <span class="t">of this particular task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1156" target="_blank">00:19:16.040</a></span> | <span class="t">Whereas transformers with the self-attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1159" target="_blank">00:19:19.640</a></span> | <span class="t">can just learn, like, OK, which part of the samples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1162" target="_blank">00:19:22.400</a></span> | <span class="t">are important and which are not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1163" target="_blank">00:19:23.960</a></span> | <span class="t">And you can keep on doing it iteratively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1166" target="_blank">00:19:26.760</a></span> | <span class="t">So it made sense to us that, OK, transformer layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1170" target="_blank">00:19:30.880</a></span> | <span class="t">should be doing way better than wave net models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1174" target="_blank">00:19:34.880</a></span> | <span class="t">The second thing which we came across was, OK,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1177" target="_blank">00:19:37.680</a></span> | <span class="t">we cannot have a lot of context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1180" target="_blank">00:19:40.640</a></span> | <span class="t">For example, the attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1182" target="_blank">00:19:42.480</a></span> | <span class="t">needs to store all of those of order n squared.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1186" target="_blank">00:19:46.120</a></span> | <span class="t">So in this case, if I'm storing data at 100 milliseconds,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1190" target="_blank">00:19:50.240</a></span> | <span class="t">then I have about 1,600 samples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1192" target="_blank">00:19:52.600</a></span> | <span class="t">And I need to store 1,600 by 1,600 at multiple layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1196" target="_blank">00:19:56.840</a></span> | <span class="t">And it just becomes like a huge problem with the data--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1201" target="_blank">00:20:01.800</a></span> | <span class="t">problem with the memory constraint.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1203" target="_blank">00:20:03.360</a></span> | <span class="t">So what we said was, OK, what if we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1206" target="_blank">00:20:06.600</a></span> | <span class="t">use the context itself as a latent code?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1210" target="_blank">00:20:10.840</a></span> | <span class="t">So in order to have much better representation at every layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1216" target="_blank">00:20:16.440</a></span> | <span class="t">we cannot have huge, big attention matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1220" target="_blank">00:20:20.040</a></span> | <span class="t">So what we said was, we would just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1221" target="_blank">00:20:21.960</a></span> | <span class="t">do a sample-wise conditioning and throw a CNN layers just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1226" target="_blank">00:20:26.240</a></span> | <span class="t">to understand what the latent code would be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1228" target="_blank">00:20:28.560</a></span> | <span class="t">So you still have, like, an attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1231" target="_blank">00:20:31.040</a></span> | <span class="t">or just a past context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1232" target="_blank">00:20:32.960</a></span> | <span class="t">But then I'm also conditioning at every sample, OK,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1236" target="_blank">00:20:36.560</a></span> | <span class="t">what the next sample should be given on this context embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1241" target="_blank">00:20:41.240</a></span> | <span class="t">And if you think about it, in a way, it is like, OK,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1243" target="_blank">00:20:43.360</a></span> | <span class="t">if there are, like, five or six notes being played in a piano,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1246" target="_blank">00:20:46.680</a></span> | <span class="t">then I'm kind of certain which notes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1248" target="_blank">00:20:48.280</a></span> | <span class="t">will be played to a certain extent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1250" target="_blank">00:20:50.040</a></span> | <span class="t">if I just throw in a CNN layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1252" target="_blank">00:20:52.600</a></span> | <span class="t">So I'll use that information along with what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1255" target="_blank">00:20:55.360</a></span> | <span class="t">my transporters are learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1257" target="_blank">00:20:57.480</a></span> | <span class="t">And then I would condition it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1259" target="_blank">00:20:59.040</a></span> | <span class="t">And I would just use that to predict the next sample.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1263" target="_blank">00:21:03.240</a></span> | <span class="t">So for the evaluation criteria, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1264" target="_blank">00:21:04.840</a></span> | <span class="t">did not look for negative log-likelihood scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1268" target="_blank">00:21:08.720</a></span> | <span class="t">We just looked at how well our prediction task was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1272" target="_blank">00:21:12.760</a></span> | <span class="t">So we took a, like, stacked WaveNet,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1275" target="_blank">00:21:15.640</a></span> | <span class="t">which was implemented by DeepMind,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1277" target="_blank">00:21:17.520</a></span> | <span class="t">and saw that, OK, what was the performance using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1281" target="_blank">00:21:21.320</a></span> | <span class="t">their benchmarks and even, like, bigger stacked WaveNets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1285" target="_blank">00:21:25.840</a></span> | <span class="t">We then started to increase the complexity of transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1289" target="_blank">00:21:29.080</a></span> | <span class="t">and started to see whatever we had proposed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1292" target="_blank">00:21:32.520</a></span> | <span class="t">in terms of, like, conditioning on the vanilla transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1296" target="_blank">00:21:36.600</a></span> | <span class="t">architectures to see how well they do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1299" target="_blank">00:21:39.520</a></span> | <span class="t">We did not look for, like, an application-specific problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1303" target="_blank">00:21:43.640</a></span> | <span class="t">which is basically, like, we don't look at, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1306" target="_blank">00:21:46.520</a></span> | <span class="t">how well perception tasks are for, like, say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1309" target="_blank">00:21:49.200</a></span> | <span class="t">text-to-speech synthesis or speech denoising.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1311" target="_blank">00:21:51.600</a></span> | <span class="t">We just look at, OK, if we are trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1313" target="_blank">00:21:53.280</a></span> | <span class="t">to model this using a cross-entropy loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1316" target="_blank">00:21:56.000</a></span> | <span class="t">then with the same model, with the same loss function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1319" target="_blank">00:21:59.560</a></span> | <span class="t">how well they do on, like, similar kind of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1323" target="_blank">00:22:03.720</a></span> | <span class="t">So this was the first kind of, like, sub-block of, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1326" target="_blank">00:22:06.640</a></span> | <span class="t">how can we use our transformers for generative modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1332" target="_blank">00:22:12.160</a></span> | <span class="t">For the second problem, I'll do a quick headway</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1335" target="_blank">00:22:15.360</a></span> | <span class="t">on how can we use, like, transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1339" target="_blank">00:22:19.160</a></span> | <span class="t">for doing language modeling, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1341" target="_blank">00:22:21.200</a></span> | <span class="t">is kind of becoming a really fancy term right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1344" target="_blank">00:22:24.880</a></span> | <span class="t">And this work was done by Julia Smith way back in 2020.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1348" target="_blank">00:22:28.880</a></span> | <span class="t">And the goal of this was, can we kind of, in a way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1352" target="_blank">00:22:32.520</a></span> | <span class="t">do language modeling with continuous audio sequences?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1357" target="_blank">00:22:37.440</a></span> | <span class="t">And I'll briefly mention about that in this sub-block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1362" target="_blank">00:22:42.240</a></span> | <span class="t">of the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1362" target="_blank">00:22:42.960</a></span> | <span class="t">And this is in regard for, like, solving acoustic scene</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1368" target="_blank">00:22:48.440</a></span> | <span class="t">understanding, which is basically, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1371" target="_blank">00:22:51.560</a></span> | <span class="t">if I'm given a chunk of audio, then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1374" target="_blank">00:22:54.280</a></span> | <span class="t">I want to understand what's in there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1376" target="_blank">00:22:56.960</a></span> | <span class="t">And if we could do that well, then in a way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1381" target="_blank">00:23:01.800</a></span> | <span class="t">we can do a lot of fancy, nice applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1386" target="_blank">00:23:06.120</a></span> | <span class="t">So for example, like, if you think about, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1388" target="_blank">00:23:08.160</a></span> | <span class="t">self-driving cars.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1389" target="_blank">00:23:09.040</a></span> | <span class="t">So Waymo has started to incorporate microphones</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1392" target="_blank">00:23:12.200</a></span> | <span class="t">into their self-driving cars.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1393" target="_blank">00:23:13.720</a></span> | <span class="t">Why?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1394" target="_blank">00:23:14.220</a></span> | <span class="t">Because, say, if there is an ambulance coming,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1396" target="_blank">00:23:16.080</a></span> | <span class="t">or if there is a fire truck coming,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1398" target="_blank">00:23:18.960</a></span> | <span class="t">then that sound would be picked up way, way before even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1403" target="_blank">00:23:23.040</a></span> | <span class="t">the LIDARs or even their sensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1405" target="_blank">00:23:25.840</a></span> | <span class="t">So they want to understand that and take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1408" target="_blank">00:23:28.320</a></span> | <span class="t">actions based upon that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1410" target="_blank">00:23:30.880</a></span> | <span class="t">Apple, during COVID, did a hand-washing detection</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1413" target="_blank">00:23:33.360</a></span> | <span class="t">on their Apple Watch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1414" target="_blank">00:23:34.600</a></span> | <span class="t">Because if you could detect when someone is washing their hands,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1417" target="_blank">00:23:37.800</a></span> | <span class="t">then you can, in a way, like, tell people that, oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1420" target="_blank">00:23:40.760</a></span> | <span class="t">you need to wash hands for 20 seconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1422" target="_blank">00:23:42.720</a></span> | <span class="t">And then that can be built upon as a cool application.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1426" target="_blank">00:23:46.880</a></span> | <span class="t">It can be used for music recommendations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1429" target="_blank">00:23:49.520</a></span> | <span class="t">So Spotify, YouTube Music kind of gives, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1431" target="_blank">00:23:51.760</a></span> | <span class="t">very, very good songs, which you are listening to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1434" target="_blank">00:23:54.640</a></span> | <span class="t">which are similar in content that you would perhaps like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1439" target="_blank">00:23:59.320</a></span> | <span class="t">It can also give, like, really cool applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1441" target="_blank">00:24:01.680</a></span> | <span class="t">Like, say, people have tried, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1443" target="_blank">00:24:03.760</a></span> | <span class="t">detecting depression from audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1445" target="_blank">00:24:05.800</a></span> | <span class="t">Or I could detect whether I'm coughing or not,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1448" target="_blank">00:24:08.760</a></span> | <span class="t">or I'm sneezing or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1449" target="_blank">00:24:09.960</a></span> | <span class="t">And these can be, like, good medical device--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1453" target="_blank">00:24:13.080</a></span> | <span class="t">medical applications, which can be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1455" target="_blank">00:24:15.360</a></span> | <span class="t">used along with the current diagnosis what doctor provides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1460" target="_blank">00:24:20.520</a></span> | <span class="t">So the question was basically, for us,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1463" target="_blank">00:24:23.320</a></span> | <span class="t">was, like, how can we do, like, language modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1466" target="_blank">00:24:26.600</a></span> | <span class="t">in a continuous audio domain?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1469" target="_blank">00:24:29.040</a></span> | <span class="t">And secondly, like, how can we train models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1471" target="_blank">00:24:31.240</a></span> | <span class="t">or how should we approach doing this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1475" target="_blank">00:24:35.320</a></span> | <span class="t">So this kind of, like, recipe has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1477" target="_blank">00:24:37.360</a></span> | <span class="t">become, like, very, very popular these days in terms of, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1480" target="_blank">00:24:40.680</a></span> | <span class="t">how would you approach this problem?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1482" target="_blank">00:24:42.240</a></span> | <span class="t">It started with, like, open AI, and to a certain extent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1486" target="_blank">00:24:46.280</a></span> | <span class="t">DeepMind proposing that in terms of, like, VQVAE models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1490" target="_blank">00:24:50.960</a></span> | <span class="t">But it turns out, like, transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1492" target="_blank">00:24:52.640</a></span> | <span class="t">love operating in discrete spaces, as of now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1496" target="_blank">00:24:56.440</a></span> | <span class="t">And what they kind of do is, as long as your representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1501" target="_blank">00:25:01.200</a></span> | <span class="t">are discrete, they are very, very good at modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1503" target="_blank">00:25:03.640</a></span> | <span class="t">what's going to come next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1506" target="_blank">00:25:06.760</a></span> | <span class="t">So what people have been proposing as a workaround</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1509" target="_blank">00:25:09.200</a></span> | <span class="t">is you could take up, like, your favorite embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1515" target="_blank">00:25:15.640</a></span> | <span class="t">in some manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1516" target="_blank">00:25:16.400</a></span> | <span class="t">You could take a VQVAE embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1518" target="_blank">00:25:18.200</a></span> | <span class="t">or you could take a Wave2Vec, or in terms of video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1521" target="_blank">00:25:21.840</a></span> | <span class="t">you can just do classic VGG or ResNet embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1527" target="_blank">00:25:27.880</a></span> | <span class="t">You can apply k-means clustering to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1531" target="_blank">00:25:31.040</a></span> | <span class="t">And k-means clustering would give you, like, discrete codes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1534" target="_blank">00:25:34.320</a></span> | <span class="t">You do language modeling with those discrete codes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1537" target="_blank">00:25:37.240</a></span> | <span class="t">and you predict the next code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1539" target="_blank">00:25:39.360</a></span> | <span class="t">And in a way, if you're doing this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1541" target="_blank">00:25:41.440</a></span> | <span class="t">then you're kind of doing language modeling over audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1545" target="_blank">00:25:45.200</a></span> | <span class="t">And if you need to get back to the audio,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1546" target="_blank">00:25:46.840</a></span> | <span class="t">then you already saw with WaveNet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1549" target="_blank">00:25:49.000</a></span> | <span class="t">that you can condition the WaveNet model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1551" target="_blank">00:25:51.120</a></span> | <span class="t">to give continuous output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1553" target="_blank">00:25:53.480</a></span> | <span class="t">So you can use those codes to get back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1555" target="_blank">00:25:55.320</a></span> | <span class="t">to the audio, similar to what jukebox and OpenAI did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1560" target="_blank">00:26:00.240</a></span> | <span class="t">So I'll quickly mention about what vector quantization is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1565" target="_blank">00:26:05.920</a></span> | <span class="t">It's one of the most underutilized algorithms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1568" target="_blank">00:26:08.680</a></span> | <span class="t">to be honest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1569" target="_blank">00:26:09.880</a></span> | <span class="t">And what it does is basically gives, in a way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1572" target="_blank">00:26:12.960</a></span> | <span class="t">discrete codes to continuous embedding spaces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1576" target="_blank">00:26:16.400</a></span> | <span class="t">So how does it do it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1578" target="_blank">00:26:18.080</a></span> | <span class="t">So you basically have an embedding space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1583" target="_blank">00:26:23.600</a></span> | <span class="t">let's say, in 2D right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1585" target="_blank">00:26:25.240</a></span> | <span class="t">You define what are the number of clusters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1587" target="_blank">00:26:27.120</a></span> | <span class="t">you want to put each of them in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1589" target="_blank">00:26:29.320</a></span> | <span class="t">You run k-means, and you would certainly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1591" target="_blank">00:26:31.520</a></span> | <span class="t">get these patches of where all of these embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1596" target="_blank">00:26:36.200</a></span> | <span class="t">are, what would be the representative embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1598" target="_blank">00:26:38.280</a></span> | <span class="t">of a continuous embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1600" target="_blank">00:26:40.720</a></span> | <span class="t">You can take all of those patches,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1602" target="_blank">00:26:42.120</a></span> | <span class="t">and you can just number them, or you can just list them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1605" target="_blank">00:26:45.960</a></span> | <span class="t">So in this case, you can perhaps have 25 numbers, or 20 numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1609" target="_blank">00:26:49.840</a></span> | <span class="t">which are, in a way, mapping from a continuous embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1613" target="_blank">00:26:53.480</a></span> | <span class="t">to a discrete token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1617" target="_blank">00:26:57.040</a></span> | <span class="t">This is another example right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1618" target="_blank">00:26:58.520</a></span> | <span class="t">So in our case, what we did was we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1622" target="_blank">00:27:02.320</a></span> | <span class="t">took a batch of spectrogram, which are basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1625" target="_blank">00:27:05.160</a></span> | <span class="t">very small patches across time, and then shared all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1630" target="_blank">00:27:10.760</a></span> | <span class="t">across the frequency axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1633" target="_blank">00:27:13.320</a></span> | <span class="t">You take those patches, you learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1635" target="_blank">00:27:15.200</a></span> | <span class="t">the embedding representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1636" target="_blank">00:27:16.600</a></span> | <span class="t">In our case, it was just like three-layer autoencoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1639" target="_blank">00:27:19.720</a></span> | <span class="t">fully-connected encoders with three layers of decoders,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1642" target="_blank">00:27:22.800</a></span> | <span class="t">and have a bottleneck layer in between.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1645" target="_blank">00:27:25.360</a></span> | <span class="t">So that bottleneck layer basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1647" target="_blank">00:27:27.000</a></span> | <span class="t">is kind of similar to this kind of diagram</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1649" target="_blank">00:27:29.280</a></span> | <span class="t">in, say, 64-dimensional space or 120-dimensional space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1653" target="_blank">00:27:33.520</a></span> | <span class="t">You take up those bottleneck codes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1655" target="_blank">00:27:35.400</a></span> | <span class="t">and then you run k-means clustering on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1657" target="_blank">00:27:37.840</a></span> | <span class="t">Suddenly, in a way, you can find discrete codes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1663" target="_blank">00:27:43.520</a></span> | <span class="t">for continuous embedding spaces or even continuous segments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1668" target="_blank">00:27:48.040</a></span> | <span class="t">And since we know that transformers kind of love</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1670" target="_blank">00:27:50.480</a></span> | <span class="t">operating in discrete spaces, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1672" target="_blank">00:27:52.600</a></span> | <span class="t">can just apply language modeling now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1675" target="_blank">00:27:55.160</a></span> | <span class="t">and then you can see what you can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1678" target="_blank">00:27:58.480</a></span> | <span class="t">So in our case, we just had very simple three-layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1682" target="_blank">00:28:02.200</a></span> | <span class="t">fully-connected autoencoder, small patches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1686" target="_blank">00:28:06.000</a></span> | <span class="t">The number of codes is important,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1687" target="_blank">00:28:07.520</a></span> | <span class="t">because if you have too many codes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1690" target="_blank">00:28:10.000</a></span> | <span class="t">then you're kind of just throwing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1691" target="_blank">00:28:11.800</a></span> | <span class="t">in all kinds of noisy things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1693" target="_blank">00:28:13.600</a></span> | <span class="t">Now, I'll give an example of why the number of codes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1697" target="_blank">00:28:17.880</a></span> | <span class="t">are important through some example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1699" target="_blank">00:28:19.680</a></span> | <span class="t">And you have two little codes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1702" target="_blank">00:28:22.480</a></span> | <span class="t">What you're, in a way, doing is you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1705" target="_blank">00:28:25.000</a></span> | <span class="t">removing all of the information which was relevant,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1707" target="_blank">00:28:27.280</a></span> | <span class="t">and you're just kind of averaging them all out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1709" target="_blank">00:28:29.480</a></span> | <span class="t">So this idea first was proposed by Jukebox,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1718" target="_blank">00:28:38.200</a></span> | <span class="t">which did it for music.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1720" target="_blank">00:28:40.240</a></span> | <span class="t">So you do the exact same thing, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1722" target="_blank">00:28:42.160</a></span> | <span class="t">I talked about, in a slightly different manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1725" target="_blank">00:28:45.240</a></span> | <span class="t">In a way that, OK, you cannot learn codes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1728" target="_blank">00:28:48.640</a></span> | <span class="t">for longer sequences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1730" target="_blank">00:28:50.520</a></span> | <span class="t">So in a way, learn sequences which are just moving slowly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1734" target="_blank">00:28:54.080</a></span> | <span class="t">and which are looking at only a certain amount of audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1737" target="_blank">00:28:57.680</a></span> | <span class="t">So you kind of encode this in these discrete levels, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1741" target="_blank">00:29:01.320</a></span> | <span class="t">are basically like--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1743" target="_blank">00:29:03.360</a></span> | <span class="t">all of these basically are codes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1744" target="_blank">00:29:04.760</a></span> | <span class="t">So at every point, I define, OK, this audio</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1748" target="_blank">00:29:08.040</a></span> | <span class="t">had, perhaps, code number 55.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1750" target="_blank">00:29:10.520</a></span> | <span class="t">And in the next level, perhaps, it had code number 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1752" target="_blank">00:29:12.840</a></span> | <span class="t">And in the very top, perhaps, it had code number 2,000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1756" target="_blank">00:29:16.440</a></span> | <span class="t">So in a way, I'm discretizing the whole codes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1759" target="_blank">00:29:19.680</a></span> | <span class="t">Now what I do is I take up my favorite transform model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1763" target="_blank">00:29:23.440</a></span> | <span class="t">perhaps like a causal autoregressive one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1766" target="_blank">00:29:26.280</a></span> | <span class="t">And I say that, OK, given these codes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1769" target="_blank">00:29:29.240</a></span> | <span class="t">try to predict what codes would come next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1771" target="_blank">00:29:31.480</a></span> | <span class="t">And for sure, transformers can do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1774" target="_blank">00:29:34.240</a></span> | <span class="t">So I would generate the codes in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1776" target="_blank">00:29:36.840</a></span> | <span class="t">Once I've generated the codes in the future,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1779" target="_blank">00:29:39.560</a></span> | <span class="t">I can say that, OK, this problem now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1781" target="_blank">00:29:41.360</a></span> | <span class="t">is kind of like a text-to-speech problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1783" target="_blank">00:29:43.680</a></span> | <span class="t">because I have these discrete codes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1785" target="_blank">00:29:45.560</a></span> | <span class="t">Text-to-speech, in a way, is going from discrete letters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1788" target="_blank">00:29:48.680</a></span> | <span class="t">to continuous audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1790" target="_blank">00:29:50.560</a></span> | <span class="t">So I would throw in the fanciest, which was WaveNet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1793" target="_blank">00:29:53.720</a></span> | <span class="t">And I would just get back the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1795" target="_blank">00:29:55.240</a></span> | <span class="t">And I would get the generated audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1798" target="_blank">00:29:58.080</a></span> | <span class="t">So this was, in a way, what I described,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1802" target="_blank">00:30:02.360</a></span> | <span class="t">that they take up a continuous audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1804" target="_blank">00:30:04.760</a></span> | <span class="t">They have these compressed codes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1806" target="_blank">00:30:06.640</a></span> | <span class="t">which they encode using a CNN in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1810" target="_blank">00:30:10.720</a></span> | <span class="t">The method doesn't matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1811" target="_blank">00:30:11.960</a></span> | <span class="t">You can throw in the fanciest of embedding or latent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1815" target="_blank">00:30:15.040</a></span> | <span class="t">representation on those continuous code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1818" target="_blank">00:30:18.400</a></span> | <span class="t">You generate the patterns, which are like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1820" target="_blank">00:30:20.120</a></span> | <span class="t">what's going to happen next in the future?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1822" target="_blank">00:30:22.000</a></span> | <span class="t">And then you decode back using a fancy WaveNet or state-of-the-art</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1826" target="_blank">00:30:26.000</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1828" target="_blank">00:30:28.000</a></span> | <span class="t">So this was what they were doing for music synthesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1832" target="_blank">00:30:32.160</a></span> | <span class="t">What we said was, yeah, this is good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1835" target="_blank">00:30:35.200</a></span> | <span class="t">This can generate a good amount of music.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1837" target="_blank">00:30:37.200</a></span> | <span class="t">But can these models be used for generating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1844" target="_blank">00:30:44.480</a></span> | <span class="t">good representation of the current audio?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1847" target="_blank">00:30:47.840</a></span> | <span class="t">And the goal there was, can language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1851" target="_blank">00:30:51.160</a></span> | <span class="t">learn representation, which can just encapsulate whatever we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1855" target="_blank">00:30:55.600</a></span> | <span class="t">are giving as an input signal?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1859" target="_blank">00:30:59.160</a></span> | <span class="t">So in this case, what we tried after that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1861" target="_blank">00:31:01.160</a></span> | <span class="t">was you do exactly similar ideas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1866" target="_blank">00:31:06.840</a></span> | <span class="t">But instead of doing on VQ-VAE end-to-end learned encodings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1871" target="_blank">00:31:11.960</a></span> | <span class="t">we just apply vanilla k-means clustering,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1874" target="_blank">00:31:14.320</a></span> | <span class="t">similar to what I described earlier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1876" target="_blank">00:31:16.880</a></span> | <span class="t">We do on spectrogram patches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1878" target="_blank">00:31:18.200</a></span> | <span class="t">So you take up these spectrograms of audio,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1880" target="_blank">00:31:20.960</a></span> | <span class="t">and you just divide them into very small chunks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1883" target="_blank">00:31:23.960</a></span> | <span class="t">learn autoencoder encodings for each of those chunks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1888" target="_blank">00:31:28.440</a></span> | <span class="t">run k-means clustering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1890" target="_blank">00:31:30.320</a></span> | <span class="t">In this case, let's say I am learning 16 codes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1894" target="_blank">00:31:34.120</a></span> | <span class="t">Represent the continuous audio in terms of the 16 codes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1898" target="_blank">00:31:38.360</a></span> | <span class="t">Have a transformer which can perhaps predict the next code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1901" target="_blank">00:31:41.880</a></span> | <span class="t">And if I keep on getting better and better at predicting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1905" target="_blank">00:31:45.120</a></span> | <span class="t">what's going to happen next, then in this linear layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1908" target="_blank">00:31:48.000</a></span> | <span class="t">I should be encapsulating what's important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1911" target="_blank">00:31:51.320</a></span> | <span class="t">or what's a good summary of what has happened in the past.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1916" target="_blank">00:31:56.640</a></span> | <span class="t">So that was our intuition behind trying this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1921" target="_blank">00:32:01.600</a></span> | <span class="t">And as I explained, the number of codes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1923" target="_blank">00:32:03.600</a></span> | <span class="t">play a very important role.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1925" target="_blank">00:32:05.560</a></span> | <span class="t">You can see here, these are just two piano notes switching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1928" target="_blank">00:32:08.920</a></span> | <span class="t">one after the other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1930" target="_blank">00:32:10.080</a></span> | <span class="t">If I just have 16 number of codes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1932" target="_blank">00:32:12.080</a></span> | <span class="t">it just happens to have just a single line of encoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1936" target="_blank">00:32:16.560</a></span> | <span class="t">a single code assigned to all of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1938" target="_blank">00:32:18.640</a></span> | <span class="t">Whereas if I'm assigning more codes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1941" target="_blank">00:32:21.040</a></span> | <span class="t">then it becomes a fine-grained prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1943" target="_blank">00:32:23.600</a></span> | <span class="t">where I'm actually able to get what the individual notes are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1949" target="_blank">00:32:29.320</a></span> | <span class="t">Recently, Facebook also said, OK, they just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1952" target="_blank">00:32:32.880</a></span> | <span class="t">had a different name to the whole thing, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1955" target="_blank">00:32:35.240</a></span> | <span class="t">is we can just call this as textless NLP also in the sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1960" target="_blank">00:32:40.760</a></span> | <span class="t">that, OK, you can do NLP without having access to text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1964" target="_blank">00:32:44.760</a></span> | <span class="t">But the idea is very, very similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1966" target="_blank">00:32:46.360</a></span> | <span class="t">You have an encoder, which is exactly similar to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1968" target="_blank">00:32:48.600</a></span> | <span class="t">what OpenAI was using.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1969" target="_blank">00:32:49.680</a></span> | <span class="t">You have a VQ-VAE, Wave2Vec, or whatever you want to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1973" target="_blank">00:32:53.320</a></span> | <span class="t">You can apply k-means clustering to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1975" target="_blank">00:32:55.240</a></span> | <span class="t">You apply language models to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1977" target="_blank">00:32:57.160</a></span> | <span class="t">And instead of a decoder being WaveNet,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1978" target="_blank">00:32:58.880</a></span> | <span class="t">they just have a decoder, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1980" target="_blank">00:33:00.280</a></span> | <span class="t">like a different version of text-to-speech, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1982" target="_blank">00:33:02.840</a></span> | <span class="t">is like Takotron in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1985" target="_blank">00:33:05.000</a></span> | <span class="t">So as you can see, these are all the same wine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1987" target="_blank">00:33:07.160</a></span> | <span class="t">and very different bottles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1988" target="_blank">00:33:08.280</a></span> | <span class="t">But the core idea is almost exactly the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1992" target="_blank">00:33:12.560</a></span> | <span class="t">So this created a huge uproar of this going to change NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=1998" target="_blank">00:33:18.120</a></span> | <span class="t">But this is very, very similar to what people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2001" target="_blank">00:33:21.560</a></span> | <span class="t">have been doing in the past.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2004" target="_blank">00:33:24.560</a></span> | <span class="t">So I've already explained what this was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2009" target="_blank">00:33:29.200</a></span> | <span class="t">So in our case, we just try to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2012" target="_blank">00:33:32.400</a></span> | <span class="t">what's going to happen next given the previous context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2015" target="_blank">00:33:35.360</a></span> | <span class="t">and use that representation similar to every single one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2020" target="_blank">00:33:40.320</a></span> | <span class="t">short learning or zero short learning-based method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2024" target="_blank">00:33:44.600</a></span> | <span class="t">I also explain why the number of codes are important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2027" target="_blank">00:33:47.720</a></span> | <span class="t">If you have too small, then you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2029" target="_blank">00:33:49.240</a></span> | <span class="t">just throwing away a lot of information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2030" target="_blank">00:33:50.920</a></span> | <span class="t">If you have too large, then you don't put in--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2035" target="_blank">00:33:55.360</a></span> | <span class="t">it is no longer robust to noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2039" target="_blank">00:33:59.480</a></span> | <span class="t">So this was our setup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2040" target="_blank">00:34:00.720</a></span> | <span class="t">And before I jump in, I should add one of the tweets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2044" target="_blank">00:34:04.080</a></span> | <span class="t">which I saw from one of the most prominent researchers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2048" target="_blank">00:34:08.800</a></span> | <span class="t">at DeepMind, which is basically like a lot of times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2051" target="_blank">00:34:11.240</a></span> | <span class="t">it is very, very easy to bump up numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2053" target="_blank">00:34:13.680</a></span> | <span class="t">I can have these details just not present</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2057" target="_blank">00:34:17.200</a></span> | <span class="t">in my paper, which actually help a lot in terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2060" target="_blank">00:34:20.280</a></span> | <span class="t">of improving the performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2062" target="_blank">00:34:22.200</a></span> | <span class="t">And sometimes don't take into account</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2065" target="_blank">00:34:25.760</a></span> | <span class="t">what the actual model is incorporating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2069" target="_blank">00:34:29.160</a></span> | <span class="t">or what model is contributing versus what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2072" target="_blank">00:34:32.000</a></span> | <span class="t">the actual these tricks for training are incorporating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2075" target="_blank">00:34:35.160</a></span> | <span class="t">So for most of these methods, what we are trying to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2078" target="_blank">00:34:38.240</a></span> | <span class="t">is we try to keep almost exactly the same approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2082" target="_blank">00:34:42.000</a></span> | <span class="t">No rate augmentation, no fancy label smoothing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2084" target="_blank">00:34:44.560</a></span> | <span class="t">or moving average of weights, or decay, or whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2088" target="_blank">00:34:48.400</a></span> | <span class="t">You just have similar-based recipes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2091" target="_blank">00:34:51.440</a></span> | <span class="t">to see how well we are doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2095" target="_blank">00:34:55.520</a></span> | <span class="t">For this case, the goal was to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2097" target="_blank">00:34:57.920</a></span> | <span class="t">that how well our models do with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2100" target="_blank">00:35:00.400</a></span> | <span class="t">to this purely supervised approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2102" target="_blank">00:35:02.120</a></span> | <span class="t">and how well it does with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2103" target="_blank">00:35:03.720</a></span> | <span class="t">to a similar unsupervised approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2106" target="_blank">00:35:06.840</a></span> | <span class="t">So in the first case, the model and all of the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2109" target="_blank">00:35:09.360</a></span> | <span class="t">have access to all of the labels, which is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2112" target="_blank">00:35:12.320</a></span> | <span class="t">shown as VGG supervised, which is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2114" target="_blank">00:35:14.840</a></span> | <span class="t">you take up an audio understanding data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2116" target="_blank">00:35:16.840</a></span> | <span class="t">and you see how well you're doing on accuracy metrics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2121" target="_blank">00:35:21.440</a></span> | <span class="t">So that was the first one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2122" target="_blank">00:35:22.560</a></span> | <span class="t">In the second one, we applied SimClear,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2124" target="_blank">00:35:24.760</a></span> | <span class="t">which was proposed by Geoff Hinton,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2126" target="_blank">00:35:26.160</a></span> | <span class="t">in which you can take up these multiple augmentations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2128" target="_blank">00:35:28.440</a></span> | <span class="t">of the same input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2130" target="_blank">00:35:30.440</a></span> | <span class="t">You can have patches removed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2132" target="_blank">00:35:32.000</a></span> | <span class="t">You can blur the signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2133" target="_blank">00:35:33.000</a></span> | <span class="t">You can flip the signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2134" target="_blank">00:35:34.800</a></span> | <span class="t">You learn an embedding out of the last layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2136" target="_blank">00:35:36.880</a></span> | <span class="t">without access to the labels, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2138" target="_blank">00:35:38.720</a></span> | <span class="t">just have a linear head to predict what's happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2142" target="_blank">00:35:42.000</a></span> | <span class="t">By using that, we got a 55% accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2145" target="_blank">00:35:45.040</a></span> | <span class="t">You do the exact same thing with transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2146" target="_blank">00:35:46.960</a></span> | <span class="t">You don't have access to labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2148" target="_blank">00:35:48.440</a></span> | <span class="t">You just run them while just to predict the next code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2151" target="_blank">00:35:51.640</a></span> | <span class="t">You take the linear layer, apply the same linear head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2154" target="_blank">00:35:54.400</a></span> | <span class="t">and try to predict what's happening inside.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2157" target="_blank">00:35:57.120</a></span> | <span class="t">And with that, we got 60% accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2159" target="_blank">00:35:59.440</a></span> | <span class="t">So even though the results are not good,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2161" target="_blank">00:36:01.160</a></span> | <span class="t">but the fact is the neural networks actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2164" target="_blank">00:36:04.920</a></span> | <span class="t">are very, very good at getting better and better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2169" target="_blank">00:36:09.360</a></span> | <span class="t">with throwing off huge amounts of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2171" target="_blank">00:36:11.520</a></span> | <span class="t">So there's still a 10% gap between purely supervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2174" target="_blank">00:36:14.720</a></span> | <span class="t">and purely unsupervised.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2177" target="_blank">00:36:17.200</a></span> | <span class="t">But that's going to improve with throwing a lot of data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2181" target="_blank">00:36:21.160</a></span> | <span class="t">to these models, because it doesn't have access</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2183" target="_blank">00:36:23.480</a></span> | <span class="t">to any label as per se.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2185" target="_blank">00:36:25.240</a></span> | <span class="t">So this is a famous paper by Dan Ellis and Nelson Morgan</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2188" target="_blank">00:36:28.560</a></span> | <span class="t">at Berkeley, in which they actually showed way back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2190" target="_blank">00:36:30.920</a></span> | <span class="t">in 1999 as to why size matters for deep neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2197" target="_blank">00:36:37.200</a></span> | <span class="t">and also the number of data points which is present.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2201" target="_blank">00:36:41.000</a></span> | <span class="t">So as they kept on increasing the size of the data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2204" target="_blank">00:36:44.400</a></span> | <span class="t">and the parameters, they kept on getting lower and lower word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2207" target="_blank">00:36:47.120</a></span> | <span class="t">error rates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2208" target="_blank">00:36:48.120</a></span> | <span class="t">And this has been true across any of the data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2211" target="_blank">00:36:51.480</a></span> | <span class="t">And that's why the whole excitement is about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2213" target="_blank">00:36:53.720</a></span> | <span class="t">unsupervised learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2216" target="_blank">00:36:56.640</a></span> | <span class="t">So this was, in a way, a flavor of how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2218" target="_blank">00:36:58.760</a></span> | <span class="t">can we do language modeling and unsupervised learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2221" target="_blank">00:37:01.320</a></span> | <span class="t">on audio for continuous signals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2225" target="_blank">00:37:05.200</a></span> | <span class="t">For the third subplot, I'll just quickly mention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2229" target="_blank">00:37:09.400</a></span> | <span class="t">ideas which are very similar to what you would have seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2231" target="_blank">00:37:11.800</a></span> | <span class="t">in vision transformers, but with the caveat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2235" target="_blank">00:37:15.680</a></span> | <span class="t">that how can we use some sort of signal processing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2238" target="_blank">00:37:18.880</a></span> | <span class="t">to improve these performance even further.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2242" target="_blank">00:37:22.040</a></span> | <span class="t">So the basic approach still remains the same exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2244" target="_blank">00:37:24.520</a></span> | <span class="t">as what you would have seen in vision transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2248" target="_blank">00:37:28.200</a></span> | <span class="t">You have a signal of interest which you want to classify.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2253" target="_blank">00:37:33.080</a></span> | <span class="t">Here, they are raw waveform instead of images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2256" target="_blank">00:37:36.400</a></span> | <span class="t">The goal is to predict what's there inside of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2261" target="_blank">00:37:41.080</a></span> | <span class="t">And also, we don't have any convolutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2263" target="_blank">00:37:43.480</a></span> | <span class="t">We don't have any other tricks which we were using before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2266" target="_blank">00:37:46.760</a></span> | <span class="t">All we have to do is they can transform as themselves,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2269" target="_blank">00:37:49.760</a></span> | <span class="t">solve this particular problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2272" target="_blank">00:37:52.880</a></span> | <span class="t">So for the data set--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2274" target="_blank">00:37:54.800</a></span> | <span class="t">and the whole setup was still the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2277" target="_blank">00:37:57.280</a></span> | <span class="t">No data augmentation and no other forms of these tricks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2283" target="_blank">00:38:03.000</a></span> | <span class="t">You are given like 40,000 snippets for training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2285" target="_blank">00:38:05.440</a></span> | <span class="t">and 10,000 for validation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2288" target="_blank">00:38:08.040</a></span> | <span class="t">Our job is to predict as good as possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2290" target="_blank">00:38:10.880</a></span> | <span class="t">as to what's there in the audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2293" target="_blank">00:38:13.480</a></span> | <span class="t">This problem is very similar to the sound which you heard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2297" target="_blank">00:38:17.120</a></span> | <span class="t">and the video which you saw, that given a spectrogram patch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2302" target="_blank">00:38:22.080</a></span> | <span class="t">you have to predict what's there inside of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2304" target="_blank">00:38:24.080</a></span> | <span class="t">We kind of do one step further than what's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2313" target="_blank">00:38:33.000</a></span> | <span class="t">like a simple transformer model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2314" target="_blank">00:38:34.840</a></span> | <span class="t">In a sense that we try to see whether some sort of hierarchy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2318" target="_blank">00:38:38.400</a></span> | <span class="t">over transformer embeddings would help us in any manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2323" target="_blank">00:38:43.960</a></span> | <span class="t">So for that, we use wavelet decomposition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2327" target="_blank">00:38:47.200</a></span> | <span class="t">on the intermediate transformer embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2330" target="_blank">00:38:50.720</a></span> | <span class="t">So what is a wavelet decomposition?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2335" target="_blank">00:38:55.040</a></span> | <span class="t">In very naive terms, it can be like a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2338" target="_blank">00:38:58.200</a></span> | <span class="t">of decomposing the intermediate embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2342" target="_blank">00:39:02.720</a></span> | <span class="t">into another intermediate embedding, in a sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2346" target="_blank">00:39:06.800</a></span> | <span class="t">that we are kind of putting these highways of like some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2349" target="_blank">00:39:09.920</a></span> | <span class="t">embeddings are moving very slowly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2351" target="_blank">00:39:11.440</a></span> | <span class="t">and some embeddings are moving very fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2353" target="_blank">00:39:13.640</a></span> | <span class="t">And some embeddings are retained exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2355" target="_blank">00:39:15.600</a></span> | <span class="t">at the rate of what the original signal was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2359" target="_blank">00:39:19.480</a></span> | <span class="t">And why this is important?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2360" target="_blank">00:39:20.640</a></span> | <span class="t">Because you can think about that at every intermediate state,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2364" target="_blank">00:39:24.000</a></span> | <span class="t">you are in a way learning some sort of hierarchy in the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2367" target="_blank">00:39:27.800</a></span> | <span class="t">So if I look at what we do with the wavelet decomposition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2375" target="_blank">00:39:35.120</a></span> | <span class="t">before and after, let's say you had time across this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2379" target="_blank">00:39:39.200</a></span> | <span class="t">and you had the embedding size across this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2381" target="_blank">00:39:41.640</a></span> | <span class="t">and this whole patch was your output of, say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2385" target="_blank">00:39:45.880</a></span> | <span class="t">the nth layer of the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2388" target="_blank">00:39:48.640</a></span> | <span class="t">What I say now is, OK, I would just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2392" target="_blank">00:39:52.320</a></span> | <span class="t">have a mapping from this to the mapping of my interest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2396" target="_blank">00:39:56.040</a></span> | <span class="t">using wavelet decomposition, in which for half of the samples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2400" target="_blank">00:40:00.280</a></span> | <span class="t">I just retain the exact same embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2402" target="_blank">00:40:02.160</a></span> | <span class="t">as what was learned by the transformer model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2405" target="_blank">00:40:05.960</a></span> | <span class="t">In the next half, I would start combining two at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2409" target="_blank">00:40:09.160</a></span> | <span class="t">So in a way, I'm learning this sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2410" target="_blank">00:40:10.600</a></span> | <span class="t">of like a tree structure within a single layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2413" target="_blank">00:40:13.800</a></span> | <span class="t">of the transformer embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2416" target="_blank">00:40:16.360</a></span> | <span class="t">And for now, the wavelet or the BCS function which I use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2421" target="_blank">00:40:21.000</a></span> | <span class="t">is simple averaging.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2422" target="_blank">00:40:22.360</a></span> | <span class="t">So let's say from all of the embedding layers in between,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2426" target="_blank">00:40:26.040</a></span> | <span class="t">I just need to have one embedding which is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2431" target="_blank">00:40:31.400</a></span> | <span class="t">moving at all, which is just representative of whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2433" target="_blank">00:40:33.600</a></span> | <span class="t">is there of the whole latent space in that nth layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2440" target="_blank">00:40:40.240</a></span> | <span class="t">Then in the next layer, I would just use two at a time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2444" target="_blank">00:40:44.160</a></span> | <span class="t">and then I would use four at a time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2446" target="_blank">00:40:46.680</a></span> | <span class="t">until I reach the exact resolution as what I had.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2450" target="_blank">00:40:50.360</a></span> | <span class="t">Doing this operation doesn't add any parameters whatsoever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2453" target="_blank">00:40:53.560</a></span> | <span class="t">You're just defining what your BCS function would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2456" target="_blank">00:40:56.320</a></span> | <span class="t">or what your wavelet function would be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2458" target="_blank">00:40:58.080</a></span> | <span class="t">In this case, it is a hard wavelet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2460" target="_blank">00:41:00.200</a></span> | <span class="t">And I start combining them and I learned a hierarchy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2464" target="_blank">00:41:04.120</a></span> | <span class="t">at every single layer of the transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2468" target="_blank">00:41:08.280</a></span> | <span class="t">And this improved our performance significantly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2472" target="_blank">00:41:12.560</a></span> | <span class="t">as compared to not using them with addition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2475" target="_blank">00:41:15.080</a></span> | <span class="t">of no extra parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2477" target="_blank">00:41:17.800</a></span> | <span class="t">And I'll come to the results later also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2480" target="_blank">00:41:20.840</a></span> | <span class="t">So this is how the whole approach looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2483" target="_blank">00:41:23.840</a></span> | <span class="t">You have a front end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2485" target="_blank">00:41:25.680</a></span> | <span class="t">The front end is basically a single layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2488" target="_blank">00:41:28.480</a></span> | <span class="t">of 2,000 neurons followed by a dense layer of 64 neurons,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2493" target="_blank">00:41:33.240</a></span> | <span class="t">which is just to make sure to conform it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2497" target="_blank">00:41:37.360</a></span> | <span class="t">to the intermediate transform embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2499" target="_blank">00:41:39.080</a></span> | <span class="t">Let's say if for the transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2500" target="_blank">00:41:40.920</a></span> | <span class="t">I define the embedding size to be 64,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2503" target="_blank">00:41:43.200</a></span> | <span class="t">then that's the dimension which I'm mapping them to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2507" target="_blank">00:41:47.200</a></span> | <span class="t">So I take a broad waveform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2508" target="_blank">00:41:48.920</a></span> | <span class="t">I patch it in very small patches similar to how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2511" target="_blank">00:41:51.960</a></span> | <span class="t">you do in vision transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2514" target="_blank">00:41:54.520</a></span> | <span class="t">I would just have a single layer of 2,000 neurons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2516" target="_blank">00:41:56.880</a></span> | <span class="t">followed by a dense layer of 64 neurons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2520" target="_blank">00:42:00.040</a></span> | <span class="t">with the hope that the first layer is learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2522" target="_blank">00:42:02.280</a></span> | <span class="t">like a Fourier BCS function, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2524" target="_blank">00:42:04.760</a></span> | <span class="t">should be adaptable according to what I'm learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2528" target="_blank">00:42:08.080</a></span> | <span class="t">After that, I keep on doing this over and over again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2531" target="_blank">00:42:11.640</a></span> | <span class="t">I don't have a classification head or anything like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2535" target="_blank">00:42:15.520</a></span> | <span class="t">I keep on adding multiple stacks of transformers after that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2540" target="_blank">00:42:20.120</a></span> | <span class="t">And then I have two approaches of what I can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2546" target="_blank">00:42:26.120</a></span> | <span class="t">do in terms of adaptation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2548" target="_blank">00:42:28.920</a></span> | <span class="t">I can do average pooling across time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2551" target="_blank">00:42:31.040</a></span> | <span class="t">of these intermediate embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2552" target="_blank">00:42:32.800</a></span> | <span class="t">because the idea is very similar to what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2555" target="_blank">00:42:35.160</a></span> | <span class="t">we do in classical vision, that each of the embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2558" target="_blank">00:42:38.080</a></span> | <span class="t">are looking at much, much broader output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2562" target="_blank">00:42:42.440</a></span> | <span class="t">in the subsequent layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2564" target="_blank">00:42:44.160</a></span> | <span class="t">Or I could do a wavelet decomposition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2567" target="_blank">00:42:47.160</a></span> | <span class="t">So what I do is that I take all of these embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2570" target="_blank">00:42:50.160</a></span> | <span class="t">and I define these highways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2571" target="_blank">00:42:51.560</a></span> | <span class="t">So some of the embeddings move fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2573" target="_blank">00:42:53.200</a></span> | <span class="t">Some of them are moving very slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2574" target="_blank">00:42:54.800</a></span> | <span class="t">And some are retained at the exact same resolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2577" target="_blank">00:42:57.280</a></span> | <span class="t">as what the transformer is learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2579" target="_blank">00:42:59.800</a></span> | <span class="t">And then I keep doing this over and over again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2582" target="_blank">00:43:02.120</a></span> | <span class="t">I have a dense layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2583" target="_blank">00:43:03.680</a></span> | <span class="t">I have my softmax or sigmoid, whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2586" target="_blank">00:43:06.560</a></span> | <span class="t">is my classification head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2588" target="_blank">00:43:08.680</a></span> | <span class="t">So this is kind of what the approach looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2592" target="_blank">00:43:12.400</a></span> | <span class="t">We compare it with all of the traditional vision-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2597" target="_blank">00:43:17.720</a></span> | <span class="t">architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2598" target="_blank">00:43:18.520</a></span> | <span class="t">So the vision-based models have been very good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2601" target="_blank">00:43:21.040</a></span> | <span class="t">And the performance have been similar in understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2604" target="_blank">00:43:24.360</a></span> | <span class="t">audio also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2605" target="_blank">00:43:25.560</a></span> | <span class="t">So we compare all of those models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2607" target="_blank">00:43:27.600</a></span> | <span class="t">in terms of mean average precision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2609" target="_blank">00:43:29.680</a></span> | <span class="t">And we see that even the tiniest models of transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2612" target="_blank">00:43:32.320</a></span> | <span class="t">were just surpassing all of the state-of-the-art CNN</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2614" target="_blank">00:43:34.680</a></span> | <span class="t">models, which was a very good sign.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2617" target="_blank">00:43:37.880</a></span> | <span class="t">Then we started to bump up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2619" target="_blank">00:43:39.800</a></span> | <span class="t">The larger model should keep on improving the performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2622" target="_blank">00:43:42.560</a></span> | <span class="t">And with the multi-scale models, as well as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2626" target="_blank">00:43:46.000</a></span> | <span class="t">with the pooling layers, they improve the performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2628" target="_blank">00:43:48.760</a></span> | <span class="t">even further, which was kind of very surprising to us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2632" target="_blank">00:43:52.520</a></span> | <span class="t">because the number of parameters are very small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2634" target="_blank">00:43:54.920</a></span> | <span class="t">These are very tiny architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2636" target="_blank">00:43:56.480</a></span> | <span class="t">Yet they are surpassing things like even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2638" target="_blank">00:43:58.360</a></span> | <span class="t">DenseNet, which are huge models with a lot of millions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2641" target="_blank">00:44:01.200</a></span> | <span class="t">of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2643" target="_blank">00:44:03.920</a></span> | <span class="t">So after that, we said--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2645" target="_blank">00:44:05.480</a></span> | <span class="t">and I'm going to conclude quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2648" target="_blank">00:44:08.160</a></span> | <span class="t">After that, we said that, OK, this is looking pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2652" target="_blank">00:44:12.200</a></span> | <span class="t">What actually is the transformer or the first-layer learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2657" target="_blank">00:44:17.480</a></span> | <span class="t">So in order to make this plot, what we said was, OK,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2664" target="_blank">00:44:24.240</a></span> | <span class="t">if you were to take a classic Fourier transform,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2667" target="_blank">00:44:27.600</a></span> | <span class="t">then this axis is kind of like frequency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2672" target="_blank">00:44:32.960</a></span> | <span class="t">This axis is the number of filters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2674" target="_blank">00:44:34.520</a></span> | <span class="t">And this axis is the frequency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2676" target="_blank">00:44:36.720</a></span> | <span class="t">Then in a way, it should be connecting all of the points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2681" target="_blank">00:44:41.640</a></span> | <span class="t">in a linear line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2683" target="_blank">00:44:43.400</a></span> | <span class="t">And this is akin to the number of points in the FFT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2685" target="_blank">00:44:45.920</a></span> | <span class="t">So how many points I'm defining here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2688" target="_blank">00:44:48.240</a></span> | <span class="t">If I'm defining 2,000 points here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2690" target="_blank">00:44:50.680</a></span> | <span class="t">then I would have 2,048 sinusoidal basis functions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2696" target="_blank">00:44:56.040</a></span> | <span class="t">which are going from lower frequency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2697" target="_blank">00:44:57.720</a></span> | <span class="t">to the most highest frequency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2700" target="_blank">00:45:00.120</a></span> | <span class="t">We said, OK, we'll do the exact same thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2702" target="_blank">00:45:02.080</a></span> | <span class="t">but now with filters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2703" target="_blank">00:45:03.640</a></span> | <span class="t">So we have a frequency along y-axis and the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2707" target="_blank">00:45:07.520</a></span> | <span class="t">of points in my x-axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2709" target="_blank">00:45:09.360</a></span> | <span class="t">And if it was a classic Fourier transform,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2711" target="_blank">00:45:11.280</a></span> | <span class="t">then it would be connecting right as a linear line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2715" target="_blank">00:45:15.560</a></span> | <span class="t">But what we did was we take up the front end, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2719" target="_blank">00:45:19.600</a></span> | <span class="t">is learned by transformer, take its Fourier transform,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2722" target="_blank">00:45:22.800</a></span> | <span class="t">sort according to its center frequency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2724" target="_blank">00:45:24.960</a></span> | <span class="t">as to what frequency it is activating the most,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2727" target="_blank">00:45:27.680</a></span> | <span class="t">and then keep on stacking them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2729" target="_blank">00:45:29.960</a></span> | <span class="t">When we did this for two problems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2731" target="_blank">00:45:31.960</a></span> | <span class="t">we saw that we are learning a different time frequency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2735" target="_blank">00:45:35.720</a></span> | <span class="t">representation, which is specific to a particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2738" target="_blank">00:45:38.080</a></span> | <span class="t">problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2738" target="_blank">00:45:38.600</a></span> | <span class="t">So if I'm trying to understand what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2740" target="_blank">00:45:40.400</a></span> | <span class="t">there in the content of the audio,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2742" target="_blank">00:45:42.480</a></span> | <span class="t">I learn a representation which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2743" target="_blank">00:45:43.880</a></span> | <span class="t">very different than Fourier transform,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2745" target="_blank">00:45:45.160</a></span> | <span class="t">which would have been a straight line, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2747" target="_blank">00:45:47.040</a></span> | <span class="t">is like a curved exponential line like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2751" target="_blank">00:45:51.640</a></span> | <span class="t">And if I do a polyphonic pitch estimation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2754" target="_blank">00:45:54.440</a></span> | <span class="t">I learn a very different front end,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2757" target="_blank">00:45:57.320</a></span> | <span class="t">which is adapting to that particular problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2760" target="_blank">00:46:00.080</a></span> | <span class="t">So this was very exciting to us because making computers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2765" target="_blank">00:46:05.520</a></span> | <span class="t">hear in a way in which they are adapting their ears</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2767" target="_blank">00:46:07.880</a></span> | <span class="t">according to a particular problem is a very cool idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2772" target="_blank">00:46:12.400</a></span> | <span class="t">Second thing is we actually saw each of the filters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2775" target="_blank">00:46:15.200</a></span> | <span class="t">as to what they were doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2777" target="_blank">00:46:17.320</a></span> | <span class="t">And these are basically just single slices like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2781" target="_blank">00:46:21.240</a></span> | <span class="t">So this is what we would have learned as a front end neuron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2785" target="_blank">00:46:25.280</a></span> | <span class="t">So we take up each of the neurons and we just plot them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2787" target="_blank">00:46:27.880</a></span> | <span class="t">And for plotting this, we basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2789" target="_blank">00:46:29.800</a></span> | <span class="t">take a Fourier transform and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2792" target="_blank">00:46:32.040</a></span> | <span class="t">sort them according to where the center frequency is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2795" target="_blank">00:46:35.400</a></span> | <span class="t">When we just saw the neurons as to what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2797" target="_blank">00:46:37.080</a></span> | <span class="t">they were learning in the front end,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2799" target="_blank">00:46:39.040</a></span> | <span class="t">we saw that it is learning properties</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2801" target="_blank">00:46:41.440</a></span> | <span class="t">which are very, very closely matching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2805" target="_blank">00:46:45.200</a></span> | <span class="t">with the traditional signal processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2806" target="_blank">00:46:46.840</a></span> | <span class="t">So you would have something like an answer detector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2808" target="_blank">00:46:48.960</a></span> | <span class="t">learned right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2810" target="_blank">00:46:50.760</a></span> | <span class="t">You're learning windowing functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2812" target="_blank">00:46:52.280</a></span> | <span class="t">In a way, it is learning to have a kernel which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2815" target="_blank">00:46:55.400</a></span> | <span class="t">is best for a time frequency representation, what people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2818" target="_blank">00:46:58.320</a></span> | <span class="t">have been using in signal processing, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2820" target="_blank">00:47:00.120</a></span> | <span class="t">is like a Hamming or a Hamming window.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2823" target="_blank">00:47:03.000</a></span> | <span class="t">We are learning these pure sinusoids</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2824" target="_blank">00:47:04.720</a></span> | <span class="t">which are responsible for activating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2827" target="_blank">00:47:07.400</a></span> | <span class="t">a particular frequency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2828" target="_blank">00:47:08.920</a></span> | <span class="t">So you can see the richness as compared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2830" target="_blank">00:47:10.560</a></span> | <span class="t">to having a fixed purely sinusoidal PCS</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2834" target="_blank">00:47:14.000</a></span> | <span class="t">function right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2836" target="_blank">00:47:16.440</a></span> | <span class="t">So this was what we had done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2840" target="_blank">00:47:20.040</a></span> | <span class="t">And then to share the final thoughts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2843" target="_blank">00:47:23.120</a></span> | <span class="t">I'll conclude by saying that, OK, transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2845" target="_blank">00:47:25.480</a></span> | <span class="t">are proving to be a major advancement in AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2847" target="_blank">00:47:27.360</a></span> | <span class="t">research across the fields.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2850" target="_blank">00:47:30.000</a></span> | <span class="t">And it seems like they're solving everything for now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2854" target="_blank">00:47:34.240</a></span> | <span class="t">And hopefully, this is not the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2856" target="_blank">00:47:36.040</a></span> | <span class="t">And we should keep an eye out on something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2858" target="_blank">00:47:38.440</a></span> | <span class="t">which would change and have an impact which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2861" target="_blank">00:47:41.080</a></span> | <span class="t">is more than what transformers have put.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2864" target="_blank">00:47:44.360</a></span> | <span class="t">And who knows what's going to come next?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2867" target="_blank">00:47:47.600</a></span> | <span class="t">Yeah, so by that, I'll just conclude.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2869" target="_blank">00:47:49.680</a></span> | <span class="t">And I'll be happy to take questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2873" target="_blank">00:47:53.120</a></span> | <span class="t">Thank you, Prateek.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2874" target="_blank">00:47:54.080</a></span> | <span class="t">That was a really good talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2875" target="_blank">00:47:55.800</a></span> | <span class="t">And you provided some really good insights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2878" target="_blank">00:47:58.720</a></span> | <span class="t">about how transformers work for the audio case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2882" target="_blank">00:48:02.360</a></span> | <span class="t">And yeah, thank you for the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2884" target="_blank">00:48:04.880</a></span> | <span class="t">And now I would invite questions from the class students.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2890" target="_blank">00:48:10.200</a></span> | <span class="t">Let me just stop the recording.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wvE2n8u3drA&t=2893" target="_blank">00:48:13.280</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>