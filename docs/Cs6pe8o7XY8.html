<html><head><title>Gemini 1.5 and The Biggest Night in AI</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Gemini 1.5 and The Biggest Night in AI</h2><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8"><img src="https://i.ytimg.com/vi_webp/Cs6pe8o7XY8/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./Cs6pe8o7XY8.html">Whisper Transcript</a> | <a href="./transcript_Cs6pe8o7XY8.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">You could call tonight the triumph of the transformers or maybe the battle for attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=6" target="_blank">00:00:06.720</a></span> | <span class="t">with two thunderous developments within a handful of hours of each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=11" target="_blank">00:00:11.920</a></span> | <span class="t">One from Google DeepMind and then OpenAI. But ultimately, the bigger picture is this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=17" target="_blank">00:00:17.760</a></span> | <span class="t">The exponential advance of artificial intelligence and its applications shows no sign of slowing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=24" target="_blank">00:00:24.720</a></span> | <span class="t">If you were hoping for a quiet year, tonight just shattered that peaceful vision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=29" target="_blank">00:00:29.680</a></span> | <span class="t">Of course, I could have done a video on either Gemini 1.5, which is arguably as of tonight the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=36" target="_blank">00:00:36.160</a></span> | <span class="t">most performant language model in the world, or Sora, the text-to-video model OpenAI released</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=43" target="_blank">00:00:43.280</a></span> | <span class="t">shortly after Google. Possibly as a purposeful grab of the spotlight, possibly coincidentally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=49" target="_blank">00:00:49.440</a></span> | <span class="t">Both developments are game-changing, but with the technical paper of Sora due out later today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=56" target="_blank">00:00:56.240</a></span> | <span class="t">it allows us to give Gemini 1.5 its due attention. Yes, I've read all 58 pages of the technical paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=64" target="_blank">00:01:04.160</a></span> | <span class="t">as well as four papers linked in the appendices in my search for how it got the results it did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=70" target="_blank">00:01:10.240</a></span> | <span class="t">I've got 62 notes, so let's dive in. Here is the big development. Gemini 1.5 can recall</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=77" target="_blank">00:01:17.760</a></span> | <span class="t">and reason over information across millions of tokens of context. Or in another example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=83" target="_blank">00:01:23.200</a></span> | <span class="t">they gave 22 hours of audio or three hours of low frame rate video or six to eight minutes of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=90" target="_blank">00:01:30.560</a></span> | <span class="t">normal frame rate videos. I don't want to bury the headline. We're talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=94" target="_blank">00:01:34.720</a></span> | <span class="t">near perfect retrieval of facts and details up to at least 10 million tokens. Performance did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=100" target="_blank">00:01:40.800</a></span> | <span class="t">not dip at 10 million tokens. Indeed, the trend line got substantially better. For reference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=106" target="_blank">00:01:46.240</a></span> | <span class="t">in text, 10 million tokens would be about 7.5 million words. To wrap your head around how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=111" target="_blank">00:01:51.840</a></span> | <span class="t">words 10 million tokens is, that's around 0.2% of all of English language Wikipedia.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=118" target="_blank">00:01:58.320</a></span> | <span class="t">And again, just for emphasis, we're talking at least 10 million tokens. Now, admittedly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=123" target="_blank">00:02:03.280</a></span> | <span class="t">in the blog post and paper, they do talk about latency trade-offs with that many tokens. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=128" target="_blank">00:02:08.240</a></span> | <span class="t">no, in case you're wondering, Gemini 1.5 isn't currently widely available just to a limited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=133" target="_blank">00:02:13.440</a></span> | <span class="t">group of developers and enterprise customers. In case that puts you off though, Google promised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=137" target="_blank">00:02:17.920</a></span> | <span class="t">this. Significant improvements in speed are also on the horizon. But before we get back to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=143" target="_blank">00:02:23.360</a></span> | <span class="t">paper, how about a demo so you can see Gemini 1.5 Pro in action? This is a demo of long context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=150" target="_blank">00:02:30.640</a></span> | <span class="t">understanding, an experimental feature in our newest model, Gemini 1.5 Pro. We'll walk through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=156" target="_blank">00:02:36.560</a></span> | <span class="t">a screen recording of example prompts using a 402 page PDF of the Apollo 11 transcript,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=162" target="_blank">00:02:42.320</a></span> | <span class="t">which comes out to almost 330,000 tokens. We started by uploading the Apollo PDF into Google</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=169" target="_blank">00:02:49.760</a></span> | <span class="t">AI Studio and asked, "Find three comedic moments. List quotes from this transcript and emoji."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=176" target="_blank">00:02:56.960</a></span> | <span class="t">This screen capture is sped up. This timer shows exactly how long it took to process each prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=183" target="_blank">00:03:03.600</a></span> | <span class="t">and keep in mind that processing times will vary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=188" target="_blank">00:03:08.000</a></span> | <span class="t">The model responded with three quotes, like this one from Michael Collins,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=192" target="_blank">00:03:12.480</a></span> | <span class="t">"I'll bet you a cup of coffee on it." If we go back to the transcript,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=196" target="_blank">00:03:16.080</a></span> | <span class="t">we can see the model found this exact quote and extracted the comedic moment accurately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=200" target="_blank">00:03:20.080</a></span> | <span class="t">Then we tested a multimodal prompt. We gave it this drawing of a scene we were thinking of and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=204" target="_blank">00:03:24.960</a></span> | <span class="t">asked, "What moment is this?" The model correctly identified it as Neil's first steps on the moon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=213" target="_blank">00:03:33.680</a></span> | <span class="t">I'll feature more demos later in the video, but notice that the inference time in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=217" target="_blank">00:03:37.200</a></span> | <span class="t">sped up video wasn't that bad. And yes, for anyone paying attention, this is Gemini 1.5 Pro.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=223" target="_blank">00:03:43.600</a></span> | <span class="t">What that means is that any results you're seeing will soon be improved upon by Gemini 1.5 Ultra.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=230" target="_blank">00:03:50.240</a></span> | <span class="t">Remember that we have Gemini Nano, then Gemini Pro, the medium-sized model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=234" target="_blank">00:03:54.480</a></span> | <span class="t">and finally Gemini Ultra. Well, how did Google DeepMind achieve this feat?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=238" target="_blank">00:03:58.640</a></span> | <span class="t">They say in the introduction that the model incorporates a novel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=242" target="_blank">00:04:02.480</a></span> | <span class="t">mixture of experts architecture, as well as major advances in training and serving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=247" target="_blank">00:04:07.760</a></span> | <span class="t">infrastructure that allows it to push the boundaries of efficiency, reasoning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=251" target="_blank">00:04:11.760</a></span> | <span class="t">and long context performance. In case you're wondering, that long context again refers to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=256" target="_blank">00:04:16.480</a></span> | <span class="t">that huge volume of text, image, video, and audio data that Gemini 1.5 can ingest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=263" target="_blank">00:04:23.200</a></span> | <span class="t">But I will admit that when I first read this introduction, I thought they have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=267" target="_blank">00:04:27.200</a></span> | <span class="t">used the Mamba architecture. That was billed as the successor architecture to the Transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=273" target="_blank">00:04:33.120</a></span> | <span class="t">and I did a video on it on the 1st of January. It too achieved amazing results in long context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=278" target="_blank">00:04:38.960</a></span> | <span class="t">tasks and outperformed the Transformer. The Transformer is of course the T in GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=285" target="_blank">00:04:45.440</a></span> | <span class="t">However, by the time I finished the paper, it was pretty clear that it wasn't based on Mamba,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=291" target="_blank">00:04:51.200</a></span> | <span class="t">and it took me a little while to figure it out and reading quite a few papers cited in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=295" target="_blank">00:04:55.840</a></span> | <span class="t">appendices. But I think I've got a pretty good guess as to what the architecture is based on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=301" target="_blank">00:05:01.360</a></span> | <span class="t">Anyway, what is the next interesting point from the paper?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=304" target="_blank">00:05:04.480</a></span> | <span class="t">Well, Google does confirm that Gemini 1.5 Pro requires significantly less compute to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=311" target="_blank">00:05:11.760</a></span> | <span class="t">than 1.0 Ultra. So it's arguably better than Ultra, and we'll see the benchmarks in a moment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=317" target="_blank">00:05:17.760</a></span> | <span class="t">but requires significantly less compute to train. That is maybe why Google DeepMind were able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=323" target="_blank">00:05:23.840</a></span> | <span class="t">get out Gemini 1.5 Pro so soon after announcing Gemini 1.0. And don't forget that Gemini 1.0 Ultra</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=331" target="_blank">00:05:31.680</a></span> | <span class="t">as part of Gemini Advanced was only released to the public a week ago. That's when my review video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=337" target="_blank">00:05:37.440</a></span> | <span class="t">came out. Google, by the way, gives you a two month free trial of Gemini Advanced, and I now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=342" target="_blank">00:05:42.400</a></span> | <span class="t">think part of the reason for that is to give them time to incorporate Gemini 1.5 Pro before that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=349" target="_blank">00:05:49.200</a></span> | <span class="t">free trial ends. And here is the first bombshell graphic in the paper. This is the task of finding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=355" target="_blank">00:05:55.360</a></span> | <span class="t">a needle in a haystack across text, video, and audio modalities. A fact or passcode might be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=361" target="_blank">00:06:01.840</a></span> | <span class="t">buried at varying depths of sequences of varying lengths. As you can see for text, these lengths</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=368" target="_blank">00:06:08.880</a></span> | <span class="t">went up to 10 million tokens. For audio, it was up to 22 hours and video up to three hours. The</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=375" target="_blank">00:06:15.360</a></span> | <span class="t">models would then be tested on that fact. As you can see, the performance was incredible with just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=380" target="_blank">00:06:20.960</a></span> | <span class="t">five missed facts. For reference, I went back to the original benchmark, which I've cited in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=386" target="_blank">00:06:26.160</a></span> | <span class="t">two previous videos to compare. Here is GPC 4's performance at up to only 128,000 tokens. As you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=394" target="_blank">00:06:34.080</a></span> | <span class="t">can see, as the sequence length gets to around 80,000 words or 100,000 tokens, the performance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=400" target="_blank">00:06:40.240</a></span> | <span class="t">especially midway through the sequence, degrades. At the time, Anthropic's Claude 2.1 performed even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=407" target="_blank">00:06:47.520</a></span> | <span class="t">worse, although they did subsequently come up with a prompt engineering hack to reduce most of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=412" target="_blank">00:06:52.560</a></span> | <span class="t">these incorrect recalls. But are you ready for the next bombshell? They see Gemini 1.5 Pro outperforming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=420" target="_blank">00:07:00.000</a></span> | <span class="t">all competing models across all modalities, even when these models are augmented with external</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=426" target="_blank">00:07:06.160</a></span> | <span class="t">retrieval methods. In the lingo, that's RAG. In layman's terms, that's grabbing relevant text to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=431" target="_blank">00:07:11.280</a></span> | <span class="t">assist them in answering the questions. Of course, with a long context, Gemini 1.5 Pro is just simply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=436" target="_blank">00:07:16.320</a></span> | <span class="t">ingesting the entire document. And now for one of the key tables in the paper. You might be wondering,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=442" target="_blank">00:07:22.480</a></span> | <span class="t">and indeed Google DeepMind were wondering, if this extra performance in long context tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=448" target="_blank">00:07:28.640</a></span> | <span class="t">would mean a trade-off for other types of tasks. Text, vision, and audio tasks that didn't require</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=454" target="_blank">00:07:34.800</a></span> | <span class="t">long context. The answer was no. Gemini 1.5 Pro is better on average compared to 1.0 Pro across text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=463" target="_blank">00:07:43.280</a></span> | <span class="t">vision, and audio. In other words, it hasn't just got better at long context, it's got better at a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=468" target="_blank">00:07:48.560</a></span> | <span class="t">range of other tasks. It beats Gemini 1.0 Pro 100% of the time in text benchmarks and most of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=475" target="_blank">00:07:55.440</a></span> | <span class="t">time in vision and audio benchmarks. And wait, there's more. It also beats most of the time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=480" target="_blank">00:08:00.800</a></span> | <span class="t">Gemini 1.0 Ultra in text benchmarks. Of course, at this point, if you only care about standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=486" target="_blank">00:08:06.960</a></span> | <span class="t">benchmarks and not about long context, it's more or less a draw with Gemini 1.0 Ultra. It has a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=493" target="_blank">00:08:13.200</a></span> | <span class="t">win rate of 54.8% and it would also be pretty much a tie with GPT-4. But I think it's fair to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=501" target="_blank">00:08:21.520</a></span> | <span class="t">that once you bring in long context capabilities, it is now indisputably the best language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=508" target="_blank">00:08:28.720</a></span> | <span class="t">in the world. I should caveat that with it's the best language model that is accessible to some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=514" target="_blank">00:08:34.160</a></span> | <span class="t">people out there. Of course, behind the scenes, Google DeepMind have Gemini 1.5 Ultra, which would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=519" target="_blank">00:08:39.280</a></span> | <span class="t">surely be better than 1.5 Pro. And I know what you're thinking, this is Google, so maybe they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=524" target="_blank">00:08:44.320</a></span> | <span class="t">use some kooky prompting method to get these results. Well, I've read the entire paper and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=529" target="_blank">00:08:49.760</a></span> | <span class="t">it doesn't seem so to me. These are genuine like-for-like comparisons to 1.0 Pro and 1.0 Ultra.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=536" target="_blank">00:08:56.800</a></span> | <span class="t">Now, I have looked in the appendices and the exact wording of the prompts may have been different to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=542" target="_blank">00:09:02.560</a></span> | <span class="t">for example, GPT-4. It does seem borderline impossible to get perfectly like-for-like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=548" target="_blank">00:09:08.160</a></span> | <span class="t">comparisons. But from everything I can see, this does seem to be a genuine result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=553" target="_blank">00:09:13.680</a></span> | <span class="t">Now, before you get too excited, as you can see, the benchmark results for 1.5 Pro</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=558" target="_blank">00:09:18.720</a></span> | <span class="t">in non-long context tasks is pretty impressive, but we're not completely changing the game here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=565" target="_blank">00:09:25.520</a></span> | <span class="t">They haven't come up with some sort of architecture that just crushes in every task. We're still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=569" target="_blank">00:09:29.760</a></span> | <span class="t">dealing with a familiar language model for most tasks. But before I give you my architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=575" target="_blank">00:09:35.680</a></span> | <span class="t">speculations, time for another demo, this time analyzing a one frame per second, 44-minute movie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=582" target="_blank">00:09:42.800</a></span> | <span class="t">This is a demo of long context understanding, an experimental feature in our newest model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=588" target="_blank">00:09:48.800</a></span> | <span class="t">Gemini 1.5 Pro. We'll walk through a screen recording of example prompts using a 44-minute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=594" target="_blank">00:09:54.960</a></span> | <span class="t">Buster Keaton film, which comes out to over 600,000 tokens. In Google AI Studio, we uploaded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=601" target="_blank">00:10:01.600</a></span> | <span class="t">the video and asked, "Find the moment when a piece of paper is removed from the person's pocket</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=607" target="_blank">00:10:07.120</a></span> | <span class="t">and tell me some key information on it with the time code." This screen capture is sped up,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=613" target="_blank">00:10:13.360</a></span> | <span class="t">and this timer shows exactly how long it took to process each prompt. And keep in mind that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=618" target="_blank">00:10:18.240</a></span> | <span class="t">processing times will vary. The model gave us this response, explaining that the piece of paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=624" target="_blank">00:10:24.160</a></span> | <span class="t">is a pawn ticket from Goldman & Co. pawnbrokers with the date and cost. And it gave us this time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=630" target="_blank">00:10:30.240</a></span> | <span class="t">code, 12/01. When we pulled up that time code, we found it was correct. The model had found the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=637" target="_blank">00:10:37.040</a></span> | <span class="t">exact moment the piece of paper is removed from the person's pocket, and it extracted text accurately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=642" target="_blank">00:10:42.880</a></span> | <span class="t">Next, we gave it this drawing of a scene we were thinking of and asked, "What is the time code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=647" target="_blank">00:10:47.680</a></span> | <span class="t">when this happens?" The model returned this time code, 15/34. We pulled that up and found that it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=654" target="_blank">00:10:54.160</a></span> | <span class="t">was the correct scene. Like all generative models, responses vary and won't always be perfect. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=660" target="_blank">00:11:00.240</a></span> | <span class="t">notice how we didn't have to explain what was happening in the drawing. Gemini being multimodal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=665" target="_blank">00:11:05.200</a></span> | <span class="t">from the ground up is really shining here, and I do think we have to take a step back and say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=671" target="_blank">00:11:11.040</a></span> | <span class="t">"Wow." At the moment, this might mean 6 to 8 minutes of a 24 or 30 frames per second video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=676" target="_blank">00:11:16.640</a></span> | <span class="t">on YouTube. But still, successfully picking out that minor detail in that short of a time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=681" target="_blank">00:11:21.920</a></span> | <span class="t">is truly groundbreaking. Given that Google owns YouTube, you will soon be querying,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=686" target="_blank">00:11:26.960</a></span> | <span class="t">say, AI Explained videos. Okay, time for my guests about how they managed it in terms of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=692" target="_blank">00:11:32.160</a></span> | <span class="t">architecture. Well, first they say simply, "It's a sparse mixture of expert transformer-based model."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=698" target="_blank">00:11:38.160</a></span> | <span class="t">Those are fairly standard terms described in other videos of mine. But then here's the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=703" target="_blank">00:11:43.120</a></span> | <span class="t">sentence, "Gemini 1.5 Pro also builds on the following research and the language model research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=711" target="_blank">00:11:51.040</a></span> | <span class="t">in this broader literature." Now, I had a look at most of these papers, particularly the recent ones,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=716" target="_blank">00:11:56.640</a></span> | <span class="t">and one stood out. This paper by Zhang et al came out just over a month ago. And remember,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=722" target="_blank">00:12:02.000</a></span> | <span class="t">Google says that Gemini 1.5 Pro builds on this work. Now, building on something that came out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=728" target="_blank">00:12:08.800</a></span> | <span class="t">that recently is pretty significant. Of course, Google have their own massive body of literature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=733" target="_blank">00:12:13.840</a></span> | <span class="t">on sparse mixture of experts and indeed invented the transformer architecture. But this tweet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=738" target="_blank">00:12:18.960</a></span> | <span class="t">tonight from one of the key authors of Gemini 1.5 does point to things developing more rapidly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=745" target="_blank">00:12:25.280</a></span> | <span class="t">recently. Pranav Shyam said this just a few months ago, "Nikolai, Dennis and I were exploring ways to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=751" target="_blank">00:12:31.120</a></span> | <span class="t">dramatically increase our context lengths. Little did we know that our ideas would ship in production</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=756" target="_blank">00:12:36.480</a></span> | <span class="t">so quickly." So yes, Google has work going back years on sparse mixture of expert models. And yes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=762" target="_blank">00:12:42.880</a></span> | <span class="t">too many people underestimated the years of innovations going on quietly at Google,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=768" target="_blank">00:12:48.000</a></span> | <span class="t">in this case for inference. But for the purposes of time, this is the paper I'm going to focus on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=773" target="_blank">00:12:53.360</a></span> | <span class="t">This is the one by Zhang released around a month ago. It's, of course, mixture of experts from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=779" target="_blank">00:12:59.520</a></span> | <span class="t">Mistral AI. That's that brand new French outfit with a multi-billion euro valuation. And no,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=785" target="_blank">00:13:05.680</a></span> | <span class="t">I don't just think it's relevant because of the date and the fact it's sparse and a mixture of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=790" target="_blank">00:13:10.240</a></span> | <span class="t">experts. Mixture of experts in a nutshell being when you have a bigger model comprised of multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=796" target="_blank">00:13:16.000</a></span> | <span class="t">smaller blocks or experts. When the tokens come in, they are dynamically rooted to just two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=802" target="_blank">00:13:22.160</a></span> | <span class="t">in most cases, relevant experts or blocks. So the entire model isn't active during inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=808" target="_blank">00:13:28.160</a></span> | <span class="t">It's lightweight and effective, but no, that's not the reason why I focused on this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=812" target="_blank">00:13:32.960</a></span> | <span class="t">It's because of this section, 3.2 long range performance mixed trial managed to achieve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=818" target="_blank">00:13:38.880</a></span> | <span class="t">a hundred percent retrieval accuracy regardless of the context length and also regardless of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=824" target="_blank">00:13:44.640</a></span> | <span class="t">the position or depth of the password. Of course, Mistral only proved that up to 32,000 tokens and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=831" target="_blank">00:13:51.280</a></span> | <span class="t">Google, I believe, have taken it much further. That's my theory. Let me know in the comments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=835" target="_blank">00:13:55.840</a></span> | <span class="t">if you think I'm right. Google do say they also made improvements in terms of data optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=841" target="_blank">00:14:01.440</a></span> | <span class="t">and systems, but if you're looking for more info on compute or the training dataset, good luck.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=847" target="_blank">00:14:07.360</a></span> | <span class="t">Other than saying that the compute is significantly less than Gemini 1.0 Ultra and that Gemini 1.5 Pro</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=854" target="_blank">00:14:14.240</a></span> | <span class="t">is trained on a variety of multimodal and multilingual data, they don't really give us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=859" target="_blank">00:14:19.520</a></span> | <span class="t">anything. Well, I tell a lie. They do say that it was trained across multiple data centers. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=864" target="_blank">00:14:24.800</a></span> | <span class="t">given that a data center maxes out around 32,000 TPUs and I know Google uses TPUs, but that still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=871" target="_blank">00:14:31.920</a></span> | <span class="t">gives us a sense about the sheer scale of Google Gemini's compute. And there is one more task that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=877" target="_blank">00:14:37.920</a></span> | <span class="t">Google DeepMind really want us to focus on. Admittedly, it is very impressive. They gave</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=883" target="_blank">00:14:43.200</a></span> | <span class="t">Gemini 1.5 Pro a grammar book and dictionary, 250,000 tokens in total from a super obscure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=890" target="_blank">00:14:50.480</a></span> | <span class="t">low resource language. The language is Kalamang and I had never heard of it. They take pains to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=895" target="_blank">00:14:55.520</a></span> | <span class="t">point out that none of that language was in the training dataset. And so what was the result?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=901" target="_blank">00:15:01.360</a></span> | <span class="t">Well, not only did Gemini 1.5 Pro crush GPT-4, it also did as well as a human who had learned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=909" target="_blank">00:15:09.360</a></span> | <span class="t">from the same materials. Now we're not talking about someone from that region of Papua New Guinea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=914" target="_blank">00:15:14.560</a></span> | <span class="t">The reason a human was used for comparison was to make that underlying point. Models are starting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=919" target="_blank">00:15:19.840</a></span> | <span class="t">to approach the learning rate, at least in terms of language of a human being. And don't forget,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=924" target="_blank">00:15:24.400</a></span> | <span class="t">this factors in data efficiency, same amount of data, similar result. Next up is what I believe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=929" target="_blank">00:15:29.920</a></span> | <span class="t">to be a fascinating graphic. It shows what happens as a model in blue Gemini 1.5 Pro is fed more and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=936" target="_blank">00:15:36.880</a></span> | <span class="t">more of a long document and of a code database. And the lower the curves go, the more accurate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=943" target="_blank">00:15:43.600</a></span> | <span class="t">the model is getting at predicting the next word. What happens to December's Gemini Pro</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=949" target="_blank">00:15:49.120</a></span> | <span class="t">as you feed it more and more tokens? Well, it starts to get overwhelmed both in terms of code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=954" target="_blank">00:15:54.960</a></span> | <span class="t">and for long documents. As the paper says that older model, and I hesitate to call it older</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=959" target="_blank">00:15:59.760</a></span> | <span class="t">because it's just two months ago, they're unable to effectively use information from the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=965" target="_blank">00:16:05.120</a></span> | <span class="t">context and are deteriorating in terms of prediction quality. But with Gemini 1.5 Pro,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=970" target="_blank">00:16:10.960</a></span> | <span class="t">the more it's fed, the better it gets. Even for a sequence of length a million for documents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=977" target="_blank">00:16:17.040</a></span> | <span class="t">or 10 million for code. It's quote, remembering things from millions of lines of code ago to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=983" target="_blank">00:16:23.520</a></span> | <span class="t">answer questions now. I think it's significant that when we get to sequence lengths of around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=988" target="_blank">00:16:28.320</a></span> | <span class="t">five to 10 million, the curve actually dips downward. It no longer follows the power law</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=994" target="_blank">00:16:34.320</a></span> | <span class="t">trend. That would suggest to me that if we went up to a hundred million, the results would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=998" target="_blank">00:16:38.960</a></span> | <span class="t">even more impressive. Here's what Google have to say. The results above suggest that the model is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1004" target="_blank">00:16:44.000</a></span> | <span class="t">able to improve its predictions by finding useful patterns, even if they occurred millions of tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1009" target="_blank">00:16:49.120</a></span> | <span class="t">in the past, as in the case of code. And to summarize this, we already knew that lower loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1014" target="_blank">00:16:54.000</a></span> | <span class="t">could be gone from more compute. It's a very similar curve, but what's new is that the power</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1019" target="_blank">00:16:59.840</a></span> | <span class="t">law is holding between loss and context length as shown above. They say from inspecting longer code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1026" target="_blank">00:17:06.160</a></span> | <span class="t">token predictions closer to 10 million, we see a phenomena of the increased context occasionally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1032" target="_blank">00:17:12.400</a></span> | <span class="t">providing outsized benefit. That could be due to repetition of code blocks. They think this deserves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1037" target="_blank">00:17:17.600</a></span> | <span class="t">further study and may be dependent on the exact data set used. So even Google aren't fully sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1043" target="_blank">00:17:23.120</a></span> | <span class="t">what's causing that dip. Now we all know that OpenAI kind of trolled Google tonight by releasing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1049" target="_blank">00:17:29.120</a></span> | <span class="t">Sora so soon after Gemini 1.5 Pro. But on this page, I feel Google were doing a little bit of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1055" target="_blank">00:17:35.760</a></span> | <span class="t">trolling to OpenAI. First, we have this comparison again of retrieval and they say they got API</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1062" target="_blank">00:17:42.800</a></span> | <span class="t">errors after 128,000 tokens. Well, of course, they knew that because GPT-4 Turbo only supports 128,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1070" target="_blank">00:17:50.720</a></span> | <span class="t">tokens. I think they kind of wanted to say that after this length, we crush it and with them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1076" target="_blank">00:17:56.160</a></span> | <span class="t">you just get an error code. And the next bit of trolling comes here. These haystack challenges</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1080" target="_blank">00:18:00.880</a></span> | <span class="t">where they secrete a phrase like this. The special magic quote city number is quote. With this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1086" target="_blank">00:18:06.880</a></span> | <span class="t">the model has to retrieve the correct city and number which is randomized. But that phrase could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1091" target="_blank">00:18:11.840</a></span> | <span class="t">have been hidden in any long text and they chose the essays of Paul Graham. Now, yes, this is almost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1097" target="_blank">00:18:17.520</a></span> | <span class="t">certainly coincidental, but Paul Graham was the guy who fired Sam Altman at Y Combinator. Sam</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1102" target="_blank">00:18:22.960</a></span> | <span class="t">Altman disputes that it was a firing. For audio, it's the same thing. Even when they break down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1108" target="_blank">00:18:28.640</a></span> | <span class="t">long audio into segments that Whisper can digest, which are then transcribed and fed to GPT-4 Turbo,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1115" target="_blank">00:18:35.280</a></span> | <span class="t">the difference is stark. Before you think, though, that Gemini 1.5 Pro is perfect at retrieval,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1121" target="_blank">00:18:41.440</a></span> | <span class="t">what happens when you feed in 100 needles into a massive haystack? Well, in that case, it still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1127" target="_blank">00:18:47.600</a></span> | <span class="t">massively outperforms GPT-4 Turbo, but can recall, as you can see, 60, 70, 80% of those needles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1134" target="_blank">00:18:54.880</a></span> | <span class="t">It is not a perfect model and no, we don't have AGI. And at this point, Google does state that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1141" target="_blank">00:19:01.120</a></span> | <span class="t">retrieval is not the same as reasoning. They basically beg for harder benchmarks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1146" target="_blank">00:19:06.960</a></span> | <span class="t">ones that require integrating disparate facts, drawing inferences, or resolving inconsistencies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1152" target="_blank">00:19:12.400</a></span> | <span class="t">essentially reasoning. If you want to know more about how reasoning, some would say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1156" target="_blank">00:19:16.320</a></span> | <span class="t">is the final holy grail of large language models, do check out my Patreon AI Insiders.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1162" target="_blank">00:19:22.080</a></span> | <span class="t">I have around a dozen videos and podcasts up as of today. In fact, it was just today that I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1167" target="_blank">00:19:27.600</a></span> | <span class="t">released this video on my Patreon. It's a 14 minute tour of deepfakes and features, interviews,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1173" target="_blank">00:19:33.920</a></span> | <span class="t">and exclusives, and more. If you are a student or retired, do email me about a potential small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1179" target="_blank">00:19:39.520</a></span> | <span class="t">discount. Now for the final demo in coding. We'll walk through some example prompts using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1185" target="_blank">00:19:45.040</a></span> | <span class="t">the 3JS example code, which comes out to over 800,000 tokens. We extracted the code for all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1191" target="_blank">00:19:51.040</a></span> | <span class="t">of the 3JS examples and put it together into this text file, which we brought into Google AI Studio</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1196" target="_blank">00:19:56.160</a></span> | <span class="t">over here. We asked the model to find three examples for learning about character animation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1201" target="_blank">00:20:01.280</a></span> | <span class="t">The model looked across hundreds of examples and picked out these three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1204" target="_blank">00:20:04.400</a></span> | <span class="t">Next, we asked, what controls the animations on the littlest Tokyo demo?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1209" target="_blank">00:20:09.040</a></span> | <span class="t">As you can see here, the model was able to find that demo,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1213" target="_blank">00:20:13.760</a></span> | <span class="t">and it explained that the animations are embedded within the glTF model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1219" target="_blank">00:20:19.760</a></span> | <span class="t">Next, we wanted to see if it could customize this code for us. So we asked,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1223" target="_blank">00:20:23.280</a></span> | <span class="t">show me some code to add a slider to control the speed of the animation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1226" target="_blank">00:20:26.800</a></span> | <span class="t">Use that kind of GUI the other demos have. This is what it looked like before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1230" target="_blank">00:20:30.400</a></span> | <span class="t">on the original 3JS site. And here's the modified version. It's the same scene,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1235" target="_blank">00:20:35.120</a></span> | <span class="t">but it added this little slider to speed up, slow down, or even stop the animation on the fly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1240" target="_blank">00:20:40.080</a></span> | <span class="t">Again, with Audio Gemini Crush's Whisper, it has a significantly lower word error rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1246" target="_blank">00:20:46.640</a></span> | <span class="t">And for video, it was pretty funny they had to invent their own benchmarks because the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1252" target="_blank">00:20:52.240</a></span> | <span class="t">ones were too easy. Or in formal language, to bridge this evaluation gap, we introduced a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1257" target="_blank">00:20:57.840</a></span> | <span class="t">new benchmark that was testing that incredible feat we saw earlier of picking out key details</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1263" target="_blank">00:21:03.040</a></span> | <span class="t">from long videos. Now, to be clear, despite the demos looking good and beating GPT-4V,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1269" target="_blank">00:21:09.040</a></span> | <span class="t">we're still not close to perfect. Just because Gemini 1.5 Pro can see across long context and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1275" target="_blank">00:21:15.760</a></span> | <span class="t">watch long videos doesn't mean it's perfect at answering questions. Remember that recalling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1281" target="_blank">00:21:21.040</a></span> | <span class="t">facts is not the same as reasoning or getting 100% on multiple choice questions. I also found this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1287" target="_blank">00:21:27.200</a></span> | <span class="t">part of the paper quite funny where they tried to highlight the extent of trade-offs of switching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1292" target="_blank">00:21:32.400</a></span> | <span class="t">architecture if it exists. And the problem was they couldn't find any. Across the board, 1.5 Pro</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1298" target="_blank">00:21:38.480</a></span> | <span class="t">was just better than 1.0 Pro. Whether that was math, science, coding, multilinguality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1304" target="_blank">00:21:44.800</a></span> | <span class="t">instruction following, image understanding, video understanding, speech recognition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1309" target="_blank">00:21:49.360</a></span> | <span class="t">or speech translation. Of course, it's obligatory at this point for me to ding them about the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1314" target="_blank">00:21:54.880</a></span> | <span class="t">accuracy level of their MMLU benchmark test for Gemini 1.5 Pro. They say for math and science,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1321" target="_blank">00:22:01.760</a></span> | <span class="t">it's 1.8% behind Gemini 1.0 Ultra. But how meaningful is that with this many errors just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1328" target="_blank">00:22:08.400</a></span> | <span class="t">in the college chemistry section of the MMLU? Buried deep is one admission that 1.5 Pro doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1334" target="_blank">00:22:14.960</a></span> | <span class="t">seem quite as good at OCR. That's optical character recognition, in other words, recognizing text from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1341" target="_blank">00:22:21.280</a></span> | <span class="t">an image. But Google Cloud Vision is state of the art anyway at OCR and soon enough, surely,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1346" target="_blank">00:22:26.560</a></span> | <span class="t">they're going to integrate that. So I don't see OCR being a long-term weakness for the Gemini</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1351" target="_blank">00:22:31.680</a></span> | <span class="t">series. And it's hard to tell, but it seems like Google found some false negatives in other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1356" target="_blank">00:22:36.800</a></span> | <span class="t">benchmarks. And so the performance there was lower bounding the model's true performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1361" target="_blank">00:22:41.920</a></span> | <span class="t">And they complain, as I did in my original Smart GPT video, that maybe we need to rely more on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1367" target="_blank">00:22:47.760</a></span> | <span class="t">human evaluations for these datasets and that maybe we should deviate from strict string matching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1373" target="_blank">00:22:53.520</a></span> | <span class="t">And there was this quite cute section in the impact assessment part of the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1378" target="_blank">00:22:58.160</a></span> | <span class="t">So what are the impacts going to be of Gemini 1.5 Pro? Well, they say the ability to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1384" target="_blank">00:23:04.000</a></span> | <span class="t">longer content enhances the efficiency of individual and commercial users in processing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1389" target="_blank">00:23:09.440</a></span> | <span class="t">various multimodal inputs. But that besides efficiency, the model enables societally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1394" target="_blank">00:23:14.800</a></span> | <span class="t">beneficial downstream use cases. And they foresee Gemini 1.5 Pro being used to explore archival</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1401" target="_blank">00:23:21.520</a></span> | <span class="t">content that might potentially benefit journalists and historians. Suffice to say, I think this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1406" target="_blank">00:23:26.560</a></span> | <span class="t">somewhat underplaying the impact of Gemini 1.5 Pro. Just for one, I think it could transform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1412" target="_blank">00:23:32.720</a></span> | <span class="t">how YouTube works. Or another obvious one. What about long term "relationships" with chatbots?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1418" target="_blank">00:23:38.480</a></span> | <span class="t">GPT-4's new memory feature, which seems to me like an only slightly more advanced custom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1423" target="_blank">00:23:43.120</a></span> | <span class="t">instruction, pales in comparison to Gemini 1.5's potential. You could have discussions lasting for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1429" target="_blank">00:23:49.520</a></span> | <span class="t">months with Gemini and it might remember a detail you said back, say, six months ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1435" target="_blank">00:23:55.280</a></span> | <span class="t">That seems to me true memory and might encourage a kind of companionship for some people with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1440" target="_blank">00:24:00.800</a></span> | <span class="t">these models. On safety, without giving too much detail, they just say it's safer than 1.0 Pro</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1447" target="_blank">00:24:07.200</a></span> | <span class="t">and 1.0 Ultra. But later they do admit two things. First, Gemini 1.5 Pro does seem a little bit more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1454" target="_blank">00:24:14.960</a></span> | <span class="t">biased. It's probably a bit harder for the model to be anti-stereotypical when it remembers so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1460" target="_blank">00:24:20.480</a></span> | <span class="t">much. Also, and I know this is going to annoy quite a few people, it has a higher refusal rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1466" target="_blank">00:24:26.800</a></span> | <span class="t">That's on questions that should be both legitimately refused and not legitimately. In</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1472" target="_blank">00:24:32.240</a></span> | <span class="t">other words, they should have been answered. Of course, by the time the model actually comes out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1476" target="_blank">00:24:36.160</a></span> | <span class="t">we'll have to see if this is still the case. But you just have to take a look at my Gemini Ultra</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1481" target="_blank">00:24:41.120</a></span> | <span class="t">review to see that at the moment the refusals are pretty extreme. This could honestly be a key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1486" target="_blank">00:24:46.880</a></span> | <span class="t">sticking point for a lot of people. We're drawing to an end here, but just a quick handful of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1491" target="_blank">00:24:51.840</a></span> | <span class="t">further observations. Remember that trick with Chachabitty where you submit the letter A with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1496" target="_blank">00:24:56.320</a></span> | <span class="t">space, say 500 times, and it regurgitates sometimes it's training data. Well, apparently that works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1502" target="_blank">00:25:02.320</a></span> | <span class="t">also on Gemini 1.5 Pro. Thing is you have to manually repeat that character many more times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1508" target="_blank">00:25:08.000</a></span> | <span class="t">up to a million times. But with those long prompts, they do admit that it becomes easier to obtain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1513" target="_blank">00:25:13.840</a></span> | <span class="t">it's memorized data. I presume that's the kind of thing that Google DeepMind are working on before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1518" target="_blank">00:25:18.400</a></span> | <span class="t">they release Gemini 1.5 Pro. And one more key detail from the blog post that many people might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1524" target="_blank">00:25:24.480</a></span> | <span class="t">have missed. When Gemini 1.5 Pro is released to the public, it's going to start at just 128,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1531" target="_blank">00:25:31.360</a></span> | <span class="t">token context window. I say just, but that's still pretty impressive. And it seems to me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1535" target="_blank">00:25:35.840</a></span> | <span class="t">based on the wording of the following sentence, that even that basic version won't be free.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1541" target="_blank">00:25:41.520</a></span> | <span class="t">They say we plan to introduce pricing tiers that start at the standard 128,000 context window. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1548" target="_blank">00:25:48.640</a></span> | <span class="t">anyone hoping to get Gemini 1.5 for free seems to have misplaced hope. And then there's going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1554" target="_blank">00:25:54.880</a></span> | <span class="t">tiers going up to 1 million tokens. I'm not sure how expensive that 1 million token tier will be,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1560" target="_blank">00:26:00.960</a></span> | <span class="t">but I'll probably be on it. Notice that we probably won't be able to buy going up to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1565" target="_blank">00:26:05.600</a></span> | <span class="t">10 million tokens. But I do want to end on a positive note for Google. There was one thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1571" target="_blank">00:26:11.440</a></span> | <span class="t">I missed out from my review of Google Gemini Ultra, and I want to make amends. And that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1577" target="_blank">00:26:17.280</a></span> | <span class="t">its creative writing ability. Gemini 1.0 Ultra is simply better at creative writing than GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1584" target="_blank">00:26:24.400</a></span> | <span class="t">And of course, we're not even talking about 1.5 Ultra. How so? Well, Gemini varies its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1589" target="_blank">00:26:29.760</a></span> | <span class="t">sentence length. We have short sentences like this, "Dibbons never really listened." We also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1594" target="_blank">00:26:34.640</a></span> | <span class="t">get far more dialogue, which is just much more realistic to real creative writing. There's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1599" target="_blank">00:26:39.520</a></span> | <span class="t">bit more humor in there. Whatever they did with their writing data set, they did better than open</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1605" target="_blank">00:26:45.120</a></span> | <span class="t">AI. GPT-4 stories tend to be far more wordy, a lot more tell not show. And I'm actually going to go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1612" target="_blank">00:26:52.000</a></span> | <span class="t">one further and prove that to you. When you put GPT-4 story into two state-of-the-art AI text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1618" target="_blank">00:26:58.640</a></span> | <span class="t">detectors, that's GPT-0 and Binoculars, which is a new tool, both of them say most likely AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1625" target="_blank">00:27:05.360</a></span> | <span class="t">generated. GPT-0 puts it at 97%. For Claude, we also get most likely AI generated. Although GPT-0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1633" target="_blank">00:27:13.440</a></span> | <span class="t">erroneously says it's only 12% likely AI generated. That's Claude's story. It's way too much to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1639" target="_blank">00:27:19.920</a></span> | <span class="t">into this video now, but remember Binoculars is state-of-the-art compared to GPT-0. But here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1646" target="_blank">00:27:26.320</a></span> | <span class="t">the punchline. This is Google Gemini's story. We get from GPT-0, 0% chance of being AI generated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1654" target="_blank">00:27:34.640</a></span> | <span class="t">And even the state-of-the-art Binoculars gives it most likely human generated. And I think this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1660" target="_blank">00:27:40.880</a></span> | <span class="t">proves two points. First, Gemini is definitely better at creative writing and making marketing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1666" target="_blank">00:27:46.320</a></span> | <span class="t">copy, by the way, but too long to get into here. And second, don't put your faith in AI text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1671" target="_blank">00:27:51.600</a></span> | <span class="t">detectors, especially not in the age of Gemini. If you want to learn more about detecting AI and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1677" target="_blank">00:27:57.360</a></span> | <span class="t">deepfakes, of course, I refer you back to my deepfakes video on my Patreon, AI Insiders.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1682" target="_blank">00:28:02.720</a></span> | <span class="t">So that is Gemini 1.5 Pro. And yes, this does seem the most significant night for AI since</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1689" target="_blank">00:28:09.520</a></span> | <span class="t">the release of GPT-4. As I said in my video on January the 1st, AI is still on an exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1696" target="_blank">00:28:16.240</a></span> | <span class="t">curve. 2024 will not be a slow year in AI. And for as long as I can, I will be here to cover it all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Cs6pe8o7XY8&t=1704" target="_blank">00:28:24.800</a></span> | <span class="t">Thank you so much for watching and have a wonderful day.</span></div></div></body></html>