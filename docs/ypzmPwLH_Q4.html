<html><head><title>Better Llama 2 with Retrieval Augmented Generation (RAG)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Better Llama 2 with Retrieval Augmented Generation (RAG)</h2><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4"><img src="https://i.ytimg.com/vi_webp/ypzmPwLH_Q4/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=0">0:0</a> Retrieval Augmented Generation with Llama 2<br><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=29">0:29</a> Python Prerequisites and Llama 2 Access<br><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=99">1:39</a> Retrieval Augmented Generation 101<br><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=233">3:53</a> Creating Embeddings with Open Source<br><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=383">6:23</a> Building Pinecone Vector DB<br><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=518">8:38</a> Creating Embedding Dataset<br><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=705">11:45</a> Initializing Llama 2<br><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=878">14:38</a> Creating the RAG RetrievalQA Component<br><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=943">15:43</a> Comparing Llama 2 vs RAG Llama 2<br><br><div style="text-align: left;"><a href="./ypzmPwLH_Q4.html">Whisper Transcript</a> | <a href="./transcript_ypzmPwLH_Q4.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">In today's video we're going to be looking at more LLAMA2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=3" target="_blank">00:00:03.360</a></span> | <span class="t">This time we're going to be looking at a very simple version of Retrieval Augmented Generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=11" target="_blank">00:00:11.040</a></span> | <span class="t">using the 13 billion parameter LLAMA2 model, which we're going to quantize and actually fit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=16" target="_blank">00:00:16.640</a></span> | <span class="t">that onto a single T4 GPU, which is included within the free tier of Colab, so anyone can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=24" target="_blank">00:00:24.560</a></span> | <span class="t">actually run this. It should be pretty fun. Let's jump straight into the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=29" target="_blank">00:00:29.680</a></span> | <span class="t">So to get started with this notebook, there'll be a link to this at the top of the video right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=33" target="_blank">00:00:33.920</a></span> | <span class="t">The first thing that you will have to do if you haven't already is actually request access to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=40" target="_blank">00:00:40.960</a></span> | <span class="t">LLAMA2, which you can do via a form. If you need some guidance on that, there'll be a link to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=48" target="_blank">00:00:48.160</a></span> | <span class="t">another video of mine, the previous LLAMA2 video, where I describe how to go through that and get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=55" target="_blank">00:00:55.120</a></span> | <span class="t">access. So first thing I'm going to want to do after getting your access is we want to go to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=62" target="_blank">00:01:02.480</a></span> | <span class="t">change runtime type and you want to make sure that you're using GPU for hardware accelerator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=68" target="_blank">00:01:08.720</a></span> | <span class="t">and T4 for your GPU type. If you have Colab Pro, you can use one of these and it will run a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=75" target="_blank">00:01:15.280</a></span> | <span class="t">faster, but T4 is good enough. Cool. So we just have to install everything we need. Okay. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=83" target="_blank">00:01:23.920</a></span> | <span class="t">once that is ready, we come down to here. So Hunkface embedding pipeline. So before we dive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=89" target="_blank">00:01:29.920</a></span> | <span class="t">into the embedding pipeline, maybe what I should do is try to explain a little bit of what this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=95" target="_blank">00:01:35.040</a></span> | <span class="t">retrieve augmented generation thing is and why it's so important. So a problem that we have with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=100" target="_blank">00:01:40.400</a></span> | <span class="t">LLMs is that they don't have access to the outside world. The only knowledge contained within them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=106" target="_blank">00:01:46.800</a></span> | <span class="t">is knowledge that they learned during training, which can be super limiting. So in this example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=112" target="_blank">00:01:52.400</a></span> | <span class="t">here, this was a little while ago, I asked GPT-4 how to use the LLM chain and Lang chain. Okay. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=118" target="_blank">00:01:58.800</a></span> | <span class="t">Lang chain being the sort of new LLM framework. And the answer it gave me specified this Lang</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=127" target="_blank">00:02:07.040</a></span> | <span class="t">chain, which is a blockchain based decentralized AI language model, which is completely wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=132" target="_blank">00:02:12.720</a></span> | <span class="t">Basically it hallucinated. And the reason for that is that GPT-4 just didn't know anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=137" target="_blank">00:02:17.760</a></span> | <span class="t">about Lang chain. And that's because it didn't have access to the outside world. It just had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=144" target="_blank">00:02:24.080</a></span> | <span class="t">knowledge. It's called parametric knowledge. This knowledge stored within the model itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=149" target="_blank">00:02:29.120</a></span> | <span class="t">that gained during training. So the idea behind retrieval augmented generation is that you give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=155" target="_blank">00:02:35.040</a></span> | <span class="t">your LLM access to the outside world. And the way that we do it, at least in this example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=160" target="_blank">00:02:40.960</a></span> | <span class="t">is we're going to give it access to the outside world, like our subset of the outside world,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=167" target="_blank">00:02:47.120</a></span> | <span class="t">not the entire outside world. And we're going to do that by searching with natural language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=172" target="_blank">00:02:52.400</a></span> | <span class="t">which is ideal when it comes to our LLM, because our LLM works with natural language. So we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=180" target="_blank">00:03:00.400</a></span> | <span class="t">interact with LLM using natural language, and then we search with natural language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=184" target="_blank">00:03:04.640</a></span> | <span class="t">And what that will allow us to do is we'll ask a question, we'll get relevant information about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=191" target="_blank">00:03:11.920</a></span> | <span class="t">that question from somewhere else. And we get to feed that relevant information plus our original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=199" target="_blank">00:03:19.280</a></span> | <span class="t">question back into the LLM, giving it access. So this is what we would call source knowledge,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=205" target="_blank">00:03:25.920</a></span> | <span class="t">rather than parametric knowledge. Now, part of this is that embedding model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=209" target="_blank">00:03:29.760</a></span> | <span class="t">So the embedding model is how we build this retrieval system. It's how we translate human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=218" target="_blank">00:03:38.320</a></span> | <span class="t">readable text into machine readable vectors. And we need machine readable vectors in order to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=225" target="_blank">00:03:45.280</a></span> | <span class="t">perform a search and to perform it based on semantic meaning, rather than my traditional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=231" target="_blank">00:03:51.040</a></span> | <span class="t">search, which would be more on keywords. So in the spirit of going with open source or open access</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=236" target="_blank">00:03:56.880</a></span> | <span class="t">models, as is the case with LLAMA2, we're going to use a open source model. So we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=242" target="_blank">00:04:02.400</a></span> | <span class="t">use the Sentence Transformers library. If you've been watching my videos for a while, this will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=248" target="_blank">00:04:08.000</a></span> | <span class="t">kind of like a flashback to a little while ago. So we used Sentence Transformers a lot before the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=257" target="_blank">00:04:17.440</a></span> | <span class="t">whole open AI chatty petite thing I kicked off. Now, this model here is a very small model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=265" target="_blank">00:04:25.280</a></span> | <span class="t">super easy to run. You can run it on CPU. Okay. Let's have a look at how much RAM I just used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=270" target="_blank">00:04:30.560</a></span> | <span class="t">Okay. At the moment, it seems like we're not really even using any. So I think it may need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=277" target="_blank">00:04:37.680</a></span> | <span class="t">to wait until we actually start creating embeddings, which we do next. So you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=282" target="_blank">00:04:42.800</a></span> | <span class="t">that we're using the CUDA device. Here, we're going to create some embeddings. Okay. You see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=287" target="_blank">00:04:47.840</a></span> | <span class="t">that we're using some GPU RAM now, but very little, 0.9 gigabytes, which is nothing. That's pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=294" target="_blank">00:04:54.320</a></span> | <span class="t">cool. So what we've done here is we've created these two documents or chunks of text. We embed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=299" target="_blank">00:04:59.280</a></span> | <span class="t">them using our embedding model. So if I just come up to here, the way that we've initialized our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=305" target="_blank">00:05:05.520</a></span> | <span class="t">Sentence Transformer is a little different to how I used to do it. So we've essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=310" target="_blank">00:05:10.400</a></span> | <span class="t">initialized it through HuggingFace. And then we have actually loaded that into the LangChain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=317" target="_blank">00:05:17.520</a></span> | <span class="t">HuggingFace embeddings object. Okay. So we're using HuggingFace via LangChain to use Sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=323" target="_blank">00:05:23.600</a></span> | <span class="t">Transformers. So there's a few abstractions there, but this will make things a lot easier for us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=328" target="_blank">00:05:28.240</a></span> | <span class="t">later on. Okay. Cool. And let's onto this. So we have loaded our embedding model. We have two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=338" target="_blank">00:05:38.640</a></span> | <span class="t">document embeddings. That's because we have two documents here. And each of those has a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=342" target="_blank">00:05:42.560</a></span> | <span class="t">dimensionality of 384. Now with OpenAI, for comparison, we're going to be embedding to a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=348" target="_blank">00:05:48.640</a></span> | <span class="t">dimensionality of 1,536, I think it is. So with this, you can, particularly with Pinecone, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=358" target="_blank">00:05:58.320</a></span> | <span class="t">vector database I'm talking about later, you can fit in five of these for every one OpenAI embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=364" target="_blank">00:06:04.640</a></span> | <span class="t">The performance is less with these, to be honest, but it kind of depends on your use case. A lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=371" target="_blank">00:06:11.600</a></span> | <span class="t">the time, you don't need the performance that OpenAI embeddings gives you. Like in this example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=377" target="_blank">00:06:17.360</a></span> | <span class="t">it actually works really well with this very small model. So that's pretty useful. Now, yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=384" target="_blank">00:06:24.480</a></span> | <span class="t">let's move on to the Pinecone bit. So when we're going to create our vector database and build our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=390" target="_blank">00:06:30.400</a></span> | <span class="t">vector index. So to do that, we're going to need a free Pinecone API key. So I'm going to click on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=397" target="_blank">00:06:37.120</a></span> | <span class="t">this link here. That's going to take us to here, app.pinecone.io. I'm going to come over to my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=405" target="_blank">00:06:45.600</a></span> | <span class="t">default project, zoom in a little bit here, and go to API keys, right? And we need the environment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=412" target="_blank">00:06:52.880</a></span> | <span class="t">here. So us-west1-gcp, remember that, or for you, this environment will be different. So whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=420" target="_blank">00:07:00.080</a></span> | <span class="t">environment you have next to your API key, remember that, and then just copy your API key. Come back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=425" target="_blank">00:07:05.200</a></span> | <span class="t">over to here. You're going to put in your API key here, and you're also going to put in that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=429" target="_blank">00:07:09.360</a></span> | <span class="t">environment or the cloud region. So it was us-west1-gcp for me. Okay. And I initialize that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=438" target="_blank">00:07:18.000</a></span> | <span class="t">with my API key. And now we move on to the next cell. So in this next cell, we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=443" target="_blank">00:07:23.600</a></span> | <span class="t">initialize the index, basically just create where we're going to store all of our vectors that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=448" target="_blank">00:07:28.960</a></span> | <span class="t">create with that embedding model. There are a few items here. So dimension, this needs to match</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=453" target="_blank">00:07:33.920</a></span> | <span class="t">the dimensionality of your embedding model. We already found ours before. So it's this 3, 8, 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=459" target="_blank">00:07:39.440</a></span> | <span class="t">So we feed that into there. And then the metric, metrics can change depending on your embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=464" target="_blank">00:07:44.800</a></span> | <span class="t">model. With OpenAI's R002, you're going to be using, you can use either cosine or dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=471" target="_blank">00:07:51.600</a></span> | <span class="t">With open source models, it varies a bit more. Sometimes you have to use cosine. Sometimes you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=477" target="_blank">00:07:57.680</a></span> | <span class="t">have to use dot product. Sometimes you have to use Euclidean, although that one is a little less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=482" target="_blank">00:08:02.160</a></span> | <span class="t">common. So it's worth just checking. You can usually find in the model cards on Huggingface</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=488" target="_blank">00:08:08.080</a></span> | <span class="t">which metric you need to use, but most common, the kind of go-to is cosine. All right, cool. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=495" target="_blank">00:08:15.840</a></span> | <span class="t">we initialize that. Okay, cool. So that initialize, it does take a minute. For me, it was like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=502" target="_blank">00:08:22.640</a></span> | <span class="t">minute right now. And then we want to connect to the index. So we do, I go index, index name,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=510" target="_blank">00:08:30.400</a></span> | <span class="t">and then we can describe that index as well, just to see what is in there at the moment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=514" target="_blank">00:08:34.400</a></span> | <span class="t">which should for now be nothing. Okay, cool. Now with the index ready and the embedding ready,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=524" target="_blank">00:08:44.480</a></span> | <span class="t">we're ready to begin populating our database. Okay. So just like a typical traditional database</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=531" target="_blank">00:08:51.040</a></span> | <span class="t">with a vector database, you need to put things in there in order to retrieve things from that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=537" target="_blank">00:08:57.040</a></span> | <span class="t">database later on. So that's what we're going to do now. So we're going to come down to here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=542" target="_blank">00:09:02.160</a></span> | <span class="t">I quickly just pulled this together. It's essentially a small dataset. I think it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=549" target="_blank">00:09:09.920</a></span> | <span class="t">just around 5,000 items in there. And it just contains chunks of text from the LLAMA2 paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=557" target="_blank">00:09:17.840</a></span> | <span class="t">and a few other related papers. So I just built that by kind of going through the LLAMA2 paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=566" target="_blank">00:09:26.320</a></span> | <span class="t">and extracting the references and extracting those papers as well. And just kind of like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=571" target="_blank">00:09:31.600</a></span> | <span class="t">repeating that loop a few times. All right. So once we download that, we come down to here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=578" target="_blank">00:09:38.160</a></span> | <span class="t">we're going to convert that HuggingFace dataset. So this is using HuggingFace datasets. We're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=584" target="_blank">00:09:44.480</a></span> | <span class="t">to convert that into a pandas data frame. And we're specifying here that we would like to upload</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=591" target="_blank">00:09:51.360</a></span> | <span class="t">everything in batches of 32. Honestly, we could definitely increase that to like 100 or so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=599" target="_blank">00:09:59.120</a></span> | <span class="t">but it doesn't really matter because it's not a big dataset. It's not going to take long to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=604" target="_blank">00:10:04.800</a></span> | <span class="t">push everything to Pinecone. So let's just have a look at this loop. We're going through in these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=609" target="_blank">00:10:09.680</a></span> | <span class="t">batches of 32. We are getting our batch from the data frame. We're getting IDs first. Then we get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=619" target="_blank">00:10:19.920</a></span> | <span class="t">the chunks of texts from the data frame, and then we get our metadata from the data frame.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=625" target="_blank">00:10:25.120</a></span> | <span class="t">So maybe what would actually be helpful here is if I just show you what's in that data frame.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=630" target="_blank">00:10:30.400</a></span> | <span class="t">So data.head. Okay. So you can see here, we just have a chunk ID. So I'm going to use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=639" target="_blank">00:10:39.760</a></span> | <span class="t">I think I use DOI and chunk ID to create the ID for each entry. Yeah. And then we have the chunk,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=646" target="_blank">00:10:46.160</a></span> | <span class="t">which is just like a chunk of text. Okay. You can kind of see that here. We have the paper IDs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=652" target="_blank">00:10:52.240</a></span> | <span class="t">the title of the paper, some summaries, the source, several other things in there. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=658" target="_blank">00:10:58.000</a></span> | <span class="t">But we don't need all of that. So for the metadata, we actually just keep the text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=662" target="_blank">00:11:02.960</a></span> | <span class="t">the source, and the title. And yeah, we can run that. It should be pretty quick. Okay. So that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=670" target="_blank">00:11:10.080</a></span> | <span class="t">took 30 seconds for me. You can also, I kind of forgot to do this, but you can do from TQDM,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=677" target="_blank">00:11:17.120</a></span> | <span class="t">auto import TQDM, and you can add like a progress bar so that you can actually see the progress</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=684" target="_blank">00:11:24.960</a></span> | <span class="t">like that. Okay. So that's just a little bit nicer if you would rather not just be staring at a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=694" target="_blank">00:11:34.800</a></span> | <span class="t">cell doing something. Okay. Cool. So now if we describe index sets, we should see about 5,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=701" target="_blank">00:11:41.680</a></span> | <span class="t">vectors in there. Okay. So it's pretty cool. Now what we're going to do, so we have our index like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=708" target="_blank">00:11:48.000</a></span> | <span class="t">database ready. What we want to do now is we want to add in the LLM. So we want to add in LLM2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=715" target="_blank">00:11:55.920</a></span> | <span class="t">To do that, we're going to be using the text generation pipeline from HuggingFace. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=720" target="_blank">00:12:00.160</a></span> | <span class="t">we're going to be loading that into the line chain. We're going to be using the LLM2 13-bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=725" target="_blank">00:12:05.520</a></span> | <span class="t">chat model, which you can see here and everything that comes with that. I've explained this stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=734" target="_blank">00:12:14.880</a></span> | <span class="t">here. So like how to load the model, the quantization, everything else several times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=739" target="_blank">00:12:19.840</a></span> | <span class="t">So I'm not going to go through that again. If you do want to go through that, it's in the video that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=745" target="_blank">00:12:25.040</a></span> | <span class="t">I linked earlier, the previous LLM2 video. But what I will do is show you how to get this HuggingFace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=750" target="_blank">00:12:30.400</a></span> | <span class="t">authentication token. So for that, we go to HuggingFace.co. We want to go to your profile</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=758" target="_blank">00:12:38.160</a></span> | <span class="t">icon at the top here, settings, and then you go to access tokens. You would have to create a new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=764" target="_blank">00:12:44.480</a></span> | <span class="t">token here. I've already created mine. Just make it a read token. You can use a write if you want,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=769" target="_blank">00:12:49.440</a></span> | <span class="t">but it just gives more permissions that you don't need for this. But I've created mine here. I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=773" target="_blank">00:12:53.840</a></span> | <span class="t">just going to copy it and I will put it into this string here and we run that. That's just going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=782" target="_blank">00:13:02.160</a></span> | <span class="t">load everything. So we need that authentication token because LLM2, all those models, you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=788" target="_blank">00:13:08.640</a></span> | <span class="t">permission to use them. You get that by signing up through Meta's forms and everything, as I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=795" target="_blank">00:13:15.600</a></span> | <span class="t">mentioned earlier. So you need to, in this case, which you don't for every model on HuggingFace,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=801" target="_blank">00:13:21.840</a></span> | <span class="t">but for this model, you do need to authenticate yourself. Okay. So that will take a moment to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=808" target="_blank">00:13:28.240</a></span> | <span class="t">load. Just note here, I'm using a GPU and then I am switching the model to like evaluation mode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=816" target="_blank">00:13:36.240</a></span> | <span class="t">And actually, sorry, we don't need to use that GPU code here because the device actually figures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=823" target="_blank">00:13:43.840</a></span> | <span class="t">it out by itself. But it's good to make sure that we actually are using CUDA. So that would just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=830" target="_blank">00:13:50.160</a></span> | <span class="t">print out down here. It should print out something like model loaded on CUDA zero. So this will take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=836" target="_blank">00:13:56.160</a></span> | <span class="t">a moment to load. So I'll just skip ahead to when it's ready. Okay. So that has finished loading.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=843" target="_blank">00:14:03.600</a></span> | <span class="t">It took eight minutes and we can see that the GPU memory has gone up to 8.2 gigabytes. So it's using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=851" target="_blank">00:14:11.040</a></span> | <span class="t">more now, considering also that that 1.2 gigabytes of that was used by the mini LLM model. We're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=857" target="_blank">00:14:17.680</a></span> | <span class="t">using like seven gigabytes for this quantized version of the model, which is pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=863" target="_blank">00:14:23.040</a></span> | <span class="t">Now I'm slowing the tokenizer, the pipeline. Again, I went through all this stuff before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=867" target="_blank">00:14:27.920</a></span> | <span class="t">so I'm not going to go through it again. And then what we do is just initialize that in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=873" target="_blank">00:14:33.840</a></span> | <span class="t">line chain. So now we can start using all the different line chain utilities. So come down to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=880" target="_blank">00:14:40.240</a></span> | <span class="t">here, what we need to do is initialize the retrieval QA chain. So this is like the simplest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=885" target="_blank">00:14:45.760</a></span> | <span class="t">form of reg that you can get in for your LLMs. So for that, for retrieval QA chain, we need a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=894" target="_blank">00:14:54.000</a></span> | <span class="t">vector store, which is like another line chain object and our LLM, which we already have. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=902" target="_blank">00:15:02.240</a></span> | <span class="t">let's initialize our vector store and we just confirmed that it works. So we have this query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=908" target="_blank">00:15:08.560</a></span> | <span class="t">I'm going to do a similar search. So this is not using the LLM or here, this is just retrieving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=913" target="_blank">00:15:13.920</a></span> | <span class="t">what it believes are relevant documents. Now it's kind of hard to read these, to be honest,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=919" target="_blank">00:15:19.600</a></span> | <span class="t">I at least struggle, but we'll see in a moment that the LLM does actually manage to get good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=925" target="_blank">00:15:25.600</a></span> | <span class="t">information from these. So we create our reg pipeline like so, so we just pass in our LLM,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=932" target="_blank">00:15:32.960</a></span> | <span class="t">our retriever and the chain type. Chain type basically just means it's going to stuff all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=937" target="_blank">00:15:37.040</a></span> | <span class="t">of the context into the context window of the LLM query. And then we can begin asking questions. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=945" target="_blank">00:15:45.040</a></span> | <span class="t">let's begin by asking what is so special about LLAMA2? We run that. This will take, again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=955" target="_blank">00:15:55.120</a></span> | <span class="t">we're using the smallest GPU possible here. So it's going to take a little bit of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=960" target="_blank">00:16:00.160</a></span> | <span class="t">Also the quantization set that we use to make this model so small adds time to the processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=966" target="_blank">00:16:06.960</a></span> | <span class="t">Or inference speeds. So we do have to wait a moment. Okay. And we get our response. It took</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=973" target="_blank">00:16:13.200</a></span> | <span class="t">like a minute. Again, if you actually want to run this in production, you're probably going to want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=978" target="_blank">00:16:18.640</a></span> | <span class="t">more GPU power and also not to quantize the model. So yeah, we get this. It's talking about actual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=985" target="_blank">00:16:25.920</a></span> | <span class="t">LLAMAs. It just tells us a load of random things like their coats can be a variety of colors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=991" target="_blank">00:16:31.520</a></span> | <span class="t">They are silky, I think it says somewhere. I know it did in the previous output. They're calm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=997" target="_blank">00:16:37.600</a></span> | <span class="t">so on and so on. We don't need that. So what we actually want to ask about is LLAMA2, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1003" target="_blank">00:16:43.600</a></span> | <span class="t">large language model. So now what we're going to do is run it through our REG pipeline and see what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1010" target="_blank">00:16:50.480</a></span> | <span class="t">we get. Okay. So that was 30 seconds to run. I think maybe the first time that you run the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1017" target="_blank">00:16:57.120</a></span> | <span class="t">it's a little bit slower. But yeah, that was quicker. So we get LLAMA2 is a collection of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1022" target="_blank">00:17:02.240</a></span> | <span class="t">pre-trained fine-tuned large language models. Additionally, they're considered a suitable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1027" target="_blank">00:17:07.360</a></span> | <span class="t">substitute for closed-source models like ChatGT, BARD, and Cloud. They are optimized for dialogue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1033" target="_blank">00:17:13.120</a></span> | <span class="t">and outperform open-source chat models on most benchmarks tested, which I think is the special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1038" target="_blank">00:17:18.480</a></span> | <span class="t">thing about LLAMA2. Cool. Now, let's try some more questions. I'll save that REG example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1046" target="_blank">00:17:26.880</a></span> | <span class="t">It works a lot better. So what safety measures we use in the development of LLAMA2? Just using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1052" target="_blank">00:17:32.640</a></span> | <span class="t">the LLM without retrieval augmentation, we get this. So it just, I don't even know what it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1059" target="_blank">00:17:39.600</a></span> | <span class="t">talking about. It kind of just, it's almost like it's rambling about something. I'm not sure what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1064" target="_blank">00:17:44.080</a></span> | <span class="t">that something is, but yeah, not a good answer. Now, if we look at what we get with retrieval</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1069" target="_blank">00:17:49.600</a></span> | <span class="t">augmentation, we get the development of LLAMA2 included safety measures, such as pre-training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1074" target="_blank">00:17:54.480</a></span> | <span class="t">fine-tuning, and model safety approaches. The release of the 34 billion parameter model was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1080" target="_blank">00:18:00.320</a></span> | <span class="t">delayed because they didn't have time to red team. That's a pretty good answer, but let's ask a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1087" target="_blank">00:18:07.680</a></span> | <span class="t">little more about the red teaming procedures. I'm not going to bother asking the LLM because it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1093" target="_blank">00:18:13.760</a></span> | <span class="t">clearly isn't capable of giving us good answers here. So let's just go straight for the retrieval</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1099" target="_blank">00:18:19.200</a></span> | <span class="t">augmented pipeline. So we asked what are the red teaming procedures for LLAMA2 and it describes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1108" target="_blank">00:18:28.000</a></span> | <span class="t">okay, red teaming procedures used for LLAMA2 included creating prompts that might elicit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1113" target="_blank">00:18:33.280</a></span> | <span class="t">unsafe or undesirable responses from the model, such as sensitive topics or prompts that could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1120" target="_blank">00:18:40.400</a></span> | <span class="t">cause harm if the model was spun inappropriately. These exercises were performed by a set of experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1127" target="_blank">00:18:47.360</a></span> | <span class="t">and it also notes that the paper mentions that multiple additional rounds of red team</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1132" target="_blank">00:18:52.400</a></span> | <span class="t">were performed over several months to ensure the robustness of the model. Cool. Now, let's ask one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1139" target="_blank">00:18:59.920</a></span> | <span class="t">more final question. How does the performance of LLAMA2 compare to other local LLMs? The performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1145" target="_blank">00:19:05.600</a></span> | <span class="t">of LLAMA2 is compared to other local LLMs such as Chinchilla and Bard in the paper, although I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1150" target="_blank">00:19:10.160</a></span> | <span class="t">wouldn't call Bard a local LLM. Fine. Specifically, the authors report that LLAMA2 outperforms the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1158" target="_blank">00:19:18.480</a></span> | <span class="t">other models on the series of helpfulness and safety benchmarks that they tested. LLAMA2 appears</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1163" target="_blank">00:19:23.760</a></span> | <span class="t">to be on par with some of the closed source models, at least on the human evaluations they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1168" target="_blank">00:19:28.080</a></span> | <span class="t">performed. So that would be models like GPT 3.5, which is, seems a little bit better than LLAMA2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1176" target="_blank">00:19:36.080</a></span> | <span class="t">but not by that much. Except for my coding stuff. Coding stuff, LLAMA2 is pretty terrible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1181" target="_blank">00:19:41.680</a></span> | <span class="t">Everything else, it seems pretty good. Now, yeah, that's the example. We can see very clearly that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1190" target="_blank">00:19:50.880</a></span> | <span class="t">retrieval augmentation works a lot better than without retrieval augmentation. That's why this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1197" target="_blank">00:19:57.440</a></span> | <span class="t">sort of technique is super powerful. It means your LLM can answer questions about more up-to-date</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1204" target="_blank">00:20:04.720</a></span> | <span class="t">topics, which it can't otherwise. It means it can answer questions about, like if you have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1210" target="_blank">00:20:10.880</a></span> | <span class="t">maybe you work in an organization, you have internal documents, it means it can answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1215" target="_blank">00:20:15.360</a></span> | <span class="t">questions about that. So overall, retrieval augmentation in most cases is really useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1223" target="_blank">00:20:23.760</a></span> | <span class="t">Now that's it for this video. I hope this has been useful and interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1229" target="_blank">00:20:29.520</a></span> | <span class="t">So thank you very much for watching and I will see you again in the next one. Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ypzmPwLH_Q4&t=1234" target="_blank">00:20:34.320</a></span> | <span class="t">[Music]</span></div></div></body></html>