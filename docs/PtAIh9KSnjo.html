<html><head><title>Deep Reinforcement Learning (John Schulman, OpenAI)</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }
        .container {
            width: 80%;
            margin: auto;
            overflow: hidden;
        }
        h2, h3 {
            color: #333;
            text-align: center;
        }
        a {
            color: #0000FF;  /* Traditional blue color for links */
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        img {
            display: block;
            margin: auto;
            max-width: 100%;
        }
        .c {
            margin: 10px 0;
        }
        .s, .t {
            display: inline-block;
            margin-right: 5px;
        }
        .max-width {
            max-width: 800px;
            margin: auto;
        }
    </style>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Deep Reinforcement Learning (John Schulman, OpenAI)</h2><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo"><img src="https://i.ytimg.com/vi_webp/PtAIh9KSnjo/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=48">0:48</a> What is Reinforcement Learning?<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=184">3:4</a> Business Operations<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=394">6:34</a> How Does RL Relate to Other ML Problems?<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=607">10:7</a> How Does RL Relate to Other Machine Learning Problems?<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=858">14:18</a> Recent Success Stories in Deep RL<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1257">20:57</a> Episodic Setting<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1316">21:56</a> Parameterized Policies<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1425">23:45</a> Policy Gradient Methods: Overview<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1685">28:5</a> Score Function Gradient Estimator: Intuition<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1893">31:33</a> Score Function Gradient Estimator for Policies Now random wariable is a whole trajectory<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2101">35:1</a> Policy Gradient: Use Temporal Structure<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2221">37:1</a> Policy Gradient: Introduce Baseline<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2344">39:4</a> Discounts for Variance Reduction<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2674">44:34</a> Extension: Step Sizes and Trust Regions<br><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3472">57:52</a> Value Functions<br><br><div style="text-align: left;"><a href="./PtAIh9KSnjo.html">Whisper Transcript</a> | <a href="./transcript_PtAIh9KSnjo.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">So good morning, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=6" target="_blank">00:00:06.080</a></span> | <span class="t">So I'm going to talk about some of the core methods in deep reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=12" target="_blank">00:00:12.740</a></span> | <span class="t">So the aim of this talk is as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=16" target="_blank">00:00:16.120</a></span> | <span class="t">First I'll do a brief introduction to what deep RL is and whether it might make sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=21" target="_blank">00:00:21.100</a></span> | <span class="t">to apply it in your problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=24" target="_blank">00:00:24.280</a></span> | <span class="t">I'll talk about some of the core techniques.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=28" target="_blank">00:00:28.940</a></span> | <span class="t">So on the one hand we have the policy gradient methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=33" target="_blank">00:00:33.240</a></span> | <span class="t">Then on the other hand we have methods that learn a Q function, including Q-learning and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=38" target="_blank">00:00:38.500</a></span> | <span class="t">SARSA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=40" target="_blank">00:00:40.960</a></span> | <span class="t">And I'll talk a little at the end about what are the pros and cons of these different methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=48" target="_blank">00:00:48.240</a></span> | <span class="t">So first, what is reinforcement learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=51" target="_blank">00:00:51.300</a></span> | <span class="t">It's a branch of machine learning concerned with taking sequences of actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=55" target="_blank">00:00:55.740</a></span> | <span class="t">So often it's described in terms of an agent interacting with a previously unknown environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=64" target="_blank">00:01:04.340</a></span> | <span class="t">And it's trying to maximize some kind of cumulative reward, some kind of reward function that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=69" target="_blank">00:01:09.240</a></span> | <span class="t">we've defined, accumulated over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=72" target="_blank">00:01:12.340</a></span> | <span class="t">And pretty much any kind of task where you have some kind of goal that you want to achieve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=76" target="_blank">00:01:16.440</a></span> | <span class="t">can be stated in these terms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=78" target="_blank">00:01:18.740</a></span> | <span class="t">So this is an extremely general formulation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=85" target="_blank">00:01:25.060</a></span> | <span class="t">What's deep reinforcement learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=86" target="_blank">00:01:26.540</a></span> | <span class="t">It's pretty simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=87" target="_blank">00:01:27.700</a></span> | <span class="t">It's just reinforcement learning where you're using neural networks as function approximators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=94" target="_blank">00:01:34.820</a></span> | <span class="t">So the interesting thing about reinforcement learning in contrast to supervised learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=99" target="_blank">00:01:39.780</a></span> | <span class="t">is it's actually not totally obvious what you should use your neural network to approximate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=104" target="_blank">00:01:44.660</a></span> | <span class="t">in reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=106" target="_blank">00:01:46.140</a></span> | <span class="t">And there are different kinds of algorithms that approximate different things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=109" target="_blank">00:01:49.300</a></span> | <span class="t">So one choice is to use the neural network to approximate your policy, which is how the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=114" target="_blank">00:01:54.460</a></span> | <span class="t">agent chooses its actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=117" target="_blank">00:01:57.020</a></span> | <span class="t">Another choice is to approximate the value functions, which measure how good or bad different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=122" target="_blank">00:02:02.060</a></span> | <span class="t">states are or actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=125" target="_blank">00:02:05.300</a></span> | <span class="t">And last, you can try to learn a model of the system, a dynamics model, which will make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=132" target="_blank">00:02:12.500</a></span> | <span class="t">predictions about next states and rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=136" target="_blank">00:02:16.060</a></span> | <span class="t">Okay, so I'll now give a few examples of different places where you might apply reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=143" target="_blank">00:02:23.900</a></span> | <span class="t">learning and what the observations and actions would be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=148" target="_blank">00:02:28.660</a></span> | <span class="t">So one example is robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=152" target="_blank">00:02:32.020</a></span> | <span class="t">So here you could imagine a robot where the observations are the camera images and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=157" target="_blank">00:02:37.180</a></span> | <span class="t">joint angles of the robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=159" target="_blank">00:02:39.180</a></span> | <span class="t">The actions are the joint torques you're applying.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=162" target="_blank">00:02:42.580</a></span> | <span class="t">And the reward is going to depend on what you want the robot to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=168" target="_blank">00:02:48.500</a></span> | <span class="t">So this is something we, as the algorithm designer, get to define.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=173" target="_blank">00:02:53.220</a></span> | <span class="t">So the rewards could be to stay balanced, to navigate to some target location, or something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=179" target="_blank">00:02:59.580</a></span> | <span class="t">more abstract like serve and protect humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=184" target="_blank">00:03:04.060</a></span> | <span class="t">So reinforcement learning has also been used in a lot of more practical applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=190" target="_blank">00:03:10.980</a></span> | <span class="t">Well, applications that have been practical in the past.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=194" target="_blank">00:03:14.460</a></span> | <span class="t">I think robotics will be very practical in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=198" target="_blank">00:03:18.860</a></span> | <span class="t">And for example, one area is inventory management.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=205" target="_blank">00:03:25.460</a></span> | <span class="t">So this is just one example of how you could use reinforcement learning for a decision-making</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=209" target="_blank">00:03:29.660</a></span> | <span class="t">problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=210" target="_blank">00:03:30.740</a></span> | <span class="t">So you have to decide how much to stock up on of every item.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=217" target="_blank">00:03:37.020</a></span> | <span class="t">And your observations would be your current inventory levels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=220" target="_blank">00:03:40.780</a></span> | <span class="t">Actions would be how much of each item you're going to purchase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=223" target="_blank">00:03:43.860</a></span> | <span class="t">And reward is your profit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=227" target="_blank">00:03:47.780</a></span> | <span class="t">So people in operations research, this is a subfield, study this kind of problem a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=238" target="_blank">00:03:58.580</a></span> | <span class="t">There are also a lot of machine learning problems where people have started to apply reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=243" target="_blank">00:04:03.100</a></span> | <span class="t">learning techniques.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=246" target="_blank">00:04:06.220</a></span> | <span class="t">So one example is attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=250" target="_blank">00:04:10.020</a></span> | <span class="t">So the idea in attention is you don't want to look at the whole input at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=253" target="_blank">00:04:13.380</a></span> | <span class="t">You want to just focus on part of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=255" target="_blank">00:04:15.620</a></span> | <span class="t">So one example of this is with a large image, you might want to just crop out part of it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=261" target="_blank">00:04:21.500</a></span> | <span class="t">and use that and just do detection on that part of the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=265" target="_blank">00:04:25.800</a></span> | <span class="t">So here, your observation would be your current image window.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=270" target="_blank">00:04:30.500</a></span> | <span class="t">Action is where to look or where to crop your image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=273" target="_blank">00:04:33.780</a></span> | <span class="t">And reward is whether you make a classification error or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=279" target="_blank">00:04:39.900</a></span> | <span class="t">So here, you have to try to choose the right area of the image to look at so you'll do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=287" target="_blank">00:04:47.740</a></span> | <span class="t">the correct classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=291" target="_blank">00:04:51.700</a></span> | <span class="t">Reinforcement learning has also been used in structured prediction problems, which in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=298" target="_blank">00:04:58.700</a></span> | <span class="t">the past often weren't considered to be reinforcement learning problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=303" target="_blank">00:05:03.340</a></span> | <span class="t">But it turns out that to actually properly solve them, it actually is a reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=309" target="_blank">00:05:09.620</a></span> | <span class="t">learning problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=310" target="_blank">00:05:10.980</a></span> | <span class="t">So machine translation, for example, so you get a sentence in the source language and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=318" target="_blank">00:05:18.660</a></span> | <span class="t">you have to omit a sentence in the target language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=322" target="_blank">00:05:22.180</a></span> | <span class="t">And here, your observations are the sentence in the source language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=327" target="_blank">00:05:27.540</a></span> | <span class="t">You're omitting one word at a time in the target language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=330" target="_blank">00:05:30.620</a></span> | <span class="t">And you have some reward function that looks at the whole sentence and tells you how good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=335" target="_blank">00:05:35.180</a></span> | <span class="t">your translation was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=337" target="_blank">00:05:37.020</a></span> | <span class="t">So since this is non-differentiable, you can't just differentiate through the whole thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=343" target="_blank">00:05:43.380</a></span> | <span class="t">and do gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=344" target="_blank">00:05:44.500</a></span> | <span class="t">So it turns out you can use a policy gradient method to optimize your translation system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=351" target="_blank">00:05:51.680</a></span> | <span class="t">So people have started to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=356" target="_blank">00:05:56.060</a></span> | <span class="t">So those are just a few examples, not exhaustive at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=362" target="_blank">00:06:02.180</a></span> | <span class="t">But I just want to say a little bit about how reinforcement learning fits into the picture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=371" target="_blank">00:06:11.980</a></span> | <span class="t">of all the other types of machine learning problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=375" target="_blank">00:06:15.860</a></span> | <span class="t">So previous courses in this series have talked about supervised learning and unsupervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=383" target="_blank">00:06:23.660</a></span> | <span class="t">learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=384" target="_blank">00:06:24.660</a></span> | <span class="t">So how does reinforcement learning relate to them?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=387" target="_blank">00:06:27.740</a></span> | <span class="t">How is it different?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=389" target="_blank">00:06:29.260</a></span> | <span class="t">So let's just first compare it to-- let's look at supervised learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=393" target="_blank">00:06:33.900</a></span> | <span class="t">So in supervised learning, first, the environment samples an input-output pair from some distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=399" target="_blank">00:06:39.900</a></span> | <span class="t">row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=402" target="_blank">00:06:42.540</a></span> | <span class="t">The agent makes a prediction, y hat, using its function f.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=408" target="_blank">00:06:48.980</a></span> | <span class="t">And it receives some loss, which tells it if it made the right prediction or the wrong</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=413" target="_blank">00:06:53.660</a></span> | <span class="t">prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=415" target="_blank">00:06:55.300</a></span> | <span class="t">So the interpretation is environment asks the agent a question and then tells her the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=419" target="_blank">00:06:59.700</a></span> | <span class="t">right answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=423" target="_blank">00:07:03.180</a></span> | <span class="t">So contextual bandits make this problem a little harder in that they give the learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=429" target="_blank">00:07:09.100</a></span> | <span class="t">agent a little bit less information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=431" target="_blank">00:07:11.700</a></span> | <span class="t">So now the environment samples an input, but notice that there's not a correct output associated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=437" target="_blank">00:07:17.340</a></span> | <span class="t">with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=439" target="_blank">00:07:19.180</a></span> | <span class="t">Then the agent takes an action, and the agent receives some cost, which is from some probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=447" target="_blank">00:07:27.860</a></span> | <span class="t">distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=448" target="_blank">00:07:28.860</a></span> | <span class="t">So here, c is the cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=450" target="_blank">00:07:30.940</a></span> | <span class="t">We're sampling it from some probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=454" target="_blank">00:07:34.180</a></span> | <span class="t">And the agent doesn't know what this probability distribution is, so that's what makes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=457" target="_blank">00:07:37.220</a></span> | <span class="t">problem hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=460" target="_blank">00:07:40.700</a></span> | <span class="t">So environment asks the agent a question, the agent answers, and the environment gives her</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=465" target="_blank">00:07:45.740</a></span> | <span class="t">a noisy score on the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=470" target="_blank">00:07:50.780</a></span> | <span class="t">So this is applied.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=472" target="_blank">00:07:52.740</a></span> | <span class="t">This actually has a lot of applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=474" target="_blank">00:07:54.320</a></span> | <span class="t">So personalized recommendations is one big one, along with advertising.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=478" target="_blank">00:07:58.220</a></span> | <span class="t">So you have to decide, like, customers who like this -- I mean, you have a customer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=485" target="_blank">00:08:05.460</a></span> | <span class="t">and you know what they liked in the past, so you have to make a prediction about what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=488" target="_blank">00:08:08.940</a></span> | <span class="t">they're going to like in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=490" target="_blank">00:08:10.620</a></span> | <span class="t">So you show them appropriate ads or links, like what book you want to try to advertise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=497" target="_blank">00:08:17.580</a></span> | <span class="t">to them, or what video you want to show them, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=502" target="_blank">00:08:22.920</a></span> | <span class="t">So here, the big difference between this and the supervised learning setting is you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=506" target="_blank">00:08:26.500</a></span> | <span class="t">have access to the function, the loss function you're trying to optimize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=510" target="_blank">00:08:30.320</a></span> | <span class="t">So in particular, you can't differentiate through it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=513" target="_blank">00:08:33.100</a></span> | <span class="t">We don't know the process that generates c, so we can't compute the grading of the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=517" target="_blank">00:08:37.740</a></span> | <span class="t">function and use that to tune the agent's parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=521" target="_blank">00:08:41.580</a></span> | <span class="t">So that makes it -- so that makes the problem a bit harder, where you have to use a different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=526" target="_blank">00:08:46.860</a></span> | <span class="t">kind of algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=528" target="_blank">00:08:48.860</a></span> | <span class="t">Lastly, reinforcement learning is almost the same as the contextual bandit setting, except</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=534" target="_blank">00:08:54.860</a></span> | <span class="t">now the environment is stateful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=537" target="_blank">00:08:57.940</a></span> | <span class="t">So now, instead of sampling the initial state from scratch every time step, from the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=543" target="_blank">00:09:03.980</a></span> | <span class="t">distribution, the state evolves over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=549" target="_blank">00:09:09.020</a></span> | <span class="t">So you have some transition probability distribution called p here, where the state x sub t is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=556" target="_blank">00:09:16.100</a></span> | <span class="t">conditioned on the previous state and the previous action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=562" target="_blank">00:09:22.500</a></span> | <span class="t">And that makes the problem quite a bit harder, because now -- well, for a number of reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=567" target="_blank">00:09:27.920</a></span> | <span class="t">For one thing, the inputs you're getting depend on what actions you're taking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=572" target="_blank">00:09:32.120</a></span> | <span class="t">So now that makes it harder to develop a stable, reliable algorithm, because now as the agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=577" target="_blank">00:09:37.320</a></span> | <span class="t">starts to learn, it gets different inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=580" target="_blank">00:09:40.280</a></span> | <span class="t">So that can lead to all sorts of out-of-control behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=586" target="_blank">00:09:46.380</a></span> | <span class="t">And it also means you have delayed effects, because since the system is stateful, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=592" target="_blank">00:09:52.280</a></span> | <span class="t">might need to take a lot of actions to get into the right state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=595" target="_blank">00:09:55.600</a></span> | <span class="t">So you might need to -- you can't just act greedily every time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=600" target="_blank">00:10:00.900</a></span> | <span class="t">You have to think ahead, effectively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=606" target="_blank">00:10:06.180</a></span> | <span class="t">Okay, so just to summarize these differences, there are two differences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=611" target="_blank">00:10:11.160</a></span> | <span class="t">The first one is, you don't have full analytic access to the function you're trying to optimize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=615" target="_blank">00:10:15.700</a></span> | <span class="t">You have to query it through interaction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=618" target="_blank">00:10:18.700</a></span> | <span class="t">Second, you're interacting with a stateful world, which means that the input you get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=623" target="_blank">00:10:23.900</a></span> | <span class="t">is going to depend on your previous actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=626" target="_blank">00:10:26.620</a></span> | <span class="t">And if you just take the first of those differences between supervised learning and reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=631" target="_blank">00:10:31.620</a></span> | <span class="t">learning, you get the contextual bandit setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=633" target="_blank">00:10:33.940</a></span> | <span class="t">So that's sort of halfway in between.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=636" target="_blank">00:10:36.740</a></span> | <span class="t">Okay, so I realize that there are multiple -- this audience probably has people with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=643" target="_blank">00:10:43.260</a></span> | <span class="t">different interests.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=644" target="_blank">00:10:44.820</a></span> | <span class="t">Some people are doing research and want to know about what's the latest in the research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=649" target="_blank">00:10:49.500</a></span> | <span class="t">world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=650" target="_blank">00:10:50.540</a></span> | <span class="t">And some people are -- want to apply these machine learning techniques to practical applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=656" target="_blank">00:10:56.420</a></span> | <span class="t">So this slide is for the latter group of people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=660" target="_blank">00:11:00.580</a></span> | <span class="t">So if you're wondering -- if you have some problem where you think reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=665" target="_blank">00:11:05.260</a></span> | <span class="t">might be relevant and you're wondering if you should apply reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=670" target="_blank">00:11:10.020</a></span> | <span class="t">So first, I should say that the answer might be no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=674" target="_blank">00:11:14.880</a></span> | <span class="t">It might be overkill, especially deep reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=678" target="_blank">00:11:18.360</a></span> | <span class="t">So this is a set of fairly new techniques where it's not going to work out of the box</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=682" target="_blank">00:11:22.140</a></span> | <span class="t">very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=684" target="_blank">00:11:24.360</a></span> | <span class="t">And it's -- these techniques aren't that well established.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=687" target="_blank">00:11:27.920</a></span> | <span class="t">So they require a lot of -- they have a lot of knobs to be tuned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=690" target="_blank">00:11:30.620</a></span> | <span class="t">So it might be overkill and, yeah, these techniques aren't that well established at the moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=697" target="_blank">00:11:37.180</a></span> | <span class="t">So it might be worth investigating some other methods first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=702" target="_blank">00:11:42.180</a></span> | <span class="t">So one -- so if your problem has a small number of parameters you're trying to optimize over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=708" target="_blank">00:11:48.800</a></span> | <span class="t">and you have a simulator that you can, like, just do lots of experiments on, then derivative-free</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=716" target="_blank">00:11:56.940</a></span> | <span class="t">optimization methods are likely to be better than reinforcement learning, or they're likely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=721" target="_blank">00:12:01.860</a></span> | <span class="t">to be easier to get working.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=724" target="_blank">00:12:04.240</a></span> | <span class="t">So these methods just look at -- they just -- you give them a black box function where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=729" target="_blank">00:12:09.480</a></span> | <span class="t">you put in a parameter vector and it'll give you a noisy estimate of the score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=734" target="_blank">00:12:14.540</a></span> | <span class="t">And these algorithms will just optimize over the parameters of that black box -- I mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=739" target="_blank">00:12:19.260</a></span> | <span class="t">that are being put into that black box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=742" target="_blank">00:12:22.180</a></span> | <span class="t">So yeah, there's a variety of different methods for derivative-free optimization, but these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=747" target="_blank">00:12:27.980</a></span> | <span class="t">are easier to understand than reinforcement learning, and they do kind of work out of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=751" target="_blank">00:12:31.980</a></span> | <span class="t">the box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=753" target="_blank">00:12:33.980</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=755" target="_blank">00:12:35.660</a></span> | <span class="t">A lot of problems are actually -- can be seen as -- are contextual bandit problems, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=761" target="_blank">00:12:41.580</a></span> | <span class="t">the statefulness of the world isn't that relevant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=764" target="_blank">00:12:44.420</a></span> | <span class="t">So for example, in advertising, this is where people -- people look at advertising as a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=770" target="_blank">00:12:50.260</a></span> | <span class="t">contextual bandit problem most of the time, because you decide what ad to present the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=774" target="_blank">00:12:54.420</a></span> | <span class="t">user with, and then they either click on it or they don't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=780" target="_blank">00:13:00.220</a></span> | <span class="t">But it's really -- the user is kind of stateful, because if you show them a terrible ad, they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=784" target="_blank">00:13:04.980</a></span> | <span class="t">might just go and download ad block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=787" target="_blank">00:13:07.460</a></span> | <span class="t">So there is -- like, your actions do have some repercussions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=792" target="_blank">00:13:12.760</a></span> | <span class="t">But often you can just approximate it as being a contextual bandit problem where there is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=797" target="_blank">00:13:17.300</a></span> | <span class="t">no state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=798" target="_blank">00:13:18.640</a></span> | <span class="t">So there's a better theoretical understanding of contextual bandit problems and methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=804" target="_blank">00:13:24.300</a></span> | <span class="t">that are -- that have some guarantees.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=806" target="_blank">00:13:26.420</a></span> | <span class="t">So in that case -- so if it is a contextual bandit problem, you might want to use those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=811" target="_blank">00:13:31.740</a></span> | <span class="t">kind of algorithms instead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=814" target="_blank">00:13:34.100</a></span> | <span class="t">And lastly, the operations research field has been using these methods for a while on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=823" target="_blank">00:13:43.000</a></span> | <span class="t">real problems, and they have a set of methods which are just pretty much the basic algorithms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=831" target="_blank">00:13:51.480</a></span> | <span class="t">policy iteration and value iteration, but they're sort of well -- they're well-developed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=837" target="_blank">00:13:57.320</a></span> | <span class="t">ways of doing feature engineering for these problems that end up working pretty decently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=841" target="_blank">00:14:01.560</a></span> | <span class="t">So these techniques are also worth considering instead of trying to throw a big neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=846" target="_blank">00:14:06.240</a></span> | <span class="t">at it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=848" target="_blank">00:14:08.560</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=851" target="_blank">00:14:11.300</a></span> | <span class="t">So now -- well, now that I've talked about what -- why not to use deep reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=855" target="_blank">00:14:15.200</a></span> | <span class="t">learning or what it's not good for, I'll just talk about some recent success stories in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=861" target="_blank">00:14:21.200</a></span> | <span class="t">deep reinforcement learning, which are achievements that probably wouldn't have been possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=865" target="_blank">00:14:25.520</a></span> | <span class="t">using these other techniques.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=867" target="_blank">00:14:27.920</a></span> | <span class="t">So a few years ago, there was a pretty influential result by Mni et al. from DeepMind where they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=877" target="_blank">00:14:37.640</a></span> | <span class="t">used a deep Q-learning algorithm to play Atari games using the screen images as input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=887" target="_blank">00:14:47.800</a></span> | <span class="t">And that's hard because you have these -- these games are -- you're trying to do different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=891" target="_blank">00:14:51.920</a></span> | <span class="t">things in all these games, and they're -- some of them are kind of complicated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=894" target="_blank">00:14:54.620</a></span> | <span class="t">So it's pretty remarkable that you can just use a simple -- that a simple algorithm can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=899" target="_blank">00:14:59.760</a></span> | <span class="t">solve them all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=901" target="_blank">00:15:01.880</a></span> | <span class="t">The same algorithm can solve them all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=904" target="_blank">00:15:04.260</a></span> | <span class="t">So since then, people have also solved -- or solved this domain using policy gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=911" target="_blank">00:15:11.760</a></span> | <span class="t">and another algorithm called Dagger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=915" target="_blank">00:15:15.640</a></span> | <span class="t">So another big groundbreaking result was beating a champion-level player at Go, also by DeepMind,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=926" target="_blank">00:15:26.500</a></span> | <span class="t">using a combination of supervised learning from, like, from expert games, plus policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=933" target="_blank">00:15:33.540</a></span> | <span class="t">gradients to fine-tune the supervised learning policy, plus Monte Carlo tree search, plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=941" target="_blank">00:15:41.220</a></span> | <span class="t">value functions to make the search work better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=944" target="_blank">00:15:44.340</a></span> | <span class="t">So a combination of techniques and reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=948" target="_blank">00:15:48.040</a></span> | <span class="t">Robotic -- so some of my colleagues at Berkeley had some very nice results learning in real</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=956" target="_blank">00:15:56.080</a></span> | <span class="t">time how to do manipulation tasks using an algorithm called guided policy search using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=963" target="_blank">00:16:03.160</a></span> | <span class="t">the PR2 robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=966" target="_blank">00:16:06.920</a></span> | <span class="t">And some of my colleagues and I have been working on robotic locomotion using policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=973" target="_blank">00:16:13.160</a></span> | <span class="t">gradient methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=977" target="_blank">00:16:17.080</a></span> | <span class="t">And people have been working on locomotion for a while and have been able to achieve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=981" target="_blank">00:16:21.720</a></span> | <span class="t">pretty good results using very, like, highly engineered domain-specific methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=987" target="_blank">00:16:27.320</a></span> | <span class="t">But previously, there hadn't been much success using general methods to solve it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=994" target="_blank">00:16:34.640</a></span> | <span class="t">And last, there have been some recent results playing 3D games using policy gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1000" target="_blank">00:16:40.800</a></span> | <span class="t">In fact, there was even a contest I heard about a couple days ago with this new VisDoom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1005" target="_blank">00:16:45.320</a></span> | <span class="t">task, which is pretty nice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1009" target="_blank">00:16:49.600</a></span> | <span class="t">So you might want to check out VisDoom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1012" target="_blank">00:16:52.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1014" target="_blank">00:16:54.160</a></span> | <span class="t">So that's it for the high-level overview part of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1018" target="_blank">00:16:58.680</a></span> | <span class="t">Now I'm going to start getting into the actual formalism and the technical details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1024" target="_blank">00:17:04.640</a></span> | <span class="t">Okay, so the basic object in the field of reinforcement learning is the Markov decision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1033" target="_blank">00:17:13.600</a></span> | <span class="t">process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1035" target="_blank">00:17:15.120</a></span> | <span class="t">So the Markov decision process is defined by the following components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1040" target="_blank">00:17:20.200</a></span> | <span class="t">You have a state space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1041" target="_blank">00:17:21.740</a></span> | <span class="t">This is all the different states of the system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1045" target="_blank">00:17:25.040</a></span> | <span class="t">The action space, these are all the actions the agent can take.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1048" target="_blank">00:17:28.600</a></span> | <span class="t">And you have this probability distribution, which determines the probability of next state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1056" target="_blank">00:17:36.120</a></span> | <span class="t">and reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1057" target="_blank">00:17:37.120</a></span> | <span class="t">So R is the reward, S prime is the next state, S and A are the actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1062" target="_blank">00:17:42.200</a></span> | <span class="t">So it's a conditional probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1064" target="_blank">00:17:44.200</a></span> | <span class="t">Sometimes people split this out into a separate reward function, but that's basically an equivalent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1069" target="_blank">00:17:49.440</a></span> | <span class="t">formulation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1070" target="_blank">00:17:50.960</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1072" target="_blank">00:17:52.300</a></span> | <span class="t">And sometimes there's some extra objects to find.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1077" target="_blank">00:17:57.900</a></span> | <span class="t">We'll be interested in the...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1079" target="_blank">00:17:59.440</a></span> | <span class="t">We'll consider an initial state distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1082" target="_blank">00:18:02.020</a></span> | <span class="t">So this is the world starts out in a certain state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1087" target="_blank">00:18:07.180</a></span> | <span class="t">And the typical optimization problem you want to solve given this MDP is to maximize expected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1092" target="_blank">00:18:12.780</a></span> | <span class="t">cumulative reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1094" target="_blank">00:18:14.380</a></span> | <span class="t">So there are various ways of defining that more precisely, which I'll go into later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1101" target="_blank">00:18:21.540</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1104" target="_blank">00:18:24.260</a></span> | <span class="t">So there are various different settings of reinforcement learning where you define a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1109" target="_blank">00:18:29.420</a></span> | <span class="t">slightly different optimization problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1111" target="_blank">00:18:31.660</a></span> | <span class="t">The one we'll be most concerned with is called the episodic setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1115" target="_blank">00:18:35.600</a></span> | <span class="t">So here the agent's experience is split up into a series of episodes which have finite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1122" target="_blank">00:18:42.740</a></span> | <span class="t">length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1124" target="_blank">00:18:44.200</a></span> | <span class="t">So in each episode, we first sample the initial state of the world from some probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1130" target="_blank">00:18:50.500</a></span> | <span class="t">mu.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1132" target="_blank">00:18:52.060</a></span> | <span class="t">And then the agent keeps on acting until the world ends up in some terminal state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1140" target="_blank">00:19:00.880</a></span> | <span class="t">So just to give an example of what terminal states might be like and how an episodic reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1147" target="_blank">00:19:07.740</a></span> | <span class="t">learning problem might look.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1149" target="_blank">00:19:09.780</a></span> | <span class="t">So one example is when termination is good and you want to terminate the episode as fast</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1155" target="_blank">00:19:15.420</a></span> | <span class="t">as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1156" target="_blank">00:19:16.820</a></span> | <span class="t">So if we imagine setting up a task with some kind of taxi robot that should get to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1161" target="_blank">00:19:21.420</a></span> | <span class="t">destination as fast as possible, then the episode would be like one trip and it's trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1168" target="_blank">00:19:28.360</a></span> | <span class="t">to terminate the episode as fast as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1173" target="_blank">00:19:33.300</a></span> | <span class="t">Another example is a waiter robot where you have a fixed length shift, but the waiter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1179" target="_blank">00:19:39.380</a></span> | <span class="t">has to accumulate, it has to do as well as possible during that shift.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1183" target="_blank">00:19:43.200</a></span> | <span class="t">So there the episode has a fixed length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1186" target="_blank">00:19:46.000</a></span> | <span class="t">The waiter has to say maximize tips or customer happiness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1192" target="_blank">00:19:52.840</a></span> | <span class="t">And then you could imagine another kind of task where termination is bad and you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1197" target="_blank">00:19:57.140</a></span> | <span class="t">the episode to last as long as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1200" target="_blank">00:20:00.060</a></span> | <span class="t">So you can view life as an example of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1203" target="_blank">00:20:03.740</a></span> | <span class="t">But also you could imagine having a walking robot where you want it to walk as far as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1210" target="_blank">00:20:10.860</a></span> | <span class="t">possible before it falls over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1215" target="_blank">00:20:15.940</a></span> | <span class="t">And in this setting, it's pretty easy to define what the goal is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1221" target="_blank">00:20:21.620</a></span> | <span class="t">We just want to maximize the expectation of the total reward per episode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1229" target="_blank">00:20:29.820</a></span> | <span class="t">And the last object we're going to introduce here is a policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1234" target="_blank">00:20:34.300</a></span> | <span class="t">So the policy is just the function that the agent uses to choose its actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1240" target="_blank">00:20:40.220</a></span> | <span class="t">So we have deterministic policies, which are just the policy is denoted by pi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1245" target="_blank">00:20:45.380</a></span> | <span class="t">So we have the action is just some function of the state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1248" target="_blank">00:20:48.780</a></span> | <span class="t">And we also have stochastic policies where the policy is a conditional probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1257" target="_blank">00:20:57.180</a></span> | <span class="t">So we're just going to make a little bit more precise the setting of the episodic MDP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1264" target="_blank">00:21:04.920</a></span> | <span class="t">So first we sample the initial state from this distribution, mu.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1271" target="_blank">00:21:11.820</a></span> | <span class="t">Then we sample the first action from the policy, a0 from the policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1276" target="_blank">00:21:16.780</a></span> | <span class="t">Then we sample next state and reward from the transition probability distribution, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1281" target="_blank">00:21:21.460</a></span> | <span class="t">so on, until we reach a terminal state, s sub t.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1285" target="_blank">00:21:25.080</a></span> | <span class="t">And then the quantity we care about is the sum of all these rewards, r0 plus r1 dot dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1291" target="_blank">00:21:31.380</a></span> | <span class="t">dot plus r sub t minus 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1293" target="_blank">00:21:33.940</a></span> | <span class="t">And we want to maximize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1296" target="_blank">00:21:36.560</a></span> | <span class="t">So eta of pi is just defined as the expected total reward of the policy pi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1306" target="_blank">00:21:46.100</a></span> | <span class="t">Here's the picture that illustrates exactly the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1309" target="_blank">00:21:49.340</a></span> | <span class="t">So you can look at it as a graphical model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1316" target="_blank">00:21:56.340</a></span> | <span class="t">And lastly, in the policy gradient section in particular, we're going to be interested</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1321" target="_blank">00:22:01.280</a></span> | <span class="t">in parameterized policies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1323" target="_blank">00:22:03.420</a></span> | <span class="t">So here we have a parameter vector, theta, which specifies exactly what the policy is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1331" target="_blank">00:22:11.020</a></span> | <span class="t">So for example, the family of policies could be just a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1335" target="_blank">00:22:15.760</a></span> | <span class="t">You have a certain neural network architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1338" target="_blank">00:22:18.020</a></span> | <span class="t">And theta specifies all the weights of this neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1344" target="_blank">00:22:24.100</a></span> | <span class="t">So we could have a deterministic policy, of course, or a stochastic policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1350" target="_blank">00:22:30.560</a></span> | <span class="t">And if you're wondering concretely what would a policy look like, I mean, how do you use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1355" target="_blank">00:22:35.620</a></span> | <span class="t">a neural network to represent your policy?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1358" target="_blank">00:22:38.100</a></span> | <span class="t">It's actually exactly, you do exactly the same thing you would do if this were a classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1362" target="_blank">00:22:42.700</a></span> | <span class="t">or a regression problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1364" target="_blank">00:22:44.500</a></span> | <span class="t">So S here, the state here is your input, and the action is your output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1371" target="_blank">00:22:51.580</a></span> | <span class="t">So if you have a discrete action space, a discrete set of actions, then you would use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1378" target="_blank">00:22:58.220</a></span> | <span class="t">a network that outputs a vector of probabilities, the probabilities of the different actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1383" target="_blank">00:23:03.260</a></span> | <span class="t">This is exactly like a classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1386" target="_blank">00:23:06.900</a></span> | <span class="t">And if you have a continuous action space, you would have your neural network output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1391" target="_blank">00:23:11.500</a></span> | <span class="t">the mean and the diagonal of a covariance matrix of a Gaussian distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1397" target="_blank">00:23:17.220</a></span> | <span class="t">So this is just like you're doing regression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1399" target="_blank">00:23:19.780</a></span> | <span class="t">So you can use the same kind of architectures you'd use in supervised learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1405" target="_blank">00:23:25.100</a></span> | <span class="t">Okay, so that's it for the formalism of MDPs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1412" target="_blank">00:23:32.760</a></span> | <span class="t">So now I'm going to go into policy gradient methods, which are one broad and general class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1419" target="_blank">00:23:39.200</a></span> | <span class="t">of reinforcement learning methods, which are quite effective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1425" target="_blank">00:23:45.260</a></span> | <span class="t">So to give a brief overview of this, here's the intuition of what policy gradient methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1432" target="_blank">00:23:52.740</a></span> | <span class="t">are going to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1434" target="_blank">00:23:54.800</a></span> | <span class="t">So here, capital R means the sum of rewards of the whole episode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1439" target="_blank">00:23:59.780</a></span> | <span class="t">So our optimization problem is we want to maximize the expectation of the total reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1445" target="_blank">00:24:05.960</a></span> | <span class="t">given our parameterized policy, pi sub theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1450" target="_blank">00:24:10.680</a></span> | <span class="t">And the intuition of how our algorithm is going to work is we're going to collect a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1455" target="_blank">00:24:15.880</a></span> | <span class="t">bunch of trajectories.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1456" target="_blank">00:24:16.880</a></span> | <span class="t">I mean, this is just run a bunch of episodes using our policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1461" target="_blank">00:24:21.160</a></span> | <span class="t">And then we want to make the good trajectories more probable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1464" target="_blank">00:24:24.320</a></span> | <span class="t">So I mean, some of the trajectories were lucky and they were really good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1468" target="_blank">00:24:28.120</a></span> | <span class="t">Some of them, the agent was unlucky and they were bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1470" target="_blank">00:24:30.980</a></span> | <span class="t">And the ones that were good, meaning there was high reward, that means the agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1475" target="_blank">00:24:35.920</a></span> | <span class="t">probably took good actions there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1478" target="_blank">00:24:38.040</a></span> | <span class="t">So we want to increase the probability of the actions from those trajectories.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1484" target="_blank">00:24:44.260</a></span> | <span class="t">So the most basic version of policy gradient methods just try to make the good trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1488" target="_blank">00:24:48.920</a></span> | <span class="t">more probable without trying to figure out which were the good actions and which were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1492" target="_blank">00:24:52.280</a></span> | <span class="t">the bad actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1494" target="_blank">00:24:54.360</a></span> | <span class="t">Slightly better methods or more elaborate methods try to figure out which actions were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1499" target="_blank">00:24:59.640</a></span> | <span class="t">good and which ones were bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1501" target="_blank">00:25:01.320</a></span> | <span class="t">And then they try to make the good actions more probable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1505" target="_blank">00:25:05.800</a></span> | <span class="t">And lastly, there's another class of methods which actually try to push the actions towards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1511" target="_blank">00:25:11.640</a></span> | <span class="t">better actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1513" target="_blank">00:25:13.160</a></span> | <span class="t">So they differentiate the loss function with respect to the actions and they try to push</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1516" target="_blank">00:25:16.960</a></span> | <span class="t">the actions to better actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1519" target="_blank">00:25:19.880</a></span> | <span class="t">So we're mostly going to talk about one and two here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1523" target="_blank">00:25:23.640</a></span> | <span class="t">Oh, there's a question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1529" target="_blank">00:25:29.920</a></span> | <span class="t">Oh, yeah, good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1534" target="_blank">00:25:34.300</a></span> | <span class="t">So while we're maximizing over the policy, we're trying to find the best policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1541" target="_blank">00:25:41.440</a></span> | <span class="t">But here, the policy is assumed to be parameterized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1544" target="_blank">00:25:44.880</a></span> | <span class="t">So there's some parameter vector theta that specifies the policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1548" target="_blank">00:25:48.220</a></span> | <span class="t">And now we just want to maximize with respect to theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1553" target="_blank">00:25:53.020</a></span> | <span class="t">Any other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1559" target="_blank">00:25:59.000</a></span> | <span class="t">So there's a very fundamental concept, which is called the score function grading estimator,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1566" target="_blank">00:26:06.620</a></span> | <span class="t">which underlies policy gradient methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1570" target="_blank">00:26:10.860</a></span> | <span class="t">So actually, to introduce this, we're not going to talk about policies in RL at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1575" target="_blank">00:26:15.120</a></span> | <span class="t">We're just going to assume we have some expectation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1578" target="_blank">00:26:18.320</a></span> | <span class="t">We have expectation of f of x, where x is sampled from some parameterized probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1584" target="_blank">00:26:24.920</a></span> | <span class="t">distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1587" target="_blank">00:26:27.320</a></span> | <span class="t">So we want to compute the grading of this expectation with respect to theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1592" target="_blank">00:26:32.340</a></span> | <span class="t">So there's a general formula that'll do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1595" target="_blank">00:26:35.440</a></span> | <span class="t">And the way you derive it is you just write the expectation as an integral.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1600" target="_blank">00:26:40.460</a></span> | <span class="t">And then you just move some things around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1604" target="_blank">00:26:44.240</a></span> | <span class="t">You swap the integral with the derivative and you turn it back into an expectation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1610" target="_blank">00:26:50.040</a></span> | <span class="t">And what you get at the end is this bottom line, which says that you take the expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1615" target="_blank">00:26:55.040</a></span> | <span class="t">of function value times grad log probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1620" target="_blank">00:27:00.360</a></span> | <span class="t">So this is an unbiased estimator of the gradient, meaning if we get enough samples, it'll converge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1626" target="_blank">00:27:06.800</a></span> | <span class="t">on the right thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1628" target="_blank">00:27:08.660</a></span> | <span class="t">So the way you can compute this estimator, meaning the way you can get a noisy estimate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1633" target="_blank">00:27:13.780</a></span> | <span class="t">of the gradient of the expectation, is you just get one sample from this distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1642" target="_blank">00:27:22.900</a></span> | <span class="t">And then you multiply f of x times grad log probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1649" target="_blank">00:27:29.820</a></span> | <span class="t">So the only requirement for being able to use this estimator is we need to be able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1655" target="_blank">00:27:35.860</a></span> | <span class="t">compute the probability density.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1658" target="_blank">00:27:38.360</a></span> | <span class="t">We need to be able to analytically compute it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1660" target="_blank">00:27:40.900</a></span> | <span class="t">And we need to be able to differentiate it with respect to theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1664" target="_blank">00:27:44.400</a></span> | <span class="t">And often it needs to be differentiable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1671" target="_blank">00:27:51.100</a></span> | <span class="t">There's another way of deriving it using importance sampling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1674" target="_blank">00:27:54.940</a></span> | <span class="t">So you write down the importance sampling estimator for the expectation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1678" target="_blank">00:27:58.380</a></span> | <span class="t">And then you just swap the derivative with the expectation and you get the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1685" target="_blank">00:28:05.740</a></span> | <span class="t">So now let me just give a little bit of intuition about this estimator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1693" target="_blank">00:28:13.060</a></span> | <span class="t">So f of x is measuring how good the sample x is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1697" target="_blank">00:28:17.500</a></span> | <span class="t">So that means that-- so g hat here is our gradient estimator, meaning this is what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1702" target="_blank">00:28:22.700</a></span> | <span class="t">get if we take one sample x sub i and we compute our estimator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1707" target="_blank">00:28:27.780</a></span> | <span class="t">This is our estimate of the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1710" target="_blank">00:28:30.220</a></span> | <span class="t">So if we move in direction g hat, that pushes up the log probability of our sample x sub</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1717" target="_blank">00:28:37.100</a></span> | <span class="t">i in proportion to how good it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1719" target="_blank">00:28:39.920</a></span> | <span class="t">So if we have really good-- if we got a really good function value, then we're going to try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1724" target="_blank">00:28:44.540</a></span> | <span class="t">to push up its log probability a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1726" target="_blank">00:28:46.940</a></span> | <span class="t">And if it was a bad function value, then we're not going to try to push it up very much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1731" target="_blank">00:28:51.140</a></span> | <span class="t">So pretty simple intuition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1736" target="_blank">00:28:56.300</a></span> | <span class="t">The really nice thing is this is valid even if f of x is discontinuous or if f of x is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1742" target="_blank">00:29:02.780</a></span> | <span class="t">unknown, meaning you only-- you don't get to differentiate it, you just get to see the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1747" target="_blank">00:29:07.300</a></span> | <span class="t">function values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1749" target="_blank">00:29:09.580</a></span> | <span class="t">Or the sample space is a discrete set, so x doesn't even have to be continuous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1755" target="_blank">00:29:15.740</a></span> | <span class="t">And this is quite remarkable, actually, that you don't even need to have access to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1761" target="_blank">00:29:21.780</a></span> | <span class="t">full-- you don't need to know exactly what the function is that you're optimizing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1766" target="_blank">00:29:26.820</a></span> | <span class="t">You just have to be able to query it for the function value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1772" target="_blank">00:29:32.500</a></span> | <span class="t">And this means-- this is a way of being able to differentiate functions through a system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1780" target="_blank">00:29:40.500</a></span> | <span class="t">that has non-differentiable pieces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1782" target="_blank">00:29:42.960</a></span> | <span class="t">So for example, in robotic locomotion, one issue is that you have contacts between the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1789" target="_blank">00:29:49.620</a></span> | <span class="t">robot's foot and the ground.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1791" target="_blank">00:29:51.540</a></span> | <span class="t">And you make and break contact, and that causes a discontinuous change in the dynamics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1798" target="_blank">00:29:58.720</a></span> | <span class="t">So that makes it really hard to do smooth optimization techniques to come up with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1802" target="_blank">00:30:02.500</a></span> | <span class="t">right behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1803" target="_blank">00:30:03.820</a></span> | <span class="t">So when you use this kind of gradient estimator, along with policy gradients, which I'm going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1808" target="_blank">00:30:08.460</a></span> | <span class="t">to talk about very soon, you can actually just differentiate-- you can optimize the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1815" target="_blank">00:30:15.220</a></span> | <span class="t">system even though it has differentiable pieces in it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1822" target="_blank">00:30:22.340</a></span> | <span class="t">So here's another little picture of what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1827" target="_blank">00:30:27.900</a></span> | <span class="t">So we have our function f of x, which we're trying to maximize the expectation of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1833" target="_blank">00:30:33.980</a></span> | <span class="t">And then we have our probability density, p of x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1837" target="_blank">00:30:37.220</a></span> | <span class="t">So we just sample a bunch of values from our probability density.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1840" target="_blank">00:30:40.840</a></span> | <span class="t">Those are the blue dots on the x-axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1843" target="_blank">00:30:43.320</a></span> | <span class="t">And then we look at the function values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1850" target="_blank">00:30:50.660</a></span> | <span class="t">And we're trying to push the probability distribution so that the probability goes up at these samples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1858" target="_blank">00:30:58.300</a></span> | <span class="t">in proportion to the function value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1861" target="_blank">00:31:01.020</a></span> | <span class="t">So over on the right side of the curve, that means we're trying to push that probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1866" target="_blank">00:31:06.900</a></span> | <span class="t">value up really hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1868" target="_blank">00:31:08.540</a></span> | <span class="t">And on the left side, we're pushing it up softly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1871" target="_blank">00:31:11.160</a></span> | <span class="t">So what's going to happen is the probability density is going to slide to the right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1875" target="_blank">00:31:15.340</a></span> | <span class="t">If you can imagine a sort of physical analogy there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1880" target="_blank">00:31:20.260</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1881" target="_blank">00:31:21.900</a></span> | <span class="t">So that's the score function gradient estimator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1884" target="_blank">00:31:24.140</a></span> | <span class="t">This is a general technique.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1886" target="_blank">00:31:26.300</a></span> | <span class="t">It can be used in various machine learning problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1892" target="_blank">00:31:32.500</a></span> | <span class="t">Now we're going to apply it to the reinforcement learning setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1896" target="_blank">00:31:36.580</a></span> | <span class="t">And we're going to take our random variable x to be a whole trajectory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1902" target="_blank">00:31:42.960</a></span> | <span class="t">So the trajectory consists of state action reward, state action reward, and so on until</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1907" target="_blank">00:31:47.220</a></span> | <span class="t">the end of the episode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1910" target="_blank">00:31:50.180</a></span> | <span class="t">And now to get our gradient estimator, to get the gradient of the expected reward, all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1919" target="_blank">00:31:59.180</a></span> | <span class="t">we've got to do is compute the grad log probability times the total reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1927" target="_blank">00:32:07.540</a></span> | <span class="t">So this probability of the trajectory, that sounds like a really unfriendly quantity because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1932" target="_blank">00:32:12.020</a></span> | <span class="t">there's a long, complicated process that generates this trajectory with lots of time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1940" target="_blank">00:32:20.020</a></span> | <span class="t">But log-- OK, so we can write out what this process is, what this probability density</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1945" target="_blank">00:32:25.700</a></span> | <span class="t">is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1947" target="_blank">00:32:27.000</a></span> | <span class="t">So we have-- it's just a product of probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1949" target="_blank">00:32:29.980</a></span> | <span class="t">We've got our initial-- we've got our mu of s0, which is just our initial state distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1956" target="_blank">00:32:36.280</a></span> | <span class="t">And then every time step, we have-- we sample the action according to pi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1960" target="_blank">00:32:40.700</a></span> | <span class="t">And we sample the next state and reward according to our dynamics model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1965" target="_blank">00:32:45.300</a></span> | <span class="t">So log turns that product into a sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1970" target="_blank">00:32:50.120</a></span> | <span class="t">And here's the cool part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1973" target="_blank">00:32:53.100</a></span> | <span class="t">Anything that doesn't contain theta drops out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1977" target="_blank">00:32:57.300</a></span> | <span class="t">So the thing is, we didn't know-- there are parts of this probability distribution, p</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1983" target="_blank">00:33:03.360</a></span> | <span class="t">of tau given theta, that we don't have access to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1986" target="_blank">00:33:06.260</a></span> | <span class="t">So if this is reinforcement learning, we don't assume that we know the dynamics model of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1990" target="_blank">00:33:10.500</a></span> | <span class="t">the system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1991" target="_blank">00:33:11.500</a></span> | <span class="t">We just find out about it by sampling-- by doing episodes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=1999" target="_blank">00:33:19.980</a></span> | <span class="t">So since this product turns into a sum, all the pieces, like the log p there and the log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2008" target="_blank">00:33:28.100</a></span> | <span class="t">mu, which we don't know, just drop out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2011" target="_blank">00:33:31.300</a></span> | <span class="t">So it doesn't matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2014" target="_blank">00:33:34.740</a></span> | <span class="t">And what we get in the end is we get a sum of log probabilities of actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2022" target="_blank">00:33:42.340</a></span> | <span class="t">So grad log pi of action given state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2027" target="_blank">00:33:47.340</a></span> | <span class="t">So our formula looks like-- our formula for the grading of the expectation is just the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2033" target="_blank">00:33:53.340</a></span> | <span class="t">expectation over trajectories of total reward of the trajectory times grad of the sum of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2041" target="_blank">00:34:01.540</a></span> | <span class="t">all the log probs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2046" target="_blank">00:34:06.320</a></span> | <span class="t">So the interpretation of this is we're taking our good trajectories and we're trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2052" target="_blank">00:34:12.300</a></span> | <span class="t">increase their probability in proportion to how good they are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2056" target="_blank">00:34:16.220</a></span> | <span class="t">And you can think of this as being similar to supervised learning, where we treat the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2060" target="_blank">00:34:20.620</a></span> | <span class="t">good trajectories with high rewards as positive examples in our supervised learning problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2066" target="_blank">00:34:26.660</a></span> | <span class="t">So we're using those to train the policy on which actions are good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2070" target="_blank">00:34:30.940</a></span> | <span class="t">We're basically treating those actions as positive examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2075" target="_blank">00:34:35.820</a></span> | <span class="t">OK, now we can improve this formula a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2081" target="_blank">00:34:41.900</a></span> | <span class="t">So that was just the most basic-- I mean, this is an unbiased estimate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2086" target="_blank">00:34:46.180</a></span> | <span class="t">This is an estimator for the policy gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2087" target="_blank">00:34:47.940</a></span> | <span class="t">So if we just take that expression inside the expectation on the right-hand side and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2092" target="_blank">00:34:52.940</a></span> | <span class="t">we take one sample of that, it has the right mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2096" target="_blank">00:34:56.020</a></span> | <span class="t">So if we just get enough of them, we're going to get the policy gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2102" target="_blank">00:35:02.580</a></span> | <span class="t">But we can also write down some other formulas that have the same mean but have lower variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2107" target="_blank">00:35:07.820</a></span> | <span class="t">So we can come up with better estimators for the policy gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2111" target="_blank">00:35:11.620</a></span> | <span class="t">And that's actually quite important because the one from the previous slide is really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2115" target="_blank">00:35:15.060</a></span> | <span class="t">bad when you have a large number of time steps, meaning it has really high variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2122" target="_blank">00:35:22.500</a></span> | <span class="t">So the first thing we can do is we can use the temporal structure of the problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2128" target="_blank">00:35:28.900</a></span> | <span class="t">By the way, to derive these next bunch of formulas, it just takes a bunch of really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2133" target="_blank">00:35:33.300</a></span> | <span class="t">straightforward manipulation where you move around expectations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2137" target="_blank">00:35:37.220</a></span> | <span class="t">And I'm not going to go through all the math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2140" target="_blank">00:35:40.060</a></span> | <span class="t">But I'll just say what the formulas are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2145" target="_blank">00:35:45.420</a></span> | <span class="t">So we can repeat the same argument from the previous slide to just derive the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2151" target="_blank">00:35:51.740</a></span> | <span class="t">estimator for a single reward term.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2154" target="_blank">00:35:54.520</a></span> | <span class="t">So we end up with that reward term times the grad sum of log probs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2160" target="_blank">00:36:00.260</a></span> | <span class="t">And just summing over that, we get a new formula where we're not multiplying the sum of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2167" target="_blank">00:36:07.460</a></span> | <span class="t">grad log prob of the whole thing times the sum of all rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2172" target="_blank">00:36:12.580</a></span> | <span class="t">So let's look at that bottom formula.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2175" target="_blank">00:36:15.220</a></span> | <span class="t">Now we have a sum over time of grad log probability of the action at that time times the sum of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2181" target="_blank">00:36:21.580</a></span> | <span class="t">future rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2184" target="_blank">00:36:24.400</a></span> | <span class="t">So now, I mean, in the formula from the previous slide, we would have had all the rewards in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2188" target="_blank">00:36:28.420</a></span> | <span class="t">that sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2190" target="_blank">00:36:30.660</a></span> | <span class="t">But now we just have the future rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2192" target="_blank">00:36:32.980</a></span> | <span class="t">And that kind of makes sense because an action can affect the probability of the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2198" target="_blank">00:36:38.980</a></span> | <span class="t">rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2199" target="_blank">00:36:39.980</a></span> | <span class="t">So to figure out if the action is good, we should only be looking at the future rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2207" target="_blank">00:36:47.340</a></span> | <span class="t">So this is a slightly better formula than the one on the previous slide, meaning it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2211" target="_blank">00:36:51.180</a></span> | <span class="t">has the exact same mean except the expression inside the expectation there has lower variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2221" target="_blank">00:37:01.340</a></span> | <span class="t">And we can further reduce the variance by introducing a baseline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2226" target="_blank">00:37:06.500</a></span> | <span class="t">So now we can take any old function, B, which takes in a state and it outputs a real number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2233" target="_blank">00:37:13.620</a></span> | <span class="t">And we can subtract it from our sum of future rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2239" target="_blank">00:37:19.500</a></span> | <span class="t">And we didn't affect the mean of the estimator at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2244" target="_blank">00:37:24.180</a></span> | <span class="t">So we didn't change the expectation at all by introducing this baseline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2253" target="_blank">00:37:33.260</a></span> | <span class="t">So yeah, for any choice of B, this gives us an unbiased estimator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2256" target="_blank">00:37:36.980</a></span> | <span class="t">By the way, if you're not that familiar with the terminology of estimators, what I'm saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2261" target="_blank">00:37:41.340</a></span> | <span class="t">is we have an expectation on the right-hand side of that formula.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2268" target="_blank">00:37:48.980</a></span> | <span class="t">And the quantity inside that expectation is what's called the estimator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2272" target="_blank">00:37:52.900</a></span> | <span class="t">And if we get a bunch of samples, then we can get an estimate of the thing on the left-hand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2279" target="_blank">00:37:59.180</a></span> | <span class="t">side, which is what we care about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2284" target="_blank">00:38:04.020</a></span> | <span class="t">So when I say it's an unbiased estimator, that just means that this equation is correct,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2289" target="_blank">00:38:09.740</a></span> | <span class="t">meaning that the thing on the right-hand side equals the thing on the left-hand side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2294" target="_blank">00:38:14.740</a></span> | <span class="t">So yeah, this works for any choice of baseline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2297" target="_blank">00:38:17.660</a></span> | <span class="t">And a near optimal choice is to use the expected return.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2302" target="_blank">00:38:22.440</a></span> | <span class="t">So the expected sum of future rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2307" target="_blank">00:38:27.180</a></span> | <span class="t">And the interpretation of that is if we took an action, we only want to increase the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2314" target="_blank">00:38:34.180</a></span> | <span class="t">of the action if it was a good action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2316" target="_blank">00:38:36.660</a></span> | <span class="t">So how do we tell if it was a good action?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2318" target="_blank">00:38:38.660</a></span> | <span class="t">Well, the sum of rewards after that action should have been better than expected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2323" target="_blank">00:38:43.840</a></span> | <span class="t">So the B of S is the expected sum of rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2327" target="_blank">00:38:47.780</a></span> | <span class="t">And we're just taking the difference between the measured thing and the expected thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2336" target="_blank">00:38:56.560</a></span> | <span class="t">So that was a pretty key thing for variance reduction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2343" target="_blank">00:39:03.120</a></span> | <span class="t">And I'm going to introduce one last variance reduction technique.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2347" target="_blank">00:39:07.120</a></span> | <span class="t">And actually, all three of these are really important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2349" target="_blank">00:39:09.200</a></span> | <span class="t">So basically, nothing's going to work except for maybe really small-scale problems unless</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2354" target="_blank">00:39:14.360</a></span> | <span class="t">you do these things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2356" target="_blank">00:39:16.100</a></span> | <span class="t">So the last variance reduction technique is to use discounts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2360" target="_blank">00:39:20.760</a></span> | <span class="t">So the discount factor ignores delayed effects between actions and rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2367" target="_blank">00:39:27.800</a></span> | <span class="t">So what we're going to do here looks kind of like a hack, but there's an explanation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2372" target="_blank">00:39:32.400</a></span> | <span class="t">for it, which is instead of taking the sum of rewards, we're going to take a discounted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2377" target="_blank">00:39:37.720</a></span> | <span class="t">sum of rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2378" target="_blank">00:39:38.720</a></span> | <span class="t">Meaning that we add this exponential factor, gamma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2384" target="_blank">00:39:44.320</a></span> | <span class="t">So when we're multiplying the grad log probability by some future reward, we multiply it by some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2393" target="_blank">00:39:53.320</a></span> | <span class="t">quantity that decays with time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2395" target="_blank">00:39:55.560</a></span> | <span class="t">So people typically use gamma equals 0.99 or gamma equals 0.95.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2401" target="_blank">00:40:01.280</a></span> | <span class="t">So that means if you use 0.99, that means after 100 time steps, you're going to be reducing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2409" target="_blank">00:40:09.820</a></span> | <span class="t">the reward by a factor of 1 over e.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2414" target="_blank">00:40:14.240</a></span> | <span class="t">So you're exponentially decaying the effect of the future rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2420" target="_blank">00:40:20.540</a></span> | <span class="t">And the intuition is that the action shouldn't affect rewards really far in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2427" target="_blank">00:40:27.240</a></span> | <span class="t">Like the assumption is that the system doesn't have really long-term memory and it sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2435" target="_blank">00:40:35.000</a></span> | <span class="t">resets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2438" target="_blank">00:40:38.360</a></span> | <span class="t">The effects aren't that far delayed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2440" target="_blank">00:40:40.500</a></span> | <span class="t">So you can just ignore the interaction between action and a reward way in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2449" target="_blank">00:40:49.260</a></span> | <span class="t">That's the intuition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2451" target="_blank">00:40:51.000</a></span> | <span class="t">So now instead of taking the baseline to be the expected sum of future rewards, we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2454" target="_blank">00:40:54.800</a></span> | <span class="t">to do a discounted sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2456" target="_blank">00:40:56.560</a></span> | <span class="t">So now we're measuring if the action was better than expected according to the discounted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2463" target="_blank">00:41:03.280</a></span> | <span class="t">sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2466" target="_blank">00:41:06.600</a></span> | <span class="t">And now there's a more general class of formulas that looks like the one that I just wrote.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2470" target="_blank">00:41:10.580</a></span> | <span class="t">So this one that's on the top of the slide is pretty good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2473" target="_blank">00:41:13.840</a></span> | <span class="t">And this is almost as good as anything you're going to do to within a small constant factor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2479" target="_blank">00:41:19.880</a></span> | <span class="t">But there's a more general class of formulas that look like grad log probability times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2488" target="_blank">00:41:28.800</a></span> | <span class="t">some quantity a hat, which we call the advantage estimate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2492" target="_blank">00:41:32.520</a></span> | <span class="t">And this is in general just going to be an estimate of, it has a more precise definition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2499" target="_blank">00:41:39.480</a></span> | <span class="t">which is how much was this action better than the average action taken by the policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2508" target="_blank">00:41:48.720</a></span> | <span class="t">But informally, this just means how much better was the action than expected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2515" target="_blank">00:41:55.240</a></span> | <span class="t">And this formula makes a lot of sense because we want to increase the probability of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2518" target="_blank">00:41:58.680</a></span> | <span class="t">good actions and decrease the probability of the bad ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2522" target="_blank">00:42:02.000</a></span> | <span class="t">So we should increase it in proportion to the goodness of the action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2530" target="_blank">00:42:10.360</a></span> | <span class="t">So just to summarize, so I just told you there's this gradient estimator, meaning there's this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2534" target="_blank">00:42:14.760</a></span> | <span class="t">expression you can compute, which gives you a noisy estimate of the policy gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2538" target="_blank">00:42:18.960</a></span> | <span class="t">So how do you actually turn this into an algorithm?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2541" target="_blank">00:42:21.560</a></span> | <span class="t">So this is silly, algorithm seven.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2547" target="_blank">00:42:27.560</a></span> | <span class="t">So here's what the algorithm looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2549" target="_blank">00:42:29.080</a></span> | <span class="t">It's pretty much what you'd expect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2552" target="_blank">00:42:32.680</a></span> | <span class="t">You take your policy, you initialize your policy parameter and your baseline function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2560" target="_blank">00:42:40.160</a></span> | <span class="t">For each iteration, you execute the current policy to get a bunch of whole episodes, meaning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2569" target="_blank">00:42:49.280</a></span> | <span class="t">whole trajectories.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2570" target="_blank">00:42:50.920</a></span> | <span class="t">And each time step in each trajectory, you should compute the return, meaning the sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2576" target="_blank">00:42:56.140</a></span> | <span class="t">of rewards following that time step, the sum of discounted rewards, and the advantage estimate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2581" target="_blank">00:43:01.120</a></span> | <span class="t">which is the sum of discounted rewards minus the baseline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2585" target="_blank">00:43:05.800</a></span> | <span class="t">Then you refit the baseline by trying to make the baseline function equal the returns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2592" target="_blank">00:43:12.360</a></span> | <span class="t">And then you update the policy using a policy gradient estimator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2596" target="_blank">00:43:16.680</a></span> | <span class="t">So you're just doing SGD while updating the baseline as you go along.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2605" target="_blank">00:43:25.520</a></span> | <span class="t">So that's the vanilla policy gradient algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2610" target="_blank">00:43:30.200</a></span> | <span class="t">And this is, I'll briefly talk, this has been used to obtain some pretty good results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2617" target="_blank">00:43:37.040</a></span> | <span class="t">So it's not that bad of an algorithm, but there are several different directions that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2623" target="_blank">00:43:43.480</a></span> | <span class="t">it can be improved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2627" target="_blank">00:43:47.440</a></span> | <span class="t">So one issue that you run into is with step sizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2632" target="_blank">00:43:52.760</a></span> | <span class="t">So in supervised learning, step sizes aren't that big of a deal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2637" target="_blank">00:43:57.960</a></span> | <span class="t">Because maybe you take too big of a step, but that's OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2642" target="_blank">00:44:02.760</a></span> | <span class="t">You'll fix it the next update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2644" target="_blank">00:44:04.360</a></span> | <span class="t">And your current function, your current classifier, for example, doesn't affect what inputs you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2649" target="_blank">00:44:09.880</a></span> | <span class="t">getting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2650" target="_blank">00:44:10.880</a></span> | <span class="t">So even if you just are doing really, even if your network is just thrashing around for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2656" target="_blank">00:44:16.800</a></span> | <span class="t">a while because you're taking too big steps, that's not going to cause any problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2662" target="_blank">00:44:22.800</a></span> | <span class="t">But yeah, so step sizes aren't that big of a deal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2668" target="_blank">00:44:28.720</a></span> | <span class="t">You can just anneal them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2670" target="_blank">00:44:30.600</a></span> | <span class="t">You can start off with a large step size and anneal them down to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2673" target="_blank">00:44:33.700</a></span> | <span class="t">And that works pretty well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2676" target="_blank">00:44:36.720</a></span> | <span class="t">In reinforcement learning, if you take too big of a step, you might wreck your policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2681" target="_blank">00:44:41.480</a></span> | <span class="t">And even if you don't actually change the network that much, so you don't lose all your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2685" target="_blank">00:44:45.640</a></span> | <span class="t">nice features, you might just change its behavior a little too much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2690" target="_blank">00:44:50.680</a></span> | <span class="t">And now it's going to do something totally different and visit a totally different part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2693" target="_blank">00:44:53.840</a></span> | <span class="t">of state space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2695" target="_blank">00:44:55.940</a></span> | <span class="t">So since in reinforcement learning, the system is stateful and your state distribution depends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2700" target="_blank">00:45:00.440</a></span> | <span class="t">on your policy, that brings a really different problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2706" target="_blank">00:45:06.760</a></span> | <span class="t">And now, after you took that step, the next batch of data you're going to get was collected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2711" target="_blank">00:45:11.720</a></span> | <span class="t">by the bad policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2713" target="_blank">00:45:13.280</a></span> | <span class="t">And now you're never going to recover because you just forgot everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2717" target="_blank">00:45:17.760</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2719" target="_blank">00:45:19.440</a></span> | <span class="t">So, one way that my colleagues and I, well, one way to fix this is to try to basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2729" target="_blank">00:45:29.920</a></span> | <span class="t">try to stop the policy from taking too big of a step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2733" target="_blank">00:45:33.120</a></span> | <span class="t">So you can look at the KL divergence between the old policy and the new policy, like before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2739" target="_blank">00:45:39.680</a></span> | <span class="t">the update and after the update, and make sure that the distributions aren't that different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2744" target="_blank">00:45:44.640</a></span> | <span class="t">So you're not taking too big of a step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2746" target="_blank">00:45:46.400</a></span> | <span class="t">It's kind of an obvious thing to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2748" target="_blank">00:45:48.200</a></span> | <span class="t">So my colleagues and I developed an algorithm called trust region policy optimization, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2753" target="_blank">00:45:53.320</a></span> | <span class="t">looks at the action distributions and tries to make sure the KL divergence isn't too large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2760" target="_blank">00:46:00.040</a></span> | <span class="t">And this is very closely related to previous natural policy gradient methods, which are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2768" target="_blank">00:46:08.320</a></span> | <span class="t">based on-- which are doing something similar, but usually it's not set up as a hard constraint</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2775" target="_blank">00:46:15.040</a></span> | <span class="t">on the KL divergence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2780" target="_blank">00:46:20.920</a></span> | <span class="t">So another type of extension of policy gradient methods is to do more-- to use value functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2788" target="_blank">00:46:28.880</a></span> | <span class="t">to do more variance reduction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2792" target="_blank">00:46:32.800</a></span> | <span class="t">Instead of just using them as a baseline, you can also-- you can use them more aggressively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2798" target="_blank">00:46:38.720</a></span> | <span class="t">and introduce some bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2801" target="_blank">00:46:41.200</a></span> | <span class="t">So I won't go into the details in this talk, but sometimes these are called actor-critic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2807" target="_blank">00:46:47.120</a></span> | <span class="t">methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2812" target="_blank">00:46:52.800</a></span> | <span class="t">There's also another type of approach, which I briefly touched on in the earlier slide,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2818" target="_blank">00:46:58.880</a></span> | <span class="t">where instead of just trying to increase the probability of the good actions, you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2823" target="_blank">00:47:03.040</a></span> | <span class="t">differentiate your loss with respect to the actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2825" target="_blank">00:47:05.880</a></span> | <span class="t">This is like the reparameterization trick, which is used for density modeling and unsupervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2832" target="_blank">00:47:12.320</a></span> | <span class="t">learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2835" target="_blank">00:47:15.060</a></span> | <span class="t">So here you're trying to-- instead of just increasing the probability of the good actions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2839" target="_blank">00:47:19.560</a></span> | <span class="t">you're trying to push the actions towards better actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2843" target="_blank">00:47:23.880</a></span> | <span class="t">And I'd say both of these bullet points, you're potentially decreasing your variance a lot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2850" target="_blank">00:47:30.720</a></span> | <span class="t">but at the cost of increasing bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2853" target="_blank">00:47:33.080</a></span> | <span class="t">So it actually makes the algorithms a little harder to understand and to get them working,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2858" target="_blank">00:47:38.840</a></span> | <span class="t">because with high variance, if you just crank up the amount of data, you can always drive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2864" target="_blank">00:47:44.180</a></span> | <span class="t">your variance down as much as you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2866" target="_blank">00:47:46.800</a></span> | <span class="t">But with bias, even if-- no matter how much data you get, you're not going to get rid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2870" target="_blank">00:47:50.800</a></span> | <span class="t">of the bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2871" target="_blank">00:47:51.800</a></span> | <span class="t">So if your gradient is pointing in the wrong direction, then you're not going to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2875" target="_blank">00:47:55.360</a></span> | <span class="t">anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2877" target="_blank">00:47:57.560</a></span> | <span class="t">OK, so now that's it for the policy gradient section of this talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2885" target="_blank">00:48:05.360</a></span> | <span class="t">So I wanted to show a quick video of some work that my colleagues and I did on learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2890" target="_blank">00:48:10.860</a></span> | <span class="t">locomotion controllers with policy gradient methods, which I think-- well, I found pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2897" target="_blank">00:48:17.020</a></span> | <span class="t">exciting when I saw it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2899" target="_blank">00:48:19.160</a></span> | <span class="t">So hopefully you find it interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2906" target="_blank">00:48:26.660</a></span> | <span class="t">So here, what we've got is a humanoid simulated-- let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2911" target="_blank">00:48:31.240</a></span> | <span class="t">OK, yeah, it's a simulated humanoid robot in a physics simulator, a realistic physics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2917" target="_blank">00:48:37.200</a></span> | <span class="t">simulator called MuJoCo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2920" target="_blank">00:48:40.320</a></span> | <span class="t">And it has a neural network policy, which takes in the joint angles of the robot and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2928" target="_blank">00:48:48.600</a></span> | <span class="t">a little bit of other kinematic information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2931" target="_blank">00:48:51.080</a></span> | <span class="t">It's got joint velocities and also positions of the different links of the robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2936" target="_blank">00:48:56.700</a></span> | <span class="t">So that's what the input is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2938" target="_blank">00:48:58.500</a></span> | <span class="t">It's pretty much the raw state of the robot, like no clever feature engineering there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2944" target="_blank">00:49:04.160</a></span> | <span class="t">And the output is going to be the joint torques, which are set 100 times a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2949" target="_blank">00:49:09.620</a></span> | <span class="t">So we're just mapping from joint angles to joint torques.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2955" target="_blank">00:49:15.300</a></span> | <span class="t">And we define a reward function, which is to move forward as fast as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2960" target="_blank">00:49:20.400</a></span> | <span class="t">So it gets a reward for moving forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2965" target="_blank">00:49:25.680</a></span> | <span class="t">So the episode ends when its head goes below a certain height, meaning it fell over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2971" target="_blank">00:49:31.560</a></span> | <span class="t">So that's basically the setup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2973" target="_blank">00:49:33.060</a></span> | <span class="t">There is a little bit of tweaking for the reward function, but not too extensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2979" target="_blank">00:49:39.480</a></span> | <span class="t">So you can see, first it just falls forward a lot of times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=2995" target="_blank">00:49:55.180</a></span> | <span class="t">And then slowly it starts to develop a half-decent looking walk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3000" target="_blank">00:50:00.520</a></span> | <span class="t">And eventually it gets it down pretty well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3006" target="_blank">00:50:06.420</a></span> | <span class="t">And at the very end of this, it could just keep running indefinitely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3011" target="_blank">00:50:11.020</a></span> | <span class="t">So I think it was actually stable in a strong sense, meaning I could just leave it for 15</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3015" target="_blank">00:50:15.180</a></span> | <span class="t">minutes and it wouldn't fall over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3016" target="_blank">00:50:16.580</a></span> | <span class="t">It would just keep going.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3018" target="_blank">00:50:18.980</a></span> | <span class="t">So here's another robot model that we just created without too much thought.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3026" target="_blank">00:50:26.220</a></span> | <span class="t">We just decided to put a bunch of legs on this thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3030" target="_blank">00:50:30.700</a></span> | <span class="t">So we don't even know how this thing is supposed to walk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3035" target="_blank">00:50:35.040</a></span> | <span class="t">And just give it to the same algorithm and it just figures out some kind of crazy way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3039" target="_blank">00:50:39.340</a></span> | <span class="t">to walk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3042" target="_blank">00:50:42.920</a></span> | <span class="t">So that's the nice thing about reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3045" target="_blank">00:50:45.260</a></span> | <span class="t">You don't even need to know what you want it to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3048" target="_blank">00:50:48.620</a></span> | <span class="t">I think this is also the physics are a little unrealistic here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3056" target="_blank">00:50:56.140</a></span> | <span class="t">Here we set up, we used a similar model to the one in the first demo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3060" target="_blank">00:51:00.940</a></span> | <span class="t">But here we just give it a reward for having its head at a certain height.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3064" target="_blank">00:51:04.880</a></span> | <span class="t">So there's a reward telling it to get its head up as high as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3069" target="_blank">00:51:09.060</a></span> | <span class="t">And then it figures out how to get up off the ground.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3072" target="_blank">00:51:12.220</a></span> | <span class="t">Let's see, I have low battery.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3079" target="_blank">00:51:19.380</a></span> | <span class="t">Does anyone have a charger that I could?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3089" target="_blank">00:51:29.060</a></span> | <span class="t">Thanks a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3090" target="_blank">00:51:30.060</a></span> | <span class="t">You're a lifesaver.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3091" target="_blank">00:51:31.060</a></span> | <span class="t">OK, any questions about policy gradients before I move on to the next part?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3117" target="_blank">00:51:57.300</a></span> | <span class="t">So the question was, is the system time invariant?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3119" target="_blank">00:51:59.780</a></span> | <span class="t">Yes, that's assumed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3121" target="_blank">00:52:01.300</a></span> | <span class="t">Is that it's stationary?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3123" target="_blank">00:52:03.900</a></span> | <span class="t">Oh, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3125" target="_blank">00:52:05.140</a></span> | <span class="t">And also that it doesn't change from one episode to the next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3130" target="_blank">00:52:10.180</a></span> | <span class="t">Of course, in some real world problems that might not be the case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3133" target="_blank">00:52:13.020</a></span> | <span class="t">So I think that's also an interesting problem setting where you have a non-stationary environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3146" target="_blank">00:52:26.180</a></span> | <span class="t">So the question was, for the baseline, to learn a good baseline, do you need to know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3150" target="_blank">00:52:30.340</a></span> | <span class="t">the dynamics of the system?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3152" target="_blank">00:52:32.340</a></span> | <span class="t">So no, you can just learn it by doing regression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3156" target="_blank">00:52:36.100</a></span> | <span class="t">You just estimate the empirical returns and then you do regression to try to fit a function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3160" target="_blank">00:52:40.860</a></span> | <span class="t">to that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3161" target="_blank">00:52:41.860</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3189" target="_blank">00:53:09.620</a></span> | <span class="t">Yeah, so the question is, there's a discount factor which should cause the policy to disregard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3198" target="_blank">00:53:18.100</a></span> | <span class="t">any effects that are delayed by more than 100 time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3201" target="_blank">00:53:21.520</a></span> | <span class="t">So how does it still work that this guy learns how to stand up, even though that might take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3206" target="_blank">00:53:26.860</a></span> | <span class="t">more than 100 time steps?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3208" target="_blank">00:53:28.580</a></span> | <span class="t">Is that correct?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3209" target="_blank">00:53:29.580</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3210" target="_blank">00:53:30.580</a></span> | <span class="t">So yeah, you're right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3212" target="_blank">00:53:32.300</a></span> | <span class="t">And in fact, I would say that these methods aren't guaranteed to work well if you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3217" target="_blank">00:53:37.180</a></span> | <span class="t">more than 100 time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3219" target="_blank">00:53:39.980</a></span> | <span class="t">So sometimes they work anyway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3222" target="_blank">00:53:42.140</a></span> | <span class="t">Often they work anyway, but there's no guarantee.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3224" target="_blank">00:53:44.740</a></span> | <span class="t">So I think there's actually something pretty fundamental missing in how to deal with really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3229" target="_blank">00:53:49.340</a></span> | <span class="t">long time scales.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3230" target="_blank">00:53:50.660</a></span> | <span class="t">And people have recently been thinking about hierarchical reinforcement learning, where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3234" target="_blank">00:53:54.700</a></span> | <span class="t">you have different levels of detail of the system, where you might have one level of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3241" target="_blank">00:54:01.640</a></span> | <span class="t">description where you have a short time, a small time step, and then you have successively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3247" target="_blank">00:54:07.180</a></span> | <span class="t">larger time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3248" target="_blank">00:54:08.940</a></span> | <span class="t">And that allows you to plan over much longer horizons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3252" target="_blank">00:54:12.940</a></span> | <span class="t">So that's something that's currently an active area of research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3255" target="_blank">00:54:15.860</a></span> | <span class="t">But yeah, I would say that none of these methods are guaranteed to do anything reasonable if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3261" target="_blank">00:54:21.460</a></span> | <span class="t">you have more than 1 over 1 minus gamma time steps between action and reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3268" target="_blank">00:54:28.100</a></span> | <span class="t">So if you introduce some anomaly in the environment, can it still handle it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3275" target="_blank">00:54:35.500</a></span> | <span class="t">Oh yeah, so in this kind of task, if you introduce terrain or something, could it-- I think if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3282" target="_blank">00:54:42.260</a></span> | <span class="t">you didn't train it to deal with terrain, then it might fail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3286" target="_blank">00:54:46.700</a></span> | <span class="t">It probably would fail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3287" target="_blank">00:54:47.700</a></span> | <span class="t">Actually, I don't think it would fail, because the funny thing about-- these policies are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3292" target="_blank">00:54:52.020</a></span> | <span class="t">actually really robust, because you train them with a stochastic policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3297" target="_blank">00:54:57.580</a></span> | <span class="t">So there's a lot of noise being generated by the policy itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3301" target="_blank">00:55:01.540</a></span> | <span class="t">So in practice, it's able to deal with huge noise introduced by the policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3308" target="_blank">00:55:08.860</a></span> | <span class="t">And as a result, I found that if you change the dynamics parameters a little, it can usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3314" target="_blank">00:55:14.380</a></span> | <span class="t">still work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3315" target="_blank">00:55:15.420</a></span> | <span class="t">But yeah, there's no guarantee that it'll do anything if you give it something you didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3318" target="_blank">00:55:18.820</a></span> | <span class="t">train it for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3320" target="_blank">00:55:20.780</a></span> | <span class="t">I think that you probably could do the same kind of training with terrain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3325" target="_blank">00:55:25.540</a></span> | <span class="t">I didn't have any terrain, so I didn't try it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3328" target="_blank">00:55:28.180</a></span> | <span class="t">But that would be nice to try.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3330" target="_blank">00:55:30.940</a></span> | <span class="t">OK, I'm going to move on to the next part of the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3335" target="_blank">00:55:35.540</a></span> | <span class="t">Feel free, if you have more questions, to find me afterwards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3342" target="_blank">00:55:42.500</a></span> | <span class="t">OK, so now I'm going to talk about a different type of reinforcement learning algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3354" target="_blank">00:55:54.100</a></span> | <span class="t">So the previous methods are distinguished by the fact that they explicitly represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3363" target="_blank">00:56:03.140</a></span> | <span class="t">a policy, which is the function that chooses your actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3366" target="_blank">00:56:06.500</a></span> | <span class="t">And they try to optimize it with respect to the parameters of the policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3370" target="_blank">00:56:10.700</a></span> | <span class="t">So the nice thing about the policy gradient methods we just talked about is that you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3374" target="_blank">00:56:14.860</a></span> | <span class="t">optimizing the thing you care about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3378" target="_blank">00:56:18.100</a></span> | <span class="t">And you're optimizing it with gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3380" target="_blank">00:56:20.100</a></span> | <span class="t">So that makes it kind of easy to understand what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3383" target="_blank">00:56:23.880</a></span> | <span class="t">Because if you're getting the proper gradient estimate, and you take small enough steps,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3388" target="_blank">00:56:28.540</a></span> | <span class="t">then you should be improving.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3389" target="_blank">00:56:29.540</a></span> | <span class="t">I mean, of course, you still could get stuck in a local minimum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3392" target="_blank">00:56:32.740</a></span> | <span class="t">But at least-- or you get stuck in a bad local minimum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3396" target="_blank">00:56:36.460</a></span> | <span class="t">But at least it's a local minimum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3397" target="_blank">00:56:37.940</a></span> | <span class="t">And you can use our understanding of optimization to figure out what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3404" target="_blank">00:56:44.080</a></span> | <span class="t">So these next class of methods are a little different, because they're not optimizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3408" target="_blank">00:56:48.580</a></span> | <span class="t">the policy directly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3410" target="_blank">00:56:50.260</a></span> | <span class="t">They're learning something else called a Q function, which measures how good state-action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3415" target="_blank">00:56:55.860</a></span> | <span class="t">pairs are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3416" target="_blank">00:56:56.860</a></span> | <span class="t">So it measures-- I'll say that more formally later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3420" target="_blank">00:57:00.620</a></span> | <span class="t">But it's just measuring how good the actions are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3424" target="_blank">00:57:04.020</a></span> | <span class="t">And these methods are actually-- these are able to exactly solve MDPs efficiently in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3435" target="_blank">00:57:15.700</a></span> | <span class="t">the setting where you have a finite number of states and actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3439" target="_blank">00:57:19.620</a></span> | <span class="t">So these are the preferred methods for exactly solving them in those settings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3445" target="_blank">00:57:25.100</a></span> | <span class="t">But you can also apply them with continuous states and actions, and using expressive function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3454" target="_blank">00:57:34.460</a></span> | <span class="t">approximators like neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3457" target="_blank">00:57:37.500</a></span> | <span class="t">But it's a little harder to understand what's going on in these methods, like when they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3461" target="_blank">00:57:41.860</a></span> | <span class="t">going to work and when they're not going to work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3465" target="_blank">00:57:45.000</a></span> | <span class="t">So I'll define the relevant quantities here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3470" target="_blank">00:57:50.380</a></span> | <span class="t">So the Q function is defined as the expected sum of rewards when we condition on the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3477" target="_blank">00:57:57.020</a></span> | <span class="t">state and the first action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3479" target="_blank">00:57:59.180</a></span> | <span class="t">So we're conditioning on S0 equals S, A0 equals A. And the Q function is the expected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3488" target="_blank">00:58:08.380</a></span> | <span class="t">discounted sum of rewards when we're acting under the policy pi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3494" target="_blank">00:58:14.620</a></span> | <span class="t">So by convention, I'm starting out with time step 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3500" target="_blank">00:58:20.900</a></span> | <span class="t">I could have also said that we're taking RT plus RT plus 1 plus RT plus 2 and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3507" target="_blank">00:58:27.740</a></span> | <span class="t">But since we're assuming the system is stationary, it should be exactly the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3512" target="_blank">00:58:32.520</a></span> | <span class="t">So just by convention, I'm going to say that the first-- I'm going to always use time 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3516" target="_blank">00:58:36.900</a></span> | <span class="t">1, 2, 3, and so on, just for ease of notation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3521" target="_blank">00:58:41.980</a></span> | <span class="t">So the Q function is just telling you how good this state action pair is under your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3526" target="_blank">00:58:46.240</a></span> | <span class="t">current policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3528" target="_blank">00:58:48.020</a></span> | <span class="t">The value function-- well, the state value function, usually called V, is just conditioning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3534" target="_blank">00:58:54.340</a></span> | <span class="t">on the state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3537" target="_blank">00:58:57.820</a></span> | <span class="t">It's telling you how good that state is, what's the expected reward of that state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3542" target="_blank">00:59:02.520</a></span> | <span class="t">And lastly, the advantage function is the difference between the Q function and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3547" target="_blank">00:59:07.080</a></span> | <span class="t">state value function, meaning how much better is that action than what the policy would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3551" target="_blank">00:59:11.700</a></span> | <span class="t">have done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3554" target="_blank">00:59:14.180</a></span> | <span class="t">We're not going to talk about advantage functions in this section, but it was actually-- this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3558" target="_blank">00:59:18.020</a></span> | <span class="t">corresponds to the notion of an advantage estimator we briefly mentioned in the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3563" target="_blank">00:59:23.300</a></span> | <span class="t">section.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3566" target="_blank">00:59:26.140</a></span> | <span class="t">So here, we're going to consider methods that explicitly store and update the Q function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3570" target="_blank">00:59:30.860</a></span> | <span class="t">instead of the policy, and updates them using what are called Bellman equations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3581" target="_blank">00:59:41.620</a></span> | <span class="t">So the Bellman equation-- so a Bellman equation, in general, is a consistency equation that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3586" target="_blank">00:59:46.780</a></span> | <span class="t">should be satisfied by a value function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3591" target="_blank">00:59:51.100</a></span> | <span class="t">So here, I'm writing down the Bellman equation for Q pi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3595" target="_blank">00:59:55.780</a></span> | <span class="t">And what it's saying is that the expected sum of rewards should be the first reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3604" target="_blank">01:00:04.460</a></span> | <span class="t">plus this expected sum of rewards after the first time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3608" target="_blank">01:00:08.500</a></span> | <span class="t">So it's saying something pretty simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3610" target="_blank">01:00:10.820</a></span> | <span class="t">So R0 is the first reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3612" target="_blank">01:00:12.940</a></span> | <span class="t">V pi of S1 is just adding up all the rewards after time step 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3622" target="_blank">01:00:22.380</a></span> | <span class="t">So in the second equation, we write out this relationship just involving the Q function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3629" target="_blank">01:00:29.380</a></span> | <span class="t">So we have a consistency equation that the Q function should satisfy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3637" target="_blank">01:00:37.640</a></span> | <span class="t">We can slightly generalize this to use k time steps instead of just one time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3642" target="_blank">01:00:42.860</a></span> | <span class="t">So we can expand out the expectation-- the expected sum of rewards to write out k rewards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3649" target="_blank">01:00:49.920</a></span> | <span class="t">explicitly and then cap it off with the value function at the very end, which accounts for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3655" target="_blank">01:00:55.240</a></span> | <span class="t">all the rewards after that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3658" target="_blank">01:00:58.640</a></span> | <span class="t">OK, so here's the Bellman equation from the previous slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3663" target="_blank">01:01:03.620</a></span> | <span class="t">So now I'm going to introduce a very important concept called the Bellman backup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3671" target="_blank">01:01:11.340</a></span> | <span class="t">So we have this equation that the value function Q pi should satisfy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3677" target="_blank">01:01:17.280</a></span> | <span class="t">But let's assume we don't know Q pi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3680" target="_blank">01:01:20.080</a></span> | <span class="t">So let's say we have some other Q function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3684" target="_blank">01:01:24.880</a></span> | <span class="t">So we define this Bellman backup operator that operates on an arbitrary Q function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3691" target="_blank">01:01:31.280</a></span> | <span class="t">So it maps a Q function to a new Q function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3694" target="_blank">01:01:34.400</a></span> | <span class="t">And it's defined by just taking the right-hand side of the Bellman equation and plugging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3699" target="_blank">01:01:39.640</a></span> | <span class="t">in our new Q function Q instead of the Q pi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3710" target="_blank">01:01:50.280</a></span> | <span class="t">So Q pi is a fixed point of this operator, meaning if we apply the backup operator, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3717" target="_blank">01:01:57.200</a></span> | <span class="t">get the same thing back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3720" target="_blank">01:02:00.980</a></span> | <span class="t">And very nicely, if we keep applying this backup operator repeatedly to any old arbitrary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3728" target="_blank">01:02:08.600</a></span> | <span class="t">initial Q function Q, the series will converge to Q pi, which is the fixed point of the operator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3735" target="_blank">01:02:15.640</a></span> | <span class="t">So that's-- yeah, so that's-- so that way you can-- one way-- you can use an iterative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3747" target="_blank">01:02:27.200</a></span> | <span class="t">algorithm to estimate Q pi by taking any old initial Q function and repeatedly applying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3752" target="_blank">01:02:32.680</a></span> | <span class="t">this backup operator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3756" target="_blank">01:02:36.380</a></span> | <span class="t">So now there's another kind of Q function that we're going to introduce called Q star.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3762" target="_blank">01:02:42.200</a></span> | <span class="t">So the previous Q function Q pi was-- this is the-- telling you the value function under</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3767" target="_blank">01:02:47.800</a></span> | <span class="t">the current-- under some policy pi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3770" target="_blank">01:02:50.220</a></span> | <span class="t">So it only makes sense with regard to some particular fixed policy pi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3774" target="_blank">01:02:54.800</a></span> | <span class="t">Q star is going to be-- is going to involve the optimal policy instead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3782" target="_blank">01:03:02.920</a></span> | <span class="t">So Q star is just defined as the Q function of the optimal policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3786" target="_blank">01:03:06.720</a></span> | <span class="t">So here we have pi star, the optimal policy, and Q star is just the Q function of the optimal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3791" target="_blank">01:03:11.680</a></span> | <span class="t">policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3792" target="_blank">01:03:12.800</a></span> | <span class="t">And it also happens to be the pointwise maximum over all policies of the Q function at each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3801" target="_blank">01:03:21.000</a></span> | <span class="t">state action pair.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3805" target="_blank">01:03:25.920</a></span> | <span class="t">So the optimal policy is deterministic, and it should satisfy this equation that it takes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3814" target="_blank">01:03:34.220</a></span> | <span class="t">the argmax of the optimal Q function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3816" target="_blank">01:03:36.800</a></span> | <span class="t">So recall that the Q function tells you your expected return if you take the given action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3823" target="_blank">01:03:43.340</a></span> | <span class="t">So obviously the optimal policy should take the action that has the best expected return.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3828" target="_blank">01:03:48.220</a></span> | <span class="t">So that's why this last equation is evident.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3838" target="_blank">01:03:58.000</a></span> | <span class="t">So now that we know this property of the optimal policy, we can rewrite the Bellman equation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3846" target="_blank">01:04:06.200</a></span> | <span class="t">So on the-- that first equation is-- that's just the Bellman equation from the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3850" target="_blank">01:04:10.200</a></span> | <span class="t">slides for a given policy pi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3852" target="_blank">01:04:12.880</a></span> | <span class="t">Now we can take that expectation over actions and replace it by what the optimal policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3858" target="_blank">01:04:18.880</a></span> | <span class="t">is going to do, which is just going to take-- it's going to take the argmax of the optimal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3863" target="_blank">01:04:23.660</a></span> | <span class="t">Q function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3864" target="_blank">01:04:24.660</a></span> | <span class="t">There's a typo on my slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3866" target="_blank">01:04:26.180</a></span> | <span class="t">That should say Q star inside-- on the right-hand side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3872" target="_blank">01:04:32.820</a></span> | <span class="t">So now we have a Bellman equation that the optimal policy should satisfy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3879" target="_blank">01:04:39.340</a></span> | <span class="t">Now we can do the same thing with the backup operator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3882" target="_blank">01:04:42.480</a></span> | <span class="t">So we take that Bellman equation and we plug in an arbitrary Q function on the right-hand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3889" target="_blank">01:04:49.300</a></span> | <span class="t">side instead of the optimal Q function Q star.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3894" target="_blank">01:04:54.900</a></span> | <span class="t">So Q star is a fixed point of this Bellman operator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3899" target="_blank">01:04:59.880</a></span> | <span class="t">That's just a restatement of the Bellman equation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3904" target="_blank">01:05:04.300</a></span> | <span class="t">And again, if we reply this Bellman operator repeatedly to an arbitrary initial Q function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3911" target="_blank">01:05:11.340</a></span> | <span class="t">it converges to Q star, which is the optimal Q function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3916" target="_blank">01:05:16.900</a></span> | <span class="t">This is the Banach fixed point theorem in both cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3919" target="_blank">01:05:19.940</a></span> | <span class="t">It can be used to prove it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3924" target="_blank">01:05:24.140</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3926" target="_blank">01:05:26.460</a></span> | <span class="t">So based on these ideas, there are two classic algorithms for exactly solving MDPs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3933" target="_blank">01:05:33.340</a></span> | <span class="t">These are sometimes called dynamic programming algorithms because they're actually quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3936" target="_blank">01:05:36.740</a></span> | <span class="t">related to the kind of dynamic programming algorithms that are used to solve search problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3944" target="_blank">01:05:44.420</a></span> | <span class="t">So one is called value iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3946" target="_blank">01:05:46.500</a></span> | <span class="t">And you just initialize your Q function arbitrarily.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3949" target="_blank">01:05:49.900</a></span> | <span class="t">And you repeatedly do Bellman backups until it converges.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3956" target="_blank">01:05:56.100</a></span> | <span class="t">The second one is called policy iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3959" target="_blank">01:05:59.420</a></span> | <span class="t">You initialize your policy arbitrarily.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3962" target="_blank">01:06:02.580</a></span> | <span class="t">Then each step, you first compute either exactly or approximately the Q function of that policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3972" target="_blank">01:06:12.880</a></span> | <span class="t">And then you update your policy to be the greedy policy for the Q function you just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3977" target="_blank">01:06:17.580</a></span> | <span class="t">computed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3979" target="_blank">01:06:19.020</a></span> | <span class="t">So that means that your new policy just takes the argmax of the Q function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3984" target="_blank">01:06:24.500</a></span> | <span class="t">So it takes the action that's best according to that Q function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3991" target="_blank">01:06:31.700</a></span> | <span class="t">So I didn't say anything about how you compute Q pi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3995" target="_blank">01:06:35.100</a></span> | <span class="t">So one way to do it is to compute it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=3997" target="_blank">01:06:37.340</a></span> | <span class="t">You can compute it exactly because it happens that the Bellman equation for Q pi is a linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4001" target="_blank">01:06:41.740</a></span> | <span class="t">system of equations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4003" target="_blank">01:06:43.500</a></span> | <span class="t">So often you can just solve them exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4006" target="_blank">01:06:46.820</a></span> | <span class="t">More commonly, well, if you have a large scale problem, you might not be able to solve this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4010" target="_blank">01:06:50.540</a></span> | <span class="t">system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4011" target="_blank">01:06:51.620</a></span> | <span class="t">So what people often do is they do a finite number of Bellman backups, which doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4018" target="_blank">01:06:58.380</a></span> | <span class="t">exactly converge on Q pi, but it gives you something that's approximately Q pi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4024" target="_blank">01:07:04.700</a></span> | <span class="t">Okay, so that's, I just told you algorithms that you can implement if you have full access</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4032" target="_blank">01:07:12.060</a></span> | <span class="t">to the MDP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4033" target="_blank">01:07:13.300</a></span> | <span class="t">Like you know the whole table of probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4036" target="_blank">01:07:16.180</a></span> | <span class="t">But in reinforcement learning, usually the assumption is that you don't know any of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4040" target="_blank">01:07:20.380</a></span> | <span class="t">probability distributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4041" target="_blank">01:07:21.740</a></span> | <span class="t">You don't know their reward function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4042" target="_blank">01:07:22.900</a></span> | <span class="t">You don't know the transition probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4044" target="_blank">01:07:24.980</a></span> | <span class="t">So all of these things have to be estimated from data or you're only able to access the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4050" target="_blank">01:07:30.540</a></span> | <span class="t">system through interaction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4052" target="_blank">01:07:32.500</a></span> | <span class="t">So now it turns out that these algorithms can be also implemented if you only access</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4059" target="_blank">01:07:39.340</a></span> | <span class="t">the system through interactions, which is kind of remarkable, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4065" target="_blank">01:07:45.260</a></span> | <span class="t">So the way it works is, so let's recall our backup formulas for Q pi and Q star.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4073" target="_blank">01:07:53.620</a></span> | <span class="t">So, we can, in both cases, we have this certain quantity inside an expectation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4082" target="_blank">01:08:02.620</a></span> | <span class="t">In both cases, we can compute an unbiased estimator of that quantity inside the expectation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4090" target="_blank">01:08:10.260</a></span> | <span class="t">just using a single sample.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4091" target="_blank">01:08:11.780</a></span> | <span class="t">Meaning if we sampled some data from our system using any old policy, then we can get an unbiased</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4101" target="_blank">01:08:21.580</a></span> | <span class="t">estimator of the quantity on the right-hand side of those expectations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4106" target="_blank">01:08:26.300</a></span> | <span class="t">I mean the quantity inside of the right-hand expectations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4111" target="_blank">01:08:31.980</a></span> | <span class="t">So basically we can do an approximate version of this Bellman backup, which is unbiased.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4121" target="_blank">01:08:41.980</a></span> | <span class="t">And even with this noise, so we're doing a noisy version of the Bellman backup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4126" target="_blank">01:08:46.040</a></span> | <span class="t">Even with this noise, it can be proven that if you choose your step sizes appropriately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4130" target="_blank">01:08:50.500</a></span> | <span class="t">with the right schedule, you're still gonna converge to Q pi or Q star, depending on which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4138" target="_blank">01:08:58.860</a></span> | <span class="t">algorithm you're implementing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4141" target="_blank">01:09:01.460</a></span> | <span class="t">Okay, so now, well I'll say at this point that this is pretty much the fundamental idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4148" target="_blank">01:09:08.700</a></span> | <span class="t">And now you can come up with algorithms that can be applied in the reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4155" target="_blank">01:09:15.840</a></span> | <span class="t">setting where you're just accessing the system through sampling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4158" target="_blank">01:09:18.820</a></span> | <span class="t">And you can also start introducing function approximation here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4163" target="_blank">01:09:23.100</a></span> | <span class="t">So I haven't said anything about what the Q function is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4166" target="_blank">01:09:26.500</a></span> | <span class="t">I've just told you it's a function of state and action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4169" target="_blank">01:09:29.460</a></span> | <span class="t">But now we can start having neural network Q functions, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4176" target="_blank">01:09:36.120</a></span> | <span class="t">So we can parameterize the Q function with the neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4179" target="_blank">01:09:39.620</a></span> | <span class="t">Let's call it Q theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4181" target="_blank">01:09:41.960</a></span> | <span class="t">And now, instead of doing the Bellman backup, I mean it doesn't make sense to do the Bellman</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4186" target="_blank">01:09:46.980</a></span> | <span class="t">backup exactly because we're not just setting the values of the neural network output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4192" target="_blank">01:09:52.420</a></span> | <span class="t">The best we can do is try to encourage the neural network to have some output values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4197" target="_blank">01:09:57.980</a></span> | <span class="t">So what we do is, instead of doing the- the way we do this backup is we set up a least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4203" target="_blank">01:10:03.140</a></span> | <span class="t">squares problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4204" target="_blank">01:10:04.700</a></span> | <span class="t">So we write down this quadratic objective that says that the Q function should be approximately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4210" target="_blank">01:10:10.200</a></span> | <span class="t">equal to the backed up value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4211" target="_blank">01:10:11.760</a></span> | <span class="t">And then we just minimize it with SGD.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4216" target="_blank">01:10:16.380</a></span> | <span class="t">So one version of this algorithm, which was introduced about 10 years ago, called neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4220" target="_blank">01:10:20.900</a></span> | <span class="t">fitted Q iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4223" target="_blank">01:10:23.260</a></span> | <span class="t">Well it works exactly the way you'd expect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4225" target="_blank">01:10:25.320</a></span> | <span class="t">You sample trajectories using your current policy, which might be determined by the Q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4232" target="_blank">01:10:32.940</a></span> | <span class="t">function or it could be any old policy as it turns out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4237" target="_blank">01:10:37.500</a></span> | <span class="t">And then you solve the least squares problem where you're trying to minimize this quadratic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4246" target="_blank">01:10:46.340</a></span> | <span class="t">you try to minimize this quadratic error which is based on the Bellman backup, the backup</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4252" target="_blank">01:10:52.820</a></span> | <span class="t">for Q star.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4255" target="_blank">01:10:55.500</a></span> | <span class="t">So one thing I haven't mentioned so far is what do you actually use as your policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4262" target="_blank">01:11:02.020</a></span> | <span class="t">So I said sample trajectory using your policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4265" target="_blank">01:11:05.040</a></span> | <span class="t">So if you have a Q function, you can turn it into a policy by just taking the action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4271" target="_blank">01:11:11.100</a></span> | <span class="t">that has the highest Q value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4272" target="_blank">01:11:12.780</a></span> | <span class="t">That's what you typically do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4274" target="_blank">01:11:14.120</a></span> | <span class="t">So the Q function measures the goodness of all your actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4276" target="_blank">01:11:16.920</a></span> | <span class="t">So you can easily turn that into a policy by taking your best action or by taking actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4282" target="_blank">01:11:22.280</a></span> | <span class="t">where the log probability is proportional to the goodness or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4288" target="_blank">01:11:28.780</a></span> | <span class="t">So you might take typically probability is exponential of Q value over some kind of temperature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4296" target="_blank">01:11:36.860</a></span> | <span class="t">parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4298" target="_blank">01:11:38.600</a></span> | <span class="t">That's called Boltzmann exploration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4300" target="_blank">01:11:40.800</a></span> | <span class="t">Whereas if you just take the argmax, that's called the greedy policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4307" target="_blank">01:11:47.340</a></span> | <span class="t">So it turns out that with these kind of Q learning algorithms, you don't have to execute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4312" target="_blank">01:11:52.540</a></span> | <span class="t">the greedy policy for learning to work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4317" target="_blank">01:11:57.440</a></span> | <span class="t">You actually have some freedom in what policy you can execute, which is actually one very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4322" target="_blank">01:12:02.700</a></span> | <span class="t">nice property of these algorithms that you can use an exploration technique where your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4329" target="_blank">01:12:09.520</a></span> | <span class="t">policy is actively trying to reach new states or do something new and still learn the correct,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4336" target="_blank">01:12:16.240</a></span> | <span class="t">still converge, still move towards Q star or Q pi as the case may be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4344" target="_blank">01:12:24.320</a></span> | <span class="t">So that's a very basic neural fitted Q iteration is sort of a basic way of doing this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4351" target="_blank">01:12:31.440</a></span> | <span class="t">A more recent algorithm that's gotten a lot of attention is the one that was from Manny</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4356" target="_blank">01:12:36.760</a></span> | <span class="t">et al from DeepMind, which is basically an online version of this algorithm with a couple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4363" target="_blank">01:12:43.040</a></span> | <span class="t">of useful tweaks in it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4367" target="_blank">01:12:47.840</a></span> | <span class="t">But actually when you look at the two tricks, they're actually kind of very, they make a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4372" target="_blank">01:12:52.960</a></span> | <span class="t">lot of sense if you just think about what value iteration is doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4376" target="_blank">01:12:56.420</a></span> | <span class="t">So one technique is you use this replay pool where it's a rolling history of your past</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4383" target="_blank">01:13:03.280</a></span> | <span class="t">data and that's just the data you're going to use to fit your Q function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4389" target="_blank">01:13:09.080</a></span> | <span class="t">So that makes sure you have like a representative sample of data to fit your Q function to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4396" target="_blank">01:13:16.040</a></span> | <span class="t">And the second idea is to use a target network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4401" target="_blank">01:13:21.440</a></span> | <span class="t">So instead of using your current Q function and just doing Bellman backups on that, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4406" target="_blank">01:13:26.960</a></span> | <span class="t">have some lagged version of your Q function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4409" target="_blank">01:13:29.840</a></span> | <span class="t">So you have this target network which is a copy of your Q function at some earlier time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4414" target="_blank">01:13:34.240</a></span> | <span class="t">and you use that in the backups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4415" target="_blank">01:13:35.920</a></span> | <span class="t">So that also, if you think about value iteration, you have your old Q function and you're trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4421" target="_blank">01:13:41.440</a></span> | <span class="t">to make the new one equal to the backed up version of the old one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4424" target="_blank">01:13:44.680</a></span> | <span class="t">So using the target network just is sort of the natural thing to do if you're trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4428" target="_blank">01:13:48.560</a></span> | <span class="t">implement value iteration in an online way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4433" target="_blank">01:13:53.600</a></span> | <span class="t">And there have been many extensions proposed since then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4435" target="_blank">01:13:55.880</a></span> | <span class="t">I've got a bunch of citations at the bottom of the slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4440" target="_blank">01:14:00.360</a></span> | <span class="t">So this algorithm, the DQN algorithm, is using the backup B, which is the backup for Q*.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4451" target="_blank">01:14:11.920</a></span> | <span class="t">Remember that I also introduced this other backup, B pi, which is the backup for Q pi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4458" target="_blank">01:14:18.560</a></span> | <span class="t">So there's another algorithm, like a very classic algorithm called SARSA, which is an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4463" target="_blank">01:14:23.580</a></span> | <span class="t">online way of doing the B pi backup, essentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4468" target="_blank">01:14:28.640</a></span> | <span class="t">Well, it's sort of an online version of policy iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4475" target="_blank">01:14:35.280</a></span> | <span class="t">So it's actually found to work as well, or better than DQN, well, better than using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4481" target="_blank">01:14:41.360</a></span> | <span class="t">B backup in some settings, not all settings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4484" target="_blank">01:14:44.980</a></span> | <span class="t">So I think the jury's still out on exactly how these things compare.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4491" target="_blank">01:14:51.480</a></span> | <span class="t">But I think it's worth considering both policy iteration and value iteration and all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4498" target="_blank">01:14:58.040</a></span> | <span class="t">different online versions of these algorithms and taking them seriously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4501" target="_blank">01:15:01.520</a></span> | <span class="t">Because it's not clear right now exactly how they all compare to each other in the function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4508" target="_blank">01:15:08.360</a></span> | <span class="t">approximation setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4512" target="_blank">01:15:12.800</a></span> | <span class="t">So that's the overview of all the technical parts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4517" target="_blank">01:15:17.380</a></span> | <span class="t">And now I just have a couple conclusion slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4522" target="_blank">01:15:22.360</a></span> | <span class="t">So let me just summarize the current state of affairs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4525" target="_blank">01:15:25.000</a></span> | <span class="t">I introduced two kinds of algorithms, policy gradient algorithms, which explicitly represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4530" target="_blank">01:15:30.840</a></span> | <span class="t">a policy and optimize it, and Q function learning algorithms, which explicitly represent a Q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4537" target="_blank">01:15:37.200</a></span> | <span class="t">function, which is the goodness of different actions, and use that to implicitly represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4542" target="_blank">01:15:42.080</a></span> | <span class="t">a policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4544" target="_blank">01:15:44.560</a></span> | <span class="t">So policy gradient methods, there's a lot of-- so there have been some successes with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4549" target="_blank">01:15:49.580</a></span> | <span class="t">different variants of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4551" target="_blank">01:15:51.740</a></span> | <span class="t">So you have vanilla policy gradient methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4555" target="_blank">01:15:55.240</a></span> | <span class="t">There is a recent paper on this A3C method, which is an async implementation of it, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4563" target="_blank">01:16:03.700</a></span> | <span class="t">gets very good results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4566" target="_blank">01:16:06.820</a></span> | <span class="t">There's also another kind of methods are the natural policy gradient methods, trust region</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4572" target="_blank">01:16:12.380</a></span> | <span class="t">methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4573" target="_blank">01:16:13.380</a></span> | <span class="t">The one I showed you was obtained using trust region policy optimization, which is one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4578" target="_blank">01:16:18.020</a></span> | <span class="t">these in the second category.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4580" target="_blank">01:16:20.100</a></span> | <span class="t">So that makes it-- I think these trust region methods and natural policy gradient methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4584" target="_blank">01:16:24.820</a></span> | <span class="t">are more sample efficient than the vanilla methods, because you end up-- you're doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4591" target="_blank">01:16:31.220</a></span> | <span class="t">more than one gradient update with each little bit of data you collect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4594" target="_blank">01:16:34.920</a></span> | <span class="t">So with the vanilla policy gradient, you just compute one little gradient estimate, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4599" target="_blank">01:16:39.900</a></span> | <span class="t">then you throw it away.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4601" target="_blank">01:16:41.220</a></span> | <span class="t">With natural policy gradient, you're solving a little optimization problem with it, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4604" target="_blank">01:16:44.740</a></span> | <span class="t">you get more juice out of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4608" target="_blank">01:16:48.540</a></span> | <span class="t">So that's what we have in the policy gradient world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4614" target="_blank">01:16:54.060</a></span> | <span class="t">And in the Q-function world, we have the DQN algorithm and some of its relatives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4621" target="_blank">01:17:01.900</a></span> | <span class="t">And these are sort of descendants of value iteration, where you're approximating the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4629" target="_blank">01:17:09.500</a></span> | <span class="t">Bellman backup using value iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4633" target="_blank">01:17:13.360</a></span> | <span class="t">And then SARSA is also-- it's related to policy iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4641" target="_blank">01:17:21.260</a></span> | <span class="t">These are both different-- I mean, these are estimating different-- they're dealing with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4645" target="_blank">01:17:25.580</a></span> | <span class="t">different Bellman equations, so it's kind of interesting that both kinds of methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4649" target="_blank">01:17:29.100</a></span> | <span class="t">work and they all-- they're both-- they have fairly similar behaviors, as it turns out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4655" target="_blank">01:17:35.980</a></span> | <span class="t">So here's what I would say that-- here's how I would compare them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4660" target="_blank">01:17:40.340</a></span> | <span class="t">And this is anecdotal evidence, but I think this is the consensus right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4667" target="_blank">01:17:47.060</a></span> | <span class="t">The Q-function methods are more sample efficient when they work, but they don't work as generally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4672" target="_blank">01:17:52.580</a></span> | <span class="t">as policy gradient methods, and it's a little harder to figure out what's going on when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4677" target="_blank">01:17:57.260</a></span> | <span class="t">they don't work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4678" target="_blank">01:17:58.940</a></span> | <span class="t">And that kind of makes sense, because in the policy gradient methods, you're optimizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4681" target="_blank">01:18:01.980</a></span> | <span class="t">exactly the thing you care about with gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4685" target="_blank">01:18:05.380</a></span> | <span class="t">Whereas with Q-function methods, you're doing something indirect, where you're trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4689" target="_blank">01:18:09.660</a></span> | <span class="t">learn a Q-function, and then you're hoping that it gives you a good policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4694" target="_blank">01:18:14.820</a></span> | <span class="t">And yeah, so I would also point out that there are also some confounds, so it's hard to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4700" target="_blank">01:18:20.660</a></span> | <span class="t">a good conclusion at this point, because people use different time horizons in the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4708" target="_blank">01:18:28.460</a></span> | <span class="t">gradient methods versus the Q-function methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4710" target="_blank">01:18:30.980</a></span> | <span class="t">So they do one-step lookaheads on the Q-functions and multi-step lookaheads on the policy gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4716" target="_blank">01:18:36.300</a></span> | <span class="t">So it's not clear if the differences come from using different time horizons or some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4723" target="_blank">01:18:43.660</a></span> | <span class="t">differences in how the algorithms are working, because you're either doing regression for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4727" target="_blank">01:18:47.860</a></span> | <span class="t">a Q-function versus learning a policy using policy gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4733" target="_blank">01:18:53.980</a></span> | <span class="t">So just to summarize it, I would say here are some of our core model-free reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4739" target="_blank">01:18:59.220</a></span> | <span class="t">learning algorithms, and they - oh, whoops, I'm missing a word in the first column, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4747" target="_blank">01:19:07.300</a></span> | <span class="t">I think should say reliability and robustness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4751" target="_blank">01:19:11.860</a></span> | <span class="t">So this just means, is it going to work on new problems without parameter tuning, or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4759" target="_blank">01:19:19.420</a></span> | <span class="t">is it going to mysteriously either work or not work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4764" target="_blank">01:19:24.260</a></span> | <span class="t">So this would be my slightly sloppy summary of all these different algorithms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4772" target="_blank">01:19:32.460</a></span> | <span class="t">I would say there's still some room for improvement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4775" target="_blank">01:19:35.540</a></span> | <span class="t">There might be some improvements in the basic methods, because there are some nice properties</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4779" target="_blank">01:19:39.540</a></span> | <span class="t">of the Q-function methods that we don't have in the policy gradient methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4784" target="_blank">01:19:44.260</a></span> | <span class="t">Like you can easily do off - you can easily explore with a different policy than the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4789" target="_blank">01:19:49.780</a></span> | <span class="t">that you're learning the Q-function for, and that's really important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4794" target="_blank">01:19:54.500</a></span> | <span class="t">You can't do that very easily with policy gradient methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4798" target="_blank">01:19:58.380</a></span> | <span class="t">Whereas the policy gradient methods just seem like they're more - you can just apply them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4803" target="_blank">01:20:03.380</a></span> | <span class="t">and they're more likely to work, and it's well understood what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4810" target="_blank">01:20:10.100</a></span> | <span class="t">So I think, yeah, there's still - I don't know if it's possible to get the best of both</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4813" target="_blank">01:20:13.980</a></span> | <span class="t">worlds, but that's the hope.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4818" target="_blank">01:20:18.060</a></span> | <span class="t">And that's it for my talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4819" target="_blank">01:20:19.540</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4820" target="_blank">01:20:20.540</a></span> | <span class="t">Any questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4821" target="_blank">01:20:21.540</a></span> | <span class="t">In model-based reinforcement learning, what lines of research do you find most interesting?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4839" target="_blank">01:20:39.540</a></span> | <span class="t">Oh yeah, so in model-based reinforcement learning, what lines of research do I find most interesting?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4847" target="_blank">01:20:47.340</a></span> | <span class="t">I think the work from my colleagues on guided policy search is very nice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4851" target="_blank">01:20:51.260</a></span> | <span class="t">So I would say that's a kind of model-based reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4856" target="_blank">01:20:56.740</a></span> | <span class="t">I also like - there's some methods that are using the model for faster learning, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4862" target="_blank">01:21:02.060</a></span> | <span class="t">for variance reduction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4863" target="_blank">01:21:03.140</a></span> | <span class="t">So there's a paper called stochastic value gradients that I like a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4870" target="_blank">01:21:10.460</a></span> | <span class="t">I think it's a pretty wide open area.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4872" target="_blank">01:21:12.640</a></span> | <span class="t">So I don't think there have been a lot of really compelling results where you're able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4878" target="_blank">01:21:18.060</a></span> | <span class="t">to learn extremely fast, like you're able to learn with much better sample efficiency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4883" target="_blank">01:21:23.060</a></span> | <span class="t">using a model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4884" target="_blank">01:21:24.060</a></span> | <span class="t">So it seems like that should be possible, but I don't think it's been demonstrated yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4889" target="_blank">01:21:29.980</a></span> | <span class="t">So maybe in the next couple of years we'll see that happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4894" target="_blank">01:21:34.060</a></span> | <span class="t">Hello.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4895" target="_blank">01:21:35.060</a></span> | <span class="t">Hi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4896" target="_blank">01:21:36.060</a></span> | <span class="t">Thanks for the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4900" target="_blank">01:21:40.340</a></span> | <span class="t">So I have a question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4901" target="_blank">01:21:41.340</a></span> | <span class="t">Is it true or not true that most of this problem requires some kind of simulated world to run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4909" target="_blank">01:21:49.540</a></span> | <span class="t">experiments in the episodes, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4911" target="_blank">01:21:51.700</a></span> | <span class="t">Oh yeah, so are you asking does this work in the real world?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4917" target="_blank">01:21:57.500</a></span> | <span class="t">Is that the question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4918" target="_blank">01:21:58.500</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4919" target="_blank">01:21:59.500</a></span> | <span class="t">Yeah, I would say it does work if you have a lot of patience and you're willing to execute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4925" target="_blank">01:22:05.620</a></span> | <span class="t">this thing for a while.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4926" target="_blank">01:22:06.820</a></span> | <span class="t">So the locomotion results I showed add up to about two weeks of real time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4932" target="_blank">01:22:12.840</a></span> | <span class="t">So it's actually not that bad, especially when you consider that toddlers take a while</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4938" target="_blank">01:22:18.020</a></span> | <span class="t">to learn how to walk properly, even though evolution already puts in a lot of built-in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4942" target="_blank">01:22:22.660</a></span> | <span class="t">information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4944" target="_blank">01:22:24.820</a></span> | <span class="t">So I'd say maybe, yeah, I'd say it can be run in the real world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4951" target="_blank">01:22:31.620</a></span> | <span class="t">Some of my colleagues in Berkeley are doing some experiments where they are running just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4956" target="_blank">01:22:36.380</a></span> | <span class="t">regular reinforcement learning algorithms in the real world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4959" target="_blank">01:22:39.980</a></span> | <span class="t">Very brave.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4961" target="_blank">01:22:41.300</a></span> | <span class="t">But I hope to see some nice results from that soon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4965" target="_blank">01:22:45.860</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4967" target="_blank">01:22:47.860</a></span> | <span class="t">Hi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4968" target="_blank">01:22:48.860</a></span> | <span class="t">Thanks for your talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4969" target="_blank">01:22:49.860</a></span> | <span class="t">Here, on the other side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4970" target="_blank">01:22:50.860</a></span> | <span class="t">Here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4971" target="_blank">01:22:51.860</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4972" target="_blank">01:22:52.860</a></span> | <span class="t">I was wondering what was your intuition on the lost surface of those deep reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4979" target="_blank">01:22:59.620</a></span> | <span class="t">learning optimization problems, and maybe especially how it evolves as the policy learns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4988" target="_blank">01:23:08.900</a></span> | <span class="t">And I should specify in the policy gradient case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4993" target="_blank">01:23:13.100</a></span> | <span class="t">So I think the situation is a little bit different in reinforcement learning from in supervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4996" target="_blank">01:23:16.900</a></span> | <span class="t">learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=4997" target="_blank">01:23:17.900</a></span> | <span class="t">So in reinforcement learning, you have one kind of local minima in policy space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5009" target="_blank">01:23:29.300</a></span> | <span class="t">So for example, let's say you want your-- so I keep going back to the locomotion example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5014" target="_blank">01:23:34.340</a></span> | <span class="t">because I spent a lot of time on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5016" target="_blank">01:23:36.020</a></span> | <span class="t">But let's say you want your robot to walk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5019" target="_blank">01:23:39.260</a></span> | <span class="t">There's one local minimum where it just stands and it doesn't bother to walk, because there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5022" target="_blank">01:23:42.980</a></span> | <span class="t">too much penalty for falling over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5024" target="_blank">01:23:44.980</a></span> | <span class="t">And there's another local minimum where it just dives forward, because it gets a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5029" target="_blank">01:23:49.140</a></span> | <span class="t">bit of reward for that before it falls to its doom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5035" target="_blank">01:23:55.040</a></span> | <span class="t">So I think that that's actually the hard part about the optimization problem, is because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5043" target="_blank">01:24:03.920</a></span> | <span class="t">of the different space of behaviors, and actually has nothing to do with the neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5049" target="_blank">01:24:09.520</a></span> | <span class="t">So I've also found that it matters surprisingly little what kind of neural network architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5058" target="_blank">01:24:18.300</a></span> | <span class="t">you use, because I think that most of the hardness and the weirdness of the problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5061" target="_blank">01:24:21.960</a></span> | <span class="t">comes from what the behavior space looks like, rather than what the actual numerical optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5068" target="_blank">01:24:28.360</a></span> | <span class="t">landscape looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5070" target="_blank">01:24:30.200</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5071" target="_blank">01:24:31.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5074" target="_blank">01:24:34.320</a></span> | <span class="t">So there are many problems where the reward is only observed at the end of the task, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5081" target="_blank">01:24:41.960</a></span> | <span class="t">in the final-- in the terminal state in each episode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5085" target="_blank">01:24:45.280</a></span> | <span class="t">And you don't see rewards in intermediate states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5088" target="_blank">01:24:48.120</a></span> | <span class="t">So how much harder do these problems become for deep reinforcement learning in your experience?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5093" target="_blank">01:24:53.560</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5094" target="_blank">01:24:54.560</a></span> | <span class="t">Yeah, so you have-- if you don't get the reward until the end, then you can't-- well, then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5102" target="_blank">01:25:02.520</a></span> | <span class="t">it's probably-- it might be harder to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5104" target="_blank">01:25:04.480</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5105" target="_blank">01:25:05.480</a></span> | <span class="t">I don't have anything precise to say about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5108" target="_blank">01:25:08.880</a></span> | <span class="t">I think it's going to be harder if you have less-- if your rewards are further away.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5114" target="_blank">01:25:14.520</a></span> | <span class="t">So for example, in your video, for the last example of getting up and getting the head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5119" target="_blank">01:25:19.600</a></span> | <span class="t">above a certain height, for example, that could be one where you only get a plus 1 if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5125" target="_blank">01:25:25.120</a></span> | <span class="t">you're above and you don't get anything below.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5127" target="_blank">01:25:27.160</a></span> | <span class="t">Oh, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5128" target="_blank">01:25:28.160</a></span> | <span class="t">But when you're doing something that was kind of-- if you get your head higher, then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5130" target="_blank">01:25:30.960</a></span> | <span class="t">still get something partial.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5132" target="_blank">01:25:32.520</a></span> | <span class="t">Yeah, so I think we came up with a reward like distance from height squared, which made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5138" target="_blank">01:25:38.880</a></span> | <span class="t">the problem easier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5139" target="_blank">01:25:39.880</a></span> | <span class="t">Yeah, the problem would have been a lot harder if you get zero reward until you get your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5143" target="_blank">01:25:43.960</a></span> | <span class="t">head above the height.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5145" target="_blank">01:25:45.560</a></span> | <span class="t">And it's actually-- that would be a problem of exploration, which is that you have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5151" target="_blank">01:25:51.920</a></span> | <span class="t">explore all the different states to figure out where you're going to get good reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5156" target="_blank">01:25:56.520</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5157" target="_blank">01:25:57.520</a></span> | <span class="t">One last question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5159" target="_blank">01:25:59.560</a></span> | <span class="t">OK, one last question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5162" target="_blank">01:26:02.720</a></span> | <span class="t">So I have a question about how do you choose to quantize your space time, because in your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5166" target="_blank">01:26:06.640</a></span> | <span class="t">locomotion example, it clearly has a continuous system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5170" target="_blank">01:26:10.920</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5171" target="_blank">01:26:11.920</a></span> | <span class="t">Yeah, so it's actually really important how you discretize time, like what time step you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5176" target="_blank">01:26:16.840</a></span> | <span class="t">use, because the algorithm has-- I mean, the algorithm does care about what the time step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5184" target="_blank">01:26:24.600</a></span> | <span class="t">is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5185" target="_blank">01:26:25.600</a></span> | <span class="t">So it's not like-- yeah, because you have discount factors, and you're also sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5192" target="_blank">01:26:32.240</a></span> | <span class="t">a different action at every time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5194" target="_blank">01:26:34.080</a></span> | <span class="t">So yeah, so if you choose too small of a time step, then the rewards will be delayed by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5203" target="_blank">01:26:43.520</a></span> | <span class="t">more time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5204" target="_blank">01:26:44.760</a></span> | <span class="t">So that makes the credit assignment harder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5207" target="_blank">01:26:47.560</a></span> | <span class="t">And also, your exploration will be more like a random walk, because you're changing your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5212" target="_blank">01:26:52.480</a></span> | <span class="t">minds really frequently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5214" target="_blank">01:26:54.960</a></span> | <span class="t">So yeah, the time step is pretty important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5216" target="_blank">01:26:56.840</a></span> | <span class="t">And I'd say that's a flaw in current methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5220" target="_blank">01:27:00.800</a></span> | <span class="t">OK, thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5222" target="_blank">01:27:02.480</a></span> | <span class="t">So at the same time, John will give a thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5229" target="_blank">01:27:09.760</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5230" target="_blank">01:27:10.760</a></span> | <span class="t">So take a short break.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5232" target="_blank">01:27:12.280</a></span> | <span class="t">We convene in 15 minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo&t=5234" target="_blank">01:27:14.040</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>