<html><head><title>Latent Space Paper Club: AIEWF Special Edition (Test of Time, DeepSeek R1/V3) — VIbhu Sapra</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Latent Space Paper Club: AIEWF Special Edition (Test of Time, DeepSeek R1/V3) — VIbhu Sapra</h2><a href="https://www.youtube.com/watch?v=9k3xPh-40mo"><img src="https://i.ytimg.com/vi_webp/9k3xPh-40mo/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=0">0:0</a> Paper Club Year in Review & Future Plans<br><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=480">8:0</a> DeepSeek Paper Discussion<br><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=550">9:10</a> DeepSeek R1 (May 28th Update)<br><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=760">12:40</a> DeepSeek Distillation<br><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1011">16:51</a> Original DeepSeek Model Overview (DeepSeek V3 and R1)<br><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1275">21:15</a> Development of reasoning capabilities through a pure RL process<br><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1486">24:46</a> DeepSeek R10<br><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2345">39:5</a> DeepSeek R1 four-stage training pipeline<br><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2655">44:15</a> Distillation Strategy<br><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3154">52:34</a> Community and Call to Action<br><br><div style="text-align: left;"><a href="./9k3xPh-40mo.html">Whisper Transcript</a> | <a href="./transcript_9k3xPh-40mo.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Okay, so PaperClub year in review. We've gone over a year, like a year and a half,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=18" target="_blank">00:00:18.640</a></span> | <span class="t">no missed weeks, we've always done PaperClub. And it's pretty interesting, you know, I don't think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=23" target="_blank">00:00:23.960</a></span> | <span class="t">any of us expected it to get this far, but every week for the past year and a half, we have always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=29" target="_blank">00:00:29.560</a></span> | <span class="t">done a PaperClub Wednesday at noon. We've had a bunch of authors come share their work, people from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=34" target="_blank">00:00:34.480</a></span> | <span class="t">Nvidia, Meta, Allen AI, Amazon Together, Ryder, bunch of authors come, we get direct one-hour sessions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=41" target="_blank">00:00:41.140</a></span> | <span class="t">with them, they share their work, we get nice feedback, and you know, some of these have started</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=45" target="_blank">00:00:45.340</a></span> | <span class="t">to do pretty well. On average, we get about 100 people in here every session, and just for, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=51" target="_blank">00:00:51.460</a></span> | <span class="t">know, for information, that's like on a Wednesday, workday at noon, we have 100 people joined to discuss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=59" target="_blank">00:00:59.120</a></span> | <span class="t">a random Paper. And some of the big ones, DeepSeek V3 had 300 live people sitting, listening to me yap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=66" target="_blank">00:01:06.500</a></span> | <span class="t">about DeepSeek. Of course, all the other speakers do great, this is all built on volunteers, right? And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=72" target="_blank">00:01:12.740</a></span> | <span class="t">along the way, we made many friends, and yeah, PaperClub went much further than we expected. Now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=78" target="_blank">00:01:18.260</a></span> | <span class="t">the launch, you know, we have to ship something, we basically have to launch at World's Fair. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=83" target="_blank">00:01:23.840</a></span> | <span class="t">we're launching our test of time PaperClub. This is going to be a V2 second PaperClub. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=89" target="_blank">00:01:29.480</a></span> | <span class="t">the one that we do now are Wednesday at noon, this is still sticking around, it will still be the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=93" target="_blank">00:01:33.800</a></span> | <span class="t">thing. Every week, we'll kind of have a paper, something that's trending, cover it, have an author,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=98" target="_blank">00:01:38.660</a></span> | <span class="t">have our Q&A session, you know, 30 minutes of paper presentation, whether that's highlights or that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=103" target="_blank">00:01:43.220</a></span> | <span class="t">slides. And then we'll continue that with some discussion. But test of time PaperClub is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=108" target="_blank">00:01:48.320</a></span> | <span class="t">to be a little bit different, you know? So, it's more curriculum based. Basically, we take this idea of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=112" target="_blank">00:01:52.940</a></span> | <span class="t">what's everything that you would need to know to be a good AI engineer? What are the themes? What are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=118" target="_blank">00:01:58.160</a></span> | <span class="t">core papers that don't change? So, stuff like, you know, what is attention? How does sequential text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=123" target="_blank">00:02:03.800</a></span> | <span class="t">generation work? So, what's going on in GPT-2? How do, like, optimizers work? What about, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=128" target="_blank">00:02:08.540</a></span> | <span class="t">the key inference techniques, right? So, stuff like speculative decoding, flash attention, stable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=134" target="_blank">00:02:14.200</a></span> | <span class="t">diffusion, whisper, the key papers that, you know, are the foundation of what's being built. We're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=139" target="_blank">00:02:19.280</a></span> | <span class="t">to kind of group those together, and session by session, we'll go over them. So, we're going to kick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=144" target="_blank">00:02:24.040</a></span> | <span class="t">off in July and run till December. That leaves us six months. Six months is about four weeks a month,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=149" target="_blank">00:02:29.560</a></span> | <span class="t">you know, 24 weeks. Every week, we plan to go through two to four pre-presented papers. So, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=155" target="_blank">00:02:35.140</a></span> | <span class="t">kind of have a bit more structure to it. There'll be presentations, but, you know, in six months, we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=160" target="_blank">00:02:40.240</a></span> | <span class="t">get through, like, 50 to 100 papers, and we can cover the core concepts. And every week will be kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=166" target="_blank">00:02:46.300</a></span> | <span class="t">different, right? So, like, one week, we might be talking about whisper, and you're interested, right? In one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=170" target="_blank">00:02:50.600</a></span> | <span class="t">week, you can get the fundamentals of speech, speech-to-text, text-to-speech, all that stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=175" target="_blank">00:02:55.520</a></span> | <span class="t">Another day, it might be something like image generation. So, you know, we'll go over clip,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=179" target="_blank">00:02:59.120</a></span> | <span class="t">stable diffusion, how does all this stuff work? Extend it out to video. Basically, we'll segment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=185" target="_blank">00:03:05.000</a></span> | <span class="t">the core topics that we should need to know, and then we'll cover it every week. The exciting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=190" target="_blank">00:03:10.080</a></span> | <span class="t">announcement here is we're also, for the first time, having SF section. So, we're gonna do in-person SF</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=197" target="_blank">00:03:17.720</a></span> | <span class="t">Clubs and a remote section. I am gonna mute someone real quick. So, we've got 40 a few. Someone needs to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=206" target="_blank">00:03:26.720</a></span> | <span class="t">be muted. We've got crazy echo. Oh, crazy. I'll just mute myself. Easy. I'll mute my own laptop. Okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=212" target="_blank">00:03:32.660</a></span> | <span class="t">continuing on. If someone needs to interrupt, just start in Zoom check, because I don't have my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=217" target="_blank">00:03:37.760</a></span> | <span class="t">speakers going. But yeah, we'll have an in-person session in San Francisco every week, and of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=223" target="_blank">00:03:43.940</a></span> | <span class="t">we'll keep the remote thing. So, we'll keep it going, and Original Paper Club will still be its own.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=228" target="_blank">00:03:48.380</a></span> | <span class="t">Every week something comes out, we'll cover that, and we won't really deviate too much on the schedule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=232" target="_blank">00:03:52.940</a></span> | <span class="t">for test of time. It's gonna be foundational papers, plus some blogs. So, what are the topics? You know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=238" target="_blank">00:03:58.940</a></span> | <span class="t">this is still up for us to all decide. So, just listing out some of the few ones here. You know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=244" target="_blank">00:04:04.760</a></span> | <span class="t">we have foundations of deep learning. Something like attention, optimization, ReLU, gradient descent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=250" target="_blank">00:04:10.700</a></span> | <span class="t">what is basic RL, how do these things work? So, you know, we'll have a section on foundations of deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=256" target="_blank">00:04:16.100</a></span> | <span class="t">learning. Another one, LLM foundations, right? So, pre-LLMs, we have like RNNs, LSTMs, bi-directional RNNs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=265" target="_blank">00:04:25.760</a></span> | <span class="t">all this stuff. Then we have like the very, very foundational models, right? So like BERT from Google,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=270" target="_blank">00:04:30.960</a></span> | <span class="t">GPT-2, stuff like that. We'll have a day of everything kind of pre-LLM, and go over those. Then after that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=277" target="_blank">00:04:37.760</a></span> | <span class="t">we'll have, you know, the actual like generative LLMs. I'm sure that's missing here, but you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=282" target="_blank">00:04:42.320</a></span> | <span class="t">like LAMA 3, Deep Seek, that the core LLMs that you would expect to hear about, we'll go over those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=287" target="_blank">00:04:47.480</a></span> | <span class="t">We'll have a day of pre-mid, post-training, right? So what are the scaling law papers? What is chinchilla?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=293" target="_blank">00:04:53.000</a></span> | <span class="t">How does distillation work? What are the kind of key papers that you would want to know for training?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=298" target="_blank">00:04:58.640</a></span> | <span class="t">And once again, this will be like a one to two sessions. So in one week, we'll go over three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=304" target="_blank">00:05:04.460</a></span> | <span class="t">to four papers. We'll have someone present. So come prepared. We'll go over, you know, what are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=309" target="_blank">00:05:09.620</a></span> | <span class="t">fundamentals in scaling laws, distillation? What is chinchilla scaling? What is overtraining? What is LAMA</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=315" target="_blank">00:05:15.600</a></span> | <span class="t">scaling laws? What are small model scaling laws? So like, the PHY team really has, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=320" target="_blank">00:05:20.160</a></span> | <span class="t">for small language models, how should we do proper RL? What is post-training for small models versus big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=325" target="_blank">00:05:25.360</a></span> | <span class="t">models? Well, we'll have someone cover all these, and that'll be a one to two-week section. Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=329" target="_blank">00:05:29.720</a></span> | <span class="t">generative models, so you know, clips, or a segment, anything, diffusion, some of the key agent papers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=335" target="_blank">00:05:35.220</a></span> | <span class="t">fine-tuning, we'll go over LORA, QLORA, DPO, RL, GRPO, voice, we'll have whisper, optimization stage,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=343" target="_blank">00:05:43.560</a></span> | <span class="t">so you know, speculative decoding, flash attention, we'll have eval tracks, Rexis,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=348" target="_blank">00:05:48.120</a></span> | <span class="t">Eugene Yan, you know, he hosts our original paper club, he'll fill in good ones there, much, much more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=352" target="_blank">00:05:52.840</a></span> | <span class="t">So yeah, you know, these categories are still up for debate. So I have a form here later,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=359" target="_blank">00:05:59.400</a></span> | <span class="t">fill it out on stuff that you would want to add, any papers that you recommend, any topics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=364" target="_blank">00:06:04.840</a></span> | <span class="t">it's very straightforward. You know, also join our discord, we have the paper club channel in discord. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=370" target="_blank">00:06:10.120</a></span> | <span class="t">join that, add in topics, anything, if you want to cover, you know, if you want to volunteer to be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=375" target="_blank">00:06:15.800</a></span> | <span class="t">speaker, over the next like week, I'll flesh out a rough schedule. And then from there, we'll kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=382" target="_blank">00:06:22.040</a></span> | <span class="t">take it, you know, start covering it, make sure that every session we have speakers, we'll figure out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=386" target="_blank">00:06:26.600</a></span> | <span class="t">logistics. So SF will have a venue, we have a few in mind, remote, it'll be the same Zoom thing. But yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=393" target="_blank">00:06:33.560</a></span> | <span class="t">we want to we want to fragment this out, get people get people interested, we'll obviously still bring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=398" target="_blank">00:06:38.280</a></span> | <span class="t">in speakers, you know, so some of these key authors, we know them. So we'll invite them to speak on these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=403" target="_blank">00:06:43.800</a></span> | <span class="t">papers, and it'll be good, we'll have discussion sessions, these these sessions might be a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=408" target="_blank">00:06:48.360</a></span> | <span class="t">bit more than an hour. Since we now have three to four papers, we still want to just go deep, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=413" target="_blank">00:06:53.160</a></span> | <span class="t">we don't want to just like do a TLDR of the paper, we want to do what are the foundation, what are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=417" target="_blank">00:06:57.160</a></span> | <span class="t">fundamentals, and then go a little bit deeper. So stay in touch on Discord, very, very basic Google</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=423" target="_blank">00:07:03.640</a></span> | <span class="t">form. But yeah, now we'll have curriculum based stuff. You know, like Flo here might talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=429" target="_blank">00:07:09.160</a></span> | <span class="t">music generation, Eugene will talk about states based stuff. So key, same members, but just yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=435" target="_blank">00:07:15.320</a></span> | <span class="t">second paper club. Now, the other part of this is, you know, if you kind of have an area of interest,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=441" target="_blank">00:07:21.960</a></span> | <span class="t">this will be like your go to session of, you know, here are the five to 10 papers, here's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=447" target="_blank">00:07:27.960</a></span> | <span class="t">presentation on them, here's discussion, and it will stay live on YouTube, you know, it's just you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=452" target="_blank">00:07:32.680</a></span> | <span class="t">kind of set in at any time and be caught up to date, you'll you'll kind of go fundamentals to what you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=458" target="_blank">00:07:38.520</a></span> | <span class="t">to know for every session. And of course, we'll have the same lively discussion. Okay, test of time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=463" target="_blank">00:07:43.480</a></span> | <span class="t">paper club, very, very hype. But today is still paper club, we can't not do a paper. So you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=469" target="_blank">00:07:49.080</a></span> | <span class="t">this is just mochi picture because everyone at the conference is having dogs in their slides. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=473" target="_blank">00:07:53.560</a></span> | <span class="t">I need them too. So we can't just have an old paper club back here, we need our OG. So today's paper is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=480" target="_blank">00:08:00.440</a></span> | <span class="t">going to be Deep Seek. So Deep Seek was obviously a popular paper. I know a lot of people haven't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=487" target="_blank">00:08:07.080</a></span> | <span class="t">had a chance to actually go through the paper. And frankly, I didn't have much time to prep. So you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=492" target="_blank">00:08:12.280</a></span> | <span class="t">I get to reuse my slides, I'm smart like that. This is also being recorded for the broader AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=497" target="_blank">00:08:17.400</a></span> | <span class="t">engineer conference workshops and speakers. So it's another point, you know, this is why we do paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=503" target="_blank">00:08:23.080</a></span> | <span class="t">club, we get these discussions on papers out, and then you know, people can find them later, like our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=509" target="_blank">00:08:29.160</a></span> | <span class="t">original Deep Seek paper reading, basically, like, you know, that was just last minute, let's highlight the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=514" target="_blank">00:08:34.440</a></span> | <span class="t">paper, make some basic slides, have some discussion. But guess what, 300 people joined live, there's over a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=519" target="_blank">00:08:39.160</a></span> | <span class="t">1000 views on a YouTube video of us just reading through a paper. So it's a pretty key paper. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=525" target="_blank">00:08:45.240</a></span> | <span class="t">like one of the big open source papers models that kind of had a big transition. So let's just go over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=530" target="_blank">00:08:50.120</a></span> | <span class="t">it again. And of course, there is new stuff. So as of this week, or last week, we have Deep Seek R1 0528.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=538" target="_blank">00:08:58.680</a></span> | <span class="t">So basically, the May 28th update. Now, you know, there have been rumors that okay, Deep Seek v2 is coming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=545" target="_blank">00:09:05.880</a></span> | <span class="t">out, it's coming out, and then people start launching stuff. Guess what, they didn't do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=549" target="_blank">00:09:09.160</a></span> | <span class="t">They just did the same model, they called it a minor update. But it's actually not that small of an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=553" target="_blank">00:09:13.560</a></span> | <span class="t">update. So let's let's dig into what it basically is. It's not v3, like revision two, it's the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=559" target="_blank">00:09:19.320</a></span> | <span class="t">naming scheme, but it's significantly better, actually. So, yep, Simon Wilson in his keynote mentioned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=567" target="_blank">00:09:27.640</a></span> | <span class="t">how we don't have good naming for models. This is basically quite a step up, but they've kept the name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=573" target="_blank">00:09:33.320</a></span> | <span class="t">So also plugging Simon's talk, he gave a keynote for the past six months, what were the 30 models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=581" target="_blank">00:09:41.080</a></span> | <span class="t">He launched Pelican Bench. It's his own benchmark of how he judges how good foundation models are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=585" target="_blank">00:09:45.800</a></span> | <span class="t">He needs to check the new Deep Seek model. I don't think he did. But basically, here's what they did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=590" target="_blank">00:09:50.360</a></span> | <span class="t">They did better post training on Deep Seek v3. And now guess what, it got better. So some of this stuff,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=598" target="_blank">00:09:58.600</a></span> | <span class="t">they put out very little information. But when you dig in, you see that one of the key things is it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=602" target="_blank">00:10:02.920</a></span> | <span class="t">much better at reasoning. So the AIME 2024 score went from 70% to 87.5%. That basically means, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=611" target="_blank">00:10:11.400</a></span> | <span class="t">this is a good reasoning benchmark. From before, now we have v3 matching the performance of O3 and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=617" target="_blank">00:10:17.960</a></span> | <span class="t">2.5 level on math, coding and reasoning, which is quite a step up. You know, it used to be like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=623" target="_blank">00:10:23.960</a></span> | <span class="t">okay, we have O1 level intelligence and open source. Yeah, all we needed was a little bit more training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=630" target="_blank">00:10:30.520</a></span> | <span class="t">And now we have O3 and 2.5 level intelligence. And this isn't even like a new model from them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=635" target="_blank">00:10:35.560</a></span> | <span class="t">This is just like, let's do a little bit better post training. Let's do a little bit more,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=640" target="_blank">00:10:40.360</a></span> | <span class="t">and we can get significant, significant performance increases. Pretty wild, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=645" target="_blank">00:10:45.880</a></span> | <span class="t">Like 18% improvement on benchmarks. And no one's really talking about this. Deep Seek got a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=652" target="_blank">00:10:52.120</a></span> | <span class="t">better. One of the quotes that they say is originally the original Deep Seek v3, it would take 12,000 tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=659" target="_blank">00:10:59.880</a></span> | <span class="t">to reason through on average on the benchmark. For an AIME, it would take 12,000 tokens of reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=666" target="_blank">00:11:06.040</a></span> | <span class="t">They basically did more RL. Now it reasons more. On average, it reasons for 25,000 tokens. So they got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=673" target="_blank">00:11:13.000</a></span> | <span class="t">the model to do double the reasoning. One thing that we talk a lot about is scaling laws, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=678" target="_blank">00:11:18.840</a></span> | <span class="t">So before we would do optimal scaling for base models. Now in Llama, we kind of did overfit our training for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=686" target="_blank">00:11:26.200</a></span> | <span class="t">inference time, right? Let's really over train so we can do fast basic inference. And then now in this world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=692" target="_blank">00:11:32.360</a></span> | <span class="t">of test time compute, where we do more inference time compute, we can also scale even more in that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=699" target="_blank">00:11:39.320</a></span> | <span class="t">dimension. So original Deep Seek, on average, with reason for 12,000 tokens, the new model can double that. So in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=707" target="_blank">00:11:47.160</a></span> | <span class="t">domain, we've doubled the amount of reasoning it could do. And we have a lot of benchmarks to increase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=711" target="_blank">00:11:51.480</a></span> | <span class="t">You know, 18% on AIME and it's a lot better at coding. So in this, they intentionally wanted to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=718" target="_blank">00:11:58.120</a></span> | <span class="t">do better JSON output, function calling, and more reasoning. And yeah, they just dropped it like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=723" target="_blank">00:12:03.400</a></span> | <span class="t">Here's kind of our benchmark chart. On most paper clubs, we don't do benchmarks. But yeah, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=729" target="_blank">00:12:09.720</a></span> | <span class="t">actually kind of up there with O3 and Gemini 2.5. So, you know, the darkest color is our new revision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=737" target="_blank">00:12:17.160</a></span> | <span class="t">of Deep Seek R1. And yeah, it's actually very good. On most benchmarks, it's like significantly better than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=744" target="_blank">00:12:24.040</a></span> | <span class="t">the original Deep Seek R1. And you can see this, you know, humanity's last exam, it basically went from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=750" target="_blank">00:12:30.440</a></span> | <span class="t">not being able to do anything to, okay, now this thing can do well. What they did is now it can basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=755" target="_blank">00:12:35.000</a></span> | <span class="t">reason for twice as long. Okay, that's not the only drop. They also launched another distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=761" target="_blank">00:12:41.880</a></span> | <span class="t">This is kind of the interesting one. Now, not many people really talked about this at all on Twitter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=766" target="_blank">00:12:46.600</a></span> | <span class="t">But if we remember in the original Deep Seek paper, what they did was outside of their original Deep Seek</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=773" target="_blank">00:12:53.640</a></span> | <span class="t">model, they did three distillation models. They distilled the Quen series and the Llama series. And they showed how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=780" target="_blank">00:13:00.280</a></span> | <span class="t">distilling from the big model, distilling on these reasoning traces, we can get really, really good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=785" target="_blank">00:13:05.560</a></span> | <span class="t">performance on small models. Well, they did it again. They took Quen 3-8B, and they did another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=791" target="_blank">00:13:11.400</a></span> | <span class="t">distillation with their new reasoning model. And they show that, you know, the new model, this new basic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=797" target="_blank">00:13:17.000</a></span> | <span class="t">distillation basically like kills the old one. So basically, when you look at their old distill versus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=804" target="_blank">00:13:24.040</a></span> | <span class="t">their new distillation, they get another 10% performance boost by just doing distillation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=809" target="_blank">00:13:29.320</a></span> | <span class="t">from a better reasoning model. So in a few months time, they were able to get Deep Seek to do more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=815" target="_blank">00:13:35.800</a></span> | <span class="t">chain of thought, more reasoning, use that to distill down to an 8B. And now we have an even better 8B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=822" target="_blank">00:13:42.920</a></span> | <span class="t">So very, very interesting little note, right? Not just do we get 10% improvement from the last base model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=828" target="_blank">00:13:48.280</a></span> | <span class="t">their 8B distillation is actually matching performance of Quen 3's 235 billion 20B active</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=837" target="_blank">00:13:57.800</a></span> | <span class="t">thinking model. Horrible naming, I know, but let's take a second to think about that, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=842" target="_blank">00:14:02.840</a></span> | <span class="t">A Quen 3 8B dense model. So a small 8B model is matching the performance of the Quen 3 235B thinking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=852" target="_blank">00:14:12.680</a></span> | <span class="t">model. And this is not a native thinking model, right? This is a base distillation model of an 8 billion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=859" target="_blank">00:14:19.640</a></span> | <span class="t">parameter model. Just on distillation, so logic matching distillation from a big model, we're matching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=865" target="_blank">00:14:25.640</a></span> | <span class="t">performance of their 235 billion MOE thinking model. That's pretty wild. They didn't do this to the 32B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=872" target="_blank">00:14:32.760</a></span> | <span class="t">the 70B, but yeah, it's pretty crazy, right? Untalked about release that the new 8B does very, very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=880" target="_blank">00:14:40.120</a></span> | <span class="t">How do we see this? We can see the chain of thought improvements distilled down really, really hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=886" target="_blank">00:14:46.920</a></span> | <span class="t">This was one of the key findings in the original paper that a better recipe for training small models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=893" target="_blank">00:14:53.480</a></span> | <span class="t">is to distill down from big models, and reasoning models make this even more efficient. And then this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=898" target="_blank">00:14:58.360</a></span> | <span class="t">is just kind of their follow-up, right? We don't have a paper, we don't have too much on this, but these are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=902" target="_blank">00:15:02.120</a></span> | <span class="t">benchmarks that show it. And of course, model is open source, open weight, everything. So those are kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=907" target="_blank">00:15:07.960</a></span> | <span class="t">of the overviews. Let's see, let's see. Now let's go over the actual, the original DeepSeq paper. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=919" target="_blank">00:15:19.400</a></span> | <span class="t">that's kind of ending where we had the new releases. So we have two models to recap. We have a new DeepSeq version.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=926" target="_blank">00:15:26.120</a></span> | <span class="t">So for May 28th, we have a new DeepSeq. It's now on par with OpenAI's O3 and Gemini 2.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=936" target="_blank">00:15:36.760</a></span> | <span class="t">It's significantly better, and it reasons for twice as long. We also took that model, we distilled it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=943" target="_blank">00:15:43.080</a></span> | <span class="t">down to Quen3 8B, and we have a much, much better small 8B reasoning model. This shows that, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=950" target="_blank">00:15:50.520</a></span> | <span class="t">reasoning models distill down very, very efficiently, and there's still a lot of juice to be squeezed out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=955" target="_blank">00:15:55.320</a></span> | <span class="t">there. Now, from here, for those that haven't seen it, we're going to take a two second pause, see if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=960" target="_blank">00:16:00.920</a></span> | <span class="t">there's any interesting questions. There's a hugging face link. If these benchmarks are public, won't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=966" target="_blank">00:16:06.840</a></span> | <span class="t">models be trained to score better on these benchmarks? Yeah, benchmarks obviously have their cons, their flaws,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=972" target="_blank">00:16:12.360</a></span> | <span class="t">but you know, there's ways to see what models are overfit on them. How do they do in general performance?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=977" target="_blank">00:16:17.400</a></span> | <span class="t">In general, the DeepSeq models are actually doing very, very well. So from there, let's go into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=983" target="_blank">00:16:23.080</a></span> | <span class="t">original DeepSeq model. So okay, DeepSeq v3, hypest paper of the year, 300 people joined us live.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=989" target="_blank">00:16:29.960</a></span> | <span class="t">Let's do a quick recap. This is basically me using my old slides because I can, but let's talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=995" target="_blank">00:16:35.720</a></span> | <span class="t">what happened in DeepSeq. So we're going to kick off with a high level model overview. So what are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1000" target="_blank">00:16:40.760</a></span> | <span class="t">models they release? When they release this, it's not just DeepSeq v3, right? They also have R1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1006" target="_blank">00:16:46.360</a></span> | <span class="t">What is inference time training? What is test time compute? What makes reasoning models different?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1011" target="_blank">00:16:51.960</a></span> | <span class="t">So if you guys don't remember, this was the first test time scaling open, you know, open model, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1018" target="_blank">00:16:58.440</a></span> | <span class="t">This was the first one that got good. OpenAI released 01. Cloud released Cloud thinking much later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1024" target="_blank">00:17:04.600</a></span> | <span class="t">Gemini thinking came much later. We didn't really understand what was happening. We thought there was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1029" target="_blank">00:17:09.160</a></span> | <span class="t">like MCTS. There was a lot of, you know, Monte Carlo tree search. What's going on? There was internal,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1035" target="_blank">00:17:15.080</a></span> | <span class="t">let's generate chain of thought. Let's train it to do chain of thought. But it turns out DeepSeq comes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1040" target="_blank">00:17:20.040</a></span> | <span class="t">out with this paper. They do a great model and they're like, yo, RL, RL works. So two models were released.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1046" target="_blank">00:17:26.680</a></span> | <span class="t">DeepSeq R1.0. Basically, they take a base model. They do a lot of GRPO RL. They have training templates, reward models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1055" target="_blank">00:17:35.160</a></span> | <span class="t">They have this emergence capability, reflection, aha moments. Then they do R1, which is basically a four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1063" target="_blank">00:17:43.720</a></span> | <span class="t">stage pipeline. They have cold start, reasoning RL stage, rejection sampling and SFT. And then, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1069" target="_blank">00:17:49.880</a></span> | <span class="t">of course, the little RL round two to get it to really, really reason. From there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1074" target="_blank">00:17:54.840</a></span> | <span class="t">we'll talk about performance and evals of how does original R1 do. How is DeepSeq? Then the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1081" target="_blank">00:18:01.320</a></span> | <span class="t">distillation models, future work, reproductions and whatnot. We'll kind of skip over the base</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1086" target="_blank">00:18:06.440</a></span> | <span class="t">DeepSeq evals and performance because, you know, we already covered the new one. But okay, continuing on. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1094" target="_blank">00:18:14.120</a></span> | <span class="t">high level overview. For those that understand, that don't really follow, you know, we keep hearing this term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1100" target="_blank">00:18:20.360</a></span> | <span class="t">of test time scaling. What is test time scaling? What are thinking models and what does this mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1105" target="_blank">00:18:25.400</a></span> | <span class="t">So basically, we got to a point where we started to overtrain our models. We basically hit a scaling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1112" target="_blank">00:18:32.520</a></span> | <span class="t">limit on how much we can train models. Originally, back in the day, we used to do sort of these chinchilla</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1118" target="_blank">00:18:38.360</a></span> | <span class="t">scaling laws, right? We had a fixed compute budget, we had a fixed amount of data set, we would design</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1124" target="_blank">00:18:44.840</a></span> | <span class="t">a model around that. So how many parameters should it be based on how much data we have? Let's fit a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1130" target="_blank">00:18:50.440</a></span> | <span class="t">model to our data, let's fit how many GPUs we have, and then let's train it to be kind of chinchilla</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1136" target="_blank">00:18:56.440</a></span> | <span class="t">optimal. From there, we kind of realized that, okay, this isn't really what we want. And we started</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1142" target="_blank">00:19:02.680</a></span> | <span class="t">really, really scaling up our training. So we had stuff like, you know, Mistral, Lama 1, Lama 2, Lama 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1148" target="_blank">00:19:08.920</a></span> | <span class="t">we started training these models from billions of tokens, to trillions of tokens. So you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1154" target="_blank">00:19:14.040</a></span> | <span class="t">we had originally like a 1B, 1 trillion token, then Lama 3 was 15 trillion tokens. Now they're up to like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1160" target="_blank">00:19:20.280</a></span> | <span class="t">45 trillion tokens. So what we shifted was, instead of training for, you know, model chinchilla optimal,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1168" target="_blank">00:19:28.520</a></span> | <span class="t">let's start training for this sort of inference optimal training regime. So instead of, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1174" target="_blank">00:19:34.920</a></span> | <span class="t">thinking about what we have now, let's think about inference time. As we scale this, we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1179" target="_blank">00:19:39.800</a></span> | <span class="t">a model to be as densely packed as smart as possible. So Lama is like, okay, basically, if we continue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1185" target="_blank">00:19:45.560</a></span> | <span class="t">training, we don't really see degradation. But the problem in this is, it gets very, very expensive, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1191" target="_blank">00:19:51.160</a></span> | <span class="t">As you train more and more, yeah, it's very, very heavily compute extensive. And you know, how many times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1197" target="_blank">00:19:57.080</a></span> | <span class="t">can you scale this up? Like how much data do we have? How much can we really fit in? If we're at 45</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1202" target="_blank">00:20:02.600</a></span> | <span class="t">trillion parameters for like a 70b, can we scale that up 10x again? You know, are we going to do 450</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1208" target="_blank">00:20:08.680</a></span> | <span class="t">trillion parameters? What if we want to 10x it again? We're basically hitting the compute scale for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1214" target="_blank">00:20:14.360</a></span> | <span class="t">training, right? We can't continually just keep scaling up our train runs because it's no longer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1219" target="_blank">00:20:19.720</a></span> | <span class="t">like cost efficient, right? We're spending millions and millions and hundreds of millions of dollars on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1224" target="_blank">00:20:24.120</a></span> | <span class="t">these train runs. You can only scale so much. So a lot of hypothesis was, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1229" target="_blank">00:20:29.240</a></span> | <span class="t">there's going to be a sort of plateau and open models will start to catch up because, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1235" target="_blank">00:20:35.480</a></span> | <span class="t">we're already all scaling so much. But that's where, you know, we need to unlock another dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1240" target="_blank">00:20:40.680</a></span> | <span class="t">which in this case was reasoning or test time training. So this is where we basically started to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1247" target="_blank">00:20:47.640</a></span> | <span class="t">reasoning capabilities without any supervised data, right? There were approaches to try to do, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1253" target="_blank">00:20:53.560</a></span> | <span class="t">let's generate a bunch of chain of thought reasoning style data. Let's do post training on it. And yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1259" target="_blank">00:20:59.000</a></span> | <span class="t">our models do a little better, but this didn't scale. What we needed was we wanted to do pure RL. Can we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1265" target="_blank">00:21:05.960</a></span> | <span class="t">pure RL to do reasoning data? So this is a quote from the paper. Basically, the DeepSeq team says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1273" target="_blank">00:21:13.800</a></span> | <span class="t">"Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1281" target="_blank">00:21:21.000</a></span> | <span class="t">focusing on their self evolution through a pure RL process." So what they're going to do is they're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1287" target="_blank">00:21:27.000</a></span> | <span class="t">to post train the DeepSeq V3 base with GRPO, which is, you know, basically pure RL. And then as they do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1294" target="_blank">00:21:34.360</a></span> | <span class="t">this, they start to notice emergence of great reasoning, reflection, aha moment, and they start to match O1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1301" target="_blank">00:21:41.160</a></span> | <span class="t">Now, fast forward a few months, as we can see today, they can basically continue this. They do a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1307" target="_blank">00:21:47.880</a></span> | <span class="t">bit more RL. They do RL on longer traces. They can now get the model to not match O1, but match O3 and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1315" target="_blank">00:21:55.960</a></span> | <span class="t">double its amount of reasoning tokens. Okay, here's kind of the four step approach to how they train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1321" target="_blank">00:22:01.560</a></span> | <span class="t">this R1 model. They start with this sort of cold start where, you know, they jump start with some SFT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1327" target="_blank">00:22:07.640</a></span> | <span class="t">then they do RL for reasoning, then they have this key step of rejection sampling for generation purposes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1332" target="_blank">00:22:12.840</a></span> | <span class="t">You know, rejection sampling is something we'll talk about later. Then once again, they do the fourth stage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1337" target="_blank">00:22:17.160</a></span> | <span class="t">of basic RL polishing. Okay, that's a very high level overview of what DeepSeq did to make RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1345" target="_blank">00:22:25.560</a></span> | <span class="t">So to recap, you know, we needed to shift from next token predictors to scaling in another axis. To do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1352" target="_blank">00:22:32.760</a></span> | <span class="t">this, we needed to scale on instead of spending the same compute for every token generated, we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1359" target="_blank">00:22:39.160</a></span> | <span class="t">dynamically generate, we want dynamically spend more compute on different queries. So we train the models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1365" target="_blank">00:22:45.640</a></span> | <span class="t">now with pure RL to reason through their questions. So instead, you know, they're now trained to do RL with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1372" target="_blank">00:22:52.520</a></span> | <span class="t">verifiable outputs on a lot of code and math data that can be verified to be, you know, whether it compiles,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1379" target="_blank">00:22:59.960</a></span> | <span class="t">whether it's factually correct, whether the math is logically correct, and then now we can basically do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1386" target="_blank">00:23:06.600</a></span> | <span class="t">native RL. Doing this, we notice emergence, reflection, aha moments, and now we basically have another domain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1393" target="_blank">00:23:13.720</a></span> | <span class="t">in which to scale models. Instead of scaling up a 10x order of magnitude of, you know, instead of 45 trillion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1400" target="_blank">00:23:20.120</a></span> | <span class="t">tokens, let's train on 450 trillion tokens, and then, you know, scale that up and up. We're kind of at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1405" target="_blank">00:23:25.960</a></span> | <span class="t">limit there. We start with a really good base model that's a good general next token predictor. We do RL, and now we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1412" target="_blank">00:23:32.040</a></span> | <span class="t">scale in the reasoning domain. So a few months ago, they showed that, you know, they can get O1 level performance, and then fast forward to now, we have O3 level performance with just more RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1421" target="_blank">00:23:41.720</a></span> | <span class="t">Tangentially, this was kind of the paper to kick it all off, right? Deep Seek showed that two things. One, you can do RL from base models, and we can get reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1433" target="_blank">00:23:53.720</a></span> | <span class="t">Two, distillation really works, and it's a much better approach to small models. Following this, we've now had a lot more papers. So shout out to like the 5-4 models, right? 5-4 showed how effective RL can be. They took 5-4 mini, and they made it a reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1449" target="_blank">00:24:09.720</a></span> | <span class="t">variant. Basically, they took about 6,000 samples, and in 6,000 samples of RL, they could take a small base model, a small base next token predictor, and do really, really good reasoning inference on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1461" target="_blank">00:24:21.720</a></span> | <span class="t">So this was the one to kick it off. And now we have, you know, the Quen models have done this. So there's Quen thinking models, or Deep Seek thinking models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1469" target="_blank">00:24:29.720</a></span> | <span class="t">And then there's formulas like the 5 models that show how to do this in small models. Okay, let's continue on from my high level overview.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1479" target="_blank">00:24:39.720</a></span> | <span class="t">So what did they release? They released two models early on. There's R1-0, which is a great reasoning model, only trained on unraveled chain of thought with RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1489" target="_blank">00:24:49.720</a></span> | <span class="t">But it's not a great general model. R1-0 is when you only do RL on chain of thought, you know, it doesn't really emerge to general performance. So R1 is trained as the second model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1503" target="_blank">00:25:03.720</a></span> | <span class="t">It uses outputs of R1-0 using this four stages of training. And then, you know, now we start to do RL back on human tasks, back on chat, and we have a good, good RL, we have a good, good reasoning model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1516" target="_blank">00:25:16.720</a></span> | <span class="t">Second thing they release is their distillations, right? So they take these thinking traces, and they take models in the same family, and then they distill them down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1526" target="_blank">00:25:26.720</a></span> | <span class="t">So these are not natively trained with RL, they're distillations from their base models in Quen and Lama families, and then they show how this works really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1535" target="_blank">00:25:35.720</a></span> | <span class="t">Okay, now, of course, you know, it's 2025, we don't get real papers anymore. So they don't talk about data, how many tokens, where the data comes from. But you know, they still do share a lot about how this stuff works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1548" target="_blank">00:25:48.720</a></span> | <span class="t">Models, of course, fully open source, MIT license, no training data, no code, they have a Deep Seek API, which at the time, you know, it was much faster than anyone, it was much cheaper than anyone, turns out this was fake news.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1561" target="_blank">00:26:01.720</a></span> | <span class="t">This API is very unreliable, it's barely ever up. But you know, now, a few months later, after this has come out, we can see stuff like from open router, you know, Deep Seek actually takes a significant chunk of the pie, they take about 10% of API usage that goes through them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1579" target="_blank">00:26:19.720</a></span> | <span class="t">So this model actually sparked a lot of adoption, it reopened up the race of, you know, okay, we're not done scaling models are still getting a lot better. And yeah, you know, once again, it's been like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1590" target="_blank">00:26:30.720</a></span> | <span class="t">once again, it's been like a few months. And as of like, last week, we have another update to this. And then the Quinn team followed along. What was fake news? Fake news was the Deep Seek API. When it launched, it was 10x faster and 10x cheaper than other inference providers. But turns out it was super unreliable. No one can make API keys, it was almost always down. But it's okay, you know, it exists if you still want to try it. It's a really good model. A lot of people use it. Okay, let's dig deep into this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1619" target="_blank">00:26:59.720</a></span> | <span class="t">Let's dig deep into these topics. Inference time scaling. So what is inference time scaling? We have 01 versus GPT40. Now there's 03, 04, 04 mini, all these things. But basically, what these do is, you increase the chain of thought reasoning process, you allow models to spend more time thinking before they respond. Now, it's very common, we've used a lot of these models, right? They're starting to become even a little bit more agentic with RL. That's kind of what's progressed since we last covered this. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1648" target="_blank">00:27:28.720</a></span> | <span class="t">Instead of spending hundreds of millions of dollars pre-training LLMs, that starts to get exponentially more expensive, right? Instead of changing from hundreds of millions of dollars to pre-training LLM,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1660" target="_blank">00:27:40.720</a></span> | <span class="t">we don't want to spend billions on trade runs, right? So we needed another access. So instead, we shifted to this paradigm of inference time scaling, where you take really hard questions, you do inference time, chain of thought, RL, and we have a new dimension to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1677" target="_blank">00:27:57.720</a></span> | <span class="t">on which we can scale. So previously, people tried to do process based reward modeling. So we would do RL basically on, we would try RL,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1689" target="_blank">00:28:09.720</a></span> | <span class="t">we would do beam search, we would do MCTS, we would do all these inference time, you know, let's predict multiple tokens, go down these processes, they were all hacks, but nothing was really close to 01, right? We would see Twitter demos, we would see like some fundraising that even came out of, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1706" target="_blank">00:28:26.720</a></span> | <span class="t">Okay, I have much better performance than LLM3 because I go down 10 trees of thought. And you know, I'm doing all this stuff on the back end using a bunch of tokens and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1715" target="_blank">00:28:35.720</a></span> | <span class="t">and gluing it together. But this is really not the right approach as we see. What really worked is just native, pure, beautiful, scaled up RL. So once again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1727" target="_blank">00:28:47.720</a></span> | <span class="t">now, this is what Deep Seek did to make V3. Here's the one slide, if you want to know what Deep Seek V3 is, it's open source GPT 4.0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1739" target="_blank">00:28:59.720</a></span> | <span class="t">Oh, sorry, this is Deep Seek V3. This is the precursor to R1. So 4.0 quality, 37 billion active parameters. This is the, you know, regular MOE</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1748" target="_blank">00:29:08.720</a></span> | <span class="t">non reasoning model. This is what they build R1 off of. So it's MOE 671 billion parameters, 37 billion active. They launched this, it was a good model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1758" target="_blank">00:29:18.720</a></span> | <span class="t">It's just really chunky, right? You can't run it on your laptop. No one can run a 700 B model. They made this whole, you know, we're better than everyone else where, you know, we could train this model in $5 million.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1770" target="_blank">00:29:30.720</a></span> | <span class="t">And I think that this is actually true, right? Looking back at it, a lot of what we see is Deep Seek and Chinese labs were really able to catch up to the United States because of the constraints, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1782" target="_blank">00:29:42.720</a></span> | <span class="t">We put a lot of trade restrictions, we couldn't give them GPUs. So they had to get clever and smart with what they got. And basically, you know, they did very, very strong inference optimization, they made the most of what they had, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1795" target="_blank">00:29:55.720</a></span> | <span class="t">We could have continued scaling, right? We could have thrown this 14.8 trillion tokens into 150 trillion tokens, but China didn't have GPUs for that, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1805" target="_blank">00:30:05.720</a></span> | <span class="t">The Deep Seek labs were like, we realized that we can't scale this in the same dimension. So they had to get creative and think about, okay, what if we do RL? And that's basically what they did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1815" target="_blank">00:30:15.720</a></span> | <span class="t">So V3 was, you know, it was an MOE 37 billion active, they introduced this concept of multi-headed latent attention, 15 trillion tokens, they did SFT, and then, you know, traditional RL, so RLHF to make it a chat model, multi-token prediction, this came out of meta, they needed it to be sample efficient with their 15 trillion tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1838" target="_blank">00:30:38.720</a></span> | <span class="t">So multi-token prediction for a little bit more sample efficiency, trained in FBA, did some long context extension, basically first trained at 32K, then they extended this down to 128K.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1851" target="_blank">00:30:51.720</a></span> | <span class="t">Came out a month ago from R1. After that, R1 came out, people got mad hyped. Now we have R1 V2 basically. So these are kind of, you know, they're fancy diagrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1863" target="_blank">00:31:03.720</a></span> | <span class="t">We have DeepSeq V3 base, we do SFT and RL, you have a SFT checkpoint, RL with RLHF to get, sorry, fine tune with RL to get DeepSeq R1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1876" target="_blank">00:31:16.720</a></span> | <span class="t">Okay, what is DeepSeq R1 0? R1 0 is where you don't do any SFT, you take a pure base model. For those that don't remember, base models are models that come when you do your pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1889" target="_blank">00:31:29.720</a></span> | <span class="t">So we train models to predict the next token, right? So these are not models like GPT 4.0 or 0.1. These are pure base models. All they do is they predict the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1901" target="_blank">00:31:41.720</a></span> | <span class="t">So they're kind of completionist models, right? You can't normally chat with these. All they do is complete your sentence, complete your word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1908" target="_blank">00:31:48.720</a></span> | <span class="t">So they take the DeepSeq V3 base model, they apply pure RL, they don't do any SFT, they don't train it as a user assistant chat model, they don't do any of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1918" target="_blank">00:31:58.720</a></span> | <span class="t">It uses GRPO for RL, which they, you know, they actually introduced quite a while in DeepSeq math. The reward is based on both accuracy and format. Responses must be verifiably correct, right? So what are they doing here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1930" target="_blank">00:32:10.720</a></span> | <span class="t">What are they doing here? They take a base non-chat model, they use GRPO style RL on math and code, and they have a verifiable output, right? So the models need to be verifiably output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1942" target="_blank">00:32:22.720</a></span> | <span class="t">They need their output to be verifiably correct, right? So for math, you have a correct output to your math question, right? Is the answer correct or not correct? If it's correct, that's good. We can RL on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1952" target="_blank">00:32:32.720</a></span> | <span class="t">For code, does your code compile? If it compiles, that's good. We can do RL on that. And, you know, this is lead code style questions. So lead code style questions, we know the answer, we know if the answer is correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1963" target="_blank">00:32:43.720</a></span> | <span class="t">Then we format the... Basically, you know, in the little minute details, they format the rewards, they kind of output the thinking between think tags. So we take our base model, we do RL to do a bunch of thinking to get its chain of thought thinking process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1977" target="_blank">00:32:57.720</a></span> | <span class="t">And from there, we kind of have DeepSeq R10. It's a reasoning thinking model that's good at outputting thinking and answers, but it hasn't really been trained to be a useful assistant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=1990" target="_blank">00:33:10.720</a></span> | <span class="t">It's not all one yet. This is just a good thinking model that can unravel its thought process to generate answers. We'll probably skip this guide. This is a GRPO. This is kind of the RL based algorithm that they use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2004" target="_blank">00:33:24.720</a></span> | <span class="t">It's, you know, comparing PPO, GRPO. This is how they do the RL. We'll kind of skip it. The key things to note, there's no critique model. You have group based rewards that are scored. It has stability updates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2017" target="_blank">00:33:37.720</a></span> | <span class="t">There's a, you know, KL divergence. But yeah, they do GRPO. We're going to skip it in this talk for now. So, okay. DeepSeq R10. We took a base model. We did RL. We now have a thinking model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2031" target="_blank">00:33:51.720</a></span> | <span class="t">How does it perform? Pretty good. AIME, you know, it passes O1 Mini for the time. Math. It passes Math 500. It passes O1 Mini. It's not, it's like on par with, but slightly worse than O1. And then, you know, the charts show that we're able to do this inference time scaling by doing RL on really hard questions. And it kind of works. It works pretty well. So, yes. Chart. Number goes up. Number goes up even more. The more you train.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2049" target="_blank">00:34:09.720</a></span> | <span class="t">This thing is starting to stably learn. What else do we learn? So, it naturally has the ability to solve these complex tasks by extending test time compute, right? Here we know that the original R10, it ranges from hundreds to thousands of reasoning tokens. Turns out that this was a pretty key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2065" target="_blank">00:34:25.720</a></span> | <span class="t">factor. In the update that they released that DeepSeq released last week, we scaled from training on from taking thousands of tokens to now taking to doubling it. So now we take from 12,000 tokens on average to 24,000 tokens. And once again, we shift from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2086" target="_blank">00:34:46.720</a></span> | <span class="t">getting 01 level to 03 level performance in the new DeepSeq model. So other stuff, you know, there's this emergence of interesting behaviors as test time compute increases. So as we're able to increase our test time training, as we can reason for more and more time, we start to see this emergence of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2106" target="_blank">00:35:06.720</a></span> | <span class="t">interesting behaviors. Basically, as models learn to reason for longer and longer, as you get to thousands of steps of reasoning, models are able to start to have these reflection moments. So, you know, the more reasoning you do, models start to learn, okay, I'm actually not forced to just output my next token. I'm not forced to do 100 tokens of thinking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2128" target="_blank">00:35:28.720</a></span> | <span class="t">I'm not forced to do 1000. I can continue down this thinking path. And we noticed that, you know, as they start to reason for longer, models start to do this sort of reflection phase, models start to revisit, reevaluate their previous steps, they start to go down alternative passes, and you know, this kind of arises this spontaneity of this spontaneity of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2155" target="_blank">00:35:55.720</a></span> | <span class="t">this spontaneity emergence, right? So spontaneously, you know, models will be like, okay, I tried this, this, this, it's not working, I don't have to answer right now. Let me try this new thing. And guess what, it works. And then we also have these aha moments. So very core takeaway of the paper, you know, this is kind of what got the DeepSeq authors to realize, okay, RL actually works. The more time that we think for the further along the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2185" target="_blank">00:36:25.660</a></span> | <span class="t">thinking traces, we start to get these models to do these aha moments. So here's a quote, again, from the paper, this moment is not only an aha moment for the model, but also for the researchers are observing its behavior. It underscores the power and beauty of reinforcement learning. Rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem solving strategies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2208" target="_blank">00:36:48.660</a></span> | <span class="t">The aha moment serves as a powerful reminder of the potential of reinforcement that reinforcement learning unlocks new levels of intelligence and AI systems. So basically, instead of telling models how to reason, what traces, instead of doing SFT on chain of thought traces, if we just give it this sparse idea of, you know, reason as much as you want. The more time it takes, we start to notice emergence of aha, I see what it should finally make.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2226" target="_blank">00:37:06.660</a></span> | <span class="t">I tried these six steps, and this seven step is working. And you know, this shows the power of RL. And this has been kind of passed down into other papers. Microsoft has put out really good scaling laws on doing RL on small models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2233" target="_blank">00:37:13.660</a></span> | <span class="t">Quen team has done really good thinking models. They have an online talk here. So please, please watch out the Quen reasoning talk. A speaker speaks about how they do that. But okay, back to deep seek, back to deep seek.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2240" target="_blank">00:37:20.660</a></span> | <span class="t">This is an example of an aha moment. So question, a basic math question, you know, not that basic question, you know, a basic question, you know, not that basic question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2247" target="_blank">00:37:27.660</a></span> | <span class="t">And this seven step is working. And this seven step is working. And this seven step is working. And you know, this shows the power of RL. And this has been kind of passed down into other papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2252" target="_blank">00:37:32.660</a></span> | <span class="t">Microsoft has put out really good scaling laws on doing RL on small models. Quen team has done really good thinking models. They have an online talk here. So please, please watch out the Quen reasoning talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2259" target="_blank">00:37:39.660</a></span> | <span class="t">speaks about how they do that. But okay, back to DeepSeq, back to DeepSeq. This is an example of an aha moment. So question, basic math question, you know, not that basic, actually, I can't answer this. If a is greater than one, then the sum of this square root of a square root is equal to something. Well, as the model starts to think, you know, it realizes, oh, there's a x squared, right? If I square this, I can get x squared, then I can isolate this out, right? It's doing reasoning. It's thinking through this math problem, step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2289" target="_blank">00:38:09.640</a></span> | <span class="t">by step, then it says this in its own generation. Wait, wait, wait, that's an aha moment I can flag here. Let's reevaluate this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2296" target="_blank">00:38:16.920</a></span> | <span class="t">And like, you know, very interesting aha moments start to pop up. So yeah, that's kind of overview of what R1 was. That's kind of an overview of what R1-0 was. R1-0 is basically where we take a base model, we train it on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2319" target="_blank">00:38:39.620</a></span> | <span class="t">pure RL for math and code, we do RL on thinking steps. And now we have a reasoning model, it works. But it's not great. It doesn't actually have great readability. It starts to reason in multiple languages, whoa, it reasoned in Chinese, who would have guessed? So we want to make it, you know, we can't just skip RL-HF, right? We want to make this thing a good chat model. So next section,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2344" target="_blank">00:39:04.380</a></span> | <span class="t">instead of R1-0, how do we make DeepSeq R1? How do we take DeepSeq R1 from being just a reasoning model to a reasoning useful assistant that's good at, you know, actually being a chatful assistant?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2357" target="_blank">00:39:17.840</a></span> | <span class="t">So key solution, giving it to you straight, cold start, instead of taking base model to just RL, take a base model, do some regular SFT. SFT is kind of, you know, here's some prompt answer, here's chat assistant, get it to, you know, do a cold start, get it to understand you're still a useful assistant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2377" target="_blank">00:39:37.120</a></span> | <span class="t">After that, do some RL. Do this core RL on very hard code math, get it to start to understand how to think, how to reason. Scale it out until it starts to see these aha moments, until they can do proper reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2389" target="_blank">00:39:49.940</a></span> | <span class="t">From there, let's do some rejection sampling. We want to take examples that don't work, right? Stuff where it is going wrong, where it has negative behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2397" target="_blank">00:39:57.820</a></span> | <span class="t">We do rejection sampling on it, and then once again, we do it, once again, our last session of stage four training, which is another round of RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2405" target="_blank">00:40:05.640</a></span> | <span class="t">So, going through these, stage one, cold start with strong SFT, prevents the model from getting unstable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2412" target="_blank">00:40:12.080</a></span> | <span class="t">So, basically, you take a base model, you have a long chain of thought style data set, so, you know, prompt answer pair, so user assistant, think through your process of how to solve this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2424" target="_blank">00:40:24.320</a></span> | <span class="t">Give your chain of thought, we take a base model, train it on this chain of thought, then from there, you know, we do our cold start with strong, strong SFT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2433" target="_blank">00:40:33.560</a></span> | <span class="t">They don't just do synthetic data, they have human annotators, we want better readability, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2441" target="_blank">00:40:41.220</a></span> | <span class="t">So, do your thinking and think tags, and then this is on the scale of a couple thousand examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2447" target="_blank">00:40:47.100</a></span> | <span class="t">So, base model, thousand examples, a couple thousand examples of SFT, then our main RL stage, you know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2453" target="_blank">00:40:53.740</a></span> | <span class="t">So, same RL process as R1-0, do a lot of RL on verifiable hard questions, so, math, lead code style coding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2462" target="_blank">00:41:02.980</a></span> | <span class="t">stuff that we can verify has the right answer, do a lot of RL, and then, you know, stage three rejection sampling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2469" target="_blank">00:41:09.920</a></span> | <span class="t">So, generate completions, rank them with a reward model, fine tune the original model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2475" target="_blank">00:41:15.420</a></span> | <span class="t">So, this was standard, so, Lama 3 showed us this concept of rejection sampling, and, you know, we take it to Deep Seek.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2482" target="_blank">00:41:22.660</a></span> | <span class="t">We do rejection sampling on samples that we don't like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2484" target="_blank">00:41:24.660</a></span> | <span class="t">We had 800,000 samples that were generated, 600,000 reasoning, 200,000 general chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2490" target="_blank">00:41:30.660</a></span> | <span class="t">We rank them, and then we do our rejection sampling training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2493" target="_blank">00:41:33.900</a></span> | <span class="t">Then, final RL stage for general use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2496" target="_blank">00:41:36.900</a></span> | <span class="t">This is very similar to, you know, similar to RLHF.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2500" target="_blank">00:41:40.900</a></span> | <span class="t">You want to make the model helpful, harmless, and reason good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2505" target="_blank">00:41:45.340</a></span> | <span class="t">So, for reasoning, we use, you know, we want to keep it a good reasoner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2509" target="_blank">00:41:49.580</a></span> | <span class="t">Add in that reasoning for hard math questions, code questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2512" target="_blank">00:41:52.580</a></span> | <span class="t">For general chat, capture human preference and nuance situations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2516" target="_blank">00:41:56.580</a></span> | <span class="t">So, you know, this question should have a very verbose, detailed answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2521" target="_blank">00:42:01.220</a></span> | <span class="t">This is a basic summary, keep it short, but keep your reasoning there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2524" target="_blank">00:42:04.420</a></span> | <span class="t">So, final stage is, you know, this final stage of RL, and now, guess what?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2529" target="_blank">00:42:09.940</a></span> | <span class="t">Model is good at being a chat model. Model is good at thinking. Model has emergence of aha.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2534" target="_blank">00:42:14.760</a></span> | <span class="t">Behaviors, and the model is just a good chat model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2539" target="_blank">00:42:19.460</a></span> | <span class="t">Performance and evals, we're going to skip this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2541" target="_blank">00:42:21.300</a></span> | <span class="t">Basically, when we launched DeepSeq, it was 01 level, but forget that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2546" target="_blank">00:42:26.740</a></span> | <span class="t">We launched an update two weeks ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2549" target="_blank">00:42:29.580</a></span> | <span class="t">So, new model, DeepSeq R1, launched May 28th.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2554" target="_blank">00:42:34.180</a></span> | <span class="t">And instead of being 01 level, the new DeepSeq R1 model is now reasoning for twice as long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2560" target="_blank">00:42:40.780</a></span> | <span class="t">It's as good as 03 and Gemini 2.5 Pro.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2564" target="_blank">00:42:44.180</a></span> | <span class="t">Much better at math. Much better at coding. Much better at reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2567" target="_blank">00:42:47.880</a></span> | <span class="t">It now has support for native function calling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2570" target="_blank">00:42:50.720</a></span> | <span class="t">JSON outputs. No longer hallucinates as much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2573" target="_blank">00:42:53.720</a></span> | <span class="t">Second model drop. And of course, you know, performance charts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2576" target="_blank">00:42:56.760</a></span> | <span class="t">All the regular benchmarks you would expect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2579" target="_blank">00:42:59.060</a></span> | <span class="t">Model is performing as good as 03 and Gemini 2.5 now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2583" target="_blank">00:43:03.860</a></span> | <span class="t">All that was done. Do more RL. AIME jumped, you know, 17.5%, doubled the reasoning tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2591" target="_blank">00:43:11.400</a></span> | <span class="t">Basically, we doubled our reasoning access. We now reason for twice as long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2596" target="_blank">00:43:16.400</a></span> | <span class="t">So, double the reasoning effort. And yeah, now we're 03 and Gemini 2.5 level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2602" target="_blank">00:43:22.940</a></span> | <span class="t">The other drop, we have a new distillation. We take our new model that reasons for twice as long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2608" target="_blank">00:43:28.940</a></span> | <span class="t">We distill this down to Quen 3.8B. And we do a distillation loss on this reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2614" target="_blank">00:43:34.480</a></span> | <span class="t">And we get performance matching the Quen 235 billion reasoning model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2619" target="_blank">00:43:39.480</a></span> | <span class="t">So, our dense 8B non-reasoning model that was distilled from our new deep-cheek model is as good as Quen 3.235B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2631" target="_blank">00:43:51.020</a></span> | <span class="t">which is an MOE reasoning model, which is pretty crazy, you know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2635" target="_blank">00:43:55.020</a></span> | <span class="t">The implications of this show that, you know, long, detailed, good reasoning really has a deep impact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2642" target="_blank">00:44:02.540</a></span> | <span class="t">Once again, check out the Microsoft work for good distillation scaling laws on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2647" target="_blank">00:44:07.040</a></span> | <span class="t">Okay, okay. Back to our paper. Instead of looking at our original deep-seek,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2653" target="_blank">00:44:13.180</a></span> | <span class="t">that's kind of the performance of where we're at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2655" target="_blank">00:44:15.480</a></span> | <span class="t">Distillation. Let's talk about these distillation models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2658" target="_blank">00:44:18.180</a></span> | <span class="t">So, what we did was, distill R1 down into Lama and Quen models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2662" target="_blank">00:44:22.380</a></span> | <span class="t">This is not RL. This is basic SFT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2665" target="_blank">00:44:25.700</a></span> | <span class="t">We have these models that reason for 25,000-30,000 steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2669" target="_blank">00:44:29.300</a></span> | <span class="t">We take these traces, do SFT-style distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2672" target="_blank">00:44:32.800</a></span> | <span class="t">So, proper distillation. Match your logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2675" target="_blank">00:44:35.600</a></span> | <span class="t">And guess what?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2677" target="_blank">00:44:37.740</a></span> | <span class="t">This showed so much performance, but we know RL can do better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2681" target="_blank">00:44:41.740</a></span> | <span class="t">So, you know, all open source, all traces are open.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2685" target="_blank">00:44:45.100</a></span> | <span class="t">Someone do RL-based distillation from the big models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2688" target="_blank">00:44:48.740</a></span> | <span class="t">No one has done this, as far as I know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2690" target="_blank">00:44:50.740</a></span> | <span class="t">But, you know, we're able to get such good performance and there's still so much left to be done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2695" target="_blank">00:44:55.560</a></span> | <span class="t">But, let's go over what we distilled out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2698" target="_blank">00:44:58.700</a></span> | <span class="t">So, these are the family of models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2700" target="_blank">00:45:00.700</a></span> | <span class="t">This is, once again, pure SFT-style distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2703" target="_blank">00:45:03.900</a></span> | <span class="t">Oh shit, my slides are gone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2705" target="_blank">00:45:05.800</a></span> | <span class="t">They're back. Okay, so, we distilled Quen 1.5B, Quen 7B, 14B, 32B, Lama 8B, 70B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2713" target="_blank">00:45:13.620</a></span> | <span class="t">Performance killed all the models they are themselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2717" target="_blank">00:45:17.760</a></span> | <span class="t">So, you take the model itself, you look at our distillation, it worked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2721" target="_blank">00:45:21.760</a></span> | <span class="t">Number went up like crazy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2723" target="_blank">00:45:23.760</a></span> | <span class="t">Really, really good performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2724" target="_blank">00:45:24.960</a></span> | <span class="t">All our distills are now basically on par with GPT-4O.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2730" target="_blank">00:45:30.100</a></span> | <span class="t">And, for our new one, our new 8B distill is much, much better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2734" target="_blank">00:45:34.500</a></span> | <span class="t">It's way better than 4O.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2735" target="_blank">00:45:35.600</a></span> | <span class="t">And, this is just, once again, RL on long chain of thought.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2739" target="_blank">00:45:39.600</a></span> | <span class="t">Take that, do SFT-style distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2741" target="_blank">00:45:41.720</a></span> | <span class="t">Question, what if we just did RL on the base model, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2746" target="_blank">00:45:46.720</a></span> | <span class="t">We tried it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2747" target="_blank">00:45:47.720</a></span> | <span class="t">We tried RL on Quen 32B for 10K steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2750" target="_blank">00:45:50.420</a></span> | <span class="t">It's actually worse than distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2752" target="_blank">00:45:52.360</a></span> | <span class="t">So, you know, for small models, we don't want to just start with native RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2757" target="_blank">00:45:57.200</a></span> | <span class="t">We saw this in our own model, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2758" target="_blank">00:45:58.900</a></span> | <span class="t">We needed this cold start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2760" target="_blank">00:46:00.160</a></span> | <span class="t">We need to kick off something from base models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2763" target="_blank">00:46:03.100</a></span> | <span class="t">R1-0 to actual R1, we had to do this SFT cold start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2767" target="_blank">00:46:07.500</a></span> | <span class="t">So, it actually performed significantly worse than distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2771" target="_blank">00:46:11.120</a></span> | <span class="t">And, of course, the Quen team at the time, they had their own reasoning model, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2774" target="_blank">00:46:14.920</a></span> | <span class="t">They had QW-Q32B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2777" target="_blank">00:46:17.020</a></span> | <span class="t">And, you know, the R1 distill, so the base model distill,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2781" target="_blank">00:46:21.060</a></span> | <span class="t">it did worse than what Quen did, but kind of on par, you know, it's probably what they did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2785" target="_blank">00:46:25.900</a></span> | <span class="t">Our distillation on the chat model, on the base chat model, actually performed so much better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2791" target="_blank">00:46:31.800</a></span> | <span class="t">We were able to do so much better than the Quen reasoning model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2795" target="_blank">00:46:35.600</a></span> | <span class="t">And, of course, now we've taken this a step further with our new model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2798" target="_blank">00:46:38.720</a></span> | <span class="t">Okay, future work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2801" target="_blank">00:46:41.320</a></span> | <span class="t">R1 is worse than V3 at function calling, multi-tap, multi-turn, and JSON mode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2806" target="_blank">00:46:46.160</a></span> | <span class="t">Guess what?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2807" target="_blank">00:46:47.900</a></span> | <span class="t">Two weeks ago, there's a new DeepSeek model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2810" target="_blank">00:46:50.000</a></span> | <span class="t">We now do native function calling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2812" target="_blank">00:46:52.000</a></span> | <span class="t">We have JSON mode, we fixed it, it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2814" target="_blank">00:46:54.800</a></span> | <span class="t">R1 struggles with language mixing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2816" target="_blank">00:46:56.900</a></span> | <span class="t">We don't know how we do on the new one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2818" target="_blank">00:46:58.900</a></span> | <span class="t">It's sensitive to prompting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2820" target="_blank">00:47:00.900</a></span> | <span class="t">I think this is something that the industry needs to figure out, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2823" target="_blank">00:47:03.820</a></span> | <span class="t">Spoiler alert for some Latent Space podcast fans, you know, we've talked to some reasoning experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2829" target="_blank">00:47:09.620</a></span> | <span class="t">So, like, some of the stuff that we're seeing, you know, researchers at OpenAI, they're saying, you know, if you're still doing scaffolding with reasoning models, we're failing as labs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2840" target="_blank">00:47:20.060</a></span> | <span class="t">So, you know, we need to learn how to prompt these things better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2843" target="_blank">00:47:23.960</a></span> | <span class="t">And it's not much better at engineering tests than V3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2846" target="_blank">00:47:26.700</a></span> | <span class="t">That's all fake.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2847" target="_blank">00:47:27.700</a></span> | <span class="t">This is old news.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2848" target="_blank">00:47:28.600</a></span> | <span class="t">This was our old DeepSeek.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2849" target="_blank">00:47:29.840</a></span> | <span class="t">The new DeepSeek is a lot better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2852" target="_blank">00:47:32.540</a></span> | <span class="t">Open recreations, we want to promote open research, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2855" target="_blank">00:47:35.780</a></span> | <span class="t">So, there were people trying to recreate this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2858" target="_blank">00:47:38.240</a></span> | <span class="t">Hugging Face has a version of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2860" target="_blank">00:47:40.680</a></span> | <span class="t">Bespoke Labs was doing this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2862" target="_blank">00:47:42.140</a></span> | <span class="t">I don't know how they're doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2863" target="_blank">00:47:43.580</a></span> | <span class="t">There's quite a few people now that have done this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2865" target="_blank">00:47:45.940</a></span> | <span class="t">But, yeah, that's kind of an overview.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2868" target="_blank">00:47:48.020</a></span> | <span class="t">I know a lot more people have joined.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2869" target="_blank">00:47:49.520</a></span> | <span class="t">We now have, like, 100, 200 people in the audience.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2872" target="_blank">00:47:52.480</a></span> | <span class="t">So, I'm going to do a quick recap in our last 10 minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2874" target="_blank">00:47:54.760</a></span> | <span class="t">So, first, we are launching a second paper club.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2880" target="_blank">00:48:00.700</a></span> | <span class="t">Every week, we do our normal paper club where we take the latest paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2885" target="_blank">00:48:05.360</a></span> | <span class="t">We have 100 people that join every week, 300 for DeepSeek.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2888" target="_blank">00:48:08.960</a></span> | <span class="t">We're turning this into a test of time paper club.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2892" target="_blank">00:48:12.800</a></span> | <span class="t">If you're interested, sign up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2895" target="_blank">00:48:15.140</a></span> | <span class="t">We're going to run this in SF and we're going to do it remotely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2898" target="_blank">00:48:18.640</a></span> | <span class="t">Over the next six months, we're going to cover 50 to 100 papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2902" target="_blank">00:48:22.040</a></span> | <span class="t">We're going to break up what you would need to know as an AI engineer into different buckets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2906" target="_blank">00:48:26.480</a></span> | <span class="t">So, stuff like, you know, what are foundations of deep learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2910" target="_blank">00:48:30.080</a></span> | <span class="t">Attention, RL, optimizers, Atom, gradient descent, foundation models that you should know about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2917" target="_blank">00:48:37.400</a></span> | <span class="t">GPT-2, BERT, RNNs, LSTMs, pre-training, post-training, mid-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2922" target="_blank">00:48:42.200</a></span> | <span class="t">So, scaling laws, chinchilla, distillation, we'll cover days of diffusion, optimization, voice, fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2930" target="_blank">00:48:50.440</a></span> | <span class="t">We basically have a paper club where every week, we're going to split up these core concepts into a few papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2937" target="_blank">00:48:57.740</a></span> | <span class="t">We'll have a presentation of three to four papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2940" target="_blank">00:49:00.240</a></span> | <span class="t">Everyone is welcome to join.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2942" target="_blank">00:49:02.240</a></span> | <span class="t">We're going to have a presentation on every core concept and then open discussion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2946" target="_blank">00:49:06.140</a></span> | <span class="t">This is not a course, courses are good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2948" target="_blank">00:49:08.580</a></span> | <span class="t">You know, you have active workshop, you build stuff, you actually like do active learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2953" target="_blank">00:49:13.720</a></span> | <span class="t">This is still a paper club, you know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2955" target="_blank">00:49:15.500</a></span> | <span class="t">This is, if you want to know the foundations of what's going on under the hood, these are the key papers to know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2961" target="_blank">00:49:21.300</a></span> | <span class="t">We'll invite a lot of speakers, we'll have people present, we'll have good discussions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2965" target="_blank">00:49:25.000</a></span> | <span class="t">But yeah, test of time paper club coming in June, scan QR code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2970" target="_blank">00:49:30.240</a></span> | <span class="t">Let us know if you want to be involved, if you want to recommend a paper, share a paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2976" target="_blank">00:49:36.160</a></span> | <span class="t">We'll share curriculum soon, join the Latent Space Discord.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2980" target="_blank">00:49:40.780</a></span> | <span class="t">We already have a list of top 2025 papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2984" target="_blank">00:49:44.440</a></span> | <span class="t">It's a paper every week that you can go through, we're going to build off of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2988" target="_blank">00:49:48.340</a></span> | <span class="t">And once again, final recap, what did we talk about today?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2992" target="_blank">00:49:52.900</a></span> | <span class="t">Today we talked about the new DeepSeq model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=2995" target="_blank">00:49:55.140</a></span> | <span class="t">So, two weeks ago, DeepSeq R1 May 28th came out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3000" target="_blank">00:50:00.760</a></span> | <span class="t">Basically, DeepSeq took the last DeepSeq model that was as good as all one, we trained it to reason for twice as long, we got significantly better performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3010" target="_blank">00:50:10.140</a></span> | <span class="t">DeepSeq R1 May 28th can now do standard structured JSON output, native function calling, hallucinates less, reasons for twice as long, and is a much, much better jump in performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3024" target="_blank">00:50:24.380</a></span> | <span class="t">From being 01 level, DeepSeq is now on par with OpenAI's 03 model and Gemini 2.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3032" target="_blank">00:50:32.240</a></span> | <span class="t">Basically, across the board on all benchmarks, we are now as good as Gemini 2.5 and OpenAI's 03.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3039" target="_blank">00:50:39.120</a></span> | <span class="t">The other model that was released is DeepSeq R1 QN3 8B base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3044" target="_blank">00:50:44.980</a></span> | <span class="t">So, we took QN3 8B, distilled it down into a reasoning model based on our longer traces, and we killed it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3054" target="_blank">00:50:54.120</a></span> | <span class="t">So, it's a small 8B where we do post-training via distillation as SFT on reasoning traces, and model got really good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3061" target="_blank">00:51:01.980</a></span> | <span class="t">You take base QN3 8B, and you take R QN3 8B, it's very good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3066" target="_blank">00:51:06.580</a></span> | <span class="t">Looking at the benchmarks here, you know, our open source, on-device, runnable QN3 8B non-reasoning model is on par with Gemini 2.5 Flash Thinking, 03 Mini Medium,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3081" target="_blank">00:51:21.980</a></span> | <span class="t">better than 5.4, significantly better than QN3 8B, our 8B reasoner is better than QN32B, it's on par with QN's 235B reasoning model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3094" target="_blank">00:51:34.780</a></span> | <span class="t">So, you know, two major updates, new reasoning model from DeepSeq is good as 03 and Gemini 2.5, new mini 8B model that is as good as 2.5 Thinking and 03 Mini.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3107" target="_blank">00:51:47.780</a></span> | <span class="t">Of course, all open source, run on your laptop, just as good, you know, and these are not even R2, this is not DeepSeq R2, this is our mini title refresh with a new date.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3119" target="_blank">00:51:59.580</a></span> | <span class="t">Yeah, so high level, that's what we talked about, you know, we see aha moments, instead of training on next token prediction, we scale this out to inference time scaling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3132" target="_blank">00:52:12.380</a></span> | <span class="t">So, we now train models to train and think for longer, we get aha moments and that's kind of the update to our new DeepSeq models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3144" target="_blank">00:52:24.180</a></span> | <span class="t">So, thanks to everyone for coming, thanks for listening, join PaperClub.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3153" target="_blank">00:52:33.980</a></span> | <span class="t">Oh yeah, so a lot of our regulars that help make PaperClub are here, Eugene, Ara, RJ, Flo is here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3162" target="_blank">00:52:42.980</a></span> | <span class="t">If anyone else is a regular in PaperClub, you know, come up, every week we have our weekly PaperClub, these are the homies that make it possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3171" target="_blank">00:52:51.780</a></span> | <span class="t">We're going to have our second PaperClub, test of time will be there soon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3175" target="_blank">00:52:55.780</a></span> | <span class="t">But yeah, you know, major shout out, this is not me, this is not SWIX, this is volunteers and more on Zoom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3183" target="_blank">00:53:03.580</a></span> | <span class="t">Every week on a weekday at noon, hundreds of you join in to discuss the Paper, so you know, big shout out to everyone here, not possible without us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3192" target="_blank">00:53:12.580</a></span> | <span class="t">You know, all the authors as well that have come, all the authors that have been able to share, let's just give it up for everyone that makes PaperClub possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3199" target="_blank">00:53:19.580</a></span> | <span class="t">Eugene Yan, who is over there running his track.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3204" target="_blank">00:53:24.580</a></span> | <span class="t">Eugene Yan: Sam, I know, is speaking right now, SWIX, who is putting all this on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3210" target="_blank">00:53:30.380</a></span> | <span class="t">Yeah, of course, once again, I'll leave our QR code here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3215" target="_blank">00:53:35.380</a></span> | <span class="t">If you're interested in test of time, volunteering for a paper, recommending a paper, fill it out, you know, help us out, this is our PaperClub selfie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3223" target="_blank">00:53:43.380</a></span> | <span class="t">But yeah, thanks for coming out, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3225" target="_blank">00:53:45.380</a></span> | <span class="t">Enjoy the rest of the conference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3227" target="_blank">00:53:47.380</a></span> | <span class="t">We'll see you next time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3231" target="_blank">00:53:51.180</a></span> | <span class="t">We'll see you next time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9k3xPh-40mo&t=3231" target="_blank">00:53:51.180</a></span> | <span class="t">We'll see you next time.</span></div></div></body></html>