<html><head><title>Building Reliable Agents: Agent Evaluations</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Building Reliable Agents: Agent Evaluations</h2><a href="https://www.youtube.com/watch?v=DsjkO2vB618" target="_blank"><img src="https://i.ytimg.com/vi/DsjkO2vB618/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>All right. Welcome back. Hopefully you guys enjoyed the last hour or so and had a good chance to talk with a lot of other attendees or sponsors or speakers as they were leaving. I think, again, I think the way Langtrain got started was going to events like these and talking to folks.</p><p>I'd highly encourage you to take advantage of that. Before this, we heard a bunch of talks about agents and how people were building, and one theme that was mentioned a few times was evals. And so for the next series of talks, we're going to deep dive into that. We've got a few speakers who are going to shed a lot of light on how they are using evals or thinking about new research in evals.</p><p>But before that, I wanted to take a little bit of time to set the context for why we think evals are important and some general things that we are doing in the space. There we go. So we ran a survey of agent builders about six months ago where we asked them what was the biggest blocker for getting more agents into production.</p><p>And the number one thing that they cited by far was quality. And so we talked a little bit with Michele about the trade-offs between quality and latency and cost. And quality is still the top thing, blocking people getting to production. And in order to kind of like bridge that gap from prototype to production and increase that quality, one of the techniques that we've seen people adopt is eval-driven development.</p><p>So using evals throughout a bunch of different stages of development to measure your app's performance, and then make sure that you're constantly climbing that ladder of performance. And one of the things that I want to emphasize is that evals is really a continuous journey. So there's three different types of evals that we see people running.</p><p>Most people are thinking about evals maybe in one, maybe two of these. But we think it's a continuous journey throughout the whole life cycle. So what do I mean by that? First, let's talk about offline evals. This is probably what most people think of when they say or hear the word evals.</p><p>So this is before you go into production, you get some data set, you run your app against that data set, and then you measure performance using some evaluators to score it. And you can track this performance over time, you can compare different models, different prompts, things like that, and get a sense for whether the changes you're making are actually increasing or decreasing your app's performance on this data set that you've constructed.</p><p>Of course, that data set isn't perfect, and so there's another type of eval called online evals, which we commonly see people doing. This is when you take your app that's running in production, you take some subset of the data that's coming into the app, and you score that. And so then you can start to track the performance of your app in real time as it's running on real production data.</p><p>And so this is real queries that users are sending in, so it's not a static kind of like golden set. And so these are the two types of evals that people are most familiar with. But we also think there's a third type of eval, which we call in-the-loop evals.</p><p>And so these are evals that occur during the agent while it's running. And so Michele talked a little bit about some of what they were doing at Replit with this, where they were adding some evals based on trying it out and testing with browser use or running code against it.</p><p>And so these would be examples of running some evals in the loop to correct the agent as it's running. And then if it messes up, like in any of these scenarios here, you can feed it back into the agent and have it kind of like self-correct. And so you can add this in a bunch of different domains and use it to basically check the agent.</p><p>And so this has some obvious benefits. It improves response quality. You're not monitoring it after the fact. It actually improves it before it responds and it can block bad responses. The big downside is that this takes more time and it costs more money. So we see this commonly being used when the tolerance for mistake is really low or when latency is not an issue.</p><p>And as we see more and more long-running agents, I actually think that's a perfect time to start thinking about putting these in-the-loop evals into your agent. When we think about evals, there's generally kind of like two parts to evals that we see. The data that you run over and then the evaluators that you use.</p><p>And so all of those three components, they had different aspects of these data and of these evaluators. So in the offline sense, you know, you've got your data set. In the online evals, the data is the production data. And it's happening after the fact. And in the loop is the production data, but it's happening in the loop.</p><p>And then the evaluators can be different as well. So when you have your golden truth data set, you can compare against it. And so that's useful. Those are called what we call ground truth or reference evals. And then reference free evals are when you don't have this ground truth.</p><p>And this is what you do online or in the loop because you don't know what the ground truth is. And so basically data and evaluators are two parts of evals no matter what type of eval you're doing. And so we want to make it easy for people to build data sets and run things over their data as well as build their own evaluators.</p><p>Because one thing that we've noticed is that all the academic data sets that you might see or get started with, those aren't representative of how users are using your application. They're oftentimes not even in the same domain. And so when we talk about data and evals, it's really about making it easy for any application developer to build a data set or build evaluators that are specific for their use case.</p><p>So how do we help? How do we help do that on the data side? One, we've talked about tracing. Traces are where you run these online evaluators over. So we have really good tracing in Langsmith. You can send everything there. We track not only the inputs and outputs but all the steps as well.</p><p>And so you can then run evaluators over these traces for the online evals part. We've also made it really, really easy to add these to a data set and start building up this ground truth data set for offline evals. And so there's a nice little button in Langsmith that you can click, add to data set.</p><p>It will take this kind of like input/output pair. You can actually modify the output pair as well and then it adds it to a data set. So we've tried to make it really easy for people to build up these data sets in Langsmith powered by the observability. And so one of the things that we like to say is that great evals start with great observability.</p><p>And that's how we think about it in Langsmith. They're tied. They're not separate things. Now let's talk about evaluators for a little bit. So there are a few different types of evaluators that we see people using. First is maybe just using code to evaluate things. So this would be like exact match or regex or checking if it's valid JSON or things like that.</p><p>These are great. These are deterministic. They're cheap. They're fast to run. But they're oftentimes not as representative of all the things you want to catch, especially if you have natural language responses. So one of the things that we see popping up here is using LLM as a judge techniques to use an LLM to score the outputs of your agent or LLM application.</p><p>And so this is useful because they can handle more complex things. There's some downsides to this. They're tricky to get started work. We'll talk about this later. But generally the idea of using an LLM to judge outputs is pretty promising. And the third type that we see is just good old human annotation.</p><p>This can happen kind of like live from the user as they're using the app. You can collect thumbs up, thumbs down, send those to Lang Smith and track them there. Or you can have a human go in the background and use something like our annotation cues to score these runs.</p><p>So one of the things that we've been building over the past month or so is a set of open source evaluators to try to make it easy to get started with these evaluators. And so there are a few common use cases that we think are common and you can use off the shelf.</p><p>These include things like code, rag, extraction, and tool calling. So for code, for example, we have some off the shelf utils that will lint Python code or lint TypeScript code. And then you can take those errors and feed them back into the LLM. This is great. You can use these off the shelf.</p><p>There's little configuration needed. But of course, for a lot of use cases, you are going to want to configure evaluators to your specific use case in your specific domain. And so we have a few customizable evals also included in open evals. One of these are LLM as a judge things, making it really easy to get started with that.</p><p>A little bit more interesting is things around agent trajectories. So taking in a trajectory of messages, passing it into one of these evaluators, and customizing it so you can choose what to look for. And then one of the things that we're launching today is chat simulations. So a lot of applications are chat bots or they have some back and forth component.</p><p>And so sure, you can test a single kind of like interaction, but you often want to see how it performs in a conversational setting. And so we're launching some utils to both run and then score those evaluators. And these are all open source in open evals package. One of the most common techniques we see being used is LLM as a judge evaluators.</p><p>These can be really powerful, but they're also tricky to get working properly. You now have a separate prompt that you have to worry about. You have to prompt engineer this prompt, which is grading your other prompt. And so there's this extra work that goes into it. And so while this is powerful, we oftentimes see people struggling to set it up or to trust the process.</p><p>And so we're launching in private preview some features specifically designed to help with this. So first, we're launching some work that's based off of align eval, which is some research by Eugene Yan. You'll hear from Shreya later on. She was actually a lot of the influence for some of this work.</p><p>She wrote a great paper called Who Validates the Validators. And so a lot of this work we're incorporating into Langsmith to make it really easy to get started with LLM as a judge techniques. And then, of course, after you get started, how do you know that it's working? So we're launching some eval calibration techniques in Langsmith where you can blindly score how the evals are doing and then compare them and see that over time.</p><p>And if they start to drift out of whack, then you go back into this align eval kind of like step. And so this is in private preview. We're really excited to launch it and work in figuring out what the right UX for building these LLM as a judge evaluators are.</p><p>The thing that I want to emphasize is that evals is a continuous journey. You're not done with it once you build a data set and run it once. You're going to want to keep on running it. You're not done with it just because you did it on the offline part.</p><p>You're going to want to do it online. And eventually, I think more and more are going to start building it into the agents themselves. And so this is one of the key takeaways that I'd kind of leave you with. with. Thank you.</p></div></div></body></html>