<html><head><title>Lesson 7 - Deep Learning for Coders (2020)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Lesson 7 - Deep Learning for Coders (2020)</h2><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc"><img src="https://i.ytimg.com/vi_webp/VEG5xT5gAHc/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=0">0:0</a> Weight decay (L2 Regularization)<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=445">7:25</a> Creating our own Embedding module<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=765">12:45</a> Interpreting embeddings and bias<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1080">18:0</a> Embedding distance<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1200">20:0</a> Deep learning for collaborative filtering<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1449">24:9</a> Notebook 9 - Tabular modelling<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1530">25:30</a> entity embeddings for categorical variables<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1811">30:11</a> beyond deep learning for tabular data (ensembles of decision trees)<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2410">40:10</a> Decision Trees<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3840">64:0</a> Random Forests<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4330">72:10</a> Out-of-bag error<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4440">74:0</a> Model Interpretation<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5640">94:0</a> extrapolation<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6180">103:0</a> using a NN<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6560">109:20</a> ensembling<br><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7060">117:40</a> conclusion<br><br><div style="text-align: left;"><a href="./VEG5xT5gAHc.html">Whisper Transcript</a> | <a href="./transcript_VEG5xT5gAHc.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi everybody and welcome to lesson 7. We're going to start by having a look at a kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7" target="_blank">00:00:07.520</a></span> | <span class="t">of regularization called weight decay. And the issue that we came to at the end of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=13" target="_blank">00:00:13.400</a></span> | <span class="t">last lesson is that we were training our simple dot product model with bias, and our loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=22" target="_blank">00:00:22.400</a></span> | <span class="t">started going down and then it started going up again. And so we have a problem that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=29" target="_blank">00:00:29.280</a></span> | <span class="t">are overfitting. And remember in this case we're using mean squared error. So try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=35" target="_blank">00:00:35.560</a></span> | <span class="t">recall why it is that we don't need a metric here, because mean squared error is pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=42" target="_blank">00:00:42.760</a></span> | <span class="t">much the thing we care about really, or we could use mean absolute error if we like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=47" target="_blank">00:00:47.800</a></span> | <span class="t">but either of those works fine as a loss function. They don't have the problem of big flat areas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=52" target="_blank">00:00:52.720</a></span> | <span class="t">like accuracy does for classification. So what we want to do is to make it less likely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=60" target="_blank">00:01:00.400</a></span> | <span class="t">that we're going to overfit by doing something we call reducing the capacity of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=66" target="_blank">00:01:06.120</a></span> | <span class="t">The capacity of the model is basically how much space does it have to find answers. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=71" target="_blank">00:01:11.800</a></span> | <span class="t">if it can kind of find any answer anywhere, those answers can include basically memorizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=78" target="_blank">00:01:18.320</a></span> | <span class="t">the data set. So one way to handle this would be to decrease the number of latent factors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=87" target="_blank">00:01:27.320</a></span> | <span class="t">But generally speaking, reducing the number of parameters in a model, particularly as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=92" target="_blank">00:01:32.780</a></span> | <span class="t">we look at more deep learning style models, ends up biasing the models towards very simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=100" target="_blank">00:01:40.720</a></span> | <span class="t">kind of shapes. So there's a better way to do it rather than reducing the number of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=107" target="_blank">00:01:47.840</a></span> | <span class="t">And we try to force the parameters to be smaller, unless they're really required to be big.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=115" target="_blank">00:01:55.480</a></span> | <span class="t">And the way we do that is with weight decay. Weight decay is also known as L2 regularization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=120" target="_blank">00:02:00.960</a></span> | <span class="t">They're very slightly different, but we can think of them as the same thing. And what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=125" target="_blank">00:02:05.320</a></span> | <span class="t">we do is we change our loss function, and specifically we change the loss function by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=130" target="_blank">00:02:10.120</a></span> | <span class="t">adding to it the sum of all the weights squared. In fact, all of the parameters squared really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=137" target="_blank">00:02:17.900</a></span> | <span class="t">should stay. Why do we do that? Well, because if that's part of the loss function, then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=144" target="_blank">00:02:24.460</a></span> | <span class="t">one way to decrease the loss would be to decrease the weights, one particular weight or all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=150" target="_blank">00:02:30.360</a></span> | <span class="t">of the weights or something like that. And so when we decrease the weights, if you think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=158" target="_blank">00:02:38.980</a></span> | <span class="t">about what that would do, then think about, for example, the different possible values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=169" target="_blank">00:02:49.100</a></span> | <span class="t">of a in y equals ax squared. The larger a is, for example, a is 50, you get these very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=177" target="_blank">00:02:57.080</a></span> | <span class="t">narrow peaks. In general, big coefficients are going to cause big swings, big changes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=185" target="_blank">00:03:05.800</a></span> | <span class="t">in the loss, small changes in the parameters. And when you have these kind of sharp peaks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=193" target="_blank">00:03:13.660</a></span> | <span class="t">or valleys, it means that a small change to the parameter can make a, sorry, a small change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=202" target="_blank">00:03:22.520</a></span> | <span class="t">to the input and make a big change to the loss. And so if you have, if you're in that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=207" target="_blank">00:03:27.760</a></span> | <span class="t">situation, then you can basically fit all the data points close to exactly with a really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=213" target="_blank">00:03:33.480</a></span> | <span class="t">complex jagged function with sharp changes, which exactly tries to sit on each data point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=221" target="_blank">00:03:41.020</a></span> | <span class="t">rather than finding a nice smooth surface which connects them all together or goes through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=226" target="_blank">00:03:46.660</a></span> | <span class="t">them all. So if we limit our weights by adding in the loss function, the sum of the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=234" target="_blank">00:03:54.580</a></span> | <span class="t">squared, then what it's going to do is it's going to fit less well on the training set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=240" target="_blank">00:04:00.760</a></span> | <span class="t">because we're giving it less room to try anything that it wants to, but we're going to hope</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=245" target="_blank">00:04:05.320</a></span> | <span class="t">that it would result in a better loss on the validation set or the test set so that it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=250" target="_blank">00:04:10.740</a></span> | <span class="t">will generalize better. One way to think about this is that the loss with weight decay is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=257" target="_blank">00:04:17.020</a></span> | <span class="t">just the loss plus the sum of the parameters squared times some number we pick, a hyperparameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=267" target="_blank">00:04:27.500</a></span> | <span class="t">This is like 0.1 or 0.01 or 0.001 kind of region. So this is basically what loss with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=275" target="_blank">00:04:35.680</a></span> | <span class="t">weight decay looks like in this equation. But remember when it actually comes to what's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=280" target="_blank">00:04:40.500</a></span> | <span class="t">how is the loss used in stochastic gradient descent? It's used by taking its gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=285" target="_blank">00:04:45.940</a></span> | <span class="t">So what's the gradient of this? Well, if you remember back to when you first learned calculus,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=292" target="_blank">00:04:52.880</a></span> | <span class="t">it's okay if you don't. The gradient of something squared is just two times that something. We've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=300" target="_blank">00:05:00.200</a></span> | <span class="t">changed some parameters to weight which is a bit confusing. So just use weight here to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=305" target="_blank">00:05:05.800</a></span> | <span class="t">keep it consistent. Maybe parameters is better. So the derivative of weight squared is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=312" target="_blank">00:05:12.880</a></span> | <span class="t">two times weight. So in other words, to add in this term to the gradient, we can just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=320" target="_blank">00:05:20.480</a></span> | <span class="t">add to the gradients weight decay times two times weight. And since weight decay is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=328" target="_blank">00:05:28.420</a></span> | <span class="t">a hyperparameter, we can just replace it with weight decay times two. So that would just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=332" target="_blank">00:05:32.500</a></span> | <span class="t">give us weight decay times weight. So weight decay refers to adding on the, to the gradients,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=342" target="_blank">00:05:42.960</a></span> | <span class="t">the weights times some hyperparameter. And so that is going to try to create these kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=348" target="_blank">00:05:48.780</a></span> | <span class="t">of more shallow, less bumpy surfaces. So to do that, we can simply, when we call fit or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=359" target="_blank">00:05:59.260</a></span> | <span class="t">fit one cycle or whatever, we can pass in a WD parameter and that's just this number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=367" target="_blank">00:06:07.100</a></span> | <span class="t">here. So if we pass in point one, then the training loss goes from point two nine to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=375" target="_blank">00:06:15.660</a></span> | <span class="t">point four nine. That's much worse, right, because we can't overfit anymore. The valid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=380" target="_blank">00:06:20.980</a></span> | <span class="t">loss goes from point eight nine to point eight two, much better. So this is an important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=387" target="_blank">00:06:27.740</a></span> | <span class="t">thing to remember for those of you that have done a lot of more traditional statistical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=391" target="_blank">00:06:31.820</a></span> | <span class="t">models is in kind of more traditional statistical models, we try to avoid overfitting and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=397" target="_blank">00:06:37.800</a></span> | <span class="t">try to increase generalization by decreasing the number of parameters. But in a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=404" target="_blank">00:06:44.040</a></span> | <span class="t">modern machine learning and certainly deep learning, we tend to instead use regularization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=411" target="_blank">00:06:51.660</a></span> | <span class="t">such as weight decay because it gives us more flexibility. It lets us use more nonlinear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=416" target="_blank">00:06:56.820</a></span> | <span class="t">functions and still, you know, still reduces the capacity of the model. Great. So we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=423" target="_blank">00:07:03.940</a></span> | <span class="t">down to point eight two three. This is a good model. This is really actually a very good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=428" target="_blank">00:07:08.780</a></span> | <span class="t">model. And so let's dig into actually what's going on here because in our, in our architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=438" target="_blank">00:07:18.300</a></span> | <span class="t">remember we basically just had four embedding layers. So what's an embedding layer? We've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=444" target="_blank">00:07:24.580</a></span> | <span class="t">described it conceptually, but let's write our own. And remember we said that an embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=449" target="_blank">00:07:29.980</a></span> | <span class="t">layer is just a computational shortcut for doing a matrix multiplication by a one hot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=455" target="_blank">00:07:35.380</a></span> | <span class="t">encoded matrix and that that is actually the same as just indexing into an array. So an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=463" target="_blank">00:07:43.860</a></span> | <span class="t">embedding is just a indexing into an array. And so it's nice to be able to create our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=470" target="_blank">00:07:50.380</a></span> | <span class="t">own versions of things that exist in PyTorch and fast.ai. So let's do that for embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=476" target="_blank">00:07:56.780</a></span> | <span class="t">So if we're going to create our own kind of layer, which is pretty cool, we need to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=482" target="_blank">00:08:02.500</a></span> | <span class="t">aware of something, which is normally a layer is basically created by inheriting as we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=489" target="_blank">00:08:09.660</a></span> | <span class="t">discussed from module or nn.module. So for example, this is an example here of a module</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=495" target="_blank">00:08:15.060</a></span> | <span class="t">where we've created a class called t that inherits from module. And when it's constructed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=500" target="_blank">00:08:20.340</a></span> | <span class="t">remember that's what dunder init does. We're just going to sit, this is just a dummy little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=505" target="_blank">00:08:25.020</a></span> | <span class="t">module here. We're going to set self.a to the number one repeated three times as a tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=511" target="_blank">00:08:31.820</a></span> | <span class="t">Now if you remember back to notebook four, we talked about how the optimizers in PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=517" target="_blank">00:08:37.820</a></span> | <span class="t">and fast.ai rely on being able to grab the parameters attribute to find a list of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=522" target="_blank">00:08:42.900</a></span> | <span class="t">the parameters. Now if you want to be able to optimize self.a, you would need to appear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=528" target="_blank">00:08:48.860</a></span> | <span class="t">in parameters, but actually there's nothing there. Why is that? That's because PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=536" target="_blank">00:08:56.340</a></span> | <span class="t">does not assume that everything that's in a module is something that needs to be learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=541" target="_blank">00:09:01.460</a></span> | <span class="t">To tell it that it's something that needs to be learned, you have to wrap it with nn.parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=545" target="_blank">00:09:05.900</a></span> | <span class="t">So here's exactly the same class, but torch.ones, which is just a list of three, three ones</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=552" target="_blank">00:09:12.500</a></span> | <span class="t">in this case is wrapped in nn.parameter. And now if I go parameters, I see I have a parameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=560" target="_blank">00:09:20.340</a></span> | <span class="t">with three ones in it. And that's going to automatically call requires grad underscore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=565" target="_blank">00:09:25.300</a></span> | <span class="t">for us as well. We haven't had to do that for things like nn.linear in the past because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=572" target="_blank">00:09:32.460</a></span> | <span class="t">PyTorch automatically uses nn.parameter internally. So if we have a look at the parameters for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=577" target="_blank">00:09:37.820</a></span> | <span class="t">something that uses nn.linear with no bias layer, you'll see again we have here a parameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=584" target="_blank">00:09:44.900</a></span> | <span class="t">with three things in it. So we want to in general be able to create a parameter. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=595" target="_blank">00:09:55.220</a></span> | <span class="t">something with a tensor with a bunch of things in and generally we want to randomly initialize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=599" target="_blank">00:09:59.260</a></span> | <span class="t">them. So to randomly initialize, we can pass in the size we want. We can initialize a tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=604" target="_blank">00:10:04.980</a></span> | <span class="t">of zeros of that size and then randomly generate some normal, normally distributed random numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=610" target="_blank">00:10:10.820</a></span> | <span class="t">with a mean of zero and a deviation of 0.01. No particular reason I'm picking those numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=616" target="_blank">00:10:16.220</a></span> | <span class="t">just to show how this works. So here's something that will give us back a set of parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=622" target="_blank">00:10:22.380</a></span> | <span class="t">of any size we want. And so now we're going to replace everywhere that used to say embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=628" target="_blank">00:10:28.140</a></span> | <span class="t">I'm going to replace it with create params. Everything else here is the same in the init</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=634" target="_blank">00:10:34.840</a></span> | <span class="t">under init. And then the forward is very, very similar to before. As you can see, I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=640" target="_blank">00:10:40.380</a></span> | <span class="t">grabbing the zero index column from x, that's my users, and I just look it up as you see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=649" target="_blank">00:10:49.260</a></span> | <span class="t">in that user factors array. And the cool thing is I don't have to do anything with gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=654" target="_blank">00:10:54.320</a></span> | <span class="t">myself for this manual embedding layer because PyTorch can figure out the gradients automatically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=659" target="_blank">00:10:59.500</a></span> | <span class="t">as we've discussed. But then I just got the dot product as before, add on the bias as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=663" target="_blank">00:11:03.700</a></span> | <span class="t">before, do the sigmoid range as before. And so here's a dot product bias without any special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=670" target="_blank">00:11:10.820</a></span> | <span class="t">PyTorch layers and we fit and we get the same result. So I think that is pretty amazingly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=678" target="_blank">00:11:18.580</a></span> | <span class="t">cool. We've really shown that the embedding layer is nothing fancy, is nothing magic, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=685" target="_blank">00:11:25.900</a></span> | <span class="t">It's just indexing into an array. So hopefully that removes a bit of the mystery for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=692" target="_blank">00:11:32.420</a></span> | <span class="t">So let's have a look at this model that we've created and we've trained and find out what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=698" target="_blank">00:11:38.100</a></span> | <span class="t">it's learned. That's already useful. We've got something we can make pretty accurate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=701" target="_blank">00:11:41.620</a></span> | <span class="t">predictions with. But let's find out what those, what the model looks like. So remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=709" target="_blank">00:11:49.180</a></span> | <span class="t">when we have a question. Okay, let's take a question before you can look at this. What's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=715" target="_blank">00:11:55.700</a></span> | <span class="t">the advantage of creating our own embedding layer over the stock PyTorch one? Oh, nothing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=722" target="_blank">00:12:02.220</a></span> | <span class="t">at all. We're just showing that we can. It's great to be able to dig under the surface</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=726" target="_blank">00:12:06.100</a></span> | <span class="t">because at some point you'll want to try doing new things. So a good way to learn to do new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=730" target="_blank">00:12:10.520</a></span> | <span class="t">things is to be able to replicate things that already exist and you can expect that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=735" target="_blank">00:12:15.380</a></span> | <span class="t">understand how they work. It's also a great way to understand the foundations of what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=739" target="_blank">00:12:19.180</a></span> | <span class="t">going on is to actually create encode your own implementation. But I wouldn't expect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=744" target="_blank">00:12:24.980</a></span> | <span class="t">you to use this implementation in practice. But basically it removes all the mystery. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=752" target="_blank">00:12:32.940</a></span> | <span class="t">if you remember we've created a learner called learn and to get to the model that's inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=757" target="_blank">00:12:37.780</a></span> | <span class="t">it, you can always call learn.model and then inside that there's going to be automatically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=765" target="_blank">00:12:45.180</a></span> | <span class="t">created for it. Well, sorry, not automatically. We've created all these attributes movie factors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=769" target="_blank">00:12:49.420</a></span> | <span class="t">movie bias and so forth. So we can grab learn.model.movieBias. And now what I'm going to do is I'm going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=779" target="_blank">00:12:59.220</a></span> | <span class="t">to sort that vector and I'm going to print out the first five titles. And so what this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=788" target="_blank">00:13:08.180</a></span> | <span class="t">is going to do is it's going to print out the movies with the smallest bias and here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=795" target="_blank">00:13:15.220</a></span> | <span class="t">they are. What does this mean? Well, it kind of means these are the five movies that people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=802" target="_blank">00:13:22.620</a></span> | <span class="t">really didn't like. But it's more than that. It's not only do people not like them, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=809" target="_blank">00:13:29.180</a></span> | <span class="t">if we take account of the genre they're in, the actors they have, you know, whatever the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=814" target="_blank">00:13:34.620</a></span> | <span class="t">latent factors are, people liked them a lot less than they expected. So maybe for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=821" target="_blank">00:13:41.380</a></span> | <span class="t">this is kind of I haven't seen any of these movies. Luckily perhaps this is a sci-fi movie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=829" target="_blank">00:13:49.900</a></span> | <span class="t">So people who kind of like these sci-fi movies found they're so bad they still didn't like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=834" target="_blank">00:13:54.100</a></span> | <span class="t">it. So we can do the exact opposite, which is to sort sending. And here are the top five</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=842" target="_blank">00:14:02.460</a></span> | <span class="t">movies and specifically they're the top five by bias, right? So these are the movies that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=847" target="_blank">00:14:07.700</a></span> | <span class="t">even after you take account of the fact that LA Confidential, I have seen all of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=851" target="_blank">00:14:11.900</a></span> | <span class="t">ones. So LA Confidential is a kind of a murder mystery cop movie, I guess. And people who</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=858" target="_blank">00:14:18.860</a></span> | <span class="t">don't necessarily like that genre or I think Guy Pearce was in it. So maybe they don't like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=862" target="_blank">00:14:22.700</a></span> | <span class="t">Guy Pearce very much, whatever. People still like this movie more than they expect. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=869" target="_blank">00:14:29.340</a></span> | <span class="t">this is a kind of a nice thing that we can look inside our model and see what it's learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=875" target="_blank">00:14:35.060</a></span> | <span class="t">We can look at not only at the bias vector, but we can also look at the factors. Now there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=883" target="_blank">00:14:43.940</a></span> | <span class="t">are 50 factors, which is too many to visualize. So we can use a technique called PCA, Principle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=890" target="_blank">00:14:50.020</a></span> | <span class="t">Components now. This, the details don't matter, but basically they're going to squish those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=893" target="_blank">00:14:53.780</a></span> | <span class="t">50 factors down to three. And then we'll plot the top two as you can see here. And what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=904" target="_blank">00:15:04.060</a></span> | <span class="t">see when we plot the top two is we can kind of see that the movies have been kind of spread</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=911" target="_blank">00:15:11.380</a></span> | <span class="t">out across a space of some kind of latent factors. And so if you look at the far right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=918" target="_blank">00:15:18.700</a></span> | <span class="t">there's a whole bunch of kind of big budget actually things. And on the far left, there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=925" target="_blank">00:15:25.540</a></span> | <span class="t">more like cult kind of things, Fargo, Schindler's List, Monty Python. By the same token at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=933" target="_blank">00:15:33.660</a></span> | <span class="t">bottom, we've got some English patient, Harry Met Sally, so kind of romance drama kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=942" target="_blank">00:15:42.860</a></span> | <span class="t">stuff. And at the top, we've got action and sci-fi kind of stuff. So you can see even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=950" target="_blank">00:15:50.660</a></span> | <span class="t">as though we haven't asked in any information about these movies, all we've seen is who</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=957" target="_blank">00:15:57.900</a></span> | <span class="t">likes what. These latent factors have automatically kind of figured out a space or a way of thinking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=965" target="_blank">00:16:05.380</a></span> | <span class="t">about these movies based on what kinds of movies people like and what other kinds of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=969" target="_blank">00:16:09.660</a></span> | <span class="t">movies they like along with those. But that's really interesting to kind of try and visualize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=975" target="_blank">00:16:15.300</a></span> | <span class="t">what's going on inside your model. Now we don't have to do all this manually. We can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=985" target="_blank">00:16:25.540</a></span> | <span class="t">actually just say give me a collab learner using this set of data loaders with this number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=992" target="_blank">00:16:32.120</a></span> | <span class="t">of factors and this y range and it does everything we've just seen again about the same number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=997" target="_blank">00:16:37.980</a></span> | <span class="t">Okay, so now you can see this is nice, right? We've actually been able to see right underneath</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1003" target="_blank">00:16:43.100</a></span> | <span class="t">inside the collab learner part of the fast AI application, the collaborative filtering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1008" target="_blank">00:16:48.060</a></span> | <span class="t">application and we can build it all ourselves from scratch. We know how to create the SGD,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1014" target="_blank">00:16:54.300</a></span> | <span class="t">know how to create the embedding layer, we know how to create the model, the architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1021" target="_blank">00:17:01.500</a></span> | <span class="t">So now you can see, you know, we've really can build up from scratch our own version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1026" target="_blank">00:17:06.580</a></span> | <span class="t">of this. So if we just type learn.model, you can see here the names are a bit more generic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1033" target="_blank">00:17:13.220</a></span> | <span class="t">This is a user weight, item weight, user bias, item bias, but it's basically the same stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1037" target="_blank">00:17:17.700</a></span> | <span class="t">we've seen before. And we can replicate the exact analysis we saw before by using this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1044" target="_blank">00:17:24.240</a></span> | <span class="t">same idea. Okay, slightly different order this time because it is a bit random but pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1054" target="_blank">00:17:34.660</a></span> | <span class="t">similar as well. Another interesting thing we can do is we can think about the distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1061" target="_blank">00:17:41.880</a></span> | <span class="t">between two movies. So let's grab all the movie factors or just pop them into a variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1071" target="_blank">00:17:51.220</a></span> | <span class="t">and then let's pick a movie and then let's find the distance from that movie to every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1085" target="_blank">00:18:05.160</a></span> | <span class="t">other movie. And so one way of thinking about distance is you might recall the Pythagorean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1090" target="_blank">00:18:10.340</a></span> | <span class="t">formula or the distance on the hypotenuse of a triangle, which is also the distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1097" target="_blank">00:18:17.820</a></span> | <span class="t">to a point in a Cartesian plane on a chart, which is root x squared plus y squared. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1105" target="_blank">00:18:25.020</a></span> | <span class="t">might know, it doesn't matter if you don't, but you can do exactly the same thing for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1108" target="_blank">00:18:28.760</a></span> | <span class="t">50 dimensions. It doesn't just work for two dimensions. There's a, that tells you how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1116" target="_blank">00:18:36.020</a></span> | <span class="t">far away a point is from another point if you, if x and y are actually differences between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1123" target="_blank">00:18:43.620</a></span> | <span class="t">two movie vectors. So then what gets interesting is you can actually then divide that kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1138" target="_blank">00:18:58.020</a></span> | <span class="t">of by the, by the length to make all the lengths the same distance to find out how the angle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1143" target="_blank">00:19:03.620</a></span> | <span class="t">between any two movies and that actually turns out to be a really good way to compare the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1147" target="_blank">00:19:07.620</a></span> | <span class="t">similarity of two things. That's called cosine similarity. And so the details don't matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1152" target="_blank">00:19:12.120</a></span> | <span class="t">You can look them up if you're interested. But the basic idea here is to see that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1156" target="_blank">00:19:16.340</a></span> | <span class="t">can actually pick a movie and find the movie that is the most similar to it based on these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1163" target="_blank">00:19:23.260</a></span> | <span class="t">factors. Kind of interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1165" target="_blank">00:19:25.140</a></span> | <span class="t">I have a question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1167" target="_blank">00:19:27.060</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1168" target="_blank">00:19:28.740</a></span> | <span class="t">What motivated learning at a 50-dimensional embedding and then using a to reduce the three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1173" target="_blank">00:19:33.660</a></span> | <span class="t">versus just learning a three-dimensional?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1176" target="_blank">00:19:36.060</a></span> | <span class="t">Oh, because the purpose of this was actually to create a good model. So the, the visualization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1182" target="_blank">00:19:42.100</a></span> | <span class="t">part is normally kind of the exploration of what's going in, on in your model. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1187" target="_blank">00:19:47.780</a></span> | <span class="t">with a 50, with 50 latent factors, you're going to get a more accurate. So that's one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1194" target="_blank">00:19:54.660</a></span> | <span class="t">approach is this dot product version. There's another version we could use, which is we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1202" target="_blank">00:20:02.660</a></span> | <span class="t">could create a set of user factors and a set of item factors and just like before we could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1212" target="_blank">00:20:12.620</a></span> | <span class="t">look them up. But what we could then do instead of doing a dot product, we could concatenate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1218" target="_blank">00:20:18.140</a></span> | <span class="t">them together into a tensor that contains both the user and the movie factors next to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1226" target="_blank">00:20:26.340</a></span> | <span class="t">each other. And then we could pass them through a simple little neural network, linear, relu,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1234" target="_blank">00:20:34.820</a></span> | <span class="t">linear, and then sigmoid range as before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1239" target="_blank">00:20:39.280</a></span> | <span class="t">So importantly here, the first linear layer, the number of inputs is equal to the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1245" target="_blank">00:20:45.020</a></span> | <span class="t">of user factors plus the number of item factors. And the number of outputs is however many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1251" target="_blank">00:20:51.360</a></span> | <span class="t">activations we have. And then we just default to 100 here. And then the final layer will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1260" target="_blank">00:21:00.160</a></span> | <span class="t">go from 100 to 1 because we're just making one prediction. And so we could create, we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1266" target="_blank">00:21:06.060</a></span> | <span class="t">call that collab nn. We can instantiate that to create a model. We can create a learner</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1271" target="_blank">00:21:11.060</a></span> | <span class="t">and we can fit. It's not going quite as well as before. It's not terrible, but it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1276" target="_blank">00:21:16.860</a></span> | <span class="t">quite as good as our dot product version. But the interesting thing here is it does give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1281" target="_blank">00:21:21.900</a></span> | <span class="t">us some more flexibility, which is that since we're not doing a dot product, we can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1287" target="_blank">00:21:27.060</a></span> | <span class="t">have a different embedding size for each of users versus items. And actually fast.ai has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1295" target="_blank">00:21:35.220</a></span> | <span class="t">a simple heuristic. If you call get embedding size and pass in your data loaders, it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1300" target="_blank">00:21:40.540</a></span> | <span class="t">suggest appropriate size embedding matrices for each of your categorical variables, each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1309" target="_blank">00:21:49.060</a></span> | <span class="t">of your user and item sensors. So that's, so if we pass in *m's settings, that's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1322" target="_blank">00:22:02.020</a></span> | <span class="t">to pass in the user, tuple and the item, tuple, which we can then pass to embedding. This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1331" target="_blank">00:22:11.340</a></span> | <span class="t">the * prefix we learned about in the last class in case you forgot. So this is kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1337" target="_blank">00:22:17.660</a></span> | <span class="t">of interesting. We can, you know, we can see here that there's two different architectures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1343" target="_blank">00:22:23.260</a></span> | <span class="t">we could pick from. It wouldn't be necessarily obvious ahead of time which one's going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1346" target="_blank">00:22:26.660</a></span> | <span class="t">work better. In this particular case, the simplest one, the dot product one, actually turned out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1352" target="_blank">00:22:32.940</a></span> | <span class="t">to work a bit better, which is interesting. This particular version here, if you call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1357" target="_blank">00:22:37.580</a></span> | <span class="t">collab_learner and pass use_nn = true, then what that's going to do is it's going to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1364" target="_blank">00:22:44.940</a></span> | <span class="t">this version, the version with concatenation and the linear layers. So collab_learner, use_nn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1376" target="_blank">00:22:56.980</a></span> | <span class="t">= true, again we get about the same result as you'd expect because it's just a draw-cut</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1381" target="_blank">00:23:01.020</a></span> | <span class="t">for this version. And it's interesting actually, we have a look at collab_learner, it actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1389" target="_blank">00:23:09.100</a></span> | <span class="t">returns an object of type embedding_nn, and it's kind of cool if you look inside the fast.io</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1394" target="_blank">00:23:14.420</a></span> | <span class="t">source code or use the double question mark trick to see the source code for embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1398" target="_blank">00:23:18.020</a></span> | <span class="t">nn, you'll see it's three lines of code. How does that happen? Because we're using this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1404" target="_blank">00:23:24.340</a></span> | <span class="t">thing called tab_ular_model, which we will learn about in a moment, but basically this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1412" target="_blank">00:23:32.620</a></span> | <span class="t">neural net version of collaborative filtering is literally just a tab_ular model in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1417" target="_blank">00:23:37.740</a></span> | <span class="t">we pass no continuous variables and some embedding sizes. So we'll see that in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1430" target="_blank">00:23:50.080</a></span> | <span class="t">Okay so that is collaborative filtering, and again take a look at the further research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1435" target="_blank">00:23:55.060</a></span> | <span class="t">section in particular after you finish the questionnaire, because there's some really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1439" target="_blank">00:23:59.740</a></span> | <span class="t">important next steps you can take to push your knowledge and your skills.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1446" target="_blank">00:24:06.620</a></span> | <span class="t">So let's now move to notebook 9, tab_ular. And we're going to look at tab_ular_modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1454" target="_blank">00:24:14.100</a></span> | <span class="t">and do a deep dive. And let's start by talking about this idea that we were starting to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1459" target="_blank">00:24:19.180</a></span> | <span class="t">here, which is embeddings. And specifically let's move beyond just having embeddings for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1468" target="_blank">00:24:28.100</a></span> | <span class="t">users and items, but embeddings for any kind of categorical variable. So really because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1474" target="_blank">00:24:34.860</a></span> | <span class="t">we know an embedding is just a lookup into an array, it can handle any kind of discrete</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1483" target="_blank">00:24:43.700</a></span> | <span class="t">categorical data. So things like age are not discrete, they're continuous numerical data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1489" target="_blank">00:24:49.140</a></span> | <span class="t">but something like sex or postcode are categorical variables. They have a certain number of discrete</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1496" target="_blank">00:24:56.420</a></span> | <span class="t">levels. The number of discrete levels they have is called their cardinality. So to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1502" target="_blank">00:25:02.740</a></span> | <span class="t">a look at an example of a dataset that contains both categorical and continuous variables,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1510" target="_blank">00:25:10.060</a></span> | <span class="t">we're going to look at the Rossman sales competition that ran on Kaggle a few years ago. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1516" target="_blank">00:25:16.540</a></span> | <span class="t">basically what's going to happen is we're going to see a table that contains information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1521" target="_blank">00:25:21.580</a></span> | <span class="t">about various stores in Germany, and the goal will be to try and predict how many sales</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1526" target="_blank">00:25:26.940</a></span> | <span class="t">there's going to be for each day in a couple of week period for each store.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1534" target="_blank">00:25:34.700</a></span> | <span class="t">One of the interesting things about this competition is that one of the gold medalists used deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1539" target="_blank">00:25:39.980</a></span> | <span class="t">learning, and it was one of the earliest known examples of a state-of-the-art deep learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1545" target="_blank">00:25:45.980</a></span> | <span class="t">tabular model. I mean this is not long ago, 2015 or something, but really this idea of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1552" target="_blank">00:25:52.660</a></span> | <span class="t">creating state-of-the-art tabular models with deep learning has not been very common and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1558" target="_blank">00:25:58.860</a></span> | <span class="t">for not very long. You know interestingly compared to the other gold medalists in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1564" target="_blank">00:26:04.020</a></span> | <span class="t">competition, the folks that use deep learning used a lot less feature engineering and a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1568" target="_blank">00:26:08.500</a></span> | <span class="t">lot less domain expertise. And so they wrote a paper called Entity Embeddings of Categorical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1573" target="_blank">00:26:13.540</a></span> | <span class="t">Variables, in which they basically described the exact thing that you saw in notebook 8,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1581" target="_blank">00:26:21.940</a></span> | <span class="t">the way you can think of one-hot encodings as just being embeddings, you can catenate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1587" target="_blank">00:26:27.420</a></span> | <span class="t">them together, and you can put them through a couple of layers, they call them dense layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1593" target="_blank">00:26:33.140</a></span> | <span class="t">we've called them linear layers, and create a neural network out of that. So this is really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1598" target="_blank">00:26:38.940</a></span> | <span class="t">a neat, you know, kind of simple and obvious hindsight trick. And they actually did exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1605" target="_blank">00:26:45.940</a></span> | <span class="t">what we did in the paper, which is to look at the results of the trained embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1612" target="_blank">00:26:52.900</a></span> | <span class="t">And so for example they had an embedding matrix for regions in Germany, because there wasn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1622" target="_blank">00:27:02.300</a></span> | <span class="t">really metadata about this, these were just learned embeddings, just like we learned embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1626" target="_blank">00:27:06.120</a></span> | <span class="t">about movies. And so then they just created, just like we did before, a chart where they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1632" target="_blank">00:27:12.300</a></span> | <span class="t">popped each region according to, I think probably a PCA of their embeddings. And then if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1638" target="_blank">00:27:18.820</a></span> | <span class="t">circle the ones that are close to each other in blue, you'll see that they're actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1644" target="_blank">00:27:24.140</a></span> | <span class="t">close to each other in Germany, and ditto for red, and ditto for green, and then here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1650" target="_blank">00:27:30.580</a></span> | <span class="t">the brown. So this is like pretty amazing, is the way that we can see that it's kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1658" target="_blank">00:27:38.660</a></span> | <span class="t">of learned something about what Germany looks like, based entirely on the purchasing behavior</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1664" target="_blank">00:27:44.180</a></span> | <span class="t">of people in those states. Something else they did was to look at every store, and they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1670" target="_blank">00:27:50.420</a></span> | <span class="t">looked at the distance between stores in practice, like how many kilometers away they are. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1678" target="_blank">00:27:58.100</a></span> | <span class="t">then they looked at the distance between stores in terms of their embedding distance, just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1683" target="_blank">00:28:03.700</a></span> | <span class="t">like we saw in the previous notebook. And there was this very strong correlation that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1689" target="_blank">00:28:09.260</a></span> | <span class="t">stores that were close to each other physically ended up having close embeddings as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1698" target="_blank">00:28:18.180</a></span> | <span class="t">even though the actual location of these stores in physical space was not part of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1706" target="_blank">00:28:26.180</a></span> | <span class="t">Ditto with days of the week, so the days of the week or another embedding, and the days</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1712" target="_blank">00:28:32.100</a></span> | <span class="t">of the week that were next to each other, ended up next to each other in embedding space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1717" target="_blank">00:28:37.740</a></span> | <span class="t">and ditto for months of the year. So pretty fascinating the way kind of information about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1724" target="_blank">00:28:44.900</a></span> | <span class="t">the world ends up captured just by looking at training embeddings, which as we know are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1730" target="_blank">00:28:50.700</a></span> | <span class="t">just index lookups into an array. So the way we then combine these categorical variables</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1740" target="_blank">00:29:00.220</a></span> | <span class="t">with these embeddings with continuous variables, what was done in both the entity embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1746" target="_blank">00:29:06.620</a></span> | <span class="t">paper that we just looked at, and then also described in more detail by Google when they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1753" target="_blank">00:29:13.060</a></span> | <span class="t">described how their recommendation system in Google Play works. This is from Google's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1758" target="_blank">00:29:18.180</a></span> | <span class="t">paper, is they have the categorical features that go through the embeddings, and then there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1763" target="_blank">00:29:23.260</a></span> | <span class="t">are continuous features, and then all the embedding results and the continuous features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1767" target="_blank">00:29:27.940</a></span> | <span class="t">are just concatenated together into this big concatenated table that then goes through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1772" target="_blank">00:29:32.700</a></span> | <span class="t">this case three layers of a neural net, and interestingly they also take the kind of collaborative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1780" target="_blank">00:29:40.620</a></span> | <span class="t">filtering bit and do the product as well and combine the two. So they use both of the tricks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1786" target="_blank">00:29:46.340</a></span> | <span class="t">were used in the previous notebook and combine them together. So that's the basic idea we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1794" target="_blank">00:29:54.340</a></span> | <span class="t">going to be seeing for moving beyond just collaborative filtering, which is just two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1801" target="_blank">00:30:01.180</a></span> | <span class="t">categorical variables to as many categorical and as many continuous variables as we like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1807" target="_blank">00:30:07.340</a></span> | <span class="t">But before we do that, let's take a step back and think about other approaches, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1812" target="_blank">00:30:12.900</a></span> | <span class="t">as I mentioned, the idea of deep learning as a kind of a best practice for tabular data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1819" target="_blank">00:30:19.940</a></span> | <span class="t">is still pretty new and it's still kind of controversial. It's certainly not always the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1825" target="_blank">00:30:25.500</a></span> | <span class="t">case that it's the best approach. So when we're not using deep learning, what would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1830" target="_blank">00:30:30.980</a></span> | <span class="t">we be using? Well, what we'd probably be using is something called an ensemble of decision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1836" target="_blank">00:30:36.620</a></span> | <span class="t">trees and the two most popular are random forests and gradient boosting machines or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1843" target="_blank">00:30:43.140</a></span> | <span class="t">something similar. So basically between multi-layered neural networks, like with SGD and ensemble</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1849" target="_blank">00:30:49.900</a></span> | <span class="t">of decision trees, that kind of covers the vast majority of approaches that you're likely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1855" target="_blank">00:30:55.700</a></span> | <span class="t">to see for tabular data. And so we're going to make sure we cover them both of course today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1861" target="_blank">00:31:01.820</a></span> | <span class="t">in fact. So although deep learning is nearly always clearly superior for stuff like images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1869" target="_blank">00:31:09.580</a></span> | <span class="t">and audio and natural language text, these two approaches tend to give somewhat similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1875" target="_blank">00:31:15.820</a></span> | <span class="t">results a lot of the time for tabular data. So let's take a look. You know, you really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1881" target="_blank">00:31:21.820</a></span> | <span class="t">should generally try both and see which works best for you for each problem you look at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1888" target="_blank">00:31:28.660</a></span> | <span class="t">Why does the range go from 0 to 5.5 if the maximum is 5?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1898" target="_blank">00:31:38.140</a></span> | <span class="t">That's a great question. The reason is if you think about it for sigmoid, it's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1903" target="_blank">00:31:43.740</a></span> | <span class="t">impossible for a sigmoid to get all the way to the top or all the way to the bottom. Those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1909" target="_blank">00:31:49.180</a></span> | <span class="t">are asymptotes. So no matter how far, how big your x is, it can never quite get to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1914" target="_blank">00:31:54.780</a></span> | <span class="t">top or no matter how small it is, it can never quite get to the bottom. So if you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1918" target="_blank">00:31:58.580</a></span> | <span class="t">be able to actually predict a rating of 5, then you need to use something higher than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1923" target="_blank">00:32:03.100</a></span> | <span class="t">5 your maximum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1927" target="_blank">00:32:07.300</a></span> | <span class="t">Are embeddings used only for highly cardinal categorical variables, or is this approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1932" target="_blank">00:32:12.380</a></span> | <span class="t">used in general? For low cardinality, can one use a one-hot encoding?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1938" target="_blank">00:32:18.500</a></span> | <span class="t">I'll remind you cardinality is the number of discrete levels in a variable. And remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1949" target="_blank">00:32:29.180</a></span> | <span class="t">that an embedding is just a computational shortcut for a one-hot encoding. So there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1956" target="_blank">00:32:36.180</a></span> | <span class="t">really no reason to use a one-hot encoding because it's, as long as you have more than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1962" target="_blank">00:32:42.260</a></span> | <span class="t">two levels, it's always going to be more memory and lower, and give you exactly mathematically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1968" target="_blank">00:32:48.060</a></span> | <span class="t">the same thing. And if there's just two levels, then it is basically identical. So there isn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1973" target="_blank">00:32:53.820</a></span> | <span class="t">really any reason not to use it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1978" target="_blank">00:32:58.180</a></span> | <span class="t">Thank you for those great questions. Okay, so one of the most important things about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1988" target="_blank">00:33:08.980</a></span> | <span class="t">decision tree ensembles is that at the current state of the technology, they do provide faster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1995" target="_blank">00:33:15.100</a></span> | <span class="t">and easier ways of interpreting the model. I think that's rapidly improving for deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=1999" target="_blank">00:33:19.840</a></span> | <span class="t">learning models on tabular data, but that's where we are right now. They also require</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2004" target="_blank">00:33:24.420</a></span> | <span class="t">less hyperparameter tuning, so they're easier to kind of get right the first time. So my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2010" target="_blank">00:33:30.260</a></span> | <span class="t">first approach for analyzing a new tabular data set is always an ensemble of decision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2015" target="_blank">00:33:35.220</a></span> | <span class="t">trees. And specifically, I pretty much always start with a random forest because it's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2019" target="_blank">00:33:39.220</a></span> | <span class="t">so reliable. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2024" target="_blank">00:33:44.260</a></span> | <span class="t">Your experience for highly imbalanced data, such as broad or medical data, what usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2030" target="_blank">00:33:50.100</a></span> | <span class="t">works best out of random forest, XGBoost, or neural networks?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2035" target="_blank">00:33:55.460</a></span> | <span class="t">I'm not sure that whether the data is balanced or unbalanced is a key reason for choosing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2043" target="_blank">00:34:03.260</a></span> | <span class="t">one of those above the others. I would try all of them and see which works best. So the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2049" target="_blank">00:34:09.660</a></span> | <span class="t">exception to the guideline about start with decision tree ensembles is your first thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2053" target="_blank">00:34:13.820</a></span> | <span class="t">to try would be if there's some very high cardinality categorical variables, then they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2058" target="_blank">00:34:18.600</a></span> | <span class="t">can be a bit difficult to get to work really well in decision tree ensembles. Or if there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2065" target="_blank">00:34:25.440</a></span> | <span class="t">something like, most importantly, if it's like plain text data or image data or audio</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2069" target="_blank">00:34:29.380</a></span> | <span class="t">data or something like that, then you're definitely going to need to use a neural net in there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2074" target="_blank">00:34:34.980</a></span> | <span class="t">but you could actually ensemble it with a random forest, as we'll see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2080" target="_blank">00:34:40.420</a></span> | <span class="t">Okay, so clearly we're going to need to understand how decision tree ensembles work. So PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2090" target="_blank">00:34:50.500</a></span> | <span class="t">isn't a great choice for decision tree ensembles. They're really designed for gradient-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2095" target="_blank">00:34:55.420</a></span> | <span class="t">methods and random forests and decision tree growing are not really gradient-based methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2101" target="_blank">00:35:01.780</a></span> | <span class="t">in the same way. So instead, we're going to use a library called scikit-learn, referred</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2108" target="_blank">00:35:08.640</a></span> | <span class="t">to as sklearn as a module. Scikit-learn does a lot of things. We're only going to touch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2116" target="_blank">00:35:16.420</a></span> | <span class="t">on a tiny piece of them, stuff we need to do to train decision trees and random forests.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2124" target="_blank">00:35:24.540</a></span> | <span class="t">We've already mentioned before Wes McKinney's book, also a great book for understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2128" target="_blank">00:35:28.600</a></span> | <span class="t">more about scikit-learn. So the dataset for learning about decision tree ensembles is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2135" target="_blank">00:35:35.500</a></span> | <span class="t">going to be another dataset. It's going to, it's called the blue book for bulldozers dataset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2142" target="_blank">00:35:42.660</a></span> | <span class="t">and it's a Kaggle competition. So Kaggle competitions are fantastic. They are machine learning competitions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2152" target="_blank">00:35:52.060</a></span> | <span class="t">where you get interesting datasets, you get feedback on whether your approach is any good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2156" target="_blank">00:35:56.820</a></span> | <span class="t">or not. You can see on a leaderboard what approaches are working best and then you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2161" target="_blank">00:36:01.140</a></span> | <span class="t">read blog posts from the winning contestants sharing tips and tricks. It's certainly not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2167" target="_blank">00:36:07.940</a></span> | <span class="t">a substitute for actual practice doing end-to-end data science projects, but for becoming good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2179" target="_blank">00:36:19.060</a></span> | <span class="t">at creating predictive models that are predictive, it's a really fantastic resource, highly recommended.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2185" target="_blank">00:36:25.980</a></span> | <span class="t">And you can also submit to old, most old competitions to see how you would have gone without having</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2191" target="_blank">00:36:31.860</a></span> | <span class="t">to worry about, you know, the kind of stress of like whether people will be looking at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2195" target="_blank">00:36:35.980</a></span> | <span class="t">your results because they're not publicized or published if you do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2201" target="_blank">00:36:41.540</a></span> | <span class="t">There's a question. Can you comment on real-time applications of random forests? In my experience,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2209" target="_blank">00:36:49.900</a></span> | <span class="t">they tend to be too slow for real-time use cases like a recommender system, neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2215" target="_blank">00:36:55.680</a></span> | <span class="t">is much faster when run on the right hardware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2218" target="_blank">00:36:58.860</a></span> | <span class="t">Let's get to that once we've seen what they are, shall we? Now you can't just download</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2228" target="_blank">00:37:08.620</a></span> | <span class="t">an untar Kaggle datasets using the untar data thing that we have in fast.ai. So you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2233" target="_blank">00:37:13.540</a></span> | <span class="t">have to sign up to Kaggle and then follow these instructions for how to download data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2240" target="_blank">00:37:20.380</a></span> | <span class="t">from Kaggle. Make sure you replace creds here with what it describes. You need to get a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2244" target="_blank">00:37:24.980</a></span> | <span class="t">special API code and then run this one time to put that up on your server. And now you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2252" target="_blank">00:37:32.020</a></span> | <span class="t">can use Kaggle to download data using the API. So after we do that, we're going to end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2261" target="_blank">00:37:41.720</a></span> | <span class="t">up with a bunch of, as you see, CSV files. So let's take a look at this data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2269" target="_blank">00:37:49.340</a></span> | <span class="t">So the main data, the main table is train.csv. Remember that's comma separated values and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2275" target="_blank">00:37:55.980</a></span> | <span class="t">the training set contains information such as unique identifier of a sale, the unique</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2280" target="_blank">00:38:00.980</a></span> | <span class="t">identifier of a machine, the sale price, sale date. So what's going on here is one row of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2287" target="_blank">00:38:07.100</a></span> | <span class="t">the data represents a sale of a single piece of heavy machinery like a bulldozer at an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2294" target="_blank">00:38:14.620</a></span> | <span class="t">auction. So it happens at a date, as a price, it's of some particular piece of equipment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2300" target="_blank">00:38:20.860</a></span> | <span class="t">and so forth. So if we use pandas again to read in the CSV file, let's combine training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2308" target="_blank">00:38:28.100</a></span> | <span class="t">and valid together. We can then look at the columns to see. There's a lot of columns there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2314" target="_blank">00:38:34.200</a></span> | <span class="t">and many things which I don't know what the hell they mean like blade extension and pad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2317" target="_blank">00:38:37.860</a></span> | <span class="t">type and ride control. But the good news is we're going to show you a way that you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2323" target="_blank">00:38:43.340</a></span> | <span class="t">have to look at every single column and understand what they mean and random forests are going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2328" target="_blank">00:38:48.120</a></span> | <span class="t">to help us with that as well. So once again, we're going to be seeing this idea that models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2333" target="_blank">00:38:53.700</a></span> | <span class="t">can actually help us with data understanding and data cleanup. One thing we can look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2339" target="_blank">00:38:59.460</a></span> | <span class="t">is ordinal columns, a good place to look at that now. If there's things there that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2343" target="_blank">00:39:03.920</a></span> | <span class="t">know are discrete values but have some order like product size, it has medium and small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2351" target="_blank">00:39:11.380</a></span> | <span class="t">and large, medium and many. These should not be in alphabetical order or some random order,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2359" target="_blank">00:39:19.340</a></span> | <span class="t">they should be in this specific order, right? They have a specific ordering. So we can use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2368" target="_blank">00:39:28.820</a></span> | <span class="t">as type to turn it into a categorical variable and then we can say setCategories, audit equals</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2374" target="_blank">00:39:34.420</a></span> | <span class="t">true to basically say this is an ordinal column. So it's got discrete values but we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2380" target="_blank">00:39:40.300</a></span> | <span class="t">want to define what the order of the classes are. We need to choose which is the dependent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2388" target="_blank">00:39:48.260</a></span> | <span class="t">variable and we do that by looking on Kaggle and Kaggle will tell us that the thing we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2392" target="_blank">00:39:52.180</a></span> | <span class="t">meant to be predicting is sale price and actually specifically they'll tell us the thing we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2396" target="_blank">00:39:56.960</a></span> | <span class="t">meant to be predicting is the log of sale price because root mean squared log error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2402" target="_blank">00:40:02.060</a></span> | <span class="t">is what we're actually going to be judged on in the competition where we take the log.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2409" target="_blank">00:40:09.020</a></span> | <span class="t">So we're not going to replace sale price with its log and that's what we'll be using from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2412" target="_blank">00:40:12.940</a></span> | <span class="t">now on. So a decision tree ensemble requires decision trees. So let's start by looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2420" target="_blank">00:40:20.340</a></span> | <span class="t">at decision trees. So a decision tree in this case is a something that asks a series of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2428" target="_blank">00:40:28.060</a></span> | <span class="t">binary that is yes or no questions about data. So such as is somebody less than or greater</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2434" target="_blank">00:40:34.540</a></span> | <span class="t">than 30? Yes they are. Are they eating healthily? Yes they are and so okay then we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2439" target="_blank">00:40:39.700</a></span> | <span class="t">to say they're fit or unfit. So like there's an example of some arbitrary decision tree</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2446" target="_blank">00:40:46.540</a></span> | <span class="t">that somebody might have come up with. It's a series of binary yes and no choices and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2451" target="_blank">00:40:51.620</a></span> | <span class="t">at the bottom are leaf nodes that make some prediction. Now of course for our bulldozers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2462" target="_blank">00:41:02.380</a></span> | <span class="t">competition we don't know what binary questions to ask about these things and in what order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2470" target="_blank">00:41:10.180</a></span> | <span class="t">in order to make a prediction about sale price. So we're doing machine learning so we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2475" target="_blank">00:41:15.180</a></span> | <span class="t">to try and come up with some automated way to create the questions. And there's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2480" target="_blank">00:41:20.700</a></span> | <span class="t">a really simple procedure for doing that. You have to think about it. So if you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2484" target="_blank">00:41:24.620</a></span> | <span class="t">to kind of stretch yourself here have a think about what's an automatic procedure that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2490" target="_blank">00:41:30.620</a></span> | <span class="t">can come up with that would automatically build a decision tree where the final answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2496" target="_blank">00:41:36.300</a></span> | <span class="t">would do a you know significantly better than random job of estimating the sale price of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2504" target="_blank">00:41:44.220</a></span> | <span class="t">one of these auctions. Alright so here's the approach that we could use. Loop through each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2513" target="_blank">00:41:53.500</a></span> | <span class="t">column of the data set. We're going to go through each of well obviously not sale price</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2519" target="_blank">00:41:59.300</a></span> | <span class="t">it's a dependent variable sale ID machine ID auctioneer year made etc. And so one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2525" target="_blank">00:42:05.140</a></span> | <span class="t">those will be for example product size. And so then what we're going to do is we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2531" target="_blank">00:42:11.660</a></span> | <span class="t">to loop through each possible value of product size large, large, medium, medium etc. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2541" target="_blank">00:42:21.380</a></span> | <span class="t">then we're going to do a split basically like where this comma is and we're going to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2545" target="_blank">00:42:25.260</a></span> | <span class="t">okay let's get all of the auctions of large equipment and put that into one group and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2552" target="_blank">00:42:32.820</a></span> | <span class="t">everything that's smaller than that and put that into another group. And so that's here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2558" target="_blank">00:42:38.900</a></span> | <span class="t">split the data into two groups based on whether they're greater than or less than that value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2565" target="_blank">00:42:45.740</a></span> | <span class="t">If it's a categorical non-ordinal value a variable it'll be just whether it's equal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2569" target="_blank">00:42:49.740</a></span> | <span class="t">or not equal at that level. And then we're going to find the average sale price for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2575" target="_blank">00:42:55.620</a></span> | <span class="t">of the two groups. So for the large group what was the average sale price? For the smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2580" target="_blank">00:43:00.900</a></span> | <span class="t">than large group what was the average sale price? And that will be our model. Our prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2586" target="_blank">00:43:06.940</a></span> | <span class="t">will simply be the average sale price for that group. And so then you can say well how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2592" target="_blank">00:43:12.460</a></span> | <span class="t">good is that model? If our model was just to ask a single question with a yes/no answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2597" target="_blank">00:43:17.380</a></span> | <span class="t">put things into two groups and take the average of the group as being our prediction and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2602" target="_blank">00:43:22.260</a></span> | <span class="t">can say how good would that model be? What would be the root mean squared error from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2606" target="_blank">00:43:26.140</a></span> | <span class="t">that model? And so we can then say all right how good would it be if we use large as a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2612" target="_blank">00:43:32.580</a></span> | <span class="t">split? And then let's try again what if we did large/medium as a split? What if we did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2618" target="_blank">00:43:38.260</a></span> | <span class="t">medium as a split? And so in each case we can find the root mean squared error of that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2622" target="_blank">00:43:42.180</a></span> | <span class="t">incredibly simple model. And then once we've done that for all of the product size levels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2627" target="_blank">00:43:47.020</a></span> | <span class="t">we can go to the next column and look at level of usage band and do every level of usage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2635" target="_blank">00:43:55.380</a></span> | <span class="t">band and then state, every level of state and so forth. And so there'll be some variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2642" target="_blank">00:44:02.860</a></span> | <span class="t">and some split level which gives the best root mean squared error of this really really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2649" target="_blank">00:44:09.540</a></span> | <span class="t">simple model. And so then we'll say okay that would be our first binary decision. It gives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2656" target="_blank">00:44:16.220</a></span> | <span class="t">us two groups and then we're going to take each one of those groups separately and find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2662" target="_blank">00:44:22.580</a></span> | <span class="t">another single binary decision for each of those two groups using exactly the same procedure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2668" target="_blank">00:44:28.820</a></span> | <span class="t">So then we'll have four groups and then we'll do exactly the same thing again separately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2673" target="_blank">00:44:33.460</a></span> | <span class="t">for each of those four groups and so forth. So let's see what that looks like and in fact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2684" target="_blank">00:44:44.180</a></span> | <span class="t">once we've gone through this you might even want to see if you can implement this algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2687" target="_blank">00:44:47.500</a></span> | <span class="t">yourself. It's not trivial but it doesn't require any special coding skills so hopefully</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2695" target="_blank">00:44:55.020</a></span> | <span class="t">you can find you'll be able to do it. There's a few things we have to do before we can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2700" target="_blank">00:45:00.820</a></span> | <span class="t">create a decision tree in terms of just some basic data munching. One is if we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2706" target="_blank">00:45:06.460</a></span> | <span class="t">to take advantage of dates we actually want to call fastai's addDatePart function and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2713" target="_blank">00:45:13.660</a></span> | <span class="t">what that does as you see after we call it is it creates a whole different a bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2718" target="_blank">00:45:18.980</a></span> | <span class="t">different bits of metadata from that data. Say a year, say a month, say a week, say a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2724" target="_blank">00:45:24.380</a></span> | <span class="t">day and so forth. So say a date of itself doesn't have a whole lot of information directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2735" target="_blank">00:45:35.020</a></span> | <span class="t">but we can pull lots of different information out of it. And so this is an example of something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2739" target="_blank">00:45:39.340</a></span> | <span class="t">called feature engineering which is where we take some piece of some piece of data and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2744" target="_blank">00:45:44.220</a></span> | <span class="t">we try to grab create lots of other pieces of data from it. So is this particular date</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2750" target="_blank">00:45:50.180</a></span> | <span class="t">the end of a month or not? At the end of a year or not? And so forth. So that handle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2756" target="_blank">00:45:56.940</a></span> | <span class="t">states there's a bit more cleaning we want to do and fastai provides some things to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2763" target="_blank">00:46:03.700</a></span> | <span class="t">cleaning easier. We can use the tabular pandas class to create a tabular data set in pandas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2773" target="_blank">00:46:13.460</a></span> | <span class="t">And specifically we're going to use two tabular processes or tabular procs. A tabular processor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2779" target="_blank">00:46:19.940</a></span> | <span class="t">is basically just a transform and we've seen transforms before so go back and remind yourself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2784" target="_blank">00:46:24.580</a></span> | <span class="t">what a transform is. Except it's just slightly different it's like three lines of code if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2790" target="_blank">00:46:30.620</a></span> | <span class="t">you look at the code for it. It's actually going to modify the object in place rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2796" target="_blank">00:46:36.260</a></span> | <span class="t">than creating a new object and giving it back to you. And that's because often these tables</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2800" target="_blank">00:46:40.420</a></span> | <span class="t">of data are kind of really big and we don't want to waste lots of RAM. And it's just going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2806" target="_blank">00:46:46.300</a></span> | <span class="t">to run the transform once and save the result rather than doing it lazily when you access</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2811" target="_blank">00:46:51.060</a></span> | <span class="t">it for the same reason. We're just going to make this a lot faster. So you can just think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2817" target="_blank">00:46:57.160</a></span> | <span class="t">of them as transforms really. One of them is called categorify and categorify is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2822" target="_blank">00:47:02.020</a></span> | <span class="t">to replace a column with numeric categories using the same basic idea of like a vocab</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2829" target="_blank">00:47:09.340</a></span> | <span class="t">like we've seen before. Fill missing is going to find any columns with missing data that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2836" target="_blank">00:47:16.240</a></span> | <span class="t">going to fill in the missing data with the median of the data and create a new column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2841" target="_blank">00:47:21.100</a></span> | <span class="t">a boolean column which is set to true for anything that was missing. So these two things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2845" target="_blank">00:47:25.760</a></span> | <span class="t">is basically enough to get you to a point where most of the time you'll be able to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2849" target="_blank">00:47:29.260</a></span> | <span class="t">a model. Now the next thing we need to do is think about our validation set. As we discussed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2857" target="_blank">00:47:37.340</a></span> | <span class="t">in lesson one, a random validation set is not always appropriate and certainly for something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2864" target="_blank">00:47:44.020</a></span> | <span class="t">like predicting auction results it almost certainly is not appropriate because we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2869" target="_blank">00:47:49.260</a></span> | <span class="t">going to be wanting to use a model in the future not at some random date in the past.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2874" target="_blank">00:47:54.660</a></span> | <span class="t">So the way this Kaggle competition was set up was that the test set the thing that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2880" target="_blank">00:48:00.680</a></span> | <span class="t">had to fill in and submit for the competition was two weeks of data that was after any of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2888" target="_blank">00:48:08.860</a></span> | <span class="t">the training set. So we should do the same thing for a validation set. We should create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2894" target="_blank">00:48:14.580</a></span> | <span class="t">something which is where the validation set is the last couple of weeks of data and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2902" target="_blank">00:48:22.820</a></span> | <span class="t">then the training set will only be data before that. So we basically can do that by grabbing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2908" target="_blank">00:48:28.340</a></span> | <span class="t">everything before October 2011, create a training and validation set based on that condition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2915" target="_blank">00:48:35.260</a></span> | <span class="t">and grabbing those bits. So that's going to split our training set and validation set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2923" target="_blank">00:48:43.520</a></span> | <span class="t">by date not randomly. We're also going to need to tell when you create a tabular pandas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2930" target="_blank">00:48:50.460</a></span> | <span class="t">object you're going to be passing in a data frame, going to be passing in your tabular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2934" target="_blank">00:48:54.980</a></span> | <span class="t">procs and you also have to say what are my categorical and continuous variables. We can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2940" target="_blank">00:49:00.100</a></span> | <span class="t">use fast.ai's cont.cat.split to automatically split a data frame to continuous and categorical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2947" target="_blank">00:49:07.820</a></span> | <span class="t">variables for you. So we can just pass those in. Tell it what is the dependent variable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2954" target="_blank">00:49:14.940</a></span> | <span class="t">you can have more than one, and what are the indexes to split into training and valid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2960" target="_blank">00:49:20.460</a></span> | <span class="t">And this is a tabular object. So it's got all the information you need about the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2964" target="_blank">00:49:24.720</a></span> | <span class="t">set, the validation set, categorical and continuous variables and the dependent variable and any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2970" target="_blank">00:49:30.060</a></span> | <span class="t">processes to run. It looks a lot like a datasets object, but it has a .train, it has a .valid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2981" target="_blank">00:49:41.060</a></span> | <span class="t">and so if we have a look at .show we can see the data. But .show is going to show us the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2990" target="_blank">00:49:50.740</a></span> | <span class="t">kind of the string data, but if we look at .items you can see internally it's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=2996" target="_blank">00:49:56.800</a></span> | <span class="t">stored these very compact numbers which we can use directly in a model. So fast.ai has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3006" target="_blank">00:50:06.140</a></span> | <span class="t">basically got us to a point here where we have our data into a format ready for modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3011" target="_blank">00:50:11.500</a></span> | <span class="t">and our validation sets being created. To see how these numbers relate to these strings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3019" target="_blank">00:50:19.580</a></span> | <span class="t">we can again just like we saw last week use the classes attribute which is a dictionary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3025" target="_blank">00:50:25.220</a></span> | <span class="t">which basically tells us the vocab. So this is how we look up. For example 6 is 0, 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3030" target="_blank">00:50:30.820</a></span> | <span class="t">2, 3, 4, 5, 6. This is a compact example. That processing took takes a little while to run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3039" target="_blank">00:50:39.260</a></span> | <span class="t">so you can go ahead and save the tabular object and so then you can load it back later without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3046" target="_blank">00:50:46.540</a></span> | <span class="t">having to rerun all the processing. So that's a nice kind of fast way to quickly get back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3052" target="_blank">00:50:52.820</a></span> | <span class="t">up and running without having to reprocess your data. So we've done the basic data munging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3059" target="_blank">00:50:59.100</a></span> | <span class="t">we need. So we can now create a decision tree and in scikit-learn a decision tree where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3064" target="_blank">00:51:04.180</a></span> | <span class="t">the dependent variable is continuous is a decision tree regressor. And let's start by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3070" target="_blank">00:51:10.440</a></span> | <span class="t">telling it we just want a total of four leaf nodes. We'll see what that means in a moment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3076" target="_blank">00:51:16.980</a></span> | <span class="t">and in scikit-learn you generally call fit so it looks quite a lot like fast.ai and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3083" target="_blank">00:51:23.060</a></span> | <span class="t">pass in your independent variables and your dependent variable and we can grab those straight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3088" target="_blank">00:51:28.340</a></span> | <span class="t">from our tabular object training set is .x's and .y and we can do the same thing for validation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3095" target="_blank">00:51:35.860</a></span> | <span class="t">just to save us in typing. Okay, question. Do you have any thoughts on what data augmentation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3101" target="_blank">00:51:41.820</a></span> | <span class="t">for tabular data might look like? I don't have a great sense of data augmentation for tabular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3113" target="_blank">00:51:53.660</a></span> | <span class="t">data. We'll be seeing later either in this course or in the next part dropout and mix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3123" target="_blank">00:52:03.200</a></span> | <span class="t">up and stuff like that which they might be able to do that in later layers in the tabular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3131" target="_blank">00:52:11.260</a></span> | <span class="t">model. Otherwise I think you'd need to think about kind of the semantics of the data and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3136" target="_blank">00:52:16.220</a></span> | <span class="t">think about what are things you could do to change the data without changing the meaning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3141" target="_blank">00:52:21.060</a></span> | <span class="t">That's like a pretty tricky route. There question. Does fast.ai distinguish between ordered categories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3149" target="_blank">00:52:29.340</a></span> | <span class="t">such as low, medium, high and unordered categorical variables? Yes, that was that ordinal thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3156" target="_blank">00:52:36.180</a></span> | <span class="t">I told you about before and all it really does is it ensures that your classes list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3162" target="_blank">00:52:42.300</a></span> | <span class="t">has a specific order so then these numbers actually have a specific order. And as you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3167" target="_blank">00:52:47.860</a></span> | <span class="t">see that's actually going to turn out to be pretty important for how we train our random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3171" target="_blank">00:52:51.820</a></span> | <span class="t">forest. Okay, so we can create a decision tree regressor. We can fit it and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3180" target="_blank">00:53:00.300</a></span> | <span class="t">can draw it, the fast.ai function. And here is the decision tree we just trained and behind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3190" target="_blank">00:53:10.380</a></span> | <span class="t">the scenes this actually used the basically the exact process that we described back here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3199" target="_blank">00:53:19.700</a></span> | <span class="t">right? So this is where you can like try and create your own decision tree implementation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3205" target="_blank">00:53:25.380</a></span> | <span class="t">if you're interested in stretching yourself. So we're going to use one that's already exists</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3211" target="_blank">00:53:31.880</a></span> | <span class="t">and the best way to understand what it's done is to look at this diagram from top to bottom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3217" target="_blank">00:53:37.060</a></span> | <span class="t">So the first step is it says like okay the initial model it created is a model with no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3224" target="_blank">00:53:44.660</a></span> | <span class="t">binary splits at all. Specifically it's always going to predict the value 10.1 for every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3230" target="_blank">00:53:50.600</a></span> | <span class="t">single row. Why is that? Well because this is the simplest possible model is to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3237" target="_blank">00:53:57.020</a></span> | <span class="t">the average of the dependent variable and always predict that. And so this is always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3242" target="_blank">00:54:02.100</a></span> | <span class="t">should be your kind of pretty much your basic baseline for regression. There are four hundred</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3248" target="_blank">00:54:08.720</a></span> | <span class="t">and four thousand seven hundred and ten rows, auctions that we're averaging and the mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3254" target="_blank">00:54:14.660</a></span> | <span class="t">squared error of this incredibly simple model in which there are no rules at all, no groups</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3260" target="_blank">00:54:20.860</a></span> | <span class="t">at all, just a single average is a point for it. So then the next most complex model is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3269" target="_blank">00:54:29.300</a></span> | <span class="t">to take a single column, a plus system and a single binary decision is coupler system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3275" target="_blank">00:54:35.980</a></span> | <span class="t">less than or equal to 0.5. True, there are three hundred and sixty thousand eight hundred</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3281" target="_blank">00:54:41.780</a></span> | <span class="t">and forty seven auctions where it's true and forty three thousand eight hundred and sixty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3287" target="_blank">00:54:47.740</a></span> | <span class="t">three where it's false. And now interestingly in the false case you can see that there are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3294" target="_blank">00:54:54.100</a></span> | <span class="t">no further binary decisions. So this is called a leaf node. It's a node where this is as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3299" target="_blank">00:54:59.620</a></span> | <span class="t">far as you can get and so if your coupler system is not less than or equal to 0.5 then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3307" target="_blank">00:55:07.340</a></span> | <span class="t">the prediction this model makes for your sale price is 9.21 versus if it's true it's 10.21.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3315" target="_blank">00:55:15.100</a></span> | <span class="t">So you can see it's actually found a very big difference here and that's why it picked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3319" target="_blank">00:55:19.220</a></span> | <span class="t">this as the first binary split. And so the mean squared error for this section here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3323" target="_blank">00:55:23.940</a></span> | <span class="t">0.12 which is far better than we started out at, 0.48. This group still has 360,000 in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3332" target="_blank">00:55:32.380</a></span> | <span class="t">it and so it does another binary split. This time is the year that this piece of equipment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3338" target="_blank">00:55:38.340</a></span> | <span class="t">made was at less than or equal to 1991.5. If it was, if it's true then we get a leaf node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3347" target="_blank">00:55:47.340</a></span> | <span class="t">and the prediction is 9.97, mean squared error 0.37. If the value is false we don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3353" target="_blank">00:55:53.420</a></span> | <span class="t">a leaf node and we have another binary split. And you can see eventually we get down to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3357" target="_blank">00:55:57.740</a></span> | <span class="t">here coupler system true, year made, false, product size, false, mean squared error 0.17.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3365" target="_blank">00:56:05.020</a></span> | <span class="t">So all of these leaf nodes have MSCs that are smaller than that original baseline model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3373" target="_blank">00:56:13.780</a></span> | <span class="t">of just taking the mean. So this is how you can grow a decision tree. And we only stopped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3379" target="_blank">00:56:19.660</a></span> | <span class="t">here because we said max leaf nodes is 4, 1, 2, 3, 4, right? And so if we want to keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3387" target="_blank">00:56:27.140</a></span> | <span class="t">training it further we can just use a higher number. There's actually a very nice library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3396" target="_blank">00:56:36.220</a></span> | <span class="t">by Terrence Park called dtree-vis which can show us exactly the same information like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3402" target="_blank">00:56:42.220</a></span> | <span class="t">so. And so here are the same leaf nodes 1, 2, 3, 4. And you can see the kind of the chart</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3409" target="_blank">00:56:49.980</a></span> | <span class="t">of how many are there. This is the split, coupler system 0.5. Here are the two groups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3415" target="_blank">00:56:55.460</a></span> | <span class="t">You can see the sale price in each of the two groups. And then here's the leaf node.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3420" target="_blank">00:57:00.660</a></span> | <span class="t">And so then the second split was on year made. And you can see here something weird is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3425" target="_blank">00:57:05.300</a></span> | <span class="t">on with year made. There's a whole bunch of year mades that are a thousand which is obviously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3429" target="_blank">00:57:09.700</a></span> | <span class="t">not a sensible year for a bulldozer to be made. So presumably that's some kind of missing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3435" target="_blank">00:57:15.140</a></span> | <span class="t">value. So when we look at the kind of the picture like this it can give us some insights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3441" target="_blank">00:57:21.400</a></span> | <span class="t">about what's going on in our data. And so maybe we should replace those thousands with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3448" target="_blank">00:57:28.700</a></span> | <span class="t">1950 because that's you know obviously a very, very early year for a bulldozer. So we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3454" target="_blank">00:57:34.940</a></span> | <span class="t">kind of pick it arbitrarily. It's actually not really going to make any difference to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3459" target="_blank">00:57:39.700</a></span> | <span class="t">the model that's created because all we care about is the order because we're just doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3464" target="_blank">00:57:44.740</a></span> | <span class="t">these binary splits that it'll make it easier to look at as you can see. Here's our 1950s</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3470" target="_blank">00:57:50.420</a></span> | <span class="t">now. And so now it's much easier to see what's going on in that binary split. So let's now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3478" target="_blank">00:57:58.420</a></span> | <span class="t">get rid of max leaf nodes and build a bigger decision tree. And then let's just for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3485" target="_blank">00:58:05.060</a></span> | <span class="t">rest of this notebook create a couple of little functions. One to create the root mean squared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3490" target="_blank">00:58:10.220</a></span> | <span class="t">error which is just here. And another one to take a model and some independent independent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3496" target="_blank">00:58:16.900</a></span> | <span class="t">variables, predict from the model on the independent variables and then take the root mean squared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3503" target="_blank">00:58:23.180</a></span> | <span class="t">error with a dependent variable. So that's going to be our models root mean squared error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3509" target="_blank">00:58:29.700</a></span> | <span class="t">So for this decision tree in which we didn't have a stopping criteria, so as many leaf</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3513" target="_blank">00:58:33.900</a></span> | <span class="t">nodes as you like, the model's root mean squared error is zero. So we've just built the perfect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3521" target="_blank">00:58:41.580</a></span> | <span class="t">model. So this is great news, right? We've built the perfect auction trading system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3529" target="_blank">00:58:49.660</a></span> | <span class="t">Well remember, we actually need to check the validation set. Let's check the check mRmse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3534" target="_blank">00:58:54.620</a></span> | <span class="t">with a validation set and oh, it's worse than zero. So our training set is zero, our validation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3542" target="_blank">00:59:02.540</a></span> | <span class="t">set is much worse than zero. Why has that happened? Well one of the things that a random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3548" target="_blank">00:59:08.660</a></span> | <span class="t">forest in sklearn can do is it can tell you the number of leaf nodes, number of leaves,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3554" target="_blank">00:59:14.540</a></span> | <span class="t">there are 341,000, number of data points 400,000. So in other words, we have nearly as many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3562" target="_blank">00:59:22.460</a></span> | <span class="t">leaf nodes as data points. Most of our leaf nodes only have a single thing in, but they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3566" target="_blank">00:59:26.780</a></span> | <span class="t">taking an average of a single thing. Clearly this makes no sense at all. So what we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3572" target="_blank">00:59:32.060</a></span> | <span class="t">actually do is pick some different stopping criteria and let's say, okay, if you get a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3578" target="_blank">00:59:38.180</a></span> | <span class="t">leaf node with 25 things or less in it, don't split things to create a leaf node with less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3585" target="_blank">00:59:45.840</a></span> | <span class="t">than 25 things in it. And now if we fit and we look at the root mean squared error for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3591" target="_blank">00:59:51.540</a></span> | <span class="t">the validation set, it's going to go down from 0.33 to 0.32. So the training sets got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3599" target="_blank">00:59:59.460</a></span> | <span class="t">worse from zero to 0.248. The validation sets got better and now we only have 12,000 leaf</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3606" target="_blank">01:00:06.300</a></span> | <span class="t">nodes. So that is much more reasonable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3610" target="_blank">01:00:10.100</a></span> | <span class="t">Alright, so let's take a five minute break and then we're going to come back and see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3615" target="_blank">01:00:15.260</a></span> | <span class="t">how we get the best of both worlds, how we're going to get something which has the kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3619" target="_blank">01:00:19.660</a></span> | <span class="t">of flexibility to get these, you know, what we're going to get down to zero, but to get,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3626" target="_blank">01:00:26.540</a></span> | <span class="t">you know, really deep trees, but also without overfitting. And the trick will be to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3632" target="_blank">01:00:32.860</a></span> | <span class="t">something called bagging. We'll come back and talk about that in five minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3639" target="_blank">01:00:39.460</a></span> | <span class="t">Okay, welcome back. So we're going to look at how we can get the best of both worlds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3649" target="_blank">01:00:49.500</a></span> | <span class="t">as we discussed and let's start by having a look at what we're doing with categorical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3656" target="_blank">01:00:56.420</a></span> | <span class="t">variables first of all. And so you might notice that previously with categorical variables,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3663" target="_blank">01:01:03.500</a></span> | <span class="t">for example, in collaborative filtering, we had to, you know, kind of think about like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3670" target="_blank">01:01:10.500</a></span> | <span class="t">how many embedding levels we have, for example, if you've used other modeling tools, you might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3675" target="_blank">01:01:15.780</a></span> | <span class="t">have doing things with creating dummy variables, stuff like that. For random forests on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3681" target="_blank">01:01:21.780</a></span> | <span class="t">whole, you don't have to. The reason is, as we've seen, all of our categorical variables</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3692" target="_blank">01:01:32.460</a></span> | <span class="t">have been turned into numbers. And so we can perfectly well have decision tree binary decisions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3701" target="_blank">01:01:41.420</a></span> | <span class="t">which use those particular numbers. Now, the numbers might not be ordered in any interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3709" target="_blank">01:01:49.260</a></span> | <span class="t">way, but if there's a particular level which kind of stands out as being important, it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3716" target="_blank">01:01:56.380</a></span> | <span class="t">only takes two binary splits to split out that level into a single, you know, into a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3724" target="_blank">01:02:04.420</a></span> | <span class="t">single piece. So generally speaking, I don't normally worry too much about kind of encoding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3734" target="_blank">01:02:14.140</a></span> | <span class="t">categorical variables in a special way. As I mentioned, I do try to encode ordinal variables</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3739" target="_blank">01:02:19.980</a></span> | <span class="t">by saying what the order of the levels is, because often, as you would expect, sizes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3746" target="_blank">01:02:26.180</a></span> | <span class="t">for example, you know, medium and small are going to mean kind of next to each other and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3750" target="_blank">01:02:30.860</a></span> | <span class="t">large and extra large would be next to each other. That's good to have those as similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3754" target="_blank">01:02:34.580</a></span> | <span class="t">numbers. Having said that, you can kind of one hot encode a categorical variable if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3763" target="_blank">01:02:43.700</a></span> | <span class="t">want to using get dummies in pandas. But there's not a lot of evidence that that actually helps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3771" target="_blank">01:02:51.380</a></span> | <span class="t">There's actually that has been stored in a paper. And so I would say in general for categorical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3777" target="_blank">01:02:57.540</a></span> | <span class="t">variables don't worry about it too much. Just use what we've shown you. You have a question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3784" target="_blank">01:03:04.500</a></span> | <span class="t">For ordinal categorical variables, how do you deal with when they have like nA or missing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3792" target="_blank">01:03:12.460</a></span> | <span class="t">values, where do you put that in the order? So in fast.ai, nA missing values always appear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3802" target="_blank">01:03:22.300</a></span> | <span class="t">as the first item. They'll always be the zero index item. And also if you get something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3807" target="_blank">01:03:27.480</a></span> | <span class="t">in the validation or test set, which is a level we haven't seen in training, that will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3812" target="_blank">01:03:32.140</a></span> | <span class="t">be considered to be that missing or nA value as well. All right, so what we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3821" target="_blank">01:03:41.020</a></span> | <span class="t">do to try and improve our random forest is we're going to use something called bagging.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3826" target="_blank">01:03:46.420</a></span> | <span class="t">This was developed by a retired Berkeley professor named Leo Breiman in 1994. And he did a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3834" target="_blank">01:03:54.180</a></span> | <span class="t">of great work and perhaps you could argue that most of it happened after he retired.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3839" target="_blank">01:03:59.700</a></span> | <span class="t">His technical report was called bagging predictors. And he described how you could create multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3845" target="_blank">01:04:05.260</a></span> | <span class="t">versions of a predictor, so multiple different models. And you could then aggregate them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3851" target="_blank">01:04:11.740</a></span> | <span class="t">by averaging over the predictions. And specifically, the way he suggested doing this was to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3860" target="_blank">01:04:20.540</a></span> | <span class="t">what he called bootstrap replicates. In other words, randomly select different subsets of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3865" target="_blank">01:04:25.860</a></span> | <span class="t">your data. Train a model on that subset, kind of store it away as one of your predictors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3871" target="_blank">01:04:31.820</a></span> | <span class="t">and then do it again a bunch of times. And so each of these models is trained on a different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3876" target="_blank">01:04:36.460</a></span> | <span class="t">random subset of your data. And then you, to predict, you predict on all of those different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3883" target="_blank">01:04:43.380</a></span> | <span class="t">versions of your model and average them. And it turns out that bagging works really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3892" target="_blank">01:04:52.300</a></span> | <span class="t">So this, the sequence of steps is basically randomly choose some subset of rows, train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3898" target="_blank">01:04:58.540</a></span> | <span class="t">a model using that subset, save that model, and then return to step one. Do that a few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3904" target="_blank">01:05:04.180</a></span> | <span class="t">times to train a few models. And then to make a prediction, predict with all the models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3910" target="_blank">01:05:10.300</a></span> | <span class="t">and take the average. That is bagging. And it's very simple, but it's astonishingly powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3918" target="_blank">01:05:18.300</a></span> | <span class="t">And the reason why is that each of these models we've trained, although they are not using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3925" target="_blank">01:05:25.480</a></span> | <span class="t">all of the data, so they're kind of less accurate than a model that uses all of the data. Each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3931" target="_blank">01:05:31.980</a></span> | <span class="t">of them is, the errors are not correlated, you know, the errors because of using that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3939" target="_blank">01:05:39.880</a></span> | <span class="t">smaller subset are not correlated with the errors of the other models because they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3944" target="_blank">01:05:44.140</a></span> | <span class="t">random subsets. And so when you take the average of a bunch of kind of errors which are not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3954" target="_blank">01:05:54.100</a></span> | <span class="t">correlated with each other, the average of those errors is zero. So therefore, the average</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3961" target="_blank">01:06:01.080</a></span> | <span class="t">of the models should give us an accurate prediction of the thing we're actually trying to predict.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3968" target="_blank">01:06:08.380</a></span> | <span class="t">So as I say here, it's an amazing result. We can improve the accuracy of nearly any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3972" target="_blank">01:06:12.540</a></span> | <span class="t">kind of algorithm by training it multiple times on different random subsets of data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3978" target="_blank">01:06:18.380</a></span> | <span class="t">and then averaging the predictions. So then Breiman in 2001 showed a way to do this specifically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3987" target="_blank">01:06:27.140</a></span> | <span class="t">for decision trees where not only did he randomly choose a subset of rows for each model, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=3993" target="_blank">01:06:33.700</a></span> | <span class="t">then for each binary split, he also randomly selected a subset of columns. And this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4000" target="_blank">01:06:40.200</a></span> | <span class="t">called the random first. And it's perhaps the most widely used, most practically important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4005" target="_blank">01:06:45.860</a></span> | <span class="t">machine learning method and astonishingly simple. To create a random forest regressor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4014" target="_blank">01:06:54.100</a></span> | <span class="t">you use sklearn's random forest regressor. If you pass njobs -1, it will use all of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4020" target="_blank">01:07:00.980</a></span> | <span class="t">CPU cores that you have to run as fast as possible. nestimators says how many trees,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4027" target="_blank">01:07:07.420</a></span> | <span class="t">how many models to train. max_sample says how many rows to use, randomly chosen rows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4035" target="_blank">01:07:15.100</a></span> | <span class="t">to use in each one. max_features is how many randomly chosen columns to use for each binary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4041" target="_blank">01:07:21.860</a></span> | <span class="t">split point. min_sample's leaf is the stopping criteria and we'll come back to. So here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4049" target="_blank">01:07:29.960</a></span> | <span class="t">a little function that will create a random first regressor and fit it through some set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4055" target="_blank">01:07:35.580</a></span> | <span class="t">of independent variables and a dependent variable. So we can give it a few default values and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4063" target="_blank">01:07:43.460</a></span> | <span class="t">create a random forest and train and our validation set RMSE is 0.23. If we compare that to what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4075" target="_blank">01:07:55.500</a></span> | <span class="t">we had before, we had 0.32. So dramatically better by using a random forest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4093" target="_blank">01:08:13.140</a></span> | <span class="t">So what's happened when we called random forest regressor is it's just using that decision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4102" target="_blank">01:08:22.020</a></span> | <span class="t">tree builder that we've already seen, but it's building multiple versions with these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4106" target="_blank">01:08:26.480</a></span> | <span class="t">different random subsets and for each binary split it does, it's also randomly selecting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4112" target="_blank">01:08:32.260</a></span> | <span class="t">a subset of columns. And then when we create a prediction, it is averaging the predictions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4118" target="_blank">01:08:38.880</a></span> | <span class="t">of each of the trees. And as you can see it's giving a really great result. And one of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4125" target="_blank">01:08:45.260</a></span> | <span class="t">amazing things we'll find is that it's going to be hard for us to improve this very much,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4130" target="_blank">01:08:50.540</a></span> | <span class="t">you know, the kind of the default starting point tends to turn out to be pretty great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4139" target="_blank">01:08:59.500</a></span> | <span class="t">The sklearn docs have lots of good information in. One of the things that has this nice picture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4143" target="_blank">01:09:03.980</a></span> | <span class="t">that shows as you increase the number of estimators, how does the accuracy improve, error rate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4151" target="_blank">01:09:11.620</a></span> | <span class="t">improves for different max features levels. And in general, the more trees you add, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4161" target="_blank">01:09:21.100</a></span> | <span class="t">more accurate your model. It's not going to overfit, right, because it's averaging more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4166" target="_blank">01:09:26.060</a></span> | <span class="t">of these, these weak models, more of these models that are trained on subsets of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4174" target="_blank">01:09:34.020</a></span> | <span class="t">data. So train as many, use as many estimators as you like, really just a case of how much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4180" target="_blank">01:09:40.420</a></span> | <span class="t">time do you have and whether you kind of reach a point where it's not really improving anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4185" target="_blank">01:09:45.980</a></span> | <span class="t">You can actually get at the underlying decision trees in a model, in a random forest model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4190" target="_blank">01:09:50.620</a></span> | <span class="t">using estimators_. So with a list comprehension, we can call predict on each individual tree.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4197" target="_blank">01:09:57.900</a></span> | <span class="t">And so here's an array, a numpy array containing the predictions from each individual tree</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4203" target="_blank">01:10:03.760</a></span> | <span class="t">for each row in our data. So if we take the mean across the zero axis, we'll get exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4215" target="_blank">01:10:15.100</a></span> | <span class="t">the same number. Because remember, that's what a random forest does, is it takes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4221" target="_blank">01:10:21.380</a></span> | <span class="t">mean of the trees, predictions. So one cool thing we could do is we could look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4231" target="_blank">01:10:31.340</a></span> | <span class="t">40 estimators we have and grab the predictions for the first i of those trees and take their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4242" target="_blank">01:10:42.020</a></span> | <span class="t">mean and then we can find the root mean squared error. And so in other words, here is the accuracy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4250" target="_blank">01:10:50.220</a></span> | <span class="t">when you've just got one tree, two trees, three trees, four trees, five trees, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4256" target="_blank">01:10:56.100</a></span> | <span class="t">And you can see, so it's kind of nice, right? You can, you can actually create your own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4261" target="_blank">01:11:01.220</a></span> | <span class="t">kind of build your own tools to look inside these things and see what's going on. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4266" target="_blank">01:11:06.540</a></span> | <span class="t">so we can see here that as you add more and more trees, the accuracy did indeed keep improving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4271" target="_blank">01:11:11.980</a></span> | <span class="t">or the root mean squared error kept improving, although the improvements slowed down after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4278" target="_blank">01:11:18.060</a></span> | <span class="t">a while. The validation set is worse than the training set and there's a couple of reasons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4288" target="_blank">01:11:28.640</a></span> | <span class="t">that could have happened. The first reason could be because we're still overfitting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4294" target="_blank">01:11:34.660</a></span> | <span class="t">which is not necessarily a problem, it's just something we could identify. Or maybe it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4299" target="_blank">01:11:39.020</a></span> | <span class="t">because the, the fact that we're trying to predict the last two weeks is actually a problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4304" target="_blank">01:11:44.700</a></span> | <span class="t">and that the last two weeks are kind of different to the other auctions in our dataset, maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4310" target="_blank">01:11:50.300</a></span> | <span class="t">something changed over time. So how do we tell which of those two reasons there are?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4316" target="_blank">01:11:56.740</a></span> | <span class="t">What is the reason that our validation set is worse? We can actually find out using a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4321" target="_blank">01:12:01.620</a></span> | <span class="t">very clever trick called out of bag error, OOB error. And we use OOB error for lots of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4326" target="_blank">01:12:06.900</a></span> | <span class="t">things. You can grab the OOB error, or you can grab the OOB predictions from the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4336" target="_blank">01:12:16.340</a></span> | <span class="t">with OOB prediction and you can grab the RMSE and you can find that the OOB error, RMSE is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4343" target="_blank">01:12:23.860</a></span> | <span class="t">0.21, which is quite a bit better than 0.23. So let me explain what OOB error is. What</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4355" target="_blank">01:12:35.420</a></span> | <span class="t">OOB error is, is we look at each row of the training set, not the validation set, each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4365" target="_blank">01:12:45.180</a></span> | <span class="t">row of the training set and we say, so we say for row number one, which trees included</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4373" target="_blank">01:12:53.220</a></span> | <span class="t">row number one in the training? And we'll say, okay, let's not use those for calculating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4378" target="_blank">01:12:58.700</a></span> | <span class="t">the error because it was part of those trees training. So we'll just calculate the error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4384" target="_blank">01:13:04.100</a></span> | <span class="t">for that row using the trees where that row was not included in training that tree. Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4390" target="_blank">01:13:10.860</a></span> | <span class="t">remember every tree is using only a subset of the data. So we do that for every row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4395" target="_blank">01:13:15.860</a></span> | <span class="t">We find the prediction using only the trees that were not used, that that row was not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4404" target="_blank">01:13:24.100</a></span> | <span class="t">used. And those are the OOB predictions. In other words, this is like giving us a validation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4411" target="_blank">01:13:31.580</a></span> | <span class="t">set result without actually needing a validation. But the thing is, it's not with that time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4419" target="_blank">01:13:39.660</a></span> | <span class="t">offset, it's not looking at the last two weeks, it's looking at the whole training set. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4423" target="_blank">01:13:43.580</a></span> | <span class="t">this basically tells us how much of the error is due to overfitting versus due to being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4430" target="_blank">01:13:50.620</a></span> | <span class="t">the last couple of weeks. So that's a cool trick. OOB error is something that very quickly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4435" target="_blank">01:13:55.700</a></span> | <span class="t">kind of gives us a sense of how much we're, we're overfitting. And we don't even need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4440" target="_blank">01:14:00.100</a></span> | <span class="t">a validation set to do it. So there's that OOB error. So that's telling us a bit about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4446" target="_blank">01:14:06.500</a></span> | <span class="t">what's going on in our model. But then there's a lot of things we'd like to find out from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4452" target="_blank">01:14:12.320</a></span> | <span class="t">our model. And I've got five things in particular here which I generally find pretty interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4458" target="_blank">01:14:18.580</a></span> | <span class="t">Which is, how confident are we about our predictions for some particular prediction we're making?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4466" target="_blank">01:14:26.460</a></span> | <span class="t">Like we can say this is what we think the prediction is, but how confident are we? Is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4471" target="_blank">01:14:31.740</a></span> | <span class="t">that exactly that or is it just about that or we really have no idea? And then for predict,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4477" target="_blank">01:14:37.900</a></span> | <span class="t">for predicting a particular item, which factors were the most important in that prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4484" target="_blank">01:14:44.860</a></span> | <span class="t">and how did they influence it? Overall, which columns are making the biggest difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4490" target="_blank">01:14:50.500</a></span> | <span class="t">in MPRL? Which ones could we maybe throw away and it wouldn't matter? Which columns are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4496" target="_blank">01:14:56.420</a></span> | <span class="t">basically redundant with each other? So we don't really need both of them. And as we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4503" target="_blank">01:15:03.580</a></span> | <span class="t">vary some column, how does it change the prediction? So those are the five things that we're, that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4509" target="_blank">01:15:09.500</a></span> | <span class="t">I'm interested in figuring out and we can do all of those things with a random first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4515" target="_blank">01:15:15.340</a></span> | <span class="t">Let's start with the first one. So the first one, we've already seen that we can grab all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4523" target="_blank">01:15:23.060</a></span> | <span class="t">of the predictions for all of the trees and take their mean to get the actual predictions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4531" target="_blank">01:15:31.340</a></span> | <span class="t">of the model and then to get the RMSE. But what if instead of saying mean, we did exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4536" target="_blank">01:15:36.060</a></span> | <span class="t">the same thing like so, but instead said standard deviation. This is going to tell us for every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4546" target="_blank">01:15:46.740</a></span> | <span class="t">row in our dataset, how much did the trees vary? And so if our model really had never</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4556" target="_blank">01:15:56.380</a></span> | <span class="t">seen kind of data like this before, it was something where, you know, different trees</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4562" target="_blank">01:16:02.020</a></span> | <span class="t">were giving very different predictions. It might give us a sense that maybe this is something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4567" target="_blank">01:16:07.900</a></span> | <span class="t">that we're not at all confident about. And as you can see, when we look at the standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4572" target="_blank">01:16:12.060</a></span> | <span class="t">deviation of the trees for each prediction, let's just look at the first five. They vary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4577" target="_blank">01:16:17.620</a></span> | <span class="t">a lot, right, 0.2, 0.1, 0.09, 0.3, okay? So this is a really interesting, it's not something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4590" target="_blank">01:16:30.820</a></span> | <span class="t">that a lot of people talk about, but I think it's a really interesting approach to kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4593" target="_blank">01:16:33.940</a></span> | <span class="t">of figuring out whether we might want to be cautious about a particular prediction because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4600" target="_blank">01:16:40.260</a></span> | <span class="t">maybe we're not very confident about it. But there's one thing we can easily do with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4606" target="_blank">01:16:46.540</a></span> | <span class="t">random forest. The next thing, and this is I think the most important thing for me in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4610" target="_blank">01:16:50.900</a></span> | <span class="t">terms of interpretation, is feature importance. Here's what feature importance looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4617" target="_blank">01:16:57.420</a></span> | <span class="t">We can call feature importance on a model with some independent variables. Let's say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4621" target="_blank">01:17:01.860</a></span> | <span class="t">grab the first 10. This says these are the 10 most important features in this random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4629" target="_blank">01:17:09.500</a></span> | <span class="t">forest. These are the things that are the most strongly driving sale price or we could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4635" target="_blank">01:17:15.020</a></span> | <span class="t">plot them. And so you can see here, there's just a few things that are by far the most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4642" target="_blank">01:17:22.940</a></span> | <span class="t">important. What year the equipment was made, bulldozer or whatever. How big is it? Upla</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4651" target="_blank">01:17:31.260</a></span> | <span class="t">system, whatever that means, and the product class, whatever that means. And so you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4660" target="_blank">01:17:40.660</a></span> | <span class="t">get this by simply looking inside your train model and grabbing the feature importances</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4666" target="_blank">01:17:46.260</a></span> | <span class="t">attribute. And so here for making it better to print out, I'm just sticking that into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4670" target="_blank">01:17:50.660</a></span> | <span class="t">a data frame and sorting the sending by importance. So how is this actually being done? It's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4680" target="_blank">01:18:00.700</a></span> | <span class="t">really neat. What Scikit-learn does, and Bryman, the inventor of random forest described, is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4687" target="_blank">01:18:07.740</a></span> | <span class="t">that you can go through each tree and then start at the top of the tree and look at each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4692" target="_blank">01:18:12.340</a></span> | <span class="t">branch and at each branch see what feature was used, the split, which binary, which the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4699" target="_blank">01:18:19.100</a></span> | <span class="t">binary split was based on which column. And then how much better was the model after that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4704" target="_blank">01:18:24.700</a></span> | <span class="t">split compared to beforehand. And we basically then say, okay, that column was responsible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4711" target="_blank">01:18:31.060</a></span> | <span class="t">for that amount of improvement. And so you add that up across all of the splits, across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4716" target="_blank">01:18:36.900</a></span> | <span class="t">all of the trees for each column, and then you normalize it so they all add to one. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4723" target="_blank">01:18:43.700</a></span> | <span class="t">that's what gives you these numbers, which we show the first few of them in this table</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4729" target="_blank">01:18:49.180</a></span> | <span class="t">and the first 30 of them here in this chart. So this is something that's fast and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4735" target="_blank">01:18:55.900</a></span> | <span class="t">easy and it kind of gives us a good sense of like, well, maybe the stuff that are less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4741" target="_blank">01:19:01.020</a></span> | <span class="t">than 0.005 we could remove. So if we did that, that would leave us with only 21 columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4752" target="_blank">01:19:12.940</a></span> | <span class="t">So let's try that. Let's just, let's just say, okay, x's which are important, the x's which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4759" target="_blank">01:19:19.340</a></span> | <span class="t">are in this list of ones to keep, do the same, they're valid, retrain our random forest and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4767" target="_blank">01:19:27.340</a></span> | <span class="t">have a look at the result. And basically our accuracy is about the same, but we've gone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4774" target="_blank">01:19:34.620</a></span> | <span class="t">down from 78 columns to 21 columns. So I think this is really important. It's not just about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4782" target="_blank">01:19:42.260</a></span> | <span class="t">creating the most accurate model you can, but you want to kind of be able to fit it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4785" target="_blank">01:19:45.460</a></span> | <span class="t">in your head as best as possible. And so 21 columns is going to be much easier for us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4790" target="_blank">01:19:50.020</a></span> | <span class="t">to check for any data issues and understand what's going on. And the accuracy is about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4795" target="_blank">01:19:55.300</a></span> | <span class="t">the same, or the RMSE. So I would say, okay, let's do that. Let's just stick with x's important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4803" target="_blank">01:20:03.980</a></span> | <span class="t">from now on. And so here's this entire set of the 21 features. And you can see it looks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4811" target="_blank">01:20:11.920</a></span> | <span class="t">now like year made and product size of the two really important things. And then there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4817" target="_blank">01:20:17.500</a></span> | <span class="t">a cluster of kind of mainly product related things that are kind of at the next level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4821" target="_blank">01:20:21.860</a></span> | <span class="t">of importance. One of the tricky things here is that we've got like a product class desk,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4833" target="_blank">01:20:33.500</a></span> | <span class="t">model ID, secondary desk, model desk, base model. They modeled a script. So they all look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4838" target="_blank">01:20:38.740</a></span> | <span class="t">like there might be similar ways of saying the same thing. So one thing that can help</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4843" target="_blank">01:20:43.360</a></span> | <span class="t">us to interpret the feature importance better and understand better what's happening in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4847" target="_blank">01:20:47.500</a></span> | <span class="t">the model is to remove redundant features. So one way to do that is to call fast.ai's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4859" target="_blank">01:20:59.020</a></span> | <span class="t">cluster columns, which is basically a thin wrapper for stuff that scikit-learn already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4862" target="_blank">01:21:02.980</a></span> | <span class="t">provides. And what that's going to do is it's going to find pairs of columns, which are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4869" target="_blank">01:21:09.420</a></span> | <span class="t">very similar. So you can see here sale year and sale elapsed. See how this line is way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4874" target="_blank">01:21:14.540</a></span> | <span class="t">out to the right or else machine ID and model ID is not at all. It's way out to the left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4879" target="_blank">01:21:19.700</a></span> | <span class="t">So that means that sale year and sale elapsed are very, very similar. When one is low, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4886" target="_blank">01:21:26.140</a></span> | <span class="t">other tends to be low and vice versa. Here's a group of three, which all seem to be much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4891" target="_blank">01:21:31.540</a></span> | <span class="t">the same, and then product group desk and product group, and then FI best-based model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4896" target="_blank">01:21:36.620</a></span> | <span class="t">and FI model desk. But these all seem like things where maybe we could remove one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4902" target="_blank">01:21:42.860</a></span> | <span class="t">each of these pairs because they're basically seem to be much the same, you know, they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4908" target="_blank">01:21:48.900</a></span> | <span class="t">when one is high, the other is high and vice versa. So let's try removing one of each of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4921" target="_blank">01:22:01.980</a></span> | <span class="t">these. Now it takes a little while to train a random forest. And so for the, just to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4929" target="_blank">01:22:09.580</a></span> | <span class="t">whether removing something makes it much worse, we could just do a very fast version. So we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4936" target="_blank">01:22:16.460</a></span> | <span class="t">could just train something where we only have 50,000 rows per tree, train for each tree,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4944" target="_blank">01:22:24.980</a></span> | <span class="t">and we'll just use 40 trees. And let's then just get the OOB for, and so for that fast</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4957" target="_blank">01:22:37.420</a></span> | <span class="t">simple version, our basic OOB with our important x's is 0.877. And here for OOB, a higher number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4968" target="_blank">01:22:48.500</a></span> | <span class="t">is better. So then let's try going through each of the things we thought we might not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4973" target="_blank">01:22:53.060</a></span> | <span class="t">need and try dropping them and then getting the OOB error for our x's with that one column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4981" target="_blank">01:23:01.580</a></span> | <span class="t">removed. And so compared to 877, most of them don't seem to hurt very much. They'll elapse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4991" target="_blank">01:23:11.220</a></span> | <span class="t">to it quite a bit, right? So for each of those groups, let's go and see which one of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=4998" target="_blank">01:23:18.420</a></span> | <span class="t">ones seems like we could remove it. So here's the five I found. Let's remove the whole lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5005" target="_blank">01:23:25.980</a></span> | <span class="t">and see what happens. And so the OOB went from 877 to 874, though hardly any difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5013" target="_blank">01:23:33.820</a></span> | <span class="t">at all, despite the fact we managed to get rid of five of our variables. So let's create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5022" target="_blank">01:23:42.180</a></span> | <span class="t">something called x's final, which is the x's important and then dropping those five, save</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5030" target="_blank">01:23:50.300</a></span> | <span class="t">them for later. We can always load them back again. And then let's check our random forest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5036" target="_blank">01:23:56.700</a></span> | <span class="t">using those and again 0.233 or 0.234. So we've got about the same thing, but we've got even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5045" target="_blank">01:24:05.460</a></span> | <span class="t">less columns now. So we're getting a kind of a simpler and simpler model without hurting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5050" target="_blank">01:24:10.780</a></span> | <span class="t">our accuracy. It's great. So the next thing we said we were interested in learning about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5057" target="_blank">01:24:17.900</a></span> | <span class="t">is for the columns that are, particularly the columns that are most important, how does,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5064" target="_blank">01:24:24.260</a></span> | <span class="t">what's the relationship between that column and the dependent variable? So for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5068" target="_blank">01:24:28.700</a></span> | <span class="t">what's the relationship between product size and sale price? So the first thing I would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5073" target="_blank">01:24:33.900</a></span> | <span class="t">do would be just to look at a histogram. So one way to do that is with value counts in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5081" target="_blank">01:24:41.420</a></span> | <span class="t">pandas. And we can see here our different levels of product size. And one thing to note here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5092" target="_blank">01:24:52.780</a></span> | <span class="t">is actually missing is actually the most common. And then next most is compact and small. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5100" target="_blank">01:25:00.180</a></span> | <span class="t">then many is pretty tiny. So we can do the same thing for year made. Now for year made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5107" target="_blank">01:25:07.420</a></span> | <span class="t">we can't just see the basic bar chart. We, according to histogram is not it's a bar chart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5116" target="_blank">01:25:16.140</a></span> | <span class="t">For year made we actually need a histogram, which pandas has stuff like this built in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5121" target="_blank">01:25:21.460</a></span> | <span class="t">so we can just call histogram. And that 1950, you remember we created it, that's kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5127" target="_blank">01:25:27.020</a></span> | <span class="t">this missing value thing that used to be a thousand. But most of them seem to have been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5132" target="_blank">01:25:32.500</a></span> | <span class="t">well into the 90's and 3000's. So let's now look at something called a partial dependence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5138" target="_blank">01:25:38.780</a></span> | <span class="t">plot. I'll show it to you first. Here is a partial dependence plot of year made against</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5152" target="_blank">01:25:52.460</a></span> | <span class="t">partial dependence. What does this mean? Well we should focus on the part where we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5159" target="_blank">01:25:59.100</a></span> | <span class="t">have a reasonable amount of data. So at least well into the 80's, go around here. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5165" target="_blank">01:26:05.900</a></span> | <span class="t">let's look at this bit here. Basically what this says is that as year made increases,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5174" target="_blank">01:26:14.220</a></span> | <span class="t">the predicted sale price, log sale price of course also increases. You can see. And the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5182" target="_blank">01:26:22.660</a></span> | <span class="t">log sale price is increasing linearly on other roughly, but roughly then this is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5188" target="_blank">01:26:28.780</a></span> | <span class="t">an exponential relationship between year made and sale price. Why do we call it a partial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5196" target="_blank">01:26:36.900</a></span> | <span class="t">dependence? Are we just plotting the kind of the year against the average sale price?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5201" target="_blank">01:26:41.700</a></span> | <span class="t">Well no we're not. We can't do that because a lot of other things change from year to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5207" target="_blank">01:26:47.540</a></span> | <span class="t">year. Example, maybe more recently people tend to buy bigger bulldozers or more bulldozers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5217" target="_blank">01:26:57.100</a></span> | <span class="t">with air conditioning or more expensive models of bulldozers. And we really want to be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5223" target="_blank">01:27:03.700</a></span> | <span class="t">to say like no just what's the impact of year and nothing else. And if you think about it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5228" target="_blank">01:27:08.820</a></span> | <span class="t">from a kind of an inflation point of view, you would expect that older bulldozers would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5238" target="_blank">01:27:18.100</a></span> | <span class="t">be kind of, that bulldozers would get kind of a constant ratio cheaper the further you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5247" target="_blank">01:27:27.220</a></span> | <span class="t">go back, which is what we see. So what we really want to say is all other things being equal,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5253" target="_blank">01:27:33.980</a></span> | <span class="t">what happens if only the year changes? And there's a really cool way we can answer that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5259" target="_blank">01:27:39.820</a></span> | <span class="t">question with a random forest. So how does year made impact sale price? All other things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5266" target="_blank">01:27:46.020</a></span> | <span class="t">being equal. So what we can do is we can go into our actual data set and replace every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5272" target="_blank">01:27:52.460</a></span> | <span class="t">single value in the year made column with 1950 and then calculate the predicted sale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5278" target="_blank">01:27:58.620</a></span> | <span class="t">price for every single auction and then take the average over all the auctions. And that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5283" target="_blank">01:28:03.820</a></span> | <span class="t">what gives us this value here. And then we can do the same from 1951, 1952 and so forth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5290" target="_blank">01:28:10.900</a></span> | <span class="t">until eventually we get to our final year of 2011. So this isolates the effect of only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5300" target="_blank">01:28:20.020</a></span> | <span class="t">year made. So it's a kind of a bit of a curious thing to do, but it's actually, it's a pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5308" target="_blank">01:28:28.580</a></span> | <span class="t">neat trick for trying to kind of pull apart and create this partial dependence to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5314" target="_blank">01:28:34.920</a></span> | <span class="t">what might be the impact of just changing year made. And we can do the same thing for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5322" target="_blank">01:28:42.060</a></span> | <span class="t">product size. And one of the interesting things if we do it for product size is we see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5326" target="_blank">01:28:46.540</a></span> | <span class="t">the lowest value of predicted sale price log sale price is NA, which is a bit of a worry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5338" target="_blank">01:28:58.700</a></span> | <span class="t">because we kind of want to know well that means it's really important the question of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5342" target="_blank">01:29:02.260</a></span> | <span class="t">whether or not the product size is labeled is really important. And that is something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5348" target="_blank">01:29:08.180</a></span> | <span class="t">that I would want to dig into before I actually use this model to find out well why is it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5352" target="_blank">01:29:12.700</a></span> | <span class="t">that sometimes things aren't labeled and what does it mean, you know, why is it that that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5356" target="_blank">01:29:16.620</a></span> | <span class="t">actually a that's just important predictor. So that is the partial dependence plot and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5363" target="_blank">01:29:23.580</a></span> | <span class="t">it's a really clever trick. So we have looked at four of the five questions we said we wanted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5374" target="_blank">01:29:34.060</a></span> | <span class="t">to answer at the start of this section. So the last one that we want to answer is one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5381" target="_blank">01:29:41.780</a></span> | <span class="t">here. We're predicting with a particular row of data what were the most important factors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5386" target="_blank">01:29:46.980</a></span> | <span class="t">and how did they influence that prediction. This is quite related to the very first thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5391" target="_blank">01:29:51.460</a></span> | <span class="t">we saw. So it's like imagine you were using this auction price model in real life. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5397" target="_blank">01:29:57.300</a></span> | <span class="t">had something on your tablet and you went into some auction and you looked up what the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5402" target="_blank">01:30:02.320</a></span> | <span class="t">predicted auction price would be for this lot that's coming up to find out whether it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5409" target="_blank">01:30:09.940</a></span> | <span class="t">seems like it's being under or overvalued and then you can decide what to do about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5415" target="_blank">01:30:15.720</a></span> | <span class="t">So one thing we said we'd be interested to know is like well are we actually confident</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5420" target="_blank">01:30:20.020</a></span> | <span class="t">in our prediction and then we might be curious to find out like oh I'm really surprised it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5425" target="_blank">01:30:25.180</a></span> | <span class="t">was predicting such a high value. Why was it predicting such a high value? So to find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5432" target="_blank">01:30:32.060</a></span> | <span class="t">the answer to that question, we can use a module called TreeInterpreter. And TreeInterpreter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5441" target="_blank">01:30:41.260</a></span> | <span class="t">the way it works is that you pass in a single row. So it's like here's the auction that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5447" target="_blank">01:30:47.620</a></span> | <span class="t">coming up, here's the model, here's the auctioneer ID, etcetera, etcetera. Please predict the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5455" target="_blank">01:30:55.220</a></span> | <span class="t">value from the random forest, what's the expected sale price and then what we can do is we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5462" target="_blank">01:31:02.700</a></span> | <span class="t">take that one row of data and put it through the first decision tree and we can see what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5467" target="_blank">01:31:07.700</a></span> | <span class="t">the first split that's selected and then based on that split does it end up increasing or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5473" target="_blank">01:31:13.340</a></span> | <span class="t">decreasing the predicted price compared to that kind of raw baseline model of just take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5479" target="_blank">01:31:19.340</a></span> | <span class="t">the average and then you can do that again at the next split and again at the next split</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5483" target="_blank">01:31:23.020</a></span> | <span class="t">and again at the next split. So for each split, we see what the increase or decrease in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5488" target="_blank">01:31:28.940</a></span> | <span class="t">well, addiction, that's not right. We see what the increase or decrease in the prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5497" target="_blank">01:31:37.420</a></span> | <span class="t">is except while I'm here compared to the parent node. And so then you can do that for every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5508" target="_blank">01:31:48.700</a></span> | <span class="t">tree and then add up the total change in importance by split variable and that allows you to draw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5516" target="_blank">01:31:56.660</a></span> | <span class="t">something like this. So here's something that's looking at one particular row of data and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5523" target="_blank">01:32:03.860</a></span> | <span class="t">overall we start at zero and so zero is the initial 10.1. Remember this number 10.1 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5534" target="_blank">01:32:14.860</a></span> | <span class="t">the average log sale price of the whole data set. They call it the bias. And so we call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5542" target="_blank">01:32:22.300</a></span> | <span class="t">that zero then for this particular row we're looking at year made as a negative 4.2 impact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5551" target="_blank">01:32:31.180</a></span> | <span class="t">on the prediction and then product size has a positive 0.2, cut plus system has a positive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5558" target="_blank">01:32:38.300</a></span> | <span class="t">0.046, model ID has a positive 0.127 and so forth, right. And so the red ones are negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5567" target="_blank">01:32:47.480</a></span> | <span class="t">and the green ones are positive and you can see how they all join up until eventually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5571" target="_blank">01:32:51.580</a></span> | <span class="t">overall the prediction is that it's going to be negative 0.122 compared to 10.1 which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5581" target="_blank">01:33:01.140</a></span> | <span class="t">is equal to 9.98. So this kind of plot is called a waterfall plot and so basically when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5592" target="_blank">01:33:12.240</a></span> | <span class="t">we say tree interpreter dot predict it gives us back the prediction which is the actual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5600" target="_blank">01:33:20.780</a></span> | <span class="t">number we get back from the random forest, the bias which is just always this 10.1 for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5605" target="_blank">01:33:25.900</a></span> | <span class="t">this data set and then the contributions which is all of these different values. It's how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5613" target="_blank">01:33:33.460</a></span> | <span class="t">important was each factor and here I've used a threshold which means anything that was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5622" target="_blank">01:33:42.140</a></span> | <span class="t">less than 0.08 all gets thrown into this other category. I think this is a really useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5628" target="_blank">01:33:48.940</a></span> | <span class="t">kind of thing to have in production because it can help you answer questions whether it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5634" target="_blank">01:33:54.620</a></span> | <span class="t">will be for the customer or for you know whoever's using your model if they're surprised about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5639" target="_blank">01:33:59.500</a></span> | <span class="t">some prediction why is that prediction. So I'm going to show you something really interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5650" target="_blank">01:34:10.540</a></span> | <span class="t">using some synthetic data and I want you to really have a think about why this is happening</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5656" target="_blank">01:34:16.640</a></span> | <span class="t">before I tell you and I pause the video if you're watching the video when I get to that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5661" target="_blank">01:34:21.660</a></span> | <span class="t">point. Let's start by creating some synthetic data like so. So we're going to grab 40 values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5669" target="_blank">01:34:29.460</a></span> | <span class="t">evenly spaced between 0 and 20 and then we're just going to create the y=x line and add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5677" target="_blank">01:34:37.740</a></span> | <span class="t">some normally distributed random data on that. Here's this kind of plot. So here's some data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5685" target="_blank">01:34:45.940</a></span> | <span class="t">we want to try and predict and we're going to use a random forest in a kind of bit of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5690" target="_blank">01:34:50.140</a></span> | <span class="t">an overkill here. Now in this case we only have one independent variable. Scikit-learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5700" target="_blank">01:35:00.180</a></span> | <span class="t">expects us to have more than one. So we can use unsqueeze in PyTorch to add that go from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5710" target="_blank">01:35:10.620</a></span> | <span class="t">a shape of 40 in other words a vector with 40 elements for a shape of 40 comma 1 in other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5716" target="_blank">01:35:16.060</a></span> | <span class="t">words a matrix of 40 rows with one column. So this unsqueeze 1 means add a unit axis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5723" target="_blank">01:35:23.500</a></span> | <span class="t">here. I don't use unsqueeze very often because I actually generally prefer the index with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5730" target="_blank">01:35:30.260</a></span> | <span class="t">a special value none. This works in PyTorch and numpy and the way it works is to say okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5737" target="_blank">01:35:37.220</a></span> | <span class="t">xlin remember that size is a vector of length 40 every row and then none means insert a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5746" target="_blank">01:35:46.180</a></span> | <span class="t">unit axis here for the column. So these are two ways of doing the same thing but this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5751" target="_blank">01:35:51.500</a></span> | <span class="t">one is a little bit more flexible so that's what I use more often. But now that we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5755" target="_blank">01:35:55.540</a></span> | <span class="t">got the shape that is expected which is a rank 2 tensor and an array with two dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5762" target="_blank">01:36:02.820</a></span> | <span class="t">or axes we can create a random forest we can fit it and let's just use the first 30 data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5768" target="_blank">01:36:08.860</a></span> | <span class="t">points right so kind of stop here. And then let's do a prediction right so let's plot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5776" target="_blank">01:36:16.580</a></span> | <span class="t">the original data points and then also plot a prediction and look what happens on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5781" target="_blank">01:36:21.100</a></span> | <span class="t">prediction it acts it's kind of nice and accurate and then suddenly what happens. So this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5787" target="_blank">01:36:27.820</a></span> | <span class="t">the bit where if you're watching the video I want you to pause and have a think bias</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5790" target="_blank">01:36:30.980</a></span> | <span class="t">is flat. So what's going on here well remember a random forest is just taking the average</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5799" target="_blank">01:36:39.380</a></span> | <span class="t">of predictions of a bunch of trees and a tree the prediction of a tree is just the average</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5806" target="_blank">01:36:46.220</a></span> | <span class="t">of the values in a leaf node and remember we fitted using a training set containing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5811" target="_blank">01:36:51.980</a></span> | <span class="t">only the first 30. So none of these appeared in the training set so the highest we could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5819" target="_blank">01:36:59.060</a></span> | <span class="t">get would be the average of values that are inside the training set. In other words there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5824" target="_blank">01:37:04.700</a></span> | <span class="t">this maximum you can get to. So random forests cannot extrapolate outside of the bounds of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5832" target="_blank">01:37:12.980</a></span> | <span class="t">the data that they're training set. This is going to be a huge problem for things like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5836" target="_blank">01:37:16.880</a></span> | <span class="t">time series prediction where there's like an underlying trend for instance. But really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5844" target="_blank">01:37:24.300</a></span> | <span class="t">it's a more general issue than just time variables. It's going to be hard for random or impossible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5849" target="_blank">01:37:29.660</a></span> | <span class="t">often for random forests to just extrapolate outside the types of data that it's seen in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5854" target="_blank">01:37:34.620</a></span> | <span class="t">a general sense. So we need to make sure that our validation set does not contain out of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5861" target="_blank">01:37:41.340</a></span> | <span class="t">domain data. So how do we find out of domain data? So we might not even know our test set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5870" target="_blank">01:37:50.900</a></span> | <span class="t">is distributed in the same way as our training data. So if they're from two different time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5874" target="_blank">01:37:54.760</a></span> | <span class="t">periods how do you kind of tell how they vary, right? Or if it's a Kaggle competition how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5880" target="_blank">01:38:00.980</a></span> | <span class="t">do you tell if the test set and the training set which Kaggle gives you have some underlying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5887" target="_blank">01:38:07.180</a></span> | <span class="t">differences? There's actually a cool trick you can do which is you can create a column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5893" target="_blank">01:38:13.020</a></span> | <span class="t">called is_valid which contains 0 for everything in the training set and 1 for everything in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5901" target="_blank">01:38:21.260</a></span> | <span class="t">the validation set. And it's concatenating all of the independent variables together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5907" target="_blank">01:38:27.580</a></span> | <span class="t">So it's concatenating the independent variables for both the training and validation set together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5912" target="_blank">01:38:32.700</a></span> | <span class="t">So this is our independent variable and this becomes our dependent variable. And we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5918" target="_blank">01:38:38.740</a></span> | <span class="t">going to create a random forest not for predicting price but a random forest that predicts is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5925" target="_blank">01:38:45.740</a></span> | <span class="t">this row from the validation set or the training set. So if the validation set and the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5931" target="_blank">01:38:51.980</a></span> | <span class="t">set are from kind of the same distribution if they're not different then this random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5937" target="_blank">01:38:57.060</a></span> | <span class="t">forest should basically have zero predictive power. If it has any predictive power then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5944" target="_blank">01:39:04.460</a></span> | <span class="t">it means that our training and validation set are different. And to find out the source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5949" target="_blank">01:39:09.100</a></span> | <span class="t">of that difference we can use feature importance. And so you can see here that the difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5957" target="_blank">01:39:17.840</a></span> | <span class="t">between the validation set and the training set is not surprisingly sale elapsed. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5966" target="_blank">01:39:26.940</a></span> | <span class="t">the number of days since I think like 1970 or something. So it's basically the date.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5972" target="_blank">01:39:32.180</a></span> | <span class="t">So yes of course you can predict whether something is in the validation set or the training set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5977" target="_blank">01:39:37.300</a></span> | <span class="t">by looking at the date because that's actually how you find them. That makes sense. This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5981" target="_blank">01:39:41.900</a></span> | <span class="t">interesting sales ID. So it looks like the sales ID is not some random identifier but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5986" target="_blank">01:39:46.940</a></span> | <span class="t">it increases over time. And ditto for machine ID. And then there's some other smaller ones</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5994" target="_blank">01:39:54.580</a></span> | <span class="t">here that kind of makes sense. So I guess for something like model desk I guess there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=5999" target="_blank">01:39:59.500</a></span> | <span class="t">are certain models that were only made in later years for instance. But you can see these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6006" target="_blank">01:40:06.860</a></span> | <span class="t">top three columns are a bit of an issue. So then we could say like okay what happens if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6014" target="_blank">01:40:14.320</a></span> | <span class="t">we look at each one of those columns those first three and remove them and then see how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6022" target="_blank">01:40:22.220</a></span> | <span class="t">it changes our RMSE on our sales price model on the validation set. So we start from point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6035" target="_blank">01:40:35.460</a></span> | <span class="t">232 and removing sales ID actually makes it a bit better. Sale elapsed makes it a bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6043" target="_blank">01:40:43.180</a></span> | <span class="t">worse, machine ID about the same. So we can probably remove sales ID and machine ID without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6049" target="_blank">01:40:49.180</a></span> | <span class="t">losing any accuracy and yep it's actually slightly improved. But most importantly it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6054" target="_blank">01:40:54.600</a></span> | <span class="t">going to be more resilient over time right because we're trying to remove the time related</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6060" target="_blank">01:41:00.380</a></span> | <span class="t">features. Another thing to note is that since it seems that you know this kind of sale elapsed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6069" target="_blank">01:41:09.280</a></span> | <span class="t">issue that maybe it's making a big difference is maybe looking at the sale year distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6076" target="_blank">01:41:16.420</a></span> | <span class="t">this is the histogram. Most of the sales are in the last few years anyway. So what happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6081" target="_blank">01:41:21.380</a></span> | <span class="t">if we only include the most recent few years. So let's just include everything after 2004.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6089" target="_blank">01:41:29.900</a></span> | <span class="t">So that is X is filtered. And if I train on that subset then my accuracy goes improves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6098" target="_blank">01:41:38.060</a></span> | <span class="t">a bit more from 331, 330. So that's interesting right. We're actually using less data, less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6106" target="_blank">01:41:46.260</a></span> | <span class="t">rows and getting a slightly better result because the more recent data is more representative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6113" target="_blank">01:41:53.980</a></span> | <span class="t">So that's about as far as we can get with our random forest. But what I will say is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6120" target="_blank">01:42:00.180</a></span> | <span class="t">this. This issue of extrapolation would not happen with a neural net would it because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6128" target="_blank">01:42:08.780</a></span> | <span class="t">a neural net is using the kind of the underlying layers are linear layers. And so linear layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6133" target="_blank">01:42:13.860</a></span> | <span class="t">can absolutely extrapolate. So the obvious thing to think then at this point is well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6139" target="_blank">01:42:19.340</a></span> | <span class="t">maybe what a neural net do a better job of this. That's going to be the thing. Next up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6145" target="_blank">01:42:25.540</a></span> | <span class="t">to this question. Question first. How do, how does feature importance relate to correlation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6157" target="_blank">01:42:37.020</a></span> | <span class="t">Feature importance doesn't particularly relate to correlation. Correlation is a concept for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6162" target="_blank">01:42:42.700</a></span> | <span class="t">linear models and this is not a linear model. So remember feature importance is calculated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6167" target="_blank">01:42:47.740</a></span> | <span class="t">by looking at the improvement in accuracy as you go down each tree and you go down each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6176" target="_blank">01:42:56.660</a></span> | <span class="t">binary split. If you're used to linear regression then I guess correlation sometimes can be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6185" target="_blank">01:43:05.620</a></span> | <span class="t">used as a measure of feature importance. But this is a much more kind of direct version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6193" target="_blank">01:43:13.660</a></span> | <span class="t">that's taking account of these non-linearities and interactions of stuff as well. So it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6199" target="_blank">01:43:19.380</a></span> | <span class="t">a much more flexible and reliable measure generally feature importance. Any more questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6210" target="_blank">01:43:30.260</a></span> | <span class="t">So I'll do the same thing with a neural network. I'm going to just copy and paste the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6214" target="_blank">01:43:34.620</a></span> | <span class="t">lines of code that I had from before but this time I'll call it NN, DFNN and these are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6220" target="_blank">01:43:40.660</a></span> | <span class="t">same lines of code. And I'll grab the same list of columns we had before in the dependent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6225" target="_blank">01:43:45.140</a></span> | <span class="t">variable to get the same data frame. Now as we've discussed for categorical columns we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6232" target="_blank">01:43:52.460</a></span> | <span class="t">probably want to use embeddings. So to create embeddings we need to know which columns should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6237" target="_blank">01:43:57.740</a></span> | <span class="t">be treated as categorical variables. And as we've discussed we can use "cont-cat-split"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6241" target="_blank">01:44:01.940</a></span> | <span class="t">for that. One of the useful things we can pass that is the maximum cardinality. So maxCard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6249" target="_blank">01:44:09.380</a></span> | <span class="t">equals 9000 means if there's a column with more than 9000 levels you should treat it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6254" target="_blank">01:44:14.660</a></span> | <span class="t">as continuous. And if it's got less than 9000 levels which it is categorical. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6260" target="_blank">01:44:20.660</a></span> | <span class="t">you know it's a simple little function that just checks the cardinality and splits them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6265" target="_blank">01:44:25.420</a></span> | <span class="t">based on how many discrete levels they have. And of course the data type if it's not actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6271" target="_blank">01:44:31.460</a></span> | <span class="t">numeric data type it has to be categorical. So there's our there's our split. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6282" target="_blank">01:44:42.020</a></span> | <span class="t">from there what we can do is we can say oh we've got to be a bit careful of "sail-elapsed"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6289" target="_blank">01:44:49.420</a></span> | <span class="t">because actually "sail-elapsed" I think has less than 9000 categories but we definitely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6293" target="_blank">01:44:53.860</a></span> | <span class="t">don't want to use that as a categorical variable. The whole point was to make it that this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6297" target="_blank">01:44:57.940</a></span> | <span class="t">something that we can extrapolate. Though we certainly anything that's kind of time dependent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6303" target="_blank">01:45:03.020</a></span> | <span class="t">or we think that we might see things outside the range of inputs in the training data we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6309" target="_blank">01:45:09.380</a></span> | <span class="t">should make them continuous variables. So let's make "sail-elapsed" put it in continuous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6314" target="_blank">01:45:14.520</a></span> | <span class="t">neural net and remove it from categorical. So here's the number of unique levels this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6322" target="_blank">01:45:22.820</a></span> | <span class="t">is from pandas for everything in our neural net data set for the categorical variables.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6328" target="_blank">01:45:28.460</a></span> | <span class="t">And I get a bit nervous when I see these really high numbers so I don't want to have too many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6332" target="_blank">01:45:32.740</a></span> | <span class="t">things with like lots and lots of categories. The reason I don't want lots of things with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6340" target="_blank">01:45:40.220</a></span> | <span class="t">lots and lots of categories is just they're going to take up a lot of parameters because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6344" target="_blank">01:45:44.020</a></span> | <span class="t">in a embedding matrix this is you know every one of these is a row in an embedding matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6348" target="_blank">01:45:48.580</a></span> | <span class="t">In this case I notice model ID and model desk might be describing something very similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6354" target="_blank">01:45:54.380</a></span> | <span class="t">So I'd quite like to find out if I could get rid of one and an easy way to do that would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6358" target="_blank">01:45:58.680</a></span> | <span class="t">be to use a random forest. So let's try removing the model desk and let's create a random forest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6370" target="_blank">01:46:10.540</a></span> | <span class="t">and let's see what happens and oh it's actually a tiny bit better and certainly not worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6376" target="_blank">01:46:16.460</a></span> | <span class="t">So that suggests that we can actually get rid of one of these levels or one of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6380" target="_blank">01:46:20.740</a></span> | <span class="t">variables. So let's get rid of that one and so now we can create a tabular pandas object</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6386" target="_blank">01:46:26.900</a></span> | <span class="t">just like before. But this time we're going to add one more processor which is normalize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6394" target="_blank">01:46:34.540</a></span> | <span class="t">And the reason we need normalize, so normalize is subtract the mean divide by the standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6399" target="_blank">01:46:39.180</a></span> | <span class="t">deviation. We didn't need that for a random forest because for a random forest we're just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6404" target="_blank">01:46:44.660</a></span> | <span class="t">looking at less than or greater than through our binary splits. So all that matters is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6409" target="_blank">01:46:49.980</a></span> | <span class="t">the order of things, how they're sorted, it doesn't matter whether they're super big or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6413" target="_blank">01:46:53.700</a></span> | <span class="t">super small. But it definitely matters for neural nets because we have these linear layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6421" target="_blank">01:47:01.460</a></span> | <span class="t">So we don't want to have you know things with kind of crazy distributions with some super</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6426" target="_blank">01:47:06.220</a></span> | <span class="t">big numbers and super small numbers because it's not going to work. So it's always a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6430" target="_blank">01:47:10.680</a></span> | <span class="t">idea to normalize things in neural nets so we can do that in a tabular neural net by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6437" target="_blank">01:47:17.580</a></span> | <span class="t">using the normalize tabular proc. So we can do the same thing that we did before with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6443" target="_blank">01:47:23.460</a></span> | <span class="t">creating our tabular pandas tabular object for the neural net. And then we can create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6449" target="_blank">01:47:29.900</a></span> | <span class="t">data loaders from that with a batch size. And this is a large batch size because tabular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6455" target="_blank">01:47:35.540</a></span> | <span class="t">models don't generally require nearly as much GPU RAM as a convolutional neural net or something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6464" target="_blank">01:47:44.000</a></span> | <span class="t">or an RNN or something. Since it's a regression model we're going to want R range. So let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6472" target="_blank">01:47:52.140</a></span> | <span class="t">find the minimum and maximum of our dependent variable. And we can now go ahead and create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6479" target="_blank">01:47:59.140</a></span> | <span class="t">a tabular learner. Our tabular learner is going to take our data loaders, our way range,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6486" target="_blank">01:48:06.140</a></span> | <span class="t">how many activations do you want in each of the linear layers. And so you can have as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6492" target="_blank">01:48:12.140</a></span> | <span class="t">many linear layers as you like here. How many outputs are there? So this is a regression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6498" target="_blank">01:48:18.380</a></span> | <span class="t">with a single output. And what loss function do you want? We can use LRfind and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6507" target="_blank">01:48:27.420</a></span> | <span class="t">can go ahead and use fit1cycle. There's no pre-trained model obviously because this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6512" target="_blank">01:48:32.500</a></span> | <span class="t">not something where people have got pre-trained models for industrial equipment options. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6519" target="_blank">01:48:39.140</a></span> | <span class="t">we just use fit1cycle and train for a minute. And then we can check. And our RMSE is 0.226</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6532" target="_blank">01:48:52.500</a></span> | <span class="t">which here was 0.230. So that's amazing. We actually have, you know, straight away a better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6538" target="_blank">01:48:58.620</a></span> | <span class="t">result than the random forest. It's a little more fussy, it takes a little bit longer. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6545" target="_blank">01:49:05.580</a></span> | <span class="t">as you can see, you know, for interesting datasets like this, we can get some great</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6550" target="_blank">01:49:10.940</a></span> | <span class="t">results with neural nets. So here's something else we could do though. The random forest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6563" target="_blank">01:49:23.380</a></span> | <span class="t">and the neural net, they each have their own pros and cons. There's some things they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6568" target="_blank">01:49:28.020</a></span> | <span class="t">good at and there's some they're less good at. So maybe we can get the best of both worlds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6574" target="_blank">01:49:34.620</a></span> | <span class="t">And a really easy way to do that is to use Ensemble. We've already seen that a random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6579" target="_blank">01:49:39.420</a></span> | <span class="t">forest is a decision tree ensemble. But now we can put that into another ensemble. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6583" target="_blank">01:49:43.740</a></span> | <span class="t">can have an ensemble of the random forest and a neural net. There's lots of super fancy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6589" target="_blank">01:49:49.180</a></span> | <span class="t">ways you can do that. But a really simple way is to take the average. So sum up the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6595" target="_blank">01:49:55.300</a></span> | <span class="t">predictions from the two models, divide by two, and use that as prediction. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6601" target="_blank">01:50:01.620</a></span> | <span class="t">our ensemble prediction is just literally the average of the random forest prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6605" target="_blank">01:50:05.540</a></span> | <span class="t">and the neural net prediction. And that gives us 0.223 versus 0.226. So how good is that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6618" target="_blank">01:50:18.900</a></span> | <span class="t">Well it's a little hard to say because unfortunately this competition is old enough that we can't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6625" target="_blank">01:50:25.540</a></span> | <span class="t">even submit to it and find out how we would have gone on Kaggle. So we don't really know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6630" target="_blank">01:50:30.980</a></span> | <span class="t">and so we're relying on our own validation set. But it's quite a bit better than even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6636" target="_blank">01:50:36.260</a></span> | <span class="t">the first place score on the test set. So if the validation set is you know doing good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6645" target="_blank">01:50:45.380</a></span> | <span class="t">job then this is a good sign that this is a really really good model. Which wouldn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6651" target="_blank">01:50:51.060</a></span> | <span class="t">necessarily be that surprising because you know in the last few years I guess we've learned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6658" target="_blank">01:50:58.620</a></span> | <span class="t">a lot about building these kinds of models. And we're kind of taking advantage of a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6663" target="_blank">01:51:03.940</a></span> | <span class="t">of the tricks that have appeared in recent years. And yeah maybe this goes to show that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6671" target="_blank">01:51:11.660</a></span> | <span class="t">well I think it certainly goes to show that both random forests and neural nets have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6677" target="_blank">01:51:17.300</a></span> | <span class="t">lot to offer. And try both and maybe even find both. We've talked about an approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6689" target="_blank">01:51:29.540</a></span> | <span class="t">to ensembling called bagging which is where we train lots of models on different subsets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6695" target="_blank">01:51:35.680</a></span> | <span class="t">of the data like the average of. Another approach to ensembling particularly ensembling of trees</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6702" target="_blank">01:51:42.500</a></span> | <span class="t">is called boosting. And boosting involves training a small model which underfits your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6710" target="_blank">01:51:50.580</a></span> | <span class="t">data set. So maybe like just have a very small number of leaf nodes. And then you calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6717" target="_blank">01:51:57.200</a></span> | <span class="t">the predictions using the small model. And then you subtract the predictions from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6722" target="_blank">01:52:02.500</a></span> | <span class="t">targets. So these are kind of like the errors of your small underfit model. We call them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6727" target="_blank">01:52:07.580</a></span> | <span class="t">residual. And then go back to step one but now instead of using the original targets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6735" target="_blank">01:52:15.440</a></span> | <span class="t">use the residuals. The train a small model which underfits your data set attempting to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6741" target="_blank">01:52:21.020</a></span> | <span class="t">predict the residuals. Then do that again and again until you reach some stopping criterion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6748" target="_blank">01:52:28.900</a></span> | <span class="t">such as the maximum number of trees. Now you that will leave you with a bunch of models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6755" target="_blank">01:52:35.620</a></span> | <span class="t">which you don't average but which use sum. Because each one is creating a model that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6762" target="_blank">01:52:42.500</a></span> | <span class="t">based on the residual of the previous one. But we've subtracted the predictions of each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6767" target="_blank">01:52:47.660</a></span> | <span class="t">new tree from the residuals of the previous tree. So the residuals get smaller and smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6773" target="_blank">01:52:53.260</a></span> | <span class="t">And then to make predictions we just have to do the opposite which is to add them all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6776" target="_blank">01:52:56.980</a></span> | <span class="t">together. So there's lots of variants of this. But you'll see things like GBMs for gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6786" target="_blank">01:53:06.780</a></span> | <span class="t">boosted machines or GBTTs for gradient boosted decision trees. And there's lots of minor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6794" target="_blank">01:53:14.460</a></span> | <span class="t">details around you know and significant details. But the basic idea is what I've shown.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6801" target="_blank">01:53:21.580</a></span> | <span class="t">All right let's take the questions. Dropping features in a model is a way to reduce the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6808" target="_blank">01:53:28.020</a></span> | <span class="t">complexity of the model and thus reduce overfitting. Is this better than adding some regularization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6813" target="_blank">01:53:33.820</a></span> | <span class="t">like weight decay? I didn't claim that we removed columns to avoid overfitting. We removed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6829" target="_blank">01:53:49.180</a></span> | <span class="t">the columns to simplify fewer things to analyze. It should also mean we don't need as many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6840" target="_blank">01:54:00.460</a></span> | <span class="t">trees but there's no particular reason to believe that this will regularize. And the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6846" target="_blank">01:54:06.380</a></span> | <span class="t">idea of regularization doesn't necessarily make a lot of sense to random forests and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6850" target="_blank">01:54:10.620</a></span> | <span class="t">always add more trees. Is there a good heuristic for picking the number of linear layers in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6858" target="_blank">01:54:18.660</a></span> | <span class="t">the tabular model? Not really. Well if there is I don't know what it is. I guess two, three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6872" target="_blank">01:54:32.900</a></span> | <span class="t">hidden layers works pretty well. So you know what I showed those numbers I showed are pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6880" target="_blank">01:54:40.300</a></span> | <span class="t">good for a large-ish model. A default it uses 200 and 100 so maybe start with the default</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6888" target="_blank">01:54:48.520</a></span> | <span class="t">and then go up to 500 and 250 if that isn't an improvement and like just keep doubling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6893" target="_blank">01:54:53.500</a></span> | <span class="t">them until it stops improving or you run out of memory or time. The main thing to note</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6900" target="_blank">01:55:00.900</a></span> | <span class="t">about boosted models is that there's nothing to stop us from overfitting. If you add more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6905" target="_blank">01:55:05.660</a></span> | <span class="t">and more trees to a bagging model sort of a random forest it's going to get, it should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6911" target="_blank">01:55:11.660</a></span> | <span class="t">generalize better and better because each time you're using a new model which is based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6916" target="_blank">01:55:16.980</a></span> | <span class="t">on a subset of the data. But boosting each model will fit the training set better and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6924" target="_blank">01:55:24.740</a></span> | <span class="t">better gradually overfit more and more. So boosting methods do require generally more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6932" target="_blank">01:55:32.460</a></span> | <span class="t">hyperparameter tuning and fiddling around with it. You know you certainly have regularization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6937" target="_blank">01:55:37.940</a></span> | <span class="t">boosting. They're pretty sensitive to their hyperparameters which is why they're not normally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6946" target="_blank">01:55:46.640</a></span> | <span class="t">my first go-to but they more often win Kaggle competition random forests do like they tend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6957" target="_blank">01:55:57.140</a></span> | <span class="t">to be good at getting that last little bit of performance. So the last thing I'm going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6964" target="_blank">01:56:04.860</a></span> | <span class="t">to mention is something super neat which a lot of people don't seem to know exists. There's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6971" target="_blank">01:56:11.500</a></span> | <span class="t">a shang so it's super cool which is something from the entity embeddings paper, the table</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6977" target="_blank">01:56:17.100</a></span> | <span class="t">from it where what they did was they built a neural network, they got the entity embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6983" target="_blank">01:56:23.900</a></span> | <span class="t">e.e. and then they tried a random forest using the entity embeddings as predictors rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=6995" target="_blank">01:56:35.220</a></span> | <span class="t">than the approach I described with just the raw categorical variables. And the the error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7003" target="_blank">01:56:43.060</a></span> | <span class="t">for a random forest went from 0.16 to 0.11. A huge improvement and very simple method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7011" target="_blank">01:56:51.100</a></span> | <span class="t">KNN went from 0.29 to 0.11. Basically all of the methods when they used entity embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7019" target="_blank">01:56:59.020</a></span> | <span class="t">suddenly improved a lot. The one thing you should try if you have a look at the further</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7024" target="_blank">01:57:04.360</a></span> | <span class="t">research section after the questionnaire is it asks to try to do this actually take those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7030" target="_blank">01:57:10.260</a></span> | <span class="t">entity embeddings that we trained in the neural net and use them in the random forest and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7034" target="_blank">01:57:14.840</a></span> | <span class="t">then maybe try ensembling again and see if you can beat the 0.223 that we had. This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7045" target="_blank">01:57:25.260</a></span> | <span class="t">a really nice idea it's like you get you know all the benefits of boosted decision trees</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7052" target="_blank">01:57:32.140</a></span> | <span class="t">but all of the nice features of entity embeddings and so this is something that not enough people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7060" target="_blank">01:57:40.100</a></span> | <span class="t">seem to be playing with for some reason. So overall you know random forests are nice and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7069" target="_blank">01:57:49.940</a></span> | <span class="t">easy to train you know they're very resilient they don't require much pre-processing they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7074" target="_blank">01:57:54.460</a></span> | <span class="t">train quickly they don't overfit you know they can be a little less accurate and they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7083" target="_blank">01:58:03.020</a></span> | <span class="t">can be a bit slow at inference time because the inference you have to go through every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7088" target="_blank">01:58:08.180</a></span> | <span class="t">one of those trees. Having said that a binary tree can be pretty heavily optimized so you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7098" target="_blank">01:58:18.700</a></span> | <span class="t">know it is something you can basically create a totally compiled version of a tree and they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7104" target="_blank">01:58:24.100</a></span> | <span class="t">can certainly also be done entirely in parallel so that's something to consider. Gradient boosting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7116" target="_blank">01:58:36.020</a></span> | <span class="t">machines are also fast to train on the whole but a little more fussy about high parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7121" target="_blank">01:58:41.260</a></span> | <span class="t">you have to be careful about overfitting but a bit more accurate. Neural nets may be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7129" target="_blank">01:58:49.380</a></span> | <span class="t">fussiest to deal with they've kind of got the least rules of thumb around or tutorials</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7136" target="_blank">01:58:56.660</a></span> | <span class="t">around saying this is kind of how to do it it's just a bit a bit newer a little bit less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7140" target="_blank">01:59:00.660</a></span> | <span class="t">well understood but they can give better results in many situations than the other two approaches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7146" target="_blank">01:59:06.580</a></span> | <span class="t">or at least with an ensemble can improve the other two approaches. So I would always start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7151" target="_blank">01:59:11.380</a></span> | <span class="t">with a random code and then see if you can beat it using these. So yeah why don't you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7159" target="_blank">01:59:19.580</a></span> | <span class="t">now see if you can find a Kaggle competition with tabular data whether it's running now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7163" target="_blank">01:59:23.740</a></span> | <span class="t">or it's a past one and see if you can repeat this process for that and see if you can get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7169" target="_blank">01:59:29.220</a></span> | <span class="t">in the top 10% of the private leaderboard that would be a really great stretch goal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7174" target="_blank">01:59:34.860</a></span> | <span class="t">at this point. Implement the decision tree algorithm yourself I think that's an important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7180" target="_blank">01:59:40.100</a></span> | <span class="t">one we really understand it and then from there create your own random forest from scratch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7184" target="_blank">01:59:44.700</a></span> | <span class="t">you might be surprised it's not that hard and then go and have a look at the tabular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7192" target="_blank">01:59:52.500</a></span> | <span class="t">model source code and at this point this is pretty exciting you should find you pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7197" target="_blank">01:59:57.980</a></span> | <span class="t">much know what all the lines do with two exceptions and if you don't you know dig around and explore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7204" target="_blank">02:00:04.900</a></span> | <span class="t">an experiment and see if you can figure it out. And with that we are I am very excited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7213" target="_blank">02:00:13.220</a></span> | <span class="t">to say at a point where we've really dug all the way in to the end of these real valuable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7220" target="_blank">02:00:20.980</a></span> | <span class="t">effective fast AI applications and we're understanding what's going on inside them. What should we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7227" target="_blank">02:00:27.420</a></span> | <span class="t">expect for next week? For next week we will at NLP and computer vision and we'll do the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VEG5xT5gAHc&t=7236" target="_blank">02:00:36.500</a></span> | <span class="t">same kind of ideas delve deep to see what's going on. Thanks everybody see you next week.</span></div></div></body></html>