<html><head><title>Training Agentic Reasoners — Will Brown, Prime Intellect</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Training Agentic Reasoners — Will Brown, Prime Intellect</h2><a href="https://www.youtube.com/watch?v=PbHm2qKnu10"><img src="https://i.ytimg.com/vi_webp/PbHm2qKnu10/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=0">0:0</a> Introduction to the idea that reasoning and agents are similar.<br><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=65">1:5</a> The growing effectiveness of Reinforcement Learning (RL) in AI.<br><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=184">3:4</a> The complexities and challenges of implementing RL.<br><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=281">4:41</a> The connection between popular AI products (agents) and RL fine-tuning.<br><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=438">7:18</a> The core process of Reinforcement Learning.<br><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=621">10:21</a> The importance of tools and real-world tasks for agents.<br><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=733">12:13</a> The problem of "reward hacking" and how to design better evaluations.<br><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=891">14:51</a> Future directions for agentic systems and a practical toolkit for implementation.<br><br><div style="text-align: left;"><a href="./PbHm2qKnu10.html">Whisper Transcript</a> | <a href="./transcript_PbHm2qKnu10.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=0" target="_blank">00:00:00.040</a></span> | <span class="t">Hi everyone, I'm Will Brown. I'm at Prime Intellect. Today I want to talk about training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=18" target="_blank">00:00:18.720</a></span> | <span class="t">agentic reasoners. Just kind of as a very high-level overview, I think a lot of people here are really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=24" target="_blank">00:00:24.880</a></span> | <span class="t">excited about reasoning and a lot of people here are really excited about agents, but I feel like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=28" target="_blank">00:00:28.480</a></span> | <span class="t">lot of the conversations between these two topics are kind of different where people are like, "Oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=32" target="_blank">00:00:32.560</a></span> | <span class="t">reasoning is this one thing and agents are this other thing." And the considerations of reasoning are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=37" target="_blank">00:00:37.760</a></span> | <span class="t">very different from the considerations of building agents. And I think the high-level thesis of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=41" target="_blank">00:00:41.200</a></span> | <span class="t">talk is like, "No, they're kind of the same thing." And you'll see why as we get into it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=45" target="_blank">00:00:45.600</a></span> | <span class="t">First, just to start, RL kind of works now. I think for a long time people were like, "Oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=50" target="_blank">00:00:50.800</a></span> | <span class="t">is RL going to work? Is it not going to work? How hard is it going to be?" And DeepSeek, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=55" target="_blank">00:00:55.440</a></span> | <span class="t">took a lot of people by surprise for many reasons, like the costs or whatever, and how good it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=59" target="_blank">00:00:59.840</a></span> | <span class="t">compared to the open models, to the big labs, as well as just it being fully open. But I think it was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=65" target="_blank">00:01:05.840</a></span> | <span class="t">also just that it was RL applied at scale working with surprisingly few tweaks needed, where you just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=73" target="_blank">00:01:13.120</a></span> | <span class="t">have a good setup, you have a good signal, you have a model that is good enough to do some learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=78" target="_blank">00:01:18.800</a></span> | <span class="t">and you see this curve where doing more RL results in the model getting better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=83" target="_blank">00:01:23.760</a></span> | <span class="t">And it's also kind of how everyone else is doing it. This is what the big labs are really banking on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=90" target="_blank">00:01:30.400</a></span> | <span class="t">to drive the next iterations of progress. The 03 release is the one that OpenAI is really excited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=95" target="_blank">00:01:35.520</a></span> | <span class="t">about, not GPT 4.5. They stopped serving the big pre-trained model via API, but they have continued to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=101" target="_blank">00:01:41.760</a></span> | <span class="t">really double down on the scaling direction of doing more and more reinforcement learning and spending more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=106" target="_blank">00:01:46.800</a></span> | <span class="t">compute on reinforcement learning once you have the right setup to enable progress. And 03 to me is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=113" target="_blank">00:01:53.040</a></span> | <span class="t">like a very naturally agentic model. The ChatGPT version has all of these tools. The kind of selling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=119" target="_blank">00:01:59.200</a></span> | <span class="t">point of it is not just that it's smarter, it's that it's really good at using lots of tools in agentic task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=125" target="_blank">00:02:05.280</a></span> | <span class="t">settings to solve harder problems that involve interacting with complex systems. And that is kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=131" target="_blank">00:02:11.120</a></span> | <span class="t">really the selling point of all of this is that like the more complex your system, the more things can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=136" target="_blank">00:02:16.480</a></span> | <span class="t">go wrong, the more that like a generic LLM API is going to be brittle and go off the rails after a certain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=142" target="_blank">00:02:22.400</a></span> | <span class="t">number of steps. And RL is kind of the way around it. It's the trick you can do to take the system that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=148" target="_blank">00:02:28.240</a></span> | <span class="t">kind of works. Maybe it works on small scales, but as you go harder, it starts going off the rails and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=153" target="_blank">00:02:33.360</a></span> | <span class="t">training the model to be better at that thing. And so this is a recipe that is still kind of like a research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=159" target="_blank">00:02:39.680</a></span> | <span class="t">topic that people are not fully sure like the best way to do it, especially outside of the big labs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=164" target="_blank">00:02:44.800</a></span> | <span class="t">But it clearly is moving in a direction where it's becoming more and more reliable, more and more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=170" target="_blank">00:02:50.400</a></span> | <span class="t">accessible. And the sort of thing that I think would be silly to disregard as a potential like key piece of the future of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=178" target="_blank">00:02:58.720</a></span> | <span class="t">agentic software and agentic applications. But it's also complicated. So on the left here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=184" target="_blank">00:03:04.400</a></span> | <span class="t">this is the like architecture diagram of Veral, which is kind of the most popular software people use in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=189" target="_blank">00:03:09.920</a></span> | <span class="t">the research world for writing papers to do RL. So if you want to like take a model and go do RL,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=196" target="_blank">00:03:16.240</a></span> | <span class="t">Veral kind of expects that you understand all of this. On the left, we have, right, we have GRPO as presented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=202" target="_blank">00:03:22.720</a></span> | <span class="t">in the original DeepSeq math paper back from early 2024. And like, there's a lot of pieces here. There's a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=207" target="_blank">00:03:27.760</a></span> | <span class="t">like complicated steps going on that I think a lot of people who are used to thinking about APIs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=213" target="_blank">00:03:33.680</a></span> | <span class="t">used to thinking about building agents, kind of like are hoping they don't have to worry about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=219" target="_blank">00:03:39.120</a></span> | <span class="t">And are hoping that like, you can just set it aside and like, something else will work and we'll just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=226" target="_blank">00:03:46.240</a></span> | <span class="t">use the APIs, it'll all be great. And I think the reality is like somewhere in the middle where like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=231" target="_blank">00:03:51.520</a></span> | <span class="t">I think it doesn't need to be this complicated. But I think you also kind of do have to be aware of it if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=235" target="_blank">00:03:55.680</a></span> | <span class="t">your goal is really like building the most performing agents, not necessarily just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=239" target="_blank">00:03:59.200</a></span> | <span class="t">today, you need to know about it. But as a piece of the toolkit to potentially make really powerful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=244" target="_blank">00:04:04.400</a></span> | <span class="t">agentic software, I think the people who are willing to do this and take the best open models and really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=251" target="_blank">00:04:11.440</a></span> | <span class="t">RL them for their tasks and configure how to do that well, are going to have a huge advantage. And that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=255" target="_blank">00:04:15.600</a></span> | <span class="t">the kind of thing that also allows you to like build a moat beyond just like being a wrapper API and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=260" target="_blank">00:04:20.480</a></span> | <span class="t">towards something where it's like, oh, I actually have my own model now, but not everyone can be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=265" target="_blank">00:04:25.520</a></span> | <span class="t">a big lab. And so we kind of need to meet in the middle somewhere of like, okay, how do we make this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=269" target="_blank">00:04:29.120</a></span> | <span class="t">a thing that starts to become feasible for startups for individual researchers to actually do? And like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=276" target="_blank">00:04:36.960</a></span> | <span class="t">at what scale does this become like feasible? And so agents are like, the type of product that everyone's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=282" target="_blank">00:04:42.240</a></span> | <span class="t">excited about, we all like, love cloud code and Devin and Manus and O3 and deep research. And like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=288" target="_blank">00:04:48.720</a></span> | <span class="t">these are the sorts of products that are really capturing people's attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=292" target="_blank">00:04:52.320</a></span> | <span class="t">They're products that in their current iteration happen to work kind of because the models that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=297" target="_blank">00:04:57.120</a></span> | <span class="t">are being used have like been RL to basically do these kinds of things. Like Claude is a very good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=303" target="_blank">00:05:03.040</a></span> | <span class="t">coding agent, probably because it has been RL done a lot of code. And so it's like, not very surprising</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=308" target="_blank">00:05:08.000</a></span> | <span class="t">that if you plug Claude into essentially a while loop with some tools, it's like quite good at doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=313" target="_blank">00:05:13.200</a></span> | <span class="t">these things because it's basically most likely been trained in almost that exact setting. Same for things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=319" target="_blank">00:05:19.440</a></span> | <span class="t">like O3, like it can do GeoGuessr and whatever, because whether it's literally GeoGuessr or something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=324" target="_blank">00:05:24.240</a></span> | <span class="t">close to it, they have talked about training it to do this image cropping trick. Like that's a technique</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=330" target="_blank">00:05:30.160</a></span> | <span class="t">that it didn't just know how to do out of the box. They said, hey, let's give it these tools to do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=334" target="_blank">00:05:34.560</a></span> | <span class="t">and use reinforcement learning to train it to do that. And so that is kind of the recipe that we have seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=339" target="_blank">00:05:39.120</a></span> | <span class="t">coming from the big labs as if you want a powerful agent that can do a certain type of task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=343" target="_blank">00:05:43.920</a></span> | <span class="t">you can use reinforcement learning to train it to do that task better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=348" target="_blank">00:05:48.640</a></span> | <span class="t">And so these are kind of the same thing, actually, like building an agent, the pieces of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=352" target="_blank">00:05:52.720</a></span> | <span class="t">making an agent in terms of the harness, the environment, the tools and the iteration is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=359" target="_blank">00:05:59.440</a></span> | <span class="t">essentially the same conceptual framing as canonical reinforcement learning in the sense of policies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=365" target="_blank">00:06:05.920</a></span> | <span class="t">actions, states, rewards, transition probabilities. And I think the more that we start to view</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=370" target="_blank">00:06:10.880</a></span> | <span class="t">agents as this umbrella, which is not just about static chaining of API calls, but as this interaction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=378" target="_blank">00:06:18.160</a></span> | <span class="t">loop with evaluations, that framing really is the way to think about RL, which is you build a system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=384" target="_blank">00:06:24.800</a></span> | <span class="t">where a thing is interacting with an environment, and you have some way of evaluating how good it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=389" target="_blank">00:06:29.760</a></span> | <span class="t">doing. And RL is simply an algorithm to improve based on the scores of these evaluations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=396" target="_blank">00:06:36.560</a></span> | <span class="t">And if you're building agents, and you're tuning your prompts, and you're fiddling with your harnesses,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=400" target="_blank">00:06:40.880</a></span> | <span class="t">this is kind of like doing RL by hand. What you're doing is you're saying like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=404" target="_blank">00:06:44.960</a></span> | <span class="t">okay, currently, my evals are saying this, let's make sure the evals are like capturing what I want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=410" target="_blank">00:06:50.240</a></span> | <span class="t">Let's look at the data. Let's see if the data matches what my evals are saying. And then, oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=415" target="_blank">00:06:55.760</a></span> | <span class="t">let's try a new prompt. Let's try giving it a new tool. Let's try switching out the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=420" target="_blank">00:07:00.800</a></span> | <span class="t">This is the process which is also being targeted by reinforcement learning in the general sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=427" target="_blank">00:07:07.600</a></span> | <span class="t">beyond individual algorithms. About these algorithms, there's a few of them that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=432" target="_blank">00:07:12.960</a></span> | <span class="t">very important. All of them have different implementation details. But in general,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=437" target="_blank">00:07:17.120</a></span> | <span class="t">the idea is you have a bunch of tasks, like versions of your problem, which are essentially prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=441" target="_blank">00:07:21.520</a></span> | <span class="t">You have rollouts, which are just completions, potentially involving many steps of interactions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=445" target="_blank">00:07:25.680</a></span> | <span class="t">but like one sequence of stuff happening. And then you have evaluation, potentially interleaved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=451" target="_blank">00:07:31.200</a></span> | <span class="t">throughout or at the end of the sequence. And what you're estimating is the advantage. The</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=455" target="_blank">00:07:35.840</a></span> | <span class="t">advantage here is the idea that sometimes your model would be better than others. Like these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=460" target="_blank">00:07:40.800</a></span> | <span class="t">LLMs are all non-deterministic. You have temperature above zero. You have different things happen in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=467" target="_blank">00:07:47.600</a></span> | <span class="t">different rolls of the dice. And this forking process of saying like, okay, this time it did better than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=474" target="_blank">00:07:54.240</a></span> | <span class="t">that time. Why was it different? RL is really about saying like, okay, this is the actual thing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=482" target="_blank">00:08:02.160</a></span> | <span class="t">changed, that resulted in the reward being better, the eval being better. This is the token at which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=488" target="_blank">00:08:08.240</a></span> | <span class="t">I went down the good path versus the bad path. And whether you're doing PPO or GRPO,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=494" target="_blank">00:08:14.480</a></span> | <span class="t">like this is the mechanism by which you get the signal of like, you have something that sometimes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=500" target="_blank">00:08:20.800</a></span> | <span class="t">went better, sometimes went worse. Now you can kind of very surgically have the model, learn to do more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=507" target="_blank">00:08:27.200</a></span> | <span class="t">of the good stuff without changing too much overall. I think this is also kind of maybe a reason why DPO,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=512" target="_blank">00:08:32.400</a></span> | <span class="t">I think people were hoping DPO would like really work well. In my view, DPO does not necessarily have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=517" target="_blank">00:08:37.280</a></span> | <span class="t">this like fine grained advantage estimate. Like it's not really clear just from like a full good completion and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=522" target="_blank">00:08:42.320</a></span> | <span class="t">a full bad completion where you're really getting the signal about these complex branching processes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=526" target="_blank">00:08:46.880</a></span> | <span class="t">PPO has this, but it's also very expensive. GRPO I think has taken a lot of people kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=532" target="_blank">00:08:52.160</a></span> | <span class="t">by storm in terms of like being a very nice like middle ground where it's more computationally efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=538" target="_blank">00:08:58.400</a></span> | <span class="t">It's like simple to implement. But also it does have this kind of forking process that comes just from sampling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=546" target="_blank">00:09:06.560</a></span> | <span class="t">There's also just too many papers. So like, I think a lot of people just see a new paper every day and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=551" target="_blank">00:09:11.520</a></span> | <span class="t">are like, do I have to read this one? And I feel that too. Like, I think it's difficult to know up front,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=557" target="_blank">00:09:17.760</a></span> | <span class="t">like which of these are going to be important, which of them are just going to be like noise, especially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=562" target="_blank">00:09:22.720</a></span> | <span class="t">because lots of them have very sensationalist titles, like, oh, Quen doesn't work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=567" target="_blank">00:09:27.040</a></span> | <span class="t">Or like, or Quen, everyone, everything only works with Quen is like kind of true. But like, there's also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=572" target="_blank">00:09:32.320</a></span> | <span class="t">more to the story than that. And I think there's like different implementation details of like, oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=576" target="_blank">00:09:36.240</a></span> | <span class="t">if you change the loss function like this in this experiment, then it works. And I think for most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=581" target="_blank">00:09:41.520</a></span> | <span class="t">people, it is best to just like kind of set this aside and to not get too caught up in the individual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=588" target="_blank">00:09:48.400</a></span> | <span class="t">details of individual experiments, individual papers, and kind of think more holistically about what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=594" target="_blank">00:09:54.240</a></span> | <span class="t">process of reinforcement learning doing? What implementation details am I willing to kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=599" target="_blank">00:09:59.040</a></span> | <span class="t">leave to other people to figure out and eventually come to me with like software that like has the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=602" target="_blank">00:10:02.880</a></span> | <span class="t">knob set correctly? And which pieces are actually important for solving the problems I care about?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=608" target="_blank">00:10:08.240</a></span> | <span class="t">And so for a lot of people, I think the things that are going to be really interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=612" target="_blank">00:10:12.640</a></span> | <span class="t">are things that are relating to actual software to actual problems that they want to sell in the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=618" target="_blank">00:10:18.560</a></span> | <span class="t">And agents, I think are kind of the instantiation of that where this makes sense. And the thing that makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=623" target="_blank">00:10:23.680</a></span> | <span class="t">an agent an agent is tools, the ability to interact with an environment with a system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=628" target="_blank">00:10:28.720</a></span> | <span class="t">A lot of people here are like very excited about MCP at the conference. Like MCP is just tools. MCP is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=633" target="_blank">00:10:33.600</a></span> | <span class="t">about giving your LM the ability to like interact with stuff, to go solve problems that involve changing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=641" target="_blank">00:10:41.440</a></span> | <span class="t">files, making requests, editing code, running code. And so I think these are the papers that I get excited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=647" target="_blank">00:10:47.600</a></span> | <span class="t">about because they feel like, like there's parts of the puzzle that are not fully solved yet of like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=651" target="_blank">00:10:51.360</a></span> | <span class="t">what's the right way to do all of this? Like there's still some open questions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=654" target="_blank">00:10:54.400</a></span> | <span class="t">but I think those are getting kind of refined. We're starting to see more and more, but like a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=661" target="_blank">00:11:01.280</a></span> | <span class="t">the code, the tools we have out in the wild, they're like, go do this. Like if you want to like go play</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=665" target="_blank">00:11:05.520</a></span> | <span class="t">around with RL, most code bases are like very set up for like either code and math tasks or things that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=672" target="_blank">00:11:12.320</a></span> | <span class="t">quite similar to that. That's kind of my fault. I had a snippet go viral that was like, here's how you do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=679" target="_blank">00:11:19.360</a></span> | <span class="t">RL on like GSM AK, which is like a kind of easy math data set. And then I think I've seen a lot of people like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=685" target="_blank">00:11:25.600</a></span> | <span class="t">stick with this as like, Oh, we're going to RL on math. And I like, this is also just like math is easy to evaluate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=690" target="_blank">00:11:30.640</a></span> | <span class="t">And I think people are the evals, writing evals are hard. Like there's a whole track going on in parallel to this about like how to build a good eval.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=698" target="_blank">00:11:38.480</a></span> | <span class="t">And so I think a lot of researchers gravitate towards things that like look like the benchmarks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=703" target="_blank">00:11:43.120</a></span> | <span class="t">that are also really easy to eval because there's like a very clear signal of like, okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=707" target="_blank">00:11:47.040</a></span> | <span class="t">this thing is like, right. This thing is wrong. Good. Okay. We're doing RL. But like real world tasks are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=713" target="_blank">00:11:53.200</a></span> | <span class="t">messier than that. We are not going to like get great software systems just by like hill climbing on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=720" target="_blank">00:12:00.960</a></span> | <span class="t">whatever question answer benchmark is popular today. What we're going to do is we're going to have to do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=726" target="_blank">00:12:06.000</a></span> | <span class="t">start thinking about like the actual systems at hand and the challenges that emerge when we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=731" target="_blank">00:12:11.040</a></span> | <span class="t">trying to design these rewards. And so like reward hacking is like a real thing. Um, I think this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=735" target="_blank">00:12:15.440</a></span> | <span class="t">one of the lessons that like RL works, but also it's not like always going to work. There are things that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=740" target="_blank">00:12:20.960</a></span> | <span class="t">can go wrong. And to me, reward hacking is really a message about the difficulty of building good evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=746" target="_blank">00:12:26.640</a></span> | <span class="t">Like, uh, what you really want with an eval is for it to be easier for your model to do the task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=752" target="_blank">00:12:32.960</a></span> | <span class="t">than to hack the eval. You want to build a reward signal that actually captures what you care about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=758" target="_blank">00:12:38.320</a></span> | <span class="t">where, uh, gaming it is like more difficult than not gaming it. If you can, if the model can learn to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=765" target="_blank">00:12:45.840</a></span> | <span class="t">the task directly just by doing what you want it to do in the spirit of the task, then like that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=773" target="_blank">00:12:53.680</a></span> | <span class="t">what will happen. It will flow in the path of least resistance. This is like models just want to learn,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=777" target="_blank">00:12:57.680</a></span> | <span class="t">but they want to learn to do better on reward signals. And so your reward signals have to point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=782" target="_blank">00:13:02.000</a></span> | <span class="t">in the direction of the thing you actually care about. Um, otherwise like models will find cheats.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=787" target="_blank">00:13:07.360</a></span> | <span class="t">Um, and I think thinking about these things in combination kind of points a little bit towards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=793" target="_blank">00:13:13.280</a></span> | <span class="t">a direction that I think is going to be very promising. And there's some very early signs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=796" target="_blank">00:13:16.480</a></span> | <span class="t">that like this actually can work, um, which is like when R1 came out, I was kind of like speculating,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=802" target="_blank">00:13:22.720</a></span> | <span class="t">like what's next? What are the things that are going to unlock this sort of technique being used more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=808" target="_blank">00:13:28.560</a></span> | <span class="t">generally? Um, and you people talk a lot about like generator, verifier gaps, like what are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=814" target="_blank">00:13:34.080</a></span> | <span class="t">differences between like solving a problem versus checking if you have a solution? And a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=817" target="_blank">00:13:37.760</a></span> | <span class="t">problems like are much easier to check than solve, but this isn't like a binary thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=822" target="_blank">00:13:42.240</a></span> | <span class="t">This is a spectrum of how difficult is it to verify a thing. But, um, there's some kind of signs that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=828" target="_blank">00:13:48.640</a></span> | <span class="t">you kind of can do evaluations on more ambiguous tasks by just breaking them down into smaller pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=836" target="_blank">00:13:56.160</a></span> | <span class="t">and by using LLMs as subroutines in your evaluations, like LLMs that judge on steroids,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=843" target="_blank">00:14:03.120</a></span> | <span class="t">or maybe you want to actually like train a specialized LLM who is really good at doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=846" target="_blank">00:14:06.800</a></span> | <span class="t">these fine grained evaluations. I like using the term rubric as a conceptual general umbrella around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=852" target="_blank">00:14:12.160</a></span> | <span class="t">reward models, reward functions, LLMs judge setups, like the criteria on which you are evaluating a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=857" target="_blank">00:14:17.760</a></span> | <span class="t">thing. There's a cool paper from deep seek that I was found very exciting when it came out a couple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=861" target="_blank">00:14:21.760</a></span> | <span class="t">months ago about like how to train reward models that like generate these rubrics on the fly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=866" target="_blank">00:14:26.240</a></span> | <span class="t">There was a paper very recently that does this for creative writing and kind of found that like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=870" target="_blank">00:14:30.080</a></span> | <span class="t">yes, you actually can train reward models that will come up with nuanced fine grained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=875" target="_blank">00:14:35.840</a></span> | <span class="t">evaluation criteria for a task on the fly, given the actual problem. And this gives you something that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=881" target="_blank">00:14:41.520</a></span> | <span class="t">results in a very like fine grained score that allows you to actually do LL and like keep getting better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=886" target="_blank">00:14:46.880</a></span> | <span class="t">And I think like this is an area that I'm really excited about to keep watching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=892" target="_blank">00:14:52.400</a></span> | <span class="t">But also like multi-turn. Multi-turn is probably where we're headed. We want to do agentic search,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=897" target="_blank">00:14:57.520</a></span> | <span class="t">we want to do tool calls, software, games, long horizon planning, computer use, memory. Scaling on tool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=903" target="_blank">00:15:03.040</a></span> | <span class="t">calls lets you solve harder problems. And so how do we actually like do this? What's the way to go about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=908" target="_blank">00:15:08.960</a></span> | <span class="t">building multi-turn agentic systems to do and that we can use RL with. And I think the conceptual pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=916" target="_blank">00:15:16.560</a></span> | <span class="t">here are environments are basically harnesses, rewards are basically evals, tasks are just prompts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=922" target="_blank">00:15:22.000</a></span> | <span class="t">and your policy in the RL sense hopefully should just be as simple as like an LLM API.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=927" target="_blank">00:15:27.280</a></span> | <span class="t">I think the programming interface that makes sense for a lot of people is to have an API</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=931" target="_blank">00:15:31.840</a></span> | <span class="t">that you're writing code as if it's just a normal agent in a loop. But then this is a thing that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=936" target="_blank">00:15:36.960</a></span> | <span class="t">can use to go do RL. And so that's what I've been building over the past couple of months. I maintain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=942" target="_blank">00:15:42.000</a></span> | <span class="t">a repo called verifiers. It's finally on pip out in the world, you can just install it, but it's been a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=949" target="_blank">00:15:49.520</a></span> | <span class="t">long time coming. And what it really is, is a toolkit of these pieces to make it so that building an agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=957" target="_blank">00:15:57.280</a></span> | <span class="t">that you can actually train with RL feels just like building an agent. So the interaction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=961" target="_blank">00:16:01.760</a></span> | <span class="t">protocol here is like quite simple. Like this is the entire rollout function on the left of like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=966" target="_blank">00:16:06.960</a></span> | <span class="t">what happens in the code when you're running an agent to do RL, which is that you kind of set up some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=971" target="_blank">00:16:11.920</a></span> | <span class="t">initial state stuff, have a while loop for is it done yet? If it's not done, do a turn. And the thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=977" target="_blank">00:16:17.520</a></span> | <span class="t">you're passing here is a client object that's just an open AI compatible API. And I think this is the kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=984" target="_blank">00:16:24.000</a></span> | <span class="t">of interface that you really want if you want people to be able to go from their agent applications to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=989" target="_blank">00:16:29.760</a></span> | <span class="t">something that's trainable, something that they can use with RL. It's been a lot of fun thinking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=994" target="_blank">00:16:34.080</a></span> | <span class="t">about like, what are the abstractions? What are the pieces here? And so like, there's things like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=997" target="_blank">00:16:37.840</a></span> | <span class="t">parsers and rubrics that I think are like nice building blocks that you sometimes want to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1001" target="_blank">00:16:41.440</a></span> | <span class="t">You can also like not use them if you don't want to, but like I've tried to make it fun and user-friendly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1005" target="_blank">00:16:45.440</a></span> | <span class="t">The other day, I was like, let's train a Wordle agent. I think this was like a fun little toy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1010" target="_blank">00:16:50.240</a></span> | <span class="t">problem where it's like, it's not that hard of like a game for us as humans, but like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1015" target="_blank">00:16:55.680</a></span> | <span class="t">it's actually like kind of tricky to get your code to be this sort of thing where you have this like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1020" target="_blank">00:17:00.400</a></span> | <span class="t">multi-turn interaction protocol that you actually can do learning with. But now it's like much easier,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1025" target="_blank">00:17:05.680</a></span> | <span class="t">like the code to do these things is like quite simple. And the reward functions can kind of be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1030" target="_blank">00:17:10.880</a></span> | <span class="t">relatively simple for this sort of setup where it's like, okay, you want to reward it for like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1034" target="_blank">00:17:14.000</a></span> | <span class="t">solving the thing eventually, but also like give it more rewards for doing it in less turns. And like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1039" target="_blank">00:17:19.760</a></span> | <span class="t">this is a 7b model like works reasonably well, but one of the reasons it works, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1044" target="_blank">00:17:24.640</a></span> | <span class="t">which I'll talk about in a sec is, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1046" target="_blank">00:17:26.720</a></span> | <span class="t">Sft warmup as a way of kind of lowering the barrier of entry. Like this, the code as it is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1051" target="_blank">00:17:31.520</a></span> | <span class="t">is very much set up so that like your environments for RL are also just like synthetic data loops or evals</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1056" target="_blank">00:17:36.240</a></span> | <span class="t">where you can plug in clod or deep seek or open AI and like test. So you don't have to like do RL to debug.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1062" target="_blank">00:17:42.160</a></span> | <span class="t">You can like debug with an API in terms of seeing, is this a good eval?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1066" target="_blank">00:17:46.080</a></span> | <span class="t">Is this a good reward? Once you're kind of comfortable with it, you can like use whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1070" target="_blank">00:17:50.080</a></span> | <span class="t">API you like that you are allowed to use and make synthetic data, do some SFT on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1075" target="_blank">00:17:55.200</a></span> | <span class="t">And now you can start doing RL and this like helps a lot with small models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1078" target="_blank">00:17:58.080</a></span> | <span class="t">I think there's a lot of efficiency challenges that are like, I've been kind of hard at work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1083" target="_blank">00:18:03.120</a></span> | <span class="t">trying to solve in terms of like having all of your computation be utilized effectively,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1086" target="_blank">00:18:06.960</a></span> | <span class="t">having everything be like fully async. So you don't have to worry about like batching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1090" target="_blank">00:18:10.400</a></span> | <span class="t">and that your trainer and your inference can kind of go at the same time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1094" target="_blank">00:18:14.080</a></span> | <span class="t">You can be like a little bit off policy. A lot of engineering that I'm hoping like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1098" target="_blank">00:18:18.320</a></span> | <span class="t">if you want to worry about that, great, dig into it, fork the repo, mess with things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1103" target="_blank">00:18:23.120</a></span> | <span class="t">If you don't want to, you shouldn't have to. And like the idea here is that this should become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1109" target="_blank">00:18:29.760</a></span> | <span class="t">something that more people are trying out, more people are having fun with, with exploring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1114" target="_blank">00:18:34.800</a></span> | <span class="t">and getting a feel for it. Because if it's going to be a thing we have to worry about, if this is the future of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1121" target="_blank">00:18:41.200</a></span> | <span class="t">building better agent models for your applications, like now's a good time to start. And so this stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1128" target="_blank">00:18:48.080</a></span> | <span class="t">is set up so you can like on a couple GPUs like do a lot of interesting research. Like the barrier of entry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1133" target="_blank">00:18:53.920</a></span> | <span class="t">is like much lower now than it used to be. I have a lot of fun doing this on like a couple GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PbHm2qKnu10&t=1139" target="_blank">00:18:59.920</a></span> | <span class="t">We sell GPUs by the way. Thanks everybody. I don't think we have time for questions. But yeah.</span></div></div></body></html>