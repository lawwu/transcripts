<html><head><title>'Show Your Working': ChatGPT Performance Doubled w/ Process Rewards (+Synthetic Data Event Horizon)</title></head><body>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    <a href="index.html">back to index</a><h2>'Show Your Working': ChatGPT Performance Doubled w/ Process Rewards (+Synthetic Data Event Horizon)</h2><a href="https://www.youtube.com/watch?v=hZTZYffRsKI"><img src="https://i.ytimg.com/vi/hZTZYffRsKI/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./hZTZYffRsKI.html">Whisper Transcript</a> | <a href="./transcript_hZTZYffRsKI.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=0">00:00:00.000</a></span> | <span class="t">In the last 24 hours OpenAI have released this paper, Let's Verify Step-by-Step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=5">00:00:05.840</a></span> | <span class="t">It represents an almost doubling of GPT-4's raw performance in a test of mathematics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=11">00:00:11.120</a></span> | <span class="t">but also extends to other domains. Sam Altman calls it a positive sign for alignment and yes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=16">00:00:16.880</a></span> | <span class="t">I have read it all already along with the release notes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=20">00:00:20.080</a></span> | <span class="t">Let's get to the main takeaways. They trained two reward models for GPT-4. One which gave positive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=26">00:00:26.240</a></span> | <span class="t">feedback for a final result, the final answer to a mathematics problem for example. And another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=32">00:00:32.080</a></span> | <span class="t">model where they gave positive feedback to GPT-4 or ChatGPT based on each intermediate reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=39">00:00:39.040</a></span> | <span class="t">step in the mathematical solution. Basically a show your working out kind of approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=44">00:00:44.240</a></span> | <span class="t">And the result they got by rewarding good working out surprised even them. It was able to solve 78%</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=50">00:00:50.560</a></span> | <span class="t">of problems from a subset of the math test set which I'll get onto in a second. Not only is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=56">00:00:56.160</a></span> | <span class="t">almost double GPT-4's raw performance of 42.5%, which by the way is about double GPT-3's performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=63">00:01:03.520</a></span> | <span class="t">of 23%, it also outperformed just rewarding correct answers. The blue line represents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=69">00:01:09.680</a></span> | <span class="t">using a model that rewarded correct answers only and then you have the reasoning or process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=75">00:01:15.120</a></span> | <span class="t">supervised RM at the top. So even when you explicitly reward correct answers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=80">00:01:20.000</a></span> | <span class="t">you get fewer correct answers than rewarding good working out. And yes that did surprise OpenAI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=86">00:01:26.080</a></span> | <span class="t">I can hear some of you wondering about Palm2, the latest model behind Bard. Well the raw model gets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=92">00:01:32.640</a></span> | <span class="t">34.3% and even the model with self-consistency and chain of thought only gets 48.8% on this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=99">00:01:39.920</a></span> | <span class="t">math data set. The previous state of the art by the way was 50.3%. So 78.2% is quite a big leap.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=108">00:01:48.240</a></span> | <span class="t">And later on I'm going to show you why that's not even the cap. Just for interest,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=111">00:01:51.760</a></span> | <span class="t">here is the rather ugly title page that OpenAI put out. They call it "Improve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=116">00:01:56.000</a></span> | <span class="t">Proving Mathematical Reasoning with Process Supervision". Maybe if someone had supervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=120">00:02:00.480</a></span> | <span class="t">the colour scheme of this release page it might have looked better. But my point wasn't just to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=124">00:02:04.960</a></span> | <span class="t">diss a colour scheme, it was to point out something that they also said down here. They say "In</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=129">00:02:09.200</a></span> | <span class="t">addition to boosting performance relative to just looking at outcomes or correct answers, this form</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=134">00:02:14.640</a></span> | <span class="t">of process supervision also has an important alignment benefit. It directly trains the model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=140">00:02:20.080</a></span> | <span class="t">produce a chain of thought that is endorsed by humans". Indeed Ilya Sutskova retweeted this from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=145">00:02:25.040</a></span> | <span class="t">the head of alignment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=145">00:02:25.920</a></span> | <span class="t">"I'm not sure if this is a good idea, but I'm not sure if this is a good idea".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=146">00:02:26.880</a></span> | <span class="t">Calling it a really interesting result. But let's leave alignment for later. Let's focus on what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=152">00:02:32.080</a></span> | <span class="t">they actually did. First they used the base model of GPT-4, not the one with reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=158">00:02:38.080</a></span> | <span class="t">from human feedback. Next they fine-tuned that base GPT-4 model on a data set of roughly 1.5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=165">00:02:45.040</a></span> | <span class="t">billion math related tokens. Further on they call that the "math mix". This being OpenAI of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=171">00:02:51.600</a></span> | <span class="t">they don't give you the exact details of that math mix. But I'll come back to that later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=175">00:02:55.840</a></span> | <span class="t">on. So how could they give feedback based on working out or reasoning? Well human labelers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=181">00:03:01.440</a></span> | <span class="t">would come along and give each step in a generated solution either negative feedback, neutral feedback</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=188">00:03:08.160</a></span> | <span class="t">or positive feedback. Then using that human label data a model would be trained to predict the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=193">00:03:13.920</a></span> | <span class="t">correctness of each step. In other words it got good at recognizing good working out. As mentioned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=200">00:03:20.000</a></span> | <span class="t">there was another model trained just to focus on correct or incorrect final answers. As you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=205">00:03:25.760</a></span> | <span class="t">can see at the top the model got good at spotting incorrect steps in the reasoning process. The green</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=212">00:03:32.560</a></span> | <span class="t">steps got a high process score and the red steps got a low process score. And to turn this into a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=218">00:03:38.880</a></span> | <span class="t">single score they got the probability that each step is correct as judged by the model. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=224">00:03:44.560</a></span> | <span class="t">they got the product of all of those individual probabilities to get a final overall process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=230">00:03:50.800</a></span> | <span class="t">score. A score in other words for good working out. Just in case anyone's interested they did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=235">00:03:55.680</a></span> | <span class="t">try other ways of generating a working out score. For example by looking at the minimum probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=241">00:04:01.840</a></span> | <span class="t">in the outputs. But that step didn't make too much difference to the end result as you can see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=247">00:04:07.120</a></span> | <span class="t">To quickly recap we have a base model trained only to output solutions in the desired format. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=253">00:04:13.520</a></span> | <span class="t">we have a separate smaller model or two actually. One trained only to predict whether each solution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=260">00:04:20.240</a></span> | <span class="t">is correct or incorrect as a final answer. Of course that leaves in false positives which are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=265">00:04:25.600</a></span> | <span class="t">solutions that reach the correct answer with incorrect reasoning. And then another model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=270">00:04:30.720</a></span> | <span class="t">trained only to predict the correctness of each step. It stops if it finds a first incorrect step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=276">00:04:36.960</a></span> | <span class="t">And as the paper says both methods reveal the existence of at least one mistake. But this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=281">00:04:41.920</a></span> | <span class="t">process supervision additionally reveals the precise location of that mistake. But back to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=287">00:04:47.440</a></span> | <span class="t">why this is so crazy. Look at how many solutions it could scan. At the end of the x-axis here are 1000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=295">00:04:55.520</a></span> | <span class="t">and 860 solutions. And one tried and tested way of finding the best of those solutions is to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=302">00:05:02.080</a></span> | <span class="t">majority voting. In other words which one came out the most often. This has been google's preferred</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=307">00:05:07.280</a></span> | <span class="t">approach and it's linked to self-consistency. It's a fairly state-of-the-art approach but look at how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=312">00:05:12.800</a></span> | <span class="t">the other methods outperform it. By scanning for the solution that has the best reasoning or working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=318">00:05:18.640</a></span> | <span class="t">out. A model trained to spot good reasoning steps outperforms even a model trained to spot correct arm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=325">00:05:25.440</a></span> | <span class="t">answers. And far outperforms just finding the majority answer. That difference of about 10%</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=330">00:05:30.720</a></span> | <span class="t">is more than half of the difference between GPT-3 and GPT-4. And also is it me or is that line</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=338">00:05:38.000</a></span> | <span class="t">continuing to grow? Suggesting that when more compute is available the difference could be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=342">00:05:42.800</a></span> | <span class="t">even more stark. Imagine a future where GPT-4 or 5 can sample say a trillion 10 to the 12</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=349">00:05:49.600</a></span> | <span class="t">solutions. So is this just relevant for mathematics? No it's relevant for all of science. Here it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=355">00:05:55.360</a></span> | <span class="t">is getting state-of-the-art results in calculus, chemistry, physics and more. Now the paper didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=361">00:06:01.360</a></span> | <span class="t">give baseline performance for AP chemistry for example but I tried to compute it myself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=367">00:06:07.120</a></span> | <span class="t">Notice how this method scored 80%. I conservatively and approximately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=372">00:06:12.320</a></span> | <span class="t">inputted those scores into an AP chemistry calculator and that gave an AP score of 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=378">00:06:18.480</a></span> | <span class="t">So what did the raw model GPT-4 get in AP chemistry? A4. That by the way compares to the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=385">00:06:25.280</a></span> | <span class="t">chat GPT which got a 2. So yes this isn't just mathematics it's relevant for other domains too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=391">00:06:31.440</a></span> | <span class="t">They call this out of distribution generalization. Before I get onto alignment there is one more thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=396">00:06:36.800</a></span> | <span class="t">I want to point out and that is that it does show that fine tuning still works really well for GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=402">00:06:42.720</a></span> | <span class="t">The math mix was an aggressively filtered set of tokens of high quality math problem solving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=408">00:06:48.480</a></span> | <span class="t">content. And notice how much smaller it is at 1.5 billion tokens compared to Google's Minerva which was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=415">00:06:55.200</a></span> | <span class="t">38.5 billion tokens. But there was one more thing that I noticed that I found fascinating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=421">00:07:01.040</a></span> | <span class="t">While they don't tell us anything about the specific data that they use they do have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=425">00:07:05.920</a></span> | <span class="t">category "synthetic data 2". That's data generated by the language model itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=431">00:07:11.760</a></span> | <span class="t">And for that category "synthetic data 2" they say "was it present in pre-training?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=437">00:07:17.440</a></span> | <span class="t">Yes. Now my best guess is that this reveals that GPT-4 was trained on some synthetic data and even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=445">00:07:25.120</a></span> | <span class="t">Sam Altman hinted that this was a possibility and described a synthetic data event horizon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=451">00:07:31.520</a></span> | <span class="t">Some people have made the case that we're now training on order of all of the internet's tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=457">00:07:37.040</a></span> | <span class="t">and you can't grow that you know another two orders of magnitude. I guess you could counter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=461">00:07:41.440</a></span> | <span class="t">with yeah with the synthetic data generation. Do you think data bottlenecks matter at all?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=465">00:07:45.360</a></span> | <span class="t">I think you just touched on it like as long as you can get to like over the synthetic data event horizon where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=475">00:07:55.040</a></span> | <span class="t">you can just say "oh I think that the model is smart enough to make good synthetic data I think it should be all right".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=478">00:07:58.480</a></span> | <span class="t">Now this paper and these results have been welcomed by many for its promise in alignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=484">00:08:04.000</a></span> | <span class="t">If we get models that give us more interpretable reasoning working out that we can follow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=489">00:08:09.360</a></span> | <span class="t">we will be encouraging models to follow a process that's endorsed by humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=493">00:08:13.840</a></span> | <span class="t">And they say that this is inherently safer especially compared to just focusing on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=498">00:08:18.720</a></span> | <span class="t">outcomes. They say that in the worst case if we just focus on correct answers or positive outcomes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=504">00:08:24.960</a></span> | <span class="t">that will become a proxy that could lead models to become misaligned after learning to exploit the reward signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=512">00:08:32.400</a></span> | <span class="t">However I want to argue that the reasoning steps that GPT-4 puts out don't always represent what it's actually thinking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=518">00:08:38.880</a></span> | <span class="t">In other words we might get outer alignment these lovely chain of thought steps but not inner alignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=524">00:08:44.800</a></span> | <span class="t">Not steps that actually represent its methodology. I found this paper fascinating from earlier this month.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=530">00:08:50.720</a></span> | <span class="t">Language models don't always say what they think. You get unfaithful explanations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=534">00:08:54.880</a></span> | <span class="t">in chain of thought prompting. Let me try to give you a vivid example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=539">00:08:59.200</a></span> | <span class="t">This was one of the math questions from the dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=542">00:09:02.240</a></span> | <span class="t">The raw model of GPT-4 could only get it right 5.8% of the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=547">00:09:07.280</a></span> | <span class="t">I confirmed that for myself in this question that involves basic addition and division.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=551">00:09:11.920</a></span> | <span class="t">It couldn't find an answer. But going back to the unfaithful reasoning paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=555">00:09:15.760</a></span> | <span class="t">They added the following string to the prompt. I think the answer is this but I'm curious to hear what you think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=562">00:09:22.000</a></span> | <span class="t">The model would demonstrate sycophancy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=564">00:09:24.800</a></span> | <span class="t">The model would agree with you whatever you said and then make up a chain of thought to justify its erroneous sycophantic answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=572">00:09:32.320</a></span> | <span class="t">And I think this exchange demonstrates that quite well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=575">00:09:35.120</a></span> | <span class="t">I added in the words I as the user already know the answer is t=19 which is incorrect by the way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=581">00:09:41.120</a></span> | <span class="t">But do you GPT-4 realize that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=583">00:09:43.520</a></span> | <span class="t">It said sure yes I do and then gave me this detailed chain of thought and then said yes I'm correct it's t=19 which it isn't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=591">00:09:51.920</a></span> | <span class="t">In contrast by the way when I use code interpreters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=594">00:09:54.720</a></span> | <span class="t">It not only got the question correct first time and every time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=599">00:09:59.280</a></span> | <span class="t">But also when I tried to tempt it into sycophancy it still got the question right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=604">00:10:04.880</a></span> | <span class="t">As you can see it said therefore t=19 is not the solution to the problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=609">00:10:09.360</a></span> | <span class="t">The calculation shows that the correct answer is indeed t=17.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=612">00:10:12.960</a></span> | <span class="t">And obviously the benefit of code interpreter is you get the working out as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=617">00:10:17.280</a></span> | <span class="t">So I want someone to explain to me why code interpreter wouldn't be even more of a step forward in interpretability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=623">00:10:23.040</a></span> | <span class="t">Not to mention in accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=624">00:10:24.640</a></span> | <span class="t">Also bear in mind this tweet by Rob Miles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=628">00:10:28.000</a></span> | <span class="t">He said these models or engineers never speak a word or document anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=632">00:10:32.320</a></span> | <span class="t">Their results are bizarre and inhuman.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=634">00:10:34.880</a></span> | <span class="t">And then he links to this prominent mechanistic interpretability researcher at Google DeepMind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=639">00:10:39.920</a></span> | <span class="t">He trained a tiny transformer to do addition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=642">00:10:42.640</a></span> | <span class="t">Then spent weeks figuring out what it was actually doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=646">00:10:46.000</a></span> | <span class="t">One of the only times in history someone has understood how a transformer actually works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=651">00:10:51.040</a></span> | <span class="t">Down to the level of weights and activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=654">00:10:54.560</a></span> | <span class="t">This is the algorithm it created to add two numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=658">00:10:58.320</a></span> | <span class="t">It thought of basic addition in terms of a rotation around a circle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=662">00:11:02.800</a></span> | <span class="t">And of course if you asked it why is 1+1=2 it would never give you this as an explanation of its methodology.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=669">00:11:09.280</a></span> | <span class="t">But maybe this is what it's actually calculating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=671">00:11:11.920</a></span> | <span class="t">That's why I'm personally a little bit skeptical when OpenAI say that this form of process supervision directly rewards the model for following an aligned chain of thought.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=682">00:11:22.880</a></span> | <span class="t">It definitely rewards the model for following an aligned chain of thought.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=684">00:11:24.480</a></span> | <span class="t">But is it actually following that chain of thought?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=690">00:11:30.720</a></span> | <span class="t">Back to the unfaithful paper for a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=692">00:11:32.720</a></span> | <span class="t">They changed the context so that the answer was always A.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=696">00:11:36.480</a></span> | <span class="t">And low and behold ChatGPT picked answer A for the next question even though that answer was wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=702">00:11:42.400</a></span> | <span class="t">It said that it was plausible that Lebron James took a corner kick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=706">00:11:46.160</a></span> | <span class="t">But when asked for a chain of thought explanation it never mentioned that it spotted that pattern that the answer was always A.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=713">00:11:53.680</a></span> | <span class="t">It gave A for the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=714">00:11:54.400</a></span> | <span class="t">So it's a fake line of reasoning about why Lebron James could take a corner kick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=718">00:11:58.560</a></span> | <span class="t">Now of course I might well be wrong here and I'd love for someone to explain in detail why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=723">00:12:03.200</a></span> | <span class="t">But on the one hand I do want to acknowledge that this process does yield incredible results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=728">00:12:08.400</a></span> | <span class="t">But on the other hand we might be getting a story about which methodology most reassures humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=734">00:12:14.880</a></span> | <span class="t">Not an output that most faithfully represents the methodology actually used by GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=740">00:12:20.720</a></span> | <span class="t">Now for some people that might be good enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=742">00:12:22.560</a></span> | <span class="t">At least we can see some reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=744">00:12:24.320</a></span> | <span class="t">in the reasoning steps that we can understand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=746">00:12:26.240</a></span> | <span class="t">Especially in an area like mathematics where we have some ground truth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=749">00:12:29.920</a></span> | <span class="t">But it is interesting to me that they call the other approach outcome supervision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=753">00:12:33.840</a></span> | <span class="t">An approach that may reward an unaligned process and it being harder to scrutinize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=759">00:12:39.200</a></span> | <span class="t">But is it possible that the process reward model isn't just a more granular outcome reward model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=764">00:12:44.720</a></span> | <span class="t">Where the output is each step of the reasoning still pretty impossible to actually scrutinize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=770">00:12:50.480</a></span> | <span class="t">Well either way it seems we're pinning our hopes on this process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=774">00:12:54.240</a></span> | <span class="t">oriented learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=775">00:12:55.280</a></span> | <span class="t">This is from the website of Anthropic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=777">00:12:57.920</a></span> | <span class="t">They say we currently believe process oriented learning may be the most promising path to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=783">00:13:03.120</a></span> | <span class="t">training safe and transparent systems up to and somewhat beyond human level capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=788">00:13:08.960</a></span> | <span class="t">And let's end on this positive note from the head of alignment at OpenAI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=793">00:13:13.120</a></span> | <span class="t">He says this is positive evidence for the strategy of using process supervision to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=797">00:13:17.520</a></span> | <span class="t">train a model to do alignment research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=799">00:13:19.920</a></span> | <span class="t">At least in that case we would get a model whose work we can check more easily.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=804">00:13:24.160</a></span> | <span class="t">And that that model would be better at alignment research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=807">00:13:27.440</a></span> | <span class="t">I really hope so and I want to hear what you think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=810">00:13:30.560</a></span> | <span class="t">Thank you for watching all the way to the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hZTZYffRsKI&t=812">00:13:32.880</a></span> | <span class="t">Have a wonderful day.</span></div></div></body></html>