<html><head><title>Vision AI in 2025 — Peter Robicheaux, Roboflow</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Vision AI in 2025 — Peter Robicheaux, Roboflow</h2><a href="https://www.youtube.com/watch?v=IQc05eCvNYE"><img src="https://i.ytimg.com/vi_webp/IQc05eCvNYE/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./IQc05eCvNYE.html">Whisper Transcript</a> | <a href="./transcript_IQc05eCvNYE.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">I'm going to be giving a quick presentation about the State of the Union regarding AI Vision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=20" target="_blank">00:00:20.280</a></span> | <span class="t">So I'm Peter Robichaux. I'm the ML lead at RoboFlow, which is a platform for building and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=29" target="_blank">00:00:29.620</a></span> | <span class="t">deploying vision models. A lot of people are really interested in LLMs these days, so I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=37" target="_blank">00:00:37.400</a></span> | <span class="t">trying to pitch why computer vision matters. If you think about systems that interact with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=44" target="_blank">00:00:44.240</a></span> | <span class="t">the real world, they have to use vision as one of their primary inputs because the built</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=50" target="_blank">00:00:50.240</a></span> | <span class="t">world is sort of built around vision as a fundamental primitive. There's a big gap between where human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=59" target="_blank">00:00:59.240</a></span> | <span class="t">is and where computer vision is. I would argue a bigger gap than exists currently for human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=65" target="_blank">00:01:05.920</a></span> | <span class="t">speech and computer speech. Computer vision has its own set of problems that are very distinct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=75" target="_blank">00:01:15.940</a></span> | <span class="t">from the problems that need to be solved by LLMs. Latency usually matters. If you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=81" target="_blank">00:01:21.260</a></span> | <span class="t">perceive motion, you have to be running your process multiple frames per second. You usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=87" target="_blank">00:01:27.420</a></span> | <span class="t">want to run at the edge. You can't have one big hub where you do all of your computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=92" target="_blank">00:01:32.900</a></span> | <span class="t">because you would introduce too much latency to make decisions based off that computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=100" target="_blank">00:01:40.100</a></span> | <span class="t">So I sort of gave a version of this talk at Latent Spaces podcast at NeurIPS. And retrospectively,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=109" target="_blank">00:01:49.480</a></span> | <span class="t">I think we identified a few problems with the field of vision in 2024, one of them being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=116" target="_blank">00:01:56.280</a></span> | <span class="t">evals or saturated. So vision evals like ImageNet and Cocoa, they're mostly like pattern matching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=123" target="_blank">00:02:03.160</a></span> | <span class="t">They measure your ability to match patterns and sort of like don't require much visual intelligence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=128" target="_blank">00:02:08.960</a></span> | <span class="t">to solve. Consequently, I think, vision models don't leverage big pre-training the way that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=137" target="_blank">00:02:17.720</a></span> | <span class="t">language models do. So right now, you can take a language model and unleash it on the internet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=141" target="_blank">00:02:21.900</a></span> | <span class="t">and get something incredibly smart. Some of the best vision models are moving in that direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=147" target="_blank">00:02:27.860</a></span> | <span class="t">But because you don't need that level of knowledge and intelligence to solve the evals, there's kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=154" target="_blank">00:02:34.360</a></span> | <span class="t">of no incentive to do so. And I think that part of that -- so there's sort of two dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=161" target="_blank">00:02:41.440</a></span> | <span class="t">here. One is that vision doesn't leverage big pre-training. So you can think of like if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=166" target="_blank">00:02:46.840</a></span> | <span class="t">building an application with language right now, you probably want to use the smartest model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=170" target="_blank">00:02:50.440</a></span> | <span class="t">to get an embedding that works really well for you. And right now, we don't have like their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=175" target="_blank">00:02:55.000</a></span> | <span class="t">downstream applications that make really good use of the pre-training and the embeddings that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=179" target="_blank">00:02:59.560</a></span> | <span class="t">they get from large language models. But there aren't really good vision models that can leverage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=185" target="_blank">00:03:05.720</a></span> | <span class="t">these embeddings. And the corollary to this is that the quality of big pre-trained models just isn't the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=193" target="_blank">00:03:13.960</a></span> | <span class="t">same in vision as it is in language. And so my underlying conclusion is vision models aren't smart. That's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=200" target="_blank">00:03:20.640</a></span> | <span class="t">the takeaway. And I can prove it to you. So last year when Cloud 3.5 was happening, you can give it an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=207" target="_blank">00:03:27.360</a></span> | <span class="t">image of a watch and it just guesses -- you ask it what time it is and it'll just guess a random time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=212" target="_blank">00:03:32.480</a></span> | <span class="t">And that's because this model, it has a good conceptual abstract idea of what a clock is or what a watch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=218" target="_blank">00:03:38.480</a></span> | <span class="t">is, but it only comes to actually identifying the location of watch hands and finding the numbers on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=223" target="_blank">00:03:43.280</a></span> | <span class="t">watch. It's hopeless. And updated for Cloud 4 still has no idea what time it is. And this is even like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=230" target="_blank">00:03:50.880</a></span> | <span class="t">a an egregious failure because 10:10 is like the stock time on like all watches. So the fact that it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=237" target="_blank">00:03:57.040</a></span> | <span class="t">couldn't even get like the most common time is pretty telling. There's so there's this really cool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=245" target="_blank">00:04:05.440</a></span> | <span class="t">data set that's trying to measure this inability of LLMs to see called MMVP, which basically you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=253" target="_blank">00:04:13.840</a></span> | <span class="t">see an example here where they ask this question that seems incredibly obvious. And the model -- so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=259" target="_blank">00:04:19.920</a></span> | <span class="t">in this case, they're asked the model, which is like ChatGPT, 4.0, which direction the school bus is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=266" target="_blank">00:04:26.960</a></span> | <span class="t">facing. Are we seeing the front or the back of the school bus? And the model gets it completely wrong</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=271" target="_blank">00:04:31.840</a></span> | <span class="t">and then hallucinates details to support its claim. And again, I think this is evidence that large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=276" target="_blank">00:04:36.880</a></span> | <span class="t">language models, which are maybe the most intelligent models that we have, like cannot see. And that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=283" target="_blank">00:04:43.280</a></span> | <span class="t">due to a lack of visual features that they can perceive with. And so the way that this dataset was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=288" target="_blank">00:04:48.720</a></span> | <span class="t">created is they went and they found pairs of images that were close in CLIP space but far in DinoV2 space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=295" target="_blank">00:04:55.840</a></span> | <span class="t">So CLIP is a vision language model that was sort of contrastedly trained on the whole internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=301" target="_blank">00:05:01.200</a></span> | <span class="t">And so what this is showing is that CLIP is not discriminative enough to tell these two images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=313" target="_blank">00:05:13.440</a></span> | <span class="t">apart, right? So according to CLIP, these two images basically look the same. And what that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=317" target="_blank">00:05:17.920</a></span> | <span class="t">pointing to is like a failure in vision language pre-training. And so the way CLIP is trained is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=325" target="_blank">00:05:25.360</a></span> | <span class="t">basically you come up with a big data set of captioned images and you ask the model to -- you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=332" target="_blank">00:05:32.400</a></span> | <span class="t">scramble the captions and scramble the images and ask the model to pair the image with the caption.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=336" target="_blank">00:05:36.800</a></span> | <span class="t">But the thing is if you go back and look at these two images, what is a caption that would distinguish</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=342" target="_blank">00:05:42.240</a></span> | <span class="t">these two images, right? It's like the peculiar pose of the dog and one image it's facing the camera</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=348" target="_blank">00:05:48.640</a></span> | <span class="t">and one it's facing away. But these are sort of details that aren't included in the caption.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=352" target="_blank">00:05:52.320</a></span> | <span class="t">So if your loss function can't tell these two images apart, then why would your model be able to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=356" target="_blank">00:05:56.400</a></span> | <span class="t">right? So vision-only pre-training kind of works is the claim. So DinoV2 is this really cool model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=364" target="_blank">00:06:04.960</a></span> | <span class="t">So what you're seeing right now is a visualization of its PCA features that have been self-discovered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=370" target="_blank">00:06:10.960</a></span> | <span class="t">by pre-training on the whole internet. So what's really cool is not only does it find the mask of the dog,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=377" target="_blank">00:06:17.680</a></span> | <span class="t">obviously. That's sort of easy because it's highly contrasted with the green background,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=382" target="_blank">00:06:22.240</a></span> | <span class="t">but it also finds the segments of the dog and it finds even analogous segments. So if you look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=388" target="_blank">00:06:28.800</a></span> | <span class="t">these principal components, you compare the legs of a dog, it'll be in the same sort of feature space as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=393" target="_blank">00:06:33.760</a></span> | <span class="t">the legs of a human. And so there's sort of this big open question which is like how do we get vision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=401" target="_blank">00:06:41.920</a></span> | <span class="t">features that are well aligned with language features and usable by VLMs that don't suck and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=408" target="_blank">00:06:48.880</a></span> | <span class="t">like have visual fidelity? Cool. So that's part of the story. The other part of the question that needs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=418" target="_blank">00:06:58.880</a></span> | <span class="t">to be answered is given that we have some sort of semi-working large pre-training of vision models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=425" target="_blank">00:07:05.520</a></span> | <span class="t">why aren't we leveraging these vision models? And I would answer that at least in the object detection space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=430" target="_blank">00:07:10.800</a></span> | <span class="t">the answer is mostly in the distinction between convolutional models and transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=436" target="_blank">00:07:16.320</a></span> | <span class="t">So this is from LW Detter, which is one of the top performing detection transformers that currently exists.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=445" target="_blank">00:07:25.200</a></span> | <span class="t">If you look at this graph, you look at Yellow V8N, which is a convolutional object detector on the edge,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=451" target="_blank">00:07:31.760</a></span> | <span class="t">with and without pre-training on Object 365, it gains like 0.2 map, which is like the main accuracy metric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=458" target="_blank">00:07:38.000</a></span> | <span class="t">for object detectors. So Object 365, which is a big million, 1.6 million image data set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=464" target="_blank">00:07:44.160</a></span> | <span class="t">pre-training on it leads almost no performance improvements on Cocoa. Whereas for LW Detter, which is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=472" target="_blank">00:07:52.880</a></span> | <span class="t">transformer-based model, you can see that without -- if you look at this column map without pre-training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=478" target="_blank">00:07:58.400</a></span> | <span class="t">and you look at the column map with pre-training, you can see that you're getting like five map</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=482" target="_blank">00:08:02.240</a></span> | <span class="t">improvements across the board, sometimes even seven map improvements, which is like a gigantic amount,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=487" target="_blank">00:08:07.040</a></span> | <span class="t">right? And so basically, while the language world knows that transformers are able to leverage big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=494" target="_blank">00:08:14.640</a></span> | <span class="t">pre-trainings and yield decent results, the visual world is sort of just now catching up. And you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=502" target="_blank">00:08:22.000</a></span> | <span class="t">this from the scale of the big pre-training. In the image world, pre-training on Object 365 with 1.6</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=508" target="_blank">00:08:28.080</a></span> | <span class="t">million images is considered a large pre-training. That would be like a tiny challenge data set for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=514" target="_blank">00:08:34.080</a></span> | <span class="t">like undergrads in the LLM world. So I want to announce RoboFlow's special new model called RF Detter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=524" target="_blank">00:08:44.480</a></span> | <span class="t">which leverages the Dyno V2 pre-trained backbone and uses it in a real-time object detection context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=532" target="_blank">00:08:52.800</a></span> | <span class="t">So this is sort of our answer to the hole that we see in the field of like, why aren't we leveraging big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=540" target="_blank">00:09:00.160</a></span> | <span class="t">pre-trainings for visual models? And so here's some of the metrics. You can see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=548" target="_blank">00:09:08.800</a></span> | <span class="t">basically what we did is we took the LW Detter backbone and we like kind of swapped it out with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=552" target="_blank">00:09:12.560</a></span> | <span class="t">the Dyno V2 backbone. And we get like a decent improvement on Cocoa. And we're still not Soda</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=559" target="_blank">00:09:19.760</a></span> | <span class="t">on Cocoa compared to Define, which is the current Soda. We're like second Soda. But I think what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=565" target="_blank">00:09:25.840</a></span> | <span class="t">really interesting is there's this other data set called RF100VL, which we created to measure the sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=572" target="_blank">00:09:32.880</a></span> | <span class="t">domain adaptability of this model. And you can see massive yields from using the Dyno V2 pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=579" target="_blank">00:09:39.360</a></span> | <span class="t">backbone, which basically is pointing to the fact that number one, Cocoa is too easily solvable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=584" target="_blank">00:09:44.560</a></span> | <span class="t">It basically has common classes like humans and like coffee cups and stuff like this. So it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=591" target="_blank">00:09:51.200</a></span> | <span class="t">a good measure of the intelligence of your model. More so the way that you optimize Cocoa is by like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=596" target="_blank">00:09:56.080</a></span> | <span class="t">really nailing the precise location of a bounding box or something, really having good iterative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=601" target="_blank">00:10:01.680</a></span> | <span class="t">refinement of your locations that you're guessing. Whereas we posit RF100VL, this new data set, is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=610" target="_blank">00:10:10.640</a></span> | <span class="t">better measure of the intelligence of a visual model. So we're introducing a new data set, RF100VL,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=618" target="_blank">00:10:18.240</a></span> | <span class="t">which is a collection of 100 different object detection data sets that were pulled from our open source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=623" target="_blank">00:10:23.920</a></span> | <span class="t">collection of data sets. We have something like, I don't know, it's something like 750,000 data sets or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=630" target="_blank">00:10:30.640</a></span> | <span class="t">whatever on RoboFlow universe. And we hand curated the 100 best, I guess, by some metrics. So like we sorted by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=639" target="_blank">00:10:39.600</a></span> | <span class="t">community engagement and we tried to find very difficult domains. So you'll notice, for instance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=646" target="_blank">00:10:46.480</a></span> | <span class="t">we have different camera poses that are common in Cocoa. So we have like aerial camera positioning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=653" target="_blank">00:10:53.920</a></span> | <span class="t">such thing, which requires your model to sort of understand different views of an object in order to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=659" target="_blank">00:10:59.840</a></span> | <span class="t">to do well. We have different visual imaging domains, like you can see, like microscopes and x-rays and all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=666" target="_blank">00:11:06.880</a></span> | <span class="t">this sort of things. So yeah, we think that this data set can measure the richness of features that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=675" target="_blank">00:11:15.280</a></span> | <span class="t">learned by object detectors in a much more comprehensive way than Cocoa. And here's the other fun thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=683" target="_blank">00:11:23.200</a></span> | <span class="t">about this is that it is a visual language model. So we are able to benchmark a bunch of different models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=688" target="_blank">00:11:28.720</a></span> | <span class="t">on RF100VL, being able to ask them things like using, contextualizing the class name in the context of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=696" target="_blank">00:11:36.240</a></span> | <span class="t">data set. Where is this action happening, for instance? So if you look at the top left, we have this class which is block,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=704" target="_blank">00:11:44.160</a></span> | <span class="t">which is representing an action, a volleyball block. But you have to be smart enough to contextualize this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=709" target="_blank">00:11:49.280</a></span> | <span class="t">like word embedding of block within the context of volleyball to be able to detect that. Same thing with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=714" target="_blank">00:11:54.320</a></span> | <span class="t">this thunderbolt type defect in this cable here. If you just ask a dumb visual language model to detect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=720" target="_blank">00:12:00.720</a></span> | <span class="t">thunderbolts in the image, it will find nothing. But if it contextualizes it in the context of a cable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=725" target="_blank">00:12:05.200</a></span> | <span class="t">defect, then it will be able to find more things. And it also increases the breadth of classes. So if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=731" target="_blank">00:12:11.680</a></span> | <span class="t">only look at Cocoa, you're basically asking your model, hey, can you find a dog? Can you find a cat? But like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=737" target="_blank">00:12:17.760</a></span> | <span class="t">can you find fibrosis? Now your model needs to have like a lot more information around the world,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=742" target="_blank">00:12:22.240</a></span> | <span class="t">about the world to solve that problem. Same thing with different imaging domains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=747" target="_blank">00:12:27.040</a></span> | <span class="t">So it is a vision language benchmark. So we also have visual descriptions and sort of instructions on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=756" target="_blank">00:12:36.800</a></span> | <span class="t">how to find the objects that are present in this image. And basically what we found is like you take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=763" target="_blank">00:12:43.600</a></span> | <span class="t">a Cocoa or you take a Yolo V8 model and you train it on like 10 examples per class. It does better than like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=770" target="_blank">00:12:50.320</a></span> | <span class="t">Quen V2 72 -- Quen 2.5 VL 72B, like state of the art gigantic vision language model. So the vision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=778" target="_blank">00:12:58.960</a></span> | <span class="t">language models are really good right now at generalizing out of distribution in the linguistic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=784" target="_blank">00:13:04.000</a></span> | <span class="t">domain, but absolutely hopeless when it comes to generalizing in the visual domain. And so we hope</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=789" target="_blank">00:13:09.520</a></span> | <span class="t">that this benchmark can sort of drive that part of the research and make sure that the visual parts of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=797" target="_blank">00:13:17.520</a></span> | <span class="t">VLMs don't get left behind. And yeah, basically by leveraging like stronger embeddings, a debtor model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=807" target="_blank">00:13:27.040</a></span> | <span class="t">does much, much better on RF 100VL than just leveraging embeddings learned on Object 265, which makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=813" target="_blank">00:13:33.280</a></span> | <span class="t">And that's my talk. Thank you. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=818" target="_blank">00:13:38.000</a></span> | <span class="t">Can you fine-tune it inside of the edge?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=821" target="_blank">00:13:41.920</a></span> | <span class="t">Fine-tune Quen on the edge?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=823" target="_blank">00:13:43.920</a></span> | <span class="t">Oh, yeah, yeah, yeah. It's like 20 million parameters at the small size. Yeah. Cool. Any other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=831" target="_blank">00:13:51.920</a></span> | <span class="t">This works. Yeah, it's publicly available. It's on -- maybe I can -- if you go to RF100VL.org,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=845" target="_blank">00:14:05.680</a></span> | <span class="t">you can find our archive paper as well as the code utilities to help download the data set. It's also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=852" target="_blank">00:14:12.320</a></span> | <span class="t">like on Huggy Face somewhere. Yeah. Yeah, so RoboFlow kind of has a pretty unique strategy when it comes to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=863" target="_blank">00:14:23.920</a></span> | <span class="t">our platform. So we make our platform freely available to all researchers, basically. And so we have like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=870" target="_blank">00:14:30.240</a></span> | <span class="t">a ton of people who use our platform to label medical data and biological data for their own papers and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=876" target="_blank">00:14:36.640</a></span> | <span class="t">their own research. And then our only ask is that they then contribute that data back to the community</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=882" target="_blank">00:14:42.480</a></span> | <span class="t">and make it open source. And so a lot of this data comes from like paper cited in nature and stuff like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=887" target="_blank">00:14:47.280</a></span> | <span class="t">Yeah, so the data set is kind of measuring up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=899" target="_blank">00:14:59.920</a></span> | <span class="t">performance of like a bunch of different imaging modalities or predictive modalities, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=910" target="_blank">00:15:10.560</a></span> | <span class="t">So I think the most interesting track of the data set is the few shot track.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=928" target="_blank">00:15:28.560</a></span> | <span class="t">So basically we've constructed like canonical 10-shot splits. So we provide the model the class name,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=938" target="_blank">00:15:38.560</a></span> | <span class="t">annotator instructions on how to find that class, as well as 10 visual examples per class. And if a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=945" target="_blank">00:15:45.360</a></span> | <span class="t">model -- basically no model exists that can leverage those three things and get higher maps than if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=952" target="_blank">00:15:52.000</a></span> | <span class="t">just deleted one of those like options. I see that as one of the big shortcomings of visual language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=957" target="_blank">00:15:57.760</a></span> | <span class="t">Yeah, so currently the specialists are by far the best.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=987" target="_blank">00:16:27.280</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=987" target="_blank">00:16:27.840</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=987" target="_blank">00:16:27.920</a></span> | <span class="t">We benchmarked Grounding Dyno specifically, both zero-shot and fine-tuned. So zero-shot Grounding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=992" target="_blank">00:16:32.720</a></span> | <span class="t">Dyno got like 19 map average on R100VL, which is like kind of good, kind of bad. So if you take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=998" target="_blank">00:16:38.400</a></span> | <span class="t">like a YOLO V8 nano and you train it from scratch on the 10-shot examples, which is not a lot of data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=1003" target="_blank">00:16:43.840</a></span> | <span class="t">obviously, it gets something like 25 map. So like to be worse than fine-tuning a YOLO from scratch is sort of bad. But if you then fine-tune the Grounding Dyno with federated loss, that's the highest performing model we have on the data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=1016" target="_blank">00:16:56.240</a></span> | <span class="t">However, that being said, like I think that the point of the data set should be, hey, like you should be able to leverage these annotator instructions, the 10-shot examples, and the class names, and come up with something more accurate, which requires a journalist model. But okay, I think I'm super over time. So yeah, thanks for the questions. Cool. Thanks, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=1036" target="_blank">00:17:16.800</a></span> | <span class="t">Cool. Thanks, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IQc05eCvNYE&t=1038" target="_blank">00:17:18.960</a></span> | <span class="t">We'll see you next time.</span></div></div></body></html>