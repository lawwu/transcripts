<html><head><title>Low Level Technicals of LLMs: Daniel Han</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Low Level Technicals of LLMs: Daniel Han</h2><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc"><img src="https://i.ytimg.com/vi_webp/pRM_P6UfdIc/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./pRM_P6UfdIc.html">Whisper Transcript</a> | <a href="./transcript_pRM_P6UfdIc.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome to the AI Engineers World Fair. This is the first workshop. There's a few other running,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=19" target="_blank">00:00:19.200</a></span> | <span class="t">but thanks for coming. We just arrived from Australia with my brother. I think he's over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=25" target="_blank">00:00:25.200</a></span> | <span class="t">there somewhere, but yes, we just came here. Yes, we didn't know a lot of stuff about SF,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=30" target="_blank">00:00:30.720</a></span> | <span class="t">and I think maybe the US is a bit different from Australia. But yeah, we're very excited to be here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=35" target="_blank">00:00:35.680</a></span> | <span class="t">So we're going to stay here for a few months. So if you want to meet up, you can just hit me off</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=40" target="_blank">00:00:40.160</a></span> | <span class="t">email or Twitter or wherever. So today I'm going to be talking about low-level technicals of language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=44" target="_blank">00:00:44.960</a></span> | <span class="t">models. Yes, yes, I'm Daniel. So we do have a website called Unsloth.ai, if you want to look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=52" target="_blank">00:00:52.560</a></span> | <span class="t">that up, there's like cute sloths and stuff. So my brother designed that. We'll be using two tiny</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=58" target="_blank">00:00:58.720</a></span> | <span class="t">URLs. Oh, did it? Okay, it's still working. Yeah, so we'll be using two tiny URLs now. So the first one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=64" target="_blank">00:01:04.640</a></span> | <span class="t">is -- oh, wait, I'll shoot. Yeah. So the slides are at tinyurl.com/unsloth. Hopefully that works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=70" target="_blank">00:01:10.800</a></span> | <span class="t">And there's also Q&A. So I'll be monitoring Q&A. You can type any question that you like. And I will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=76" target="_blank">00:01:16.880</a></span> | <span class="t">answering questions as we go. And that is at tinyurl.com/unslothqa. So if those two work -- so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=85" target="_blank">00:01:25.120</a></span> | <span class="t">they'll be like on the bottom if, you know, anyone doesn't get this, like this on the very bottom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=89" target="_blank">00:01:29.280</a></span> | <span class="t">of the footer, they'll be like -- we'll re-show these links. Okay. Just -- it doesn't work? Yes? Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=99" target="_blank">00:01:39.040</a></span> | <span class="t">Okay. Good. Good. Good. Okay. Yes. So you might know me from my tweets. So Gemma like kind of like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=108" target="_blank">00:01:48.080</a></span> | <span class="t">released an open source model a few months ago. And we just found a few like issues and bugs for like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=113" target="_blank">00:01:53.680</a></span> | <span class="t">different implementations. So like, for example, the first tweet that we ever did was about some sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=120" target="_blank">00:02:00.560</a></span> | <span class="t">like the approximate Jellu bug issue. And so like multiple implementations of Gemma that had different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=127" target="_blank">00:02:07.600</a></span> | <span class="t">implementations. Some of them use exact Jellu. Some of them use approximate. And like, so which one is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=132" target="_blank">00:02:12.400</a></span> | <span class="t">correct? And so that's the question. And so like, we just tweeted about this. And that was like our first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=137" target="_blank">00:02:17.920</a></span> | <span class="t">issue that we found. We thought this was just like one issue. But actually, there were like many issues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=142" target="_blank">00:02:22.640</a></span> | <span class="t">And so like we found more bugs. And so I'm assuming maybe you know me from this. We did get partially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=150" target="_blank">00:02:30.240</a></span> | <span class="t">recognizable for our Gemma bug fixes. So yeah. So today we'll be showing you how you can actually find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=157" target="_blank">00:02:37.520</a></span> | <span class="t">these bugs and issues in language models. And how you can actually like analyze and do this yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=163" target="_blank">00:02:43.360</a></span> | <span class="t">Without, you know, us just doing it manually ourselves. And hopefully this can be like an open</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=168" target="_blank">00:02:48.400</a></span> | <span class="t">source project where everyone can find these issues automatically and help us to solve these issues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=173" target="_blank">00:02:53.360</a></span> | <span class="t">I always thought about like, can we like automate this? I don't think so. It can be automated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=177" target="_blank">00:02:57.200</a></span> | <span class="t">There are actually many issues of these implementations. And it's not just Gemma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=181" target="_blank">00:03:01.680</a></span> | <span class="t">For example, like, you know, we also analyzed Grok. And there's like some weird things in their code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=188" target="_blank">00:03:08.640</a></span> | <span class="t">Like they're scaled by 30 times 10 h x over 30. It's just a it's just a clamping mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=195" target="_blank">00:03:15.600</a></span> | <span class="t">You can see I also make mistakes sometimes. I like, you know, said it's division and not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=200" target="_blank">00:03:20.800</a></span> | <span class="t">multiplication. So sometimes I misread the code. That's because like, you know, I spend when the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=205" target="_blank">00:03:25.280</a></span> | <span class="t">code gets released, I quickly try to analyze them. And sometimes I mistakenly say stuff. So I have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=210" target="_blank">00:03:30.640</a></span> | <span class="t">like lodge, you know, showcase corrections. So yes, I'm still human. But yes, like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=215" target="_blank">00:03:35.600</a></span> | <span class="t">we analyze lots of models and stuff like this. And hopefully by the end of these workshops, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=221" target="_blank">00:03:41.520</a></span> | <span class="t">actually, this one, okay, it's actually not one workshop, it's going to be like multiple things in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=225" target="_blank">00:03:45.120</a></span> | <span class="t">one. So like, I decided to like, talk about three things. But I'll tell you about that later. Hopefully,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=229" target="_blank">00:03:49.680</a></span> | <span class="t">you guys can like, analyze and learn about how to do, like, find bugs and stuff like that by the end of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=235" target="_blank">00:03:55.360</a></span> | <span class="t">today. So another one I did recently was like, you know, Nvidia's Nemetron. I don't know if you saw this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=242" target="_blank">00:04:02.080</a></span> | <span class="t">but Nvidia released a 340 billion parameter model, which is extremely large. I'm assuming this is in,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=248" target="_blank">00:04:08.640</a></span> | <span class="t">like, you know, in preparation, like they have to release this earlier before Lama 405 billion,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=254" target="_blank">00:04:14.480</a></span> | <span class="t">right? So they have to do this quickly. And but there are like some weird, interesting things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=258" target="_blank">00:04:18.880</a></span> | <span class="t">like, you know, they use the squared value, and not the normal sweet glue and the other types. So that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=264" target="_blank">00:04:24.320</a></span> | <span class="t">was very interesting. They were actually the first, well, actually, not the first, but like the first big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=268" target="_blank">00:04:28.240</a></span> | <span class="t">model trained to be using these, like other activation functions. And there's other like other weird</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=273" target="_blank">00:04:33.920</a></span> | <span class="t">quirks and stuff like that. And hopefully, you'll be also able to like, analyze, like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=279" target="_blank">00:04:39.120</a></span> | <span class="t">whenever the code comes out, just read it, and you'll get it. Um, it does take some practice. Okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=283" target="_blank">00:04:43.280</a></span> | <span class="t">like, I take like, okay, the first time when I read this code, like, it took me like many, many days</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=287" target="_blank">00:04:47.440</a></span> | <span class="t">to read for like, all these architectures and understand exactly what they are. But now it takes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=293" target="_blank">00:04:53.600</a></span> | <span class="t">like 10 minutes. So like, I'm sure you're getting like, just the code comes and just read it. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=298" target="_blank">00:04:58.000</a></span> | <span class="t">that's the whole goal today. And also language models, if you don't know, they're not just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=303" target="_blank">00:05:03.680</a></span> | <span class="t">issues and bugs and analysis of these architectures, the tokenizer is a totally separate beast from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=308" target="_blank">00:05:08.960</a></span> | <span class="t">language models. My tokenization is like extremely annoying. Um, so like, I also tweeted about like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=316" target="_blank">00:05:16.240</a></span> | <span class="t">you know, there's like different types of tokenization issues, like Mistral, Lama,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=320" target="_blank">00:05:20.560</a></span> | <span class="t">like all these different types of like variants of Mistral from the Mistral team, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=324" target="_blank">00:05:24.960</a></span> | <span class="t">they have like different tokenizations if you didn't notice. Um, like if you, so the smiley face,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=329" target="_blank">00:05:29.600</a></span> | <span class="t">the sun's smiley face is a space. And if you actually tokenize them, um, depending on the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=336" target="_blank">00:05:36.400</a></span> | <span class="t">you'll have different results. And the question is, which one is correct? Um, unfortunately,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=341" target="_blank">00:05:41.600</a></span> | <span class="t">I do not know. Um, I did ask the HockeyFace team for this. Um, and according to them, some of them are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=346" target="_blank">00:05:46.880</a></span> | <span class="t">correct. And some of them are just because the Mistral team forgot to update the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=350" target="_blank">00:05:50.400</a></span> | <span class="t">model to the fast tokenization variant. Um, we will be talking about this later as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=354" target="_blank">00:05:54.640</a></span> | <span class="t">Um, but you can see even before you can train or run the model, the tokenizer is broken.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=360" target="_blank">00:06:00.720</a></span> | <span class="t">So what are you going to do? Um, it's a multi-pronged problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=364" target="_blank">00:06:04.480</a></span> | <span class="t">So we don't just do language models. Um, you know, like we don't, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=370" target="_blank">00:06:10.320</a></span> | <span class="t">experience is, you know, a bit broader than that. So we actually like, I used to actually major in maths</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=376" target="_blank">00:06:16.720</a></span> | <span class="t">and computer science. So, um, yes, very fun. Well, actually I kind of did very badly in maths,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=381" target="_blank">00:06:21.920</a></span> | <span class="t">but anyways, um, yes, very, very fun. So like SVD, I don't know if anyone, has anyone done normal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=387" target="_blank">00:06:27.040</a></span> | <span class="t">machine learning here? Oh yes. Very good. There is a few people. Um, so like the SVD, I'm assuming like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=394" target="_blank">00:06:34.800</a></span> | <span class="t">most people know PCA. Yes. Principle component analysis. Yes. Okay. Very good. Data visualization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=400" target="_blank">00:06:40.000</a></span> | <span class="t">It's a very powerful technique. More people should know about it. Um, SVD. Okay. I don't,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=404" target="_blank">00:06:44.560</a></span> | <span class="t">I don't know if people know about SVD or like, okay. Yes. It's a bit less well known. I'm actually a bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=409" target="_blank">00:06:49.680</a></span> | <span class="t">confused why people don't know SVD. It's actually the algorithm that's one of the most important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=414" target="_blank">00:06:54.000</a></span> | <span class="t">algorithms in all of like maths and computer science. Um, it literally underpins like many applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=420" target="_blank">00:07:00.240</a></span> | <span class="t">Um, and it is extremely important. Um, maybe we'll talk about that, but yes, I'm a huge proponent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=426" target="_blank">00:07:06.720</a></span> | <span class="t">of like telling people to learn more about SVD. So please do the singular value decomposition. That's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=431" target="_blank">00:07:11.680</a></span> | <span class="t">like the must, must, must, must, must. Okay. Like that's the most important algorithm. It's because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=435" target="_blank">00:07:15.760</a></span> | <span class="t">it's like one algorithm. It can like spawn many other algorithms and it's like, can be used for many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=440" target="_blank">00:07:20.480</a></span> | <span class="t">purposes. Um, there's also like the QR decomposition. Okay. Okay. Okay. No one knows. The LU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=446" target="_blank">00:07:26.080</a></span> | <span class="t">Choletsky, like there's a lot, um, randomized SVD. Yes. That's extremely important as well. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=451" target="_blank">00:07:31.840</a></span> | <span class="t">and yeah, so we don't just do language models. You can ask me any questions about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=456" target="_blank">00:07:36.720</a></span> | <span class="t">you know, stuff, um, maps or computer science. You, you have a question or real quick. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=462" target="_blank">00:07:42.800</a></span> | <span class="t">so do you think, um, so for the Nemo Tron, uh, 340 B, um, is it a unique architecture for,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=471" target="_blank">00:07:51.280</a></span> | <span class="t">because you can only use Nemo loader right now to load and train, you know, do the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=477" target="_blank">00:07:57.040</a></span> | <span class="t">I think the data is just the most valuable part, but we are attempting to try to convert it to a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=481" target="_blank">00:08:01.600</a></span> | <span class="t">hugging face, a transformer safe tensors. So, but we've had issues because we don't have the modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=487" target="_blank">00:08:07.200</a></span> | <span class="t">file. So, um, I was wondering, do you think that it's similar to, so at the same day, they uploaded a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=494" target="_blank">00:08:14.000</a></span> | <span class="t">7db of llama 3 that's Nemo Tron and 12. Do you think that we can get some clues of how to build a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=501" target="_blank">00:08:21.840</a></span> | <span class="t">hugging face implementation? Yes. So the question was, for Nemo Tron, the code was not released</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=508" target="_blank">00:08:28.880</a></span> | <span class="t">for the actual inference and bot training. And you have to go through the Nemo training framework from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=513" target="_blank">00:08:33.200</a></span> | <span class="t">MIM video. Um, yes, but the code. Yeah. I was actually planning on doing something like that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=522" target="_blank">00:08:42.240</a></span> | <span class="t">but as you know, we didn't have time to do that. So I probably, yeah, I might take a crack at that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=528" target="_blank">00:08:48.080</a></span> | <span class="t">And also, is there going to be a Q and A? I don't want to ask any more questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=530" target="_blank">00:08:50.720</a></span> | <span class="t">Oh, yes. No, no, there is. So you can log questions on Q and A. There is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=533" target="_blank">00:08:53.680</a></span> | <span class="t">there will be Q and A. So we, no, no. Yeah. No, you cannot. Anyone can like raise their hands and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=538" target="_blank">00:08:58.720</a></span> | <span class="t">like ask a question. Like I don't, I'll just repeat the question. Um, and, but there is like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=542" target="_blank">00:09:02.640</a></span> | <span class="t">slider. If you want to random questions, I will keep monitoring. Um, yeah. And yeah, so like, oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=549" target="_blank">00:09:09.600</a></span> | <span class="t">yes. The other one was like another paper called Laura learns less and forgets less. It shows that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=553" target="_blank">00:09:13.520</a></span> | <span class="t">you know, fine tuning by Laura does not really work for learning new knowledge. And, um, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=559" target="_blank">00:09:19.280</a></span> | <span class="t">it depends. Like it depends on how you actually read the paper. Like some components were incorrect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=564" target="_blank">00:09:24.800</a></span> | <span class="t">They didn't actually train on all linear layers. They've kind of forgot a few. And they also,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=568" target="_blank">00:09:28.400</a></span> | <span class="t">you need to do some sort of like, like, you know, special parameters to make this work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=572" target="_blank">00:09:32.560</a></span> | <span class="t">And we will also be talking about that as well. But I was just trying to show you that, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=575" target="_blank">00:09:35.760</a></span> | <span class="t">we don't just do language models. So we have like a whole wealth of knowledge across different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=580" target="_blank">00:09:40.000</a></span> | <span class="t">industries and stuff. Well, not industries, topics. Um, and you can ask me any question that you like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=584" target="_blank">00:09:44.560</a></span> | <span class="t">Um, so unsloft. Yes. Um, we launched just last December. So I launched this with my brother. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=590" target="_blank">00:09:50.800</a></span> | <span class="t">it's a bit outdated. But anyways, um, I think we have 11.9 something K or something. I don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=596" target="_blank">00:09:56.080</a></span> | <span class="t">even know now. But anyways, um, we launched this last December. It generally makes fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=600" target="_blank">00:10:00.320</a></span> | <span class="t">tuning of, um, language models like Lama, Mr. Gemma faster to two times faster, um, generally speaking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=607" target="_blank">00:10:07.040</a></span> | <span class="t">And we have like 70% less memory usage. Now we have like some new methodologies which reduce memory even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=612" target="_blank">00:10:12.000</a></span> | <span class="t">further. And the trick is there is no degradation and accuracy. So like, you know, we don't do any,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=616" target="_blank">00:10:16.400</a></span> | <span class="t">like, approximations. That's the whole purpose of the optimizations is we don't want to lose any accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=621" target="_blank">00:10:21.680</a></span> | <span class="t">Um, and so we do like Triton kernels. So this is like from OpenAI. Um, it's a language to do like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=629" target="_blank">00:10:29.360</a></span> | <span class="t">Cuda programming. Um, essentially it's like an intermediary between the Cuda code and Python itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=636" target="_blank">00:10:36.560</a></span> | <span class="t">And we'll be showing some Triton code. Um, I don't know if we have time for like programming Triton,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=641" target="_blank">00:10:41.520</a></span> | <span class="t">but that'll be another topic. Um, and yeah, so like the whole purpose of unsloft is to make everyone be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=648" target="_blank">00:10:48.400</a></span> | <span class="t">able to fine tune the language models with very bad GPUs, right? So like Tesla T4s, the free Google.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=654" target="_blank">00:10:54.160</a></span> | <span class="t">Does anyone like people do know that Google collab has free Tesla T4s, right? Yes. Yes. Right. 65</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=660" target="_blank">00:11:00.160</a></span> | <span class="t">teraflops, right? It's actually not that bad. Um, if you use it properly, um, just a reminder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=665" target="_blank">00:11:05.600</a></span> | <span class="t">there is a common misconception that the P100s on Kaggle is faster. That's actually not correct. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=670" target="_blank">00:11:10.400</a></span> | <span class="t">P100s I think are five times slower than Tesla T4s. Um, so although it's actually more expensive as a GPU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=676" target="_blank">00:11:16.480</a></span> | <span class="t">I think, I think, um, but it's actually slower for, um, so please do not select the P100s on Kaggle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=683" target="_blank">00:11:23.920</a></span> | <span class="t">Right? So Kaggle has 30 hours for free per week GPUs. Um, and you get two Tesla T4s. So that's 130</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=691" target="_blank">00:11:31.040</a></span> | <span class="t">teraflops per week, um, 30 hours. And that is actually very powerful. I think that's the same as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=696" target="_blank">00:11:36.080</a></span> | <span class="t">RTX 3070. Although I can't, yeah, I can't, I can't remember exactly. But, um, yeah, so that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=702" target="_blank">00:11:42.480</a></span> | <span class="t">so Kaggle has 30 hours for free per week. Google collab, it depends on how much you use. Normally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=707" target="_blank">00:11:47.920</a></span> | <span class="t">you get four hours per day, I think. Um, I guess the pro is not that bad. It's like $10 per month. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=713" target="_blank">00:11:53.440</a></span> | <span class="t">can actually get, like, yeah, it's pretty good. Um, yeah, so probably get pro. Um, I do not suggest,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=719" target="_blank">00:11:59.680</a></span> | <span class="t">I mean, you could use, like, Runprod and Lambda Labs and stuff like that. I guess that's another option. But we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=724" target="_blank">00:12:04.480</a></span> | <span class="t">actually share a pricing comparison. So what you need to, like, be careful when you use GPUs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=729" target="_blank">00:12:09.440</a></span> | <span class="t">there is a big issue is, like, oh, look, this is the most I want to use a H100. Um, did you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=735" target="_blank">00:12:15.440</a></span> | <span class="t">check how much flops the H100 provides? Um, be careful of NVIDIA's marketing. It's times two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=741" target="_blank">00:12:21.280</a></span> | <span class="t">because it has sparsity. Um, so just be careful of that. And also, you have to be careful of the flops</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=746" target="_blank">00:12:26.560</a></span> | <span class="t">when it's, like, float eight or float 16. So just be careful of those. Um, I do have a pricing comparison</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=751" target="_blank">00:12:31.760</a></span> | <span class="t">where we, like, normalize by, like, the flops, um, with no sparsity. And we'd, like, look through,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=756" target="_blank">00:12:36.800</a></span> | <span class="t">like, Lambda Labs, Runpod, Google Colab, AWS, um, Google Cloud. And I think Runpod is mostly pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=764" target="_blank">00:12:44.400</a></span> | <span class="t">good. Oh, yes. Question. Oh, the sparsity? So, okay, so the question was, why is times two for flops for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=773" target="_blank">00:12:53.600</a></span> | <span class="t">sparsity feature on the NVIDIA GPUs? So sparsity, the sparsity feature, what it does is you take 50%</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=779" target="_blank">00:12:59.680</a></span> | <span class="t">of the weights and make them go to zero. And NVIDIA essentially allows you to train this two times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=785" target="_blank">00:13:05.040</a></span> | <span class="t">faster by not doing matrix multiplications on the zeros, right? So you're, like, two times zero is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=790" target="_blank">00:13:10.480</a></span> | <span class="t">zero. So, like, you essentially don't fire the transistors. And essentially, this makes it two times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=794" target="_blank">00:13:14.320</a></span> | <span class="t">faster. Um, well, that's actually not, that's just a higher level overview. But, like, essentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=798" target="_blank">00:13:18.640</a></span> | <span class="t">you, like, compress the matrix into this special format. And then this NVIDIA special format allows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=803" target="_blank">00:13:23.440</a></span> | <span class="t">you to do matrix multiplications two times faster. Um, yeah, so it's sparsity. So it's on H100s,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=811" target="_blank">00:13:31.280</a></span> | <span class="t">it's on, it's on A100s as well. So your RTX 30, 60, your RTX 30 series has that feature from RTX 30,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=818" target="_blank">00:13:38.720</a></span> | <span class="t">the 30 series. I think it has, yes, I think it does. Um, so if you want to enable it, the biggest issue is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=827" target="_blank">00:13:47.360</a></span> | <span class="t">is that the language models, most companies do not train their language models with sparsity enabled.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=832" target="_blank">00:13:52.960</a></span> | <span class="t">If you set this, like, weights go to zero, um, you will actually ruin the behavior of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=838" target="_blank">00:13:58.640</a></span> | <span class="t">So there are, like, papers which show that you can actually do, like, turn on the feature and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=843" target="_blank">00:14:03.440</a></span> | <span class="t">you can do fine tuning to make it work. So there are actually papers which do that. Um, so in theory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=849" target="_blank">00:14:09.040</a></span> | <span class="t">you could enable this. Um, but, you know, it depends on, like, what models are released from the large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=854" target="_blank">00:14:14.240</a></span> | <span class="t">companies. Um, yeah. I'm assuming, I think I do know if Facebook is, like, they did implement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=859" target="_blank">00:14:19.920</a></span> | <span class="t">sparsity in the PyTorch and, um, Xformers library. So I'm assuming there might be focused on sparsity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=866" target="_blank">00:14:26.560</a></span> | <span class="t">because you get times two faster. Um, and if you know, like, open AI, you know, like, they keep saying,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=870" target="_blank">00:14:30.880</a></span> | <span class="t">oh, it's two times faster. Hmm. I wonder why. Why is it two times faster? Why is it the two times faster?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=876" target="_blank">00:14:36.240</a></span> | <span class="t">Right? So, like, could it be sparsity? Could it be float eight? Right? Float eight is generally two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=881" target="_blank">00:14:41.120</a></span> | <span class="t">times faster. Okay. Not exactly, but, like, approximately two times faster. So, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=884" target="_blank">00:14:44.560</a></span> | <span class="t">all these, like, things when you hear two times faster, where does it come from? Could it be these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=888" target="_blank">00:14:48.240</a></span> | <span class="t">things? Although we don't know, but, like, we're just guessing. Um, yeah. Any other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=894" target="_blank">00:14:54.080</a></span> | <span class="t">Okay. Just remember, you can raise your hand or you can do slide -- wait, are there any -- I'm assuming there's no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=899" target="_blank">00:14:59.760</a></span> | <span class="t">slider questions yet. Um, yeah. Okay. Yeah. Just raise your hand. Um, so we -- so for Unsloth,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=905" target="_blank">00:15:05.840</a></span> | <span class="t">we do benchmarking against HuggingFace plus Flash Attention 2. Um, and we just show our benchmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=910" target="_blank">00:15:10.880</a></span> | <span class="t">This is kind of old already. The memory reduction is much more now. Um, so -- and we do a blog -- we did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=917" target="_blank">00:15:17.120</a></span> | <span class="t">a blog post with them. So thanks to HuggingFace for the collaboration. Um, and essentially, all you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=921" target="_blank">00:15:21.840</a></span> | <span class="t">to do is, you know, from Unsloth import fast language model, and we try to make it as easy as possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=927" target="_blank">00:15:27.920</a></span> | <span class="t">for people to fine tune a language model. Um, and, yes, we'll be talking about Unsloth a bit later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=933" target="_blank">00:15:33.520</a></span> | <span class="t">Um, oh, there is a question. Is it a myth or solid hypothesis that linear versus cosine,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=939" target="_blank">00:15:39.040</a></span> | <span class="t">then short one to two epochs versus three to five epochs is the highly -- is -- oh, is highly generalized?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=944" target="_blank">00:15:44.720</a></span> | <span class="t">Um, I think it depends. Um, so, like, for training methodologies -- sorry. So, is it a myth or solid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=952" target="_blank">00:15:52.480</a></span> | <span class="t">hypothesis that linear versus cosine, then, of course, short epochs versus long epochs is the highly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=960" target="_blank">00:16:00.640</a></span> | <span class="t">generalized best way to train as any standard-based model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=963" target="_blank">00:16:03.600</a></span> | <span class="t">I think it depends. So, there are, like, some research papers which show that cosine or linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=969" target="_blank">00:16:09.120</a></span> | <span class="t">schedules -- I mean, it depends. To tell you the truth, I think it's a toss of a coin. I don't think so. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=974" target="_blank">00:16:14.080</a></span> | <span class="t">actually that important for the learning rate scheduler. I think it more depends on, like, the data set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=979" target="_blank">00:16:19.040</a></span> | <span class="t">the number of parameters. Um, so, like, research papers would show that if you just simply change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=984" target="_blank">00:16:24.160</a></span> | <span class="t">from tied weights -- from untied weights to tied weights, you can get, like, better accuracy for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=988" target="_blank">00:16:28.240</a></span> | <span class="t">smaller models. So, I think it's -- so, the learning rate schedule is not that important. You might get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=992" target="_blank">00:16:32.240</a></span> | <span class="t">accuracy plus 0.1%. Just train for more data. There we go. Right, get more data. Oh, wait, I pressed back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=998" target="_blank">00:16:38.240</a></span> | <span class="t">Just train for more data, and I'm assuming it will be similar. Um, but to tell you the truth, I think it's best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1002" target="_blank">00:16:42.720</a></span> | <span class="t">to, like, do small experiments and then, like, test which schedule is the best. But I don't think so. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1007" target="_blank">00:16:47.280</a></span> | <span class="t">that important. Um, I think for the number of epochs, that's actually important. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1012" target="_blank">00:16:52.480</a></span> | <span class="t">these, like, big companies -- I'm not sure what LAMA, like, 15 trillion tokens, is it actually 15</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1018" target="_blank">00:16:58.240</a></span> | <span class="t">trillion tokens? Or is it, like, 5 trillion tokens times 3 epochs? I do not know. Right? These questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1024" target="_blank">00:17:04.240</a></span> | <span class="t">are very important. If it's 5 trillion tokens times 3, that's actually very different from actually 15</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1028" target="_blank">00:17:08.800</a></span> | <span class="t">trillion tokens in total. But that's actually very, very different. But in general speaking, like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1032" target="_blank">00:17:12.640</a></span> | <span class="t">if you train for more epochs, 3 is generally the well, you know, good, like, approximate 3 epochs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1039" target="_blank">00:17:19.040</a></span> | <span class="t">One is actually the best for pre-training, generally. Um, you shouldn't, like, retrain your data, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1043" target="_blank">00:17:23.600</a></span> | <span class="t">multiple times. Um, but yeah. Um, did you have a follow-up question, or no?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1048" target="_blank">00:17:28.560</a></span> | <span class="t">Well, um, so basically, um, learning rate is one was one of the big issues to fix with the Gemma</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1057" target="_blank">00:17:37.440</a></span> | <span class="t">implementation. Oh, yes. So that's why I kind of -- that's where, um, my pitfall was when I was training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1064" target="_blank">00:17:44.160</a></span> | <span class="t">my 2D for, um, Gemma. And, um, so I actually trained it pre-YourFix, and somehow it turned out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1073" target="_blank">00:17:53.680</a></span> | <span class="t">the benchmark, and then after YourFix was better than -- I don't know what happened, but it's now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1080" target="_blank">00:18:00.640</a></span> | <span class="t">it's, like, one of the highest ranking benchmark in 2D. Oh, okay. So I don't know, like, what -- do you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1087" target="_blank">00:18:07.440</a></span> | <span class="t">have any theories about what could happen? I trained it on the transformers, the broken version.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1092" target="_blank">00:18:12.000</a></span> | <span class="t">Hmm. And then something about using octalado and using, uh, a, um, a very hard -- we grew forth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1099" target="_blank">00:18:19.920</a></span> | <span class="t">a learning rate. Hmm. But it turned out surprisingly well, and, um, we also used -- we didn't use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1107" target="_blank">00:18:27.040</a></span> | <span class="t">unslopped, but we used, like, uh, APU, so, um -- So this is after the fixes that we did? Like, it does --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1112" target="_blank">00:18:32.880</a></span> | <span class="t">Before. Before, so before does better? No, well, no. Before, it was -- it was -- it was --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1117" target="_blank">00:18:37.200</a></span> | <span class="t">It was usable. Everybody else's was unusual, right? Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1120" target="_blank">00:18:40.640</a></span> | <span class="t">But it was usable. Was it surprised me? Because everybody else's was unusual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1125" target="_blank">00:18:45.920</a></span> | <span class="t">But then, after your fixes, we are now the top -- well, my company has -- on the OpenL on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1131" target="_blank">00:18:51.760</a></span> | <span class="t">Lee Award has the highest -- So you didn't even retrain? You, like, you just --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1136" target="_blank">00:18:56.080</a></span> | <span class="t">No. So after your fixes, somehow -- It made it better? Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1139" target="_blank">00:18:59.520</a></span> | <span class="t">Do you have any theories on that? To be honest, I do not know. I think, like, because the fixes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1144" target="_blank">00:19:04.000</a></span> | <span class="t">that we did for Gemma are, like, multi-pronged. Like, it's not like one fix. It's, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1146" target="_blank">00:19:06.960</a></span> | <span class="t">nine or something. So I don't know which fix caused the change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1150" target="_blank">00:19:10.720</a></span> | <span class="t">It could be, like, all of them, maybe. I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1152" target="_blank">00:19:12.800</a></span> | <span class="t">It's kind of like, um, just for me, when I -- when I -- when I did the training, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1159" target="_blank">00:19:19.280</a></span> | <span class="t">and it turned out good, and then I heard from, like, all my friends,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1162" target="_blank">00:19:22.880</a></span> | <span class="t">"Oh, I can't do this. I can't do this." It's, like, um, it just shocked me, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1167" target="_blank">00:19:27.680</a></span> | <span class="t">Okay, yes, that is quite shocking. If, like, you didn't -- yeah, we change the code, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1172" target="_blank">00:19:32.160</a></span> | <span class="t">we kind of, like, fix all the issues, and then you don't need to retrain it, and it does better?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1176" target="_blank">00:19:36.720</a></span> | <span class="t">Okay, that's -- okay, that's a very interesting phenomenon. I do not know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1179" target="_blank">00:19:39.680</a></span> | <span class="t">I just sent you the code that -- Oh, yeah, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1181" target="_blank">00:19:41.440</a></span> | <span class="t">It's actually open source online on the PowerPoint.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1183" target="_blank">00:19:43.440</a></span> | <span class="t">Okay, yeah, great. Um, yes. To be honest, like, language models -- I mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1188" target="_blank">00:19:48.240</a></span> | <span class="t">these are all active areas of research. Please, someone will do a research on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1191" target="_blank">00:19:51.520</a></span> | <span class="t">Yeah. Okay, yeah. Yes. I cannot say anything other than I just read the code and fix the box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1200" target="_blank">00:20:00.240</a></span> | <span class="t">I do not know. So, yeah, yeah, yeah, don't worry. Um, okay. We also, like, do long context fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1208" target="_blank">00:20:08.080</a></span> | <span class="t">tuning. So, like, we show that if you use a new methodology which does gradient checkpointing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1213" target="_blank">00:20:13.040</a></span> | <span class="t">and offload it to system RAM, you can randomly increase your context size by four. And your -- and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1219" target="_blank">00:20:19.920</a></span> | <span class="t">the weird part is, if you offload correctly to system RAM from the GPU, weirdly, the time of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1226" target="_blank">00:20:26.400</a></span> | <span class="t">execution is just slower by one to two percent. Right? So, like, this is very weird. It's like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1232" target="_blank">00:20:32.480</a></span> | <span class="t">if you can do non-blocking cores and offload the GPU, like, memory into system RAM, if you do it correctly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1240" target="_blank">00:20:40.640</a></span> | <span class="t">it's not slower. Um, some implementations, unfortunately, offload incorrectly. Um, I don't know what to name</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1247" target="_blank">00:20:47.280</a></span> | <span class="t">anyone, but they offload incorrectly. Sometimes they offload to disk. I don't know who came up with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1251" target="_blank">00:20:51.840</a></span> | <span class="t">the idea of offloading to disk. But anyways, please try to offload to memory first and then disk. Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1257" target="_blank">00:20:57.680</a></span> | <span class="t">Disk is extremely slow. Um, and if you can offload to memory system RAM, you can actually get away with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1263" target="_blank">00:21:03.200</a></span> | <span class="t">a lot of memory usage. Um, okay. So, I should have put this at the first time. But anyways, so today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1269" target="_blank">00:21:09.600</a></span> | <span class="t">we'll be having three approximate topics. Um, and these -- I wanted to make them into like three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1275" target="_blank">00:21:15.440</a></span> | <span class="t">different separate topics, but I guess I just mix them together. Whatever. Um, so you'll be learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1280" target="_blank">00:21:20.320</a></span> | <span class="t">about low-level technicals of language models. For example, backpropagation. Um, why is -- why is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1286" target="_blank">00:21:26.640</a></span> | <span class="t">transformers not O of N squared -- sorry, O of N cubed for training and rather O of N squared. And there is a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1293" target="_blank">00:21:33.040</a></span> | <span class="t">maths. But I will try my best to reduce -- I think I already tried my best to reduce the maths, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1296" target="_blank">00:21:36.720</a></span> | <span class="t">there are still some maths. So, please handle the maths. Um, I will try my best to explain as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1301" target="_blank">00:21:41.920</a></span> | <span class="t">simply as possible. Um, that's the whole goal of the workshop. So, not that bad maths. You will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1307" target="_blank">00:21:47.840</a></span> | <span class="t">actually understand the formulas very well. Um, just a reminder, I kind of nearly failed my maths in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1312" target="_blank">00:21:52.320</a></span> | <span class="t">university. So, do not worry. Do not be scared. Um, it's very fine. Um, we were talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1318" target="_blank">00:21:58.400</a></span> | <span class="t">unslawed fast fine-tuning. The best tips and tricks for fine-tuning. How do we write the fast kernels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1323" target="_blank">00:22:03.440</a></span> | <span class="t">for fine-tuning? Um, you know, how do we actually make it two times faster? Use 70% less memory? Like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1328" target="_blank">00:22:08.480</a></span> | <span class="t">how? And with no accuracy degradation? Um, and we'll be talking, like, some, you know, Triton,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1333" target="_blank">00:22:13.840</a></span> | <span class="t">OpenAI's Triton language and stuff like that. Um, and we'll be doing finding and fixing bugs. Um, so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1339" target="_blank">00:22:19.040</a></span> | <span class="t">this would be a constant phenomenon and theme. How do we find and fix bugs in Lama, Mr. Gemma? Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1344" target="_blank">00:22:24.960</a></span> | <span class="t">we'll be talking about a mixture of experts as well. Oh, wait, maybe not. But it depends on time. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1349" target="_blank">00:22:29.120</a></span> | <span class="t">and we'll be doing lots of bug hunting, bug fixing, and more. And everyone here will be a fantastic bug</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1354" target="_blank">00:22:34.400</a></span> | <span class="t">hunter and bug fixer. And we can, like, essentially open source our effort to fix open source models to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1359" target="_blank">00:22:39.440</a></span> | <span class="t">everyone here. Um, oh, yes. And we also have stickers. Um, yes. I don't know where they are. But,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1366" target="_blank">00:22:46.240</a></span> | <span class="t">like, oh, yes. Yeah, my brother has some stickers. Um, and we bought a few of these stickers, which look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1371" target="_blank">00:22:51.600</a></span> | <span class="t">pretty cute. Right? So, like, you can wait. My laptop has some, right? I put them on my laptop. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1376" target="_blank">00:22:56.880</a></span> | <span class="t">and they're pretty cute. I really like them. Um, so my brother has them. We'll be handing them out. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1383" target="_blank">00:23:03.040</a></span> | <span class="t">yeah, as well at the end. Um, okay, so let us start. Um, so the transformer, right? So, like, what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1392" target="_blank">00:23:12.640</a></span> | <span class="t">transformer? I'm assuming everyone knows what the transformer is? Um, does anyone not know what the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1398" target="_blank">00:23:18.320</a></span> | <span class="t">transformer is? Yes or no? Like, you can simply... Okay. Yes. Okay. So the transformer is just an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1406" target="_blank">00:23:26.720</a></span> | <span class="t">architecture that is behind all language models. Um, so, like, GPT-4, GPT-3, you know, like, Lama,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1413" target="_blank">00:23:33.440</a></span> | <span class="t">Mistral, Gemma, all these open source models. What are they? Like, what's the architecture behind them? And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1419" target="_blank">00:23:39.600</a></span> | <span class="t">all of them rely on the, like, the transformer. And the transformer is essentially a architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1426" target="_blank">00:23:46.320</a></span> | <span class="t">which seems to be very good for sequence modeling. Um, so it's not just for languages. It can be for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1430" target="_blank">00:23:50.560</a></span> | <span class="t">any sequence modeling. Right? So, like, if you know, like, you know, Sora is a transformer. Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1434" target="_blank">00:23:54.880</a></span> | <span class="t">not just a transformer. It's probably plus diffusion, but, like, it's generally a transformer. Um, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1439" target="_blank">00:23:59.280</a></span> | <span class="t">there's other different types of models, which doesn't have to be language modeling. Okay? It's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1442" target="_blank">00:24:02.960</a></span> | <span class="t">sequence modeling. And I'll probably shoot some pictures later. Um, I probably should have explained it a bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1448" target="_blank">00:24:08.960</a></span> | <span class="t">better, but, like, just, just, just assume that transformers are the method behind all language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1455" target="_blank">00:24:15.040</a></span> | <span class="t">models. Okay. GPT-4, GPT-3, GPT-5. Okay. I don't know if, okay, who knows what GPT-5 is, but, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1460" target="_blank">00:24:20.960</a></span> | <span class="t">I'm assuming it's a transformer. Um, the transformers just seem to be very good at learning new knowledge,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1467" target="_blank">00:24:27.360</a></span> | <span class="t">injecting knowledge into the model. It seems to be very, very good at changing the weights to fit the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1472" target="_blank">00:24:32.400</a></span> | <span class="t">training data. Um, which is very interesting. Um, and the GPT-2 architecture was actually very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1479" target="_blank">00:24:39.280</a></span> | <span class="t">popular for the, like, you know, most decoder-style transformer. That was very, very popular. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1484" target="_blank">00:24:44.000</a></span> | <span class="t">still used to this day. Um, and it kind of got reincarnated by adding extra components to it. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1490" target="_blank">00:24:50.880</a></span> | <span class="t">this new architecture is called transformer plus plus. Um, I don't know if people have heard of this, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1496" target="_blank">00:24:56.240</a></span> | <span class="t">transformer plus plus is the GPT-2 transformer architecture, plus rope embeddings, plus Wiggaloo,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1502" target="_blank">00:25:02.400</a></span> | <span class="t">plus RMS layer norm, and with no bias. Um, and I think it's untied weights, although I'm not sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1507" target="_blank">00:25:07.520</a></span> | <span class="t">of GPT, I can't remember that exactly, but I think it's plus untied weights. Um, and transformer plus plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1511" target="_blank">00:25:11.840</a></span> | <span class="t">is the architecture which most people think is the best, you know, transformer architecture for now. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1519" target="_blank">00:25:19.520</a></span> | <span class="t">yes, for now. Um, there are probably, like, some other tweaks and little small things that transformer still can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1525" target="_blank">00:25:25.600</a></span> | <span class="t">do, but like, in general, this would be considered the best architecture. Um, and how does the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1530" target="_blank">00:25:30.800</a></span> | <span class="t">architecture look like? It is just a list of math equations. Um, so I just wrote down the entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1537" target="_blank">00:25:37.520</a></span> | <span class="t">transformer architecture. Um, well, this is Lama 2, right? So Lama's transformer architecture in one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1543" target="_blank">00:25:43.440</a></span> | <span class="t">slide. Um, and all you need to do is get some inputs, like some sort of like inputs, do some layer norm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1549" target="_blank">00:25:49.600</a></span> | <span class="t">do some rope embeddings, do some attention, plus some residual, do some layer norm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1554" target="_blank">00:25:54.960</a></span> | <span class="t">swigaloo, whatever, residual layer norm, and you get logits. Um, and you essentially repeat this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1560" target="_blank">00:26:00.480</a></span> | <span class="t">middle section L times or many times. Um, and that is the transform architecture. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1566" target="_blank">00:26:06.160</a></span> | <span class="t">okay, then maybe the math equations, I'm not sure. Does the math equation scare anyone? Or I'll be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1571" target="_blank">00:26:11.520</a></span> | <span class="t">explaining each one. Okay. So like, hopefully I try to make the math equations as like reasonable as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1577" target="_blank">00:26:17.680</a></span> | <span class="t">possible. Um, in theory, if you write this down on in PyTorch, um, oh yes, if you write this down in,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1584" target="_blank">00:26:24.800</a></span> | <span class="t">in PyTorch, you actually have a working implementation of, of a transformer architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1589" target="_blank">00:26:29.840</a></span> | <span class="t">Um, and yeah, so like, we'll be talking about each component separately as well. Um, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1596" target="_blank">00:26:36.960</a></span> | <span class="t">Is anyone scared for the math? No. Yes. No. No. Okay. Very good. Okay. Let me just check questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1602" target="_blank">00:26:42.240</a></span> | <span class="t">Does anyone have any questions? Okay. Okay. So did you have a question? Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1608" target="_blank">00:26:48.720</a></span> | <span class="t">Well, um, so from my understanding, from the layer level for Transformers, um, so it's almost comparable to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1618" target="_blank">00:26:58.880</a></span> | <span class="t">Cosmic Plinko in a way. Is that, I mean, of course it's a hard map behind it. Cosmic, sorry? What?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1624" target="_blank">00:27:04.960</a></span> | <span class="t">Cosmic Plinko, like the drop the, you know, cause it's 80 layers, right? And a grid, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1631" target="_blank">00:27:11.360</a></span> | <span class="t">So like low energy something. Sorry. I'm, I'm actually not familiar with that. Do I, you'll have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1637" target="_blank">00:27:17.280</a></span> | <span class="t">to explain it to me. What did you say? Cosmic, what? Sorry. Cosmic Plinko. Cosmic Plinko? Do I know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1643" target="_blank">00:27:23.760</a></span> | <span class="t">that? I'm not that smart. So you'll have to like explain to me what that is. Should I search that up?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1649" target="_blank">00:27:29.520</a></span> | <span class="t">Not gotcha, not gotcha. Um, like, uh, the arcade game with a bunch of pegs and you drop the bottom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1655" target="_blank">00:27:35.040</a></span> | <span class="t">Oh, that. Oh, yes. It's a Windows XP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1658" target="_blank">00:27:38.880</a></span> | <span class="t">Output layer, output, you know, on the bottom right, you have to visualize it, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1663" target="_blank">00:27:43.920</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1664" target="_blank">00:27:44.480</a></span> | <span class="t">Like if you take this and visualize it, and then visualize the map. For example, let's take llama 380 AP,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1672" target="_blank">00:27:52.080</a></span> | <span class="t">right? It's 32 layers, right? And then layer zero, or I guess, it doesn't matter. There's layer zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1679" target="_blank">00:27:59.200</a></span> | <span class="t">and then layer 32, which is the output layer. Yes. Now, when you drop a, uh, prompt into this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1686" target="_blank">00:28:06.640</a></span> | <span class="t">cosmic machine. Yes. Which, it's not actually cosmic. You just don't understand. Okay. Yes. It's not cosmic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1693" target="_blank">00:28:13.200</a></span> | <span class="t">Yes. It's just mass. Yup. But so far, it's cosmic. Yes. I don't think we understand. Personally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1698" target="_blank">00:28:18.800</a></span> | <span class="t">I don't think we understand if you pretend it again. Like where we're. I know. I think we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1702" target="_blank">00:28:22.560</a></span> | <span class="t">studied a lot, but okay. Yes. Okay. It's just mass. Yes. You're the expert, not me. I'm, you know, so. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1708" target="_blank">00:28:28.880</a></span> | <span class="t">But, um, so you're just trying to say an analogy. Like it's kind of like the game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1712" target="_blank">00:28:32.800</a></span> | <span class="t">Yeah. So, so people can visualize the map versus like. I think so we. Oh, yeah. I'll talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1719" target="_blank">00:28:39.760</a></span> | <span class="t">that in the slides, but like, I think it's more like, um, an analogy. I think it's more like you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1725" target="_blank">00:28:45.920</a></span> | <span class="t">going through like a maze. Like, okay. It's not a maze. I would say it's more like you have like a,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1730" target="_blank">00:28:50.480</a></span> | <span class="t">every single layer has like someone trying to make you change clothes. And then each layer, there's like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1738" target="_blank">00:28:58.560</a></span> | <span class="t">fashion designer trying to get you to wear different clothes. And each layer, the fashion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1743" target="_blank">00:29:03.600</a></span> | <span class="t">does designer doesn't like the previous fashion designers choices. And they will change your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1747" target="_blank">00:29:07.200</a></span> | <span class="t">clothes. Something like that. I think that's more like a transformer. It's like each fashion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1750" target="_blank">00:29:10.960</a></span> | <span class="t">designer has like views on their own. Okay. Yeah. I guess you could say it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1756" target="_blank">00:29:16.480</a></span> | <span class="t">Oh, yes. Yes. Yes. Wait, it is in Windows XP, right? Is that? I'm making me confused. Windows XP,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1763" target="_blank">00:29:23.120</a></span> | <span class="t">like the game. I think I played it before. Oh, okay. Anyways. Okay. Yeah. Sorry. Yeah. Question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1768" target="_blank">00:29:28.320</a></span> | <span class="t">Oh. Um, so when I put subscript I, it generally is effect. Well, technically everything is a matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1781" target="_blank">00:29:41.520</a></span> | <span class="t">But if you see any summation signs, um, with subscript I, then it generally means row wise or column wise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1787" target="_blank">00:29:47.840</a></span> | <span class="t">Um, the W, like if you see small, small, small, small, like a small W that generally means vector. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1794" target="_blank">00:29:54.960</a></span> | <span class="t">but in general, everything that's capital is a matrix. Um, and why is it a matrix? Because it's just faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1802" target="_blank">00:30:02.240</a></span> | <span class="t">Um, I mean, in theory, you could convert this all to vectors, but for speed purposes, this should be matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1810" target="_blank">00:30:10.240</a></span> | <span class="t">Um, yeah. Any other questions? Okay. Next. Um, so, why did I put this? Hello, my name is Daniel. Hi,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1819" target="_blank">00:30:19.920</a></span> | <span class="t">my brother's name is Michael. I hope everyone will have tons of fun. AI engineers, what fares is the best.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1825" target="_blank">00:30:25.040</a></span> | <span class="t">Um, why did I put this? Well, does anyone notice any similarities between these sentences? Um, or differences?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1835" target="_blank">00:30:35.040</a></span> | <span class="t">Sorry? Okay. Yes. Okay. Yes. Okay. Yes. Okay. Except for the first sentence. Okay. Anything else?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1844" target="_blank">00:30:44.240</a></span> | <span class="t">Just say random stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1847" target="_blank">00:30:47.040</a></span> | <span class="t">Interesting. Yes. Okay. Hello and hi are the same thing, but kind of different words. Okay. Yes. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1856" target="_blank">00:30:56.560</a></span> | <span class="t">Semantic embeddings. Basically this is an example of semantic embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1860" target="_blank">00:31:00.800</a></span> | <span class="t">Okay. Okay. Somewhat. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1863" target="_blank">00:31:03.600</a></span> | <span class="t">Okay. Yes. Okay. Okay. Okay. Well, to turn the truth, I didn't have any intention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1872" target="_blank">00:31:12.080</a></span> | <span class="t">The intention. Yes. Go on. I mean, it's the king plus queen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1876" target="_blank">00:31:16.800</a></span> | <span class="t">Yes. Yes. Yes. For water back. Yeah. Yeah. Yeah. I know what you're talking about. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1879" target="_blank">00:31:19.840</a></span> | <span class="t">So like, you know, it's king minus man plus woman equals queen. That's the, I think that's what you're trying to show.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1887" target="_blank">00:31:27.120</a></span> | <span class="t">Okay. Well, in theory, I guess you could have just seen the next slide. There are just five.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1897" target="_blank">00:31:37.440</a></span> | <span class="t">If you simply just look, um, the first one, if you consider punctuation as combined with the word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1904" target="_blank">00:31:44.720</a></span> | <span class="t">right? Like hello comma, treat that as one separate component. And if you do this, ignore all spaces. The first one has just five components. Right? What do you think the second one? Okay. I guess you can already have the slides. But what do you think the second one has? Right? Anyone?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1922" target="_blank">00:32:02.240</a></span> | <span class="t">I already wrote all the answers there. Okay. Yes. I wrote all the answers. Right? Six, eight, seven. Right? One, two, three, four, five, six, seven, eight. Right? One, two, three, four, five, six, seven. Right? So like, if you do this, um, just assume all punctuation from now on is combined with the word and ignore all spaces. Um, and this, we just invented a tokenizer. Um, right? So this is a very general,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1948" target="_blank">00:32:28.560</a></span> | <span class="t">a random tokenizer we just invented. Um, and each one is, has an, essentially the reason why we want to do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1955" target="_blank">00:32:35.120</a></span> | <span class="t">is because computers doesn't, they don't understand words. They only understand numbers. Right? So like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1961" target="_blank">00:32:41.040</a></span> | <span class="t">you have to essentially assign each of these tokens as a number, like an ID. Right? So hello is ID zero. Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1967" target="_blank">00:32:47.120</a></span> | <span class="t">Oh, hello comma actually is ID zero. My is ID one. Name is ID two and so on. Right? So like, you have to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1974" target="_blank">00:32:54.160</a></span> | <span class="t">you have to assign an ID to each of these components. If you don't do that, then a computer doesn't know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1979" target="_blank">00:32:59.520</a></span> | <span class="t">what you're actually doing. Right? Computers only know numbers. So we just invented a tokenizer. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1986" target="_blank">00:33:06.160</a></span> | <span class="t">I would not suggest you to use this tokenizer, but in general, it's actually not that bad. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1991" target="_blank">00:33:11.440</a></span> | <span class="t">because can anyone in see any issues with this new tokenizer we just created? Um, what are some issues?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=1998" target="_blank">00:33:18.640</a></span> | <span class="t">Yes, very good. So what do you think we should do? Okay. So the point was we included the punctuation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2010" target="_blank">00:33:30.800</a></span> | <span class="t">for the words and that is not helpful. Right? So like Michael explanation mark or Michael not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2015" target="_blank">00:33:35.600</a></span> | <span class="t">explanation mark. So what would you suggest to fix this? Um, interesting. So hello and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2024" target="_blank">00:33:44.400</a></span> | <span class="t">hello will be one token and then comma itself will be a, okay. Interesting. Anyone have any other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2030" target="_blank">00:33:50.400</a></span> | <span class="t">suggestions? Yes. Yes. Very good. Exactly. So do you have any suggestions for how to improve?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2047" target="_blank">00:34:07.280</a></span> | <span class="t">Oh yes. Very good. Yes. I haven't heard that in a long time. Very good. Yes. Old natural language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2052" target="_blank">00:34:12.640</a></span> | <span class="t">stuff. Um, so the idea was to stem the word. So essentially you can remove like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2057" target="_blank">00:34:17.520</a></span> | <span class="t">for example, like skipping can become skip, right? So like skipped, skipping, skip, like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2062" target="_blank">00:34:22.000</a></span> | <span class="t">they're all kind of the same. Well, I wouldn't say the same thing, but like in theory, they're the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2065" target="_blank">00:34:25.600</a></span> | <span class="t">thing. Um, any other suggestions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2067" target="_blank">00:34:27.680</a></span> | <span class="t">Very good idea, right? If you lowercase them all, then you can reduce a lot of issues. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2080" target="_blank">00:34:40.800</a></span> | <span class="t">is that a good idea? Capital my and small my, what do you think is the difference? If you do capital my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2087" target="_blank">00:34:47.840</a></span> | <span class="t">generally means it's the start of the sentence. If you do my with no lowercase, then it means it's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2092" target="_blank">00:34:52.560</a></span> | <span class="t">middle of the sentence. So good idea though. Um, actually I think, I think, um, so but it's an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2098" target="_blank">00:34:58.640</a></span> | <span class="t">old, okay, people still use but, um, I think there is a lowercase version of but, so they essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2104" target="_blank">00:35:04.400</a></span> | <span class="t">lowercase everything. Um, and I think it does okay for like semantics and stuff, but it doesn't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2109" target="_blank">00:35:09.520</a></span> | <span class="t">do well for decoder type style. So don't lowercase, but good idea. Any other suggestions? Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2116" target="_blank">00:35:16.720</a></span> | <span class="t">Uh, you build a vocabulary by starting with just the individual tokens instead of size for how large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2126" target="_blank">00:35:26.080</a></span> | <span class="t">your vocabulary would be. Yes. You start building word pieces from the less frequently of the algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2132" target="_blank">00:35:32.080</a></span> | <span class="t">Oh, you just said the name of the algorithm. It's called word piece or BPE tokenization. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2138" target="_blank">00:35:38.480</a></span> | <span class="t">yes, exactly. So that's actually the correct, well, I wouldn't say it's the correct way. I actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2141" target="_blank">00:35:41.840</a></span> | <span class="t">don't like that approach, but yes, it's the most recognizable and most industry standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2146" target="_blank">00:35:46.640</a></span> | <span class="t">approach is to do what you suggested, which is to start off with like small little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2150" target="_blank">00:35:50.800</a></span> | <span class="t">individual characters and then combine them together based on some sort of statistic, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2155" target="_blank">00:35:55.760</a></span> | <span class="t">You shouldn't like, you know, hello, like maybe hello is a word that we have to select, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2160" target="_blank">00:36:00.800</a></span> | <span class="t">So like, but, but like H E like H E L L. Okay. Hell. Okay. That might be a popular word as well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2167" target="_blank">00:36:07.840</a></span> | <span class="t">in like the dictionary, but anyways, or he, right? So he, hell, hello. Um, each of these might have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2173" target="_blank">00:36:13.440</a></span> | <span class="t">assigned different tokens, but they might not be right. So like, it depends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2176" target="_blank">00:36:16.560</a></span> | <span class="t">Um, and the correct industry standard is to use these methodologies to build up the component,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2182" target="_blank">00:36:22.720</a></span> | <span class="t">um, which we won't be discussing today, but like that's for later research. Um, yes. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2188" target="_blank">00:36:28.880</a></span> | <span class="t">So let us just look at one sentence, right? The first one is, hello, my name is Daniel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2197" target="_blank">00:36:37.840</a></span> | <span class="t">Assuming our tokenization is useful. Okay. Let's just assume the tokenization which we created is helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2204" target="_blank">00:36:44.400</a></span> | <span class="t">Okay. So remember it is just put all the punctuation together. Um, ignore spaces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2209" target="_blank">00:36:49.840</a></span> | <span class="t">and don't do lowercase or whatever. Just, just, you know, tokenize it as it is. It's not very useful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2216" target="_blank">00:36:56.480</a></span> | <span class="t">but who cares? We just say this is a good tokenizer. Um, the question now is if I select the first token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2223" target="_blank">00:37:03.600</a></span> | <span class="t">hello, right? Let's assume, I don't know if, okay, the color's not very good, but like my name is Daniel is grayed out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2228" target="_blank">00:37:08.960</a></span> | <span class="t">I think I'm not sure if you can see that grayed out. Pretend someone types in hello, comma, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2234" target="_blank">00:37:14.560</a></span> | <span class="t">A language model should predict what's the next word, right? So how does it know to predict my, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2240" target="_blank">00:37:20.080</a></span> | <span class="t">You know, when you type chat GPT, it like, it goes from left to right. Like you type something, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2244" target="_blank">00:37:24.240</a></span> | <span class="t">you type some sort of instruction and then chat GPT will like print the words from left to right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2249" target="_blank">00:37:29.200</a></span> | <span class="t">Right. So like, why is it printing words? Can anyone tell me why is chat GPT printing words from left to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2254" target="_blank">00:37:34.240</a></span> | <span class="t">right? Why is it not doing right to left? Or why does it not just spit out everything in one go? Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2272" target="_blank">00:37:52.400</a></span> | <span class="t">Exactly. Correct. So because it's a transformer type decoder type architecture, it is predicting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2278" target="_blank">00:37:58.160</a></span> | <span class="t">the next word based on the previous words. So very good. Um, and so the point is, is we only can have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2286" target="_blank">00:38:06.080</a></span> | <span class="t">the language model see previous words, not future words, right? If you accidentally put the future data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2292" target="_blank">00:38:12.720</a></span> | <span class="t">in, Oh, you know, your accuracy might go to 100% if you use future data. So please do not do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2298" target="_blank">00:38:18.000</a></span> | <span class="t">This is actually a very common mistake. Like I'm not saying this jokingly. It's a very big issue in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2303" target="_blank">00:38:23.600</a></span> | <span class="t">research. So please read the paper before you actually, like, if you read research papers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2307" target="_blank">00:38:27.520</a></span> | <span class="t">please see how they do the methodology. Um, always read. Did they put the, always ask the question,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2313" target="_blank">00:38:33.920</a></span> | <span class="t">did they put future data in when they did the training or doing the research paper? Okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2318" target="_blank">00:38:38.320</a></span> | <span class="t">this is a pervasive problem. I'm not joking. Um, you can see like weird accuracies. Like if you see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2323" target="_blank">00:38:43.200</a></span> | <span class="t">papers which have like 98% accuracy, question, question, question, question, question, question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2327" target="_blank">00:38:47.600</a></span> | <span class="t">Okay. Or 100% accuracy. How's that even possible? How can something be 100% accurate? Question,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2332" target="_blank">00:38:52.960</a></span> | <span class="t">right? And so the most likely scenario is they use future data. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2335" target="_blank">00:38:55.760</a></span> | <span class="t">that is a very good question. So that the question was like, is it the paper, the research paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2363" target="_blank">00:39:23.600</a></span> | <span class="t">like the methodology itself using future data, or is it the code implementation, the problem?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2369" target="_blank">00:39:29.440</a></span> | <span class="t">Now that's actually a very good question because it depends. So I think if the researchers, I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2375" target="_blank">00:39:35.120</a></span> | <span class="t">normally in research papers, one person does the coding, some, some other people do like secondary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2379" target="_blank">00:39:39.200</a></span> | <span class="t">coding. And then the main researcher who makes the idea, they don't actually do the coding. So the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2383" target="_blank">00:39:43.200</a></span> | <span class="t">the coder might have misinterpreted the methodology. To be honest, I don't think so. That's the case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2390" target="_blank">00:39:50.400</a></span> | <span class="t">I think it's actually a methodology. That's the problem. I think if the researcher, like the main</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2394" target="_blank">00:39:54.640</a></span> | <span class="t">researcher, if they find out that their method has 90%, 98% accuracy, like why didn't they question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2401" target="_blank">00:40:01.280</a></span> | <span class="t">the results? Like I would like, wouldn't that be very sketchy, like 98% accuracy, 100% accuracy? I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2406" target="_blank">00:40:06.720</a></span> | <span class="t">not joking. This is actually a very serious problem. So if you read enough research papers, you'll see this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2411" target="_blank">00:40:11.360</a></span> | <span class="t">problem is always the same problem. It's always using future data. So yeah, I think that, yeah. I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2419" target="_blank">00:40:19.120</a></span> | <span class="t">like if you don't, yeah. Okay. I won't comment on any papers. Anyways. Yes. Okay. Does that kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2424" target="_blank">00:40:24.240</a></span> | <span class="t">answer your question? So it depends. I think in general, I would say it's the, it is the researcher,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2428" target="_blank">00:40:28.720</a></span> | <span class="t">the main research, the lead researcher's responsibility to correct these mistakes. Then you shouldn't be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2434" target="_blank">00:40:34.880</a></span> | <span class="t">really lead researcher. So like, that's my take on that. Um, I think the coder, I, I mean the program,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2440" target="_blank">00:40:40.160</a></span> | <span class="t">I might have some issues, but I don't know. I, I think I blame the lead researcher. Um, any other points?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2447" target="_blank">00:40:47.040</a></span> | <span class="t">It seems like you're saying the main red flag is like a huge leap in performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2451" target="_blank">00:40:51.120</a></span> | <span class="t">Yes. If you see huge leap in performance, question, question, question. It's probably future data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2457" target="_blank">00:40:57.360</a></span> | <span class="t">Actually, I wouldn't say it's probably, it's like 50% sure it's future data. Um, yes, question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2471" target="_blank">00:41:11.360</a></span> | <span class="t">Exactly. So the point was, um, in the test and train split, sometimes, although you might not be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2500" target="_blank">00:41:40.000</a></span> | <span class="t">using future data, you might like accidentally put different components in the trip, like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2505" target="_blank">00:41:45.360</a></span> | <span class="t">training set from the test set. And then depending on how you split the data set, you might actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2509" target="_blank">00:41:49.680</a></span> | <span class="t">mix the data sets. Is that kind of correct? Yes. The distribution itself. Yes. So that is why you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2518" target="_blank">00:41:58.160</a></span> | <span class="t">should, you have to be very careful to how you split the data sets, right? For training and testing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2522" target="_blank">00:42:02.000</a></span> | <span class="t">you have to use stratification. You have to inspect the data before you do it. You must enable random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2527" target="_blank">00:42:07.200</a></span> | <span class="t">shuffling. Right? There is actually a very, very common question on PyTorch. How do you not enable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2532" target="_blank">00:42:12.400</a></span> | <span class="t">random for hugging face? How do you not enable random shuffling for hugging face? Um, they purposefully</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2538" target="_blank">00:42:18.400</a></span> | <span class="t">disabled this. Like think about like why, because like people might forget to randomly shuffle. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2543" target="_blank">00:42:23.280</a></span> | <span class="t">and actually this is a pervasive problem with Kaggle competitions. People like to like train on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2548" target="_blank">00:42:28.400</a></span> | <span class="t">tests. Like they give you like 20, normally speaking, you should have 20 tests, 80 train, right? So like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2553" target="_blank">00:42:33.600</a></span> | <span class="t">but people like to like the final submission, they use even the test in the submission. So, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2559" target="_blank">00:42:39.440</a></span> | <span class="t">it is a pervasive problem. Um, any other points and questions? Okay. So what is the language model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2567" target="_blank">00:42:47.520</a></span> | <span class="t">Given the word hello, can you somehow predict my name is Daniel? Right? So like given the word, right? So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2574" target="_blank">00:42:54.960</a></span> | <span class="t">given that token, can you predict these extra tokens? Um, remember carefully, I purposely only went from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2583" target="_blank">00:43:03.360</a></span> | <span class="t">left to right, right? So like, hello can only predict my, can only predict name is and Daniel, right? It</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2589" target="_blank">00:43:09.360</a></span> | <span class="t">cannot predict, right? You can't use the word hello to predict previous words, right? That's cheating. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2596" target="_blank">00:43:16.080</a></span> | <span class="t">that is sequence modeling. And the point is when you chain them together, let's assume that you predicted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2604" target="_blank">00:43:24.000</a></span> | <span class="t">the word my as the next word, right? So now you have two pieces of information. The text that you see is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2609" target="_blank">00:43:29.120</a></span> | <span class="t">hello comma my using these two pieces of information. You want to predict name is Daniel and you keep doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2615" target="_blank">00:43:35.920</a></span> | <span class="t">this. And that is a language model, right? That is essentially a language model. What is it doing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2621" target="_blank">00:43:41.840</a></span> | <span class="t">Is you start from the first word, you predict the words into the future, um, and you keep doing this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2627" target="_blank">00:43:47.920</a></span> | <span class="t">iteratively, right? And so that is what chat GPT is doing. Um, does that kind of make sense? Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2635" target="_blank">00:43:55.360</a></span> | <span class="t">I remember the point is never use future data. Okay. I'm, I'm, I know I keep stressing this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2639" target="_blank">00:43:59.680</a></span> | <span class="t">but like, this is actually a very big issue in machine learning and AI, right? This is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2643" target="_blank">00:44:03.840</a></span> | <span class="t">the biggest issues I find in my opinion is using future data. There's so many papers which do this. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2650" target="_blank">00:44:10.640</a></span> | <span class="t">Yeah. So the second point is you must tokenize each component into numbers. Remember this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2657" target="_blank">00:44:17.360</a></span> | <span class="t">So I'm just going to, you know, cook up some numbers. Hello will be 0.11 minus 0.123 102. Okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2664" target="_blank">00:44:24.480</a></span> | <span class="t">just made those numbers up. Um, Daniel is 0.11 123 minus 0.122. Okay. I just randomly made them up. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2672" target="_blank">00:44:32.560</a></span> | <span class="t">And remember you must, each component must have the same number of numbers, right? So like if hello</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2682" target="_blank">00:44:42.400</a></span> | <span class="t">has three, Daniel must also have three. Can someone tell me how many combinations of, if you assign this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2690" target="_blank">00:44:50.160</a></span> | <span class="t">case that each number must have three, um, you know, three numbers, how many combinations do you think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2696" target="_blank">00:44:56.240</a></span> | <span class="t">there can be for each token or for each component? How many combinations? What do you think the answer is?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2703" target="_blank">00:45:03.200</a></span> | <span class="t">So remember you can choose any single number in the three numbers, right? 0.11, 0.1, 1, 1, 1, 2, 0.1, 1, 1, 1, 1, 1, 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2712" target="_blank">00:45:12.080</a></span> | <span class="t">whatever number, how many combinations are possible?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2714" target="_blank">00:45:14.960</a></span> | <span class="t">Depends from the flow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2716" target="_blank">00:45:16.960</a></span> | <span class="t">Okay. Let's assume it's infinite precision. Correct. The answer is infinity. You can do as many as you like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2724" target="_blank">00:45:24.800</a></span> | <span class="t">Um, but normally speaking, you should use not three. All right. Now the question is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2730" target="_blank">00:45:30.160</a></span> | <span class="t">why don't you just use one number then? Right? How low can be 0.1, 1, 1. If it's already infinity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2734" target="_blank">00:45:34.000</a></span> | <span class="t">0.1, 1, 1. If you use two numbers, isn't also infinity, right? So what's, what's the problem? Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2738" target="_blank">00:45:38.640</a></span> | <span class="t">please don't, you should use as many numbers as possible. Try your best to use more numbers. It's because the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2743" target="_blank">00:45:43.200</a></span> | <span class="t">computer, you know, it's not an infinite machine. So please use like more numbers so it can like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2748" target="_blank">00:45:48.640</a></span> | <span class="t">learn which numbers to like assign it to. Um, and when you start training a language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2753" target="_blank">00:45:53.680</a></span> | <span class="t">these numbers will be randomly initialized, right? So like all these numbers will be randomly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2758" target="_blank">00:45:58.960</a></span> | <span class="t">initialized. And can someone even notice my initialization? What is the issue with this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2763" target="_blank">00:46:03.440</a></span> | <span class="t">There are like some, there is a glaring issue, quite obvious, um, very big problematic issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2769" target="_blank">00:46:09.040</a></span> | <span class="t">Okay, same number. Okay. Okay. Good point. Yes. Did someone say something? What was the other point?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2783" target="_blank">00:46:23.680</a></span> | <span class="t">Yes, the magnitude is the most important. Right? 123 and 102 are terrible. Um, when you randomly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2792" target="_blank">00:46:32.160</a></span> | <span class="t">initialized, please do not initialize with random large components. Um, this will destroy your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2798" target="_blank">00:46:38.400</a></span> | <span class="t">training. That is why your training might have infinities. And sometimes the training loss goes to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2803" target="_blank">00:46:43.920</a></span> | <span class="t">zero. Okay. That is not, that does not mean your model learned anything. That just means it's some sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2808" target="_blank">00:46:48.880</a></span> | <span class="t">of error in your training data. Uh, sorry, your initialization. So be careful of that. Um, don't worry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2814" target="_blank">00:46:54.880</a></span> | <span class="t">Huggy face does this automatically for you. So you don't need to worry. Um, and now each component has a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2823" target="_blank">00:47:03.360</a></span> | <span class="t">list of numbers that is it it's associated with. Okay. I just use the same number for now. It's easier for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2827" target="_blank">00:47:07.920</a></span> | <span class="t">me to, you know, do the slides. Um, but essentially, hello comma. My name is Daniel. Each of them has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2834" target="_blank">00:47:14.320</a></span> | <span class="t">numbers associated with them. And this is the thing that you're trying to learn for each of these components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2842" target="_blank">00:47:22.720</a></span> | <span class="t">And remember, this can be converted into a table of numbers. Right? So like if you replace all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2848" target="_blank">00:47:28.000</a></span> | <span class="t">commas with just column columns, right, these are just tables of numbers. Um, and this table is what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2854" target="_blank">00:47:34.240</a></span> | <span class="t">you need to train. Um, and again, remember, given the word hello, you want to predict my name is Daniel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2862" target="_blank">00:47:42.160</a></span> | <span class="t">Right? So like essentially given that, you know, vector of numbers, can you predict the other vectors of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2868" target="_blank">00:47:48.160</a></span> | <span class="t">numbers? Um, yes. Oh yeah, you can do as many numbers. So you have to select a option. How many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2878" target="_blank">00:47:58.480</a></span> | <span class="t">numbers you want to select to represent these numbers. So for example, you can select six numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2883" target="_blank">00:48:03.920</a></span> | <span class="t">or you can select 1024 numbers or 2048. It depends on the model creators choice. Um, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2890" target="_blank">00:48:10.880</a></span> | <span class="t">Yes. Every single row must be, well, it doesn't have to, okay, not, it might not be the case that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2896" target="_blank">00:48:16.960</a></span> | <span class="t">might be not, but it should be, yeah, with high probability, it will be unique. Um, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2901" target="_blank">00:48:21.920</a></span> | <span class="t">Wait, is it like, okay. Um, and so when you do training of a language model, um, there's a trick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2912" target="_blank">00:48:32.400</a></span> | <span class="t">that you use and remember we want to predict the next word, right? So hello, you want to predict my,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2918" target="_blank">00:48:38.960</a></span> | <span class="t">right? So can someone notice any pattern with this? Like, why did I do the arrow? And what's the pattern</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2926" target="_blank">00:48:46.320</a></span> | <span class="t">that can anyone see this? Any special, special, like things with this? No.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2937" target="_blank">00:48:57.120</a></span> | <span class="t">What happens if you take, hello, my name is Daniel and just shift it up by one place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2940" target="_blank">00:49:00.400</a></span> | <span class="t">Is it right? If you shift it up by one place, hello is now aligned with my, my is now aligned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2949" target="_blank">00:49:09.040</a></span> | <span class="t">with name and so on. Right. And there'll be a gap at the very bottom. Right. So we simply,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2952" target="_blank">00:49:12.800</a></span> | <span class="t">we just put us, which means end of sentence token. That just means it's the end of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2957" target="_blank">00:49:17.120</a></span> | <span class="t">And we just, you know, put it there because it's a gap, right? So remember machines, machines do not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2961" target="_blank">00:49:21.520</a></span> | <span class="t">like gaps and you must use all numbers. So that's the reason why we did that. And this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2966" target="_blank">00:49:26.720</a></span> | <span class="t">is kind of the training mechanism, right? So we essentially, we have that list of, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2972" target="_blank">00:49:32.880</a></span> | <span class="t">list of words and we want to predict the shifted words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2976" target="_blank">00:49:36.240</a></span> | <span class="t">And the transformer, all it does is there's a function to predict that. So given hello,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2985" target="_blank">00:49:45.280</a></span> | <span class="t">can you predict my, given my, can you predict name and so on, right? That's the transformer architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2990" target="_blank">00:49:50.400</a></span> | <span class="t">It's kind of a bit wrong, but like something like that. Okay. Like the FX is like this gigantic model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2994" target="_blank">00:49:54.960</a></span> | <span class="t">that can be like, you know, there's lots of turning knobs in it. Okay. Let's check if there's any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=2998" target="_blank">00:49:58.800</a></span> | <span class="t">questions. Okay. And the point is, remember, remember the point is we can only use predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3008" target="_blank">00:50:08.400</a></span> | <span class="t">the future words, right? So hello can only predict my name is Daniel and so on, right? That's the purple,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3012" target="_blank">00:50:12.800</a></span> | <span class="t">the purple component. And the blue box is called the attention mechanism. The FFX, I factored it out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3020" target="_blank">00:50:20.960</a></span> | <span class="t">and that is called the multi-layer perception or MLP layer, right? So there's actually two components</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3024" target="_blank">00:50:24.800</a></span> | <span class="t">in a language model. One does the prediction of the next word and the FFX, which is the MLP, just does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3031" target="_blank">00:50:31.040</a></span> | <span class="t">this, you know, changing component. It just makes it like, you know, a bit better than just simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3037" target="_blank">00:50:37.600</a></span> | <span class="t">predicting the next word, the attention. Oh, okay. There is a question. Oh, I didn't see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3045" target="_blank">00:50:45.200</a></span> | <span class="t">Okay. Well, um, yes. So the comment was normal transformers predict only one token at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3068" target="_blank">00:51:08.560</a></span> | <span class="t">How about transformers predict multiple tokens at a time? Yes. So actually you can predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3073" target="_blank">00:51:13.440</a></span> | <span class="t">multiple tokens at a time. It just depends on what is your training objective at the very last layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3078" target="_blank">00:51:18.000</a></span> | <span class="t">You don't have to remember we shifted by one place, right? Why don't you shift by two places</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3082" target="_blank">00:51:22.560</a></span> | <span class="t">and three places, then you'll be predicting two tokens in the future, three tokens into the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3086" target="_blank">00:51:26.960</a></span> | <span class="t">Exactly. So before it's just one objective. So like one column of the output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3092" target="_blank">00:51:32.560</a></span> | <span class="t">we just add more. And yes, you could do multiple tokens. Um, was it the, it was the Facebook paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3098" target="_blank">00:51:38.400</a></span> | <span class="t">right? I can't remember. Yeah, it was a Facebook paper. Um, I forgot the accuracy though. Um, yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3103" target="_blank">00:51:43.760</a></span> | <span class="t">but yeah, you could do that. Um, I guess like, I don't see any, I guess it's just good for inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3109" target="_blank">00:51:49.520</a></span> | <span class="t">time. Like you can predict, you know, you can make inference. If you do four tokens in the future,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3113" target="_blank">00:51:53.840</a></span> | <span class="t">you can predict four times faster. Um, okay. Yeah. I think it's mainly for when you do inference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3124" target="_blank">00:52:04.800</a></span> | <span class="t">you can do this. If you do two tokens in the future, you can have like two tokens in one go. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3129" target="_blank">00:52:09.120</a></span> | <span class="t">I don't know. I don't like that approach. I think predicting one token is better. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3133" target="_blank">00:52:13.840</a></span> | <span class="t">cause you're already forcing the language model to do so much. You're making even more problematic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3138" target="_blank">00:52:18.240</a></span> | <span class="t">So I think predicting one, okay, maybe for inference time, it could work. Um, yeah. Yes. Question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3154" target="_blank">00:52:34.240</a></span> | <span class="t">So token, yeah. Tokenizer converted to ID. So like, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3163" target="_blank">00:52:43.120</a></span> | <span class="t">So, so the token tokenizer when it has like, when it says like it has 32,000 words in the tokenizer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3174" target="_blank">00:52:54.240</a></span> | <span class="t">essentially it's an ID from zero to three, one, nine, nine, nine, right? So like, hello,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3178" target="_blank">00:52:58.960</a></span> | <span class="t">we'll have ID 2,557. So then what you do is you take 2,557, go to the hash table, which has this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3185" target="_blank">00:53:05.680</a></span> | <span class="t">which has the, um, this table, right? I made the table, right? So hello has an ID. My has an ID,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3191" target="_blank">00:53:11.600</a></span> | <span class="t">name as an ID and so on. And you essentially, you hash, you go to that specific row and then you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3196" target="_blank">00:53:16.880</a></span> | <span class="t">that is your like sample and your training data. And you do this like for the whole thing. So the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3202" target="_blank">00:53:22.480</a></span> | <span class="t">tokenizer does, you do get a number, it's an integer. Um, and you just have to hash it to the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3208" target="_blank">00:53:28.400</a></span> | <span class="t">you know, the embedding matrix and you will get like a vector of numbers. Does that kind of answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3213" target="_blank">00:53:33.520</a></span> | <span class="t">your question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3218" target="_blank">00:53:38.640</a></span> | <span class="t">Oh, you're talking about the embedding dimension. So if you see 2,540 or whatever those numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3224" target="_blank">00:53:44.080</a></span> | <span class="t">um, that is just the, how many, how many like columns, how many numbers you want to represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3229" target="_blank">00:53:49.120</a></span> | <span class="t">for each, um, component?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3231" target="_blank">00:53:51.120</a></span> | <span class="t">Okay. Okay. Any other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3237" target="_blank">00:53:57.760</a></span> | <span class="t">Yeah. Yes. Oh. Oh, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3241" target="_blank">00:54:01.120</a></span> | <span class="t">So you can enter them at late, way later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3243" target="_blank">00:54:03.680</a></span> | <span class="t">Okay. Okay. Okay. Yeah. Yeah. You had a...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3253" target="_blank">00:54:13.120</a></span> | <span class="t">I guess it just depends like if you make, so the point, so remember we said how many combinations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3265" target="_blank">00:54:25.680</a></span> | <span class="t">can we do? Infinity because we can do right, because floating in person representation, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3270" target="_blank">00:54:30.800</a></span> | <span class="t">in theory, if you have infinite position, it can be infinity. But someone mentioned how depending on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3274" target="_blank">00:54:34.560</a></span> | <span class="t">the position of your float, it's actually limited position, right? So like the point is you want your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3279" target="_blank">00:54:39.040</a></span> | <span class="t">model, you want the training objective to actually learn, give it as much freedom as you like, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3284" target="_blank">00:54:44.480</a></span> | <span class="t">So like if you're trying to restrict the model when it learns, it might actually not be helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3288" target="_blank">00:54:48.640</a></span> | <span class="t">And that is why normally people have like these large numbers, like, you know, 6144 embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3293" target="_blank">00:54:53.920</a></span> | <span class="t">dimension or 8192 embedding dimension, right? The more numbers you give it to the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3298" target="_blank">00:54:58.160</a></span> | <span class="t">it just has more freedom to move. Um, so if I, in theory, I mean, in theory, it should have better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3306" target="_blank">00:55:06.560</a></span> | <span class="t">accuracy. Um, in theory, everything's in theory. Um, I don't know if there are research papers to show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3311" target="_blank">00:55:11.840</a></span> | <span class="t">this. I think someone should write a research paper on that, you know, each embedding dimension test,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3317" target="_blank">00:55:17.040</a></span> | <span class="t">you know, do 3 trillion tokens. Okay. That's probably too many. And then see which one has,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3321" target="_blank">00:55:21.840</a></span> | <span class="t">I'm assuming the more you add the, um, the higher accuracy. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3336" target="_blank">00:55:36.960</a></span> | <span class="t">I agree on that. I think someone needs to do a research paper. Yes, that should be a new research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3344" target="_blank">00:55:44.400</a></span> | <span class="t">topic. I've never seen this paper before. So like, that would be very interesting. So yes, question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3348" target="_blank">00:55:48.640</a></span> | <span class="t">Yes. The reason for that is it just makes training faster sometimes. So depending on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3366" target="_blank">00:56:06.880</a></span> | <span class="t">so there is, I think Andre was one who tweeted about depending if you pad the token, if you pad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3371" target="_blank">00:56:11.600</a></span> | <span class="t">the vocabulary to a specific number, you can actually make training faster. Because in the NVIDIA GPUs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3376" target="_blank">00:56:16.720</a></span> | <span class="t">when you use tensor cores, if you pad it correctly, it can get the data and cache it more appropriately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3383" target="_blank">00:56:23.840</a></span> | <span class="t">So like, for example, the cache size is like 64. I think it's 64. Okay. Maybe I'm making stuff up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3388" target="_blank">00:56:28.160</a></span> | <span class="t">But like essentially you have to pad it to a multiple of 64, something like that. And so sometimes that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3391" target="_blank">00:56:31.920</a></span> | <span class="t">happens. Another one is like some people want to add when you want to do more fine tuning, when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3395" target="_blank">00:56:35.760</a></span> | <span class="t">want to like train the model for more, you want to use one of those unused tokens for your own purpose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3401" target="_blank">00:56:41.120</a></span> | <span class="t">And so they left some holes in there. Yeah. Does that kind of answer your question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3405" target="_blank">00:56:45.600</a></span> | <span class="t">So when you do tokenization, assuming you don't encounter these tokens, you won't have any problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3418" target="_blank">00:56:58.480</a></span> | <span class="t">But if you do, then there are problems. Yes. So for example, if you do LLAMA3 fine tuning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3424" target="_blank">00:57:04.560</a></span> | <span class="t">if you use the base model for LLAMA3, and you accidentally use one of those tokens, you will get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3430" target="_blank">00:57:10.640</a></span> | <span class="t">NANs for fine tuning. Right? So you have to be very, very careful. And so like, I think what we did is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3436" target="_blank">00:57:16.320</a></span> | <span class="t">for unsloft, we actually find these untrained tokens first, set them to the mean of all the embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3442" target="_blank">00:57:22.640</a></span> | <span class="t">and you won't have these issues. So I think that's actually a model creators problem. It's like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3446" target="_blank">00:57:26.320</a></span> | <span class="t">they probably should have not set it to zero. I don't know why they did that. But anyways,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3450" target="_blank">00:57:30.880</a></span> | <span class="t">yeah, they should have set it to be like, you know, normal distribution or like some, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3455" target="_blank">00:57:35.200</a></span> | <span class="t">just random initialization. Yes. Kind of. Yeah. Okay. Any other questions? Okay. Oh, yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3469" target="_blank">00:57:49.680</a></span> | <span class="t">Oh, yeah. Yeah. You can put a beginning of sentence token. I just didn't do that. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3480" target="_blank">00:58:00.240</a></span> | <span class="t">you should put a beginning of most language models will put a beginning of sentence. Like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3484" target="_blank">00:58:04.160</a></span> | <span class="t">what is it? You know, I put the end of sentence, you should probably put a beginning of sentence as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3488" target="_blank">00:58:08.080</a></span> | <span class="t">well. That's actually very important as well. Um, most models do that now. Um, they found that to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3493" target="_blank">00:58:13.040</a></span> | <span class="t">very helpful. To be honest, I don't think so. It's actually that effective. I think the beginning of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3498" target="_blank">00:58:18.000</a></span> | <span class="t">BOS token came from the old style, um, um, the old style, the CLS token for, um, I think it was the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3505" target="_blank">00:58:25.120</a></span> | <span class="t">first token for like bird style. Um, so that they had a classified token at the very start. I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3509" target="_blank">00:58:29.600</a></span> | <span class="t">it was at the very start. I'm not a hundred percent sure, but I think that's where it came from. The big,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3512" target="_blank">00:58:32.880</a></span> | <span class="t">I don't think so. The beginning of sentence token actually makes that much difference. Um, but you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3517" target="_blank">00:58:37.520</a></span> | <span class="t">should put it, you should put it, you know, giving the model more freedom to move is always better. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3523" target="_blank">00:58:43.040</a></span> | <span class="t">so yes, I probably should have put the beginning of sentence, but you know, yeah, for demonstration,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3527" target="_blank">00:58:47.040</a></span> | <span class="t">I did not do that. Um, yes. Okay. We did that. Right. So like the green one, um, right. So like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3533" target="_blank">00:58:53.440</a></span> | <span class="t">the, the attention block is kind of encoding the stuff that we described, right? Predicting the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3540" target="_blank">00:59:00.640</a></span> | <span class="t">word based on the previous words, right? And so like the attention block is that first part,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3545" target="_blank">00:59:05.680</a></span> | <span class="t">the MLP block is just the mixing pump component. Um, and this is kind of the transform architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3551" target="_blank">00:59:11.040</a></span> | <span class="t">and kind of like in visualized. And you just repeat this L times. Um, that is a transformer. Oh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3556" target="_blank">00:59:16.640</a></span> | <span class="t">Now, another question I always have is, why is training language models not O of N cubed? Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3567" target="_blank">00:59:27.600</a></span> | <span class="t">like, aren't you like, given the word hello, you're predicting my, right? And now we have hello,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3572" target="_blank">00:59:32.240</a></span> | <span class="t">my, you're predicting name, and then you have hello, my name, and you're predicting is, and so on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3577" target="_blank">00:59:37.040</a></span> | <span class="t">right? Shouldn't this be the training data? Why is the training data just hello, my, my name,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3583" target="_blank">00:59:43.920</a></span> | <span class="t">name is, is Daniel, Daniel EOS, right? This is the training data that you actually see. Why is it not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3589" target="_blank">00:59:49.200</a></span> | <span class="t">this? Can, does anyone know why? Sorry? The complexity? Yes, the complexity. Yeah. Very bad complexity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3600" target="_blank">01:00:00.960</a></span> | <span class="t">right? So like if the sentence is like one, okay. If the sentence is like 100 words, what do you think?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3607" target="_blank">01:00:07.520</a></span> | <span class="t">How many? Quite bad. Yes. Basically. Yeah. So like a one plus two plus three plus four plus five,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3615" target="_blank">01:00:15.120</a></span> | <span class="t">all the way to plus 100, right? So like N divided by two, one plus 100, I think. I can't remember my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3621" target="_blank">01:00:21.360</a></span> | <span class="t">maths, but yeah, something like that. So it ends, yeah, it's very bad. Um, and that's if you have one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3625" target="_blank">01:00:25.200</a></span> | <span class="t">sentence. What happens if you have like 10 sentences? Oh my. Yeah. But like, does anyone know why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3629" target="_blank">01:00:29.680</a></span> | <span class="t">language models don't need to do this? Like we don't actually need to do this, right? So like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3634" target="_blank">01:00:34.320</a></span> | <span class="t">we can skip essentially, instead of, instead of having this as the training data, your training data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3639" target="_blank">01:00:39.280</a></span> | <span class="t">is simply, my name is Daniel and, and shift it by one up. And that's your training data. Why is it not this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3645" target="_blank">01:00:45.120</a></span> | <span class="t">Oh, yes. We haven't talked about position encodings yet. Yeah. Okay. But you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3652" target="_blank">01:00:52.000</a></span> | <span class="t">don't need position encodings. Oh, okay. Yeah. Attention. What? Oh yeah. The attention mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3658" target="_blank">01:00:58.400</a></span> | <span class="t">Oh, correct. That's the answer. Yes. It's because of attention. Well, actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3663" target="_blank">01:01:03.280</a></span> | <span class="t">specifically, um, masked attention. Right? So that's, that's a trick. Okay. We'll be talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3668" target="_blank">01:01:08.000</a></span> | <span class="t">about that. Um, we've been talking about a few times. Um, and I'll give you the code again. Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3674" target="_blank">01:01:14.560</a></span> | <span class="t">actually the math formulas for transformer architecture, right? So like attention block, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3678" target="_blank">01:01:18.960</a></span> | <span class="t">we will be now talking about the attention block, right? So like the Z is equal to the soft max of QK</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3685" target="_blank">01:01:25.200</a></span> | <span class="t">transpose over root H plus M V. Um, and as, as you mentioned, it is the attention mechanism,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3693" target="_blank">01:01:33.840</a></span> | <span class="t">which allows us to skip the O of N cubed complexity and make it O of N squared. Why? Because remember,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3702" target="_blank">01:01:42.160</a></span> | <span class="t">we want to mask out future tokens because we don't want to predict on future data, right? So like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3706" target="_blank">01:01:46.800</a></span> | <span class="t">by using this mask, weirdly, this mask allows you to train more efficiently. Um, you know, it's funny,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3712" target="_blank">01:01:52.640</a></span> | <span class="t">because like attention is O of N squared. So the longer your sequence is, the worst the complexity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3717" target="_blank">01:01:57.840</a></span> | <span class="t">but actually there is a special trick, which you use a mask and this actually makes attention not that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3724" target="_blank">01:02:04.240</a></span> | <span class="t">bad. Um, so instead of doing hello to predict my and so on, so on, so on, so on, the attention mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3732" target="_blank">01:02:12.080</a></span> | <span class="t">acts as this methodology, right? So the attention mask itself acts as, um, you don't need to do like all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3738" target="_blank">01:02:18.800</a></span> | <span class="t">complicated, you know, all of the words predict the next word. Um, okay. This is okay. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3745" target="_blank">01:02:25.920</a></span> | <span class="t">so we'll be now talking just about the attention itself, right? So like soft max QK transfers over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3750" target="_blank">01:02:30.400</a></span> | <span class="t">root DV. Um, just a reminder that whenever you see QK transfers, the query and keys, um, I do not like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3756" target="_blank">01:02:36.720</a></span> | <span class="t">to like, there's like explanations. Like what is a query? What is the key? I do not like that actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3760" target="_blank">01:02:40.640</a></span> | <span class="t">approach. I would like this to be a math approach. Um, so my view is given the matrix X, which is your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3767" target="_blank">01:02:47.840</a></span> | <span class="t">embeddings, right? So remember hello is a vector of numbers, right? You multiply this by some weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3774" target="_blank">01:02:54.160</a></span> | <span class="t">WQ, WK and WV and you get back QKV. Q is query. Okay. Keys and values, but, um, that's a very vague</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3784" target="_blank">01:03:04.000</a></span> | <span class="t">interpretation. I don't really believe, like, I don't really trust those interpretations. It's not that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3787" target="_blank">01:03:07.840</a></span> | <span class="t">clear. Um, just assume it's just math. Okay. Just like get your X matrix and multiply by weights and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3792" target="_blank">01:03:12.720</a></span> | <span class="t">you get some extra weights. That's my view. Um, and so that is kind of, so like, if you see why I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3799" target="_blank">01:03:19.200</a></span> | <span class="t">stacked it like this, does anyone know why I stacked it like this? Like why did a presenter like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3803" target="_blank">01:03:23.200</a></span> | <span class="t">specifically? Why is it like the presentation like this? Any, any points? Sorry. What?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3812" target="_blank">01:03:32.240</a></span> | <span class="t">Composition? Decomposition. Interesting. Okay. That's a very interesting point. Um, but no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3819" target="_blank">01:03:39.200</a></span> | <span class="t">Correct. I just, yes, that's correct. So I just lined it up such that it's easier to see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3826" target="_blank">01:03:46.400</a></span> | <span class="t">And if you take the matrix X and you multiply by WQ, you'll get Q, right? And this is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3833" target="_blank">01:03:53.600</a></span> | <span class="t">the correct maths, um, dimensions and stuff like that. Um, and so like, I like, I like to normally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3839" target="_blank">01:03:59.120</a></span> | <span class="t">tell people to like visualize transformers as maths. It's actually, in my view, it's easier. Okay. I'm not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3844" target="_blank">01:04:04.160</a></span> | <span class="t">sure for other people, but my view is easier. I do not like it when they say, oh, queries and like you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3849" target="_blank">01:04:09.680</a></span> | <span class="t">trying to do keys and values. I don't even know what that even means. Anyways. Um, and the yellow components</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3855" target="_blank">01:04:15.600</a></span> | <span class="t">are the ones you want to train. X is what you want to train. WQ is what you want to train. WK and WV.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3861" target="_blank">01:04:21.120</a></span> | <span class="t">And QKV are just the components afterwards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3864" target="_blank">01:04:24.880</a></span> | <span class="t">When you have the, so remember you have the Q, you have the K, all you need to do is when you do K</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3872" target="_blank">01:04:32.480</a></span> | <span class="t">transpose, you transpose the matrix and you do Q times K transpose and you get this big square matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3879" target="_blank">01:04:39.760</a></span> | <span class="t">called QK transpose, right? Hello, my name is Daniel and so on. Right? So like that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3884" target="_blank">01:04:44.080</a></span> | <span class="t">that's kind of what I want to visualize is like, you know, it's actually a, when you do QK times K</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3888" target="_blank">01:04:48.640</a></span> | <span class="t">transpose, you get a square matrix. Um, and all you need to do now is do the soft max divided by root</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3895" target="_blank">01:04:55.760</a></span> | <span class="t">D, right? So soft max, essentially each row, you normalize to one, right? The sum of the exponentials</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3901" target="_blank">01:05:01.200</a></span> | <span class="t">must be right. You need to like normalize them. Do you, does anyone know why you should do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3906" target="_blank">01:05:06.640</a></span> | <span class="t">And why should you use soft max?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3912" target="_blank">01:05:12.560</a></span> | <span class="t">Any clues? Why do? Yes. Yes. Okay. That's the answer. Yes. But like, why? What? Why?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3920" target="_blank">01:05:20.160</a></span> | <span class="t">Sorry. When you multiply them, you can get NANDs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3927" target="_blank">01:05:27.120</a></span> | <span class="t">Oh yes. Very good. Oh, that's okay. Do you know how to fix that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3932" target="_blank">01:05:32.960</a></span> | <span class="t">Close.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3938" target="_blank">01:05:38.400</a></span> | <span class="t">You have to minus the maximum of the row. That's how you fix it. Um, yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3942" target="_blank">01:05:42.080</a></span> | <span class="t">Oh yes. Very good. Okay. Yes. We want to sample from that. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3947" target="_blank">01:05:47.680</a></span> | <span class="t">Sample from that distribution. But what happens if you don't do the soft max?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3951" target="_blank">01:05:51.680</a></span> | <span class="t">Doesn't this still work or not? Like what happens if you just do QK transpose over root D?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3957" target="_blank">01:05:57.120</a></span> | <span class="t">Remove the soft max. Like why do I have to do soft max? Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3960" target="_blank">01:06:00.080</a></span> | <span class="t">Interesting. Then you can fix that with like minus max of the row as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3968" target="_blank">01:06:08.240</a></span> | <span class="t">We're exploding. Anyone else?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3969" target="_blank">01:06:09.440</a></span> | <span class="t">Okay. What happens if you don't have a non linearity then?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3974" target="_blank">01:06:14.240</a></span> | <span class="t">So does it have to be soft max? Can it be something else?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3978" target="_blank">01:06:18.480</a></span> | <span class="t">Yes. It could be. Yes. That is another active error research which people should focus on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3985" target="_blank">01:06:25.760</a></span> | <span class="t">which is like, why do we need to use soft max? Um, generally speaking,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3989" target="_blank">01:06:29.440</a></span> | <span class="t">research papers show that is actually the most accurate. Um, if you use other activation functions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3993" target="_blank">01:06:33.920</a></span> | <span class="t">it might actually not be that accurate. Right? So like, um, but this also is the bottleneck of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=3998" target="_blank">01:06:38.880</a></span> | <span class="t">transformers is because it's a soft max, it does the row sum of the exponentials. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4004" target="_blank">01:06:44.400</a></span> | <span class="t">this means that you can't actually decompose this, right? You can't actually bring the matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4008" target="_blank">01:06:48.960</a></span> | <span class="t">matrix multiplications out. Um, and so if someone can find ways to make this faster,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4013" target="_blank">01:06:53.920</a></span> | <span class="t">you know, you'll get like millions of dollars. Oh, okay. Maybe like much more than that. But, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4018" target="_blank">01:06:58.640</a></span> | <span class="t">um, yes. And V is just, remember the V comes from here, right? So we just take the V, multiply it up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4026" target="_blank">01:07:06.800</a></span> | <span class="t">again, and we get this matrix at the very end. And that is, right? Oh yeah. That, that is the final</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4032" target="_blank">01:07:12.640</a></span> | <span class="t">component, right? This, this empty box is what you get out from the attention mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4036" target="_blank">01:07:16.640</a></span> | <span class="t">For the layer norms, um, I don't really want to explain too much, but the layer norms essentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4043" target="_blank">01:07:23.120</a></span> | <span class="t">you take the, um, you take, you square all the elements per row, you sum them, you divide them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4048" target="_blank">01:07:28.560</a></span> | <span class="t">by the square root and you take the mean and they just do one divided by, right? All this does is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4053" target="_blank">01:07:33.440</a></span> | <span class="t">normalizes the rows to make it easier for the language model to learn, right? So like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4057" target="_blank">01:07:37.360</a></span> | <span class="t">why do people do layer norm? It just makes training easier. Um, it's more stable. There's no other,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4062" target="_blank">01:07:42.240</a></span> | <span class="t">like, there's no other like point. There are like some theories of like, you know, batch normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4066" target="_blank">01:07:46.240</a></span> | <span class="t">like, you know, um, you know, out of distribution, you want to make like shift towards the distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4071" target="_blank">01:07:51.040</a></span> | <span class="t">of the distribution data. I just like to think of this as an optimization method. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4076" target="_blank">01:07:56.080</a></span> | <span class="t">layer norms just make a training easier and more stable. Um, and layer norm is simply, remember,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4085" target="_blank">01:08:05.040</a></span> | <span class="t">as I said, is you take the X matrix, you do a row sum of all the squares and you take the mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4090" target="_blank">01:08:10.160</a></span> | <span class="t">And then you just divide it. And then you multiply by some weights. It's a vector of weights. And that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4096" target="_blank">01:08:16.080</a></span> | <span class="t">just layer norm. Um, you don't worry too much about like what is layer norm or what it does. It just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4100" target="_blank">01:08:20.080</a></span> | <span class="t">does training better, more stable. Um, please add as many layer norms as possible. Um, yes, add everywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4107" target="_blank">01:08:27.440</a></span> | <span class="t">Layer norms everywhere. Um, and you'll make training much better. Um, okay. I probably, okay. I don't know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4114" target="_blank">01:08:34.320</a></span> | <span class="t">if you can see this, but in Triton, right, in order to write Triton code for the layer norm, this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4119" target="_blank">01:08:39.360</a></span> | <span class="t">forward kernel. Um, we will not be talking about Triton today, but, um, it's actually not that complicated. If you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4125" target="_blank">01:08:45.840</a></span> | <span class="t">read more intently, um, ignore all of the like components, there was only very few lines for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4132" target="_blank">01:08:52.480</a></span> | <span class="t">the layer norm. Um, it's actually not that complicated. Um, the rest is simply just how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4136" target="_blank">01:08:56.400</a></span> | <span class="t">to load the data. Um, it's actually not that hard. Um, yeah, the backward kernel is when the problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4142" target="_blank">01:09:02.320</a></span> | <span class="t">comes. Um, how do we actually do the differentiation of the layer norms, right? Remember you have to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4148" target="_blank">01:09:08.960</a></span> | <span class="t">the w right is yellow. Um, you actually have to train that. Um, how do we find the derivatives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4155" target="_blank">01:09:15.600</a></span> | <span class="t">of the w? Um, it is very complicated. Um, and if you want to learn in your own time, you can have fun</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4163" target="_blank">01:09:23.440</a></span> | <span class="t">learning the derivatives. Um, it is extremely complicated because there is like sums. There is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4169" target="_blank">01:09:29.760</a></span> | <span class="t">um, you know, row sums. How do we do the derivative of a row sum? Um, it can get quite complex. I wanted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4176" target="_blank">01:09:36.640</a></span> | <span class="t">to talk about backpropagation today, but I thought like it's probably too heavy math. Um, so no backpropagation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4183" target="_blank">01:09:43.760</a></span> | <span class="t">but we'll be showing, but I do have tutorials on that. So if you go to the Triton tutorial, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4189" target="_blank">01:09:49.040</a></span> | <span class="t">I followed that. That's actually quite helpful. Um, and the backward kernel is just very, very problematic. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4196" target="_blank">01:09:56.240</a></span> | <span class="t">Now up to the rope embeddings. Why do we do rope embeddings? Does anybody know what is the rope embedding?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4201" target="_blank">01:10:01.920</a></span> | <span class="t">Yes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4206" target="_blank">01:10:06.080</a></span> | <span class="t">So you could use the rope embeddings to extend context. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4216" target="_blank">01:10:16.320</a></span> | <span class="t">Okay. Do you know how? How does it extend context?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4219" target="_blank">01:10:19.840</a></span> | <span class="t">So, well, how does it work for the yarn or like, or?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4224" target="_blank">01:10:24.080</a></span> | <span class="t">How would you use rope embeddings to extend context? How, what would you do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4226" target="_blank">01:10:26.880</a></span> | <span class="t">How would I, how would I do that? I would create a basically, what I would do is I would just create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4235" target="_blank">01:10:35.280</a></span> | <span class="t">kind of a, um, you just multiply the base by two and then you get two times longer context. You multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4241" target="_blank">01:10:41.920</a></span> | <span class="t">the base by 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4250" target="_blank">01:10:50.560</a></span> | <span class="t">Correct. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4251" target="_blank">01:10:51.440</a></span> | <span class="t">So that's where yarn might begin. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4253" target="_blank">01:10:53.920</a></span> | <span class="t">So is that the dynamic dynamic? Well, that's static or dynamic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4258" target="_blank">01:10:58.880</a></span> | <span class="t">Yeah. So how would you solve the problem if it's like, you want to train, if you want to have 1 million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4263" target="_blank">01:11:03.760</a></span> | <span class="t">context them, but your data set is only 1000 words. How would you solve that problem? How would you think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4268" target="_blank">01:11:08.880</a></span> | <span class="t">of solving that problem? Because like, some people have said they do 10 million context them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4273" target="_blank">01:11:13.680</a></span> | <span class="t">Is there any data sets which is 10 million tokens? Um, how would you?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4277" target="_blank">01:11:17.680</a></span> | <span class="t">It's 15 trillion tokens, 5 lines. So, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4280" target="_blank">01:11:20.800</a></span> | <span class="t">Oh, no, no, but that's 15 trillion tokens for like the data set. I mean, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4284" target="_blank">01:11:24.400</a></span> | <span class="t">how do we do long context? Remember, when you do long context training, you have to have a document,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4289" target="_blank">01:11:29.120</a></span> | <span class="t">which is at least 10 million words for it to learn how to predict the 10 million plus one token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4294" target="_blank">01:11:34.560</a></span> | <span class="t">So, um, how I would solve the problem would just be to gather better and more diverse data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4301" target="_blank">01:11:41.920</a></span> | <span class="t">Yes, that's the ideal. So what happens if there is no data set, which is 100 million tokens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4309" target="_blank">01:11:49.600</a></span> | <span class="t">Then what would you do? How would you synthesize if the model, it's like a chicken and egg problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4323" target="_blank">01:12:03.440</a></span> | <span class="t">How would you do synthesis? So, no, no, no. I would, I would basically, um, just create,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4330" target="_blank">01:12:10.160</a></span> | <span class="t">I would basically use like a quad or like any of the state of the art models with like a Laura and then get,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4337" target="_blank">01:12:17.840</a></span> | <span class="t">and then basically circulate the data. But are they trained on 10 million tokens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4341" target="_blank">01:12:21.520</a></span> | <span class="t">Huh? If the model itself wasn't trained on 10 million tokens, does it do long context?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4345" target="_blank">01:12:25.600</a></span> | <span class="t">So, if I was to try to solve this problem for like clients, for example, like let's say their code base is in 10 million tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4353" target="_blank">01:12:33.120</a></span> | <span class="t">or, or, or, you know, and they wanted 10 million specific context or whatever, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4357" target="_blank">01:12:37.840</a></span> | <span class="t">Then I would, um, basically like, uh, create a like, uh, synthetic data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4364" target="_blank">01:12:44.800</a></span> | <span class="t">So not synthetic, but a derived data set from what we have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4369" target="_blank">01:12:49.680</a></span> | <span class="t">Okay. Interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4370" target="_blank">01:12:50.800</a></span> | <span class="t">So assuming we do not have, but I can't assume that we have no data, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4375" target="_blank">01:12:55.040</a></span> | <span class="t">Good point. Okay. I don't know. I, I think it remains to be seen, like many claims by companies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4381" target="_blank">01:13:01.360</a></span> | <span class="t">10 million context, 100 million context. I question, question, question, question, question, question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4384" target="_blank">01:13:04.800</a></span> | <span class="t">Well, I've only seen one million actually work, so I mean, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4388" target="_blank">01:13:08.800</a></span> | <span class="t">And that's bringing attention to theory, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4392" target="_blank">01:13:12.000</a></span> | <span class="t">Okay. Okay. Okay. Now we're going into, okay. Yes. Okay. Yeah. Okay. No, no, no, that's fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4396" target="_blank">01:13:16.480</a></span> | <span class="t">I was asking the questions, but okay. Wait, the question was like, what is a rope embedding?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4399" target="_blank">01:13:19.920</a></span> | <span class="t">Someone who did mention like positions. What does that actually entail?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4403" target="_blank">01:13:23.600</a></span> | <span class="t">What do you think is the point of a rope embedding?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4407" target="_blank">01:13:27.840</a></span> | <span class="t">All it does is you want to tell the model to learn what is the position of the words, right? So like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4415" target="_blank">01:13:35.280</a></span> | <span class="t">hello, my name is Daniel. It actually has a meaning. Like, hello is like the first token, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4420" target="_blank">01:13:40.800</a></span> | <span class="t">But then if you put hello as a third token, what's the difference? There is a difference, right? So like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4426" target="_blank">01:13:46.640</a></span> | <span class="t">depending on where the word is in the sentence, it matters. So the whole point of embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4432" target="_blank">01:13:52.400</a></span> | <span class="t">rope embeddings is that it tells your model to learn where is the position of this component.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4438" target="_blank">01:13:58.160</a></span> | <span class="t">And old style, they use absolute, like relative, like, you know, absolute positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4445" target="_blank">01:14:05.920</a></span> | <span class="t">Rope embeddings does like some special tricks, like, you know, times a cosine, plus times a sine,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4451" target="_blank">01:14:11.440</a></span> | <span class="t">and does some sort of like special rotation and stuff like that. The paper found that if you do rope</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4456" target="_blank">01:14:16.720</a></span> | <span class="t">embeddings, it actually has high accuracy. And, you know, everyone does rope embeddings now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4461" target="_blank">01:14:21.600</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4464" target="_blank">01:14:24.320</a></span> | <span class="t">So why do you use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4471" target="_blank">01:14:31.040</a></span> | <span class="t">You mean lower, sorry. Yes, there is. I think bird did not. I don't know. Did bird use rope? I don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4482" target="_blank">01:14:42.080</a></span> | <span class="t">think so. But use absolute. Yes, that's the problem. I think but use absolute, I think. I don't remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4488" target="_blank">01:14:48.640</a></span> | <span class="t">anymore, but oh, oh, yes, yes, exactly. So rope did not exist. Yeah. And so like this paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4494" target="_blank">01:14:54.320</a></span> | <span class="t">the reformer paper shows. So previously people use absolute position encodings, which simply just adds a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4499" target="_blank">01:14:59.840</a></span> | <span class="t">position. Like you can literally just add, like if the position is one, zero, just add zero. If the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4504" target="_blank">01:15:04.800</a></span> | <span class="t">position is one, add one. If the position is two, just add two. That's literally what they do. Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4510" target="_blank">01:15:10.080</a></span> | <span class="t">actually, well, not exactly, but like, you know what I mean, right? You have to divide it by some sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4514" target="_blank">01:15:14.480</a></span> | <span class="t">normalizing factor, right? If the position is 30,000, don't add 30,000, right? You would like destroy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4519" target="_blank">01:15:19.440</a></span> | <span class="t">training. But that's kind of what they do. And what they show is if you do rope, you can essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4525" target="_blank">01:15:25.680</a></span> | <span class="t">increase accuracy somehow. And we just use this as gospel. We just treat this as true. And everyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4533" target="_blank">01:15:33.760</a></span> | <span class="t">uses rope now. Um, yeah. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4536" target="_blank">01:15:36.800</a></span> | <span class="t">In that case, do you have an opinion on yarn versus rope versus and yarn--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4542" target="_blank">01:15:42.640</a></span> | <span class="t">So yarn is kind of rope. So yarn just does-- I'm assuming yarn is rope, but it does--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4548" target="_blank">01:15:48.640</a></span> | <span class="t">It does like-- actually, I don't think I should comment on this because I'm not an expert on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4552" target="_blank">01:15:52.640</a></span> | <span class="t">Okay. Doesn't it does like-- it does-- yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4554" target="_blank">01:15:54.880</a></span> | <span class="t">I mean, not as-- so I'm no expert on-- obviously, I--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4559" target="_blank">01:15:59.040</a></span> | <span class="t">I'm not expert on long context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4560" target="_blank">01:16:00.640</a></span> | <span class="t">Like, on rope for the yarn, but since VLM only supports static yarn,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4566" target="_blank">01:16:06.800</a></span> | <span class="t">unless you're planning on trying to go to one million context, it's actually kind of terrible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4572" target="_blank">01:16:12.720</a></span> | <span class="t">I-- is yarn the one which it does, like, the base, like, randomly changes? Like, if you have position--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4579" target="_blank">01:16:19.360</a></span> | <span class="t">if you have, like, up to one million context, and you do one million and one context,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4584" target="_blank">01:16:24.560</a></span> | <span class="t">the base changes with that, like, the factor changes. Is that yarn? Dynamic changing. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4589" target="_blank">01:16:29.600</a></span> | <span class="t">So, and that's the issue is that in short-- in, like, let's say, five-shot, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4593" target="_blank">01:16:33.760</a></span> | <span class="t">That's not even-- a short five-shot, right? That's not anywhere close to one million context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4599" target="_blank">01:16:39.200</a></span> | <span class="t">In theory, or maybe, like, let's say, a three-year shot or two-shot, um, like, so--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4604" target="_blank">01:16:44.720</a></span> | <span class="t">Did it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4605" target="_blank">01:16:45.760</a></span> | <span class="t">And, but, obviously, dynamic yarn, in theory, could fix this rope issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4611" target="_blank">01:16:51.200</a></span> | <span class="t">Uh-huh. Where, like, we just take this rope as gospel, where, um, but are you following kind of my question here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4620" target="_blank">01:17:00.240</a></span> | <span class="t">Yes, I-- no, I think dynamic yarn is just rope, though. Like, okay, that's weird.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4624" target="_blank">01:17:04.560</a></span> | <span class="t">Okay, maybe I'll unplug and re-plug. So, like, the screen kind of went away.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4628" target="_blank">01:17:08.880</a></span> | <span class="t">Let me just read this-- do this again. Is this, like-- is it-- is it-- is it not-- not working? Or--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4636" target="_blank">01:17:16.240</a></span> | <span class="t">Is it, like, screen? Or-- no screen? That's weird.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4640" target="_blank">01:17:20.880</a></span> | <span class="t">Can you answer questions while we're at it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4645" target="_blank">01:17:25.120</a></span> | <span class="t">Oh, yes, okay. Yes, I'll answer some questions. Um, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4649" target="_blank">01:17:29.200</a></span> | <span class="t">Anyone else have questions? Wait, okay. Wait. I need to read your brush.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4655" target="_blank">01:17:35.120</a></span> | <span class="t">Oh, if anyone has, like, take a break, you can take a break now, if you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4658" target="_blank">01:17:38.480</a></span> | <span class="t">Um, and if you have, like, other questions-- yes. Okay, question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4670" target="_blank">01:17:50.320</a></span> | <span class="t">Sorry, sorry. Yan is--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4672" target="_blank">01:17:52.320</a></span> | <span class="t">So, I don't know what yarn is, but I will hear some talk about, like, extending the context window,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4679" target="_blank">01:17:59.040</a></span> | <span class="t">and I'm just wondering that, like, what are the keywords that do when it comes to, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4684" target="_blank">01:18:04.320</a></span> | <span class="t">the context window, like, .</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4688" target="_blank">01:18:08.720</a></span> | <span class="t">So-- so the question is for context windows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4695" target="_blank">01:18:15.440</a></span> | <span class="t">Like, what is-- what is-- what is the improvement like yarn?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4697" target="_blank">01:18:17.920</a></span> | <span class="t">Oh, okay. What is the improvement? Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4701" target="_blank">01:18:21.440</a></span> | <span class="t">So, so the point of yarn-- like, what we were talking about is, like, how do we make a language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4705" target="_blank">01:18:25.360</a></span> | <span class="t">model learn-- how do we make it do long context without training on long context? Kind of. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4713" target="_blank">01:18:33.360</a></span> | <span class="t">And so, like, what yarn does is you can essentially extend the context window automatically by dividing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4719" target="_blank">01:18:39.360</a></span> | <span class="t">the base of the rope embeddings. You change the scale factor. I don't actually have slides for this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4724" target="_blank">01:18:44.800</a></span> | <span class="t">but it's just a methodology which allows you to scale the factor, and you essentially magically make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4730" target="_blank">01:18:50.960</a></span> | <span class="t">the model learn new context, long context. That's kind of what yarn does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4735" target="_blank">01:18:55.200</a></span> | <span class="t">Yes, it's extremely-- if you do one million context length, then your O of n squared is one million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4743" target="_blank">01:19:03.920</a></span> | <span class="t">squared, which is horrible. There are, like, some other methodologies, like, you know, they want to do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4748" target="_blank">01:19:08.800</a></span> | <span class="t">like, linear transformers, and, like, you know, yes, I guess you could try that, but I don't suggest that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4753" target="_blank">01:19:13.760</a></span> | <span class="t">Um, yeah. So, hope-- yeah. Yeah, sorry, question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4758" target="_blank">01:19:18.000</a></span> | <span class="t">What is the difference between in-context learning and fine-tuning, or what's the benefit?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4770" target="_blank">01:19:30.320</a></span> | <span class="t">That is a good question. So, fine-tuning changes the grade-- you have gradients for fine-tuning. So, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4778" target="_blank">01:19:38.400</a></span> | <span class="t">I think it depends. I think I would do in-context learning first. So, if you have, like, few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4790" target="_blank">01:19:50.000</a></span> | <span class="t">sharp prompts, you shove it in to see if it works. But I would still resort to fine-tuning if you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4794" target="_blank">01:19:54.720</a></span> | <span class="t">to be more efficient. And if your model doesn't seem to be learning, then you have to go back to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4798" target="_blank">01:19:58.640</a></span> | <span class="t">fine-tuning. There was a paper which was released yesterday, I think? Was it yesterday? It showed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4802" target="_blank">01:20:02.960</a></span> | <span class="t">that in-context learning is very useful. And it, like, learns kind of, like, how to be, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4808" target="_blank">01:20:08.240</a></span> | <span class="t">a random forest or, like, a tree. I think it was yesterday. The paper. That was quite useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4812" target="_blank">01:20:12.560</a></span> | <span class="t">Very interesting. But I think fine-tuning is still very important. Especially if your model is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4818" target="_blank">01:20:18.320</a></span> | <span class="t">learning anything and it doesn't seem to be working, then you have to, like, use fine-tuning to change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4822" target="_blank">01:20:22.000</a></span> | <span class="t">your behavior. I don't really have a comment on this. Like, I just feel like you should do everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4827" target="_blank">01:20:27.440</a></span> | <span class="t">and try everything. Um, yeah. Any other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4830" target="_blank">01:20:30.080</a></span> | <span class="t">Well, I have a couple of my apps. I don't know if, uh...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4835" target="_blank">01:20:35.600</a></span> | <span class="t">Oh, did you... Okay. I think my app kind of glitched.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4838" target="_blank">01:20:38.480</a></span> | <span class="t">Do you want me to read them off one by one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4842" target="_blank">01:20:42.400</a></span> | <span class="t">Oh, maybe I'll take... Okay. Wait. Yeah, wait.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4845" target="_blank">01:20:45.040</a></span> | <span class="t">I'm sorry, I asked. Okay. Okay. Okay. Maybe... Okay. Let's just... I'll continue,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4849" target="_blank">01:20:49.280</a></span> | <span class="t">and then we will do the questions. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4850" target="_blank">01:20:50.960</a></span> | <span class="t">Yeah. Okay. I probably have to... Okay. Okay. I did not see time. I've been speaking to...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4856" target="_blank">01:20:56.240</a></span> | <span class="t">Okay. Anyways, that is the rope kernel. Um, and it might look horrifying. It's actually not that bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4862" target="_blank">01:21:02.640</a></span> | <span class="t">Um, it's literally just the formula that it did. Q times cosine plus Q times a rotation matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4869" target="_blank">01:21:09.840</a></span> | <span class="t">times sine. And that is just rope. It's actually not that hard. Um, just the code is a bit more annoying,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4876" target="_blank">01:21:16.320</a></span> | <span class="t">but it's just like moving the data. It's just data moving and data moving data and stuff like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4881" target="_blank">01:21:21.440</a></span> | <span class="t">All of the code is just related to data movement. So not that complicated. Um, the most complicated part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4887" target="_blank">01:21:27.760</a></span> | <span class="t">is the derivatives for rope. Um, and you have to use something called rotate half, which essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4895" target="_blank">01:21:35.040</a></span> | <span class="t">rotates half of the... So, okay, just read the code. It's like minus X2 concatenated with X1. So you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4901" target="_blank">01:21:41.280</a></span> | <span class="t">like, you essentially take the matrix X, you divide it by two, you take the first half, you put on the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4905" target="_blank">01:21:45.280</a></span> | <span class="t">half and you put, you switch the ordering and you, the first half becomes minus. And the code generally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4911" target="_blank">01:21:51.120</a></span> | <span class="t">is reasonably well, hopefully, um, for understanding. But the question is, as you like, this is hugging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4916" target="_blank">01:21:56.880</a></span> | <span class="t">face code, right? So like Q times cosine plus rotate half Q times sine. The question is, how do we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4922" target="_blank">01:22:02.080</a></span> | <span class="t">actually take, find the derivatives of this? Um, this was actually very, a very complicated phenomenon,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4927" target="_blank">01:22:07.200</a></span> | <span class="t">because I could see many implementations not doing this correctly. Um, and it is very special,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4934" target="_blank">01:22:14.160</a></span> | <span class="t">the derivative. Um, simply, if you notice, is rotate half, the function is literally a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4941" target="_blank">01:22:21.120</a></span> | <span class="t">multiplication, right? It's Q times R, where R is a rotation matrix. And the rotation matrix is minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4947" target="_blank">01:22:27.440</a></span> | <span class="t">identity and identity, um, and zeros on the diagonal. Um, and if you do this, it's because it's a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4954" target="_blank">01:22:34.000</a></span> | <span class="t">notation now. Simply the derivative is the transpose. And so if you do the transpose,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4958" target="_blank">01:22:38.880</a></span> | <span class="t">the minus sign just flips. Um, and if the minus sign flips, it's literally the same as your previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4965" target="_blank">01:22:45.760</a></span> | <span class="t">example, just for the minus. Um, I can probably like explain this too quickly. Um, but the point is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4972" target="_blank">01:22:52.960</a></span> | <span class="t">if you do matrix multiplication, you can derive derivatives very simply. My suggestion is shove the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4978" target="_blank">01:22:58.720</a></span> | <span class="t">derivatives and will from Africa or like, you know, your favorite tool, you can use chat as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4983" target="_blank">01:23:03.920</a></span> | <span class="t">And you will get the derivatives back, but you must put it in the form that's useful for the computer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4989" target="_blank">01:23:09.120</a></span> | <span class="t">disease. Um, now we've been talking about the MLP component, right? So we completed the attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=4996" target="_blank">01:23:16.400</a></span> | <span class="t">we completed the rope, the layer norms. The MLP is just a mixing component, right? So it's an activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5003" target="_blank">01:23:23.200</a></span> | <span class="t">function times some weights, you know, multiply some of the other weights and stuff like that. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5008" target="_blank">01:23:28.560</a></span> | <span class="t">all it does is it mixes the signals to make it like, you know, more fancy. Um, you do in theory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5013" target="_blank">01:23:33.840</a></span> | <span class="t">you don't actually need the MLP component. Um, like most attention, you don't actually need this part,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5019" target="_blank">01:23:39.040</a></span> | <span class="t">but you must put it for the model to have more freedom to learn. Um, and you know, the famous paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5025" target="_blank">01:23:45.360</a></span> | <span class="t">um, glue variants, improve transformer, um, you know, very famous author, I'm assuming most people know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5032" target="_blank">01:23:52.800</a></span> | <span class="t">him. Um, but he showed that, um, if you add glue, um, sweet glue and all these other variants, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5040" target="_blank">01:24:00.960</a></span> | <span class="t">you can actually increase accuracy. Um, once again, this is just treated as fact in the machine learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5046" target="_blank">01:24:06.240</a></span> | <span class="t">community. Um, we should be doing more experiments, um, than just using this methodology, but you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5051" target="_blank">01:24:11.840</a></span> | <span class="t">we just treat it as fact. And this is in transformer plus plus, um, the architecture, which everyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5056" target="_blank">01:24:16.080</a></span> | <span class="t">uses. Um, there is a very big difference though. Um, in GPT two, in GPT two, they don't use the glue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5064" target="_blank">01:24:24.880</a></span> | <span class="t">variants. Um, they simply just use a normal MLP, right? So x times the weights up, do some sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5071" target="_blank">01:24:31.280</a></span> | <span class="t">activation function, and then you down project it. So the weights down, right? So that's, but then if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5076" target="_blank">01:24:36.240</a></span> | <span class="t">do sweet glue and these new variants, the glue variants, you essentially add this component</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5081" target="_blank">01:24:41.440</a></span> | <span class="t">where you do element wise multiplication, um, and then you do a gate a down projection. So it's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5086" target="_blank">01:24:46.800</a></span> | <span class="t">very similar to the GPT two architecture. You just add an extra component. Um, and so I try to like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5092" target="_blank">01:24:52.080</a></span> | <span class="t">there's also like a naming change, like up and down, um, up and gate and up like, you know, changes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5097" target="_blank">01:24:57.120</a></span> | <span class="t">and stuff like that. But in general, it's, you can see, it's very similar. Um, just the extra element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5101" target="_blank">01:25:01.840</a></span> | <span class="t">wise multiplication component. Um, yeah. But there is like a new, um, the Numitron paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5109" target="_blank">01:25:09.120</a></span> | <span class="t">for example, oh, sorry, the Numitron, the new model by NVIDIA did not use glue. And instead,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5114" target="_blank">01:25:14.480</a></span> | <span class="t">they use squared value, right? And they showed that you don't actually have to do glue anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5120" target="_blank">01:25:20.480</a></span> | <span class="t">You can just use squared value and it seems to do okay. Um, although it remains to be seen if actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5125" target="_blank">01:25:25.920</a></span> | <span class="t">is good, but yeah, so like they showed that if you do squared value, you can remove this. You can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5131" target="_blank">01:25:31.680</a></span> | <span class="t">essentially go back to the GPT two architecture, right? You don't need to use Lama architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5135" target="_blank">01:25:35.440</a></span> | <span class="t">anymore. So Lama and Mistral Gemma all used the second equation. GPT two use the first equation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5141" target="_blank">01:25:41.280</a></span> | <span class="t">You can go back to the first equation, but the trick is the F must be very special. And that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5146" target="_blank">01:25:46.000</a></span> | <span class="t">called squared regular value. And they show that if you do this, your accuracy does not degrade that much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5152" target="_blank">01:25:52.560</a></span> | <span class="t">And so, yes, the paper. And interestingly, if you see one of the authors, it's the same author.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5157" target="_blank">01:25:57.840</a></span> | <span class="t">One of the authors' names is similar. Um, you know, the name is the same. So, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5163" target="_blank">01:26:03.440</a></span> | <span class="t">they were also the ones to showcase the squared value. You don't need to do glue anymore. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5168" target="_blank">01:26:08.960</a></span> | <span class="t">and you can simply just use squared value as well. There is a research paper which I highly suggest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5177" target="_blank">01:26:17.440</a></span> | <span class="t">people to read and it's called the physics of language models. Um, it is extremely long though,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5182" target="_blank">01:26:22.720</a></span> | <span class="t">but it has many nuggets inside and I highly suggest people to read this. Um, they show example. They</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5189" target="_blank">01:26:29.440</a></span> | <span class="t">actually did so many testing tests and experiments and they showed that if you used glue variants or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5195" target="_blank">01:26:35.280</a></span> | <span class="t">gated MLP, it actually reduces the models capacity to learn on small models. Okay. So that's the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5201" target="_blank">01:26:41.360</a></span> | <span class="t">that's the point. It's on small models. On small models, on small models, if you do GPT-2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5206" target="_blank">01:26:46.320</a></span> | <span class="t">the first formula, it does better than if you do Lama, Mistral, Gemma, the second formula.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5211" target="_blank">01:26:51.120</a></span> | <span class="t">Only on small models, right? That's the point. Only on small models. Um, and there are other special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5216" target="_blank">01:26:56.960</a></span> | <span class="t">things inside the paper, which I highly suggest. Um, it's extremely useful. For example, they say that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5221" target="_blank">01:27:01.920</a></span> | <span class="t">if you do mod, if you change, if you don't, if you like, for example, the activation function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5225" target="_blank">01:27:05.200</a></span> | <span class="t">which activation function did you use? Um, should you use the sily or jelly or rally or whatever?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5231" target="_blank">01:27:11.680</a></span> | <span class="t">That's not that important, right? So like, it's not, not that important. If you use like biases,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5235" target="_blank">01:27:15.440</a></span> | <span class="t">oh, it's not that important. Um, so like, there's so many different things that you don't need to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5239" target="_blank">01:27:19.440</a></span> | <span class="t">Um, the paper shows, but you know, people just treat it as gospel or we have to use this specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5245" target="_blank">01:27:25.760</a></span> | <span class="t">component. Um, my suggestion is, you know, we should do more testing in the AI space, like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5251" target="_blank">01:27:31.280</a></span> | <span class="t">which variants are the most important, but I think this paper is pretty useful. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5255" target="_blank">01:27:35.200</a></span> | <span class="t">the code for like the, you know, the swigler kernel is, that's the forward kernels. Again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5263" target="_blank">01:27:43.680</a></span> | <span class="t">it's not that complicated to like the second part is actually useless. It's the swigler kernel is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5268" target="_blank">01:27:48.080</a></span> | <span class="t">literally three lines. It's just, it's just the three lines which are commented. The rest is just data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5273" target="_blank">01:27:53.280</a></span> | <span class="t">loading. How do we actually load the data into the GPU? Or it's not that important. Um, and you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5278" target="_blank">01:27:58.640</a></span> | <span class="t">if you use Torch to compile now, you can simply generate these kernels automatically. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5282" target="_blank">01:28:02.720</a></span> | <span class="t">and this makes your training much faster. So that's what I suggest people to do. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5286" target="_blank">01:28:06.080</a></span> | <span class="t">just use Torch to compile. You don't have to re rewrite, try to kernels. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5302" target="_blank">01:28:22.640</a></span> | <span class="t">Sorry. W W K. Oh, gate. Oh, it's just another matrix. So w gate, w up, you train this, um, and w down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5312" target="_blank">01:28:32.480</a></span> | <span class="t">These are all you train them. W down, w up. These are all you train. They're just numbers. Um, so like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5318" target="_blank">01:28:38.400</a></span> | <span class="t">matrices X times w gate is like, it's, so remember the, um, uh, this thing, right. W, this is for attention. W, uh, x times w, q, w, k, w, v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5332" target="_blank">01:28:52.000</a></span> | <span class="t">Assume it's just w gate, w up and w down. Um, and it's the same thing. So you just train these. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5340" target="_blank">01:29:00.560</a></span> | <span class="t">does that kind of answer your question or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5348" target="_blank">01:29:08.720</a></span> | <span class="t">Oh no, no, no, no. It's just a naming convention. W up is just, it's just, it's just, it's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5354" target="_blank">01:29:14.560</a></span> | <span class="t">called upper protection. The naming convention is like w b w a w c. It's just a naming convention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5360" target="_blank">01:29:20.480</a></span> | <span class="t">that people like to use w up w gates and w down. Um, they do have meetings. So w down is a down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5365" target="_blank">01:29:25.680</a></span> | <span class="t">projection. Up means up projection. Um, so essentially you take the matrix and you like make it larger and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5371" target="_blank">01:29:31.600</a></span> | <span class="t">then you project back to a smaller version. It just makes the model better to like, you know, make the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5375" target="_blank">01:29:35.600</a></span> | <span class="t">the model has have capacity to learn. Um, so it's just a naming thing. Um, any other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5381" target="_blank">01:29:41.440</a></span> | <span class="t">Okay. Um, the, as I said, like before, the derivatives are always a pain. If you did the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5391" target="_blank">01:29:51.360</a></span> | <span class="t">derivatives of Swigaloo, it is a nightmare to do, and I do not suggest you to do this. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5396" target="_blank">01:29:56.320</a></span> | <span class="t">but I had to manually do all the, you can see all my comments. So like the comments are actually there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5400" target="_blank">01:30:00.320</a></span> | <span class="t">If you do see more carefully, I wrote it in math formulas of how to actually take the derivatives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5404" target="_blank">01:30:04.960</a></span> | <span class="t">and it's extremely painful. Um, I highly suggest you not to inflict pain on yourself by doing this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5411" target="_blank">01:30:11.280</a></span> | <span class="t">Um, it took me many days to do so. Do not, I don't suggest this. Um, yes question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5421" target="_blank">01:30:21.120</a></span> | <span class="t">That is a very good question. So I use Desmos. Um, so Desmos is a graphing, graphing, online graphing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5427" target="_blank">01:30:27.360</a></span> | <span class="t">calculator. You type all these equations in and then you can see, does the graph align? Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5432" target="_blank">01:30:32.480</a></span> | <span class="t">Oh, every single component, you'll have to be careful. So every single component, you have to check.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5440" target="_blank">01:30:40.640</a></span> | <span class="t">Um, oh, is this component correct? Is this component correct? Check all of them. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5446" target="_blank">01:30:46.160</a></span> | <span class="t">And so like I normally, so like you isolate each component separately and then test it. I'll talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5451" target="_blank">01:30:51.520</a></span> | <span class="t">about that actually. Yeah. Um, any other questions though? Yes. Okay. Um, and this is the cross entropy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5458" target="_blank">01:30:58.320</a></span> | <span class="t">loss kernel. I'll probably just skip this. Don't have enough time. So, um, I wrote this as if you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5464" target="_blank">01:31:04.560</a></span> | <span class="t">to inspect the formulas and stuff like that, how do we do the derivatives for this? Um, you can do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5470" target="_blank">01:31:10.400</a></span> | <span class="t">It's not that complicated. Um, actually it is very complicated. Um, I did spend a lot of time trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5476" target="_blank">01:31:16.240</a></span> | <span class="t">to like work out the derivatives. Um, uh, it might be a bit foreign for some people for the derivatives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5482" target="_blank">01:31:22.640</a></span> | <span class="t">The main reason why it gets complicated is when there's sums, right? For like, whenever there's sums,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5488" target="_blank">01:31:28.400</a></span> | <span class="t">I just like, you know, doing derivatives when the sums is always painful. Um, matrix differential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5494" target="_blank">01:31:34.720</a></span> | <span class="t">is actually very easy. If you do X times W, the derivative of W is just X transpose. It's very simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5499" target="_blank">01:31:39.760</a></span> | <span class="t">But if you do derivatives when the sums, uh, it's horrible. Um, yeah, it's quite horrible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5506" target="_blank">01:31:46.880</a></span> | <span class="t">What you can do for the sum when you do derivatives, if you transform the sum into a matrix also matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5512" target="_blank">01:31:52.720</a></span> | <span class="t">multiplication, right? So a sum is just X times, and not like a vector of all ones. So that's called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5517" target="_blank">01:31:57.760</a></span> | <span class="t">the bro sum. And essentially, if you do this, you can actually make differentiation much easier. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5521" target="_blank">01:32:01.600</a></span> | <span class="t">but I won't be talking about that. Um, that's for another topic. Um, and someone was talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5527" target="_blank">01:32:07.040</a></span> | <span class="t">stability for soft max. If you minus the maximum of the road, you can make soft max much more stable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5533" target="_blank">01:32:13.120</a></span> | <span class="t">Um, and this is to like reduce exponentials of large numbers. And then you like, essentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5538" target="_blank">01:32:18.560</a></span> | <span class="t">it takes over the entire exponential, right? And so like, if you do this trick, when you minus the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5543" target="_blank">01:32:23.760</a></span> | <span class="t">maximum of the road, this makes training much more stable. Um, always do this. Um, yeah, always do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5550" target="_blank">01:32:30.640</a></span> | <span class="t">Um, yeah, and that's the code for the forward. Um, not that important. Oh yes, I wrote the code for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5556" target="_blank">01:32:36.720</a></span> | <span class="t">backward as well. Um, always say this is the, sorry, this is the forward, um, the forward, um, and it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5564" target="_blank">01:32:44.080</a></span> | <span class="t">quite long, but I wrote all of this down for your own leisure. If you want to read and implement this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5569" target="_blank">01:32:49.760</a></span> | <span class="t">have fun. Um, but I wrote this step by step, right? So like take Y is equal to log sum of X, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5575" target="_blank">01:32:55.600</a></span> | <span class="t">then I simplified it out. Like, you know, if you, if you exponentiate both sides, right, you can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5580" target="_blank">01:33:00.960</a></span> | <span class="t">exponential of the Y is equal to the sum of it, sum of exponentials and so on. All right. And so like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5585" target="_blank">01:33:05.520</a></span> | <span class="t">I wrote this all down, um, for the, but there is a methodology which we showed in unsloth as you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5592" target="_blank">01:33:12.080</a></span> | <span class="t">use chunked cross-interprey. And this is actually very helpful for large models. Um, your logits are very large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5599" target="_blank">01:33:19.760</a></span> | <span class="t">So if you chunk them, you can actually, you can make multi-threading much better for the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5604" target="_blank">01:33:24.320</a></span> | <span class="t">And so like the problem though, is like the derivatives, the forward propagation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5607" target="_blank">01:33:27.840</a></span> | <span class="t">you have to be careful now when you do chunking. Um, so like essentially you divide these into slivers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5612" target="_blank">01:33:32.240</a></span> | <span class="t">and parallelize each component. Um, I also wrote some, you know, maths and stuff like that for you to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5618" target="_blank">01:33:38.960</a></span> | <span class="t">review. Um, and how you actually do the chunk sum is very interesting. Um, the chunks, the log chunk sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5626" target="_blank">01:33:46.000</a></span> | <span class="t">of the log sum exponentials is just the log sum exponentials. Um, there's like, you have to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5629" target="_blank">01:33:49.920</a></span> | <span class="t">some manipulation and it's actually very interesting. Um, you don't actually need to change that much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5634" target="_blank">01:33:54.000</a></span> | <span class="t">code to make a work. Um, okay. Now we'll be going to the next component, which is to investigate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5641" target="_blank">01:34:01.360</a></span> | <span class="t">llama architecture. Um, hopefully this works. Yes. Question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5646" target="_blank">01:34:06.400</a></span> | <span class="t">It is part of Unsloft currently. Um, I think I heard that the PyTorch team will be including this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5660" target="_blank">01:34:20.320</a></span> | <span class="t">in Torsha compile, although I'm not a hundred percent sure. Um, this reduces memory. This just makes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5665" target="_blank">01:34:25.040</a></span> | <span class="t">this makes long, large context, uh, sorry, large vocabulary sizes work. So the biggest issue why you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5670" target="_blank">01:34:30.480</a></span> | <span class="t">have to do chunking is CUDA, um, NVIDIA GPUs has a limit. Six, five, five, three, six. I think that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5676" target="_blank">01:34:36.400</a></span> | <span class="t">two to the power of 16. I think so. Yeah. Yeah. So there is a limit. And so if you go, if your vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5682" target="_blank">01:34:42.480</a></span> | <span class="t">size is larger than six, five, five, three, six, you must do chunking. Um, yeah, you have to do chunking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5688" target="_blank">01:34:48.080</a></span> | <span class="t">So like if you have like, I think was a Gemma 128,000 or them. I'm getting confused. I think it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5693" target="_blank">01:34:53.680</a></span> | <span class="t">128,000. You have to divide it into two. Um, and so like your chunks would be two. Um, so it should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5698" target="_blank">01:34:58.720</a></span> | <span class="t">be in PyTorch in the future release maybe. Um, yeah. So now we learned all of this. Now you can read</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5707" target="_blank">01:35:07.600</a></span> | <span class="t">the code for llama. So if you go to, um, if you go to the code by modeling llama dot py. Um, it should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5716" target="_blank">01:35:16.160</a></span> | <span class="t">be in the slides, but you can also type in Google like modeling, uh, you know, modeling py. Um, this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5723" target="_blank">01:35:23.280</a></span> | <span class="t">right. There is your, if you go to line 94, you have your room embedding, which we talked about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5730" target="_blank">01:35:30.240</a></span> | <span class="t">right? Just assume, don't worry. Oh, oh, okay. I need to, okay. I probably use my mouse. Yeah. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5736" target="_blank">01:35:36.160</a></span> | <span class="t">I don't really like how GitHub does the, it's kind of annoying sometimes, but I think I can disable it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5741" target="_blank">01:35:41.200</a></span> | <span class="t">if I log in anyways, the symbols. Um, so if you go to this, right, this is not important. If you go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5748" target="_blank">01:35:48.640</a></span> | <span class="t">to the first one is line 74, the llama RMS norm, right? This is the layer norm kernel, right? This</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5754" target="_blank">01:35:54.800</a></span> | <span class="t">is the code for the layer norm kernel. It's just this much. Um, right. This is the, you take the row,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5760" target="_blank">01:36:00.640</a></span> | <span class="t">you take the sum of the, you take the squares of the, like each row,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5764" target="_blank">01:36:04.640</a></span> | <span class="t">you sum them, you divide it by the mean. Remember, this is the only reason why you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5767" target="_blank">01:36:07.840</a></span> | <span class="t">do layer norm is to make training more stable. Um, and it's actually not that complicated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5771" target="_blank">01:36:11.920</a></span> | <span class="t">It's just these few lines. Um, the rest as the rest is just bloat code for like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5776" target="_blank">01:36:16.960</a></span> | <span class="t">you know, you have to set stuff up. You have to do random initialization, blah, blah, blah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5780" target="_blank">01:36:20.480</a></span> | <span class="t">comments and stuff like that. Um, but that's the layer norm kernel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5786" target="_blank">01:36:26.400</a></span> | <span class="t">The rope kernel, the rotary embedding is a rope kernel. This is just setting stuff up. This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5791" target="_blank">01:36:31.760</a></span> | <span class="t">setting stuff up, um, forward, right? This is the most important component, which is the forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5798" target="_blank">01:36:38.000</a></span> | <span class="t">component. Now, don't get scared by this. That's because we fixed a bug. So this is actually our bug</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5803" target="_blank">01:36:43.120</a></span> | <span class="t">fix that we did. So this is all in transformer architectures is like, you have to be very careful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5807" target="_blank">01:36:47.200</a></span> | <span class="t">when you downcast to float 16. If you use B float 16, right? This is actually very important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5812" target="_blank">01:36:52.320</a></span> | <span class="t">Before you have to be very, very careful when you use float 16 and B float 16 training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5817" target="_blank">01:36:57.760</a></span> | <span class="t">mixed position training, because you were downcast incorrectly and your rope embeddings will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5822" target="_blank">01:37:02.320</a></span> | <span class="t">wrong. Um, so it's actually, it's not supposed to look as ugly as this, but unfortunately it looks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5827" target="_blank">01:37:07.440</a></span> | <span class="t">ugly now. Um, before it was just this, right? But this is just setting up code, um, that, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5834" target="_blank">01:37:14.720</a></span> | <span class="t">we have to like fix the bug and stuff like that. Um, yeah. So it's actually not that long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5839" target="_blank">01:37:19.360</a></span> | <span class="t">Right. So like, it's actually, it's just this, um, yeah, up to here. It's the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5844" target="_blank">01:37:24.560</a></span> | <span class="t">so whenever you see these architectures, it just looks complicated, but it's like, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5848" target="_blank">01:37:28.240</a></span> | <span class="t">No, no, no. So there are like some, if you do torture compile, we're still like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5858" target="_blank">01:37:38.880</a></span> | <span class="t">I think 30, 30% faster. So no matter if, sorry, if you use torture compile plus unsloth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5865" target="_blank">01:37:45.600</a></span> | <span class="t">Oh, it's, we're still 30 times faster. Oh, not 30, 30%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5873" target="_blank">01:37:53.520</a></span> | <span class="t">So we're two times faster than hugging face plus flash attention to, but torture compile,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5878" target="_blank">01:37:58.080</a></span> | <span class="t">you know, they're adding kernels from multiple packages in. So yes, they could learn from unsloth and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5883" target="_blank">01:38:03.440</a></span> | <span class="t">put it in. I'm assuming they're doing that. I did have a talk with them. So yes, they're probably doing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5888" target="_blank">01:38:08.160</a></span> | <span class="t">already. But yes, we're still 30% faster. Um, there was like some, I think someone tested this last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5892" target="_blank">01:38:12.800</a></span> | <span class="t">week. Um, yeah, does that any other questions? Okay. This is not important. This is for, this is the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5903" target="_blank">01:38:23.440</a></span> | <span class="t">we're talking about for linear scaling for rib embeddings. If you want to extend the context then,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5907" target="_blank">01:38:27.120</a></span> | <span class="t">that's kind of what they do. Right. This is, this is for scaling. Not, not that there is a rotate half,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5912" target="_blank">01:38:32.240</a></span> | <span class="t">rotate half part, which I was talking about. Right? How do we actually derive the formulas for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5916" target="_blank">01:38:36.960</a></span> | <span class="t">gradient? Uh, how do we actually do the rope embeddings? It's just this. Um, the MLP, which we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5922" target="_blank">01:38:42.320</a></span> | <span class="t">talked about again, remember the MLP layer, right? So there's a gate projection, the up projection,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5926" target="_blank">01:38:46.240</a></span> | <span class="t">the down projection. Um, this code is bloat again. Ignore this. It's just this. It's just one line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5935" target="_blank">01:38:55.760</a></span> | <span class="t">That's the rest bloat. Um, ignore that. Right? So like the down projection is just the down projection,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5941" target="_blank">01:39:01.440</a></span> | <span class="t">right? The activation function, the gate projection times our projection, right? So that's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5944" target="_blank">01:39:04.800</a></span> | <span class="t">that is MLP. That is the, you know, sweet glue. One line. Not all of this. And that's just for training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5951" target="_blank">01:39:11.600</a></span> | <span class="t">purposes. Um, repeat KV is just an, that's for like, you know, um, I can't explain this too much,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5958" target="_blank">01:39:18.160</a></span> | <span class="t">but that's for the attention part. Um, this is just to make inference faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5962" target="_blank">01:39:22.720</a></span> | <span class="t">When you do Q K and when you go back to slides, um, if you go to, um, where is it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5968" target="_blank">01:39:28.960</a></span> | <span class="t">Right. WQ, WK and WV. Instead of training WK and WV, you train a small sliver and you repeat this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5981" target="_blank">01:39:41.440</a></span> | <span class="t">And this can make inference faster. Um, and so like, you don't, we don't actually train the full mate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5987" target="_blank">01:39:47.520</a></span> | <span class="t">the matrix size WK anymore. We train a small sliver. Um, and we just repeat this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5992" target="_blank">01:39:52.000</a></span> | <span class="t">The attention again, all of this is just preparing, right? This is just preparing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=5999" target="_blank">01:39:59.520</a></span> | <span class="t">right? And they're like, okay, ignore, ignore, ignore, blah, blah, blah, bloat. Get rid of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6004" target="_blank">01:40:04.480</a></span> | <span class="t">Um, don't look at that. Um, there is Q K and V. That's the matrix part. Um, there is some,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6011" target="_blank">01:40:11.760</a></span> | <span class="t">the only problem I find that people struggle with is there is like dimension manipulation. You have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6016" target="_blank">01:40:16.960</a></span> | <span class="t">like manipulated the dimensions of the output. That is actually kind of annoying. I do agree.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6020" target="_blank">01:40:20.960</a></span> | <span class="t">This is actually very annoying, but just assume it is the, this, right? That's all we're trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6026" target="_blank">01:40:26.000</a></span> | <span class="t">do is this. Um, and that is just these, these three lines do this, uh, do this. Um, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6034" target="_blank">01:40:34.800</a></span> | <span class="t">we want to do Q K transpose, right? Oh, you have to, sorry, you have to apply the rope embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6040" target="_blank">01:40:40.240</a></span> | <span class="t">Don't forget to apply the rope embedding. Um, and then Q K, that's a repeat K V. Repeat K V is the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6046" target="_blank">01:40:46.160</a></span> | <span class="t">that are the trick that I said to make inference faster, um, by repeating the, so if you go back here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6053" target="_blank">01:40:53.120</a></span> | <span class="t">the K and the V, the K and the V, you take only small slivers, you only train small slivers and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6058" target="_blank">01:40:58.240</a></span> | <span class="t">repeat them four times. Um, and it does not produce accuracy that bad. Um, and that's the repeat K V.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6064" target="_blank">01:41:04.400</a></span> | <span class="t">This is the Q K transpose, right? Torch on map ball, Q K transpose. This is the attention part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6072" target="_blank">01:41:12.720</a></span> | <span class="t">Soft max dropout. Okay. No one uses dropout anymore. Get rid of that line. Um, matrix multiplication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6079" target="_blank">01:41:19.760</a></span> | <span class="t">right? So this is, so up to here, up to here is Q K transpose over root D soft max times V. Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6089" target="_blank">01:41:29.840</a></span> | <span class="t">So up to here. And then we do some sort of, and then we have to do some, um, output projection as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6095" target="_blank">01:41:35.280</a></span> | <span class="t">well. Um, the rest, okay. Whenever you see this part, just ignore that, right? Whenever you see like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6100" target="_blank">01:41:40.080</a></span> | <span class="t">if self dot convict dot pre training TP is more than one, this is just for, um, this is just for faster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6106" target="_blank">01:41:46.320</a></span> | <span class="t">training. So you can get rid of that. Well, not for faster training for like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6109" target="_blank">01:41:49.440</a></span> | <span class="t">training across multiple GPUs. You don't, you can ignore that and just assume it's just that one line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6114" target="_blank">01:41:54.000</a></span> | <span class="t">Um, now there is more code like flash attention to ignore. That's just for faster flash attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6121" target="_blank">01:42:01.760</a></span> | <span class="t">Ignore, ignore, ignore. Right? No, you don't need to see this. Um, scale dot product attention is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6128" target="_blank">01:42:08.240</a></span> | <span class="t">faster version of attention that is native to fight torch. Also ignore. You don't need to see that as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6133" target="_blank">01:42:13.280</a></span> | <span class="t">Pretend you didn't see that. Um, and then finally we get to the decoder layer, right? Remember each we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6138" target="_blank">01:42:18.880</a></span> | <span class="t">show, uh, okay, maybe I should exit the slides. Um, where is it? No.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6147" target="_blank">01:42:27.680</a></span> | <span class="t">So remember the decoder layer. Um, so this, right? Remember we said we repeat L times, right? This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6156" target="_blank">01:42:36.560</a></span> | <span class="t">repeat L times. This is just shoved in, in the decoder layer, right? We call this one decoder layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6163" target="_blank">01:42:43.360</a></span> | <span class="t">And again, we do the layer norm. Remember put layer norms everywhere, do layer norm, do attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6170" target="_blank">01:42:50.800</a></span> | <span class="t">add some residual. Um, this also makes training more stable, right? When you add residual, it makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6175" target="_blank">01:42:55.760</a></span> | <span class="t">training more stable, do more layer norm, do more, um, do an MLP, add more residual, and then we complete it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6182" target="_blank">01:43:02.000</a></span> | <span class="t">Right? That's just one, that's one component of the decoder. And remember, we repeat this L times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6188" target="_blank">01:43:08.160</a></span> | <span class="t">And the rest of the code is just doing this L times. Now, where is it L times? Um, comments,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6194" target="_blank">01:43:14.560</a></span> | <span class="t">comments, comments, comments, forward, right? You go to forward. Um, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6199" target="_blank">01:43:19.360</a></span> | <span class="t">In my opinion, put it everywhere. Yeah, but in terms of calculating the residual and then adding the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6212" target="_blank">01:43:32.800</a></span> | <span class="t">five. Oh, that's just by, yes. Why is the ordering? Why is the ordering do, I think it's layer norm first,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6220" target="_blank">01:43:40.640</a></span> | <span class="t">then add residual. Is that correct? Or maybe I'm getting confused.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6223" target="_blank">01:43:43.040</a></span> | <span class="t">Yes. The ordering, to tell the truth, it doesn't really matter. I think there are some research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6236" target="_blank">01:43:56.160</a></span> | <span class="t">papers which show if you switch the ordering, it maybe increases, like it decreases accuracy by like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6241" target="_blank">01:44:01.200</a></span> | <span class="t">0.01%. To be honest, we need to do more testing again. Like, in my view, shove layer norms everywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6248" target="_blank">01:44:08.560</a></span> | <span class="t">Um, this actually, this should make training more stable. Um, but it makes training slower. That's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6254" target="_blank">01:44:14.480</a></span> | <span class="t">problem. Um, that is the only problem. But I suggest you to put layer norms everywhere. Wherever you see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6260" target="_blank">01:44:20.800</a></span> | <span class="t">you at, just shove layer norms. Um, yeah. Layer norms always work. Um, okay. And,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6268" target="_blank">01:44:28.560</a></span> | <span class="t">So why do you add the residuals of the previous, so you do, you take the residuals, you, you save the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6291" target="_blank">01:44:51.360</a></span> | <span class="t">state before the layer norm, and then you, and then you do a layer norm, and then you add back it in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6297" target="_blank">01:44:57.120</a></span> | <span class="t">So why do we have to do that? Is that your question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6300" target="_blank">01:45:00.000</a></span> | <span class="t">Yeah. Why the, the formalization is between that rather than like...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6305" target="_blank">01:45:05.200</a></span> | <span class="t">Oh, you could do it before, if you want to. You could try it. It depends. I think it's for,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6309" target="_blank">01:45:09.840</a></span> | <span class="t">yeah, it depends. As I said, all of this is like gospel, like, oh, why did we do this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6315" target="_blank">01:45:15.040</a></span> | <span class="t">It's just people tried it, and they said it works. Um.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6318" target="_blank">01:45:18.400</a></span> | <span class="t">I, I defend why this isn't impossible, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6322" target="_blank">01:45:22.240</a></span> | <span class="t">Oh, what do you mean by not an issue? Like, why does this work? Or why is this like not a,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6326" target="_blank">01:45:26.480</a></span> | <span class="t">what, what do you mean by issue? I mean, the, the visualization kind of changes the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6332" target="_blank">01:45:32.000</a></span> | <span class="t">in some way, the representation. Yes, it scales it. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6336" target="_blank">01:45:36.240</a></span> | <span class="t">It's just math. You just let it just, it just works. Because like, because like, if you do that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6353" target="_blank">01:45:53.120</a></span> | <span class="t">the autograd engine will still know that you did it, and the derivatives will still be applied correctly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6358" target="_blank">01:45:58.160</a></span> | <span class="t">So you just assume that it works. Um, to be honest, that is another research topic. You should try that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6364" target="_blank">01:46:04.080</a></span> | <span class="t">Um, I mean, I'm being like serious. Like, oh, there's so many research questions that are like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6368" target="_blank">01:46:08.240</a></span> | <span class="t">open. Like, why doesn't everyone just put layer norms everywhere? Like, why don't you put a layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6372" target="_blank">01:46:12.240</a></span> | <span class="t">norm after the multi-head, multi-head attention? Put a layer norm after the Swigaloo. Put a layer norm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6377" target="_blank">01:46:17.040</a></span> | <span class="t">after the inputs. Put a layer norm, you know, everywhere. Um, and my, my hypothesis is it makes training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6382" target="_blank">01:46:22.560</a></span> | <span class="t">more stable, but like, it makes it slower though. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6384" target="_blank">01:46:24.720</a></span> | <span class="t">In this theory, could mechanistic interpretability, um, solve, like, not solve, but explain this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6392" target="_blank">01:46:32.400</a></span> | <span class="t">Do you mean like, what do you mean by like, explain this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6396" target="_blank">01:46:36.880</a></span> | <span class="t">Oh, the mechanistic interpretability where you take the...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6399" target="_blank">01:46:39.600</a></span> | <span class="t">Do, which method? Do you mean like doing the autoencoder style? Like, sparse...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6403" target="_blank">01:46:43.840</a></span> | <span class="t">Do you mean like the, uh, what, which method? Like the, um, because I know there's like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6416" target="_blank">01:46:56.480</a></span> | <span class="t">many methods for mechanistic interpretability, like there's different types of methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6420" target="_blank">01:47:00.400</a></span> | <span class="t">This is the sparse autoencoder one or the... Well, transformer lens is Neil Nando's tool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6425" target="_blank">01:47:05.200</a></span> | <span class="t">Okay, I, I don't, I don't use that, so I'm not that, I'm not expert on that, but...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6429" target="_blank">01:47:09.680</a></span> | <span class="t">No, but you can see the, um, the literal, like, um, activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6436" target="_blank">01:47:16.160</a></span> | <span class="t">Yes, yes, the activation one. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6438" target="_blank">01:47:18.160</a></span> | <span class="t">Yeah, so basically, well, so, I mean, I'm gonna not divert from this, but, um, so basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6445" target="_blank">01:47:25.600</a></span> | <span class="t">can, do you think we can figure out why this works this way? Like, because you said it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6451" target="_blank">01:47:31.920</a></span> | <span class="t">kind of an open question, right? But do you think that using tools like transformer lens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6456" target="_blank">01:47:36.960</a></span> | <span class="t">where we can look at training activation, or not activation, but like steps, where in fact we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6464" target="_blank">01:47:44.080</a></span> | <span class="t">would have to, like, um, I'm not sure if I explained it correctly, but like, do you think mechanistic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6470" target="_blank">01:47:50.240</a></span> | <span class="t">interpretability is a path to understanding this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6474" target="_blank">01:47:54.960</a></span> | <span class="t">Good question. Um, could mechanistic interpretability, okay, hmm, it depends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6481" target="_blank">01:48:01.040</a></span> | <span class="t">I think my view is that just layer norms, if it's specifically on the topic of layer norms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6487" target="_blank">01:48:07.600</a></span> | <span class="t">it just makes training more stable. I don't think so has any, like, meaning, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6491" target="_blank">01:48:11.040</a></span> | <span class="t">that's my view. Like, I think, like, the math equations don't show that it has any meaning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6496" target="_blank">01:48:16.240</a></span> | <span class="t">I just find it to be just stabilized training. There was, like, papers like the, what was the one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6501" target="_blank">01:48:21.840</a></span> | <span class="t">Batch normalization, I forgot what the term was. Um, yeah, like, there was, like, a view which shows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6507" target="_blank">01:48:27.600</a></span> | <span class="t">that batch normalization reduces problems of out of distribution data and stuff like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6512" target="_blank">01:48:32.080</a></span> | <span class="t">Oh, reduces internal covariate shift or something. That was the phrase, which, yeah, I don't know what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6520" target="_blank">01:48:40.240</a></span> | <span class="t">that even means. But anyways, um, does anyone know what that means? There was, like, a, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6525" target="_blank">01:48:45.120</a></span> | <span class="t">there was, like, a video for that as well. The, yeah, does anyone know what that means? Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6531" target="_blank">01:48:51.760</a></span> | <span class="t">So, layer norms, my view of layer norms is when you do, if you don't do layer norms, if you keep,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6543" target="_blank">01:49:03.360</a></span> | <span class="t">okay, let's say you take a number, two, you multiply by two, you get four. Remember, there's 32 layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6548" target="_blank">01:49:08.400</a></span> | <span class="t">right? If you multiply by two continuously, you will get infinities in the end, right? Because you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6553" target="_blank">01:49:13.200</a></span> | <span class="t">like, go out of scope of the float 32. So what layer norm does is it makes your number go back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6558" target="_blank">01:49:18.960</a></span> | <span class="t">to a good scale. So if you do two times two is four, let's divide it by four, go back to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6564" target="_blank">01:49:24.880</a></span> | <span class="t">Right? And so now it's one again. If you times two, it's two again, let's divide it by two again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6569" target="_blank">01:49:29.680</a></span> | <span class="t">go back to one. So all layer norm does is it makes a scale go back to a good scale. Like, it doesn't,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6575" target="_blank">01:49:35.200</a></span> | <span class="t">your numbers don't like diverge on both sides. That's what layer norm kind of does. Does that kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6579" target="_blank">01:49:39.680</a></span> | <span class="t">of? Okay. Any other questions? Okay. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6585" target="_blank">01:49:45.200</a></span> | <span class="t">so all we remember the decoder style. Oh, I think we actually kind of finished reviewing the Lama</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6592" target="_blank">01:49:52.160</a></span> | <span class="t">architecture. There's nothing else to do. Um, the decoder, right? You do this 32 times. Remember,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6596" target="_blank">01:49:56.400</a></span> | <span class="t">like four decoder layer is in self dot layers. You do this 32. I think it was 32. I can't remember.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6601" target="_blank">01:50:01.280</a></span> | <span class="t">Um, multiple times, um, that's the decoder. You just do, you apply this multiple times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6606" target="_blank">01:50:06.720</a></span> | <span class="t">You do a lay and all. And finally, you get your logits. Where is it? Your LM head, right? This</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6613" target="_blank">01:50:13.680</a></span> | <span class="t">outputs the probabilities of which token. Remember, we're trying to predict the next token. We output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6619" target="_blank">01:50:19.600</a></span> | <span class="t">probabilities for each token. And that is called the LM head. Um, and where's the forward function?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6627" target="_blank">01:50:27.600</a></span> | <span class="t">The forward, right? There's a forward, but always with the forward, um, self, you do, you go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6633" target="_blank">01:50:33.360</a></span> | <span class="t">the model and then you do, okay. Remember, ignore this, right? Ignore this. And you should LM head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6638" target="_blank">01:50:38.560</a></span> | <span class="t">That's just one line, one line. Okay. One line. And then you do the float. Now, another question people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6643" target="_blank">01:50:43.280</a></span> | <span class="t">have is like, why do you have to do the float? Um, um, does anyone know why you have to do, you have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6648" target="_blank">01:50:48.800</a></span> | <span class="t">upcast a float? Why? Any clues? Have a guess. Have a guess. Have a guess. Have a guess. Why do we have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6659" target="_blank">01:50:59.760</a></span> | <span class="t">upcast a float? Sorry? Gradients? Okay. Close. Why? Why gradients? It is related somehow to gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6670" target="_blank">01:51:10.400</a></span> | <span class="t">Anyone have a guess? Okay. It's for training and stability purposes. So, the softmax, you should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6677" target="_blank">01:51:17.040</a></span> | <span class="t">always upcast the float32 because it makes training more stable. If you take the derivatives and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6682" target="_blank">01:51:22.640</a></span> | <span class="t">gradients, if you do not do float32, you might get NANDs as well. Remember, the exponential can be very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6690" target="_blank">01:51:30.080</a></span> | <span class="t">large. So, you want to take the float32, which has larger precision than float16. Float16 is the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6696" target="_blank">01:51:36.880</a></span> | <span class="t">number of 65536, I think. I think it's 65536. Right? But float32, the maximum is like some large number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6703" target="_blank">01:51:43.600</a></span> | <span class="t">to the power of 38 or something. 10 to the power of 38. So, that's why you have to upcast it to float32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6709" target="_blank">01:51:49.120</a></span> | <span class="t">This just makes training more stable. Right? So, all of these things that we do tricks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6712" target="_blank">01:51:52.560</a></span> | <span class="t">it's just to make training stable. Yes? You said you're doing the operation 32 times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6718" target="_blank">01:51:58.160</a></span> | <span class="t">Is that just like an arbitrary thing that people figured out works?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6721" target="_blank">01:52:01.280</a></span> | <span class="t">Oh, that's up to you. So, like, if you want to do more parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6724" target="_blank">01:52:04.960</a></span> | <span class="t">you can do 300 times. Up to you. That's just make your model 10 times larger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6729" target="_blank">01:52:09.280</a></span> | <span class="t">So, like, when you see, when you hear, like, you know, llamas. Yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6732" target="_blank">01:52:12.240</a></span> | <span class="t">Each time you do it, it's generating, like, a weight for the model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6735" target="_blank">01:52:15.600</a></span> | <span class="t">Like, each layer, like, each time you iterate, it's going to generate, like, a sort of weight?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6739" target="_blank">01:52:19.680</a></span> | <span class="t">Is that one of them? So, the weights you train,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6742" target="_blank">01:52:22.320</a></span> | <span class="t">when you take the tokens, you go through the architecture and, like, it changes the, it changes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6746" target="_blank">01:52:26.960</a></span> | <span class="t">the tokens. And these tokens keep shifting to, like, some sort of, like, new direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6751" target="_blank">01:52:31.360</a></span> | <span class="t">And you keep doing this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6755" target="_blank">01:52:35.680</a></span> | <span class="t">The problem is you'll have to train more weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6758" target="_blank">01:52:38.960</a></span> | <span class="t">So, each, each 32 times has different weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6762" target="_blank">01:52:42.720</a></span> | <span class="t">Correct. Each of the, each of the iterations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6768" target="_blank">01:52:48.400</a></span> | <span class="t">but it's not, yeah, each, there'll be 32 different weights for each layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6771" target="_blank">01:52:51.520</a></span> | <span class="t">And so, like, yeah, normally people just, if you see, like, this, like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6776" target="_blank">01:52:56.560</a></span> | <span class="t">GPT-4, what is it, like, one something trillion tokens, I'm assuming there's more layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6781" target="_blank">01:53:01.200</a></span> | <span class="t">Larger embedding dimension, larger this, larger that, more layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6785" target="_blank">01:53:05.120</a></span> | <span class="t">Normally speaking, the more layers you do, the model can learn more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6789" target="_blank">01:53:09.200</a></span> | <span class="t">So, that's the whole reason why you wanted to add more layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6791" target="_blank">01:53:11.600</a></span> | <span class="t">You just want to increase the capacity of the model to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6795" target="_blank">01:53:15.200</a></span> | <span class="t">Again, it's to make training more stable, again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6796" target="_blank">01:53:16.880</a></span> | <span class="t">And so this, remember the shifting trick that we did?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6801" target="_blank">01:53:21.920</a></span> | <span class="t">In PyTorch, the shifting trick is just this and this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6806" target="_blank">01:53:26.000</a></span> | <span class="t">That's the shifting trick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6808" target="_blank">01:53:28.320</a></span> | <span class="t">That's the thing that makes it learn to predict the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6811" target="_blank">01:53:31.360</a></span> | <span class="t">And then you pass it through the loss function, the cross entropy loss, which we discussed about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6816" target="_blank">01:53:36.960</a></span> | <span class="t">And then that's the Lama architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6819" target="_blank">01:53:39.600</a></span> | <span class="t">And that's normal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6820" target="_blank">01:53:40.320</a></span> | <span class="t">The rest is not useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6821" target="_blank">01:53:41.920</a></span> | <span class="t">The rest is, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6823" target="_blank">01:53:43.680</a></span> | <span class="t">So, in theory, you could write the entire Lama architecture in, like, I think, 50 lines or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6828" target="_blank">01:53:48.960</a></span> | <span class="t">something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6829" target="_blank">01:53:49.280</a></span> | <span class="t">The rest is just unnecessary bloat, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6833" target="_blank">01:53:53.120</a></span> | <span class="t">This, all of this is 1,600 lines of comments and stuff like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6836" target="_blank">01:53:56.000</a></span> | <span class="t">But, you know, this is for hugging faces implementation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6838" target="_blank">01:53:58.000</a></span> | <span class="t">It's highly respected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6839" target="_blank">01:53:59.920</a></span> | <span class="t">And this is what you should look at first when you read a new architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6844" target="_blank">01:54:04.320</a></span> | <span class="t">So, we just kind of went through the Lama architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6847" target="_blank">01:54:07.120</a></span> | <span class="t">Hopefully, you can kind of get a sense of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6848" target="_blank">01:54:08.960</a></span> | <span class="t">Obviously, if this is your first time reading the source code, it's actually not that hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6853" target="_blank">01:54:13.600</a></span> | <span class="t">It's not that complicated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6855" target="_blank">01:54:15.520</a></span> | <span class="t">You just have to see which components you can ignore, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6858" target="_blank">01:54:18.400</a></span> | <span class="t">It's not that scary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6859" target="_blank">01:54:19.440</a></span> | <span class="t">Yep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6861" target="_blank">01:54:21.280</a></span> | <span class="t">Does that kind of get it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6862" target="_blank">01:54:22.480</a></span> | <span class="t">Or do you guys kind of get that feel?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6865" target="_blank">01:54:25.360</a></span> | <span class="t">We're going to do more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6866" target="_blank">01:54:26.880</a></span> | <span class="t">Obviously, this is the first one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6868" target="_blank">01:54:28.240</a></span> | <span class="t">Any questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6869" target="_blank">01:54:29.440</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6872" target="_blank">01:54:32.800</a></span> | <span class="t">Is there a major architectural difference between Mava 2 and Mava 3?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6875" target="_blank">01:54:35.840</a></span> | <span class="t">No, not really.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6878" target="_blank">01:54:38.160</a></span> | <span class="t">Other than more tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6879" target="_blank">01:54:39.440</a></span> | <span class="t">I think they changed embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6882" target="_blank">01:54:42.720</a></span> | <span class="t">They did change some of the numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6883" target="_blank">01:54:43.920</a></span> | <span class="t">Like, how many numbers you want to represent for each number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6886" target="_blank">01:54:46.720</a></span> | <span class="t">They changed that large vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6889" target="_blank">01:54:49.600</a></span> | <span class="t">They did much larger vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6891" target="_blank">01:54:51.360</a></span> | <span class="t">And more tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6892" target="_blank">01:54:52.320</a></span> | <span class="t">Other than that, no, there's no change at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6894" target="_blank">01:54:54.560</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6895" target="_blank">01:54:55.120</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6895" target="_blank">01:54:55.440</a></span> | <span class="t">The reason why -- it's funny.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6908" target="_blank">01:55:08.240</a></span> | <span class="t">I used to work at NVIDIA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6909" target="_blank">01:55:09.680</a></span> | <span class="t">Why shouldn't I be writing Cuda, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6911" target="_blank">01:55:11.360</a></span> | <span class="t">The reason is, I see Cuda as extremely annoying to write.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6916" target="_blank">01:55:16.720</a></span> | <span class="t">And if you want to optimize for just NVIDIA hardware, okay, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6919" target="_blank">01:55:19.920</a></span> | <span class="t">You can do Cuda.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6921" target="_blank">01:55:21.040</a></span> | <span class="t">But my view is like, I don't think so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6923" target="_blank">01:55:23.040</a></span> | <span class="t">That's going to be forever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6924" target="_blank">01:55:24.480</a></span> | <span class="t">So, as like a safety precaution, let's just do Triton, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6931" target="_blank">01:55:31.920</a></span> | <span class="t">Let Triton handle the compiling down to Cuda or AMD or whatever, Intel or whatever, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6938" target="_blank">01:55:38.080</a></span> | <span class="t">And Triton can be the intermediary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6940" target="_blank">01:55:40.720</a></span> | <span class="t">If you want to get like 10% faster, yes, you should do Cuda.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6943" target="_blank">01:55:43.680</a></span> | <span class="t">But it's only 10%, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6945" target="_blank">01:55:45.280</a></span> | <span class="t">If you do fine-tuning two times faster, it's already like, it's already nearly at the ceiling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6948" target="_blank">01:55:48.720</a></span> | <span class="t">You can only go so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6949" target="_blank">01:55:49.760</a></span> | <span class="t">And so like, if you want to go down the extra mile, yes, more than happy to welcome you to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6954" target="_blank">01:55:54.560</a></span> | <span class="t">But I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6955" target="_blank">01:55:55.280</a></span> | <span class="t">I do not like -- it's funny because I used to do Cuda all the time, but I don't suggest it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6959" target="_blank">01:55:59.040</a></span> | <span class="t">You will get more performance though, but I don't suggest it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6962" target="_blank">01:56:02.000</a></span> | <span class="t">Yes, question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6962" target="_blank">01:56:02.640</a></span> | <span class="t">So we never had to drop down to Cuda, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6964" target="_blank">01:56:04.720</a></span> | <span class="t">Oh, sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6965" target="_blank">01:56:05.200</a></span> | <span class="t">Yes, what?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6965" target="_blank">01:56:05.600</a></span> | <span class="t">We never had to drop down to Cuda.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6967" target="_blank">01:56:07.040</a></span> | <span class="t">You could deal with Triton.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6968" target="_blank">01:56:08.640</a></span> | <span class="t">Yes, you don't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6969" target="_blank">01:56:09.280</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6969" target="_blank">01:56:09.520</a></span> | <span class="t">So Triton, you write it in Triton, then it compiles down to Cuda.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6972" target="_blank">01:56:12.880</a></span> | <span class="t">Yeah, sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6973" target="_blank">01:56:13.440</a></span> | <span class="t">Wait.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6973" target="_blank">01:56:13.680</a></span> | <span class="t">Yeah, yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6974" target="_blank">01:56:14.000</a></span> | <span class="t">Actually, it could work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6982" target="_blank">01:56:22.640</a></span> | <span class="t">The only problem why it doesn't work on AMD is Triton.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6985" target="_blank">01:56:25.600</a></span> | <span class="t">Oh, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6988" target="_blank">01:56:28.320</a></span> | <span class="t">And Xformis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6989" target="_blank">01:56:29.120</a></span> | <span class="t">Actually, if -- so if Triton works on AMD, we work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=6994" target="_blank">01:56:34.400</a></span> | <span class="t">If Triton -- if Xformis, so Facebook's flash attention library, if that works on AMD, then we work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7000" target="_blank">01:56:40.720</a></span> | <span class="t">Oh, funny.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7002" target="_blank">01:56:42.240</a></span> | <span class="t">We work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7002" target="_blank">01:56:42.720</a></span> | <span class="t">But anyways, it depends on those conditions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7005" target="_blank">01:56:45.440</a></span> | <span class="t">So if AMD has those to work, then yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7007" target="_blank">01:56:47.920</a></span> | <span class="t">In theory, you can remove Xformis and just use scale dot product attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7011" target="_blank">01:56:51.840</a></span> | <span class="t">So there's only one dependency, which is Triton.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7015" target="_blank">01:56:55.040</a></span> | <span class="t">I think some people have gotten it to work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7016" target="_blank">01:56:56.720</a></span> | <span class="t">So it depends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7017" target="_blank">01:56:57.360</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7017" target="_blank">01:56:57.600</a></span> | <span class="t">I kind of have an answer to that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7019" target="_blank">01:56:59.840</a></span> | <span class="t">I've trained on a MI300X instinct with one card with Triton and it works with AMD.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7028" target="_blank">01:57:08.560</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7029" target="_blank">01:57:09.840</a></span> | <span class="t">I mean, if Triton works, then yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7031" target="_blank">01:57:11.120</a></span> | <span class="t">It works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7032" target="_blank">01:57:12.320</a></span> | <span class="t">So I just have an answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7035" target="_blank">01:57:15.200</a></span> | <span class="t">Sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7035" target="_blank">01:57:15.520</a></span> | <span class="t">Okay, good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7036" target="_blank">01:57:16.960</a></span> | <span class="t">You answered, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7037" target="_blank">01:57:17.600</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7037" target="_blank">01:57:17.920</a></span> | <span class="t">Yeah, but we don't -- so officially, we do not support AMD, but I guess it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7043" target="_blank">01:57:23.760</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7044" target="_blank">01:57:24.320</a></span> | <span class="t">That's interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7045" target="_blank">01:57:25.040</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7046" target="_blank">01:57:26.080</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7046" target="_blank">01:57:26.880</a></span> | <span class="t">What's next?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7049" target="_blank">01:57:29.600</a></span> | <span class="t">Where's my -- where's the Gemma one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7051" target="_blank">01:57:31.520</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7052" target="_blank">01:57:32.880</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7053" target="_blank">01:57:33.040</a></span> | <span class="t">So now we're talking about Gemma bugs, specifically Gemma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7056" target="_blank">01:57:36.560</a></span> | <span class="t">So if you go to our blog post, I actually -- we wrote a blog post about all the issues</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7062" target="_blank">01:57:42.720</a></span> | <span class="t">that we found in Gemma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7064" target="_blank">01:57:44.720</a></span> | <span class="t">For example, you must add a VOS token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7066" target="_blank">01:57:46.480</a></span> | <span class="t">There is a typo in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7068" target="_blank">01:57:48.560</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7069" target="_blank">01:57:49.680</a></span> | <span class="t">So we don't just find bugs and, you know, we have to read the paper first to understand the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7074" target="_blank">01:57:54.320</a></span> | <span class="t">Now, the problem is sometimes when people release models, they don't release papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7078" target="_blank">01:57:58.720</a></span> | <span class="t">That is very painful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7080" target="_blank">01:58:00.480</a></span> | <span class="t">That happens a lot now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7081" target="_blank">01:58:01.920</a></span> | <span class="t">So please, model creators, please provide papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7085" target="_blank">01:58:05.360</a></span> | <span class="t">Otherwise, it gets more complicated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7087" target="_blank">01:58:07.680</a></span> | <span class="t">There's also, like, some other issues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7091" target="_blank">01:58:11.120</a></span> | <span class="t">And we have a Colab notebook, which provides all these -- so if you open up the link, Gemma details,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7097" target="_blank">01:58:17.280</a></span> | <span class="t">in the -- remember, if you don't have access to these slides, it is tinyurl.com/unsloth, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7105" target="_blank">01:58:25.840</a></span> | <span class="t">That's the slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7107" target="_blank">01:58:27.120</a></span> | <span class="t">If you open up the Colab notebook, this is actually runnable in Colab.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7112" target="_blank">01:58:32.400</a></span> | <span class="t">Please log into your Google account for this to actually work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7116" target="_blank">01:58:36.640</a></span> | <span class="t">But we show that this is the logl2 norm, so we check the -- so this layer number, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7124" target="_blank">01:58:44.560</a></span> | <span class="t">There's, like, 18 layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7125" target="_blank">01:58:45.760</a></span> | <span class="t">We check every single layer, the output of the actual good implementation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7131" target="_blank">01:58:51.280</a></span> | <span class="t">So the DeepMind implementation with the HuggingFace one, with the PyTorch one, with the other ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7135" target="_blank">01:58:55.760</a></span> | <span class="t">And if you do the L2 norm, you find that the error is very high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7139" target="_blank">01:58:59.680</a></span> | <span class="t">And what we showed is that you can actually move the error down by doing multiple changes, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7145" target="_blank">01:59:05.360</a></span> | <span class="t">So each line -- you can see there's, like, multiple lines -- each line is actually a method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7149" target="_blank">01:59:09.760</a></span> | <span class="t">that we apply to make it better, right? So, like, we finally found that approximately either</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7154" target="_blank">01:59:14.640</a></span> | <span class="t">the blue line -- I mean, it depends on which one you like -- either the blue line or the black line</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7159" target="_blank">01:59:19.440</a></span> | <span class="t">makes training much better. Does anyone notice any interesting things about this graph?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7164" target="_blank">01:59:24.880</a></span> | <span class="t">Anything interesting? Like, do you see the -- so remember, each line is a fix that we did, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7172" target="_blank">01:59:32.880</a></span> | <span class="t">So, like, there's many lines, and we did a fix, and it changes the error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7179" target="_blank">01:59:39.040</a></span> | <span class="t">And we selected the black line to be the final one. Does anyone have any -- what is, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7186" target="_blank">01:59:46.160</a></span> | <span class="t">anything interesting?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7186" target="_blank">01:59:46.960</a></span> | <span class="t">Yes. So one of them caused a huge jump, and that is a float 32 fix that we did for all architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7199" target="_blank">01:59:59.120</a></span> | <span class="t">Yes. And the other ones are less prominent. But anything else? Anything else interesting? Yes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7208" target="_blank">02:00:08.320</a></span> | <span class="t">Yes. Fantastic. Why? I do not know. And that is a good question, and I don't actually know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7215" target="_blank">02:00:15.600</a></span> | <span class="t">I think it's just language -- yeah, I have a theory. The theory is --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7218" target="_blank">02:00:18.320</a></span> | <span class="t">Yeah, but unfortunately, I can't say everything. Like, I mean, my theory is -- and there was also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7224" target="_blank">02:00:24.640</a></span> | <span class="t">a jump as well in the middle -- and the blue line, you know, it starts from very low, it goes up very high,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7229" target="_blank">02:00:29.920</a></span> | <span class="t">and everything does this, right? So, like, there is some weird transition boundary in the Gemma model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7234" target="_blank">02:00:34.880</a></span> | <span class="t">right? And so, like, I'm just going to guess. My guess is that when you train a transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7242" target="_blank">02:00:42.160</a></span> | <span class="t">the later layers get harder and harder to train, right? The earlier layers actually get very easy to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7247" target="_blank">02:00:47.760</a></span> | <span class="t">train. And so this transition boundary is when the model probably wasn't really trained that well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7253" target="_blank">02:00:53.200</a></span> | <span class="t">So I'm going to guess, this is just guessing, that maybe the model should have been trained for more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7258" target="_blank">02:00:58.000</a></span> | <span class="t">data, and the boundary should disappear. This is just my guess. So there is a phenomenon, essentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7266" target="_blank">02:01:06.160</a></span> | <span class="t">it's like more data. The model, the last layers are much harder to train. And that's kind of my theory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7274" target="_blank">02:01:14.640</a></span> | <span class="t">But I don't think so. That's correct. But, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7277" target="_blank">02:01:17.680</a></span> | <span class="t">And the blue one kind of dropped for a moment, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7282" target="_blank">02:01:22.400</a></span> | <span class="t">Yes. Right before the last one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7284" target="_blank">02:01:24.560</a></span> | <span class="t">Yeah, exactly. So in the end -- so now the question is, like, why did we choose the black one, then?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7289" target="_blank">02:01:29.840</a></span> | <span class="t">Why didn't we choose the green -- the blue line? So that's adding the exact jellu that we found.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7294" target="_blank">02:01:34.800</a></span> | <span class="t">So if you add the rope fix plus the exact jellu, you get the blue line. But we, in the end,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7299" target="_blank">02:01:39.680</a></span> | <span class="t">decided to do the black line. And why do you think that is? We did not choose the blue line. We should have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7304" target="_blank">02:01:44.240</a></span> | <span class="t">chose the blue line, right? But the final -- all the fixes that we did. So essentially, the answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7309" target="_blank">02:01:49.440</a></span> | <span class="t">why we did not choose the blue line -- the blue line should actually have had lower error, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7312" target="_blank">02:01:52.800</a></span> | <span class="t">The reason why we didn't choose that is because there was not just one error. There were two errors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7317" target="_blank">02:01:57.600</a></span> | <span class="t">There was many errors. And all of the errors compounded together. We finally chose the black</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7322" target="_blank">02:02:02.080</a></span> | <span class="t">line because it matches the original implementation. So because, remember, the trick is you have to match</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7328" target="_blank">02:02:08.480</a></span> | <span class="t">the original implementation of whatever the gemma model creators did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7333" target="_blank">02:02:13.840</a></span> | <span class="t">So you can't just look for this error. I mean, maybe if someone chose different fixes that we did,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7342" target="_blank">02:02:22.160</a></span> | <span class="t">you can probably get a lower training loss. I guess you could. But we decided to choose the black line</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7349" target="_blank">02:02:29.760</a></span> | <span class="t">because that's what the original implementation did. Any other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7354" target="_blank">02:02:34.640</a></span> | <span class="t">Oh, I'm talking about the weights. So the weights are the ones in the -- so the model weights are the ones</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7363" target="_blank">02:02:43.680</a></span> | <span class="t">training. All right? So the rest you don't actually train. It's just the weights itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7367" target="_blank">02:02:47.680</a></span> | <span class="t">Yeah, it's just a bit better, right? At the end, you end up with the model --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7380" target="_blank">02:03:00.720</a></span> | <span class="t">you end up with the . And that leads me to my follow-up question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7393" target="_blank">02:03:13.040</a></span> | <span class="t">Do you have examples of the training, like, is it -- like, what are those training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7401" target="_blank">02:03:21.440</a></span> | <span class="t">based properties? Is it iterative where you're just going to see the training loss over time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7407" target="_blank">02:03:27.360</a></span> | <span class="t">? Yes. So remember, the goal of a transformer is you want to predict the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7418" target="_blank">02:03:38.960</a></span> | <span class="t">Right? So the sentence, "Hello, my name is Daniel." You're trying to predict, "Hello,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7422" target="_blank">02:03:42.160</a></span> | <span class="t">predict my." Right? "My, predict name." And so on. You have this data, correct? Like, you have -- just take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7428" target="_blank">02:03:48.560</a></span> | <span class="t">novels. You shove in the novels. You already -- you essentially create the data out of thin air. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7433" target="_blank">02:03:53.600</a></span> | <span class="t">then you change these weights using, like, backpropagation, do derivatives. And try to, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7439" target="_blank">02:03:59.200</a></span> | <span class="t">change these weights such that you get the highest accuracy. And this training procedure is called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7444" target="_blank">02:04:04.560</a></span> | <span class="t">backpropagation. And so, like, I was trying to show you, like, how do we actually derive the derivatives?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7449" target="_blank">02:04:09.280</a></span> | <span class="t">When you do backpropagation, you need to derive the derivatives. Just use PyTorch. PyTorch will do the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7454" target="_blank">02:04:14.560</a></span> | <span class="t">derivatives for you. Um, and -- yes, but that's -- does that kind of answer your question, or?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7458" target="_blank">02:04:18.720</a></span> | <span class="t">Yeah, I think like this one is working. Okay, yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7463" target="_blank">02:04:23.040</a></span> | <span class="t">So you mentioned that the layers are embedded in the frame. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7467" target="_blank">02:04:27.680</a></span> | <span class="t">So you mentioned that the layers are embedded in the frame. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7467" target="_blank">02:04:27.680</a></span> | <span class="t">So you mentioned that the layers are embedded in the frame.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7469" target="_blank">02:04:29.840</a></span> | <span class="t">I know that's why I did layer-free thing now, but is there a way to change the layer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7477" target="_blank">02:04:37.120</a></span> | <span class="t">Yes, the answer actually has that. So you can actually -- depending on your layer -- so for now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7482" target="_blank">02:04:42.160</a></span> | <span class="t">what we do is your embedding and your final layer, you can change different weights -- different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7486" target="_blank">02:04:46.240</a></span> | <span class="t">learning rates. So we found that if you train on -- if you train the last layer with the embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7491" target="_blank">02:04:51.120</a></span> | <span class="t">weights and the first -- sorry, the embedding weights in the LM head by a factor of 10 smaller,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7496" target="_blank">02:04:56.720</a></span> | <span class="t">the learning rate, you can actually have increased accuracy. So yes, you should. You should change the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7501" target="_blank">02:05:01.440</a></span> | <span class="t">-- you should change the learning rates for each layer. But people don't actually do that. I think it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7506" target="_blank">02:05:06.080</a></span> | <span class="t">because if you set a learning rate for each layer beforehand, you're kind of like -- it's like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7513" target="_blank">02:05:13.120</a></span> | <span class="t">this -- you're like doing subjective bias. So that's why people just set one learning rate for all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7518" target="_blank">02:05:18.720</a></span> | <span class="t">layers. But I think in this case, I'm just going to guess, okay, this might be a transformer. This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7522" target="_blank">02:05:22.960</a></span> | <span class="t">transformer general. This is not just for JAMA. This is for all transformers. Maybe I guess layer-wise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7528" target="_blank">02:05:28.080</a></span> | <span class="t">layer-wise learning rate could work. I think there are some papers which do that. I think it's called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7532" target="_blank">02:05:32.720</a></span> | <span class="t">Lars, I think. Lars does layer-wise -- I think it's called Lars -- layer-wise learning rate. I hope that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7540" target="_blank">02:05:40.960</a></span> | <span class="t">answered your question. Yes? Oh, it's a log L2 norm. So it's -- you take the deepmine implementation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7552" target="_blank">02:05:52.880</a></span> | <span class="t">you code it up correctly. Then you take the other implementations like PyTorch, HuggingVase,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7557" target="_blank">02:05:57.600</a></span> | <span class="t">even deepmine's own implementations. And then you check each layer, the output, you compare it with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7564" target="_blank">02:06:04.160</a></span> | <span class="t">the original -- the correct implementation and check what's the error. And that's the thing that I graphed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7569" target="_blank">02:06:09.120</a></span> | <span class="t">And your goal is you want the error to go to zero, right? So like you want it to go all the way to zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7574" target="_blank">02:06:14.080</a></span> | <span class="t">so like, you know, on the bottom and not like very high. And that's log scale, right? So the error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7579" target="_blank">02:06:19.520</a></span> | <span class="t">is not like a small number. It's 1,000, right? So like every single line, every single step you go down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7584" target="_blank">02:06:24.960</a></span> | <span class="t">is a log difference, right? It's not -- it's not like a -- it's -- I essentially logged it. If you did not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7590" target="_blank">02:06:30.240</a></span> | <span class="t">log it, it would look very bad. But I just logged it. Yeah, does that -- okay. Any other questions? Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7596" target="_blank">02:06:36.880</a></span> | <span class="t">This actually happens a lot, very frequently. And I think like -- so like, for example, TinyLlama,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7607" target="_blank">02:06:47.280</a></span> | <span class="t">someone trying TinyLlama, and then training already 80% completed. They found a bug for tokenization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7615" target="_blank">02:06:55.680</a></span> | <span class="t">They're like -- so it happens very frequently. And it depends on what you want to do. I think it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7621" target="_blank">02:07:01.600</a></span> | <span class="t">depends to the model creator. If you already spent millions of dollars, if you already spent millions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7626" target="_blank">02:07:06.560</a></span> | <span class="t">of dollars, you have to change the way you have to change the way you're tokenizing -- would you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7629" target="_blank">02:07:09.040</a></span> | <span class="t">like -- so it happens very frequently. And it depends on what you want to do. I think it depends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7634" target="_blank">02:07:14.800</a></span> | <span class="t">to the model creator. If you already spent millions of dollars, maybe just keep -- just train it with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7640" target="_blank">02:07:20.000</a></span> | <span class="t">the bug, and then you release the bugged version. But it should still work. Hopefully. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7647" target="_blank">02:07:27.440</a></span> | <span class="t">So in theory, let's say if OpenAI would have a lot of difficulty shifting, if they found -- like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7653" target="_blank">02:07:33.760</a></span> | <span class="t">if somebody else found an optimized tokenizer or something like that, they would have trouble</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7658" target="_blank">02:07:38.960</a></span> | <span class="t">shifting to that model, because they would have to spend like -- You have to retrain everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7662" target="_blank">02:07:42.400</a></span> | <span class="t">Correct. So you just -- just assume it -- just leave it. If you already spent like billions of dollars,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7667" target="_blank">02:07:47.840</a></span> | <span class="t">I'm probably not a good idea to retrain. So even if it offers like a -- like 2x optimization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7673" target="_blank">02:07:53.200</a></span> | <span class="t">somehow, like they would have to spend -- like they would have to retrain and spend --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7677" target="_blank">02:07:57.120</a></span> | <span class="t">Yes, you have to retrain everything from scratch. But that's why like -- I think like -- that's why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7680" target="_blank">02:08:00.880</a></span> | <span class="t">you should do like small scale experiments. You know, get like a smaller model, train it for less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7685" target="_blank">02:08:05.040</a></span> | <span class="t">data, test it, and then see if the accuracy is good, and then you scale it up. Yeah. Any other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7691" target="_blank">02:08:11.600</a></span> | <span class="t">Okay. I will -- yes. So there's a notebook. So we show step by step exactly what we did. And if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7700" target="_blank">02:08:20.960</a></span> | <span class="t">inspect the code -- okay, the Gemma code is now -- the Gemma code -- if you -- oh, okay. Wait, no, it's modeling Gemma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7710" target="_blank">02:08:30.480</a></span> | <span class="t">Oh, okay. Maybe I should just go to HuggingFace itself. Wait, let me go to -- you can actually find this. If you copy paste this -- right, you edit the -- you go to Gemma, and you go to modeling Gemma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7727" target="_blank">02:08:47.680</a></span> | <span class="t">All right. This is -- oh, did I not -- okay. Let me just -- okay. Maybe I typed it wrong. Did they not -- oh, okay. Maybe I did two Ls. My bad. I always get confused on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7741" target="_blank">02:09:01.680</a></span> | <span class="t">Oh, what is this? Hmm. This is interesting. Okay. This is like new. So -- okay. Yeah, I did not -- yeah. So all of this -- so we wrote inside the -- like, you know, Lama does -- so we showed, for example, in the code now, if you go to, like, HuggingFace's code for Gemma, we wrote -- I tried to write some comments, you know, for it to be more clear on why we are doing this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7765" target="_blank">02:09:25.680</a></span> | <span class="t">And so, for example, the layer norm, right, you have to be careful to where you upcast and downcast. And we write this in here. Where is it? I think it's -- no, no, no, no. Oh, wait. Is it -- no, I'm pretty sure I've wrote it somewhere. No, it is here. Yes. Okay. It's a bit unclear. I need to make this bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7787" target="_blank">02:09:47.680</a></span> | <span class="t">Okay. It's a bit blurry. But you can see that, depending on the model, in Gemma, you have to actually upcast to float32. Everywhere. You must use float32 everywhere. Because the original implementation used float32. Right? So you must always follow the original implementation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7802" target="_blank">02:10:02.680</a></span> | <span class="t">If you don't follow the original implementation, then you will get wrong, like, you know, somewhat worse results. And the problem was other implementations just copied Lama and Mistral's code. And they did not do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7814" target="_blank">02:10:14.680</a></span> | <span class="t">And so we found that you actually have to upcast correctly over here. Right? You have to upcast immediately. And then you downcast at the very end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7825" target="_blank">02:10:25.680</a></span> | <span class="t">And so we wrote a few comments, right? Lama does x.2 float16, whilst Gemma is x -- you know, it really -- like, Lama does that. Right? But Gemma does this. Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7837" target="_blank">02:10:37.680</a></span> | <span class="t">So there were, like, small little issues, downcasting, upcasting. Another question is, like, why do we have to do downcasting? Does anyone know why? Like, why is there always, like, downcasting, upcasting? Float32, float16, float8? Does anyone know why we have to do downcasting, upcasting?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7852" target="_blank">02:10:52.680</a></span> | <span class="t">Yes, correct. It's for faster speed. So do you know how much faster? Like --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7859" target="_blank">02:10:59.680</a></span> | <span class="t">Eight? I don't know. So float32 to float16. What do you think?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7865" target="_blank">02:11:05.680</a></span> | <span class="t">It depends. Who said two? Okay, good guess. Why? Why did you guess two?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7874" target="_blank">02:11:14.680</a></span> | <span class="t">Well, that's for sparsity. Okay. Okay. Yes, okay. Float8, approximately two. Actually, it could be more. So float32 to float16 is actually not two. It's actually much more. I think it's five. I think. Or is it six?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7894" target="_blank">02:11:34.680</a></span> | <span class="t">The reason is because the representation of the float is different. Right? So float32, I have floating point representation Wikipedia. I think it's in here somewhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7906" target="_blank">02:11:46.680</a></span> | <span class="t">Oh, and maybe go to beatfloat16. Where is beatfloat16? Brain float. Beat float? Beat float. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7917" target="_blank">02:11:57.680</a></span> | <span class="t">Right? So like, there it is. Oh, there's more pictures now. Oh, they edited this. I did not. Okay. This is new. I didn't see amd ft24 format or Pixar. Oh, okay. They have like weird formats now. This is float32. Right? And float32, the exponent has eight numbers. Right? Eight bits. And the fraction of a bit has 23.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7945" target="_blank">02:12:25.680</a></span> | <span class="t">And when you do. And when you do matrix, when you do matrix multiplication, does anyone know how to calculate the number of transistors you need for float32? Does anyone know? It's a formula. That's related to the exponent, the fraction, and just the exponent and fraction. What do you think the formula is? Have a guess. Right? I said that it's approximately. So if you have beatfloat16, the fraction is seven.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7966" target="_blank">02:12:46.680</a></span> | <span class="t">Right? But if you have beatfloat16 has 16 bits you can use. The exponent, the exponent is used for the dynamic range of the number. Right? So if you want larger numbers, you have to have larger exponents. Right? So this means 16 only has a range of two to the path. Okay. It's not two to the eight. Like, just assume, you know, it's two to the power of eight. Okay. That's not right. But just assume that. Um, two to the power of eight. Right? But.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=7993" target="_blank">02:13:13.680</a></span> | <span class="t">Yeah. Um, and this one, float32 also has two to the power of eight. There's another format called float16, which is two to the power of five. Um, and then the fraction of component is 10. So all of these numbers, you can scale, right? How many do you want for the exponent? How many do you want for the fraction? You must include the signed it. And the trick is you must have 16. You need to fit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8012" target="_blank">02:13:32.680</a></span> | <span class="t">You know, 16. So you could have, like, exponent one. And fraction could be, um, 14. That could also work. Um, but does anyone know how many transistors you need to use for float16, for example? And be for 16? Remember, I said it's like around five times faster. It's actually not right. I think it's even more. Um, what is the formula? Have a guess. How many transistors do you need to use?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8036" target="_blank">02:13:56.680</a></span> | <span class="t">How many transistors do you need to do float16 multiplication, approximately? Or float multiplication? It's, it's a formula related to exponent and fraction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8043" target="_blank">02:14:03.680</a></span> | <span class="t">The answer is exponent plus fraction squared. That's the answer. Um, so what does that mean? That means float16 is 5 plus 10 squared. Right? And float32 is 8 plus 23 squared. So it is not two times faster. It is much faster. Right? So like, I don't know what that is. What is, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8070" target="_blank">02:14:30.680</a></span> | <span class="t">So it's 8 plus 23 squared. So it's 8 plus 23 squared. So it's 8 plus 23 squared. Yeah. And so what, what is the other one? I think it goes, um, what's that? I can't remember. So, um, so it's 8 and 7, right? So 8 and 7. This is, this is Google's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8100" target="_blank">02:15:00.660</a></span> | <span class="t">format. It is 57. So what does that mean? How many times faster? Yeah. So it's actually 10 times faster. Right? So 332 to float16 is around 10 times faster. Right? Float16 is 5 plus 10. Right? So 5 plus 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8117" target="_blank">02:15:17.660</a></span> | <span class="t">So B for 16 is approximately 2 times faster than float16. Although no one really notices any difference. Um, but in general, B for 16 is actually faster. Right? So that's why it's not 2 times faster. It's 10 times faster. Um, and that's why you must use Tesla T4s, as I said, because it has tensor cores, which does this, right? The tensor cores does float16 multiplication very effectively and very efficiently. Um, and so do not use P100s again. Right? P100s do not have this methodology.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8147" target="_blank">02:15:47.640</a></span> | <span class="t">Um, yes. Question? Yes. Float8. So float8. I don't know. Um, there are two formats for float8.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8157" target="_blank">02:15:57.520</a></span> | <span class="t">Oh, wait, I don't think so. It's in Wikipedia. Float eight. Oh, okay. Floating point. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8172" target="_blank">02:16:12.140</a></span> | <span class="t">called EEM -- oh, I'll just use mini float. They have some. Yeah, there we go. Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8178" target="_blank">02:16:18.620</a></span> | <span class="t">So you get to -- remember, if you want to have eight bits, you get to decide how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8183" target="_blank">02:16:23.080</a></span> | <span class="t">you want to do for the exponent, how many you want to do for the fraction or the mantis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8187" target="_blank">02:16:27.020</a></span> | <span class="t">apart, right? You get to decide. And depending on the company, you know, it's unclear. There's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8193" target="_blank">02:16:33.400</a></span> | <span class="t">no standard. So this one's 1, 4, 3. So what's 1, 4, 3? 1 plus 4, right? 4, 3 squared. Is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8201" target="_blank">02:16:41.560</a></span> | <span class="t">that 4, 3? Yeah. 13. So float eight is? I think it's around -- yeah. So around four times faster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8209" target="_blank">02:16:49.540</a></span> | <span class="t">than Bfloat 16. But in general, it's not. In general, it's like two to three. It's not going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8214" target="_blank">02:16:54.360</a></span> | <span class="t">to be four. The reason is because you're packing so many transistors in. You also have to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8218" target="_blank">02:16:58.600</a></span> | <span class="t">like energy. You have to like do the data movement. There's like other transistors you have to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8222" target="_blank">02:17:02.740</a></span> | <span class="t">I just -- approximately it's two to three times faster. That's float eight. Can you go even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8228" target="_blank">02:17:08.320</a></span> | <span class="t">lower? Why don't we do one bit?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8232" target="_blank">02:17:12.740</a></span> | <span class="t">One bit. So you must have the sign though. So you can't do one bit. So 1.58 bit, some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8239" target="_blank">02:17:19.940</a></span> | <span class="t">people have been talking about. Two bit. Two bit could be possible. The problem with two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8244" target="_blank">02:17:24.420</a></span> | <span class="t">bit is it's problematic. Because when you do two bit training -- yes, okay. So let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8251" target="_blank">02:17:31.300</a></span> | <span class="t">Let's do two bit, right? So what do you want to do? How many exponent? Zero? Remember, you have to have a sign</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8256" target="_blank">02:17:36.500</a></span> | <span class="t">bit. That's the most important. One for the exponent and fraction zero, right? Because remember, it's squared.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8262" target="_blank">02:17:42.740</a></span> | <span class="t">So plus one. Oh, wait. No. It's zero plus one. Okay. So it's one. Okay. 10 times faster? I don't think so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8273" target="_blank">02:17:53.220</a></span> | <span class="t">Okay. Maybe two bit is probably too low. Maybe four bit. Four bit could work. Yes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8279" target="_blank">02:17:59.380</a></span> | <span class="t">Oh, that's just because they wanted to do that. Just for easier calculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8297" target="_blank">02:18:17.620</a></span> | <span class="t">And then, for their tensor32, they use it. So that's what I would say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8302" target="_blank">02:18:22.340</a></span> | <span class="t">Tensor32 is not 32. It is -- they have it somewhere. NVIDIA. TensorFlow, it's 19.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8310" target="_blank">02:18:30.020</a></span> | <span class="t">That's the trick. They like to do marketing. And they say it's 32. But it's actually 19.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8315" target="_blank">02:18:35.460</a></span> | <span class="t">Yes. That's why it's the same. Okay. Any other questions? Someone else raised their hand?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8322" target="_blank">02:18:42.100</a></span> | <span class="t">Okay. But yes, I was going to say, like, you can do four bit, right? So four bit is actually a new -- NVIDIA's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8326" target="_blank">02:18:46.980</a></span> | <span class="t">new GPUs. The B100s do have four bit. So that is approximately two times faster. Now, the reason it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8335" target="_blank">02:18:55.780</a></span> | <span class="t">not -- okay, let me just try. Four bit. I think it's one plus -- it's probably like two plus two or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8342" target="_blank">02:19:02.260</a></span> | <span class="t">I don't know. Six. Okay. Right. It's not going to be that much faster because, as I said, there's power</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8348" target="_blank">02:19:08.260</a></span> | <span class="t">transistors and there's other transistors. You can only go so far. Just the jump from flow32 to flow16</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8352" target="_blank">02:19:12.980</a></span> | <span class="t">was very large. Yes. Yes. So a quick question. So for the example, the one bit bit net, that --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8359" target="_blank">02:19:19.860</a></span> | <span class="t">1.58 bit. Yeah. Yeah. So that would be an example of one bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8363" target="_blank">02:19:23.140</a></span> | <span class="t">So it's a different -- so actually, I had a tweet about this. 1.58 bit and float4 is the same in terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8370" target="_blank">02:19:30.900</a></span> | <span class="t">of number of transistors. You'd rather use float4. The reason why is 1.58 bit is you have to do more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8376" target="_blank">02:19:36.260</a></span> | <span class="t">manipulation to make it work. You have to use the straight through estimator. It's a horrible mess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8382" target="_blank">02:19:42.340</a></span> | <span class="t">You'd rather just use float4. So float4 and 1.58 bit are similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8386" target="_blank">02:19:46.820</a></span> | <span class="t">You get to create your own base model if you replicate the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8392" target="_blank">02:19:52.740</a></span> | <span class="t">Yes. What do you mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8396" target="_blank">02:19:56.100</a></span> | <span class="t">Which most of us have never done, right? Which would be --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8399" target="_blank">02:19:59.140</a></span> | <span class="t">Technium and the NOAA's research, we've replicated there some -- it does work. Somewhat. Somewhat works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8406" target="_blank">02:20:06.500</a></span> | <span class="t">I mean, yeah. It's one bit, but I mean --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8408" target="_blank">02:20:08.660</a></span> | <span class="t">1.58. Yeah. It's not actually one bit. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8410" target="_blank">02:20:10.740</a></span> | <span class="t">Yeah. I think it's like three. They call it one bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8412" target="_blank">02:20:12.660</a></span> | <span class="t">Oh, they like to call it one bit. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8414" target="_blank">02:20:14.100</a></span> | <span class="t">So, but my question is, like, so in theory, like, obviously, I don't know who works here, but like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8422" target="_blank">02:20:22.260</a></span> | <span class="t">most of us have never built a base model. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8426" target="_blank">02:20:26.420</a></span> | <span class="t">So -- Well, you could. Yeah. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8428" target="_blank">02:20:28.340</a></span> | <span class="t">You can with enough GPU power, but one bit, you know, that was -- and they even had, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8434" target="_blank">02:20:34.980</a></span> | <span class="t">a really great tutorial, and like -- but do you think that that's just like -- I'm just asking your opinion about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8442" target="_blank">02:20:42.420</a></span> | <span class="t">I don't think so if 1.8 -- 1.58 bit will be the future. I think NVIDIA is, like, the focus is on float4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8448" target="_blank">02:20:48.340</a></span> | <span class="t">They might go to float -- I think float4 might be the final precision. I don't think you can go any faster with that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8453" target="_blank">02:20:53.140</a></span> | <span class="t">I think float4 is a final -- no more. So we won't be having that much faster GPUs. I don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8458" target="_blank">02:20:58.100</a></span> | <span class="t">think so. I mean, float4 is actually -- they don't actually do float4 anymore. It's like you float6 for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8463" target="_blank">02:21:03.380</a></span> | <span class="t">the gradients, float6, and then float4 for the activations. Like, you know, it's very weird.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8467" target="_blank">02:21:07.620</a></span> | <span class="t">I mean, you could do, like, float3, float2, but, like, it's your diminishing returns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8472" target="_blank">02:21:12.900</a></span> | <span class="t">So, in ARM silicon, though, there's -- there's been, like, advances in, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8480" target="_blank">02:21:20.900</a></span> | <span class="t">super low bits, like, bits versus -- Fixpoint stuff? Is it called fixpoint stuff? Or --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8487" target="_blank">02:21:27.860</a></span> | <span class="t">Huh?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8488" target="_blank">02:21:28.420</a></span> | <span class="t">I think it's called fixed -- I knew ARM has fixpoint.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8490" target="_blank">02:21:30.980</a></span> | <span class="t">Oh, well, yeah. So it's -- I mean, just like the Snapdragon X, like the new --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8496" target="_blank">02:21:36.580</a></span> | <span class="t">Yes. They have -- so it's, like, customizable as well? Or -- I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8500" target="_blank">02:21:40.020</a></span> | <span class="t">Yeah. Well, okay. So the SDK is broken. You have to pass the -- so this is why you can technically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8506" target="_blank">02:21:46.580</a></span> | <span class="t">run Mixtrel 8x7v on your phone at, like, 20-something FPS -- sorry -- TPS is because you can use UFS 4.0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8518" target="_blank">02:21:58.100</a></span> | <span class="t">as flash storage and subsequently use that as memory. And then the -- but the thing is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8526" target="_blank">02:22:06.100</a></span> | <span class="t">then you're running at 2-bit precision, which is -- So that's probably why this -- so if you use 2-bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8532" target="_blank">02:22:12.660</a></span> | <span class="t">precision, that's why you have memory reductions. But there's actually papers which show that if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8535" target="_blank">02:22:15.780</a></span> | <span class="t">you do 2-bit for the MLP plus 4-bit for attention, that's actually the most appropriate. You can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8540" target="_blank">02:22:20.660</a></span> | <span class="t">actually do that. So that's not an invalid approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8544" target="_blank">02:22:24.260</a></span> | <span class="t">No, that's not invalid. Actually, it works. It works. The -- I've seen it work --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8547" target="_blank">02:22:27.700</a></span> | <span class="t">Möbius people did that, I think. Yeah. Yes, question. Sorry. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8553" target="_blank">02:22:33.140</a></span> | <span class="t">Two kind of related questions about precision. First one is, like, why is the negative bit -- like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8559" target="_blank">02:22:39.220</a></span> | <span class="t">you must have -- The sine bit? Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8561" target="_blank">02:22:41.300</a></span> | <span class="t">You don't have to. But it's generally -- generally, like, standard practice to have the sine bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8567" target="_blank">02:22:47.620</a></span> | <span class="t">In theory, you don't have to. The only problem is, if you don't have a sine bit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8572" target="_blank">02:22:52.260</a></span> | <span class="t">your numbers will be 0, 1, 2, right? But what happens if you wanted to, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8577" target="_blank">02:22:57.380</a></span> | <span class="t">make the model -- like, you're trying to not make the model learn negative directions anymore?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8582" target="_blank">02:23:02.180</a></span> | <span class="t">You could do that. I don't know if there are papers -- maybe you should write a paper about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8585" target="_blank">02:23:05.700</a></span> | <span class="t">Train a model on that, and let's see what -- okay, but -- yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8589" target="_blank">02:23:09.140</a></span> | <span class="t">Well, it's very related. I think it's all bits. It's all bits, right? Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8596" target="_blank">02:23:16.020</a></span> | <span class="t">Softmax, you're basically just linearly, like, getting stuff down to a certain number of bits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8600" target="_blank">02:23:20.820</a></span> | <span class="t">There's nothing special about softmax and things that could be, like, exponentially big,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8604" target="_blank">02:23:24.500</a></span> | <span class="t">sort of going this way, right? Like --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8606" target="_blank">02:23:26.260</a></span> | <span class="t">The reason -- it's because -- remember, when you do softmax,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8609" target="_blank">02:23:29.780</a></span> | <span class="t">you also have to normalize by the sum of the exponentials.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8612" target="_blank">02:23:32.740</a></span> | <span class="t">And if you do exponential of 10, you already get, like, some large number,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8616" target="_blank">02:23:36.020</a></span> | <span class="t">and this -- this probability will take over the entire sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8619" target="_blank">02:23:39.220</a></span> | <span class="t">Well, but you're not -- you're not, like, logging it. You're just square-booting it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8624" target="_blank">02:23:44.340</a></span> | <span class="t">No, no. It's -- it's -- it's -- the sum of exponentials divided by -- oh, sorry -- the exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8629" target="_blank">02:23:49.620</a></span> | <span class="t">divided by the sum of the exponentials. Yeah, but, like, the biggest exponential dominates the sum,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8633" target="_blank">02:23:53.860</a></span> | <span class="t">right? Yes. That's the problem, though. If you do that, then your model's not learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8637" target="_blank">02:23:57.940</a></span> | <span class="t">You're just trying to learn to predict one token. Why don't you just predict that one token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8642" target="_blank">02:24:02.100</a></span> | <span class="t">then? Like, the largest one that you did. That's kind of what you -- you're forcing the model to not learn anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8648" target="_blank">02:24:08.100</a></span> | <span class="t">That is why you have to, like, minus the maximum. That's a trick that we showed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8651" target="_blank">02:24:11.140</a></span> | <span class="t">It's, like, minus the maximum, and then you can, like, reduce this effect of this one token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8655" target="_blank">02:24:15.140</a></span> | <span class="t">or this one issue. So it's for training stability purposes. I don't know if that kind of -- okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8660" target="_blank">02:24:20.900</a></span> | <span class="t">probably that didn't answer your question, but -- okay. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8663" target="_blank">02:24:23.620</a></span> | <span class="t">From a back-to-back perspective, how much slower was that layer norm, and then do we know whether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8670" target="_blank">02:24:30.500</a></span> | <span class="t">that's actually more accurate than the way that -- That is a good question. To be honest,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8676" target="_blank">02:24:36.900</a></span> | <span class="t">I do not know. I don't think so. It changes too much. Layer norms, if you upcast, that's probably,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8683" target="_blank">02:24:43.860</a></span> | <span class="t">yeah, small effect. Small effect. But the reason why you need to upcast is because Gemma did it before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8689" target="_blank">02:24:49.700</a></span> | <span class="t">so you have to do it. Remember, the trick is you must follow what the original implementation does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8694" target="_blank">02:24:54.420</a></span> | <span class="t">Any other questions? Or -- okay. There are, like, some other issues which we showed that more -- okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8703" target="_blank">02:25:03.140</a></span> | <span class="t">it's funny, because it's all about upcasting, downcasting, and stuff like that. Each implementation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8706" target="_blank">02:25:06.580</a></span> | <span class="t">does its own thing. Unfortunately, how do you actually analyze this? You have to open three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8711" target="_blank">02:25:11.860</a></span> | <span class="t">screens up. The DeepMind one. The DeepMind one. Okay. Okay, I'm too excited. You have to open up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8719" target="_blank">02:25:19.700</a></span> | <span class="t">three implementations. DeepMind one, the HuggingFace one, the KRS one. You have to open up three screens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8724" target="_blank">02:25:24.180</a></span> | <span class="t">and you see line by line, what did they do, and then now you have to guess which one's the correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8728" target="_blank">02:25:28.740</a></span> | <span class="t">one. The guessing part is the most painful, so you have to, like, inquire your ass-hugging face,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8734" target="_blank">02:25:34.980</a></span> | <span class="t">which one's the correct one? You look at the paper, which one's the correct one? You assume the DeepMind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8738" target="_blank">02:25:38.980</a></span> | <span class="t">one's correct and stuff like that. So there's, like, some human component you have to guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8742" target="_blank">02:25:42.420</a></span> | <span class="t">Guessing. So that's probably why it can't be automated, right? These error-checking things cannot be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8748" target="_blank">02:25:48.020</a></span> | <span class="t">automated. It's because there's a human there which made these decisions. And so you have to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8753" target="_blank">02:25:53.220</a></span> | <span class="t">now you have to decide which one, which of those decisions did they choose. And you can't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8758" target="_blank">02:25:58.820</a></span> | <span class="t">automate this away. I guess you could automate this by doing the methodology which we described,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8763" target="_blank">02:26:03.460</a></span> | <span class="t">try all combinations and see which one has a lower error. I guess you could do that. But remember,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8767" target="_blank">02:26:07.220</a></span> | <span class="t">you must have the original implementation first. That is a problem. So there's, like, chicken and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8772" target="_blank">02:26:12.900</a></span> | <span class="t">egg problems. The rope position, so this is the one I was talking about, upcasting rope. This is in all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8780" target="_blank">02:26:20.340</a></span> | <span class="t">architectures now. You must not downcast rope. If you do, you will get wrong results. So previously on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8790" target="_blank">02:26:30.340</a></span> | <span class="t">left, right, if you see 8192, 8192, 8192, that's the positions. That is definitely incorrect, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8798" target="_blank">02:26:38.500</a></span> | <span class="t">What does that mean? Like, do you know why that's incorrect? 8192, 8192, 8192? Does anyone know why?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8802" target="_blank">02:26:42.900</a></span> | <span class="t">Remember, this is positions. Why is it all the same? Like, does anyone know why this is really bad?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8808" target="_blank">02:26:48.340</a></span> | <span class="t">So we kind of, like, essentially now we, the three words have the same position, right? 8192 is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8817" target="_blank">02:26:57.620</a></span> | <span class="t">position. And what is another big error of this? There's actually one more error. Let's assume the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8823" target="_blank">02:27:03.700</a></span> | <span class="t">maximum is 8192, the sequence length. What is 8192? It's out of bounds. Remember it's minus one for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8836" target="_blank">02:27:16.980</a></span> | <span class="t">Python, right? 8191 is the correct number, right? So if you correct this, you get 8189, 8190 and 8191,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8845" target="_blank">02:27:25.140</a></span> | <span class="t">right? And you can see all the numbers are like this. So the point is if you use, remember the whole point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8849" target="_blank">02:27:29.700</a></span> | <span class="t">of this problem is because we're using float 16 for faster training. Remember float 16 is how much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8854" target="_blank">02:27:34.900</a></span> | <span class="t">times faster? Yes, around 10 or 5 to 10, something around there, right? That is why you have to do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8861" target="_blank">02:27:41.140</a></span> | <span class="t">And these are the issues pop out because of this issue, right? We're trying to make training faster,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8864" target="_blank">02:27:44.900</a></span> | <span class="t">but then these issues come. And the jelly one, which we described before, this was the first bug that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8874" target="_blank">02:27:54.580</a></span> | <span class="t">found. Actually, I think this is the main reason why we were trying to look through bugs is we found</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8878" target="_blank">02:27:58.980</a></span> | <span class="t">that, oh, look, there is this bug in Jellu in the activation function. And so the point is Keras used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8885" target="_blank">02:28:05.780</a></span> | <span class="t">approximate Jellu, the PyTorch and the PyTorch version used exact Jellu, and Hugging Face also used exact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8893" target="_blank">02:28:13.380</a></span> | <span class="t">Jellu. And the question is, which one is the correct one? Is the exact Jellu correct? Is approximate Jellu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8899" target="_blank">02:28:19.780</a></span> | <span class="t">Jellu correct? So what's the difference between exact and Jellu activation function?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8904" target="_blank">02:28:24.580</a></span> | <span class="t">There is the, where is the, I don't know if they have the exact and the, that's called flex.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8912" target="_blank">02:28:32.580</a></span> | <span class="t">Oh, okay. That's night mode. Oh, that's even worse. Okay. Whatever. Oh wait, that's Prelude. Where is Jellu?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8924" target="_blank">02:28:44.420</a></span> | <span class="t">Oh wait, no, I have to find it right. Yes. Right. So like the, the exact Jellu is this one, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8934" target="_blank">02:28:54.340</a></span> | <span class="t">There's an error function. Um, okay. My thing is not rendering it properly. Um, but if you essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8940" target="_blank">02:29:00.340</a></span> | <span class="t">what you do is you use Desmos. So what I like to do is I use Desmos, um, Desmos, right. And literally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8947" target="_blank">02:29:07.220</a></span> | <span class="t">plot them together, plot them on the graph, right? So like, if you have, right. X, Y is equal to X over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8953" target="_blank">02:29:13.140</a></span> | <span class="t">two, right. You literally type this in. And what is this one? I think you can do error function. Oh yes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8958" target="_blank">02:29:18.740</a></span> | <span class="t">you can. Right. You can do error function, right. X divided by square root of two. All right. That's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8965" target="_blank">02:29:25.700</a></span> | <span class="t">exact Jellu. Right. Now you type in this complicated formula for X divided by two. Um, one, one plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8975" target="_blank">02:29:35.540</a></span> | <span class="t">than what? I don't remember this square root of two divided by what? Was it pi? Oh, it's pi. Pi. And the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8984" target="_blank">02:29:44.900</a></span> | <span class="t">what? Um, X plus zero point. Can I? Oh, we can't. Okay. Zero, four, four, seven. Zero, four, four, seven.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=8994" target="_blank">02:29:54.660</a></span> | <span class="t">What was the? Oh, one, five. One, five X to the cubed. Was it cubed? Yeah. Okay. Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9003" target="_blank">02:30:03.780</a></span> | <span class="t">Oh, is it? Oh, you're right. Okay. Wait, is that? Oh, is it just the rendering problem? Or is it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9011" target="_blank">02:30:11.380</a></span> | <span class="t">square root? No, no, no. It's square root of two over pi, I think. Wait, is that correct? Wait,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9016" target="_blank">02:30:16.820</a></span> | <span class="t">something. I did something wrong. Maybe I did something wrong. Oh, whatever. Who cares? Just,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9021" target="_blank">02:30:21.300</a></span> | <span class="t">just assume. Okay. Oh, wait. Oh, yeah. You're right. You're right. I put the square root everywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9025" target="_blank">02:30:25.220</a></span> | <span class="t">Oh, is that what you're saying? Yeah. Oh, okay. Oh, no, no, no, no, no. What? No, get rid of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9032" target="_blank">02:30:32.180</a></span> | <span class="t">Okay. Let me just, no, it's, it's tan of everything. Now I have to do this. Oh, yeah. This is probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9039" target="_blank">02:30:39.780</a></span> | <span class="t">have to play around with this. Oh, there we go. There we go. Right. So the blue line,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9043" target="_blank">02:30:43.620</a></span> | <span class="t">if you remove it, the blue line and the red line, right? They're the same thing. But what's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9048" target="_blank">02:30:48.660</a></span> | <span class="t">difference? Remember in Desmos, I don't know if people know this, but you can actually do derivatives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9052" target="_blank">02:30:52.980</a></span> | <span class="t">D over DX. Did anyone know this? You can actually do derivatives. You get your D over DX. And then you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9060" target="_blank">02:31:00.420</a></span> | <span class="t">do this as well. D over DX. Right. And they generally align by the exact JLU and the approximate JLU generally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9070" target="_blank">02:31:10.820</a></span> | <span class="t">align. And guess what? You can also do integration integral of minus infinity. Oh, did I spell it wrong?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9079" target="_blank">02:31:19.540</a></span> | <span class="t">Oh, infinity to infinity, right? Of, I think this works. I'm not a hundred percent sure. Right. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9088" target="_blank">02:31:28.820</a></span> | <span class="t">take your exact JLU, you minus the difference. Oh, I don't think so. This works. I, I, I, yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9094" target="_blank">02:31:34.420</a></span> | <span class="t">I don't think so. Oh yes, it works. Yes, it works. So what you do is you can take the integral of minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9099" target="_blank">02:31:39.140</a></span> | <span class="t">infinity to infinity. So the entire line, you minus exact JLU and the approximate JLU and you do DX.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9104" target="_blank">02:31:44.900</a></span> | <span class="t">And there is a difference, right? But the difference is very small, right? It's like 10 to the minus 16.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9109" target="_blank">02:31:49.860</a></span> | <span class="t">It's very, very small. And notice it's like, when you do, when we do fast triton kernels,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9115" target="_blank">02:31:55.620</a></span> | <span class="t">I generally use this feature. So you do, you can do integration, integration and derivatives,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9121" target="_blank">02:32:01.940</a></span> | <span class="t">and you know, you can use Desmos. So I highly recommend Desmos. Um, and if you do this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9127" target="_blank">02:32:07.300</a></span> | <span class="t">that's where we found the problem. It's like, oh, okay, there is some sort of issue. And if you fix it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9133" target="_blank">02:32:13.220</a></span> | <span class="t">remember the JLU fix does do some effect. It does do some effect. But remember, we only showed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9139" target="_blank">02:32:19.540</a></span> | <span class="t">there was only very small effect. So it's not that useful. Um, the rope fix was the most important,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9144" target="_blank">02:32:24.180</a></span> | <span class="t">right? The rope fix actually caused issues. Um, so you must fix that. And that's the most important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9149" target="_blank">02:32:29.940</a></span> | <span class="t">fix that you must do. Um, and finally, there is like some other things that we do, um, depending on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9156" target="_blank">02:32:36.260</a></span> | <span class="t">the precision that you use, there is actually a difference between float 16 and bleed float 16.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9160" target="_blank">02:32:40.420</a></span> | <span class="t">And if you do this, we show that float 32, um, remember we showed before that in the fixes that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9167" target="_blank">02:32:47.700</a></span> | <span class="t">we did, the lines sometimes go back up, right? But actually, if you do float 32, it actually does work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9175" target="_blank">02:32:55.140</a></span> | <span class="t">If you do float 32 position, the lines actually don't do separate very well. But once you use float 16,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9180" target="_blank">02:33:00.820</a></span> | <span class="t">the lines then match up again, right? And be float 16, the lines match up again, right? So this is just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9186" target="_blank">02:33:06.100</a></span> | <span class="t">phenomenon that you're using faster, smaller positions. And that is why you have this problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9190" target="_blank">02:33:10.260</a></span> | <span class="t">But if you do use full precision, you get good results. And the fine tuning notebook for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9196" target="_blank">02:33:16.900</a></span> | <span class="t">Gemma one also works. So Gemma is two times faster and uses like, I think 60% less memory as well. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9202" target="_blank">02:33:22.100</a></span> | <span class="t">more now. Um, so if you run this, remember you have to connect to, um, you have to connect to your Google</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9207" target="_blank">02:33:27.380</a></span> | <span class="t">account and you will get this to run. Any questions on the Gemma one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9211" target="_blank">02:33:31.220</a></span> | <span class="t">Okay. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9215" target="_blank">02:33:35.140</a></span> | <span class="t">Yes. Um, where did I put the picture? Oh, it's in the blog post. Um, yes, that's fine. Um, wait,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9232" target="_blank">02:33:52.580</a></span> | <span class="t">where did I put it? Oh, it's the first picture. Right. Yeah. This one, right? So the x axis is the layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9239" target="_blank">02:33:59.700</a></span> | <span class="t">number. So Gemma has 18 layers. So each of those, the x axis just indicates which layer the, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9245" target="_blank">02:34:05.940</a></span> | <span class="t">which layer it is. The y axis is log to log to norm, um, log, log L2 norm. So what you do is you take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9253" target="_blank">02:34:13.780</a></span> | <span class="t">the original implementation, like deep minds implementation, you take hugging face, PyTorch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9258" target="_blank">02:34:18.420</a></span> | <span class="t">Gemma, like, you know, the other implementations, you check the output of both of them. So the output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9264" target="_blank">02:34:24.340</a></span> | <span class="t">you run the model through, you take output to layer one and outputs layer one, the other implementations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9269" target="_blank">02:34:29.860</a></span> | <span class="t">and you just find the error. Um, and so this is just the error. Um, and this is log scale. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9274" target="_blank">02:34:34.580</a></span> | <span class="t">so when it's log scale, it looks better when it's not log scale, it looks very bad. Um, so does that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9280" target="_blank">02:34:40.980</a></span> | <span class="t">is that better? Yes. Output of each layer. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9289" target="_blank">02:34:49.540</a></span> | <span class="t">Um, that's called Gemma. So that's for Gemma. For fee three, um, similar. What you do is you open</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9295" target="_blank">02:34:55.940</a></span> | <span class="t">up the fee three implementation, you read through the fee three implementation. And because like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9300" target="_blank">02:35:00.020</a></span> | <span class="t">you guys like probably most likely can go through Lama and like, just look at it in general, remember,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9305" target="_blank">02:35:05.300</a></span> | <span class="t">delete useless parts of the code, you will see there are differences in fee three. Um, and the differences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9312" target="_blank">02:35:12.740</a></span> | <span class="t">are, um, they use other methodologies. Um, they use upcasting, they use stuff, but there was a weird</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9318" target="_blank">02:35:18.740</a></span> | <span class="t">thing that we found in the config file. Um, I will show you, um, fee three config.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9325" target="_blank">02:35:25.140</a></span> | <span class="t">Okay. I'll just use the instruct version. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9331" target="_blank">02:35:31.380</a></span> | <span class="t">if you go to always, when you go to like new models, always read the config file, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9338" target="_blank">02:35:38.580</a></span> | <span class="t">Config.json. When you open it up, it tells you all the tricks you need to know about the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9344" target="_blank">02:35:44.020</a></span> | <span class="t">architecture. Um, and I highly, right. It tells you what is the EOS token ID? 32,000. Right? When</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9350" target="_blank">02:35:50.500</a></span> | <span class="t">you look at this, Hmm, is that a good idea? 32,000, 32,000. What is the EOS token ID? Right? 32,000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9356" target="_blank">02:35:56.820</a></span> | <span class="t">Okay. That's fine. The pad token. No. Hmm. Is that a good idea? Like you have to think about like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9362" target="_blank">02:36:02.020</a></span> | <span class="t">why are these, why are they there? How many layers does fee three have? It's 40, right? So 40 layers. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9370" target="_blank">02:36:10.260</a></span> | <span class="t">how many positional encodings does it have? So how long, what is the context length? It is one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9374" target="_blank">02:36:14.660</a></span> | <span class="t">three, one, zero, seven, two. That's the context. Um, remember it's 100. So this model, the fee three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9381" target="_blank">02:36:21.140</a></span> | <span class="t">medium is 128 K, right? It's not one, one, two, eight, zero, zero, zero, zero, right? Just be careful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9386" target="_blank">02:36:26.180</a></span> | <span class="t">It's actually 128 K, right? It's one, three, one, zero, seven, two. Um, there are other issues with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9392" target="_blank">02:36:32.180</a></span> | <span class="t">this model as well. Um, okay. This is, okay. That's the, okay. That's probably, okay. Probably don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9396" target="_blank">02:36:36.820</a></span> | <span class="t">use the instruct version. The instruct, sorry, the true, choose a small version. Um, this is a smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9403" target="_blank">02:36:43.940</a></span> | <span class="t">version. There is a thing we noticed as a sliding window. So Mr. Has sliding window, um, sliding window</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9409" target="_blank">02:36:49.060</a></span> | <span class="t">essentially attends to only 2048 tokens. Um, and this just makes training much faster. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9415" target="_blank">02:36:55.460</a></span> | <span class="t">and does anyone notice what the problem is for this? Why is it 2047? Anyone notice any issues?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9420" target="_blank">02:37:00.980</a></span> | <span class="t">Yes. Well, it's not a power of two, right? Correct. So is that weird? I mean, that's horrible. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9429" target="_blank">02:37:09.700</a></span> | <span class="t">So I did ask the hugging face people and they said, yes, it is a bug. So they actually did fix it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9434" target="_blank">02:37:14.660</a></span> | <span class="t">but then I don't know why they reverted it back. So I'm a bit confused. Um, they never,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9438" target="_blank">02:37:18.180</a></span> | <span class="t">they kind of forgot about this. Yeah. So it's actually, it's supposed to be 2048. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9442" target="_blank">02:37:22.740</a></span> | <span class="t">yeah, because that's, that only makes sense. Because if you're training on like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9446" target="_blank">02:37:26.740</a></span> | <span class="t">you're training on the correct context, right? Then this sliding window makes no sense. In fact,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9452" target="_blank">02:37:32.740</a></span> | <span class="t">I've seen a lot of sliding window above reaches. Yeah. So yeah, I, I, yeah, I'm not sure why. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9459" target="_blank">02:37:39.540</a></span> | <span class="t">but I'm pretty sure this should be 2048. Yeah. I'm very confident. I, I've actually, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9465" target="_blank">02:37:45.220</a></span> | <span class="t">what I have to say is 2048. Yeah, it's not, yeah. And yeah, so these, these small issues are,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9471" target="_blank">02:37:51.860</a></span> | <span class="t">they need to fix. Um, they still have not fixed. Um, but when on stuff, it's fixed. So we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9477" target="_blank">02:37:57.220</a></span> | <span class="t">uploaded models, which fixed them, right? So if you go to, uh, on stuff, hugging face repo,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9481" target="_blank">02:38:01.620</a></span> | <span class="t">we actually have models, which we fixed all of them. Oh, this is too big. Um, where's the fee one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9488" target="_blank">02:38:08.260</a></span> | <span class="t">Oh, I didn't put it up. Okay. I need to find the fee one now. Um, where's fee? Oh, that fee three,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9496" target="_blank">02:38:16.260</a></span> | <span class="t">mini four K instruct, right? If you go to files, you go to config dot Jason, we fixed it. Uh, and there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9503" target="_blank">02:38:23.380</a></span> | <span class="t">other things that we did to fix it. Um, for example, the pad token ID is 30. Okay. That's actually wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9508" target="_blank">02:38:28.180</a></span> | <span class="t">Okay. Um, okay. I need to fix my own. Okay. Anyways, um, there is a bug which we discovered ourselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9512" target="_blank">02:38:32.820</a></span> | <span class="t">It should be 30. This is actually wrong. Another thing is you must not make the pad token the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9518" target="_blank">02:38:38.420</a></span> | <span class="t">token ID as EOS. Never, never, never, never, never. Um, this must be a different token to the EOS token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9525" target="_blank">02:38:45.140</a></span> | <span class="t">Um, I do not unstop. We automatically fixed this during the loading. It's just the config itself is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9529" target="_blank">02:38:49.460</a></span> | <span class="t">right. Um, but that's okay. Unstop itself is fine. Um, just the config is a bit wrong. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9534" target="_blank">02:38:54.100</a></span> | <span class="t">oh, okay. I found my own box, but okay. Yes. Um, so, okay. Oh, yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9539" target="_blank">02:38:59.620</a></span> | <span class="t">I'm not going to slow down the, the, I'm not going to, you keep going because there's a lot of slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9543" target="_blank">02:39:03.220</a></span> | <span class="t">Okay. Oh, yeah. Yeah. Actually this, okay. There's not that much slides. Okay. Actually, there is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9546" target="_blank">02:39:06.820</a></span> | <span class="t">Oh, okay. I just noticed we have more. Okay. Um, so another one is like fee three used. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9553" target="_blank">02:39:13.220</a></span> | <span class="t">they merged the Q, K and K. Remember we did Q, K and V. They're unmerged, right? The weights are separate for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9558" target="_blank">02:39:18.900</a></span> | <span class="t">attention matrices. V three did a very interesting move is that they fuse them into one matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9564" target="_blank">02:39:24.340</a></span> | <span class="t">And we found that to be very problematic for fine tuning. Um, because if you fuse them together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9569" target="_blank">02:39:29.220</a></span> | <span class="t">um, when you do Laura adapters, you actually, you actually only learn new extra weights and that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9574" target="_blank">02:39:34.740</a></span> | <span class="t">very less. Um, so please unfuse them. Um, and we do this. So our version of the V three actually unfuses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9582" target="_blank">02:39:42.420</a></span> | <span class="t">the weights. Um, you must unfuse. Actually, I have to like highly suggest you to unfuse the weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9587" target="_blank">02:39:47.300</a></span> | <span class="t">You can only fuse them if you want to do training faster. Um, this will make training like maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9591" target="_blank">02:39:51.300</a></span> | <span class="t">five percent faster. It's actually not that much. It's like two, two percent. You actually increase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9595" target="_blank">02:39:55.300</a></span> | <span class="t">memory usage a lot. So just be careful of that as well. Um, oh, yes. They actually did. So this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9600" target="_blank">02:40:00.820</a></span> | <span class="t">the sliding window one. They actually fixed it. Um, and then they unfixed it. I think they just forgot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9605" target="_blank">02:40:05.620</a></span> | <span class="t">about it. I'll probably like push them again to fix it. Um, and this is the fusing of the weights. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9610" target="_blank">02:40:10.260</a></span> | <span class="t">so we show that if you actually unfuse the weights, so Q, K and V must be separate. You must not combine them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9615" target="_blank">02:40:15.620</a></span> | <span class="t">Um, if you combine them, you actually have lower accuracy. So please do not do that. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9620" target="_blank">02:40:20.260</a></span> | <span class="t">for tokenization, remember this slide, which I show you about the, um, the smiley faces are like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9625" target="_blank">02:40:25.700</a></span> | <span class="t">spaces and each ones are different tokenizations. There are actually many issues, um, for tokenizations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9630" target="_blank">02:40:30.660</a></span> | <span class="t">Um, this is a totally different separate topic from finding bugs and issues in language models. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9636" target="_blank">02:40:36.180</a></span> | <span class="t">this is a whole topic of its own because tokenizers are very problematic. Um, and they're very hard to find and fix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9643" target="_blank">02:40:43.220</a></span> | <span class="t">Did I double this slide? Okay, I doubled that. Um, and also we have new Olama support, which we have not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9649" target="_blank">02:40:49.940</a></span> | <span class="t">announced yet, which you can try out. So lots of people have asked us for how do we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9655" target="_blank">02:40:55.140</a></span> | <span class="t">fine tune a language model and export it to Olama, um, effectively. Um, does any, does people do know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9662" target="_blank">02:41:02.660</a></span> | <span class="t">what's Olama or no, or does anyone not know what's Olama? Okay. So Olama is like a interface. When you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9669" target="_blank">02:41:09.940</a></span> | <span class="t">when you fine tune a model, you have to run it, right? You have to run the model somewhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9674" target="_blank">02:41:14.100</a></span> | <span class="t">And Olama just makes you run the model much easier. Um, so like you know, chattpt, chattpt is like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9679" target="_blank">02:41:19.860</a></span> | <span class="t">running mechanism. Olama is just like chattpt, but they don't have the model. You have to select a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9683" target="_blank">02:41:23.780</a></span> | <span class="t">model. Um, that's kind of Olama. Um, yes. How did you manage to like, um, so I've been working on converting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9690" target="_blank">02:41:30.820</a></span> | <span class="t">uh, creating model files using, uh, um, the, uh, automated pipeline, but we've been found many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9700" target="_blank">02:41:40.020</a></span> | <span class="t">issues trying to automate model files. Um, is this using Unslap though? Or is using Axolotl or something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9706" target="_blank">02:41:46.820</a></span> | <span class="t">or other ones? Did you automate the model file yourself or? Well, we, yeah, we, because we need,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9712" target="_blank">02:41:52.020</a></span> | <span class="t">we need our model file, right? Oh, so we do this automatically now. So with Unslap, we actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9716" target="_blank">02:41:56.740</a></span> | <span class="t">we spent, I spent like a few, one month on trying to automate the model file creation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9720" target="_blank">02:42:00.580</a></span> | <span class="t">That's why we were struggling so hard as a company. Yes, um, uh, we, I have code for that somewhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9726" target="_blank">02:42:06.740</a></span> | <span class="t">Yeah, if you could, yeah, okay, this is open source. Oh, yeah, it's, it's already in the GitHub repos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9732" target="_blank">02:42:12.900</a></span> | <span class="t">If you go to Unslap, um, you go to chat templates, we have code for that, um, Olama. It is still very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9739" target="_blank">02:42:19.460</a></span> | <span class="t">ugly. Um, so these are the chat templates for, remember the BOS token, someone mentioned you have to add it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9743" target="_blank">02:42:23.300</a></span> | <span class="t">Um, yeah, add the BOS token. This is the Olama chat template.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9746" target="_blank">02:42:26.420</a></span> | <span class="t">Um, which we, so Olama has a, um, Olama has a specific requirement that you must have a chat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9751" target="_blank">02:42:31.460</a></span> | <span class="t">template because if you don't use a correct chat template, your model will output incorrect, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9756" target="_blank">02:42:36.420</a></span> | <span class="t">substandard responses. Um, so this is the chat template for like some of them. I had to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9761" target="_blank">02:42:41.460</a></span> | <span class="t">we had to write chat templates for all of the architectures. Um, and we have an automatic one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9766" target="_blank">02:42:46.740</a></span> | <span class="t">So these are Vakuna and blah, blah, blah, um, alpaca style, um, Gemma, the Gemma style. We also have that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9774" target="_blank">02:42:54.340</a></span> | <span class="t">We have many, many, even the Lama three chat template we have as well. Um, now for the automatic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9779" target="_blank">02:42:59.380</a></span> | <span class="t">one. So what we do is we can actually make an automatic chat template, a model five for you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9784" target="_blank">02:43:04.020</a></span> | <span class="t">automatically. Um, and this makes your fine tune much more accurately. Um, wait, I'll show you the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9790" target="_blank">02:43:10.420</a></span> | <span class="t">where is the code for that? Um, where is the code? Okay. You can see the code is quite large for the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9795" target="_blank">02:43:15.540</a></span> | <span class="t">just the chat templates, right? This is just for tokenization. So it's not even the, yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9800" target="_blank">02:43:20.020</a></span> | <span class="t">This is Apache 2.0, right? Yes. It's Apache. Yes. It's open source. Yeah. Um, wait, where is it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9806" target="_blank">02:43:26.580</a></span> | <span class="t">Okay. So we have something called pass combined prompt, which does some O of N squared. I didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9813" target="_blank">02:43:33.460</a></span> | <span class="t">actually optimize this. It does O of N squared. I should have done O of N, but anyways, it's O of N squared.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9818" target="_blank">02:43:38.660</a></span> | <span class="t">Um, checking the prompt. Um, here's the prompt format. So we do, it looks quite ugly, the code for automatic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9826" target="_blank">02:43:46.180</a></span> | <span class="t">model file creation, but we actually made it so you can actually automatically create a model file</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9831" target="_blank">02:43:51.460</a></span> | <span class="t">from your chat template. Um, you can see it's quite ugly. Um, but it works. Um, and, uh, yes. Oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9839" target="_blank">02:43:59.380</a></span> | <span class="t">it's even more ugly. Yup. It's quite ugly code. Um, but unfortunately the model file is very hard to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9844" target="_blank">02:44:04.260</a></span> | <span class="t">create automatically. And so we have the notebook, which allows you to do this. Um, so, so this notebook</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9851" target="_blank">02:44:11.220</a></span> | <span class="t">is in here for Alpaca. So this one's for the Alpaca dataset. And so this is our installation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9857" target="_blank">02:44:17.860</a></span> | <span class="t">Lama 3. Um, where is it? So we, so we'll be using Alpaca GPT-4, um, GPT-4 dataset. So you use the Alpaca</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9866" target="_blank">02:44:26.740</a></span> | <span class="t">dataset and you use GPT-4 to create the dataset. Um, and the trick is though, um, we also have a CSV file</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9875" target="_blank">02:44:35.700</a></span> | <span class="t">now, so you can actually upload a CSV file and use unsoft directly to fine tune the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9880" target="_blank">02:44:40.180</a></span> | <span class="t">Um, and, but the problem is a language model must have an instruction and output, right? Only two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9885" target="_blank">02:44:45.780</a></span> | <span class="t">columns. CSV files and Excel files can have many columns. So what do you do? You have to merge the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9891" target="_blank">02:44:51.300</a></span> | <span class="t">columns into one. Um, so remember each of those columns in your Excel file, convert them into text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9897" target="_blank">02:44:57.460</a></span> | <span class="t">And for example, the Titanic dataset, you merge them to say they have one siblings and spouses and so on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9903" target="_blank">02:45:03.780</a></span> | <span class="t">right? You merge the rope into one row. Um, and that's what you do. And with unsoft, you can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9909" target="_blank">02:45:09.220</a></span> | <span class="t">this now. It's, I still probably need to edit the, um, the like syntax calling, but this merging technique</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9916" target="_blank">02:45:16.980</a></span> | <span class="t">says, okay, your first column is called an instruction column. And the two double brackets means it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9922" target="_blank">02:45:22.100</a></span> | <span class="t">optional. Um, so if, if the input column exists, then it will say the instruction followed by your input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9929" target="_blank">02:45:29.140</a></span> | <span class="t">is. Um, and you can like make this very crazy. You can do as many columns as you like. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9934" target="_blank">02:45:34.740</a></span> | <span class="t">I don't know if the syntax is useful, but like, I will probably be editing this. We're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9938" target="_blank">02:45:38.340</a></span> | <span class="t">make a YouTube video about this to talk about this. Um, this is actually very important for fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9942" target="_blank">02:45:42.740</a></span> | <span class="t">Um, we noticed that every single provider requires you to use only one column for instruction and one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9949" target="_blank">02:45:49.140</a></span> | <span class="t">output column. Now you can have infinite columns. Well, how many you like, but you must define the chat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9953" target="_blank">02:45:53.700</a></span> | <span class="t">template. Um, and, and we also have a custom customizable chat template. Um, so before when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9960" target="_blank">02:46:00.980</a></span> | <span class="t">you do fine tuning of language models, you have to use the alpaca prompt, um, and our other notebooks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9964" target="_blank">02:46:04.900</a></span> | <span class="t">right? Below is an instruction that describes the task paired with an input, blah, blah, blah. You put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9968" target="_blank">02:46:08.900</a></span> | <span class="t">your instruction here, you put your input here, and you put your output here, right? But notice,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9973" target="_blank">02:46:13.300</a></span> | <span class="t">what is the problem with this? Is there a problem with this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9978" target="_blank">02:46:18.020</a></span> | <span class="t">So you must only put one instruction and one output or response, right? The input is the problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9983" target="_blank">02:46:23.140</a></span> | <span class="t">right? So how do you solve this? You solve this by merging the input into your instruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9987" target="_blank">02:46:27.860</a></span> | <span class="t">prompt, right? So this actually should be removed entirely, right? And your, your input should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9994" target="_blank">02:46:34.020</a></span> | <span class="t">something else. And what you do is we can actually, you're now, we can do this now, right? So you must</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=9998" target="_blank">02:46:38.180</a></span> | <span class="t">do, you must put the input and you must put an output, right? You can only use two columns now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10002" target="_blank">02:46:42.900</a></span> | <span class="t">but you can use, remember, even though you can only use two columns, you can use this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10007" target="_blank">02:46:47.220</a></span> | <span class="t">to convert your data set into two columns. Um, yes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10011" target="_blank">02:46:51.460</a></span> | <span class="t">Do you lose any of the semantic meanings, though?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10014" target="_blank">02:46:54.660</a></span> | <span class="t">Oh, no, I don't think so. No.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10015" target="_blank">02:46:55.780</a></span> | <span class="t">You don't think so?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10016" target="_blank">02:46:56.580</a></span> | <span class="t">No, I don't think. So it depends on how you, it depends on how you format the data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10020" target="_blank">02:47:00.340</a></span> | <span class="t">Remember, it's a language model, so you can do, the more you tell the language model what to do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10024" target="_blank">02:47:04.580</a></span> | <span class="t">the better. Of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10025" target="_blank">02:47:05.940</a></span> | <span class="t">Yeah. But the problem is, to do the model file creation, you must do two iterations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10030" target="_blank">02:47:10.820</a></span> | <span class="t">repetitions of this, right? You must do instruction response, and then you do another one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10034" target="_blank">02:47:14.900</a></span> | <span class="t">you must. Okay, you must do this for Unsloth. I found this to be very, very important for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10038" target="_blank">02:47:18.180</a></span> | <span class="t">model file creation. If you do not do this, you have dangling new lines, and you actually make your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10041" target="_blank">02:47:21.460</a></span> | <span class="t">model output terrible. Um, so you must do two repetitions of this. Okay, it's a must, must, must.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10046" target="_blank">02:47:26.020</a></span> | <span class="t">Um, and if you don't do that, we'll error out. Um, and so once you do this, we also have examples of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10051" target="_blank">02:47:31.700</a></span> | <span class="t">for example, this is Lama 3's chat template, right? We again, do two iterations. You must do two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10057" target="_blank">02:47:37.060</a></span> | <span class="t">iterations. Most important. Um, and when you finish training the model, um, remember you can do run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10063" target="_blank">02:47:43.700</a></span> | <span class="t">time, run all. Um, you can do inference now, right? Continue the Fibonacci sequence. Your input is one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10070" target="_blank">02:47:50.020</a></span> | <span class="t">one, two, whatever. And the next Fibonacci sequence is 13. I think that's correct. Yes, that's correct. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10075" target="_blank">02:47:55.860</a></span> | <span class="t">So your language model has learned how to do Fibonacci. And because it's a chat template,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10080" target="_blank">02:48:00.180</a></span> | <span class="t">you can also do, you can shove in multiple messages into the model. Um, so this becomes a ChatGPT for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10085" target="_blank">02:48:05.940</a></span> | <span class="t">you. This is a customized ChatGPT that you can use. Um, and finally, when you want to save the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10091" target="_blank">02:48:11.860</a></span> | <span class="t">um, you can save it to LoRa adapters. So this only is 100 MB in size. So once you find true the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10097" target="_blank">02:48:17.940</a></span> | <span class="t">you have 100 MB. But some people also want to like merge the model back. Um, and that will take 16 GB.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10105" target="_blank">02:48:25.940</a></span> | <span class="t">Um, but you must merge this for like, um, olama support and GG ref and stuff like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10110" target="_blank">02:48:30.500</a></span> | <span class="t">And what we show for olama support is you first have to like, you know, install olama. Um, you select</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10116" target="_blank">02:48:36.660</a></span> | <span class="t">what you want to save the model to GG ref. So this is eight bit. Um, we now support multiple quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10123" target="_blank">02:48:43.220</a></span> | <span class="t">methods, right? You don't have to do eight bit. You can do like four bit, five bit, whatever you like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10127" target="_blank">02:48:47.300</a></span> | <span class="t">And this will be saved into one go a much faster. Um, in fact, I think this will save you like 20 minutes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10131" target="_blank">02:48:51.540</a></span> | <span class="t">of your time. Um, and we save this automatically. Um, okay. And this does all the saving, blah, blah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10136" target="_blank">02:48:56.820</a></span> | <span class="t">blah saves. And we also, you see, we automatically create an olama model file automatically using a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10143" target="_blank">02:49:03.220</a></span> | <span class="t">chat template. And I can verify this is actually correct because I tried it. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10146" target="_blank">02:49:06.500</a></span> | <span class="t">and then when you want to serve the model file, you can actually print out the model file, which we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10150" target="_blank">02:49:10.180</a></span> | <span class="t">created. And this is the model file. Um, whoops, I pressed run already. Um, anyways, um, and finally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10156" target="_blank">02:49:16.100</a></span> | <span class="t">you to serve it, you can just do a model file to serve it. Um, and you can solve this. Um, and we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10162" target="_blank">02:49:22.260</a></span> | <span class="t">have a CSV version, so you can actually use the Titanic data set. Um, okay, it's loading. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10169" target="_blank">02:49:29.700</a></span> | <span class="t">so if you want to use the Titanic data set, you can upload the Titanic data set, right? I upload the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10174" target="_blank">02:49:34.660</a></span> | <span class="t">Titanic CSV. You can use the CSV file for this. Um, and again, you have the merger columns and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10180" target="_blank">02:49:40.580</a></span> | <span class="t">Right? This is a more complicated example. Um, in fact, I should provide this entire example for you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10185" target="_blank">02:49:45.940</a></span> | <span class="t">for the entire Titanic data set to merge all the columns into one. Um, and it's the same exact output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10191" target="_blank">02:49:51.620</a></span> | <span class="t">So that's the new books that we're sharing for. We did not release this yet. So this is for you guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10195" target="_blank">02:49:55.940</a></span> | <span class="t">to like experiment and see if there's any issues. Um, yeah. And just tell me, um, we also have blog</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10200" target="_blank">02:50:00.980</a></span> | <span class="t">posts on our website, which you can see, um, our unslaw GitHub repo. Um, and we have stickers available.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10208" target="_blank">02:50:08.260</a></span> | <span class="t">Um, and they're very, very cute for you to take and thank. Yeah. And also, yeah, we have Q and A now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10213" target="_blank">02:50:13.540</a></span> | <span class="t">Yeah. Yes. Oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10214" target="_blank">02:50:14.740</a></span> | <span class="t">the problem is if you put them in Jason format, you still need to have instruction and output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10233" target="_blank">02:50:33.700</a></span> | <span class="t">So how would you do that? You need to have two columns only for fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10238" target="_blank">02:50:38.180</a></span> | <span class="t">Can you do like, you know, your, in your template here, you have instruction and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10245" target="_blank">02:50:45.860</a></span> | <span class="t">you add, you add all of the other columns onto it. Can you just put, put the same instruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10252" target="_blank">02:50:52.660</a></span> | <span class="t">and then Jason of the, of the values of the columns?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10255" target="_blank">02:50:55.940</a></span> | <span class="t">Yes, you could, you could do the Jason file. Yes, you can. Um, but we just show you that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10260" target="_blank">02:51:00.100</a></span> | <span class="t">can do multiple columns now. So like if you have like 10 columns, you can now make the 10 columns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10264" target="_blank">02:51:04.740</a></span> | <span class="t">into one, um, by merging them together. Does that kind of, there's a big difference in representing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10270" target="_blank">02:51:10.900</a></span> | <span class="t">that merge columns as an English sentence or like a dictionary. Oh no, you can't use, you mean like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10277" target="_blank">02:51:17.780</a></span> | <span class="t">you shove in the actual dictionary for fine tuning. Um, you could do that. I don't, I think you should do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10283" target="_blank">02:51:23.300</a></span> | <span class="t">English language because our language model predicts the next word. Jason is probably less useful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10288" target="_blank">02:51:28.660</a></span> | <span class="t">always convert it into English. Research paper. Yes. There should be another research paper. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10297" target="_blank">02:51:37.460</a></span> | <span class="t">Any other questions? Yes. A lot of upvoted questions from me on the chat. Sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10304" target="_blank">02:51:44.260</a></span> | <span class="t">Oh yeah. I, I, I didn't actually check the slider questions. Whoopsies. Um, it actually didn't load.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10312" target="_blank">02:51:52.580</a></span> | <span class="t">So, Oh, there's lots of questions. Okay. I will. Okay. Oh, okay. Oh, okay. Oh, okay. I need to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10318" target="_blank">02:51:58.340</a></span> | <span class="t">I need to, I need to, um, answer each of them afterwards. I think I'm already out of time though.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10324" target="_blank">02:52:04.100</a></span> | <span class="t">So, yes. Thanks a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10327" target="_blank">02:52:07.300</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10327" target="_blank">02:52:07.540</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10327" target="_blank">02:52:07.780</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10327" target="_blank">02:52:07.780</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.020</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.020</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.020</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.020</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.260</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.260</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.260</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.260</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.260</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.260</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.260</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.260</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.500</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.500</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.500</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.500</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.500</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.500</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.500</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.740</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.740</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.740</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.980</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.980</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10328" target="_blank">02:52:08.980</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10329" target="_blank">02:52:09.220</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10329" target="_blank">02:52:09.220</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10329" target="_blank">02:52:09.460</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10329" target="_blank">02:52:09.460</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10329" target="_blank">02:52:09.460</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10329" target="_blank">02:52:09.460</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10329" target="_blank">02:52:09.460</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10329" target="_blank">02:52:09.700</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10329" target="_blank">02:52:09.700</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10329" target="_blank">02:52:09.700</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10329" target="_blank">02:52:09.940</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10331" target="_blank">02:52:11.940</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10331" target="_blank">02:52:11.940</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10333" target="_blank">02:52:13.940</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10335" target="_blank">02:52:15.940</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10335" target="_blank">02:52:15.940</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10337" target="_blank">02:52:17.940</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10337" target="_blank">02:52:17.940</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10339" target="_blank">02:52:19.940</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10341" target="_blank">02:52:21.940</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pRM_P6UfdIc&t=10344" target="_blank">02:52:24.000</a></span> | <span class="t">you</span></div></div></body></html>