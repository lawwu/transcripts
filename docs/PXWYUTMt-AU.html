<html><head><title>LoRA: Low-Rank Adaptation of Large Language Models - Explained visually + PyTorch code from scratch</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>LoRA: Low-Rank Adaptation of Large Language Models - Explained visually + PyTorch code from scratch</h2><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU"><img src="https://i.ytimg.com/vi_webp/PXWYUTMt-AU/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=47">0:47</a> How neural networks work<br><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=108">1:48</a> How fine tuning works<br><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=230">3:50</a> LoRA<br><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=538">8:58</a> Math intuition<br><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=625">10:25</a> Math explanation<br><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=845">14:5</a> PyTorch implementation from scratch<br><br><div style="text-align: left;"><a href="./PXWYUTMt-AU.html">Whisper Transcript</a> | <a href="./transcript_PXWYUTMt-AU.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys, welcome back to my channel. Today we will be exploring a very influential people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=5" target="_blank">00:00:05.600</a></span> | <span class="t">called LORA. LORA stands for Low Rank Adaptation of Large Language Models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=11" target="_blank">00:00:11.360</a></span> | <span class="t">and it's a very influential people. It came out I think two years ago from Microsoft</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=17" target="_blank">00:00:17.200</a></span> | <span class="t">and we will see in this video, we will see what is LORA, how does it work and we will also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=22" target="_blank">00:00:22.720</a></span> | <span class="t">implement it in PyTorch from zero without using any external libraries except for Torch of course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=29" target="_blank">00:00:29.280</a></span> | <span class="t">and let's go. So we are in the domain of language models but actually LORA can be applied to any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=37" target="_blank">00:00:37.680</a></span> | <span class="t">kind of model and in fact in my demo that I will also show you later we will apply it to a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=43" target="_blank">00:00:43.920</a></span> | <span class="t">simple classification task and so before we study LORA we need to understand why we need LORA in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=52" target="_blank">00:00:52.560</a></span> | <span class="t">the first place. So let's review some basics about neural networks. So imagine we have some input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=57" target="_blank">00:00:57.680</a></span> | <span class="t">which could be one number or it could be a vector of numbers and then we have some hidden layer in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=62" target="_blank">00:01:02.400</a></span> | <span class="t">a neural network which is usually represented by a matrix but here I show you the graphical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=66" target="_blank">00:01:06.880</a></span> | <span class="t">representation and then we have another hidden layer and finally we have the output right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=72" target="_blank">00:01:12.400</a></span> | <span class="t">Usually when we train a network we also have a target and what we do is we compare the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=79" target="_blank">00:01:19.120</a></span> | <span class="t">and the target to produce a loss and finally we back propagate the loss to each of the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=86" target="_blank">00:01:26.240</a></span> | <span class="t">of all the layers. So in this case for example we may have many weights in this layer we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=93" target="_blank">00:01:33.200</a></span> | <span class="t">have a weights matrix and a bias matrix and each of these weights will be modified by the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=100" target="_blank">00:01:40.880</a></span> | <span class="t">function and also here we will have a weight and a bias matrix here. Now what is fine tuning? Fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=110" target="_blank">00:01:50.320</a></span> | <span class="t">tuning basically means that we have a pre-trained model and we want to fine tune it on some other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=115" target="_blank">00:01:55.840</a></span> | <span class="t">data that the original model may have not seen. For example imagine we work for a company that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=121" target="_blank">00:02:01.680</a></span> | <span class="t">has built its own database so this new database has its own sql language right and we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=128" target="_blank">00:02:08.560</a></span> | <span class="t">downloaded a pre-trained model let's say gpt that was trained on a lot of programming languages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=134" target="_blank">00:02:14.960</a></span> | <span class="t">but we want to fine tune it on our own sql language so that it can answer so that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=143" target="_blank">00:02:23.280</a></span> | <span class="t">model can help our users build queries for our database and what we used to do is we train this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=150" target="_blank">00:02:30.400</a></span> | <span class="t">model with this entire model here on new data and we alter all these weights using the new data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=157" target="_blank">00:02:37.520</a></span> | <span class="t">however this creates some problem. The problem with full fine tuning is that we must train the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=163" target="_blank">00:02:43.760</a></span> | <span class="t">full network which first of all is computationally expensive for the average user because you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=168" target="_blank">00:02:48.960</a></span> | <span class="t">to load all the language in the memory then you need to run back propagation on all the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=174" target="_blank">00:02:54.880</a></span> | <span class="t">plus the storage requirements for the checkpoints are expensive because for every checkpoint for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=180" target="_blank">00:03:00.400</a></span> | <span class="t">every epoch usually we save a checkpoint and we save it on the disk plus if we save also the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=188" target="_blank">00:03:08.320</a></span> | <span class="t">optimizer state let's say we are using adam optimizer adam optimizer for each of the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=193" target="_blank">00:03:13.520</a></span> | <span class="t">keeps also some statistics to better optimize the models so we are saving a lot of data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=200" target="_blank">00:03:20.480</a></span> | <span class="t">and if we suppose we want to use the same base model but fine-tuned on two different data sets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=208" target="_blank">00:03:28.080</a></span> | <span class="t">so we will have basically two different fine-tuned models if we need to switch between them it's very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=215" target="_blank">00:03:35.680</a></span> | <span class="t">expensive because we need to unload the previous model and then load again all the weights of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=221" target="_blank">00:03:41.920</a></span> | <span class="t">other fine-tuned model so we need to replace the all the weights metrics of the model however we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=228" target="_blank">00:03:48.720</a></span> | <span class="t">have a better solution to these problems with LoRa. In LoRa there is this difference so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=234" target="_blank">00:03:54.240</a></span> | <span class="t">start with an input and we have our pre-trained model so we want to fine-tune it right so we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=240" target="_blank">00:04:00.880</a></span> | <span class="t">our pre-trained model with its weights and we freeze them basically we tell PyTorch to never</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=247" target="_blank">00:04:07.600</a></span> | <span class="t">touch these weights just use them as read only never never run back propagation on these weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=254" target="_blank">00:04:14.400</a></span> | <span class="t">then we create two other matrices one for each of the metrics that we want to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=262" target="_blank">00:04:22.400</a></span> | <span class="t">so basically in LoRa we don't have to create matrices the matrices b and a for each of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=272" target="_blank">00:04:32.560</a></span> | <span class="t">layers of the original model we can just do it for some layers and we will see later how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=278" target="_blank">00:04:38.800</a></span> | <span class="t">but in this case suppose we only have one layer and we introduce the matrix b and a so what's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=285" target="_blank">00:04:45.680</a></span> | <span class="t">difference between this matrix b and a and the original matrix w first of all let's look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=291" target="_blank">00:04:51.440</a></span> | <span class="t">dimension the original matrix was d by k suppose d is let's say 1000 and k is equal to 5000 we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=303" target="_blank">00:05:03.680</a></span> | <span class="t">to create two new matrices that when multiplied together they produce the same dimension so d by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=311" target="_blank">00:05:11.360</a></span> | <span class="t">k so in fact we can see it here d by r when it's multiplied by r by k will produce a new matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=318" target="_blank">00:05:18.480</a></span> | <span class="t">that is d by k because the inner dimensions cancel out and we want r to be much smaller than d or k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=328" target="_blank">00:05:28.400</a></span> | <span class="t">we may as well choose r equal to 1 so if we choose r equal to 1 basically we will have a matrix that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=336" target="_blank">00:05:36.240</a></span> | <span class="t">is d by 1 so 1000 by 1 and another matrix that is 1 by 5000 and if we compare the numbers of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=345" target="_blank">00:05:45.040</a></span> | <span class="t">parameters in this matrix in this part in the original matrix w we have the number of parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=350" target="_blank">00:05:50.480</a></span> | <span class="t">let's call it p is equal to d multiplied by k which is equal to 5 million numbers in this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=360" target="_blank">00:06:00.640</a></span> | <span class="t">in this case however we have two matrices so if r is 1 we will have one matrix that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=368" target="_blank">00:06:08.960</a></span> | <span class="t">d by r so 1000 plus 5000 only 6000 numbers in this the combined matrix but with the advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=383" target="_blank">00:06:23.360</a></span> | <span class="t">that when we multiply them together we will still produce a matrix of d by k of course you may think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=389" target="_blank">00:06:29.200</a></span> | <span class="t">that this matrix will not capture the same information as the original matrix w because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=395" target="_blank">00:06:35.280</a></span> | <span class="t">it's much smaller right even if they produce the same dimension they actually have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=399" target="_blank">00:06:39.920</a></span> | <span class="t">the it's a smaller representation of something so it should you lose some information but this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=408" target="_blank">00:06:48.160</a></span> | <span class="t">the whole idea behind LoRa actually we the whole idea behind LoRa is that the matrix w contains a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=414" target="_blank">00:06:54.960</a></span> | <span class="t">lot of weights a lot of numbers that are actually not meaningful for our purpose they are actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=421" target="_blank">00:07:01.600</a></span> | <span class="t">not adding any information to the model they are just a combination of the other weights so they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=427" target="_blank">00:07:07.440</a></span> | <span class="t">are kind of redundant so we don't need the whole matrix w we can create a lower representation of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=433" target="_blank">00:07:13.600</a></span> | <span class="t">this w and fine-tune that one so let's continue with our journey of this model let me delete the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=440" target="_blank">00:07:20.640</a></span> | <span class="t">link okay so we create these two matrix b and a what we do is we combine them because we can sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=449" target="_blank">00:07:29.840</a></span> | <span class="t">them right because they have the same dimension when we multiply b by a it will have the dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=455" target="_blank">00:07:35.520</a></span> | <span class="t">uh d by k so we can sum it with the original w we produce the output and then we have our usual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=462" target="_blank">00:07:42.400</a></span> | <span class="t">target we calculate the loss and we only back propagate the loss to the matrix that we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=467" target="_blank">00:07:47.680</a></span> | <span class="t">train that is the b and a matrix so we never touch the w matrix so our original model which was the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=477" target="_blank">00:07:57.440</a></span> | <span class="t">pre-trained model is frozen and we never touch its weights we only modify the b and a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=483" target="_blank">00:08:03.360</a></span> | <span class="t">so what are the benefits first of all as we saw before we have less parameters to train and store</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=488" target="_blank">00:08:08.960</a></span> | <span class="t">because in the case i showed before we have for example five million parameters when the w matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=496" target="_blank">00:08:16.080</a></span> | <span class="t">in the original one and using r equal to five we only have thirty thousand parameters in total so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=501" target="_blank">00:08:21.680</a></span> | <span class="t">less than one percent of the original less parameters also means that we have less storage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=506" target="_blank">00:08:26.320</a></span> | <span class="t">requirements and faster back propagation because we don't need to evaluate the gradient for most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=510" target="_blank">00:08:30.480</a></span> | <span class="t">of the parameters and we can easily switch between two fine-tuned models because for example imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=516" target="_blank">00:08:36.160</a></span> | <span class="t">we have two different models one for sql and one for generating javascript code we only need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=523" target="_blank">00:08:43.120</a></span> | <span class="t">reload these two matrices if we want to switch between them we don't need to reload the w matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=528" target="_blank">00:08:48.640</a></span> | <span class="t">because it was never touched so it's still the same as the original pre-trained model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=536" target="_blank">00:08:56.720</a></span> | <span class="t">why does this work so the idea is that and it's written in the paper is that the pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=547" target="_blank">00:09:07.360</a></span> | <span class="t">model have they saw the intuition is that they have an interesting dimension that is smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=553" target="_blank">00:09:13.520</a></span> | <span class="t">than their actual dimension and inspired by this they hypothesize that the updates to the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=559" target="_blank">00:09:19.920</a></span> | <span class="t">also have a low intrinsic rank during adaptation and the rank of a matrix basically means it's we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=567" target="_blank">00:09:27.120</a></span> | <span class="t">will see it later with a practical example basically it means imagine we have a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=572" target="_blank">00:09:32.080</a></span> | <span class="t">made of many vectors column vectors and the rank of the matrix is the number of the vectors that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=580" target="_blank">00:09:40.080</a></span> | <span class="t">are linearly independent from each other so you cannot combine linearly any of them to produce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=586" target="_blank">00:09:46.160</a></span> | <span class="t">another one this also indicates kind of how many columns are redundant because they can be obtained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=593" target="_blank">00:09:53.520</a></span> | <span class="t">by linearly combining the other ones and what they what they mean in this paper is that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=601" target="_blank">00:10:01.360</a></span> | <span class="t">w matrix actually is is a rank deficient it means that it does not have full rank so it has a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=607" target="_blank">00:10:07.440</a></span> | <span class="t">dimension maybe 1000 by 1000 but maybe the actual rank is let's say 10 so actually we can use a 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=613" target="_blank">00:10:13.680</a></span> | <span class="t">by 10 matrix to capture most of the information and the idea between this rank reduction is used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=620" target="_blank">00:10:20.080</a></span> | <span class="t">in a lot of scenarios also for example in compression algorithms so let's review some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=626" target="_blank">00:10:26.720</a></span> | <span class="t">mathematics of ranking and metric decomposition and then we check the lora implementation in pytorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=636" target="_blank">00:10:36.160</a></span> | <span class="t">so let's switch here let's go here first so i will show you a very simple example of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=643" target="_blank">00:10:43.200</a></span> | <span class="t">matrix decomposition and how a matrix can be rank deficient and how we can produce a smaller matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=649" target="_blank">00:10:49.520</a></span> | <span class="t">that captures most of the information so let's start by importing the very simple libraries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=655" target="_blank">00:10:55.760</a></span> | <span class="t">torch and numpy then i will create a 10 by 10 matrix here that is artificially rank deficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=663" target="_blank">00:11:03.760</a></span> | <span class="t">so i create it in such a way that it is rank deficient with the rank actual rank of 2 so even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=668" target="_blank">00:11:08.880</a></span> | <span class="t">if this matrix is 10 by 10 we can see that it has 100 numbers we will this the rank of this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=676" target="_blank">00:11:16.400</a></span> | <span class="t">is actually 2 and we can evaluate that using a numpy so we will see that the rank of this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=682" target="_blank">00:11:22.640</a></span> | <span class="t">is actually 2 this means that we can decompose it using an algorithm called svd which means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=690" target="_blank">00:11:30.480</a></span> | <span class="t">singular value decomposition which produces three matrices u s and v that when multiplied together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=698" target="_blank">00:11:38.880</a></span> | <span class="t">they give us w but the dimension of this u s and v can be much smaller based on the rank so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=705" target="_blank">00:11:45.760</a></span> | <span class="t">basically it produces three matrices that if we take only the first r columns of these matrices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=712" target="_blank">00:11:52.800</a></span> | <span class="t">where r indicates the rank of the original matrix they will capture most of the information of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=718" target="_blank">00:11:58.560</a></span> | <span class="t">original matrix and we can visualize that in a simple way what we do is we calculate the b and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=726" target="_blank">00:12:06.000</a></span> | <span class="t">the a matrix just like in the lora case using this decomposition and we can see that we created the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=732" target="_blank">00:12:12.560</a></span> | <span class="t">lower representation of the w matrix which is originally was 10 by 10 but now we created two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=738" target="_blank">00:12:18.400</a></span> | <span class="t">matrices one b and one a that is 10 by 2 and 2 by 10 and what we do is we take some input let's call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=746" target="_blank">00:12:26.960</a></span> | <span class="t">it x and some bias and it's random we compute the output using the w original matrix which was the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=754" target="_blank">00:12:34.800</a></span> | <span class="t">10 by 10 matrix so we multiply it by x we add the bias and we also compute the output using the b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=762" target="_blank">00:12:42.720</a></span> | <span class="t">and a matrix that is the result of the decomposition so we calculate y prime using b multiplied by a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=770" target="_blank">00:12:50.080</a></span> | <span class="t">just like lora multiplied by x plus bias and we see that the output is the same even if b and a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=779" target="_blank">00:12:59.760</a></span> | <span class="t">actually have much less elements so in this case i renamed it i forgot to change the names this is b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=786" target="_blank">00:13:06.720</a></span> | <span class="t">and a okay b and a and what's okay so what i want to show and this is not a proof because i actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=798" target="_blank">00:13:18.960</a></span> | <span class="t">created artificially this w matrix and i made it rank deficient artificially i actually took this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=806" target="_blank">00:13:26.000</a></span> | <span class="t">code from somewhere i don't remember where and so the the idea is that we can have a smaller matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=814" target="_blank">00:13:34.160</a></span> | <span class="t">that can produce the same output for the same given input but by using much less numbers the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=821" target="_blank">00:13:41.120</a></span> | <span class="t">much less parameters so as you can see the b and a elements combined the number of elements in the b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=827" target="_blank">00:13:47.360</a></span> | <span class="t">matrix and a matrix combined are 40 while in the original matrix we had 100 elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=833" target="_blank">00:13:53.600</a></span> | <span class="t">and they still produce the same output for the same given input which means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=838" target="_blank">00:13:58.720</a></span> | <span class="t">b and a captured most of the information the most important information of w now let's go to lora</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=845" target="_blank">00:14:05.360</a></span> | <span class="t">so let's implement lora step by step what we will do is we will do a classification task so imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=852" target="_blank">00:14:12.240</a></span> | <span class="t">we have a very simple neural network for classifying mnist digits and we want to fine tune</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=859" target="_blank">00:14:19.200</a></span> | <span class="t">it on a one specific digit because we see that the performance on one specific digit is not very good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=865" target="_blank">00:14:25.200</a></span> | <span class="t">so we want to fine tune it on only one and we will use lora and show that we when we fine tune with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=871" target="_blank">00:14:31.760</a></span> | <span class="t">lora we are actually modifying a very small number of parameters and we only need to save very small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=877" target="_blank">00:14:37.520</a></span> | <span class="t">number of parameters compared to the pre-trained model let's start so we import the usual libraries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=884" target="_blank">00:14:44.480</a></span> | <span class="t">so torch and matplotlib actually we will not need it and tqdm for visualizing the progress bar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=891" target="_blank">00:14:51.680</a></span> | <span class="t">we make it deterministic so it always returns the same results and we load mnist the data set it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=901" target="_blank">00:15:01.760</a></span> | <span class="t">already integrated into torch vision so it's not a big deal and we create the loader we create a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=908" target="_blank">00:15:08.960</a></span> | <span class="t">very unoptimized neural network for classifying these digits so basically this is a very big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=915" target="_blank">00:15:15.600</a></span> | <span class="t">network for the task we don't need such a big network but i want to make it specific i made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=920" target="_blank">00:15:20.320</a></span> | <span class="t">it on purpose such big because i want to show the the savings in parameters that we get so i call it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=928" target="_blank">00:15:28.000</a></span> | <span class="t">rich boy net so because daddy got money so i don't care about efficiency right and it's a very simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=934" target="_blank">00:15:34.000</a></span> | <span class="t">network made of three linear layers and with the rule activation and the final layer is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=940" target="_blank">00:15:40.080</a></span> | <span class="t">basically the classification of the digit into one of its categories 0 1 or 2 or up to 9 so we create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=948" target="_blank">00:15:48.480</a></span> | <span class="t">this network and we train it on mnist so we run for only one epoch and we train it just simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=957" target="_blank">00:15:57.120</a></span> | <span class="t">training of mnist for classification and then what we do is we keep a copy of the original weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=965" target="_blank">00:16:05.600</a></span> | <span class="t">because we will need it later to prove that the laura didn't modify the original weights so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=972" target="_blank">00:16:12.320</a></span> | <span class="t">weights of the original pretty pre-trained model will not be altered by laura we can also test the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=980" target="_blank">00:16:20.560</a></span> | <span class="t">model the pre-trained model we can test it on and check what is the accuracy so if we test it we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=987" target="_blank">00:16:27.440</a></span> | <span class="t">see the accuracy is very high but we can see that for the digit number nine the accuracy is not as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=993" target="_blank">00:16:33.280</a></span> | <span class="t">good as the other digits so maybe we want to fine-tune especially on the digit nine okay laura</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=999" target="_blank">00:16:39.280</a></span> | <span class="t">actually in the paper was fine-tuned on large language models which i cannot do because i don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1003" target="_blank">00:16:43.840</a></span> | <span class="t">have the computational resources so that's why i'm using mnist and this very simple example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1009" target="_blank">00:16:49.520</a></span> | <span class="t">anyway so we have one digit that we want to fine-tune better right let's visualize before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1015" target="_blank">00:16:55.760</a></span> | <span class="t">we do any fine-tuning how many parameters we have in this network that we created here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1021" target="_blank">00:17:01.360</a></span> | <span class="t">this network here rich boy net so we have in the layer 1 we have this matrix weights and this bias</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1028" target="_blank">00:17:08.880</a></span> | <span class="t">this weights for the layer 2 and this bias this weights matrix for the layer 3 and this bias in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1035" target="_blank">00:17:15.680</a></span> | <span class="t">total we have two million eight hundred seven thousand and ten parameters now let's introduce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1042" target="_blank">00:17:22.400</a></span> | <span class="t">laura so as we saw uh before laura introduces two two matrices called a and b and the um the size of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1054" target="_blank">00:17:34.400</a></span> | <span class="t">these matrices is if the original weights is d by k the b is d by r and a is r by k so i just call it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1069" target="_blank">00:17:49.040</a></span> | <span class="t">features in and features out in the paper it's written that they initialize the b matrix with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1074" target="_blank">00:17:54.960</a></span> | <span class="t">zero and a matrix with random gaussian initialization and this is what i do here as well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1081" target="_blank">00:18:01.520</a></span> | <span class="t">then they also introduce a scale parameter this is from the section 4.1 of the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1085" target="_blank">00:18:05.760</a></span> | <span class="t">that basically allows to change the rank without changing the the scale of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1091" target="_blank">00:18:11.120</a></span> | <span class="t">items and i just use alpha alpha is fixed uh you and because maybe you want to try the same model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1100" target="_blank">00:18:20.560</a></span> | <span class="t">on different ranks so instead of the scale allow us to keep the scale of the numbers the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1108" target="_blank">00:18:28.080</a></span> | <span class="t">if laura is enabled we want the weights matrix so we will basically we will run laura only on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1115" target="_blank">00:18:35.440</a></span> | <span class="t">the weights matrix not on the bias because also in the paper they don't do it for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1119" target="_blank">00:18:39.680</a></span> | <span class="t">bias matrix only on the weights so if laura is enabled the weights matrix will be x so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1128" target="_blank">00:18:48.960</a></span> | <span class="t">original weights plus b multiplied by a just like in the paper multiplied by the scale this is also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1135" target="_blank">00:18:55.600</a></span> | <span class="t">introduced by the paper so basically instead of multiplying the this should be w instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1144" target="_blank">00:19:04.000</a></span> | <span class="t">multiplying x by w just like in the original network we multiply it by w plus b multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1152" target="_blank">00:19:12.080</a></span> | <span class="t">a and this is written in the paper we can see it here let's go down it's written here so instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1161" target="_blank">00:19:21.200</a></span> | <span class="t">of multiplying x only by w we multiply it by this delta w which is how much the weights have moved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1169" target="_blank">00:19:29.920</a></span> | <span class="t">moved because of the fine tuning which is b by a and this is what we are doing here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1176" target="_blank">00:19:36.800</a></span> | <span class="t">and we add this parametrization to our network so to add this parametrization i'm using a special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1184" target="_blank">00:19:44.560</a></span> | <span class="t">function of pytorch called pytorch parametrization so if you want to have more information how it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1190" target="_blank">00:19:50.000</a></span> | <span class="t">works this is the link but i will briefly introduce it parametrization basically means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1196" target="_blank">00:19:56.000</a></span> | <span class="t">allow us to replace the weights matrix of the linear one layer in this case with this function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1204" target="_blank">00:20:04.720</a></span> | <span class="t">so every time the neural network wants to access the weights layer the weights matrix it will not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1210" target="_blank">00:20:10.880</a></span> | <span class="t">access directly the weights matrix it will access this function and when this function is what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1217" target="_blank">00:20:17.040</a></span> | <span class="t">basically our lora parametrization so when it will ask for the weights matrix it will call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1223" target="_blank">00:20:23.200</a></span> | <span class="t">this function giving us the original weights and we just alter the original weights by introducing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1229" target="_blank">00:20:29.360</a></span> | <span class="t">the b and a matrix so when it will multiply the the pytorch will keep doing its work so it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1237" target="_blank">00:20:37.040</a></span> | <span class="t">just multiply the w so the weights by x but actually the weights will be the original weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1242" target="_blank">00:20:42.560</a></span> | <span class="t">plus the b and a that we combined in this way according to the paper and we can easily enable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1249" target="_blank">00:20:49.120</a></span> | <span class="t">or disable lora in each of the layers by modifying the enabled property we can see it here so if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1256" target="_blank">00:20:56.240</a></span> | <span class="t">it's enabled we will use the b and a matrix if it's disabled we will only use the original weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1262" target="_blank">00:21:02.240</a></span> | <span class="t">if we enable basically it means that we enable also the fine-tuned weights if we disable it the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1269" target="_blank">00:21:09.200</a></span> | <span class="t">model should behave just like the pre-trained model and we can also visualize the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1275" target="_blank">00:21:15.280</a></span> | <span class="t">added by lora so how many parameters were added well in the original layer 1 2 and 3 we only had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1282" target="_blank">00:21:22.400</a></span> | <span class="t">the weights and the bias now we also have the lora a matrix and the lora b matrix and i chose a rank</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1289" target="_blank">00:21:29.840</a></span> | <span class="t">of 1 and this i defined it here rank of 1 and so the the the matrix b is 1000 by 1 because the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1301" target="_blank">00:21:41.840</a></span> | <span class="t">weight matrix is 1000 by 784 so 1000 by 1 multiplied by 1 by 784 gives you the same dimension of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1311" target="_blank">00:21:51.680</a></span> | <span class="t">weights matrix and we do it for all the layers so in the original model without lora we had 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1318" target="_blank">00:21:58.240</a></span> | <span class="t">million 807 010 parameters by adding the lora matrices we have 2 million 813 804 parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1328" target="_blank">00:22:08.240</a></span> | <span class="t">but the only 6 000 of them so the one introduced by lora will be actually trained all the others</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1337" target="_blank">00:22:17.120</a></span> | <span class="t">will not be trained and to do it we freeze the non-lora parameters so we can see here i created</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1344" target="_blank">00:22:24.960</a></span> | <span class="t">the code to freeze the parameters so we just set requires grad equal false for them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1350" target="_blank">00:22:30.000</a></span> | <span class="t">and then what we do is we fine-tune the model only on the digit 9 because originally as i show you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1358" target="_blank">00:22:38.960</a></span> | <span class="t">here we want to improve the accuracy of the digit 9 so we don't fine-tune it on any other thing so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1366" target="_blank">00:22:46.880</a></span> | <span class="t">we have a pre-trained model that was trained on all the digits but now we will train it fine-tune</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1372" target="_blank">00:22:52.080</a></span> | <span class="t">it only on the digit 9 hoping that it will improve the accuracy of the digit 9 maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1376" target="_blank">00:22:56.880</a></span> | <span class="t">decreasing the accuracy of the other digits so let's go back here i train it i fine-tune this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1384" target="_blank">00:23:04.080</a></span> | <span class="t">model only on the digits 9 and i do it for only 100 batches because i don't want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1390" target="_blank">00:23:10.880</a></span> | <span class="t">alter the model too much so i do it with the training it is very fast and then basically i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1399" target="_blank">00:23:19.680</a></span> | <span class="t">want to show you that the frozen parameter are still unchanged by the fine-tuning so the frozen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1405" target="_blank">00:23:25.840</a></span> | <span class="t">parameters are this one and they are still the same as the original weights that we saved after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1411" target="_blank">00:23:31.680</a></span> | <span class="t">pre-training our model here so here we save the original parameters we actually clone them so they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1417" target="_blank">00:23:37.680</a></span> | <span class="t">don't get altered and we can see that they are still the same and then what we do is we enable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1427" target="_blank">00:23:47.200</a></span> | <span class="t">lora and we see that the weights so when we access the weights pytorch will actually replace the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1433" target="_blank">00:23:53.440</a></span> | <span class="t">weights by the original weights plus b multiplied by a multiplied by the scale according to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1441" target="_blank">00:24:01.360</a></span> | <span class="t">formula that we have defined here so every time pytorch tries to access the weight matrix it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1447" target="_blank">00:24:07.040</a></span> | <span class="t">actually run this function and this function will return the original weights plus b multiplied by a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1452" target="_blank">00:24:12.480</a></span> | <span class="t">multiplied by the scale and this is what is happening here if we enable lora if we disable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1458" target="_blank">00:24:18.560</a></span> | <span class="t">lora we are disabling the parameterization so it will just return the original weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1463" target="_blank">00:24:23.360</a></span> | <span class="t">and why does this happen because here we said that when lora is disabled just return the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1469" target="_blank">00:24:29.040</a></span> | <span class="t">weights and so what we can do now is that we can enable lora and test the model and we can see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1478" target="_blank">00:24:38.160</a></span> | <span class="t">now the digit 9 is performing much better but of course we lost some information about the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1483" target="_blank">00:24:43.760</a></span> | <span class="t">digits and if we disable lora the model will behave exactly the same as the pre-trained model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1490" target="_blank">00:24:50.160</a></span> | <span class="t">so without any fine tuning and we can see these numbers are the same as the pre-trained model here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1498" target="_blank">00:24:58.320</a></span> | <span class="t">so the number zero had a wrong count for 33 the wrong count for the digit 9 was 107 and it's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1506" target="_blank">00:25:06.080</a></span> | <span class="t">same as this one so when we disable lora the model will behave exactly the same as the pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1511" target="_blank">00:25:11.680</a></span> | <span class="t">model when we enable lora we introduce the matrix b and a that make the model behave like the fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1519" target="_blank">00:25:19.040</a></span> | <span class="t">tuned one and the best the best thing about lora is that we didn't alter the original weights and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1526" target="_blank">00:25:26.880</a></span> | <span class="t">the only weights that we altered are the b and a matrix and their dimension is much smaller compared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1533" target="_blank">00:25:33.040</a></span> | <span class="t">to the w matrix so now if we want to save this fine-tuned model we only need to save this 6794</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1541" target="_blank">00:25:41.360</a></span> | <span class="t">numbers instead of 2 million etc we can fine-tune many versions of this model and by we can easily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1550" target="_blank">00:25:50.160</a></span> | <span class="t">switch between them just by changing the b and the w matrix in this parameterization we don't need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1556" target="_blank">00:25:56.000</a></span> | <span class="t">reload again all the w matrix of the original pre-trained model and this is the power of lora</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1563" target="_blank">00:26:03.280</a></span> | <span class="t">uh i hope my video was clear because i try to make videos that are theoretical but also practical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1569" target="_blank">00:26:09.040</a></span> | <span class="t">please let me know in the comments if there is something that you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1572" target="_blank">00:26:12.080</a></span> | <span class="t">be explained a little better you can use my repository it's pytorch lora on my account</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1580" target="_blank">00:26:20.160</a></span> | <span class="t">and you can play with it and you can try to use different sizes of their ranking or you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1588" target="_blank">00:26:28.480</a></span> | <span class="t">different models it's very easy i suggest you also read the parameterization this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1594" target="_blank">00:26:34.080</a></span> | <span class="t">parameterization function of pytorch because it's very easy to introduce a different kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1600" target="_blank">00:26:40.000</a></span> | <span class="t">of parameterization and also play with the parameterization of a neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1603" target="_blank">00:26:43.680</a></span> | <span class="t">thank you again for listening and i hope you and i hope you enjoyed the video and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU&t=1609" target="_blank">00:26:49.360</a></span> | <span class="t">please come back back to my channel for more videos about machine learning and deep learning</span></div></div></body></html>