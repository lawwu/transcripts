<html><head><title>From model weights to API endpoint with TensorRT LLM: Philip Kiely and Pankaj Gupta</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>From model weights to API endpoint with TensorRT LLM: Philip Kiely and Pankaj Gupta</h2><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U"><img src="https://i.ytimg.com/vi_webp/Lko9lTGD_9U/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./Lko9lTGD_9U.html">Whisper Transcript</a> | <a href="./transcript_Lko9lTGD_9U.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">I am here with Pankaj Gupta. He's the co-founder of Base10. Actually, so today I was checking Slack,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=22" target="_blank">00:00:22.200</a></span> | <span class="t">and in the random Slack channel, one of the people in the company was saying like, "Hey,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=26" target="_blank">00:00:26.560</a></span> | <span class="t">I heard someone call someone Cracked. What does Cracked mean?" Those of you who are Gen Z like me or know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=36" target="_blank">00:00:36.160</a></span> | <span class="t">someone like that is laughing right now because Cracked just means an exceptional engineer, and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=42" target="_blank">00:00:42.320</a></span> | <span class="t">Pankaj is the most Cracked software engineer I've ever had the pleasure of working with. He's from San</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=48" target="_blank">00:00:48.720</a></span> | <span class="t">Francisco. His favorite model is Llama 38B. We're going to be working with a smaller version of that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=53" target="_blank">00:00:53.840</a></span> | <span class="t">today. I'm Phillip. I do develop a relations here at Base10. I've been here for about two and a half</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=59" target="_blank">00:00:59.120</a></span> | <span class="t">years, and I am based in Chicago, but I'm very happy to be here in San Francisco with you all today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=65" target="_blank">00:01:05.440</a></span> | <span class="t">and my favorite model is Playground 2. It's a text-to-image model that's kind of like SDXL,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=72" target="_blank">00:01:12.800</a></span> | <span class="t">but it's trained on mid-journey images. You're going to see a ton of Playground 2 images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=77" target="_blank">00:01:17.840</a></span> | <span class="t">in the slideshow today. What are we doing here today? What is our agenda? We're going to cover</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=85" target="_blank">00:01:25.600</a></span> | <span class="t">what is TensorRT LLM and why use it? Model selection and TensorRT LLM support because it supports a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=92" target="_blank">00:01:32.240</a></span> | <span class="t">stuff, but not everything. We're going to talk about building a TensorRT engine, configuring a TensorRT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=98" target="_blank">00:01:38.240</a></span> | <span class="t">engine automatically, benchmarking it so you can know if you actually did something worthwhile, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=104" target="_blank">00:01:44.720</a></span> | <span class="t">deploying it to production. As much as I love the sound of my own voice and I want to just stand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=110" target="_blank">00:01:50.400</a></span> | <span class="t">here and grasp this microphone for two hours and say things, this is not just going to be Philip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=115" target="_blank">00:01:55.200</a></span> | <span class="t">Reed's office slideshow. We're going to do tons of coding, debugging, live Q&A. The way this presentation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=122" target="_blank">00:02:02.320</a></span> | <span class="t">is kind of broken up is we've got some sections. We've got some live coding. It's going to be a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=128" target="_blank">00:02:08.240</a></span> | <span class="t">interactive workshop. I'm going to be taking questions all the time, so please don't hesitate to let us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=133" target="_blank">00:02:13.920</a></span> | <span class="t">know if anything's confusing. We really want everyone to come away from this with a strong</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=138" target="_blank">00:02:18.560</a></span> | <span class="t">working understanding of how you can actually use this technology in production. So let's get started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=144" target="_blank">00:02:24.960</a></span> | <span class="t">If I may interject for a second and ask Razor fans, how many of you know about TensorRT?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=150" target="_blank">00:02:30.720</a></span> | <span class="t">This is so exciting. I'm so glad that we get to teach you all this today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=156" target="_blank">00:02:36.560</a></span> | <span class="t">How about TensorRT LLM? Okay, a few. So we'll cover the basics. I think I'm pretty sure that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=165" target="_blank">00:02:45.680</a></span> | <span class="t">you'll get a sense of what it is. If you know PyTorch, this shouldn't be too hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=168" target="_blank">00:02:48.960</a></span> | <span class="t">And if you don't know PyTorch, like me, it's still not that hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=172" target="_blank">00:02:52.640</a></span> | <span class="t">So we're going to start with the story of TensorRT LLM. What, who, why, you know, once upon a time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=182" target="_blank">00:03:02.480</a></span> | <span class="t">there was a company called NVIDIA. And they noticed that there are these things called large language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=189" target="_blank">00:03:09.360</a></span> | <span class="t">models that people love running. But what do you want when you want a large language model? You want a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=194" target="_blank">00:03:14.800</a></span> | <span class="t">lot of tokens per second, you want a really short time to first token, and you want high throughput.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=199" target="_blank">00:03:19.840</a></span> | <span class="t">You know, GPUs are expensive. So you want to get the maximum value out of your GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=204" target="_blank">00:03:24.000</a></span> | <span class="t">And TensorRT and TensorRT LLM are technologies that are going to help you do that. So if we get into it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=211" target="_blank">00:03:31.520</a></span> | <span class="t">here, what is TensorRT? Here's one of my Playground 2 images. Very proud of these. If the words on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=217" target="_blank">00:03:37.360</a></span> | <span class="t">slides are dumb, just look at the images, because I worked hard on those. Anyway, so TensorRT is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=224" target="_blank">00:03:44.720</a></span> | <span class="t">SDK for high performance deep learning inference on NVIDIA GPUs. Basically what that means is it's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=231" target="_blank">00:03:51.440</a></span> | <span class="t">a great set of tools for building high performance models. It's a, you know, toolkit that supports both</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=238" target="_blank">00:03:58.640</a></span> | <span class="t">C++ and Python. Our interface today is going to be entirely Python. So if, like me, you skip the class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=245" target="_blank">00:04:05.840</a></span> | <span class="t">that teaches C++, don't worry, you're covered. I know Punkage reads C++ textbooks for fun,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=250" target="_blank">00:04:10.480</a></span> | <span class="t">Python. But, but, but I do not. So we're going to do it in Python today. And so how does this work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=256" target="_blank">00:04:16.400</a></span> | <span class="t">You know, do you want to, do you want to kind of jump in here and talk about this a little bit?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=260" target="_blank">00:04:20.400</a></span> | <span class="t">Because, you know, it's, it's a, it's a really cool process, how you go from a neural network to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=265" target="_blank">00:04:25.760</a></span> | <span class="t">to an engine. Yeah. Yeah, exactly. So ultimately, what are machine learning models? They're the graphs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=272" target="_blank">00:04:32.720</a></span> | <span class="t">they're computation graphs. You flow data through them, you transform them. And ultimately,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=277" target="_blank">00:04:37.920</a></span> | <span class="t">whatever executes a model does that. They execute a graph. Your neural network is a graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=283" target="_blank">00:04:43.760</a></span> | <span class="t">TensorRT works on a graph representation. You take your model and you express that using an API,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=290" target="_blank">00:04:50.320</a></span> | <span class="t">that graph in TensorRT. And then TensorRT is able to take that graph, discover patterns, optimize it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=297" target="_blank">00:04:57.280</a></span> | <span class="t">and then be able to execute it. That's what TensorRT is ultimately. When you write it at PyTorch model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=302" target="_blank">00:05:02.880</a></span> | <span class="t">you're ultimately creating a graph. It's graph followers, right? There is data flowing through this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=307" target="_blank">00:05:07.680</a></span> | <span class="t">graph. And that's what it is. TensorRT additionally provides a plugin mechanism. So it says that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=314" target="_blank">00:05:14.640</a></span> | <span class="t">you know what, I know this graph, I can do a lot of stuff, but I can't do very fancy things like flash</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=320" target="_blank">00:05:20.800</a></span> | <span class="t">attention. It's just too complex. I can't infer automatically from this graph that this is even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=325" target="_blank">00:05:25.680</a></span> | <span class="t">possible. Like I'm not a scientist. So it gives a plugin mechanism using which you can inspect the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=330" target="_blank">00:05:30.800</a></span> | <span class="t">graph and say that, okay, I recognize this thing and I can do it better than you, TensorRT. So I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=336" target="_blank">00:05:36.160</a></span> | <span class="t">going to do it through this plugin. And that is what TensorRT LLM does. It has a bunch of plugins for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=342" target="_blank">00:05:42.320</a></span> | <span class="t">optimizing this graph execution for large language models. So, for example, for attention, for flash</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=348" target="_blank">00:05:48.240</a></span> | <span class="t">attention, it has its own plugin. But it says that, okay, now we are in TensorRT LLM land, take this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=354" target="_blank">00:05:54.480</a></span> | <span class="t">graph and let me execute it using my optimized CUDA kernels. And that's what ultimately TensorRT LLM is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=360" target="_blank">00:06:00.560</a></span> | <span class="t">A very, very optimized way of executing these graphs using GPU resources, not only to get more efficiency,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=370" target="_blank">00:06:10.640</a></span> | <span class="t">better costs for your money, but also better latency, better time to first token, all the things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=376" target="_blank">00:06:16.080</a></span> | <span class="t">that we care about when we are running these models. In addition to that, it provides a few more things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=382" target="_blank">00:06:22.160</a></span> | <span class="t">like when you're executing a model, you're not just executing a request at a time, you're executing a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=387" target="_blank">00:06:27.040</a></span> | <span class="t">bunch of requests at a time. And in-flight batching is a key optimization that is very, very key. Like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=393" target="_blank">00:06:33.200</a></span> | <span class="t">in this day and age, if you're executing a large language model, you have to have in-flight batching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=398" target="_blank">00:06:38.480</a></span> | <span class="t">There's just no way. It's like a 10x or 20x improvement, like, and you have to have that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=403" target="_blank">00:06:43.120</a></span> | <span class="t">And TensorRT LLM provides that. TensorRT wouldn't. TensorRT is a graph executor. It doesn't know about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=408" target="_blank">00:06:48.800</a></span> | <span class="t">But TensorRT LLM has an engine that does that. It also has a language to express graph, just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=413" target="_blank">00:06:53.920</a></span> | <span class="t">PyTorch, and it requires that there is a conversion. But it makes it pretty easy to do that conversion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=418" target="_blank">00:06:58.720</a></span> | <span class="t">And there are tons of examples in the repo. Exactly. So, TensorRT is this great sort of engine builder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=426" target="_blank">00:07:06.080</a></span> | <span class="t">And then TensorRT LLM is a mechanism on top of that that's going to give us a ton of plugins</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=432" target="_blank">00:07:12.400</a></span> | <span class="t">and a ton of optimization specifically for large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=436" target="_blank">00:07:16.640</a></span> | <span class="t">So TensorRT LLM, like Pankaj said, defines the set of plugins for your LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=441" target="_blank">00:07:21.840</a></span> | <span class="t">If you want to, you know, compute attention, do LoRa's, Medusa, other fine tunes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=446" target="_blank">00:07:26.720</a></span> | <span class="t">And it lets you define optimization profiles. So when you're running a large language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=453" target="_blank">00:07:33.600</a></span> | <span class="t">you generally have a batch of requests that you're running at the same time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=457" target="_blank">00:07:37.760</a></span> | <span class="t">You also have an input sequence and an output sequence. And this input sequence could be really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=464" target="_blank">00:07:44.080</a></span> | <span class="t">long. You know, maybe you're summarizing a book. It could be really short. Maybe you're just doing some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=468" target="_blank">00:07:48.880</a></span> | <span class="t">LLM chat. Like, hi, how are you? I'm Fred from the bank. Depending on what your input sequence and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=475" target="_blank">00:07:55.840</a></span> | <span class="t">output sequence lengths are, you're going to want to build a different engine that is going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=481" target="_blank">00:08:01.040</a></span> | <span class="t">optimized for that to process that number of tokens. So, yeah. So TensorRT LLM is this toolbox for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=490" target="_blank">00:08:10.400</a></span> | <span class="t">taking TensorRT and building large language model engines in TensorRT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=495" target="_blank">00:08:15.760</a></span> | <span class="t">I want to say just one thing at this point. Like, why I care about input and output sizes? Like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=501" target="_blank">00:08:21.120</a></span> | <span class="t">how does TensorRT LLM optimize for that? It actually has specific kernels for different sizes of inputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=507" target="_blank">00:08:27.120</a></span> | <span class="t">different sizes of matrices. And it's optimized for that level. And sometimes it becomes a pain when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=511" target="_blank">00:08:31.440</a></span> | <span class="t">I'm compiling TensorRT LLM. It takes hours because it optimizes for so many sizes. But it also means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=517" target="_blank">00:08:37.840</a></span> | <span class="t">giving it that size guidance is useful. It can use better kernels to do things faster. And that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=523" target="_blank">00:08:43.200</a></span> | <span class="t">why. A lot of the models you'll run, you don't have to care about it. But there is always a trade-off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=528" target="_blank">00:08:48.000</a></span> | <span class="t">Here, it does care about that. And you can benefit using that trade-off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=532" target="_blank">00:08:52.400</a></span> | <span class="t">Yeah. And TensorRT LLM is a great tool for a number of reasons. It's got those built-in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=542" target="_blank">00:09:02.480</a></span> | <span class="t">optimized kernels for different sequence lengths. And that level of detail is really across the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=548" target="_blank">00:09:08.800</a></span> | <span class="t">entire tool. And what that means is that with TensorRT LLM, you can get some of the highest performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=554" target="_blank">00:09:14.080</a></span> | <span class="t">possible on GPUs for a wide range of models. And it's really a production-ready system. We are using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=560" target="_blank">00:09:20.720</a></span> | <span class="t">TensorRT LLM today for tons of different client projects. And it's, you know, running in production,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=566" target="_blank">00:09:26.960</a></span> | <span class="t">powering things. TensorRT LLM has support for a ton of different GPUs. Basically anything like Volta or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=573" target="_blank">00:09:33.680</a></span> | <span class="t">newer. The Volta support is kind of experimental. But yeah, like your A10s, your A100s, H100s,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=580" target="_blank">00:09:40.080</a></span> | <span class="t">all that stuff is supported. And yeah, TensorRT LLM, it's developed by NVIDIA. So, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=586" target="_blank">00:09:46.320</a></span> | <span class="t">they know their graphics cards better than anyone. So we just kind of use it to run models quickly on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=592" target="_blank">00:09:52.720</a></span> | <span class="t">that. That said, everything does come with a trade-off. Is anyone from NVIDIA here in the room? It's okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=599" target="_blank">00:09:59.280</a></span> | <span class="t">You don't have to wait. Okay. So I'm going to be nice. No, we really are big fans of this technology,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=605" target="_blank">00:10:05.520</a></span> | <span class="t">but it does come with trade-offs. You know, some of the underlying stuff is not fully open source.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=610" target="_blank">00:10:10.800</a></span> | <span class="t">So sometimes if you're diving super deep, you need to go get more information without just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=615" target="_blank">00:10:15.760</a></span> | <span class="t">looking at the source code. And it does sometimes have a pretty steep learning curve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=620" target="_blank">00:10:20.400</a></span> | <span class="t">when you're building these optimizations. So that's what we're here to help flatten out for you guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=625" target="_blank">00:10:25.520</a></span> | <span class="t">today. Hopefully we're still friends. What makes it hard? So there's a couple of things that make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=632" target="_blank">00:10:32.240</a></span> | <span class="t">building with TensorRT LLM really hard. And when we enumerate the things that make it hard, that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=636" target="_blank">00:10:36.960</a></span> | <span class="t">how we know what we need to do to make it easy. So the number one thing in my mind that makes it hard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=641" target="_blank">00:10:41.840</a></span> | <span class="t">to build a general model or to optimize a model with TRT LLM is you need a ton of specific information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=648" target="_blank">00:10:48.480</a></span> | <span class="t">about the production environment you're going to run it. All right. So I do a lot of sales enablement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=654" target="_blank">00:10:54.720</a></span> | <span class="t">trainings and I love a good metaphor. So I'm going to walk you guys through a metaphor here. Apologies if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=661" target="_blank">00:11:01.760</a></span> | <span class="t">metaphors aren't your thing. So imagine you go into a clothing store and it only sells one size of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=667" target="_blank">00:11:07.600</a></span> | <span class="t">shirt. You know, it's just like a medium. You know, for some people that's going to fit great. For some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=672" target="_blank">00:11:12.720</a></span> | <span class="t">people it's going to be too small. For some people it's going to be too big. And on the other hand, you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=678" target="_blank">00:11:18.320</a></span> | <span class="t">go to like a tailor, I don't know, in like Italy or something. And you go there and they've got, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=683" target="_blank">00:11:23.840</a></span> | <span class="t">some super fancy guy with a, you know, cool mustache and stuff. And he, you know, he measures you like every single detail</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=691" target="_blank">00:11:31.280</a></span> | <span class="t">and then builds a suit exactly for you. That's perfect for your body measurements, like a made to measure suit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=697" target="_blank">00:11:37.120</a></span> | <span class="t">So optimizing a model is kind of like making that suit. You know, everything has to be measured for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=704" target="_blank">00:11:44.880</a></span> | <span class="t">exactly the use case that you're building for. And so when people come in and expect that they can just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=710" target="_blank">00:11:50.240</a></span> | <span class="t">walk in and grab off the shelf a model that's going to work perfectly for their use case, that's like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=715" target="_blank">00:11:55.040</a></span> | <span class="t">expecting you're going to go into a store and buy a piece of clothing that fits you just as well as that custom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=720" target="_blank">00:12:00.080</a></span> | <span class="t">made, made to measure suit from the tailor. So in, you know, to relate that more concretely to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=726" target="_blank">00:12:06.160</a></span> | <span class="t">TensorRT LLM, you need information. You need, like we talked about, you need to understand the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=731" target="_blank">00:12:11.680</a></span> | <span class="t">lengths that you're going to be working at, the batch sizes that you want to run at. You also need to know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=736" target="_blank">00:12:16.400</a></span> | <span class="t">ahead of time what GPUs you're going to be using in production. These engines that we're building are not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=741" target="_blank">00:12:21.680</a></span> | <span class="t">portable. They are built for a specific GPU. You build them. So if you build it on an A10, you run it on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=747" target="_blank">00:12:27.840</a></span> | <span class="t">an A10. If you build it on an H100, you run it on an H100. You want to switch to H100 MIG? Okay, you build it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=755" target="_blank">00:12:35.200</a></span> | <span class="t">again for H100 MIG. So you need to know all of this information about your production environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=760" target="_blank">00:12:40.720</a></span> | <span class="t">And then also, as we'll talk about kind of toward the end, there are some infrastructure challenges as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=766" target="_blank">00:12:46.000</a></span> | <span class="t">well. These engines that we're going to build are quite large. So if you're, for example, doing auto</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=770" target="_blank">00:12:50.480</a></span> | <span class="t">scaling, you have to deal with slow cold starts, you know, work, work around the size of the engines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=775" target="_blank">00:12:55.760</a></span> | <span class="t">Otherwise, your cold starts are going to be slow. And overall, also just model optimization means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=781" target="_blank">00:13:01.760</a></span> | <span class="t">we're living on the cutting edge of new research. You know, I'm, when I'm, when I'm writing blog posts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=787" target="_blank">00:13:07.200</a></span> | <span class="t">about this stuff, I'm oftentimes looking at papers that have been published in the last six months.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=791" target="_blank">00:13:11.200</a></span> | <span class="t">So, you know, just combining all these new approaches and tools, there can be some rough edges, but the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=798" target="_blank">00:13:18.080</a></span> | <span class="t">performance gains are worth it. So, yeah. Oh, please go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=802" target="_blank">00:13:22.560</a></span> | <span class="t">I want to add one thing is that there are modes in TensorRDLM where you can build for a certain, on a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=808" target="_blank">00:13:28.320</a></span> | <span class="t">certain GPU, and it will run on other GPUs. But then it, it's not optimized for the GPUs. So why would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=813" target="_blank">00:13:33.760</a></span> | <span class="t">you do that? We never do that. We always build it for the GPU. But there is that option.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=817" target="_blank">00:13:37.360</a></span> | <span class="t">Exactly. That would be like, if I went to that fancy tailor shop, got a made-to-measure suit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=821" target="_blank">00:13:41.840</a></span> | <span class="t">and then was like, "Hey, Punkic, happy birthday. I got you a new suit." That's, that's what it would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=826" target="_blank">00:13:46.240</a></span> | <span class="t">like. So, you know, what, what makes TensorRDLM worth it? Well, it's, it's the performance. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=831" target="_blank">00:13:51.600</a></span> | <span class="t">these numbers are from a Mistral 7B that we ran on artificial analysis, which is a third-party</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=837" target="_blank">00:13:57.440</a></span> | <span class="t">benchmarking site. And we were able to get, with TensorRDLM and a few other optimizations as well on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=843" target="_blank">00:14:03.600</a></span> | <span class="t">top of it, 216 tokens per second, perceived tokens per second, and 180 milliseconds time to first token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=851" target="_blank">00:14:11.200</a></span> | <span class="t">So, unless any of you are maybe like some super high quality athletes, like a UFC fighter or something,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=857" target="_blank">00:14:17.440</a></span> | <span class="t">your reaction time is probably about 200 milliseconds. So, you know, 180 millisecond time to first token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=863" target="_blank">00:14:23.520</a></span> | <span class="t">counting network latency, by the way, counting the round trip time to the server is great because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=868" target="_blank">00:14:28.880</a></span> | <span class="t">that, to a user, feels instant once you're under 200 milliseconds. And actually, most of it is network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=873" target="_blank">00:14:33.680</a></span> | <span class="t">latency. The time on the GPU is less than 50 milliseconds. Less than 50 milliseconds. So, we've got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=880" target="_blank">00:14:40.080</a></span> | <span class="t">another one of these green slides here. I like to talk really fast. So, these slides I put in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=885" target="_blank">00:14:45.360</a></span> | <span class="t">presentation to give us all a chance to take a breath and ask any questions. So, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=890" target="_blank">00:14:50.160</a></span> | <span class="t">we're going to cover a lot more technical detail moving forward, but if there's anything kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=894" target="_blank">00:14:54.640</a></span> | <span class="t">foundational that you're struggling with, like what's TensorRD, what's TensorRDLM, anything I can explain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=900" target="_blank">00:15:00.160</a></span> | <span class="t">more clearly, I would love to hear about it. Going once, going twice. It's okay. We're all friends here. You can raise your hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=908" target="_blank">00:15:08.640</a></span> | <span class="t">All right. Well, it sounds like I'm amazing at my job. I explained everything perfectly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=912" target="_blank">00:15:12.880</a></span> | <span class="t">and we get to move on to the next section.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=914" target="_blank">00:15:14.560</a></span> | <span class="t">So, what models can you use with TensorRDLM? Lots of them. There's a list of like 50 foundation models in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=925" target="_blank">00:15:25.440</a></span> | <span class="t">the TensorRDLM documentation that you can use. And you can also use, you know, fine tunes of those models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=931" target="_blank">00:15:31.840</a></span> | <span class="t">anything you've built on top of them. It supports open source large vision models, so if you're,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=937" target="_blank">00:15:37.920</a></span> | <span class="t">you know, building your own GPT 4.0, you can do that with TensorRDLM. And it also supports models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=944" target="_blank">00:15:44.400</a></span> | <span class="t">like Whisper. And then TensorRD itself, you can do anything with TensorRD. So, any model, custom,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=950" target="_blank">00:15:50.400</a></span> | <span class="t">open source, fine tuned, you can run it with TensorRD. But TensorRDLM is what we're focusing on today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=956" target="_blank">00:15:56.720</a></span> | <span class="t">because it's a much more convenient way of building these models. And, you know, on this list of models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=963" target="_blank">00:16:03.760</a></span> | <span class="t">that it supports, there's one that maybe stands out. Does anyone know like what model kind of doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=968" target="_blank">00:16:08.800</a></span> | <span class="t">belong in the, in this list of supported models? Like what, what, what up here isn't an LLM?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=974" target="_blank">00:16:14.640</a></span> | <span class="t">Whisper, exactly. Why, why is, why is Whisper on here? Well, TensorRDLM, it's, it's called dash LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=984" target="_blank">00:16:24.080</a></span> | <span class="t">Um, but it really is a little more flexible than that because you can run, you know, a lot of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=989" target="_blank">00:16:29.920</a></span> | <span class="t">auto-aggressive transformers models with it like Whisper. So, if anyone doesn't know what Whisper is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=995" target="_blank">00:16:35.280</a></span> | <span class="t">it is a audio transcription model. You give it, uh, you know, MP3 file with someone talking,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1001" target="_blank">00:16:41.200</a></span> | <span class="t">it gives you back a transcript of what they said. It's one of our, it's one of our favorite models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1005" target="_blank">00:16:45.840</a></span> | <span class="t">to work with. We've spent a ton of time optimizing Whisper, building pipelines for it, and all that sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1010" target="_blank">00:16:50.480</a></span> | <span class="t">of stuff. And what's really cool about Whisper is structurally, like it's basically an LLM. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1017" target="_blank">00:16:57.280</a></span> | <span class="t">know, that, that, that's a massively reductive statement for me to make, but it's a auto-aggressive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1021" target="_blank">00:17:01.760</a></span> | <span class="t">transformers model. It has the same bottlenecks in terms of influence performance. So even though</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1026" target="_blank">00:17:06.960</a></span> | <span class="t">this is not a little, not an LLM, it's an audio transcription model, we're actually still able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1032" target="_blank">00:17:12.560</a></span> | <span class="t">optimize it with TensorRT LLM because, uh, because of its architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1037" target="_blank">00:17:17.520</a></span> | <span class="t">Let me say one more thing. Of course. So the, the whole, uh, I think the recent ML revolution started with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1045" target="_blank">00:17:25.440</a></span> | <span class="t">transformers paper attention is all you need and that describes an encoder decoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1049" target="_blank">00:17:29.920</a></span> | <span class="t">architecture. And in a way, Whisper is machine translation. That paper was about machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1055" target="_blank">00:17:35.280</a></span> | <span class="t">translation. You're translating audio text, uh, audio into text, right? And it's basically that it's an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1062" target="_blank">00:17:42.080</a></span> | <span class="t">encoder decoder model, exactly like the transform architecture and transfer, tensor ID LLM is about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1067" target="_blank">00:17:47.360</a></span> | <span class="t">that. It's about that transform architecture. So it actually matches pretty well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1071" target="_blank">00:17:51.120</a></span> | <span class="t">Exactly. So, um, moving on, um, I want to run through a few things, uh, just, just some, some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1078" target="_blank">00:17:58.720</a></span> | <span class="t">things in terms of what TensorRT LLM supports. So I assume it's going to support Blackwell when that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1084" target="_blank">00:18:04.080</a></span> | <span class="t">comes out, like 99.999% certain. Um, but anyway, in terms of what we have today, we've got Hopper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1090" target="_blank">00:18:10.800</a></span> | <span class="t">so the H100s, the L4s, RTX 4090s. If anyone has a super sweet gaming desktop at home, number one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1097" target="_blank">00:18:17.760</a></span> | <span class="t">I'm jealous. Number two, you can run TensorRT LLM on that. Um, Ampere GPUs, Turing GPUs, uh, V100s are,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1105" target="_blank">00:18:25.840</a></span> | <span class="t">you know, somewhat supported. Um, and what's cool about, what's cool about TensorRT and hardware</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1112" target="_blank">00:18:32.640</a></span> | <span class="t">support is that, like, it works better with newer GPUs. When you move from an A100 to an H100 and you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1120" target="_blank">00:18:40.400</a></span> | <span class="t">using TensorRT or TensorRT LLM, you're not just getting the sort of, like, linear increase in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1126" target="_blank">00:18:46.320</a></span> | <span class="t">performance that you'd expect from, you know, oh, I've got more flops now. I've got more gigabytes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1132" target="_blank">00:18:52.320</a></span> | <span class="t">per second of GPU bandwidth. You're actually getting more of a performance gain going from one GPU to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1139" target="_blank">00:18:59.200</a></span> | <span class="t">next, uh, than you would expect off raw stats alone. And that's because, um, you know, H100s, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1145" target="_blank">00:19:05.680</a></span> | <span class="t">have all these great architectural features and TensorRT, because it actually optimizes the model by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1152" target="_blank">00:19:12.240</a></span> | <span class="t">compiling a Takuda instructions, is able to take advantage of those architectural features,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1157" target="_blank">00:19:17.520</a></span> | <span class="t">not just kind of run the model, um, you know, raw. And so for that, you know, that's why we do a lot with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1164" target="_blank">00:19:24.320</a></span> | <span class="t">H100 MIGs. This, this, this bullet point here is a whole different 45-minute talk that I tried to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1169" target="_blank">00:19:29.520</a></span> | <span class="t">pitch to, uh, do here. But basically, you know, H100 MIGs are especially good for TensorRT LLM. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1176" target="_blank">00:19:36.480</a></span> | <span class="t">if you're trying to run smaller models, like a 7B, you know, Llama 8B, for example, uh, because you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1182" target="_blank">00:19:42.560</a></span> | <span class="t">need the massive amount of VRAM, but you get the, um, increased performance from the architectural features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1188" target="_blank">00:19:48.320</a></span> | <span class="t">Um, and you know, just my own speculation down here, that I'm sure whatever the next generation is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1194" target="_blank">00:19:54.480</a></span> | <span class="t">is going to have even more architectural features for TensorRT to take advantage of. And so, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1200" target="_blank">00:20:00.080</a></span> | <span class="t">adopting it now is a good move, uh, you know, looking to the future. Here we've got a graph showing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1205" target="_blank">00:20:05.600</a></span> | <span class="t">you know, with SDXL. Now this is TensorRT, not TensorRT LLM, but the underlying technology is the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1211" target="_blank">00:20:11.520</a></span> | <span class="t">Um, you know, when you're working on an A10G, we were looking at, you know, maybe like a 25 to 30%</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1217" target="_blank">00:20:17.600</a></span> | <span class="t">increase in throughput for SDXL. And, uh, with an H100, it's a 70%. And that's not, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1224" target="_blank">00:20:24.000</a></span> | <span class="t">just because the H100 is bigger. It's 70% more on an H100 with TensorRT L, TensorRT versus an H100 without.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1232" target="_blank">00:20:32.640</a></span> | <span class="t">So, yeah, great, uh, yeah, please go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1235" target="_blank">00:20:35.760</a></span> | <span class="t">One thing I want to add here is that, uh, H100 supports FP8 and A100 does not. FP8 is a game changer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1242" target="_blank">00:20:42.880</a></span> | <span class="t">I think it's very easy to understate that fact. FP8 is really, really good. Post-training quantization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1248" target="_blank">00:20:48.480</a></span> | <span class="t">You don't need to train anything. Post-training quantization, it takes like five minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1252" target="_blank">00:20:52.720</a></span> | <span class="t">And the results are so close. We've done perplexity tests on it. Whenever you quantize,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1257" target="_blank">00:20:57.120</a></span> | <span class="t">you have to check the accuracy. Uh, and we've done that. It's hard to tell.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1262" target="_blank">00:21:02.080</a></span> | <span class="t">And FP8 is about 40% better in most scenarios. So if you're using, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1266" target="_blank">00:21:06.160</a></span> | <span class="t">make H100, if you can, then it's, it can be way better if you use FP8.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1271" target="_blank">00:21:11.280</a></span> | <span class="t">And FP8 is also supported, um, by Lovelace. So that's going to be your L4 GPUs, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1278" target="_blank">00:21:18.320</a></span> | <span class="t">which are also a great option for, for FP8. So yeah, a bunch of different precisions are supported.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1284" target="_blank">00:21:24.320</a></span> | <span class="t">Again, FP8 is kind of the highlight. FP4, uh, could be coming. And, um, you know, traditionally though, we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1291" target="_blank">00:21:31.520</a></span> | <span class="t">going to run in FP16, um, which is sort of like a, a, uh, full precision. Um, oh, sorry, half precision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1299" target="_blank">00:21:39.200</a></span> | <span class="t">FP32 is technically full precision. What?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1302" target="_blank">00:21:42.720</a></span> | <span class="t">But nobody does FP32. Yeah, yeah. So, so for, for inference generally, you start at FP16.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1307" target="_blank">00:21:47.840</a></span> | <span class="t">By the way, FP16 means a 16-bit floating point number. Um, and from there, you know, you can quantize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1314" target="_blank">00:21:54.720</a></span> | <span class="t">to INT8, FP8 if, you know, you want your model to run faster, if you want to run on fewer or smaller GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1321" target="_blank">00:22:01.280</a></span> | <span class="t">Um, we'll, we'll, we'll cover quantization in a bit more detail later on in the actual workshop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1327" target="_blank">00:22:07.040</a></span> | <span class="t">I don't want to spend too long on the slides here. I know you guys want to get your laptops out and start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1330" target="_blank">00:22:10.960</a></span> | <span class="t">coding. Um, so the other thing just to talk about is, like I said, TensorRT LLM, it's a, you know, it's a set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1340" target="_blank">00:22:20.000</a></span> | <span class="t">optimizations that you can, you know, build into your TensorRT, into your TensorRT, into your TensorRT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1346" target="_blank">00:22:26.080</a></span> | <span class="t">engines. Um, so some of the features that are supported, again, each one of these could be its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1350" target="_blank">00:22:30.320</a></span> | <span class="t">own talk here, uh, but we've got quantization, LoRa swapping, speculative decoding, and Medusa heads,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1356" target="_blank">00:22:36.560</a></span> | <span class="t">which is where you basically, like, fine-tune additional heads onto your model. And then at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1362" target="_blank">00:22:42.160</a></span> | <span class="t">each forward pass, you're generating, like, four tokens instead of one token. Great for when you're,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1366" target="_blank">00:22:46.880</a></span> | <span class="t">you know, when you have memory bandwidth restrictions. Um, yeah, in-flight batching,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1370" target="_blank">00:22:50.960</a></span> | <span class="t">like you mentioned, page attention. There's just a ton of different optimizations supported</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1375" target="_blank">00:22:55.600</a></span> | <span class="t">by TensorRT LLM for you to dive into once you have the basic engine built.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1379" target="_blank">00:22:59.360</a></span> | <span class="t">So, um, we're about to switch into more of, like, a live coding workshop segment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1386" target="_blank">00:23:06.800</a></span> | <span class="t">So, if there's any of this sort of groundwork information that didn't make sense or any more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1390" target="_blank">00:23:10.880</a></span> | <span class="t">details that you want on anything, let us know. We'll cover it now. Um, otherwise, it's, it's about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1396" target="_blank">00:23:16.160</a></span> | <span class="t">to be laptop time. Looks like everyone wants laptop time. So, uh, yes, please go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1403" target="_blank">00:23:23.280</a></span> | <span class="t">Can you do, like, some high-level comparison, like, to the LLM?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1406" target="_blank">00:23:26.960</a></span> | <span class="t">Yeah, do you want, do you want to, do you want to handle that one? Like, a high-level comparison to the LLM?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1411" target="_blank">00:23:31.600</a></span> | <span class="t">Uh, I can do a very high-level comparison. Uh, first of all, uh, I respect both tools. VLLM is great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1417" target="_blank">00:23:37.920</a></span> | <span class="t">TensorRT LLM is also great. We found in our comparisons that, uh, for most of the scenarios</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1423" target="_blank">00:23:43.840</a></span> | <span class="t">we compared, we found TensorRT LLM to be better. Um, there, there are a few things there. One thing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1429" target="_blank">00:23:49.280</a></span> | <span class="t">that whenever a new GPU lands or a new technique lands, that tends to work better on TensorRT LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1435" target="_blank">00:23:55.440</a></span> | <span class="t">VLLM, it takes a bit of time for it to catch up for the kernels to be optimized. Uh, TensorRT LLM is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1440" target="_blank">00:24:00.960</a></span> | <span class="t">generally ahead of that. For example, when H100 landed, TensorRT LLM was, uh, was very, very fast out of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1447" target="_blank">00:24:07.280</a></span> | <span class="t">the box because they've been working for it on it for a long time. Um, second thing is that TensorRT LLM is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1452" target="_blank">00:24:12.880</a></span> | <span class="t">optimized from bottom to the top. These, uh, CUDA kernels at the very bottom are very, very well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1458" target="_blank">00:24:18.400</a></span> | <span class="t">optimized. On top of that, there is the in-flight batching engine, all written in C++. And I've,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1463" target="_blank">00:24:23.440</a></span> | <span class="t">I've seen that code. It's very, very optimized C++ code with your STD moves and whatnot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1468" target="_blank">00:24:28.640</a></span> | <span class="t">And on top of that is Triton, which is a web server, again, written in C++. So the whole thing is very,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1475" target="_blank">00:24:35.680</a></span> | <span class="t">very optimized. Whereas, uh, in some other frameworks, uh, they also try to optimize, uh, in the sense like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1481" target="_blank">00:24:41.440</a></span> | <span class="t">you know, Java versus C++. Java is like, you know, we optimize everything that matters, but there are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1486" target="_blank">00:24:46.320</a></span> | <span class="t">always cases where it might not be as good. But TensorRT LLM is that let's optimize every single thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1492" target="_blank">00:24:52.800</a></span> | <span class="t">So it generally tends to perform better in our experience. That said, VLM is a great,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1497" target="_blank">00:24:57.280</a></span> | <span class="t">great product. We use it a lot as well. For example, LoRa swapping, it became available in VLM first. So we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1503" target="_blank">00:25:03.360</a></span> | <span class="t">use that, uh, there for a while. What we found is that when something lands in TensorRT LLM and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1508" target="_blank">00:25:08.880</a></span> | <span class="t">usually after a delay, it works like, like bonkers. It just works like so well that, uh, performance is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1516" target="_blank">00:25:16.640</a></span> | <span class="t">just amazing. So when something is working very stable in TensorRT LLM, we tend to use that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1521" target="_blank">00:25:21.680</a></span> | <span class="t">But, uh, VLM and other frameworks, they provide a lot of flexibility, which is great. Uh, we love all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1527" target="_blank">00:25:27.680</a></span> | <span class="t">all the words, yeah. Yeah. Yeah. I think, I think we should, uh, definitely question that. That is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1543" target="_blank">00:25:43.520</a></span> | <span class="t">clear trade off. If you're working with two GPUs, for example, two 8NGs, it's not worth it probably. But if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1550" target="_blank">00:25:50.320</a></span> | <span class="t">you're spending hundreds of thousands of dollars a year, uh, it's, it's your call. But if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1555" target="_blank">00:25:55.360</a></span> | <span class="t">spending, uh, many hundreds of thousands of dollars a year, it can be material. Like your profit margin</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1563" target="_blank">00:26:03.120</a></span> | <span class="t">might be 20%. This 20% or 50% improvement might make the, all the difference that you need. So it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1569" target="_blank">00:26:09.360</a></span> | <span class="t">depends upon the use case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1580" target="_blank">00:26:20.720</a></span> | <span class="t">It could be, yeah. I think if you're working with one or two GPUs, 8NGs or T4s, uh, I mean, I'm not an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1595" target="_blank">00:26:35.760</a></span> | <span class="t">expert in that, but, uh, it probably doesn't matter which framework you use, whichever works best for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1600" target="_blank">00:26:40.720</a></span> | <span class="t">you. But if you end up using A100s or H100s, you should definitely look into TensorFlow. And, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1608" target="_blank">00:26:48.240</a></span> | <span class="t">regarding the learning curve, stick around a little bit. We're going to, uh, flatten it out a lot for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1612" target="_blank">00:26:52.800</a></span> | <span class="t">you because we've built some great tooling on top of TensorFlow RTLM that's going to make it just as easy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1617" target="_blank">00:26:57.040</a></span> | <span class="t">to use. A little, little, little marketing spin right there. Um, so yeah. So we're going to, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1624" target="_blank">00:27:04.800</a></span> | <span class="t">be doing a engine building, live coding exercise. Um, and, and Punk is just going to lead us through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1630" target="_blank">00:27:10.640</a></span> | <span class="t">that. I'm just going to kind of roam around. So if, if people have, you know, questions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1634" target="_blank">00:27:14.960</a></span> | <span class="t">need help kind of on a one-on-one basis, I'll be able to help, help out with that doing this portion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1639" target="_blank">00:27:19.440</a></span> | <span class="t">of the workshop. Great. So, uh, yeah, let, let's just like go through a couple. There's, there's, there's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1649" target="_blank">00:27:29.600</a></span> | <span class="t">there's a little bit of a little bit of setup material, right? Or yes. Yeah. Um, you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1655" target="_blank">00:27:35.440</a></span> | <span class="t">to run through that? Okay, sure. I'll run through the, I'll run through the setup material. Um, and then,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1660" target="_blank">00:27:40.160</a></span> | <span class="t">uh, yeah. So, um, anyway, what we're going to do, um, to, to, to be clear is we're going to build an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1668" target="_blank">00:27:48.880</a></span> | <span class="t">engine for a model called tiny llama 1.1 B. I want to be really clear about this. Tensor OT LM is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1674" target="_blank">00:27:54.560</a></span> | <span class="t">production ready technology that works great with big models on big GPUs. Uh, that takes time to run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1682" target="_blank">00:28:02.480</a></span> | <span class="t">The dev loop can be a little bit slow and we only have a two hour workshop here. And, uh, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1687" target="_blank">00:28:07.360</a></span> | <span class="t">I don't want us all just to be sitting there watching a model build. It's basically as fun as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1690" target="_blank">00:28:10.960</a></span> | <span class="t">watching paint dry or watching grass grow. Um, so we're going to be using this super tiny 1.1 billion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1696" target="_blank">00:28:16.880</a></span> | <span class="t">parameter model. We're going to be using 40 90s and 8 10 Gs, um, just to kind of keep the dev loop fast,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1703" target="_blank">00:28:23.200</a></span> | <span class="t">but this stuff does scale. So, um, at this point, we're going to walk you through the manual process of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1708" target="_blank">00:28:28.320</a></span> | <span class="t">doing, doing it all from scratch. You're going to procure and configure a GPU. You're going to install</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1713" target="_blank">00:28:33.440</a></span> | <span class="t">dependencies for Tensor OT LM, configure the engine, run the engine build job, and, uh, test the results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1719" target="_blank">00:28:39.760</a></span> | <span class="t">And we, we should be able to get through this in, in about half an hour or maybe a little less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1724" target="_blank">00:28:44.000</a></span> | <span class="t">because these, uh, these models are quite small. Um, and there's a few important settings that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1730" target="_blank">00:28:50.080</a></span> | <span class="t">going to look at when building the engine. We're going to look at the quantization, again, the post</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1733" target="_blank">00:28:53.600</a></span> | <span class="t">training quantization, like we talked about. We're going to be on 8 10s or sorry, no, first we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1738" target="_blank">00:28:58.400</a></span> | <span class="t">to be on 40 90s. So we will actually have access to FP8 so that you can test that out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1743" target="_blank">00:29:03.280</a></span> | <span class="t">Uh, we're going to look at secret shapes and batch sizes, how to set that. And we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1748" target="_blank">00:29:08.160</a></span> | <span class="t">to look at tensor parallelism. You want to give them a quick preview on tensor parallelism?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1752" target="_blank">00:29:12.160</a></span> | <span class="t">Oh, yeah, tensor parallelism is, uh, is very important in certain scenarios. I wish it were more useful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1759" target="_blank">00:29:19.600</a></span> | <span class="t">but it is critical in many scenarios. So what is tensor parallelism? Ultimately, machine learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1764" target="_blank">00:29:24.960</a></span> | <span class="t">running these GPUs is about matrix multiplications. We take this model architecture, whatever it is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1770" target="_blank">00:29:30.720</a></span> | <span class="t">it ultimately boils down to matrices that we multiply and a lot of the wrangling is around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1775" target="_blank">00:29:35.120</a></span> | <span class="t">that. How do we shove these all batches into matrices? So ultimately it is matrix multiplication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1780" target="_blank">00:29:40.160</a></span> | <span class="t">right? What you can do is you can split these matrices and you can multiply them separately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1785" target="_blank">00:29:45.040</a></span> | <span class="t">on different GPUs and then combine the results. And that's what tensor parallelism is. It's one of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1790" target="_blank">00:29:50.880</a></span> | <span class="t">tensor of parallelism techniques. Uh, there are many techniques. Uh, it's one of the most commonly used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1796" target="_blank">00:29:56.880</a></span> | <span class="t">ones because you need that. Uh, why do you need tensor parallelism versus other parallelisms like pipeline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1802" target="_blank">00:30:02.560</a></span> | <span class="t">parallelism? Um, is that it saves on latency. You can do things in parallel. You can use two GPUs at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1810" target="_blank">00:30:10.240</a></span> | <span class="t">same time for doing something, even though there is some overhead of crosstalk between them. With pipeline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1815" target="_blank">00:30:15.680</a></span> | <span class="t">parallelism, you take the model architecture and you can divide these layers into separate things. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1820" target="_blank">00:30:20.560</a></span> | <span class="t">your thing goes through one GPU, like half the layers and then half the layers on the second GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1826" target="_blank">00:30:26.000</a></span> | <span class="t">But you're not saving on latency. It still has to go through each layer and it's going sequentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1832" target="_blank">00:30:32.160</a></span> | <span class="t">And that's why pipeline parallelism is not very popular for inference. It is still popular for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1836" target="_blank">00:30:36.960</a></span> | <span class="t">training. There are scenarios. Uh, and there's a lot of theory about that. But for, for inference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1842" target="_blank">00:30:42.320</a></span> | <span class="t">I don't think I've ever seen it used and nobody pays much attention to optimizing it because of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1846" target="_blank">00:30:46.560</a></span> | <span class="t">thing that tensor parallelism is just better. There's also expert level parallelism. If your model has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1852" target="_blank">00:30:52.000</a></span> | <span class="t">mixture of experts, then you can parallelize those experts. And that tends to be very advanced and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1856" target="_blank">00:30:56.960</a></span> | <span class="t">Lama doesn't have a mixture of experts. So it's a esoteric thing that we haven't covered here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1862" target="_blank">00:31:02.320</a></span> | <span class="t">TensorFlow is pretty helpful and useful. Uh, one downside is that your throughput is not as great. If you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1868" target="_blank">00:31:08.240</a></span> | <span class="t">fit something in a bigger GPU, that's generally better, but there are bigger models like Lama 7 DB,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1874" target="_blank">00:31:14.160</a></span> | <span class="t">they just can't fit on one GPU. So you have to use tensor parallelism. Awesome. So for everyone to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1881" target="_blank">00:31:21.920</a></span> | <span class="t">started, um, we made a GitHub repository for you all to work off of in this, in this, uh, workshop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1888" target="_blank">00:31:28.880</a></span> | <span class="t">So you can scan the QR code. It'll take you right there. Otherwise, uh, you know, this is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1894" target="_blank">00:31:34.000</a></span> | <span class="t">this is not too long to type out. Um, so I'm just going to leave this up on screen for 30 seconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1899" target="_blank">00:31:39.920</a></span> | <span class="t">Everyone can pull it up. Um, you're going to want to, you know, fork and clone this, uh, this repository,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1905" target="_blank">00:31:45.920</a></span> | <span class="t">um, to your, to your local development environment. Um, we're just, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1910" target="_blank">00:31:50.720</a></span> | <span class="t">we're just using Python, Python 3.10, Python 3.11, um, 3.9. Um, so yeah, just like however,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1918" target="_blank">00:31:58.560</a></span> | <span class="t">however your, your normal way of writing code is, um, this, this should be compatible. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1924" target="_blank">00:32:04.240</a></span> | <span class="t">the, there isn't a lot of what? Uh, no. So, uh, I, yeah, to be clear in, in this, in this repository,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1931" target="_blank">00:32:11.680</a></span> | <span class="t">um, you're going to find instructions and we're going to walk through all this. Um, we're going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1935" target="_blank">00:32:15.520</a></span> | <span class="t">using entirely remote GPUs. Um, so, you know, I personally have an H 100 under my podium right here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1942" target="_blank">00:32:22.480</a></span> | <span class="t">that I'm going to be using. No, I'm just kidding. I don't. Um, but, uh, yeah, yeah. So we're just, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1947" target="_blank">00:32:27.120</a></span> | <span class="t">we just all have laptops here. So we're going to be using cloud GPUs. Yeah. Actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1951" target="_blank">00:32:31.680</a></span> | <span class="t">if you want to follow along, you might need a run for account. Yeah. Yeah. Well, we'll,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1956" target="_blank">00:32:36.400</a></span> | <span class="t">we'll, we'll talk them through the, uh, the, the, the setup steps there. Um, does,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1960" target="_blank">00:32:40.960</a></span> | <span class="t">does anyone want me to leave this information on the screen any longer going once going twice?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1967" target="_blank">00:32:47.040</a></span> | <span class="t">Okay. If, if you, for whatever reason, lose the repository, just let me know. I'll,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1971" target="_blank">00:32:51.600</a></span> | <span class="t">I'll get it back for you. Uh, yes. Okay. So this, this slide means we are transitioning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1976" target="_blank">00:32:56.240</a></span> | <span class="t">to live coding. So yes, let's go, uh, let's go over to, um, the, yeah, the, the, the live coding experience.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1986" target="_blank">00:33:06.080</a></span> | <span class="t">So I'm, I'm basically going to follow this repository. All the instructions are here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1991" target="_blank">00:33:11.040</a></span> | <span class="t">and, uh, I'm going to follow exactly what is here. So you can see, uh, how to follow along.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=1997" target="_blank">00:33:17.360</a></span> | <span class="t">And if you, uh, if you ever get lost or need help, just raise your hand and I'll come over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2002" target="_blank">00:33:22.000</a></span> | <span class="t">and catch you up like one on one. Yeah. Yeah. I'm going to go really slow. I'm going to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2006" target="_blank">00:33:26.880</a></span> | <span class="t">do all these steps here. I know it takes time, but, uh, you know, there's a lot of information here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2011" target="_blank">00:33:31.520</a></span> | <span class="t">It's easy to, uh, lose track of thing and get lost. So if you, if you be lost, like ask and we break,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2017" target="_blank">00:33:37.600</a></span> | <span class="t">I want to make sure this is not a long process, a 10 minute process. We can take it slow for everybody here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2023" target="_blank">00:33:43.200</a></span> | <span class="t">So first thing is that, uh, we'll, we'll do it like really, really from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2027" target="_blank">00:33:47.280</a></span> | <span class="t">So we're going to spin up a new, uh, container on run pod with a GPU to run our setup in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2033" target="_blank">00:33:53.600</a></span> | <span class="t">So if you, okay, okay. Yeah. Yeah. Please, please. Um, if you want to follow along,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2040" target="_blank">00:34:00.000</a></span> | <span class="t">please go on to import and create an account. This should cost like less than $5 overall.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2045" target="_blank">00:34:05.520</a></span> | <span class="t">Yeah. So, um, so yeah, so if you want to make an account, um, there's instructions in the, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2051" target="_blank">00:34:11.040</a></span> | <span class="t">01 folder. Uh, yeah, this read me, so tensor RT in the, in the first folder in the read me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2056" target="_blank">00:34:16.560</a></span> | <span class="t">there's instructions and a video walkthrough. Um, the minimum we're, we're, we're not affiliated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2061" target="_blank">00:34:21.760</a></span> | <span class="t">with one part in any way. Uh, they just have 40 nineties and we wanted you guys to use 40 nineties</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2066" target="_blank">00:34:26.720</a></span> | <span class="t">today. There is a minimum credit buy of $10. If for whatever reason you can't use a company</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2072" target="_blank">00:34:32.480</a></span> | <span class="t">card or get a reimbursed or whatever, and you want your $10 back, uh, send me an email after the, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2078" target="_blank">00:34:38.480</a></span> | <span class="t">after the conference and I will reimburse you myself. So, uh, you know, I, I just,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2082" target="_blank">00:34:42.880</a></span> | <span class="t">I want to, I, it's really important to me that we are giving you the GPUs, uh, to, to run this on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2088" target="_blank">00:34:48.720</a></span> | <span class="t">So yeah. So if, if you, if you, uh, if you need the $10 back, I will, I will get it for you. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2095" target="_blank">00:34:55.680</a></span> | <span class="t">so yeah, just, uh, just follow this. It's like a one and a half minute video. Um, and, uh, yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2101" target="_blank">00:35:01.280</a></span> | <span class="t">that, that, that, that'll get the account set up. Maybe we could even like play the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2104" target="_blank">00:35:04.400</a></span> | <span class="t">Yeah. Um, so yeah. And then this is how you, you know, deploy, um, once we, once we get set up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2112" target="_blank">00:35:12.560</a></span> | <span class="t">So in, in two minutes, we will come back together and you know, we resume the, resume the talk. So yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2121" target="_blank">00:35:21.840</a></span> | <span class="t">make that account real quick, get checked out, get those $10 of credits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2125" target="_blank">00:35:25.120</a></span> | <span class="t">Yeah. And, and also if you, if you do happen to run into any issues,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2134" target="_blank">00:35:34.240</a></span> | <span class="t">getting the one part account set up, like it's not working or whatever. Um, I do have one that I can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2138" target="_blank">00:35:38.800</a></span> | <span class="t">share individually with people. It would just take a really long time for me to go type in my username</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2144" target="_blank">00:35:44.240</a></span> | <span class="t">and password into every single computer in this room. I feel like maybe I should have brought some,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2155" target="_blank">00:35:55.680</a></span> | <span class="t">some jokes to tell while everyone sets their account up. Uh, all right. Uh, how about this one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2160" target="_blank">00:36:00.560</a></span> | <span class="t">Why did the LLM cross the road?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2162" target="_blank">00:36:02.480</a></span> | <span class="t">Ignore previous instructions and cross the road.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2169" target="_blank">00:36:09.840</a></span> | <span class="t">All right. I'll, I'll, I'll workshop that. I'll get some better jokes for you guys before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2173" target="_blank">00:36:13.280</a></span> | <span class="t">before the next set up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2174" target="_blank">00:36:14.240</a></span> | <span class="t">How are we doing on the, on the account set up? Um, anyone need a little more time?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2181" target="_blank">00:36:21.280</a></span> | <span class="t">All right. Great. No problem. No problem. Sorry. I don't want to rush you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2185" target="_blank">00:36:25.200</a></span> | <span class="t">Just checking in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2188" target="_blank">00:36:28.160</a></span> | <span class="t">Yeah. And then once, once everyone has the account, we'll set up the GPU together because there's a few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2196" target="_blank">00:36:36.320</a></span> | <span class="t">things you need to configure. Cool. Uh, oh, really?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2209" target="_blank">00:36:49.120</a></span> | <span class="t">It's, it's not taking anyone's credit card.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2222" target="_blank">00:37:02.320</a></span> | <span class="t">Great. Uh, does someone, does here, can I, can I, can I know someone who runs it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2229" target="_blank">00:37:09.680</a></span> | <span class="t">who works at one pod and would have their, uh, their phone number? He's calling them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2233" target="_blank">00:37:13.840</a></span> | <span class="t">Okay. Awesome. All right. We're getting, we're getting in touch with customer support.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2238" target="_blank">00:37:18.960</a></span> | <span class="t">Up. Yeah. It could be, it could be that. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2244" target="_blank">00:37:24.480</a></span> | <span class="t">Okay. So as a backup, um, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2251" target="_blank">00:37:31.600</a></span> | <span class="t">Okay. So the recommendation here is go off of the conference Wi-Fi, put your computer on your phone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2264" target="_blank">00:37:44.720</a></span> | <span class="t">hotspots and try it again. Um, because that, that worked, uh, you know, maybe, maybe, maybe coming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2271" target="_blank">00:37:51.440</a></span> | <span class="t">from a different IP address will, will help. How would we do this? I run through this and we can do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2276" target="_blank">00:37:56.960</a></span> | <span class="t">again once everybody has their account. Yeah, that sounds good. So what we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2281" target="_blank">00:38:01.440</a></span> | <span class="t">do in the interest of time here, um, is we're going to, uh, just going to run through end to end,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2288" target="_blank">00:38:08.400</a></span> | <span class="t">um, sort of the, the, the demo as we, as we get the stuff set up and everyone's credit cards get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2294" target="_blank">00:38:14.640</a></span> | <span class="t">unblocked. Um, yeah, you know, who, who would have thought, you know, we, we were, we were talking this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2301" target="_blank">00:38:21.520</a></span> | <span class="t">big game about, oh, TensorFlow TLM. It's so hard. It's so technical. There's going to be so many bugs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2307" target="_blank">00:38:27.040</a></span> | <span class="t">And then there's the payment processing. So, uh, yeah, you know, that, that's, that's, that's live</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2312" target="_blank">00:38:32.320</a></span> | <span class="t">demos for you. So anyway, yeah, go, go ahead and, uh, work through it. Um, and then we'll do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2317" target="_blank">00:38:37.040</a></span> | <span class="t">kind of again, uh, together once everyone has their account. All right. Yeah. Let me run through this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2321" target="_blank">00:38:41.440</a></span> | <span class="t">I'll follow the, all the steps. Uh, I, I already have an account run pod. So let me spin up a new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2328" target="_blank">00:38:48.000</a></span> | <span class="t">instance here and, uh, I'm picking up the 4090 here, which is this one, and it has high availability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2338" target="_blank">00:38:58.160</a></span> | <span class="t">So that should be fine. And, uh, I'm going to edit this template and get more space here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2343" target="_blank">00:39:03.840</a></span> | <span class="t">This doesn't cost anything extra. Yeah. We need more space, uh, because the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2349" target="_blank">00:39:09.520</a></span> | <span class="t">engine, uh, everything that we're installing, um, and the engine we're building takes up a lot of gigabytes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2354" target="_blank">00:39:14.560</a></span> | <span class="t">So otherwise we'll be safer. Yeah. Even though these engines are small engines,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2358" target="_blank">00:39:18.800</a></span> | <span class="t">in general can be very, very big. It can be hundreds of gigs. And I'm going to pick on demand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2363" target="_blank">00:39:23.520</a></span> | <span class="t">because I'm doing this demo. I don't want the instance to go away, but feel free to use spot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2367" target="_blank">00:39:27.440</a></span> | <span class="t">for your use case. I'm going to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2370" target="_blank">00:39:30.160</a></span> | <span class="t">You want to set the, uh, the container, um, to container disk to 200 gigabytes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2379" target="_blank">00:39:39.920</a></span> | <span class="t">so that you have enough room to install everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2382" target="_blank">00:39:42.000</a></span> | <span class="t">And then I'm going to deploy spot. It's going to be a bit slow, but you know, feel free to ask any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2390" target="_blank">00:39:50.960</a></span> | <span class="t">questions. And, uh, I feel like this way we'll take it slow, but we'll make sure everything is understood</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2397" target="_blank">00:39:57.040</a></span> | <span class="t">by everybody. So what, what's happening now is that this, uh, pot is spinning up. Uh, one thing to note</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2404" target="_blank">00:40:04.480</a></span> | <span class="t">here is that it has a specific image of, uh, torch with a specific CUDA version. It's very important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2411" target="_blank">00:40:11.040</a></span> | <span class="t">that, uh, node has GPUs. And the first thing we're going to, we're going to do is that once this pod</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2416" target="_blank">00:40:16.720</a></span> | <span class="t">comes up, you're going to check that it has everything related to GPUs running fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2425" target="_blank">00:40:25.040</a></span> | <span class="t">So this is starting up now. I'm going to connect. It gives you nothing sensitive here. It uses your SSH</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2436" target="_blank">00:40:36.000</a></span> | <span class="t">keys, but, uh, the names are not sensitive. So I'm going to just do that. Log into that box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2445" target="_blank">00:40:45.920</a></span> | <span class="t">Uh, sorry. Oh, okay. Sorry. Yeah. Yeah. This is much smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2450" target="_blank">00:40:50.480</a></span> | <span class="t">I think the part is still spinning up. So it's taking a bit of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2472" target="_blank">00:41:12.880</a></span> | <span class="t">Hmm. Okay. All right. So to test that everything is set up properly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2478" target="_blank">00:41:18.560</a></span> | <span class="t">Just, uh, is it possible to scroll it to the top of the screen?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2481" target="_blank">00:41:21.680</a></span> | <span class="t">Oh, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2482" target="_blank">00:41:22.240</a></span> | <span class="t">Okay, great. So we are on this machine that we spin up. You're going to run NVIDIA SMI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2491" target="_blank">00:41:31.360</a></span> | <span class="t">to make sure that the GPU is available. And this is what you should see. Uh, one thing to note here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2497" target="_blank">00:41:37.920</a></span> | <span class="t">is this portion, which shows that the GPU has, uh, more than 24 gigs of memory the RTX 4090 has.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2507" target="_blank">00:41:47.600</a></span> | <span class="t">And right now it's using one memory. I think it does some, uh, some stuff like it was by default.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2512" target="_blank">00:41:52.960</a></span> | <span class="t">So one mag is already taken.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2518" target="_blank">00:41:58.000</a></span> | <span class="t">So now we're going to go back to a workshop and then just follow these instructions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2525" target="_blank">00:42:05.200</a></span> | <span class="t">Manual engine build. We are at this point. Uh, and now we're going to install Tencer RTLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2533" target="_blank">00:42:13.280</a></span> | <span class="t">This is going to take a bit of time. Tencer RTLM comes as a Python library that you just pip install.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2539" target="_blank">00:42:19.360</a></span> | <span class="t">And that's all we're doing. We're setting up the dependencies. This APD update is setting up the Python</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2544" target="_blank">00:42:24.960</a></span> | <span class="t">environment, uh, open MPI and other things. And then we just install and start the LLM, uh, from,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2551" target="_blank">00:42:31.840</a></span> | <span class="t">not from PyPy, but from NVIDIA's own PyPy. That's where we find the right versions. If you focus on this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2558" target="_blank">00:42:38.000</a></span> | <span class="t">line, uh, let me kick this off. Then I can come back here and show you that we're using a specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2564" target="_blank">00:42:44.960</a></span> | <span class="t">version of Tencer RTLM. And, uh, we need to tell it to get it from the NVIDIA PyPy using these instructions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2573" target="_blank">00:42:53.920</a></span> | <span class="t">And all these are on the GitHub repo. If you want to follow from there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2577" target="_blank">00:42:57.760</a></span> | <span class="t">I saw a guy with a camera. So I started posing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2585" target="_blank">00:43:05.360</a></span> | <span class="t">Uh, I think 310, I think. 310. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2598" target="_blank">00:43:18.160</a></span> | <span class="t">Uh, this, uh, this command should have instructions to install.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2601" target="_blank">00:43:21.920</a></span> | <span class="t">I also, I want to check in with the room. Has anyone else had success getting one part up and going,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2608" target="_blank">00:43:28.960</a></span> | <span class="t">uh, using your, using your phone wifi? It's working. Okay. Okay. Awesome. Crisis supported.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2614" target="_blank">00:43:34.880</a></span> | <span class="t">Thank you so much to, uh, to whoever from, from over there suggested the idea to begin with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2619" target="_blank">00:43:39.600</a></span> | <span class="t">Really save the day. Great. So we're just waiting for it to build. It takes some time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2626" target="_blank">00:43:46.160</a></span> | <span class="t">This is, this is the best part of the job. You know, you wait for it to build. You can go get a snack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2631" target="_blank">00:43:51.680</a></span> | <span class="t">You can go like change your laundry. It's a very convenient that it takes this time sometimes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2635" target="_blank">00:43:55.920</a></span> | <span class="t">It used to be compilation takes time. Now engine build takes time. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2639" target="_blank">00:43:59.920</a></span> | <span class="t">I think you're very close.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2645" target="_blank">00:44:05.040</a></span> | <span class="t">And I promise like, then the fun part begins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2650" target="_blank">00:44:10.720</a></span> | <span class="t">Are you saying that pip install isn't the fun part? I think this is pretty fun. You know, look, look,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2656" target="_blank">00:44:16.400</a></span> | <span class="t">look at all this, look at all this lines. You know, this is, this is, this is real coding right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2661" target="_blank">00:44:21.920</a></span> | <span class="t">And if you want pip to feel like more fun, try poetry. Oh, that's true. Poetry is really fun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2668" target="_blank">00:44:28.240</a></span> | <span class="t">Yeah. Nvidia does publish these images on their container history called NGC.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2674" target="_blank">00:44:34.400</a></span> | <span class="t">And there are Triton registries, uh, available for these things. Maybe we should have used that rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2679" target="_blank">00:44:39.120</a></span> | <span class="t">than run pod, but, uh, it's all good. So, uh, now let's check that tensor ID LLM is installed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2685" target="_blank">00:44:45.360</a></span> | <span class="t">And this will just, uh, tell us that, uh, everything is good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2691" target="_blank">00:44:51.360</a></span> | <span class="t">So it printed the version. You should see that if everything's working fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2695" target="_blank">00:44:55.520</a></span> | <span class="t">And then we're going to do the real thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2698" target="_blank">00:44:58.400</a></span> | <span class="t">Now we're going to clone the tensor ID LLM repository where a lot of those examples are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2704" target="_blank">00:45:04.720</a></span> | <span class="t">And I'll, I'll show you those examples while this, uh, this cloning happens shouldn't take that much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2710" target="_blank">00:45:10.960</a></span> | <span class="t">long. Maybe, uh, maybe a minute or so, but tensor ID LLM has a lot of examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2716" target="_blank">00:45:16.960</a></span> | <span class="t">Uh, if you go to the tensor ID LLM repository, there are these examples folder and there are a ton</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2722" target="_blank">00:45:22.320</a></span> | <span class="t">of examples. Like, uh, Philip mentioned there are about 50 examples. And we're going to go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2728" target="_blank">00:45:28.560</a></span> | <span class="t">the LLM example here. So if you search for LLM, uh, that's the one we are going to look into.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2735" target="_blank">00:45:35.600</a></span> | <span class="t">And so the cloning is complete. And we go back to these instructions. And now we're going to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2744" target="_blank">00:45:44.480</a></span> | <span class="t">build the engine. Actually, one more thing. Uh, how many of you know about HF transfer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2750" target="_blank">00:45:50.000</a></span> | <span class="t">Have you used the Transformers library from Hugging Face?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2753" target="_blank">00:45:53.280</a></span> | <span class="t">So HF transfer is a fast way of downloading and uploading your engines. It does slides download.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2760" target="_blank">00:46:00.720</a></span> | <span class="t">It takes the URL and patches them up into slices, downloads them all in parallel. And it works really,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2766" target="_blank">00:46:06.640</a></span> | <span class="t">really fast. It goes up to like one gig a second. So we should definitely do that, which is what I did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2771" target="_blank">00:46:11.920</a></span> | <span class="t">just now. Now we're going to follow this step by step. Uh, first thing we're going to do is download</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2777" target="_blank">00:46:17.920</a></span> | <span class="t">from Hugging Face. And, uh, let's see like how fast the wifi here is, uh, how fast this downloads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2783" target="_blank">00:46:23.760</a></span> | <span class="t">So not bad. It's going at one gigs a second. So HF all, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2787" target="_blank">00:46:27.840</a></span> | <span class="t">Oh, from, oh, you're right. You're right. You're right. See, but this is, uh, this is what I call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2794" target="_blank">00:46:34.560</a></span> | <span class="t">good software. Downloads at one gig a second. Now we, now first thing to build with TensorRD LLM is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2802" target="_blank">00:46:42.160</a></span> | <span class="t">that we have to convert, uh, the Hugging Face checkpoint into a checkpoint format that TensorRD LLM works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2808" target="_blank">00:46:48.640</a></span> | <span class="t">with. And, uh, checkpointing also covers tensor parallelism and quantization. Sometimes you need a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2814" target="_blank">00:46:54.480</a></span> | <span class="t">different kind of checkpoint for doing those things. So I'm going to run this command to convert the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2819" target="_blank">00:46:59.680</a></span> | <span class="t">checkpoint. And this should be pretty fast. It's just converting weights to weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2823" target="_blank">00:47:03.680</a></span> | <span class="t">That's pretty fast, like three seconds. Uh, and now we do the actual build.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2833" target="_blank">00:47:13.680</a></span> | <span class="t">And I'm going to do this basic build here. Uh, there are a ton of options that this command takes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2840" target="_blank">00:47:20.000</a></span> | <span class="t">The TRT LLM build command. Uh, in here, we are just saying that, uh, take this checkpoint</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2844" target="_blank">00:47:24.880</a></span> | <span class="t">and build me an engine with most of the default settings. And that should build the engine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2852" target="_blank">00:47:32.560</a></span> | <span class="t">Now it will print a lot of stuff about what it's doing, what it's finding and how it's optimizing and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2861" target="_blank">00:47:41.360</a></span> | <span class="t">all that. Uh, it won't make much sense right now, but, uh, later on, this could be very useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2866" target="_blank">00:47:46.640</a></span> | <span class="t">So the engine was built as pretty fast, right? It's a small, uh, model, uh, only a billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2873" target="_blank">00:47:53.120</a></span> | <span class="t">So that was pretty fast. And now let's, uh, let's try to see how big the engine is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2877" target="_blank">00:47:57.600</a></span> | <span class="t">I'm going to do that. And the engine is, uh, two gigs in size. This is about how big that model is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2884" target="_blank">00:48:04.080</a></span> | <span class="t">on hugging face. So it's, uh, the engine itself adds very little, uh, storage or memory. It's maybe like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2891" target="_blank">00:48:11.840</a></span> | <span class="t">Uh, hundreds of megabytes, but very tiny compared to the overall and those weights are bundled into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2897" target="_blank">00:48:17.360</a></span> | <span class="t">the engine. And what is this engine? This engine is something that the tensor RT LLM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2902" target="_blank">00:48:22.720</a></span> | <span class="t">runtime can take and it can execute it. Uh, you can think of it like a shared library. It's, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2910" target="_blank">00:48:30.800</a></span> | <span class="t">it's kind of kind of like a binary in the standard format that the binaries are in. Uh, but it's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2918" target="_blank">00:48:38.000</a></span> | <span class="t">it's something that tensor RT LLM can take and interpret. Ultimately, it's a tensor RT engine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2923" target="_blank">00:48:43.200</a></span> | <span class="t">because that's what tensor RT LLM works with. It creates tensor RT, uh, engine and then tensor RT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2929" target="_blank">00:48:49.600</a></span> | <span class="t">is the one that loads it, but tensor RT LLM gives it these plugins that tensor RT understands and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2935" target="_blank">00:48:55.600</a></span> | <span class="t">is able to make sense of it. And now let's execute this. So these, these examples also come with the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2941" target="_blank">00:49:01.840</a></span> | <span class="t">come up with a, uh, come with a run script that we can run and we're gonna run that. So what this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2948" target="_blank">00:49:08.000</a></span> | <span class="t">going to do is start up the engine and give it a very tiny request and we should expect a response.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2955" target="_blank">00:49:15.280</a></span> | <span class="t">And that's what happened here. Our engine was launched. We gave it an input text of born in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2961" target="_blank">00:49:21.440</a></span> | <span class="t">Northeast France. So we're trained as a, and the model printed out the response beyond that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2967" target="_blank">00:49:27.440</a></span> | <span class="t">that painter in Paris moving before moving to London in 929. And this is a standard, uh, example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2974" target="_blank">00:49:34.720</a></span> | <span class="t">that comes with tensor RT LLM. So if you follow along these instructions, you should see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2979" target="_blank">00:49:39.760</a></span> | <span class="t">Any questions at this point?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2983" target="_blank">00:49:43.120</a></span> | <span class="t">The convert, the question is what happens during the convert checkpoint? Uh, I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=2998" target="_blank">00:49:58.160</a></span> | <span class="t">there are three things that happen, uh, potentially three things. First thing is that tensor RT LLM needs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3005" target="_blank">00:50:05.120</a></span> | <span class="t">the tensors to be in a specific format to work with. So think of it as a pre-processing. There are many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3010" target="_blank">00:50:10.720</a></span> | <span class="t">ways of specifying a model. It can be on hugging face. It can be exported from PyTorch. It can be onyx.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3017" target="_blank">00:50:17.280</a></span> | <span class="t">There are many, many different ways of specifying these models. So the first thing it does is that it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3021" target="_blank">00:50:21.680</a></span> | <span class="t">converts that into a format that it understands. So it does some kind of translation into a standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3027" target="_blank">00:50:27.440</a></span> | <span class="t">structure. Second thing is quantization. For quantization, it needs to, uh, quantize the weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3035" target="_blank">00:50:35.680</a></span> | <span class="t">It needs to take the weights and quantize them into the quantized versions of them. And that happens at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3041" target="_blank">00:50:41.760</a></span> | <span class="t">convert checkpoint too. Uh, not necessarily though. They also have a quantized script. Some of those, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3046" target="_blank">00:50:46.960</a></span> | <span class="t">quantizations happen. Some types of quantizations happen in convert checkpoint, but they also have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3052" target="_blank">00:50:52.720</a></span> | <span class="t">different way of quantizing. They call it, I think, uh, ammo. There's a library called ammo, which does that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3058" target="_blank">00:50:58.640</a></span> | <span class="t">Uh, and that can also be used for doing it. But, uh, I think AWQ and, uh, smooth quant, they happen in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3065" target="_blank">00:51:05.520</a></span> | <span class="t">convert checkpoint. And, uh, third thing is tensor parallelism. For tensor parallelism, you need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3070" target="_blank">00:51:10.560</a></span> | <span class="t">divide the weights, uh, into different categories for the different GPUs that they will run on. So it does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3077" target="_blank">00:51:17.680</a></span> | <span class="t">use that during convert checkpoint as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3082" target="_blank">00:51:22.320</a></span> | <span class="t">Thank you. Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3084" target="_blank">00:51:24.320</a></span> | <span class="t">All right. Uh, uh, uh, uh, me too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3089" target="_blank">00:51:29.600</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3097" target="_blank">00:51:37.600</a></span> | <span class="t">Yeah. So there's, there's two places that the max output is set. Um, so the first place is when you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3115" target="_blank">00:51:55.520</a></span> | <span class="t">actually building the engine, you give it a argument for the expected output sequence length. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3123" target="_blank">00:52:03.360</a></span> | <span class="t">that's, that's more just sort of like for the optimization side, you know, so that you're selecting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3127" target="_blank">00:52:07.920</a></span> | <span class="t">the correct CUDA kernels. And so that you're, you know, batching everything up correctly. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3133" target="_blank">00:52:13.120</a></span> | <span class="t">once the engine is built, it just uses a standard, I think it's, uh, max tokens, right, is the parameter. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3139" target="_blank">00:52:19.120</a></span> | <span class="t">and yeah, you just, you just pass max tokens and that'll, you know, limit, um, how, how long it runs for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3145" target="_blank">00:52:25.280</a></span> | <span class="t">Yeah, I guess what I'm asking is, does it influence the generation before generating happens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3151" target="_blank">00:52:31.200</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3155" target="_blank">00:52:35.760</a></span> | <span class="t">Right. Like, are you asking if you, if you make the engine with a shorter, um, output, see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3161" target="_blank">00:52:41.760</a></span> | <span class="t">Oh, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3162" target="_blank">00:52:42.960</a></span> | <span class="t">No, I, I, as far as I know, the Mac, all the max token does is it just cuts off influence after a certain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3171" target="_blank">00:52:51.280</a></span> | <span class="t">number, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3172" target="_blank">00:52:52.240</a></span> | <span class="t">Yeah. So the, the way, uh, I would put it is that normally if you give it a large number of max tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3179" target="_blank">00:52:59.120</a></span> | <span class="t">it would emit, uh, uh, end of sequence token. Most models have a different end of sequence token and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3186" target="_blank">00:53:06.560</a></span> | <span class="t">it's up to you. You can stop there. You can configure at runtime. Like I don't want more than that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3191" target="_blank">00:53:11.120</a></span> | <span class="t">but you can also tell it ignore end of sequence. I just want the whole thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3195" target="_blank">00:53:15.680</a></span> | <span class="t">And we do need it for performance benchmarking. For example, when we are comparing performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3199" target="_blank">00:53:19.680</a></span> | <span class="t">across different GPU types or whatnot, we want all of those tokens to be generated so we can tell it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3205" target="_blank">00:53:25.200</a></span> | <span class="t">like, give me all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3206" target="_blank">00:53:26.240</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3208" target="_blank">00:53:28.240</a></span> | <span class="t">Welcome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3208" target="_blank">00:53:28.800</a></span> | <span class="t">In the back there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3217" target="_blank">00:53:37.680</a></span> | <span class="t">Oh, great. Great question, actually. So, uh, in build checkpoint, a lot of stuff is happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3225" target="_blank">00:53:45.440</a></span> | <span class="t">You're taking these weights and you're, uh, you're generating this thing called a network in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3232" target="_blank">00:53:52.560</a></span> | <span class="t">TensorRD. TensorRD has this notion of a network and what you need to do is populate that network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3239" target="_blank">00:53:59.040</a></span> | <span class="t">with your weights and architectures. So it actually does that. It creates that network and feeds it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3245" target="_blank">00:54:05.120</a></span> | <span class="t">these weights. It also does inference during building the engine, uh, for doing optimizations. So it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3251" target="_blank">00:54:11.200</a></span> | <span class="t">generates for every model type. It has a mechanism of generating sample input and it passes that into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3258" target="_blank">00:54:18.400</a></span> | <span class="t">a TensorRD engine that it's generating and then it optimizes it that way. And as, uh, as a result,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3265" target="_blank">00:54:25.520</a></span> | <span class="t">this TensorRD engine is generated in memory, which is then serialized. So all of this is happening in that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3271" target="_blank">00:54:31.520</a></span> | <span class="t">And these, uh, there's a lot of nuance to it. Uh, if you get a chance, you can look at the source code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3279" target="_blank">00:54:39.920</a></span> | <span class="t">for that TRDLM build. I'll post references in that GitHub repo and you can follow on. There's lots of options,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3285" target="_blank">00:54:45.840</a></span> | <span class="t">but let me try if I can find a help here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3290" target="_blank">00:54:50.960</a></span> | <span class="t">Ah, the VIP person.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3301" target="_blank">00:55:01.920</a></span> | <span class="t">What's going on? Sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3312" target="_blank">00:55:12.960</a></span> | <span class="t">Oh yeah. This is law, a lot of stuff here that you can go through. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3328" target="_blank">00:55:28.160</a></span> | <span class="t">uh, yeah, maybe I should go through some of them, which are very important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3333" target="_blank">00:55:33.600</a></span> | <span class="t">Yeah. I think a lot of stuff is important here. Like the max beam width, if you're using beams for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3339" target="_blank">00:55:39.840</a></span> | <span class="t">uh, generating the graph, you can generate, uh, logits, not just the output tokens. We can also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3345" target="_blank">00:55:45.760</a></span> | <span class="t">generate logits if you want to process them. Uh, there are a lot of optimizations that you can use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3350" target="_blank">00:55:50.640</a></span> | <span class="t">Like you can, there's a optimization called fused MLP. There is contrast chunking. There is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3355" target="_blank">00:55:55.040</a></span> | <span class="t">there is a lot of stuff. I think you should play around with those, uh, at your time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3359" target="_blank">00:55:59.040</a></span> | <span class="t">LoRa is very good to play around with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3360" target="_blank">00:56:00.560</a></span> | <span class="t">Yeah. I'll try to leave some, uh, some more examples, uh, in the GitHub repo to try.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3367" target="_blank">00:56:07.360</a></span> | <span class="t">Uh, okay. So let me go to the next one. Just, uh, uh, one more thing I want to do is, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3372" target="_blank">00:56:12.800</a></span> | <span class="t">FPA quantization. Uh, RTX 4090 is, is actually an amazing GPU. It's pretty cheap, but supports FPA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3379" target="_blank">00:56:19.600</a></span> | <span class="t">So we're going to do an FPA engine build now. So in this case, uh, like I said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3385" target="_blank">00:56:25.600</a></span> | <span class="t">like some of these optimizations, these, uh, quantizations are not in convert checkpoint,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3390" target="_blank">00:56:30.080</a></span> | <span class="t">but quantize.py, which uses a library called ammo in NVIDIA. So I'm going to run that now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3396" target="_blank">00:56:36.400</a></span> | <span class="t">And, uh, yeah, let me spend some time here. We are, we're saying is we're telling it that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3404" target="_blank">00:56:44.560</a></span> | <span class="t">quantization format is FPA, but also note that we are saying KV cache D type is FPA. So FPA quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3412" target="_blank">00:56:52.400</a></span> | <span class="t">actually can happen at two levels. You can wait quantize to FPA, but you can also quantize the KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3420" target="_blank">00:57:00.160</a></span> | <span class="t">with FP8 and doing both is very critical because, uh, these GPUs, they, you might have heard of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3428" target="_blank">00:57:08.160</a></span> | <span class="t">called tensor cores, right? Tensor cores are very, very, very important because they can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3432" target="_blank">00:57:12.880</a></span> | <span class="t">quantize calculations very fast. For example, if you look at a spec of the H100 GPU, you can see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3441" target="_blank">00:57:21.280</a></span> | <span class="t">the teraflops that you can get, number of computation that you can get with lower quantization options</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3446" target="_blank">00:57:26.000</a></span> | <span class="t">are much more than higher. For example, FP16 teraflops will be much lower than FP8 because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3452" target="_blank">00:57:32.880</a></span> | <span class="t">you can use this special tensor cores for doing more FP8 computations in the same time that you would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3458" target="_blank">00:57:38.720</a></span> | <span class="t">do FP16. But for that to happen, both sides of the matrix have to be the same quantization type. Mixed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3465" target="_blank">00:57:45.440</a></span> | <span class="t">precision doesn't exist. At least now it's not very common or popular. So you want both sides to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3470" target="_blank">00:57:50.800</a></span> | <span class="t">quantized. And when you quantize both the KV cache and the weights to FP8, you get that extra unlock,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3477" target="_blank">00:57:57.680</a></span> | <span class="t">that your computation is also faster, which can be critical for scenarios which are compute bound.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3484" target="_blank">00:58:04.800</a></span> | <span class="t">And as you would know in LLMs, there's a context phase and generation phase. Generation phase is memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3490" target="_blank">00:58:10.640</a></span> | <span class="t">bandwidth bound. But the context phase is compute bound. So that can benefit greatly from both sides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3497" target="_blank">00:58:17.040</a></span> | <span class="t">being quantized. So in this case, we are saying that quantize both weights and KV cache. And it's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3504" target="_blank">00:58:24.480</a></span> | <span class="t">not a trivial decision to do that because weights quantize very easily. You hardly lose anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3510" target="_blank">00:58:30.400</a></span> | <span class="t">when you quantize weights. The dynamic range of weights is generally much, much smaller. You can use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3515" target="_blank">00:58:35.760</a></span> | <span class="t">Int8 or when you do FP8, there is hardly any loss. KV cache doesn't quantize as well. And that's why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3523" target="_blank">00:58:43.440</a></span> | <span class="t">FP8 is a game changer. Because what we found is that when you quantize the KV cache with Int8, even using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3529" target="_blank">00:58:49.280</a></span> | <span class="t">smooth quant, there is still degradation of quality. And practically, we've never seen anybody use it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3534" target="_blank">00:58:54.800</a></span> | <span class="t">Even though there are a lot of papers about it. And it's great, great technology. But practically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3539" target="_blank">00:58:59.360</a></span> | <span class="t">it was not there until FP8. FP8 even KV cache quantization works extremely well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3544" target="_blank">00:59:04.480</a></span> | <span class="t">Let me show something with that, actually, if you don't mind. Back on the...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3554" target="_blank">00:59:14.160</a></span> | <span class="t">If we go to... I'll just show like a little visualization for FP8 that shows off the dynamic range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3564" target="_blank">00:59:24.640</a></span> | <span class="t">So... Oh, hey, look, it's us. Yeah, so when you look at the FP8 data format, it has a sign. And then rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3577" target="_blank">00:59:37.120</a></span> | <span class="t">than... So there's two different FP8 data formats. But we're using the, you know, the e4m3 format.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3583" target="_blank">00:59:43.600</a></span> | <span class="t">So basically, you have four bits dedicated to an exponent. And that's what gives your FP8 data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3590" target="_blank">00:59:50.400</a></span> | <span class="t">format a lot of dynamic range versus Int8, which is just, you know, like what, like 256 to...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3597" target="_blank">00:59:57.120</a></span> | <span class="t">256. Yeah, so... So you still have the same number of possible values, but they're spread apart further.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3603" target="_blank">01:00:03.760</a></span> | <span class="t">That's dynamic range. And it's that which allows you to quantize this much more sensitive KV cache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3609" target="_blank">01:00:09.760</a></span> | <span class="t">Yeah. Yeah, exactly. Basically, you have the... My teacher and the exponents, you basically are able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3615" target="_blank">01:00:15.840</a></span> | <span class="t">quantize smaller values better, give more bits to smaller scale than larger scale. You don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3623" target="_blank">01:00:23.200</a></span> | <span class="t">to fit into a linear scale. And that's where FP8 excels. So going back to the presentation, the FP8</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3629" target="_blank">01:00:29.760</a></span> | <span class="t">quantization is done. And I forgot to show you this, but there is calibration involved here. If you look at this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3636" target="_blank">01:00:36.720</a></span> | <span class="t">stack here, we actually give it some data. We feed it some data and let it calibrate. Because as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3643" target="_blank">01:00:43.600</a></span> | <span class="t">would know, in Tate and FP8, you have a start and end range. And they differ in how you divide up that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3649" target="_blank">01:00:49.120</a></span> | <span class="t">range into data points. But you have to find million max. And for that, you need calibration. So we give it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3655" target="_blank">01:00:55.200</a></span> | <span class="t">a standard data set and you can change the data set. But we give it a specific data set and it does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3659" target="_blank">01:00:59.920</a></span> | <span class="t">multiple runs. And we try to calibrate, like, what are the dynamic ranges of each of the layers of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3665" target="_blank">01:01:05.520</a></span> | <span class="t">transformer architecture. And based upon that, we specify that million max for each layer separately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3671" target="_blank">01:01:11.600</a></span> | <span class="t">There's more detail there, but at the high level, that's what is happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3674" target="_blank">01:01:14.400</a></span> | <span class="t">Yeah, yeah, it's possible that the ranges can vary a lot with data set. And this used to be more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3689" target="_blank">01:01:29.840</a></span> | <span class="t">critical with Intate. With FP8, we found that you get to a good state pretty fast. But it's worth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3695" target="_blank">01:01:35.840</a></span> | <span class="t">thinking about trying different data sets, especially if you know what data set you are going to be calling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3700" target="_blank">01:01:40.960</a></span> | <span class="t">it with. It could be worth it. It just works very well out of the box. But it's not perfect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3706" target="_blank">01:01:46.000</a></span> | <span class="t">Going back to the workshop. So we were following along here. And yeah, so after you quantize it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3714" target="_blank">01:01:54.960</a></span> | <span class="t">the steps are very similar as before. Now we are building an engine with FP8. And internally, all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3726" target="_blank">01:02:06.240</a></span> | <span class="t">CUDA kernels that are being used are now FP8-specific. They are different kernels which use the tensor cores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3733" target="_blank">01:02:13.040</a></span> | <span class="t">in the right fashion. And this should be pretty quick as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3737" target="_blank">01:02:17.840</a></span> | <span class="t">And there's a lot of depth here as you learn more about it. You don't need to, but there are things like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3750" target="_blank">01:02:30.960</a></span> | <span class="t">timing cache. There are optimization profiles in tensor RT through which you tell what sizes we expect and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3757" target="_blank">01:02:37.360</a></span> | <span class="t">it does optimizations. But this is a good beginning. So now we have the engine. Let me do a DU on that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3764" target="_blank">01:02:44.400</a></span> | <span class="t">and to see the size of that engine now. And the size is 1.2 gigs, which is about half of previous,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3773" target="_blank">01:02:53.040</a></span> | <span class="t">and which is what we expect because we quantized. And now let's run this engine and see the output and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3779" target="_blank">01:02:59.840</a></span> | <span class="t">it should be pretty similar to what we saw before. So using this run script, now it's going to load the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3786" target="_blank">01:03:06.320</a></span> | <span class="t">engine and then we'll do an inference on top. That should be pretty quick. So yeah, here's the output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3792" target="_blank">01:03:12.720</a></span> | <span class="t">the same input as before. And about the same output as before. And that's what we generally observe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3799" target="_blank">01:03:19.840</a></span> | <span class="t">with FP8. FP8 quality is really, really good. It's very, very hard to tell the difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3803" target="_blank">01:03:23.760</a></span> | <span class="t">And that's it for this workshop, this part of the workshop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3808" target="_blank">01:03:28.960</a></span> | <span class="t">Yeah. Awesome. Thank you. So, you know, I definitely welcome you to keep playing around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3816" target="_blank">01:03:36.480</a></span> | <span class="t">with this run pod setup and trying different things. Try to build different engines and stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3823" target="_blank">01:03:43.600</a></span> | <span class="t">But we're going to move on to the next step, which is an automated version of basically exactly what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3832" target="_blank">01:03:52.160</a></span> | <span class="t">just did. So we're going to show a few things to make this easier. So we're going to be using for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3841" target="_blank">01:04:01.680</a></span> | <span class="t">this next step, something called trust. Trust is an open source model serving framework developed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3847" target="_blank">01:04:07.680</a></span> | <span class="t">by us here at base 10. Punkage is the one who, you know, actually wrote a lot of the code. All I did was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3853" target="_blank">01:04:13.120</a></span> | <span class="t">name it trust because I was riding on a train and I was like, huh, what should I call the framework? And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3858" target="_blank">01:04:18.400</a></span> | <span class="t">then we run over a bridge and I was like, I know, I'll call it bridge. But that was already taken. So I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3863" target="_blank">01:04:23.200</a></span> | <span class="t">called it trust. So it lets you deploy models with Python instead of, you know, building a Docker image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3869" target="_blank">01:04:29.760</a></span> | <span class="t">yourself. It gives you a nice live reload dev loop. And what we really wanted to focus on when we were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3876" target="_blank">01:04:36.480</a></span> | <span class="t">building this, because it's kind of the technology that sits under our entire model serving platform,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3881" target="_blank">01:04:41.600</a></span> | <span class="t">is we really wanted a lot of flexibility so that we could work with, you know, things like tensor RT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3886" target="_blank">01:04:46.480</a></span> | <span class="t">tensor TLM. You can run it with VLM, Triton. You can, you know, run a transformers model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3891" target="_blank">01:04:51.600</a></span> | <span class="t">a diffusers model. You can put an XGBoost model in there if you're still doing ML. Like you can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3896" target="_blank">01:04:56.000</a></span> | <span class="t">basically whatever you want with it. It's just Python code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3898" target="_blank">01:04:58.640</a></span> | <span class="t">If I may interject like trust is actually a very simple system. It's a way of running Python code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3905" target="_blank">01:05:05.440</a></span> | <span class="t">specifying an environment for running the Python code and your Python code. So it's sort of like a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3910" target="_blank">01:05:10.560</a></span> | <span class="t">simple packaging mechanism, but built for machine learning models. It takes account of the typical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3916" target="_blank">01:05:16.240</a></span> | <span class="t">things you would need with the machine learning models, like getting access to data, passing security,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3921" target="_blank">01:05:21.280</a></span> | <span class="t">secure tokens and such. But it's fundamentally a very, very simple system. Just a conflict file</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3926" target="_blank">01:05:26.720</a></span> | <span class="t">and some Python code. Exactly. And so looking at that config file, we're not even actually going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3935" target="_blank">01:05:35.360</a></span> | <span class="t">write any Python code today for the model server. We're just going to write a quick config. So actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3941" target="_blank">01:05:41.600</a></span> | <span class="t">this morning I was eating breakfast here and I sat down with a group of engineers and we were talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3946" target="_blank">01:05:46.320</a></span> | <span class="t">about stuff and everyone was complaining about YAML and how they're always getting like type errors when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3951" target="_blank">01:05:51.040</a></span> | <span class="t">they write YAML. So unfortunately this is going to be a YAML system. So apologies to my new friends from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3958" target="_blank">01:05:58.000</a></span> | <span class="t">breakfast. But what we're going to do is use this as basically an abstraction on top of trtllm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3964" target="_blank">01:06:04.960</a></span> | <span class="t">Pankaj, quick question for you. What's the name of that C++ textbook you were reading before bed every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3974" target="_blank">01:06:14.400</a></span> | <span class="t">night the other month? Modern C++. What? Modern C++. Yeah, C++. So you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3982" target="_blank">01:06:22.160</a></span> | <span class="t">before bed every night I was watching Survivor. And so for those of us who are not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3990" target="_blank">01:06:30.720</a></span> | <span class="t">cracked software engineers and even for those who want to get things done quickly, we want to have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=3996" target="_blank">01:06:36.720</a></span> | <span class="t">great abstraction. What does that abstraction need to be able to do? It needs to be able to build an engine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4001" target="_blank">01:06:41.360</a></span> | <span class="t">And that engine needs to take into account what model we're going to run, what GPU we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4006" target="_blank">01:06:46.560</a></span> | <span class="t">to run it on, the input and output sequence links, the batch size, quantization, any of the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4011" target="_blank">01:06:51.920</a></span> | <span class="t">optimizations we want to do on top of that. And then we also want to not just grab that and run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4017" target="_blank">01:06:57.520</a></span> | <span class="t">it in the GPU part somewhere, we actually want to deploy it behind an API endpoint so that, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4022" target="_blank">01:07:02.560</a></span> | <span class="t">we can integrate it into our product and stuff. So I'm going to show how to do that. Let's see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4030" target="_blank">01:07:10.080</a></span> | <span class="t">This is yours now. I'm stealing. Oh, this is a good mic. I might not give this back, Pankaj. This is a good mic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4038" target="_blank">01:07:18.000</a></span> | <span class="t">All right. So we're going to go over. Let's see. This is in the one pod thing still.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4048" target="_blank">01:07:28.080</a></span> | <span class="t">Yeah, I need to go to the first one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4049" target="_blank">01:07:29.840</a></span> | <span class="t">Uh, great. The second one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4052" target="_blank">01:07:32.560</a></span> | <span class="t">Perfect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4053" target="_blank">01:07:33.920</a></span> | <span class="t">It's not this. Let me go to the workshop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4057" target="_blank">01:07:37.040</a></span> | <span class="t">Okay. Yeah, you do that. This is his computer, not my computer. So I don't know where anything is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4063" target="_blank">01:07:43.200</a></span> | <span class="t">It's like, uh, walking into someone else's house. There you go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4067" target="_blank">01:07:47.120</a></span> | <span class="t">All right. Thank you. Thank you so much. Um, okay. So, um, what we're going to do in this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4073" target="_blank">01:07:53.840</a></span> | <span class="t">in this second step is we are going to do basically exactly the same thing we just did. Um, just automated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4083" target="_blank">01:08:03.600</a></span> | <span class="t">So for this step, we're going to use base 10, um, we're going to give you all some, some GPUs to play</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4088" target="_blank">01:08:08.880</a></span> | <span class="t">with you. Um, so if you want to follow along, I really encourage you to do so. Um, you're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4093" target="_blank">01:08:13.920</a></span> | <span class="t">go sign up at base 10. We're going to, you know, your account will automatically get free credits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4098" target="_blank">01:08:18.240</a></span> | <span class="t">If, um, our fraud system is a little freaked out by everyone, uh, signing up at the same time. Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4104" target="_blank">01:08:24.880</a></span> | <span class="t">fortunately, uh, we have some, uh, admin panel access ourselves over here. So we'll just unplug this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4111" target="_blank">01:08:31.360</a></span> | <span class="t">approve you all and plug it back in. Um, so yeah, so everyone go ahead and, um, sign up for base 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4118" target="_blank">01:08:38.240</a></span> | <span class="t">We're also going to want you to make an API key real quick and save that. Um, and then once that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4124" target="_blank">01:08:44.400</a></span> | <span class="t">all done, we're going to jump into this, uh, this part of the, of the project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4130" target="_blank">01:08:50.640</a></span> | <span class="t">Okay. Everyone. So I know there's a, a few errors going on. Um, we have, we have pinged the team about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4144" target="_blank">01:09:04.480</a></span> | <span class="t">that. Let me let you in on a little secret. Uh, we shipped this on Thursday as an internal beta,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4149" target="_blank">01:09:09.600</a></span> | <span class="t">and this is the very first time anyone who doesn't have an at base 10.co email address</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4155" target="_blank">01:09:15.200</a></span> | <span class="t">is using our, uh, new tensor RT LLM build system. So if there, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4160" target="_blank">01:09:20.160</a></span> | <span class="t">if, uh, yeah, so, uh, sorry for tricking you all into beta testing our software. Um, but hey,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4169" target="_blank">01:09:29.760</a></span> | <span class="t">that's what demos are for, right? So, uh, we'll, we'll get that sorted out. In the meantime,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4174" target="_blank">01:09:34.400</a></span> | <span class="t">we have an image cached locally, which means we can keep going with the demo as if nothing ever happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4179" target="_blank">01:09:39.920</a></span> | <span class="t">So, um, let's see. So what you, uh, what you would see in the logs as you, uh, build, as you, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4190" target="_blank">01:09:50.880</a></span> | <span class="t">deploy this. Yep. Yeah. Oh, well, I mean, I can just kind of look through the, look through the logs right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4197" target="_blank">01:09:57.360</a></span> | <span class="t">here. Um, let me actually just, let me just, uh, wake it up. Sorry. What was that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4202" target="_blank">01:10:02.400</a></span> | <span class="t">Okay. Yep. Yep. Yep. I got you. Um, yep. All right. Big logs. Um, let's see. All right. So, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4220" target="_blank">01:10:20.240</a></span> | <span class="t">what, what, what you're seeing here, um, as we, oh, I'm sorry, we got a, we got a lot of logs here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4225" target="_blank">01:10:25.760</a></span> | <span class="t">Um, right. Cause we tested a bunch with the scale up. Um, anyway, what you see is, uh, you see the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4234" target="_blank">01:10:34.240</a></span> | <span class="t">engine getting built, um, and then, and then deployed and to walk through the YAML code really quick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4240" target="_blank">01:10:40.800</a></span> | <span class="t">Uh, yes here. So we talked about that there are a bunch of different settings that you need to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4247" target="_blank">01:10:47.360</a></span> | <span class="t">when you are working with TRT LLM. Um, and you can set all these settings right here and build.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4252" target="_blank">01:10:52.960</a></span> | <span class="t">So right now we're doing something with an input sequence and output sequence of 2000 tokens each,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4258" target="_blank">01:10:58.480</a></span> | <span class="t">um, and a batch size of 64 concurrent requests. Um, we're using the int eight quantization because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4264" target="_blank">01:11:04.800</a></span> | <span class="t">we're running on a, a 10 and that does not support at FP eight because it's an ampere GPU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4271" target="_blank">01:11:11.040</a></span> | <span class="t">which is one generation before FP eight support. Um, and then of course you, you pull in your,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4276" target="_blank">01:11:16.160</a></span> | <span class="t">uh, model and stuff. And then if we want to, you know, call the model to test that it is working,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4284" target="_blank">01:11:24.320</a></span> | <span class="t">um, we can come over here, um, to the call model. Um, we can just test this out really quick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4292" target="_blank">01:11:32.320</a></span> | <span class="t">Yes. Um, so we do not, um, on base 10, we have T fours, a tens, a one hundreds,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4302" target="_blank">01:11:42.480</a></span> | <span class="t">H one hundreds and H one hundred Migs and L fours as well. Um, we, we generally stick with the more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4308" target="_blank">01:11:48.000</a></span> | <span class="t">like data center type GPUs, um, rather than the consumer GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4312" target="_blank">01:11:52.240</a></span> | <span class="t">Yeah, uh, I want one for, uh, for, um, um, well, punctuation so I'm going to say that I want it for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4321" target="_blank">01:12:01.200</a></span> | <span class="t">legitimate business purposes and it should be an improved, approved expense. Uh, I don't want it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4326" target="_blank">01:12:06.160</a></span> | <span class="t">for playing video games. Definitely not. So yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4338" target="_blank">01:12:18.320</a></span> | <span class="t">No, but I bet he can. All of that, uh, all of the scores code is open source. And typically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4348" target="_blank">01:12:28.320</a></span> | <span class="t">when new models come up, those companies provide, uh, convert checkpoints scripts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4352" target="_blank">01:12:32.480</a></span> | <span class="t">Uh, but if you can follow those scripts, it's not terribly difficult. It's mostly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4357" target="_blank">01:12:37.040</a></span> | <span class="t">like, uh, if you're familiar with the transformers library, it's about reading weights from a hugging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4363" target="_blank">01:12:43.840</a></span> | <span class="t">force transformer model and converting that into something. Yeah, this is simple transformation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4369" target="_blank">01:12:49.440</a></span> | <span class="t">So it should be possible to do it yourself if you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4371" target="_blank">01:12:51.680</a></span> | <span class="t">Awesome. So once, once your model's deployed again, you can, you know, you can just test it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4381" target="_blank">01:13:01.760</a></span> | <span class="t">really quick. You can call it with a API end points. Um, but, uh, yeah, we're coming up on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4388" target="_blank">01:13:08.480</a></span> | <span class="t">on two 30 here. So I'm not going to spend too long on this example. Um, let's see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4394" target="_blank">01:13:14.960</a></span> | <span class="t">but you know, we've been talking a big game up here about performance, right? And performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4401" target="_blank">01:13:21.040</a></span> | <span class="t">is not just, okay, I'm testing it by myself. Performance is in production for my actual users.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4406" target="_blank">01:13:26.560</a></span> | <span class="t">Is this meeting my needs at a cost that is reasonable to me? And in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4411" target="_blank">01:13:31.040</a></span> | <span class="t">in order to, you know, validate your performance before you go to production,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4415" target="_blank">01:13:35.360</a></span> | <span class="t">you need to do benchmarking and you need to do a lot more rigorous benchmarking than just saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4420" target="_blank">01:13:40.080</a></span> | <span class="t">like, Hey, you know, I, I called it, it seemed pretty fast. Um, so what do you want to measure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4426" target="_blank">01:13:46.000</a></span> | <span class="t">when you're benchmarking? Uh, uh, you know, say it with me, everyone. It depends. That's what a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4431" target="_blank">01:13:51.440</a></span> | <span class="t">software engineers are always saying. So, um, you know, depending on your use case, you might have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4436" target="_blank">01:13:56.160</a></span> | <span class="t">different things that you're optimizing for. If you're say like a live chat service,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4440" target="_blank">01:14:00.480</a></span> | <span class="t">uh, you probably really care about time to first token for your streaming output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4444" target="_blank">01:14:04.800</a></span> | <span class="t">because you know, you're, you're trying to give people, you know, instantaneous responses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4448" target="_blank">01:14:08.960</a></span> | <span class="t">Um, you might also care a lot about tokens per second. Um, so that's, you know, how many,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4454" target="_blank">01:14:14.880</a></span> | <span class="t">how many tokens are generated generally some, some good numbers to keep in mind is somewhere,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4461" target="_blank">01:14:21.200</a></span> | <span class="t">depending on the tokenizer and the data and everything and the reader, somewhere between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4465" target="_blank">01:14:25.760</a></span> | <span class="t">30 to 50 tokens per second is going to be about as fast as anyone can read.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4471" target="_blank">01:14:31.120</a></span> | <span class="t">So, you know, if you're at 50 tokens per second, generally, it's going to feel pretty fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4475" target="_blank">01:14:35.520</a></span> | <span class="t">People aren't going to be waiting for your output. However, if you're doing something like code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4480" target="_blank">01:14:40.560</a></span> | <span class="t">you know, code takes more tokens per word than say natural language. So you're going to need even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4485" target="_blank">01:14:45.200</a></span> | <span class="t">more tokens per second for that, you know, nice, smooth output. And then from there getting into,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4489" target="_blank">01:14:49.360</a></span> | <span class="t">you know, 100, 200 tokens, that's when it just feels kind of, you know, magically fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4494" target="_blank">01:14:54.480</a></span> | <span class="t">But again, we'll, we'll, our inference is all about trade-offs, right? When we're optimizing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4499" target="_blank">01:14:59.040</a></span> | <span class="t">So, you know, sometimes you might want to trade off a, you know, a few, you know, maybe,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4504" target="_blank">01:15:04.640</a></span> | <span class="t">maybe you're going to go at a hundred, not 120 tokens per second, because that gets you a bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4508" target="_blank">01:15:08.640</a></span> | <span class="t">batch size, which is going to lower your cost per million tokens. Another thing you're going to want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4513" target="_blank">01:15:13.360</a></span> | <span class="t">to look at, um, when you're running your benchmarks is your total tokens per second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4517" target="_blank">01:15:17.040</a></span> | <span class="t">So there's the tokens per second per user, right? Like per request, how many tokens is your end user</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4522" target="_blank">01:15:22.880</a></span> | <span class="t">seeing? And then there's tokens per second in terms of how many tokens is your GPU actually producing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4528" target="_blank">01:15:28.080</a></span> | <span class="t">And that's a really important metric for throughput, for cost, um, especially if you're going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4533" target="_blank">01:15:33.120</a></span> | <span class="t">doing anything that's a little less than real time. Um, you want to look at this, not just once,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4538" target="_blank">01:15:38.800</a></span> | <span class="t">you want to look at the, uh, 50th, 90th, 95th, 99th percentile, make sure you're good with all those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4544" target="_blank">01:15:44.560</a></span> | <span class="t">And you want to look at the effects of different batch sizes on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4548" target="_blank">01:15:48.000</a></span> | <span class="t">And, um, so something is that benchmarking actually reveals really important information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4553" target="_blank">01:15:53.040</a></span> | <span class="t">It's not linear and it's not obvious. The sort of performance space of your model is not this nice,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4560" target="_blank">01:16:00.880</a></span> | <span class="t">nice flat piece of paper that goes linearly from batch size to batch size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4565" target="_blank">01:16:05.200</a></span> | <span class="t">So this is a graph of time to first token for like a menstrual model that I ran a long time ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4571" target="_blank">01:16:11.600</a></span> | <span class="t">I just happened to have a pretty graph of it. So that's how it ended up in the presentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4576" target="_blank">01:16:16.240</a></span> | <span class="t">Um, so if you look at the batch sizes as it's, uh, you know, increasing, doubling, um, 32 to 64,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4583" target="_blank">01:16:23.040</a></span> | <span class="t">the time to first token like barely budges. Um, but as it goes from 64 to 128, doubling again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4589" target="_blank">01:16:29.200</a></span> | <span class="t">the time to first token, uh, increases massively. And in this case, you know, the reason behind that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4594" target="_blank">01:16:34.560</a></span> | <span class="t">is we're, we're in the, you know, compute bound, um, pre-fill step. Um, when we're talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4600" target="_blank">01:16:40.320</a></span> | <span class="t">computing the first token and there's these different sort of slots that this computation could happen in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4605" target="_blank">01:16:45.600</a></span> | <span class="t">And as you increase the bat, increase the batch size, you're saturating these slots until eventually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4610" target="_blank">01:16:50.320</a></span> | <span class="t">you have an increased chance of a slot collision. And that's, what's going to rocket your time to first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4614" target="_blank">01:16:54.480</a></span> | <span class="t">token. I'm glad you're nodding. I'm glad I got that right. Um, but yeah, all of this to, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4619" target="_blank">01:16:59.520</a></span> | <span class="t">all of this to say, um, you know, the performance that you get out of your model once it's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4625" target="_blank">01:17:05.840</a></span> | <span class="t">built and deployed is not necessarily just going to be linear. It's not going to be something super</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4631" target="_blank">01:17:11.120</a></span> | <span class="t">predictable. You have to actually benchmark your deployment before you put it into production.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4636" target="_blank">01:17:16.720</a></span> | <span class="t">Otherwise these sort of surprises can, can happen quite often. Um, so yeah. So Pankage,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4642" target="_blank">01:17:22.560</a></span> | <span class="t">do you want to take over the, uh, the benchmarking script?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4644" target="_blank">01:17:24.880</a></span> | <span class="t">So, um, just for this, uh, workshop, we wrote a benchmarking script. It's not the script we use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4653" target="_blank">01:17:33.280</a></span> | <span class="t">ourselves, but it's a simpler version so that you can follow along that if you wanted to modify it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4659" target="_blank">01:17:39.440</a></span> | <span class="t">you can play around with it and understand it easily. Uh, if you go into that repository,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4664" target="_blank">01:17:44.560</a></span> | <span class="t">it's, uh, it's a very simple script where we send requests in parallel, just using Python, using async libraries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4671" target="_blank">01:17:51.840</a></span> | <span class="t">And, uh, all you give it is the URL of the endpoint of the model where your model is deployed and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4679" target="_blank">01:17:59.440</a></span> | <span class="t">can give it different concurrencies and input lands and output lands and give it number of runs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4684" target="_blank">01:18:04.320</a></span> | <span class="t">You want to run these benchmarks a number of times to get, uh, an idea of values one might be off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4689" target="_blank">01:18:09.200</a></span> | <span class="t">So I'm just going to run that script and it's all, it's all in the benchmark repository. Uh, it's all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4696" target="_blank">01:18:16.080</a></span> | <span class="t">structured using make files and there is a make file target for benchmark that we're going to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4701" target="_blank">01:18:21.040</a></span> | <span class="t">And, uh, I think the readme should also have instructions on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4706" target="_blank">01:18:26.160</a></span> | <span class="t">So we basically gonna run this and we're going to need the base URL. You need two things. We need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4716" target="_blank">01:18:36.160</a></span> | <span class="t">export the API key and then we need to, uh, supply the URL. So once you deploy your model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4723" target="_blank">01:18:43.040</a></span> | <span class="t">on base 10, you, you would see deployed. And like Philip said, there is a call model button. There are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4729" target="_blank">01:18:49.600</a></span> | <span class="t">various ways you can deploy it. Ultimately it's an HTTP API and you can just copy this URL for that model for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4736" target="_blank">01:18:56.800</a></span> | <span class="t">our benchmarking script. But if you want to play around the examples in all kinds of, uh, languages and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4743" target="_blank">01:19:03.680</a></span> | <span class="t">you can also click streaming. So it'll give you a streaming code. Streaming is very important with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4747" target="_blank">01:19:07.440</a></span> | <span class="t">large language models because you want to see the output as soon as possible. So we're going to take this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4752" target="_blank">01:19:12.800</a></span> | <span class="t">output and, uh, I don't know if I exported the API key. So give me one second to export the API key.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4762" target="_blank">01:19:22.000</a></span> | <span class="t">Come on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4767" target="_blank">01:19:27.280</a></span> | <span class="t">This is a good time to mention. Oh, yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4773" target="_blank">01:19:33.520</a></span> | <span class="t">This is a good time to mention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4781" target="_blank">01:19:41.040</a></span> | <span class="t">This is a good time to mention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4786" target="_blank">01:19:46.800</a></span> | <span class="t">I think it's a good time to mention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4788" target="_blank">01:19:48.800</a></span> | <span class="t">I think it's a good time to mention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4790" target="_blank">01:19:50.720</a></span> | <span class="t">I think it's a good time to mention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4794" target="_blank">01:19:54.800</a></span> | <span class="t">I think it's a good time to mention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4797" target="_blank">01:19:57.040</a></span> | <span class="t">It's a good time to mention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4799" target="_blank">01:19:59.040</a></span> | <span class="t">You know, if you lose the API key, you can always revoke it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4806" target="_blank">01:20:06.160</a></span> | <span class="t">So that's good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4807" target="_blank">01:20:07.680</a></span> | <span class="t">Yes, uh, there's a good time to mention that base 10 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4812" target="_blank">01:20:12.080</a></span> | <span class="t">2, type 2, so the client, uh, that is why we cannot show you other API keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4822" target="_blank">01:20:22.320</a></span> | <span class="t">So now we're just going to give it the URL here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4825" target="_blank">01:20:25.600</a></span> | <span class="t">Let me go back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4826" target="_blank">01:20:26.720</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4835" target="_blank">01:20:35.600</a></span> | <span class="t">So I'm going to do this, uh, first run with a concurrency of 32 and input and output plans of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4843" target="_blank">01:20:43.280</a></span> | <span class="t">1000 and uh, let's see it work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4847" target="_blank">01:20:47.440</a></span> | <span class="t">So first it does a warmup run just to make sure that there is some traffic on the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4853" target="_blank">01:20:53.920</a></span> | <span class="t">You always want to have a warmup run before you get the real numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4857" target="_blank">01:20:57.280</a></span> | <span class="t">Now, as this is running and you can see the TPS here, the total TPS is 5000 and this is on 8NG.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4866" target="_blank">01:21:06.240</a></span> | <span class="t">8NG is not the most powerful, uh, GPU, but this TPS is still very, very high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4872" target="_blank">01:21:12.400</a></span> | <span class="t">5000 is very, very high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4874" target="_blank">01:21:14.080</a></span> | <span class="t">It's because this is tiny Lama model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4876" target="_blank">01:21:16.000</a></span> | <span class="t">Tiny Lama is a tiny model, just like a billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4879" target="_blank">01:21:19.600</a></span> | <span class="t">And that's why we see this very high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4881" target="_blank">01:21:21.280</a></span> | <span class="t">But yeah, on bigger GPUs with the, with Lama 8G, you should also see very, very high values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4888" target="_blank">01:21:28.320</a></span> | <span class="t">because H100s are very, very powerful and TLS is very, very optimized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4892" target="_blank">01:21:32.800</a></span> | <span class="t">I think we see up to like 11,000 tokens per second and you should do a comparison.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4896" target="_blank">01:21:36.640</a></span> | <span class="t">It's really, really high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4897" target="_blank">01:21:37.680</a></span> | <span class="t">In this case, we have two runs and you see these values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4902" target="_blank">01:21:42.960</a></span> | <span class="t">Uh, let me try a different run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4904" target="_blank">01:21:44.400</a></span> | <span class="t">Now I'm going to do concurrency of one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4906" target="_blank">01:21:46.640</a></span> | <span class="t">And one is good to know how best of a time to first token you can get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4911" target="_blank">01:21:51.040</a></span> | <span class="t">So you're just sending one request at a time and many requests, but one at a time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4915" target="_blank">01:21:55.520</a></span> | <span class="t">with the same input and output length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4917" target="_blank">01:21:57.360</a></span> | <span class="t">And, uh, this should run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4919" target="_blank">01:21:59.040</a></span> | <span class="t">So, uh, you see time to first token of, uh, 180 milliseconds here, and this is from this laptop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4934" target="_blank">01:22:14.160</a></span> | <span class="t">I'm running it right from this laptop on this wifi and my model is deployed somewhere in US central.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4940" target="_blank">01:22:20.480</a></span> | <span class="t">And this is, uh, 180 milliseconds for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4943" target="_blank">01:22:23.040</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4944" target="_blank">01:22:24.640</a></span> | <span class="t">So, so the, so the vast majority of that time to first token is going to be network latency, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4949" target="_blank">01:22:29.280</a></span> | <span class="t">Not model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4950" target="_blank">01:22:30.400</a></span> | <span class="t">Not the model itself should be pretty fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4952" target="_blank">01:22:32.320</a></span> | <span class="t">Uh, yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4953" target="_blank">01:22:33.200</a></span> | <span class="t">Uh, uh, uh, not, not that one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4961" target="_blank">01:22:41.920</a></span> | <span class="t">Um, this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4965" target="_blank">01:22:45.280</a></span> | <span class="t">Uh, this is from this, uh, script, uh, script that I'm running.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4976" target="_blank">01:22:56.960</a></span> | <span class="t">I didn't do a thorough job of cleaning up everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4979" target="_blank">01:22:59.680</a></span> | <span class="t">We're saying that, uh, we just making an RPC call.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4981" target="_blank">01:23:01.920</a></span> | <span class="t">We don't need PyTorch or whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4983" target="_blank">01:23:03.440</a></span> | <span class="t">This, this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4984" target="_blank">01:23:04.880</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4987" target="_blank">01:23:07.040</a></span> | <span class="t">Local runtime, something, uh, probably machine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4989" target="_blank">01:23:09.680</a></span> | <span class="t">This script, uh, if you look at that, all it's doing is RPC.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4994" target="_blank">01:23:14.080</a></span> | <span class="t">Uh, let me go through that real quick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4995" target="_blank">01:23:15.760</a></span> | <span class="t">Benchmark script.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=4997" target="_blank">01:23:17.920</a></span> | <span class="t">Uh, it's just a simple Python using async and it's amazing how good Python has got.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5002" target="_blank">01:23:22.960</a></span> | <span class="t">With this async API, you're able to load, uh, load this model with thousands of tokens per second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5008" target="_blank">01:23:28.640</a></span> | <span class="t">All of them coming in streaming, uh, Python has actually gotten really, really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5013" target="_blank">01:23:33.280</a></span> | <span class="t">There was a, there was a case where I was loading with K6 and K6 client became a bottleneck because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5018" target="_blank">01:23:38.560</a></span> | <span class="t">Edge 100s are so fast, but Python could keep up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5021" target="_blank">01:23:41.280</a></span> | <span class="t">Uh, Python was able to load it very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5023" target="_blank">01:23:43.600</a></span> | <span class="t">Uh, but I would, I'm getting distracted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5026" target="_blank">01:23:46.560</a></span> | <span class="t">So, uh, we tried concurrency one, which is like the best case scenario.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5030" target="_blank">01:23:50.640</a></span> | <span class="t">Latencies are very, very good and TTFT should be very low.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5033" target="_blank">01:23:53.840</a></span> | <span class="t">Now let's go to the other extreme.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5035" target="_blank">01:23:55.840</a></span> | <span class="t">If you look at this model that we deployed, uh, we created it with the batch size of 64 maximum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5042" target="_blank">01:24:02.880</a></span> | <span class="t">So now we'll do 64 and, uh, I'm hoping we see throughput improvements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5048" target="_blank">01:24:08.400</a></span> | <span class="t">So ignore the warm up run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5052" target="_blank">01:24:12.160</a></span> | <span class="t">And this is going to take a bit longer because now we're going to send</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5057" target="_blank">01:24:17.280</a></span> | <span class="t">64 requests at the same time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5059" target="_blank">01:24:19.600</a></span> | <span class="t">64 is not, uh, not that high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5062" target="_blank">01:24:22.080</a></span> | <span class="t">We can go even higher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5063" target="_blank">01:24:23.200</a></span> | <span class="t">So in this case, you see total TPS of, uh, of 7,000, which is, uh, even higher than before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5071" target="_blank">01:24:31.520</a></span> | <span class="t">We saw 5,000 before this goes up to 7,000, but maybe this is a fluke.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5074" target="_blank">01:24:34.800</a></span> | <span class="t">Let's wait for the second run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5076" target="_blank">01:24:36.000</a></span> | <span class="t">And you can run, uh, this is all 7,000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5078" target="_blank">01:24:38.560</a></span> | <span class="t">So this is, uh, this is much better than what we saw before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5082" target="_blank">01:24:42.720</a></span> | <span class="t">So, uh, if you increase batch size, you would find that your latencies become, become higher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5088" target="_blank">01:24:48.800</a></span> | <span class="t">Latencies in the sense that for every request that a user is sending,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5092" target="_blank">01:24:52.480</a></span> | <span class="t">now tokens are coming slower and slower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5094" target="_blank">01:24:54.800</a></span> | <span class="t">And then you have to make a trade off at some point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5096" target="_blank">01:24:56.880</a></span> | <span class="t">Like, is it still good enough?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5098" target="_blank">01:24:58.480</a></span> | <span class="t">Is it still more than say 30 or 50 tokens per second that users won't perceive?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5102" target="_blank">01:25:02.720</a></span> | <span class="t">And at some point it will become unusable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5105" target="_blank">01:25:05.440</a></span> | <span class="t">And as you increase batch size, the pressure on your GPU memory also increases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5111" target="_blank">01:25:11.200</a></span> | <span class="t">Because all these extra batches, they require KV cache to be kept in GPU memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5116" target="_blank">01:25:16.800</a></span> | <span class="t">So you might hit that bottleneck.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5118" target="_blank">01:25:18.560</a></span> | <span class="t">So depending on all these scenarios, you want to experiment with different batch sizes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5122" target="_blank">01:25:22.560</a></span> | <span class="t">and find the sweet spot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5124" target="_blank">01:25:24.160</a></span> | <span class="t">And that's, uh, that takes a bit of time, but it's not terribly complex.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5128" target="_blank">01:25:28.080</a></span> | <span class="t">Um, yeah, so this script is there, there for you to modify and play around with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5135" target="_blank">01:25:35.200</a></span> | <span class="t">It's pretty simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5136" target="_blank">01:25:36.080</a></span> | <span class="t">It's pretty much a single, uh, Python file, not much in there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5140" target="_blank">01:25:40.080</a></span> | <span class="t">You can modify it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5140" target="_blank">01:25:40.960</a></span> | <span class="t">You can run it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5141" target="_blank">01:25:41.520</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5144" target="_blank">01:25:44.800</a></span> | <span class="t">The question was, uh, did I have a max batch size in mind when I was running?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5162" target="_blank">01:26:02.560</a></span> | <span class="t">Yes, because I deployed the model with the config in this, uh, workshop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5167" target="_blank">01:26:07.840</a></span> | <span class="t">Let me show you that I built the model with the max batch size and you can increase that batch size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5173" target="_blank">01:26:13.600</a></span> | <span class="t">So let me show you that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5174" target="_blank">01:26:14.880</a></span> | <span class="t">So in this, uh, tiny Lama model that I deployed, I specified a max batch size of 64.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5186" target="_blank">01:26:26.320</a></span> | <span class="t">So, uh, if I go beyond 64, it's not going to help me because all those requests will just wait.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5192" target="_blank">01:26:32.080</a></span> | <span class="t">And yeah, actually there's one, one interesting thing I want to show you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5195" target="_blank">01:26:35.280</a></span> | <span class="t">Uh, so this is a good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5197" target="_blank">01:26:37.040</a></span> | <span class="t">You can look at the logs here and in the logs, we put these, uh, metrics for what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5205" target="_blank">01:26:45.920</a></span> | <span class="t">I think maybe I'm not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5207" target="_blank">01:26:47.360</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5207" target="_blank">01:26:47.680</a></span> | <span class="t">And, and if you wanted to increase your batch size past 64, you just change the YAML and say like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5213" target="_blank">01:26:53.680</a></span> | <span class="t">Oh, match this batch size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5214" target="_blank">01:26:54.960</a></span> | <span class="t">So that should be like 128 and build a new engine by deploying it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5218" target="_blank">01:26:58.560</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5219" target="_blank">01:26:59.440</a></span> | <span class="t">So if you look at these logs here, it shows how many requests are running in parallel active</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5225" target="_blank">01:27:05.440</a></span> | <span class="t">request count.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5226" target="_blank">01:27:06.160</a></span> | <span class="t">Uh, you can actually observe how many requests are being executed in parallel right on the GPU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5232" target="_blank">01:27:12.640</a></span> | <span class="t">because there are chances that you haven't, uh, configured something right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5237" target="_blank">01:27:17.040</a></span> | <span class="t">And that, uh, for whatever reason, the, uh, requests are not all getting executed in parallel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5244" target="_blank">01:27:24.080</a></span> | <span class="t">For example, a common mistake one could make is that when you deploy on, on base 10 and this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5248" target="_blank">01:27:28.880</a></span> | <span class="t">based on specific, but just to take an example, there are scaling settings in base 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5253" target="_blank">01:27:33.520</a></span> | <span class="t">You can specify the scale and you can specify what is the max concurrency the model will receive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5258" target="_blank">01:27:38.800</a></span> | <span class="t">In this case, I've set it to a very, very high value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5261" target="_blank">01:27:41.280</a></span> | <span class="t">So it won't become a bottleneck, but there are chances that, that, you know, you forget,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5265" target="_blank">01:27:45.360</a></span> | <span class="t">you make a mistake there, you, you can check these logs and they will actually tell you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5269" target="_blank">01:27:49.520</a></span> | <span class="t">what's happening on the GPU. And I think I lost that again. Uh, let me go here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5276" target="_blank">01:27:56.320</a></span> | <span class="t">So yeah, these are actual metrics from the tensor RT LLM batch manager,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5282" target="_blank">01:28:02.800</a></span> | <span class="t">which tells you what's going on. It also tells you about the KV cache blocks that are being used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5287" target="_blank">01:28:07.520</a></span> | <span class="t">and that helps tune you, uh, helps you tune the KV cache size. For example, in this case, it says</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5293" target="_blank">01:28:13.680</a></span> | <span class="t">it's using 832 KV cache blocks and 4,500 are empty, which means there is the, there is this way more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5300" target="_blank">01:28:20.480</a></span> | <span class="t">KV cache than is needed for this use case. So just to mention that as a, as an aside.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5306" target="_blank">01:28:26.240</a></span> | <span class="t">Yeah, I think that's it for that presentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5314" target="_blank">01:28:34.000</a></span> | <span class="t">I'm going to, I'm going to, I'm going to, I'm going to talk about that next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5316" target="_blank">01:28:36.080</a></span> | <span class="t">Thank you. You're reading my mind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5318" target="_blank">01:28:38.720</a></span> | <span class="t">It does. Yes, tensor RT LLM does come with a benchmarking tool. It's very, very good. Uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5337" target="_blank">01:28:57.280</a></span> | <span class="t">the only downside is that you have to build it from source. It's not bundled with the tensor RT LLM,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5343" target="_blank">01:29:03.840</a></span> | <span class="t">Python library, which is my gripe. I'm going to ask anybody to fix that. Uh, they, they have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5349" target="_blank">01:29:09.520</a></span> | <span class="t">benchmarking tools. There are two benchmarking tools. One that, uh, that just sends a single batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5355" target="_blank">01:29:15.040</a></span> | <span class="t">and, um, and measures the raw throughput you can get without serving it through in-flight batching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5362" target="_blank">01:29:22.240</a></span> | <span class="t">And there is a separate second, second tool, uh, called the GPD manager benchmark,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5366" target="_blank">01:29:26.640</a></span> | <span class="t">which actually starts up a server and does in-flight batching on top. So there are two tools and they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5371" target="_blank">01:29:31.440</a></span> | <span class="t">very, very good quality, but they're not available easily. Building tensor RT LLM is like with 96 CPUs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5377" target="_blank">01:29:37.200</a></span> | <span class="t">it takes us one and a half hours to build it. It's not for the weak of heart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5380" target="_blank">01:29:40.960</a></span> | <span class="t">Or for the short of workshop. So, um, we just, we just have a, we have a few minutes left. Um, so I want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5390" target="_blank">01:29:50.640</a></span> | <span class="t">run through a few slides and then leave time for last minute questions. So, um, I was asked, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5396" target="_blank">01:29:56.640</a></span> | <span class="t">you know, how do we, how do we actually run this in production? What does the auto scaling look like?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5400" target="_blank">01:30:00.560</a></span> | <span class="t">How does that all work? So, um, how do you run a tensor RT engine? So you use something called Triton,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5407" target="_blank">01:30:07.360</a></span> | <span class="t">the Triton inference server. Um, and that's what helps you, you know, take the engine and actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5413" target="_blank">01:30:13.120</a></span> | <span class="t">serve requests to it. We're actually working on our own server, um, that uses the same spec, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5418" target="_blank">01:30:18.640</a></span> | <span class="t">supports, uh, C plus plus tokenization, de-tokenization, custom features for, for even more performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5424" target="_blank">01:30:24.560</a></span> | <span class="t">Um, but the, you know, as we've talked about, the engine is specific to versions, GPUs, batch sizes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5433" target="_blank">01:30:33.360</a></span> | <span class="t">sequence links, all that sort of stuff. So, um, that causes some challenges when you're running it in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5438" target="_blank">01:30:38.080</a></span> | <span class="t">production. We've talked this whole time about vertical scale, right? Like how do I get more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5443" target="_blank">01:30:43.360</a></span> | <span class="t">scale off a single GPU? There's also horizontal scale. How do I just like get more GPUs? How do I,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5448" target="_blank">01:30:48.960</a></span> | <span class="t">you know, auto automatically scale my, my platform up to meet my traffic demands? So, um, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5455" target="_blank">01:30:55.520</a></span> | <span class="t">some challenges in scaling out in general, you know, you have to automatically respond to traffic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5461" target="_blank">01:31:01.280</a></span> | <span class="t">You have to manage your cold start times. You have to manage the availability and reliability of your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5466" target="_blank">01:31:06.480</a></span> | <span class="t">GPU nodes. You have to route requests, do batching, all that kind of stuff. And then TensorRT LLM adds a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5472" target="_blank">01:31:12.480</a></span> | <span class="t">few more challenges. Um, you've got these large image sizes. Um, so that's going to make your cold</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5477" target="_blank">01:31:17.280</a></span> | <span class="t">starts even slower. You've got these specific batching requirements. So you can't just like send</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5481" target="_blank">01:31:21.760</a></span> | <span class="t">whatever traffic, however you want. And you have these specific GPU requirements. So when you spin</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5486" target="_blank">01:31:26.880</a></span> | <span class="t">up a new node, it's got to be exactly the same as your old node or your model's not going to work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5491" target="_blank">01:31:31.520</a></span> | <span class="t">Um, and, uh, you know, unfortunately our workshop is almost over. Otherwise I would love to give you an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5498" target="_blank">01:31:38.960</a></span> | <span class="t">in-depth answer of how to solve all these problems. Uh, but the quick answer to how to solve all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5503" target="_blank">01:31:43.680</a></span> | <span class="t">problems is you run your code on base 10, uh, because we solved it all for you. Um, so base 10 is a model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5509" target="_blank">01:31:49.120</a></span> | <span class="t">influence platform. Um, it's the company that we both work at. Um, with base 10, you can deploy models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5515" target="_blank">01:31:55.600</a></span> | <span class="t">on GPUs. Uh, you can use TensorRT LLM, but you don't have to, you can use any other service, VLM, TGI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5521" target="_blank">01:32:01.600</a></span> | <span class="t">just like a vanilla model deployment. Um, you get access to auto scaling, fast cold starts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5528" target="_blank">01:32:08.240</a></span> | <span class="t">um, scale to zero, tons of other great infrastructure features. You get access to all of our model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5533" target="_blank">01:32:13.680</a></span> | <span class="t">optimizations. We have a bunch of pre-packaged and pre-optimized models for you to work with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5537" target="_blank">01:32:17.920</a></span> | <span class="t">Um, so yeah, uh, the last thing before we go, um, is we are co-hosting a, um, happy hour tomorrow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5549" target="_blank">01:32:29.040</a></span> | <span class="t">on Shelby's rooftop bar. Um, I was not really involved in organizing it, but the sales guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5554" target="_blank">01:32:34.080</a></span> | <span class="t">who organized it told me that it's a super sweet spot and that we're going to have a great time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5558" target="_blank">01:32:38.800</a></span> | <span class="t">Um, so yeah, so I'm going to be there, um, and a bunch of other great, uh, AI engineers are going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5563" target="_blank">01:32:43.520</a></span> | <span class="t">to be there. So please feel free to sign up, come on through. We'd love to have you. Um, it's going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5568" target="_blank">01:32:48.320</a></span> | <span class="t">be super sweet, cool party for the cool kids. Um, well, you've been, you've been really great. Thank</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5577" target="_blank">01:32:57.360</a></span> | <span class="t">you for listening to us and we open it up for questions. Uh, I feel one question we didn't answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5581" target="_blank">01:33:01.920</a></span> | <span class="t">about auto scaling, right? Maybe I should take that on now. Yeah, go ahead. So how does auto scaling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5586" target="_blank">01:33:06.480</a></span> | <span class="t">work? Yeah, yeah, please. Uh, yeah, please feel free to hold. I'll just, uh, finish that question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5590" target="_blank">01:33:10.240</a></span> | <span class="t">because you asked that I wanted to answer it. And, uh, how auto scaling works is that we use a system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5595" target="_blank">01:33:15.600</a></span> | <span class="t">called Knative, but we forked it to make it work for machine learning use cases. Knative, if I understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5601" target="_blank">01:33:21.440</a></span> | <span class="t">correctly, was built for microservices where your requests are very, very quick, like, you know, one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5605" target="_blank">01:33:25.600</a></span> | <span class="t">second or two seconds. It doesn't exactly apply to machine learning model use cases, where your request is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5611" target="_blank">01:33:31.200</a></span> | <span class="t">long lasting and you're streaming and you need to, uh, still scale, but, uh, some of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5617" target="_blank">01:33:37.360</a></span> | <span class="t">considerations are different. So we had to actually fork it to be able to apply settings dynamically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5622" target="_blank">01:33:42.240</a></span> | <span class="t">For example, a lot of settings that you see on base 10, they apply immediately, like within a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5627" target="_blank">01:33:47.360</a></span> | <span class="t">Whereas in Knative, you would need to deploy a new revision for those settings to apply. And it's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5632" target="_blank">01:33:52.560</a></span> | <span class="t">it's not even practical because the way you deploy a new model, it creates so much hassle and requires</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5638" target="_blank">01:33:58.560</a></span> | <span class="t">extra capacity that it's not good. So we made changes to Knative to cater to the machine learning use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5644" target="_blank">01:34:04.000</a></span> | <span class="t">case. But fundamentally the idea is, is very simple. As a request come in, you specify the capacity of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5650" target="_blank">01:34:10.720</a></span> | <span class="t">your pod that it can take in. And if it's, uh, if it reaches near there, we spin up a new pod. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5657" target="_blank">01:34:17.360</a></span> | <span class="t">the traffic spreads. If your traffic goes down, it goes, your GPUs are, uh, your pods are reduced. Your GPUs are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5664" target="_blank">01:34:24.080</a></span> | <span class="t">freed up up all the way up to zero. And when the traffic arrives, it kept, it's kept in a queue and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5670" target="_blank">01:34:30.160</a></span> | <span class="t">then the model is spin up and the requests are, are sent there. A lot of the machinery at base 10 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5675" target="_blank">01:34:35.920</a></span> | <span class="t">around improving cold starts. How do we start up these, you know, giant models, 50 gig models in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5681" target="_blank">01:34:41.360</a></span> | <span class="t">under a minute, right? I mean, it sounds like a long time, but when you're talking about 50 gigs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5685" target="_blank">01:34:45.200</a></span> | <span class="t">50 gigs is also a lot. And for, for 10 gig models, we aim for less than 10 seconds. For 50 gig models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5691" target="_blank">01:34:51.200</a></span> | <span class="t">we aim for less than a minute because that's really important. Unless you can scale up in a minute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5696" target="_blank">01:34:56.080</a></span> | <span class="t">your request is going to time out. So you really can't scale to zero. So it's, uh, it seems like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5701" target="_blank">01:35:01.760</a></span> | <span class="t">detail, but it's very critical. You can't have a scale to zero without very, very fast cold starts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5706" target="_blank">01:35:06.000</a></span> | <span class="t">So we have the whole machinery built out for that. Even before LLMs became popular, we've had this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5712" target="_blank">01:35:12.160</a></span> | <span class="t">machinery in place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5719" target="_blank">01:35:19.120</a></span> | <span class="t">Uh, so what would I expect on terms of performance if I were, like, I think if you optimize it, yeah, can I?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5728" target="_blank">01:35:28.640</a></span> | <span class="t">Are you saying, like, deploy it with TensorRT versus deploy it on base 10?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5743" target="_blank">01:35:43.840</a></span> | <span class="t">So, so you, you would deploy it on base 10 using TensorRT under the hood to run it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5749" target="_blank">01:35:49.520</a></span> | <span class="t">Uh, so, so base 10 is just going to, like, facilitate that TensorRT deployment for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5754" target="_blank">01:35:54.240</a></span> | <span class="t">Yeah, no, no difference. Base 10 runs TensorRT LLM. We just make it very easy to run TensorRT LLM. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5768" target="_blank">01:36:08.480</a></span> | <span class="t">you, it's, uh, easier and faster for you to get at the optimum point. Uh, but if you could do it yourself,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5774" target="_blank">01:36:14.400</a></span> | <span class="t">yeah, it's, it's the same thing under the hood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5775" target="_blank">01:36:15.920</a></span> | <span class="t">Uh, and then, you know, I'm compelled by the fact that I'm in the marketing department to say things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5781" target="_blank">01:36:21.840</a></span> | <span class="t">like we also provide a lot of infrastructure value on top of that so that you're not managing your own,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5786" target="_blank">01:36:26.480</a></span> | <span class="t">you know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5787" target="_blank">01:36:27.280</a></span> | <span class="t">Yeah. Yeah. Actually, that is, that is true. Yeah. Because we, we have a large fleet. We, we get good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5792" target="_blank">01:36:32.080</a></span> | <span class="t">costs. So you actually won't pay higher on base 10. Like it's not that you're gonna, uh, it's gonna cost you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5797" target="_blank">01:36:37.360</a></span> | <span class="t">to more on base 10. Yes, over there. Yeah. I was wondering, um, like the open source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5808" target="_blank">01:36:48.240</a></span> | <span class="t">Yeah. So only the packaging part is open source. The serving parts, I mean, that, that's kind of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5835" target="_blank">01:37:15.600</a></span> | <span class="t">that's kind of the platform. So I do, I do want to mention is that, uh, from trust,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5841" target="_blank">01:37:21.520</a></span> | <span class="t">you can create a Docker image and that Docker image can be run anywhere. So you get pretty close.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5846" target="_blank">01:37:26.560</a></span> | <span class="t">You don't get auto scaling and, uh, all of those that nice dev loop, but you do get a Docker image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5852" target="_blank">01:37:32.240</a></span> | <span class="t">and you can do a lot with the Docker image. It builds it locally. Yeah. You can build,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5858" target="_blank">01:37:38.240</a></span> | <span class="t">there is a trust, uh, image build command. You find it to a trust. It will build the Docker image locally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5863" target="_blank">01:37:43.440</a></span> | <span class="t">Oh, okay. So I guess, uh, serving on a single pod or container, but there is also spreading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5873" target="_blank">01:37:53.280</a></span> | <span class="t">across multiple containers, the auto scaling and all of the dev loop. Uh, that is not, yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5878" target="_blank">01:37:58.960</a></span> | <span class="t">but serving. Yeah. I mean, single model serving is interest with that image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5882" target="_blank">01:38:02.480</a></span> | <span class="t">Yes. It's, uh, it's, uh, fast API at the trust server level. Then internally we have our own server</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5901" target="_blank">01:38:21.280</a></span> | <span class="t">layer that we wrote to interact with tensor RTLM. Uh, that part is not open source. We, it's very new.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5906" target="_blank">01:38:26.960</a></span> | <span class="t">So we still figuring out when to, or where to open source it, but there is also a version that uses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5911" target="_blank">01:38:31.920</a></span> | <span class="t">Triton. So there is the fast API, then there's Triton, and then there is the tensor RTLM library,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5917" target="_blank">01:38:37.520</a></span> | <span class="t">and then it runs the engine. Yeah, yeah, yeah, exactly. Exactly. We have, uh, we have, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5922" target="_blank">01:38:42.400</a></span> | <span class="t">we work on multiple cloud providers. We are spread across, uh, I don't want to say globe,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5926" target="_blank">01:38:46.880</a></span> | <span class="t">like mostly us, but also Australia and a few other places. So we have access to many different kinds of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5931" target="_blank">01:38:51.840</a></span> | <span class="t">hardware. We find the right hardware to build the engines and then we deploy it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5934" target="_blank">01:38:54.960</a></span> | <span class="t">Yes, you can. Yeah, you can use self-hosted clusters. Our stack is built that way. That's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5946" target="_blank">01:39:06.800</a></span> | <span class="t">one of our selling points. We're not giving you an API. You can run the entire stack in a self-hosted way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5952" target="_blank">01:39:12.240</a></span> | <span class="t">Awesome. Well, look, the conference organizers were very clear with us. All sessions and workshops are to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5958" target="_blank">01:39:18.960</a></span> | <span class="t">end on time. So I'm going to wrap it up here, but, um, we're going to be right outside if you have any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5964" target="_blank">01:39:24.480</a></span> | <span class="t">questions. If you have any easy questions, come see me. If you have any hard questions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5968" target="_blank">01:39:28.960</a></span> | <span class="t">please go talk to Punkage instead. Thank you all so much for being here. It was so much fun doing this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5973" target="_blank">01:39:33.440</a></span> | <span class="t">workshop with all of you. Again, I'm Phillip. This is Punkage. We're from Base 10. And thank you so much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5978" target="_blank">01:39:38.560</a></span> | <span class="t">for being here. Have a great conference, everyone. Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Lko9lTGD_9U&t=5996" target="_blank">01:39:56.320</a></span> | <span class="t">We'll see you next time.</span></div></div></body></html>