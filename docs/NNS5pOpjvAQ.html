<html><head><title>All You Need to Know on Multilingual Sentence Vectors (1 Model, 50+ Languages)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>All You Need to Know on Multilingual Sentence Vectors (1 Model, 50+ Languages)</h2><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ"><img src="https://i.ytimg.com/vi/NNS5pOpjvAQ/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=79">1:19</a> Multilingual Vectors<br><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=355">5:55</a> Multi-task Training (mUSE)<br><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=576">9:36</a> Multilingual Knowledge Distillation<br><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=673">11:13</a> Knowledge Distillation Training<br><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=823">13:43</a> Visual Walkthrough<br><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=893">14:53</a> Parallel Data Prep<br><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1223">20:23</a> Choosing a Student Model<br><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1495">24:55</a> Initializing the Models<br><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1805">30:5</a> ParallelSentencesDataset<br><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2034">33:54</a> Loss and Fine-tuning<br><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2219">36:59</a> Model Evaluation<br><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2363">39:23</a> Outro<br><br><div style="text-align: left;"><a href="./NNS5pOpjvAQ.html">Whisper Transcript</a> | <a href="./transcript_NNS5pOpjvAQ.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Today we're going to be having a look at multilingual sentence transformers. We're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=4" target="_blank">00:00:04.480</a></span> | <span class="t">going to look at how they work, how they're trained, and why they're so useful. We're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=11" target="_blank">00:00:11.200</a></span> | <span class="t">to be focusing on one specific training method, which I think is quite useful because all it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=19" target="_blank">00:00:19.280</a></span> | <span class="t">really needs is a reasonably small data set of parallel data, which is simply translation pairs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=28" target="_blank">00:00:28.800</a></span> | <span class="t">from a source language like English to whichever other language you're using. So obviously, if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=34" target="_blank">00:00:34.800</a></span> | <span class="t">are wanting to train a sentence transformer in a language that doesn't really have that much data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=42" target="_blank">00:00:42.640</a></span> | <span class="t">it's particularly sentence similarity data, this can be really useful for actually taking a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=50" target="_blank">00:00:50.400</a></span> | <span class="t">high-performing, for example, English sentence transformer and transferring that knowledge or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=57" target="_blank">00:00:57.680</a></span> | <span class="t">distilling that knowledge into a sentence transformer for your own language. So I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=65" target="_blank">00:01:05.440</a></span> | <span class="t">this will be pretty useful for a lot of you. And let's jump straight into it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=72" target="_blank">00:01:12.000</a></span> | <span class="t">Before we really get into the whole multilingual sentence transformer part of the video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=85" target="_blank">00:01:25.200</a></span> | <span class="t">I just want to give an impression of what these multilingual sentence transformers are actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=90" target="_blank">00:01:30.000</a></span> | <span class="t">doing. So on here, we can see a single English sentence or brief phrase down at the bottom,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=99" target="_blank">00:01:39.120</a></span> | <span class="t">"isle of plants," and the rest of these are all in Italian. So what we have here are vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=107" target="_blank">00:01:47.360</a></span> | <span class="t">representations or dense vector representations of these phrases. And a monolingual sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=115" target="_blank">00:01:55.680</a></span> | <span class="t">transformer, which is most of the sentence transformers, will only cope with one language. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=122" target="_blank">00:02:02.160</a></span> | <span class="t">we would hope that phrases that have a similar meaning end up within the same sort of vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=128" target="_blank">00:02:08.880</a></span> | <span class="t">space. So like we have for "amo lupiante" here and "I love plants," these are kind of in the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=140" target="_blank">00:02:20.240</a></span> | <span class="t">space. A monolingual sentence transformer would do that for similar sentences. So in English,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=149" target="_blank">00:02:29.120</a></span> | <span class="t">we might have "I love plants" and "I like plants," which is actually what we have up here. So this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=155" target="_blank">00:02:35.760</a></span> | <span class="t">here is Italian for "I like plants." And we would hope that they're in a similar area,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=161" target="_blank">00:02:41.040</a></span> | <span class="t">whereas irrelevant or almost contradictory sentences we would hope would be far off</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=169" target="_blank">00:02:49.920</a></span> | <span class="t">somewhere else, like our vector over here. So that's how, obviously, a monolingual sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=178" target="_blank">00:02:58.160</a></span> | <span class="t">transformer works, and it's exactly the same for a multilingual sentence transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=183" target="_blank">00:03:03.680</a></span> | <span class="t">The only difference is that rather than having a single language, it will comprehend multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=190" target="_blank">00:03:10.080</a></span> | <span class="t">languages. And that's what you can see in this visual. So in this example, I have "I love plants"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=198" target="_blank">00:03:18.240</a></span> | <span class="t">and "amo lupiante." They have the same meaning, just in different languages. So that means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=205" target="_blank">00:03:25.760</a></span> | <span class="t">they should be as close together as possible in this vector space. So here we're just visualizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=214" target="_blank">00:03:34.400</a></span> | <span class="t">three dimensions. In reality, it'd be a lot more. I think most transformer models go with 768</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=222" target="_blank">00:03:42.320</a></span> | <span class="t">dimensions. But obviously, we can't visualize that, so we have 3D here. So we want different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=229" target="_blank">00:03:49.600</a></span> | <span class="t">languages or similar sentences from different languages to end up in the same area. And we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=235" target="_blank">00:03:55.280</a></span> | <span class="t">also want to be able to represent relationships between different sentences that are similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=242" target="_blank">00:04:02.640</a></span> | <span class="t">And we can kind of see that relationship here. So we have "mi piacciono lupiante" and "amo lupiante"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=248" target="_blank">00:04:08.800</a></span> | <span class="t">and "I love plants" are all kind of in the same sort of area. "Mi piacciono lupiante," so "I like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=255" target="_blank">00:04:15.760</a></span> | <span class="t">plants," is obviously separated somewhat, but it's still within the same area. And then in the bottom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=264" target="_blank">00:04:24.320</a></span> | <span class="t">left down there, we have "ho un cane arancione," which means "I have a orange dog." So obviously,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=273" target="_blank">00:04:33.760</a></span> | <span class="t">you know, that's really nothing to do with "I love plants." Although I suppose you could say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=278" target="_blank">00:04:38.160</a></span> | <span class="t">you're talking about yourself, so maybe it's a little bit similar. But otherwise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=282" target="_blank">00:04:42.800</a></span> | <span class="t">they're completely different topics. So that's kind of what we want to build,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=291" target="_blank">00:04:51.280</a></span> | <span class="t">something that takes sentences from different languages and maps them into a vector space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=297" target="_blank">00:04:57.520</a></span> | <span class="t">which has some sort of numerical structure to represent the semantic meaning of those sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=304" target="_blank">00:05:04.720</a></span> | <span class="t">And it should be language agnostic. So obviously, we can't -- well, maybe we can train on every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=311" target="_blank">00:05:11.120</a></span> | <span class="t">language. I don't know any models that are trained on every single language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=314" target="_blank">00:05:14.400</a></span> | <span class="t">but we want it to be able to comprehend different languages and not be biased towards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=323" target="_blank">00:05:23.120</a></span> | <span class="t">different phrases in different languages, but just have a very balanced comprehension of all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=329" target="_blank">00:05:29.680</a></span> | <span class="t">Okay? So that's how the vectors should look. And then, okay, so what would the training data for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=342" target="_blank">00:05:42.640</a></span> | <span class="t">this look like, and what are the training approaches? So like I said before, there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=348" target="_blank">00:05:48.400</a></span> | <span class="t">two training approaches that I'm going to just briefly touch upon, but we're going to focus on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=352" target="_blank">00:05:52.640</a></span> | <span class="t">the latter of those. So the first one that I want to mention is what the MUSE, or Multilingual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=364" target="_blank">00:06:04.000</a></span> | <span class="t">Universal Sentence Encoder Model, was trained on, which is a multitask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=375" target="_blank">00:06:15.520</a></span> | <span class="t">translation bridging approach to training. So what I mean by that is it uses two or uses a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=384" target="_blank">00:06:24.640</a></span> | <span class="t">dual encoder structure, and those encoders deal with two different tasks. So on one end, you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=396" target="_blank">00:06:36.000</a></span> | <span class="t">the parallel data training. So when we say parallel data, these are sentence pairs in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=404" target="_blank">00:06:44.960</a></span> | <span class="t">different languages. So like we had before, we had the Amalopoeia and Isle of Plants,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=412" target="_blank">00:06:52.160</a></span> | <span class="t">which is just the Italian and English phrases for Isle of Plants. So we would have our source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=420" target="_blank">00:07:00.240</a></span> | <span class="t">language and also the translation, or the target language is probably a better way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=427" target="_blank">00:07:07.280</a></span> | <span class="t">but I'll put translation for now. So we have the source and translation. That's our parallel data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=432" target="_blank">00:07:12.720</a></span> | <span class="t">set. And what we're doing is optimizing to get those two vectors or the two sentence vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=438" target="_blank">00:07:18.560</a></span> | <span class="t">produced by either one of those sentences as close as possible. And then there is also the source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=447" target="_blank">00:07:27.520</a></span> | <span class="t">data. So we basically have sentence similarity or NLI data, but we have it just for the source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=457" target="_blank">00:07:37.840</a></span> | <span class="t">language. So we have source, sentence A, and source, sentence B. And we train on both of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=467" target="_blank">00:07:47.760</a></span> | <span class="t">Now, it works, and that's good. But obviously, we're training on a multi-task architecture here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=475" target="_blank">00:07:55.680</a></span> | <span class="t">and training on a single task in machine learning is already hard enough. Training on two and getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=483" target="_blank">00:08:03.040</a></span> | <span class="t">them to balance and train well is harder. And the amount of data, at least for Muse, and I believe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=492" target="_blank">00:08:12.000</a></span> | <span class="t">for if you're training using this approach, you're going to need to use a similar amount of data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=496" target="_blank">00:08:16.560</a></span> | <span class="t">is pretty significant. I think Muse is something like a billion pairs, so it's pretty high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=503" target="_blank">00:08:23.760</a></span> | <span class="t">And another thing is that we also need something called hard negatives in the training data in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=511" target="_blank">00:08:31.680</a></span> | <span class="t">order for this model to perform well. So what I mean by hard negative is, say we have our source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=520" target="_blank">00:08:40.560</a></span> | <span class="t">sentence A here, and we have this source B, which is like a similar sentence, a high similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=527" target="_blank">00:08:47.920</a></span> | <span class="t">sentence. They mean basically the same thing. We'd also have to add a source C. And this source C</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=539" target="_blank">00:08:59.200</a></span> | <span class="t">will have to be similar in the words I use to source A, but actually means something different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=545" target="_blank">00:09:05.040</a></span> | <span class="t">So it's harder for the model to differentiate between them. And the model would have to figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=550" target="_blank">00:09:10.640</a></span> | <span class="t">out these two sentences are not similar, even though they seem similar at first, but they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=556" target="_blank">00:09:16.320</a></span> | <span class="t">not. So it makes the training task harder for the model, which, of course, makes the model better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=563" target="_blank">00:09:23.760</a></span> | <span class="t">So that is training approach number one. And we've mentioned the parallel data there. That's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=571" target="_blank">00:09:31.840</a></span> | <span class="t">the data set we're going to be using for the second training approach. And that second training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=577" target="_blank">00:09:37.600</a></span> | <span class="t">approach is called multi-lingual knowledge distillation. So that is a mouthful. And it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=593" target="_blank">00:09:53.840</a></span> | <span class="t">takes me a while to write that. I'm sorry. So multi-lingual knowledge distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=601" target="_blank">00:10:01.520</a></span> | <span class="t">So this was introduced in 2020 by, who we've mentioned before, the Sentence Transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=608" target="_blank">00:10:08.720</a></span> | <span class="t">people, Nils Reimers and Irina Gruevich. And the sort of advantage of using this approach is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=617" target="_blank">00:10:17.600</a></span> | <span class="t">we only need the parallel data set. So we only need those translation pairs. And the amount of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=622" target="_blank">00:10:22.480</a></span> | <span class="t">training data you need is a lot smaller. And using this approach, the Sentence Transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=631" target="_blank">00:10:31.440</a></span> | <span class="t">people have actually trained Sentence Transformers that can use more than 50</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=637" target="_blank">00:10:37.920</a></span> | <span class="t">languages at once. And the performance is good. It's not just that they managed to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=643" target="_blank">00:10:43.920</a></span> | <span class="t">get a few phrases correct. The performance is actually quite good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=648" target="_blank">00:10:48.000</a></span> | <span class="t">So I think it's pretty impressive. And the training time for these is super quick,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=657" target="_blank">00:10:57.280</a></span> | <span class="t">as we'll see. And like I said, it's using just translation data, parallel data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=663" target="_blank">00:11:03.920</a></span> | <span class="t">which is reasonably easy to get for almost every language. So I think that's pretty useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=672" target="_blank">00:11:12.000</a></span> | <span class="t">Now, well, let's have a look at what that multi-lingual knowledge distillation training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=679" target="_blank">00:11:19.360</a></span> | <span class="t">process actually looks like. So it's what we have here. So same example as before. I've got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=685" target="_blank">00:11:25.120</a></span> | <span class="t">"I like plants" this time and "Mi piacere non li piante," which is, again, the same thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=690" target="_blank">00:11:30.960</a></span> | <span class="t">in Italian. Now, we have both of those. We have a teacher model and a student model. Now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=697" target="_blank">00:11:37.040</a></span> | <span class="t">when we say knowledge distillation, that means where you basically take one model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=704" target="_blank">00:11:44.080</a></span> | <span class="t">and you distill the knowledge from that one model into another model here. The model that already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=711" target="_blank">00:11:51.520</a></span> | <span class="t">knows some of the stuff that we want, that we want to distill knowledge from,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=715" target="_blank">00:11:55.920</a></span> | <span class="t">is called the teacher model. Now, the teacher model, in this case, is going to be a monolingual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=723" target="_blank">00:12:03.840</a></span> | <span class="t">model. So it's probably going to be a sentence transformer. That's very good at English tests</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=729" target="_blank">00:12:09.040</a></span> | <span class="t">only. And what we do is we take the student model, which is going to be-- it doesn't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=737" target="_blank">00:12:17.120</a></span> | <span class="t">to be a sentence transformer. It's just a pre-trained transform model. We'll be using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=742" target="_blank">00:12:22.720</a></span> | <span class="t">XLM Roberta later on. And it needs to be capable of understanding multiple languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=750" target="_blank">00:12:30.000</a></span> | <span class="t">So in this case, we feed the English sentence into both our teacher model and student model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=757" target="_blank">00:12:37.360</a></span> | <span class="t">And then we optimize the student model to reduce the difference between the two vectors output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=764" target="_blank">00:12:44.640</a></span> | <span class="t">by those two models. And that makes the student model almost mimic the monolingual aspect of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=771" target="_blank">00:12:51.600</a></span> | <span class="t">teacher model. But then we take it a little further, and we process the Italian, or the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=777" target="_blank">00:12:57.360</a></span> | <span class="t">target language, through the student model. And then we do the same thing. So we try to reduce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=783" target="_blank">00:13:03.680</a></span> | <span class="t">the difference between the Italian vector and the teacher's English vector. And what we're doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=789" target="_blank">00:13:09.440</a></span> | <span class="t">there is making the student model mimic the teacher for a different language. So through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=796" target="_blank">00:13:16.400</a></span> | <span class="t">that process, you can add more and more languages to a student model, which mimics your teacher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=803" target="_blank">00:13:23.920</a></span> | <span class="t">model. I mean, it seems at least really simple just to think of it like that, in my opinion,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=814" target="_blank">00:13:34.320</a></span> | <span class="t">anyway. But it works really well. So it's a very cool technique, in my opinion. I do like it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=822" target="_blank">00:13:42.880</a></span> | <span class="t">So just a more visual way of going through that. We have these different circles. They represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=831" target="_blank">00:13:51.600</a></span> | <span class="t">different language tasks, or different languages, but pretty similar, or the same task in each one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=837" target="_blank">00:13:57.440</a></span> | <span class="t">of those. We have our monolingual teacher model. And that can perform on one of these languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=844" target="_blank">00:14:04.240</a></span> | <span class="t">But fails on the others. We take that monolingual model, or our teacher model, and then we also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=850" target="_blank">00:14:10.800</a></span> | <span class="t">take a pre-trained multilingual model. So the important thing here is that it can handle new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=856" target="_blank">00:14:16.080</a></span> | <span class="t">languages, like I said with XLM and Roberta. This is our student. We perform multilingual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=862" target="_blank">00:14:22.640</a></span> | <span class="t">knowledge distillation, meaning the student learns how the teacher performs well on the single task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=868" target="_blank">00:14:28.000</a></span> | <span class="t">by mimicking its sentence vector outputs. The student then performs this mimicry across multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=874" target="_blank">00:14:34.240</a></span> | <span class="t">languages. And then hopefully, the student model can now perform across all of the languages that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=882" target="_blank">00:14:42.240</a></span> | <span class="t">we are wanting to train on. That's how the multilingual knowledge distillation works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=889" target="_blank">00:14:49.280</a></span> | <span class="t">Let's have a look at that in code. Okay, so we're in our code here. And the first thing I'm going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=895" target="_blank">00:14:55.920</a></span> | <span class="t">to do is actually get our data. So in the paper that introduced the multilingual knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=904" target="_blank">00:15:04.320</a></span> | <span class="t">distillation, Rimas and Gurevich use the focus partly on this TED subtitles data. So yeah, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=914" target="_blank">00:15:14.960</a></span> | <span class="t">know TED Talks, they're just low talks where people present on a particular topic, usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=920" target="_blank">00:15:20.640</a></span> | <span class="t">pretty interesting. And those TED Talks have subtitles in loads of different languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=928" target="_blank">00:15:28.720</a></span> | <span class="t">So they scraped that subtitle data and use that as sentence pairs for the different languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=937" target="_blank">00:15:37.200</a></span> | <span class="t">Okay, so that's the parallel data. Now, what I'm going to do is use Hug and Face Transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=943" target="_blank">00:15:43.680</a></span> | <span class="t">to download that. So we just import datasets here. So I said Hug and Face Transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=950" target="_blank">00:15:50.640</a></span> | <span class="t">actually Hug and Face Datasets here. So import datasets, and I'm going to load that dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=957" target="_blank">00:15:57.440</a></span> | <span class="t">And just have a look at what the structure of that dataset is. So it's the TED multi,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=964" target="_blank">00:16:04.400</a></span> | <span class="t">and I'm just getting the training data here. You see in here, we have this Features,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=968" target="_blank">00:16:08.480</a></span> | <span class="t">Translations, and Talk Name. Now, it's not really very clear, but inside the translations data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=976" target="_blank">00:16:16.560</a></span> | <span class="t">we have the language tag. So these are language codes, ISO language codes. If you type that into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=983" target="_blank">00:16:23.440</a></span> | <span class="t">Google, they'll pop up. If you don't know which one, which are which. And below, we also have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=993" target="_blank">00:16:33.200</a></span> | <span class="t">in here, it's not very clear again. So if I come here, we have translations, and each one of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=998" target="_blank">00:16:38.400</a></span> | <span class="t">corresponds to the language code up here. Okay, so if we came here, we see EN, it's English,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1005" target="_blank">00:16:45.200</a></span> | <span class="t">and we find it here. Okay, and then we also have Talk Name. It's not really important for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1013" target="_blank">00:16:53.520</a></span> | <span class="t">So we can get the index of our English text, because we need to extract that for our source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1021" target="_blank">00:17:01.440</a></span> | <span class="t">language. So we extract that, we get number four, so we're going into those language pairs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1027" target="_blank">00:17:07.200</a></span> | <span class="t">finding EN. And then we use that index to get the corresponding translation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1032" target="_blank">00:17:12.320</a></span> | <span class="t">which is here. And then we'd use that to create all of our pairs. Now, here, I've just created</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1039" target="_blank">00:17:19.840</a></span> | <span class="t">loads of pairs. This is the first one, so this is English to Arabic. But if we have a look,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1045" target="_blank">00:17:25.520</a></span> | <span class="t">there's actually loads of pairs here. So we have 27 in total, which is obviously quite a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1050" target="_blank">00:17:30.000</a></span> | <span class="t">Probably not going to use all of those. I mean, you could do if you want to. It depends on what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1054" target="_blank">00:17:34.320</a></span> | <span class="t">you're trying to build. But I think most of us are probably not going to be trying to build some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1059" target="_blank">00:17:39.200</a></span> | <span class="t">model that crosses all these different languages. So what I'm going to do is just initialize a list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1067" target="_blank">00:17:47.120</a></span> | <span class="t">of languages that we would like to train on. So we're going to be feeding all of this into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1076" target="_blank">00:17:56.960</a></span> | <span class="t">a sentence transformer class called ParallelSentencesDataSet. And that requires that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1084" target="_blank">00:18:04.400</a></span> | <span class="t">we, one, separate out our pairs using a tab character, and two, keep all those pairs separated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1092" target="_blank">00:18:12.160</a></span> | <span class="t">in different gzip files. So that's why I'm using this particular structure. So data preprocessing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1100" target="_blank">00:18:20.400</a></span> | <span class="t">steps here, I'm just running through them quickly because I want to focus more on the actual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1104" target="_blank">00:18:24.560</a></span> | <span class="t">sentence transformer training part. So run that, and we can-- well, it's actually going to take a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1111" target="_blank">00:18:31.040</a></span> | <span class="t">moment, so let me skip forward. And then we'll see how many pairs-- well, I just want to see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1118" target="_blank">00:18:38.880</a></span> | <span class="t">We don't have to do this. But I want to see how many pairs we have for each language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1122" target="_blank">00:18:42.640</a></span> | <span class="t">And you see here, we have about 200,000 for each of them. The German one is slightly less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1130" target="_blank">00:18:50.720</a></span> | <span class="t">And then let's have a look at what those source and translations look like. So here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1135" target="_blank">00:18:55.360</a></span> | <span class="t">we have applause and applause. Now, I think that's Italian. It seems so. But here, we can see, OK,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1144" target="_blank">00:19:04.960</a></span> | <span class="t">the end of the talk ends in applause. So obviously, the subtitles say applause. Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1151" target="_blank">00:19:11.520</a></span> | <span class="t">hopefully, it ends in applause. And then we just have the tab character, and that separates the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1157" target="_blank">00:19:17.520</a></span> | <span class="t">source language, English in this case, from the translated language. Now what we want to do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1166" target="_blank">00:19:26.960</a></span> | <span class="t">save that data. So we sort all that in these dictionaries. So initialize dictionary here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1173" target="_blank">00:19:33.280</a></span> | <span class="t">and access them here. So we have ENIT, ES, AR, FR, and DE. And now I'm just going to save them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1182" target="_blank">00:19:42.240</a></span> | <span class="t">So run this. That will save. And what I'll do is just write OSLister. So we can see what is in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1192" target="_blank">00:19:52.080</a></span> | <span class="t">there. Where is it? It's data. Just data. Is that right? OK. And then we have these five files. OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1203" target="_blank">00:20:03.920</a></span> | <span class="t">Now let's continue. So now what we want to do is, OK, we have-- that's our training data. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1212" target="_blank">00:20:12.000</a></span> | <span class="t">ready, or mostly ready, before we feed it into the Sentence Transformer's parallel sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1219" target="_blank">00:20:19.600</a></span> | <span class="t">data set object later on. So OK, let's leave that for now and move on to the next step, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1226" target="_blank">00:20:26.320</a></span> | <span class="t">choosing our teacher and student models. So I already mentioned before, we want our student</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1233" target="_blank">00:20:33.920</a></span> | <span class="t">model to be capable of multilingual comprehension. So what I mean by that-- or not just what I mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1242" target="_blank">00:20:42.320</a></span> | <span class="t">but one big component of that is, can the Transformer Tokenizer deal with different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1248" target="_blank">00:20:48.080</a></span> | <span class="t">languages? In some cases, they really can't. So let me show you what the BERT Tokenizer does with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1255" target="_blank">00:20:55.920</a></span> | <span class="t">these four different sentences. So we'll just loop through each one. So four texts in sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1263" target="_blank">00:21:03.200</a></span> | <span class="t">And what I'm going to do is just print. I'm going to print the output of the BERT Tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1269" target="_blank">00:21:09.600</a></span> | <span class="t">And if I tokenize that text, what does it give me? OK. So what we have here-- OK, English,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1280" target="_blank">00:21:20.080</a></span> | <span class="t">of course. BERT is fine. The Tokenizer, or the vocabulary of the Tokenizer of BERT is, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1287" target="_blank">00:21:27.920</a></span> | <span class="t">roughly 30,000 tokens. And most of those are English-based. You can see here that it has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1298" target="_blank">00:21:38.400</a></span> | <span class="t">picked up some Chinese characters, because it does-- other languages do feed into it a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1302" target="_blank">00:21:42.880</a></span> | <span class="t">bit, because it's just-- all the data is pulled from the internet. Other bits do get in there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1309" target="_blank">00:21:49.200</a></span> | <span class="t">But it's mostly English. So that's why we see, OK, we have these unknown characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1314" target="_blank">00:21:54.720</a></span> | <span class="t">Now, as soon as we have an unknown character in our sentence, the Tokenizer-- or no, sorry,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1320" target="_blank">00:22:00.960</a></span> | <span class="t">the Transformer is ready to struggle to understand what is in that position? What is that unknown</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1327" target="_blank">00:22:07.920</a></span> | <span class="t">token supposed to represent? In the case of-- I think of it as it's like when you're a kid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1336" target="_blank">00:22:16.400</a></span> | <span class="t">in school, and they had those-- had a paragraph, and you had to fill in the blanks. So you had a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1343" target="_blank">00:22:23.600</a></span> | <span class="t">paragraph, and occasionally, in a couple of sentences, there'll be a couple of blank lines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1348" target="_blank">00:22:28.880</a></span> | <span class="t">where you need to guess what the correct word should be. If you only have a couple of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1354" target="_blank">00:22:34.160</a></span> | <span class="t">blanks, as a person, you can probably guess accurately. And the same for BERT. BERT can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1361" target="_blank">00:22:41.760</a></span> | <span class="t">probably guess accurately what the occasional unknown token is. But if in school, they gave</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1370" target="_blank">00:22:50.320</a></span> | <span class="t">you a sheet, and they said, OK, fill out these blanks, and it was literally just a paragraph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1375" target="_blank">00:22:55.680</a></span> | <span class="t">of blank, and you had to guess it correctly, you probably-- I don't know. I think your chances are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1381" target="_blank">00:23:01.360</a></span> | <span class="t">pretty slim of getting that correct. So the same is true for BERT. BERT, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1389" target="_blank">00:23:09.440</a></span> | <span class="t">in our Georgian example down here, how can BERT know what that means? It will not know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1394" target="_blank">00:23:14.960</a></span> | <span class="t">So the tokenizer from BERT is not suitable for non-Latin character languages whatsoever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1403" target="_blank">00:23:23.360</a></span> | <span class="t">And then it does know some Greek characters here. And maybe it knows all of them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1408" target="_blank">00:23:28.000</a></span> | <span class="t">because I suppose Greek feeds into Latin languages a bit more than Georgian or Chinese.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1415" target="_blank">00:23:35.760</a></span> | <span class="t">But it doesn't know what to do with them. They're all single-character tokens. And the issue with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1420" target="_blank">00:23:40.560</a></span> | <span class="t">single-character tokens is that you can't really encode that much information into a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1426" target="_blank">00:23:46.240</a></span> | <span class="t">character. Because if you have 24 characters in your alphabet, that means you have 24 encodings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1434" target="_blank">00:23:54.160</a></span> | <span class="t">to represent your entire language, which is not going to happen. So that's also not good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1441" target="_blank">00:24:01.440</a></span> | <span class="t">So basically, don't use a BERT tokenizer. It's not a good idea. What you can do is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1448" target="_blank">00:24:08.560</a></span> | <span class="t">OK, how is this xlmr token or tokenizer? Now, xlmr is trained for multilingual comprehension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1460" target="_blank">00:24:20.160</a></span> | <span class="t">It uses a sentence piece transformer, which uses byte-level logic to split up the sentence or the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1467" target="_blank">00:24:27.360</a></span> | <span class="t">words. So it can deal with tokens it's never seen before, which is pretty nice. And the vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1475" target="_blank">00:24:35.840</a></span> | <span class="t">size for this is not 30k. I think it's 250k. It could be off a few k there, but it's around that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1484" target="_blank">00:24:44.000</a></span> | <span class="t">mark. And it's been trained on many languages. So it's obviously a much better option for our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1493" target="_blank">00:24:53.280</a></span> | <span class="t">student model. So let's have a look at how we initialize that. So this xlmr model is just coming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1501" target="_blank">00:25:01.520</a></span> | <span class="t">from Transformers. So I need to convert that model from just a Transformer model into an--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1510" target="_blank">00:25:10.320</a></span> | <span class="t">or initialize it as a Sentence Transformer model using the Sentence Transformers library.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1516" target="_blank">00:25:16.160</a></span> | <span class="t">So from Sentence Transformers, I'm going to import models and also Sentence Transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1523" target="_blank">00:25:23.680</a></span> | <span class="t">So xlmr, so this is going to be our actual Transformer model. We're going to write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1530" target="_blank">00:25:30.480</a></span> | <span class="t">models.transformer. And Sentence Transformers under hood uses HuggingFace Transformers as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1539" target="_blank">00:25:39.200</a></span> | <span class="t">So we would access this as the normal model identifier that we would with normal HuggingFace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1547" target="_blank">00:25:47.200</a></span> | <span class="t">Transformers, which is xlmr RobertaBase. As well as that, we need a pooling layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1557" target="_blank">00:25:57.040</a></span> | <span class="t">So we write models.pooling. And in here, we need to pass the output embeddings dimensions. So it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1568" target="_blank">00:26:08.240</a></span> | <span class="t">this get word embedding dimension for our model. And also what type of pooling we'd like to do. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1575" target="_blank">00:26:15.520</a></span> | <span class="t">have max pooling, CLS token pooling. And what we want is a mean pooling. So is pooling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1589" target="_blank">00:26:29.440</a></span> | <span class="t">mode mean tokens equals true. Okay. So that two components of our Sentence Transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1600" target="_blank">00:26:40.240</a></span> | <span class="t">And then from there, we can initialize our students. So student equals Sentence Transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1608" target="_blank">00:26:48.000</a></span> | <span class="t">And we're initializing that using the modules, which is just a list of our two components there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1617" target="_blank">00:26:57.120</a></span> | <span class="t">So xlmr followed by pooling. And that's it. So let's have a look at what we have there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1626" target="_blank">00:27:06.240</a></span> | <span class="t">Okay. We can just ignore this top bit here. We just want to focus on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1630" target="_blank">00:27:10.880</a></span> | <span class="t">So you see we have our transformer model followed by the pooling here. And we also see that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1637" target="_blank">00:27:17.840</a></span> | <span class="t">using the mean tokens pooling set to true, rest of them are false. Okay. So that's our student</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1644" target="_blank">00:27:24.240</a></span> | <span class="t">model initialized. And now what we want to do is initialize our teach model. Now the teach model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1652" target="_blank">00:27:32.080</a></span> | <span class="t">let me show you. You just have to be a little bit careful with this. So Sentence Transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1658" target="_blank">00:27:38.000</a></span> | <span class="t">So maybe you'd like to use one of the top forming ones, which a lot of them are the old models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1668" target="_blank">00:27:48.400</a></span> | <span class="t">So these are monolingual models, all MPNet base V2. And okay, let's initialize this and let's see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1682" target="_blank">00:28:02.160</a></span> | <span class="t">what is inside it. Okay. So we have the transformer, we have the pooling as we had before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1688" target="_blank">00:28:08.400</a></span> | <span class="t">but then we also have this normalization layer. So the outputs from this model are normalized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1696" target="_blank">00:28:16.080</a></span> | <span class="t">And obviously, if you're trying to make another model mimic that normalization layer outputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1703" target="_blank">00:28:23.440</a></span> | <span class="t">well, it's not ideal because the model is going to be trying to normalize its own vectors. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1712" target="_blank">00:28:32.480</a></span> | <span class="t">you don't really want to do that. You want to choose a model. You either want to remove the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1716" target="_blank">00:28:36.720</a></span> | <span class="t">normalization layer or just choose a model that doesn't have a normalization layer, which I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1723" target="_blank">00:28:43.840</a></span> | <span class="t">is probably the better option. So that's what I'm going to do. So for the teacher, I'm going to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1729" target="_blank">00:28:49.840</a></span> | <span class="t">a Sentence Transformer. I'm going to use a paraphrase models because these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1735" target="_blank">00:28:55.840</a></span> | <span class="t">don't use normalization layers. Distill, Roberta, base V2. Okay. Let's have a look.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1750" target="_blank">00:29:10.640</a></span> | <span class="t">Okay. So now you can see we have the transformer followed directly by the pooling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1756" target="_blank">00:29:16.240</a></span> | <span class="t">Now, another thing that you probably should just be aware of here is that we have this max</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1760" target="_blank">00:29:20.800</a></span> | <span class="t">sequence length here is 512, which doesn't align with our paraphrase model here. But that's fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1768" target="_blank">00:29:28.560</a></span> | <span class="t">because I'm going to limit the maximum sequence length anyway to 250. So it's not really an issue,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1778" target="_blank">00:29:38.560</a></span> | <span class="t">but just look out for that if you're training your own models. This one's of 384. So none of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1784" target="_blank">00:29:44.480</a></span> | <span class="t">those align. But yeah, just be aware that the sequence lengths might not align there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1793" target="_blank">00:29:53.280</a></span> | <span class="t">So we've formatted our training data. We have our two models, the teacher and the student.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1804" target="_blank">00:30:04.880</a></span> | <span class="t">So now what we can do is prepare that data for loading into our training process,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1811" target="_blank">00:30:11.680</a></span> | <span class="t">our fine tuning process. So I said before, we're going to be using the parallel sentences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1819" target="_blank">00:30:19.360</a></span> | <span class="t">sorry, from Sentence Transformers import parallel sentences dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1825" target="_blank">00:30:25.360</a></span> | <span class="t">And first thing we need to do here is actually initialize the object. And that requires that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1834" target="_blank">00:30:34.080</a></span> | <span class="t">we pass the two models that we're training with because this kind of handles the interaction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1839" target="_blank">00:30:39.760</a></span> | <span class="t">between those two models as well. So obviously we have our student model, which is our student.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1847" target="_blank">00:30:47.040</a></span> | <span class="t">And we have the teacher model, which is our teacher. Alongside this, we want batch size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1858" target="_blank">00:30:58.800</a></span> | <span class="t">I'm going to use 32, but I think actually you can probably use higher batches here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1865" target="_blank">00:31:05.280</a></span> | <span class="t">or you probably should use higher batches. I think 64 is one that I see used a lot in these training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1873" target="_blank">00:31:13.280</a></span> | <span class="t">codes. And you also use embedding cache equal to true. Okay. So that initializes the parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1887" target="_blank">00:31:27.600</a></span> | <span class="t">sentences dataset object. And now what we want to do is add our data to it. So we need our training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1895" target="_blank">00:31:35.360</a></span> | <span class="t">files. So training files equal to OS list that we did before. I think it's in the data file,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1904" target="_blank">00:31:44.400</a></span> | <span class="t">in the data directory. Yeah. So that's what we want. And what I'll do is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1916" target="_blank">00:31:56.800</a></span> | <span class="t">for F in those train files, I'm going to load each one of those into the dataset object.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1924" target="_blank">00:32:04.160</a></span> | <span class="t">Print F and data.loaddata. I need to make sure I include the path there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1932" target="_blank">00:32:12.560</a></span> | <span class="t">followed by the actual file name. You need to pass your max sentences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1942" target="_blank">00:32:22.240</a></span> | <span class="t">which is the maximum number of sentences that you're going to take from that load data batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1947" target="_blank">00:32:27.120</a></span> | <span class="t">So basically the maximum number of sentences we're going to use from each language there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1953" target="_blank">00:32:33.920</a></span> | <span class="t">Now, I'm just going to set this to 250,000, which is higher than any of the batches we have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1962" target="_blank">00:32:42.080</a></span> | <span class="t">That's fine. I don't think, I mean, if you want to try and balance it out, that's fine. You can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1967" target="_blank">00:32:47.920</a></span> | <span class="t">do that here. And then the other option is where we set the maximum length of the sentences that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1979" target="_blank">00:32:59.120</a></span> | <span class="t">we're going to be processing. So that is max sentence length. And I said before, look,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1987" target="_blank">00:33:07.040</a></span> | <span class="t">the maximum we have here is 256 or 512. So let's just trim all of those down to 256.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=1997" target="_blank">00:33:17.920</a></span> | <span class="t">Okay. That will load our data. And now we just need to initialize a data loader. So we're just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2006" target="_blank">00:33:26.960</a></span> | <span class="t">using PyTorch here. So run from Torch, utils.data, import data loader. Loader is equal to data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2019" target="_blank">00:33:39.920</a></span> | <span class="t">loader. That's our data. We want to shuffle that data. And we also want to set the batch size,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2029" target="_blank">00:33:49.440</a></span> | <span class="t">which is same as before, 32. Okay. So models are ready. Data is ready. Now we initialize our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2039" target="_blank">00:33:59.760</a></span> | <span class="t">loss function. So from sentence transformers again, dot losses, import MSE loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2049" target="_blank">00:34:09.280</a></span> | <span class="t">And then loss is equal to MSE loss. And then here we have model equals student model. Okay. So we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2062" target="_blank">00:34:22.640</a></span> | <span class="t">only optimizing our student model, not the teacher model. The teacher model is there to teach our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2069" target="_blank">00:34:29.360</a></span> | <span class="t">student, not the other way around. Okay. So that's everything we need ready for training. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2077" target="_blank">00:34:37.920</a></span> | <span class="t">let's move on to the actual training function. So we can train. I'm going to train for one epoch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2084" target="_blank">00:34:44.640</a></span> | <span class="t">but you can do more. I think in the actual, so in the other codes I've seen that do this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2094" target="_blank">00:34:54.240</a></span> | <span class="t">they will train for like five epochs. But even just training on one epoch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2099" target="_blank">00:34:59.280</a></span> | <span class="t">you actually get a pretty good model. So I think you don't need to train on too many.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2107" target="_blank">00:35:07.040</a></span> | <span class="t">But obviously, if you want better performance, I would go with the five that I've seen in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2114" target="_blank">00:35:14.080</a></span> | <span class="t">other codes. So we need to pass our train objectors here. So we have the data loader</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2123" target="_blank">00:35:23.040</a></span> | <span class="t">and then the loss function. Now we want to say, okay, how many epochs? I've said before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2129" target="_blank">00:35:29.200</a></span> | <span class="t">I'm going to go with one, a number of warmup steps. So before you jump straight up to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2135" target="_blank">00:35:35.680</a></span> | <span class="t">learning rate that we select in a moment, do we want to warm up first? Yes, we do. I'm going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2143" target="_blank">00:35:43.440</a></span> | <span class="t">warm up for 10% of the training data, which is just the length of the loader and multiply by 0.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2155" target="_blank">00:35:55.120</a></span> | <span class="t">Okay, and from there, where do you want to save the model? I'm going to try,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2163" target="_blank">00:36:03.200</a></span> | <span class="t">I'm going to save it in xmlTED, our optimizer parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2171" target="_blank">00:36:11.200</a></span> | <span class="t">So we're going to set a learning rate of 2e to the minus 5, epsilon of 1e to the minus 6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2186" target="_blank">00:36:26.480</a></span> | <span class="t">I'm also going to set correct bias equal to false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2192" target="_blank">00:36:32.160</a></span> | <span class="t">Okay, there are the optimizer parameters, and then we can also save the best model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2200" target="_blank">00:36:40.000</a></span> | <span class="t">Save the best model equal to true. And then we run it. Okay, so run that. It's going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2210" target="_blank">00:36:50.640</a></span> | <span class="t">take a long time, so I'm actually going to stop it because I've already run it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2215" target="_blank">00:36:55.520</a></span> | <span class="t">And let's have a look at actually evaluating that and have a look at the results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2219" target="_blank">00:36:59.520</a></span> | <span class="t">Okay, so I just have this notebook where I've evaluated the model. So I'm using this STS</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2227" target="_blank">00:37:07.760</a></span> | <span class="t">sentence textual similarity benchmark data set, which is multilingual. I'm getting the English</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2234" target="_blank">00:37:14.960</a></span> | <span class="t">data and also the Italian. And you can see they are similar. So each row in the English data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2246" target="_blank">00:37:26.240</a></span> | <span class="t">corresponds to the other language data sets as well. So in here, sentence 1 in the English means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2251" target="_blank">00:37:31.600</a></span> | <span class="t">the same thing as sentence 0 in the Italian. Okay, same sentence 2, also the same similarity score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2259" target="_blank">00:37:39.680</a></span> | <span class="t">So the first thing we do is normalize that similarity score, and then we go down a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2266" target="_blank">00:37:46.320</a></span> | <span class="t">bit. So we reformat the data using Sentence Transformer's InputExample class. And through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2274" target="_blank">00:37:54.960</a></span> | <span class="t">this, I've created three different evaluation sets. So we have the English to English,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2280" target="_blank">00:38:00.480</a></span> | <span class="t">Italian to Italian, and then English to Italian. And then what we do here is we initialize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2289" target="_blank">00:38:09.600</a></span> | <span class="t">a similarity evaluator for each of these data sets. Again, we're using Sentence Transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2295" target="_blank">00:38:15.680</a></span> | <span class="t">just makes life a lot easier. We initialize those, and then we can just pass our model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2301" target="_blank">00:38:21.280</a></span> | <span class="t">to each one of those evaluators to get its performance. So here, 81.6 on the English set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2308" target="_blank">00:38:28.640</a></span> | <span class="t">74.3 and 71 here. Now, I just trained on one epoch. If you want better performance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2318" target="_blank">00:38:38.320</a></span> | <span class="t">you can train on what epochs, and you should be able to get more towards 80% or maybe a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2324" target="_blank">00:38:44.240</a></span> | <span class="t">bit higher. So pretty straightforward and incredibly easy. And then here, I wanted to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2333" target="_blank">00:38:53.040</a></span> | <span class="t">compare that to the student before we trained it. So I initialized a new student and had a look,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2338" target="_blank">00:38:58.800</a></span> | <span class="t">and you can see the evaluation is pretty low. So for English, 47.5. Italian, actually 50%,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2347" target="_blank">00:39:07.680</a></span> | <span class="t">surprisingly. Although it's already a multilingual model, so it does make sense I can just send</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2355" target="_blank">00:39:15.120</a></span> | <span class="t">Italian. And then from English to Italian, it really struggles, drops down to 23.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2362" target="_blank">00:39:22.800</a></span> | <span class="t">So that's it for this video. I think it's been pretty useful, at least for me. I can kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2372" target="_blank">00:39:32.000</a></span> | <span class="t">see where you can build a Sentence Transformer in a lot of different languages using this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2379" target="_blank">00:39:39.040</a></span> | <span class="t">which is, I think, really cool and will probably be useful for a lot of people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2383" target="_blank">00:39:43.680</a></span> | <span class="t">So I hope you enjoyed the video. Thank you very much for watching,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNS5pOpjvAQ&t=2389" target="_blank">00:39:49.680</a></span> | <span class="t">and I'll see you again in the next one.</span></div></div></body></html>