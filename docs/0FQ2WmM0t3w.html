<html><head><title>SPLADE: the first search model to beat BM25</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>SPLADE: the first search model to beat BM25</h2><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w"><img src="https://i.ytimg.com/vi_webp/0FQ2WmM0t3w/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=0">0:0</a> Sparse and dense vector search<br><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=44">0:44</a> Comparing sparse vs. dense vectors<br><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=239">3:59</a> Using sparse and dense together<br><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=406">6:46</a> What is SPLADE?<br><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=546">9:6</a> Vocabulary mismatch problem<br><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=591">9:51</a> How SPLADE works (transformers 101)<br><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=868">14:28</a> Masked language modeling (MLM)<br><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=957">15:57</a> How SPLADE builds embeddings with MLM<br><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1055">17:35</a> Where SPLADE doesn't work so well<br><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1214">20:14</a> Implementing SPLADE in Python<br><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1238">20:38</a> SPLADE with PyTorch and Hugging Face<br><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1448">24:8</a> Using the Naver SPLADE library<br><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1631">27:11</a> What's next for vector search?<br><br><div style="text-align: left;"><a href="./0FQ2WmM0t3w.html">Whisper Transcript</a> | <a href="./transcript_0FQ2WmM0t3w.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">In information retrieval vector embeddings represent documents and queries in a numerical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=8" target="_blank">00:00:08.080</a></span> | <span class="t">vector format. That means that we can take some text which could be web pages from the internet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=15" target="_blank">00:00:15.200</a></span> | <span class="t">in the case of Google or maybe product descriptions in the case of Amazon and we can encode it using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=22" target="_blank">00:00:22.080</a></span> | <span class="t">some sort of embedding method or model and we will get something that looks like this. So we have now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=29" target="_blank">00:00:29.120</a></span> | <span class="t">represented our text in a vector space. Now there are different ways of doing this and sparse and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=37" target="_blank">00:00:37.200</a></span> | <span class="t">dense vectors are two different forms of this representation each with their own pros and their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=43" target="_blank">00:00:43.520</a></span> | <span class="t">own cons. Typically when we think of sparse vectors things like TF-IDF and BM25 they have very high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=51" target="_blank">00:00:51.760</a></span> | <span class="t">dimensionality and they contain very few non-zero values. So the information within those vectors is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=59" target="_blank">00:00:59.680</a></span> | <span class="t">very sparsely located and with these types of vectors we have decades of research looking at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=67" target="_blank">00:01:07.520</a></span> | <span class="t">how they can be used and how they can be represented using compact data structures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=74" target="_blank">00:01:14.000</a></span> | <span class="t">and there are naturally many very very efficient retrieval systems designed specifically for these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=81" target="_blank">00:01:21.840</a></span> | <span class="t">vectors. On the other hand we have dense vectors. Dense vectors are lower dimensional but they are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=88" target="_blank">00:01:28.720</a></span> | <span class="t">very information rich and that's because all of the information is compressed into this much smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=95" target="_blank">00:01:35.200</a></span> | <span class="t">dimensional space so we don't have these non-zero values that we would get in a sparse vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=101" target="_blank">00:01:41.040</a></span> | <span class="t">and hence all of the information is very densely packed and hence why we call them dense vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=106" target="_blank">00:01:46.560</a></span> | <span class="t">These types of vectors are typically built using neural network type architectures like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=111" target="_blank">00:01:51.680</a></span> | <span class="t">transformers and through this they can represent more abstract information like the semantic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=119" target="_blank">00:01:59.680</a></span> | <span class="t">meaning behind some text. When it comes to sparse embeddings the pros are typically faster retrieval,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=126" target="_blank">00:02:06.240</a></span> | <span class="t">a good baseline performance, we don't need to do any model fine-tuning and we also get to do exact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=134" target="_blank">00:02:14.240</a></span> | <span class="t">matching of terms. Whereas on the cons we have a few things as well so the performance cannot really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=140" target="_blank">00:02:20.880</a></span> | <span class="t">be improved significantly over the baseline performance of these algorithms. They also suffer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=146" target="_blank">00:02:26.400</a></span> | <span class="t">from something called the vocabulary mismatch problem and we'll talk about that in more detail</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=151" target="_blank">00:02:31.760</a></span> | <span class="t">later and it also doesn't align with the human-like thought of abstract concepts that I described</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=158" target="_blank">00:02:38.400</a></span> | <span class="t">earlier. Naturally we have a completely different set of pros and cons when it comes to dense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=162" target="_blank">00:02:42.880</a></span> | <span class="t">vectors. On the pros we know that dense vectors can outperform sparse vectors with fine-tuning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=169" target="_blank">00:02:49.520</a></span> | <span class="t">we also know that using these we can search with human-like abstract concepts, we have great</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=175" target="_blank">00:02:55.840</a></span> | <span class="t">support of multi-modalities so we can search across text, images, audio etc and we can even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=182" target="_blank">00:03:02.960</a></span> | <span class="t">do cross-modal search so we can go from text to image or image to text or whatever you can think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=189" target="_blank">00:03:09.360</a></span> | <span class="t">of. But of course there's also the cons. We know that in order to outperform sparse vector embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=197" target="_blank">00:03:17.120</a></span> | <span class="t">or even get close to sparse vector embeddings in terms of performance we very often require training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=203" target="_blank">00:03:23.520</a></span> | <span class="t">and training requires a lot of data which is very difficult to find when we are find ourselves in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=209" target="_blank">00:03:29.280</a></span> | <span class="t">low resource scenarios. These models also do not generalize very well particularly when we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=214" target="_blank">00:03:34.880</a></span> | <span class="t">moving from one domain with very specific terminology to another domain with completely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=220" target="_blank">00:03:40.160</a></span> | <span class="t">different terminology. These embeddings also require more compute and memory to build and store</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=226" target="_blank">00:03:46.560</a></span> | <span class="t">and search across than sparse methods. We do not get any exact match search and it's kind of hard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=234" target="_blank">00:03:54.000</a></span> | <span class="t">to understand why we're getting results some of the time so it's not very interpretable. Ideally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=240" target="_blank">00:04:00.080</a></span> | <span class="t">we want a way of getting the best of both worlds, we want the pros of dense and the pros of sparse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=246" target="_blank">00:04:06.880</a></span> | <span class="t">and just we don't want any of these cons. But that's very hard to do. There have been some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=253" target="_blank">00:04:13.040</a></span> | <span class="t">band-aid solutions. One of those is to perform two-stage retrieval. In this scenario we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=260" target="_blank">00:04:20.720</a></span> | <span class="t">two stages to retrieve and rank relevant documents for a given query. In the first stage our system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=268" target="_blank">00:04:28.480</a></span> | <span class="t">would use a sparse retrieval method to search through and return relevant documents from a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=275" target="_blank">00:04:35.360</a></span> | <span class="t">large set of candidate documents. These are then passed on to the second stage which is a re-ranking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=282" target="_blank">00:04:42.560</a></span> | <span class="t">stage and this uses a dense embedding model to re-rank from that smaller set of candidate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=289" target="_blank">00:04:49.440</a></span> | <span class="t">documents which one it believes is the most relevant using its more human-like semantic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=296" target="_blank">00:04:56.640</a></span> | <span class="t">comprehension of language. There are some benefits to this. First we can apply the sparse method to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=302" target="_blank">00:05:02.480</a></span> | <span class="t">the full set of documents which makes it more efficient to actually search through those and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=307" target="_blank">00:05:07.600</a></span> | <span class="t">then after that we can re-rank everything with our dense model which is naturally much slower but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=313" target="_blank">00:05:13.600</a></span> | <span class="t">we're dealing with a smaller amount of data. Another benefit is that this re-ranking stage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=319" target="_blank">00:05:19.280</a></span> | <span class="t">is detached from the retrieval system so we can modify one of those stages without affecting the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=326" target="_blank">00:05:26.960</a></span> | <span class="t">other and this is particularly useful if we have multiple models that take for example the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=334" target="_blank">00:05:34.640</a></span> | <span class="t">of the sparse retrieval stage. So that's another thing to consider. However of course this is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=340" target="_blank">00:05:40.640</a></span> | <span class="t">perfect. Two stages of retrieval and re-ranking can be slower than using a single stage system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=349" target="_blank">00:05:49.280</a></span> | <span class="t">that uses approximate nearest neighbor search algorithms and of course having two stages within</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=355" target="_blank">00:05:55.920</a></span> | <span class="t">the system is more complicated and there are naturally going to be many engineering challenges</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=362" target="_blank">00:06:02.240</a></span> | <span class="t">that come with that and we're also very reliant on that first stage retriever. If that first stage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=369" target="_blank">00:06:09.120</a></span> | <span class="t">retriever doesn't perform very well then there's nothing we can do with the second stage re-ranking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=376" target="_blank">00:06:16.000</a></span> | <span class="t">model because if it is just being given a load of rubbish results it's just going to re-rank</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=381" target="_blank">00:06:21.920</a></span> | <span class="t">rubbish results and the final result will still be rubbish. So they're the main problems with this and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=387" target="_blank">00:06:27.840</a></span> | <span class="t">ideally we want to solve that and we want to do that by improving single-stage systems. Now a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=394" target="_blank">00:06:34.640</a></span> | <span class="t">of work has been put into improving single-stage retrieval systems. A big part of that research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=400" target="_blank">00:06:40.240</a></span> | <span class="t">has been in building more robust and learnable sparse embedding models and one of the most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=407" target="_blank">00:06:47.280</a></span> | <span class="t">promising models within this space is known as SPLADE. Now the idea behind the sparse lexical and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=416" target="_blank">00:06:56.080</a></span> | <span class="t">expansion models is that a pre-trained model like BERT can identify connections between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=422" target="_blank">00:07:02.560</a></span> | <span class="t">words and sub-words which we can call word pieces or terms and use that knowledge to enhance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=429" target="_blank">00:07:09.520</a></span> | <span class="t">our sparse vector embeddings. This works in two ways it allows us to measure the relevance of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=437" target="_blank">00:07:17.920</a></span> | <span class="t">different terms so the word 'the' will carry less significance in most cases than a less common word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=446" target="_blank">00:07:26.400</a></span> | <span class="t">like orangutan. The second thing it helps us with is it enables learnable term expansion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=454" target="_blank">00:07:34.080</a></span> | <span class="t">where term expansion is the inclusion of alternative but relevant terms beyond those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=461" target="_blank">00:07:41.040</a></span> | <span class="t">that are found in the original sentence or sequence. Now it's very important to take note of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=467" target="_blank">00:07:47.120</a></span> | <span class="t">the fact that I said learnable term expansion. The big advantage of SPLADE is not they can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=473" target="_blank">00:07:53.280</a></span> | <span class="t">term expansion that is something that has been done for a while but they can learn term expansions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=479" target="_blank">00:07:59.600</a></span> | <span class="t">In the past term expansion could be done with more traditional methods but it required</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=484" target="_blank">00:08:04.880</a></span> | <span class="t">rule-based logic and rule-based logic someone would have to write that and this is naturally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=490" target="_blank">00:08:10.640</a></span> | <span class="t">time consuming and fundamentally limited because you can't write rules for every single scenario in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=498" target="_blank">00:08:18.240</a></span> | <span class="t">human language. Now by using SPLADE we can simply learn these using a transformer model which is of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=506" target="_blank">00:08:26.480</a></span> | <span class="t">course much more robust and much less time consuming for us. Now another benefit of using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=512" target="_blank">00:08:32.960</a></span> | <span class="t">a context-aware transform model like BERT is that it will modify these term expansions based on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=520" target="_blank">00:08:40.240</a></span> | <span class="t">context based on the sentence that's being input so it won't just expand the word rainforest to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=527" target="_blank">00:08:47.200</a></span> | <span class="t">three different words it will expand the right word rainforest to many different words that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=534" target="_blank">00:08:54.400</a></span> | <span class="t">entirely depends on the context or the sentence that was fed in with and this is one of the big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=540" target="_blank">00:09:00.400</a></span> | <span class="t">benefits of attention models like transformers that is very context aware. Now term expansion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=547" target="_blank">00:09:07.840</a></span> | <span class="t">is crucial in minimizing a very key problem with sparse embedding methods and that is the vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=556" target="_blank">00:09:16.320</a></span> | <span class="t">mismatch problem. Now the vocabulary mismatch problem is the very typical lack of overlap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=563" target="_blank">00:09:23.040</a></span> | <span class="t">between a query and the documents that we are searching for. It's because we think of things in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=570" target="_blank">00:09:30.480</a></span> | <span class="t">abstract ideas and concepts and we have many different words in order to explain the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=575" target="_blank">00:09:35.280</a></span> | <span class="t">thing it's very unlikely that the way that we describe something when we're searching for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=579" target="_blank">00:09:39.600</a></span> | <span class="t">something contains the exact terms the exact words that this relevant information contains</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=587" target="_blank">00:09:47.600</a></span> | <span class="t">and this is just a side effect of the complexity of human language. Now let's move on to SPLADE</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=593" target="_blank">00:09:53.920</a></span> | <span class="t">and how SPLADE actually builds these sparse embeddings. Now it's actually relatively easy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=600" target="_blank">00:10:00.880</a></span> | <span class="t">to grasp what is happening here. We first start with the transform model like BERT. Now these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=607" target="_blank">00:10:07.200</a></span> | <span class="t">transform models use something called mass language modeling in order to perform their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=612" target="_blank">00:10:12.960</a></span> | <span class="t">pre-training on a ton of text data. Not all transform models use this but most do. Now if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=618" target="_blank">00:10:18.880</a></span> | <span class="t">you're familiar with BERT and mass language modeling that's great if not we're going to just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=624" target="_blank">00:10:24.400</a></span> | <span class="t">quickly break it down. So starting with BERT it's a very popular transform model and like all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=631" target="_blank">00:10:31.280</a></span> | <span class="t">transform models its core functionality is actually to create information rich token embeddings. Now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=639" target="_blank">00:10:39.120</a></span> | <span class="t">what exactly does that mean? Well we start with some text like orangutans are native to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=646" target="_blank">00:10:46.400</a></span> | <span class="t">forests of Indonesia and Malaysia. With a transform model like BERT we would begin by tokenizing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=652" target="_blank">00:10:52.800</a></span> | <span class="t">text into BERT specific sub-word or word level tokens and we can see that here. So using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=660" target="_blank">00:11:00.320</a></span> | <span class="t">HuggingFace transformers library we have this tokenizer object here. This is what is going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=666" target="_blank">00:11:06.320</a></span> | <span class="t">handle the tokenization of our text. So we have the same sentence I described before orangutans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=672" target="_blank">00:11:12.160</a></span> | <span class="t">are native to the rainforests of Indonesia and Malaysia and we convert it into these tokens which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=678" target="_blank">00:11:18.240</a></span> | <span class="t">is what you can see here. Now these are just the token IDs which are integer numbers but each one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=684" target="_blank">00:11:24.160</a></span> | <span class="t">of these represents something within our text. So here for example this 2030 probably represents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=691" target="_blank">00:11:31.840</a></span> | <span class="t">orangutan and the 5654 here maybe represents the S at the end of orangutan. They can be word level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=700" target="_blank">00:11:40.240</a></span> | <span class="t">or sub-word level like that. Now these are just the numbers let's have a look down here and we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=705" target="_blank">00:11:45.600</a></span> | <span class="t">actually see how our words are broken up into these token IDs or tokens. So we convert those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=713" target="_blank">00:11:53.440</a></span> | <span class="t">IDs back into human readable tokens and we can see okay we have this this called a classified token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=719" target="_blank">00:11:59.360</a></span> | <span class="t">that is a special token used by BERT. We'll see that at the start of every sequence tokenized by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=724" target="_blank">00:12:04.880</a></span> | <span class="t">BERT tokenizer and then we have orangutans. So it's actually split between four tokens and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=731" target="_blank">00:12:11.040</a></span> | <span class="t">can see the rest of the sentence there as well. Now why do we create these tokens and these token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=737" target="_blank">00:12:17.840</a></span> | <span class="t">IDs? Well that's because these token IDs are then mapped to what is called an embedding matrix. The</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=745" target="_blank">00:12:25.040</a></span> | <span class="t">embedding matrix is the first layer of our transformer model. Now in this embedding matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=752" target="_blank">00:12:32.560</a></span> | <span class="t">we will find learned vector representations that literally represent the tokens that we fed in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=759" target="_blank">00:12:39.680</a></span> | <span class="t">within a vector space. So the vector representation for the token rainforest will have a high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=767" target="_blank">00:12:47.040</a></span> | <span class="t">proximity because it has a high semantic similarity to the vector representations for the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=774" target="_blank">00:12:54.000</a></span> | <span class="t">jungle or the token forest. Whereas it will be further away in that vector space from somewhat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=781" target="_blank">00:13:01.280</a></span> | <span class="t">less related tokens like native or the. Now from here the token representations of our original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=788" target="_blank">00:13:08.320</a></span> | <span class="t">text are going to go through several encoder blobs. These blobs encode more and more contextual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=795" target="_blank">00:13:15.360</a></span> | <span class="t">information into each one of these token embeddings. So as we progress through all of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=802" target="_blank">00:13:22.160</a></span> | <span class="t">encoder blobs the embeddings are basically going to be moved within that vector space in order to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=808" target="_blank">00:13:28.720</a></span> | <span class="t">consider the meaning within the context of the sentence it appears in rather than just the meaning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=814" target="_blank">00:13:34.160</a></span> | <span class="t">of the token by itself. And after all this progressive iteration of encoding more contextual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=822" target="_blank">00:13:42.000</a></span> | <span class="t">information into our embeddings we arrive at the transformers output layer. Here we have our final</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=831" target="_blank">00:13:51.200</a></span> | <span class="t">information rich vector embeddings. Each embedding represents the early token but obviously with that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=836" target="_blank">00:13:56.400</a></span> | <span class="t">context encoded into it. This process is the core of BERT and every other transformer model. However</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=842" target="_blank">00:14:02.640</a></span> | <span class="t">the power of transformers comes from the considerable number of things for which these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=849" target="_blank">00:14:09.360</a></span> | <span class="t">information rich embeddings can be used. Typically what will happen is we'll add a task-specific head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=855" target="_blank">00:14:15.520</a></span> | <span class="t">onto the end of the transform model that will transform these information rich embeddings or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=861" target="_blank">00:14:21.840</a></span> | <span class="t">vector embeddings into something else like sentiment predictions or sparse vectors. The</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=868" target="_blank">00:14:28.880</a></span> | <span class="t">mass language modeling head is one of the most common of these task-specific heads because it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=876" target="_blank">00:14:36.880</a></span> | <span class="t">is used for pre-training most transformer models. This works by taking a input sentence again let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=883" target="_blank">00:14:43.440</a></span> | <span class="t">use the orangutans are native to the forests of Indonesia and Malaysia example again. We will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=889" target="_blank">00:14:49.600</a></span> | <span class="t">tokenize this text and then mask a few of those tokens at random. This mask token sequence is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=896" target="_blank">00:14:56.080</a></span> | <span class="t">then passed as input to BERT and at the other end we actually feed in the original unmasked sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=902" target="_blank">00:15:02.960</a></span> | <span class="t">to the mass language modeling head and what will happen is BERT and the mass language modeling head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=909" target="_blank">00:15:09.040</a></span> | <span class="t">will have to adjust their internal weights in order to produce accurate predictions for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=916" target="_blank">00:15:16.240</a></span> | <span class="t">tokens that have been masked. For this to work the mass language modeling head contains 30,522</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=925" target="_blank">00:15:25.520</a></span> | <span class="t">output tokens which is the vocabulary size of the BERT base model. So that means we have a output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=933" target="_blank">00:15:33.440</a></span> | <span class="t">for every possible prediction for every possible token prediction and the output as a whole acts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=940" target="_blank">00:15:40.000</a></span> | <span class="t">as a probability distribution over this entire vocabulary and the highest activation across that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=946" target="_blank">00:15:46.080</a></span> | <span class="t">probability distribution represents the token that BERT and the mass language modeling head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=951" target="_blank">00:15:51.440</a></span> | <span class="t">have predicted as being the token behind that masked token position. Now at the same time we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=958" target="_blank">00:15:58.880</a></span> | <span class="t">can think of this probability distribution as a representation of the words or tokens that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=967" target="_blank">00:16:07.680</a></span> | <span class="t">most relevant to a particular token within the context of the wider sentence. With that what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=974" target="_blank">00:16:14.560</a></span> | <span class="t">can do with SPLADE is take all of these distributions and aggregate them into a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=981" target="_blank">00:16:21.440</a></span> | <span class="t">distribution called the importance estimation. The importance estimation is actually the sparse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=988" target="_blank">00:16:28.320</a></span> | <span class="t">vector produced by SPLADE and that is done using this equation here and this allows us to identify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=994" target="_blank">00:16:34.800</a></span> | <span class="t">relevant tokens that do not exist in the original sequence. For example if we masked the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1000" target="_blank">00:16:40.160</a></span> | <span class="t">rainforest we might return high predictions for the words jungle, land and forest. These words and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1008" target="_blank">00:16:48.800</a></span> | <span class="t">their associated probabilities would then be represented in the SPLADE built sparse vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1014" target="_blank">00:16:54.640</a></span> | <span class="t">and that doesn't mean we need to mask everything. The predictions will be made relevant to each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1020" target="_blank">00:17:00.400</a></span> | <span class="t">token whether it is masked or not. So in the end all we have to input is the unmasked sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1027" target="_blank">00:17:07.920</a></span> | <span class="t">and what we will get is all of these probability distributions for similar words to whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1032" target="_blank">00:17:12.560</a></span> | <span class="t">has been input based on the sentence in the context. Now many transform models are trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1039" target="_blank">00:17:19.600</a></span> | <span class="t">with mass language modeling which means there are a huge number of models that have already got these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1045" target="_blank">00:17:25.280</a></span> | <span class="t">mass language modeling weights and we can actually use that to fine-tune those models as SPLADE</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1051" target="_blank">00:17:31.360</a></span> | <span class="t">models and that's something that we will cover in another video. Now let's have a quick look at where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1057" target="_blank">00:17:37.680</a></span> | <span class="t">SPLADE works kind of less well. So as we've seen SPLADE is a really good tool for minimizing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1064" target="_blank">00:17:44.800</a></span> | <span class="t">vocabulary mismatch problem however there are of course some drawbacks that we should consider.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1069" target="_blank">00:17:49.680</a></span> | <span class="t">Compared to other sparse methods retrieval with SPLADE is very slow. There are three primary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1076" target="_blank">00:17:56.320</a></span> | <span class="t">reasons for this. First the number of non-zero values in SPLADE query and document vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1082" target="_blank">00:18:02.400</a></span> | <span class="t">is typically much greater than in traditional sparse vectors because of that term expansion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1088" target="_blank">00:18:08.320</a></span> | <span class="t">and sparse retrieval systems are rarely optimized for this. Second the distribution of these non-zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1095" target="_blank">00:18:15.760</a></span> | <span class="t">values also deviates from the traditional distribution expected by most sparse retrieval</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1102" target="_blank">00:18:22.080</a></span> | <span class="t">systems again causing slowdowns and third SPLADE vectors are not natively supported by most sparse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1111" target="_blank">00:18:31.760</a></span> | <span class="t">retrieval systems meaning that we have to perform multiple pre and post processing steps,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1118" target="_blank">00:18:38.240</a></span> | <span class="t">weight discretization and other things in order to make it work if it works at all and it again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1125" target="_blank">00:18:45.520</a></span> | <span class="t">it's not optimized for that. Fortunately there are some solutions to all of these problems. For one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1131" target="_blank">00:18:51.440</a></span> | <span class="t">the authors of SPLADE actually address this in a later paper that minimizes the number of non-zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1139" target="_blank">00:18:59.440</a></span> | <span class="t">values in the query vectors and they do that with two steps. First they improved the performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1146" target="_blank">00:19:06.960</a></span> | <span class="t">of displayed document encodings using max pooling rather than the traditional pooling strategy and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1154" target="_blank">00:19:14.720</a></span> | <span class="t">second they limited the term expansion to the document encodings only so they didn't do the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1161" target="_blank">00:19:21.760</a></span> | <span class="t">query expansions and thanks to the improved document encoding performance dropping those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1168" target="_blank">00:19:28.320</a></span> | <span class="t">query expansions still leaves us with better performance than the original SPLADE model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1174" target="_blank">00:19:34.800</a></span> | <span class="t">And then if we look at the final two problems so two and three these can both be solved by using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1181" target="_blank">00:19:41.280</a></span> | <span class="t">the Pinecone vector database. Two is solved by Pinecone's retrieval engine being designed to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1188" target="_blank">00:19:48.000</a></span> | <span class="t">agnostic to data distribution and for number three Pinecone supports real valued sparse vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1195" target="_blank">00:19:55.440</a></span> | <span class="t">meaning SPLADE vectors are supported natively without needing to do any of those weird things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1203" target="_blank">00:20:03.040</a></span> | <span class="t">in pre-processing post-processing or discretization. Now with all of that I think we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1210" target="_blank">00:20:10.880</a></span> | <span class="t">have covered everything we could possibly cover in order to understand SPLADE. Now let's have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1214" target="_blank">00:20:14.960</a></span> | <span class="t">look at how we would actually implement SPLADE in practice. Now we have two options for implementing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1221" target="_blank">00:20:21.040</a></span> | <span class="t">SPLADE we can do directly with Hugging Face Transformers and PyTorch or with a high-level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1227" target="_blank">00:20:27.280</a></span> | <span class="t">abstraction using the official SPLADE library. We'll take a look at doing both starting with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1232" target="_blank">00:20:32.480</a></span> | <span class="t">the Hugging Face and PyTorch implementation just so we can understand how it actually works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1238" target="_blank">00:20:38.640</a></span> | <span class="t">Okay so first we start by just installing a few prerequisites so we have SPLADE, Transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1244" target="_blank">00:20:44.720</a></span> | <span class="t">and PyTorch and then what we need to do is install this and then what we need to do is initialize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1250" target="_blank">00:20:50.960</a></span> | <span class="t">the tokenizer it's very similar to the BERT tokenizer we initialized earlier and the auto</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1257" target="_blank">00:20:57.280</a></span> | <span class="t">model for MastLM so this is Mast Language Modeling. So we're going to be using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1262" target="_blank">00:21:02.720</a></span> | <span class="t">Naver SPLADE model here and we just initialize all of that. Okay and we have one pretty large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1271" target="_blank">00:21:11.920</a></span> | <span class="t">chunk of text here so this is very domain specific so it has a lot of very specific words in there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1279" target="_blank">00:21:19.200</a></span> | <span class="t">that a typical dense embedding model would probably struggle with unless it has been fine-tuned on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1285" target="_blank">00:21:25.200</a></span> | <span class="t">data containing these exact same terms. So we'll run that and what we do is we tokenize everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1291" target="_blank">00:21:31.520</a></span> | <span class="t">so that will give us our token IDs that you saw earlier and then we process those through our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1297" target="_blank">00:21:37.200</a></span> | <span class="t">model to create our logits output which is what we will see in a moment this here. Okay so as we saw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1306" target="_blank">00:21:46.080</a></span> | <span class="t">before those logits will be each one of them contains our probability distribution over the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1313" target="_blank">00:21:53.760</a></span> | <span class="t">30.5 thousand possible tokens from the vocabulary and we have 91 of those. Now the reason we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1321" target="_blank">00:22:01.440</a></span> | <span class="t">91 of those is because from our tokens here we actually had 91 input tokens so if we have a look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1329" target="_blank">00:22:09.840</a></span> | <span class="t">at tokens input IDs dot shape we see that there was 91 input in there so that will change depending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1341" target="_blank">00:22:21.120</a></span> | <span class="t">on how many input tokens we have. Now from here what we're going to do is take these output logits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1347" target="_blank">00:22:27.120</a></span> | <span class="t">and we want to transform them into a sparse vector. Now to do that we're going to be using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1352" target="_blank">00:22:32.080</a></span> | <span class="t">the formula that you saw earlier to create the importance estimation and if we run that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1359" target="_blank">00:22:39.680</a></span> | <span class="t">we'll get a single probability distribution which represents the actual sparse vector from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1365" target="_blank">00:22:45.920</a></span> | <span class="t">SPLATE and we can have a look at that vector and we see there's mostly zeros in there there are a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1370" target="_blank">00:22:50.320</a></span> | <span class="t">few values but very few. So what I'm going to do now is first I want to just ignore this bit we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1378" target="_blank">00:22:58.400</a></span> | <span class="t">going to come down to here and we're going to create a dictionary format of our sparse vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1385" target="_blank">00:23:05.280</a></span> | <span class="t">so we run this and there's a few things I want to look at here so number of non-zero values that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1392" target="_blank">00:23:12.480</a></span> | <span class="t">actually have is 174 and all of them are now contained within this sparse dictionary. Okay so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1400" target="_blank">00:23:20.160</a></span> | <span class="t">these are the token IDs and these are the weights or the relevance of each one of those particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1408" target="_blank">00:23:28.160</a></span> | <span class="t">tokens. Now we can't read any of these token IDs so similar to before what we're going to do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1415" target="_blank">00:23:35.200</a></span> | <span class="t">convert those into actual human readable tokens so to do that we'll need to run this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1420" target="_blank">00:23:40.880</a></span> | <span class="t">and then we come down here and we're going to convert them into a more readable format.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1426" target="_blank">00:23:46.960</a></span> | <span class="t">Okay we can see what it believes is important is all of these values so we've sorted everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1433" target="_blank">00:23:53.920</a></span> | <span class="t">here so that's why the numbers have changed here and we can see that most importantly it's seeing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1439" target="_blank">00:23:59.200</a></span> | <span class="t">like programmed, death, cell, lattice, so a lot of very relevant words within that particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1448" target="_blank">00:24:08.000</a></span> | <span class="t">domain. Now if we come a little bit further down we can also see how to do that using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1454" target="_blank">00:24:14.080</a></span> | <span class="t">the Naver SPLADE library. So for that we would have to pip install SPLADE we did that at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1459" target="_blank">00:24:19.760</a></span> | <span class="t">top of the notebook so we don't need to do it again. We're going to be using the max aggregation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1465" target="_blank">00:24:25.280</a></span> | <span class="t">so this is using the max pooling method. Run this again using the same model ID here because it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1470" target="_blank">00:24:30.800</a></span> | <span class="t">also downloading the model from Hugging Face Transformers and what we do is we set torch to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1476" target="_blank">00:24:36.720</a></span> | <span class="t">no grab so this is saying we don't want to update any of the model weights because we're not doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1481" target="_blank">00:24:41.440</a></span> | <span class="t">fine tuning we're just performing inference eg prediction here and we just pass into the Naver</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1486" target="_blank">00:24:46.880</a></span> | <span class="t">model our tokens which we built using the tokenizer earlier on. From there we need to extract</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1493" target="_blank">00:24:53.200</a></span> | <span class="t">the drep tensor and we'll squeeze that to remove one of the dimensions that is unnecessary and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1500" target="_blank">00:25:00.800</a></span> | <span class="t">can then have a look we have 30.5 thousand dimensions here so this is our probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1507" target="_blank">00:25:07.040</a></span> | <span class="t">distribution or importance estimation and that is obviously our sparse vector and what we can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1514" target="_blank">00:25:14.720</a></span> | <span class="t">is actually use what we've done so far in order to compare different documents. So let's take a few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1522" target="_blank">00:25:22.400</a></span> | <span class="t">of these so we have program cell def no no no this is the original text and then the ones below here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1528" target="_blank">00:25:28.880</a></span> | <span class="t">are just me attempting to write something that is either relevant or not relevant that uses a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1535" target="_blank">00:25:35.440</a></span> | <span class="t">similar type of language. So we can run that we'll encode everything we're going to use the PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1543" target="_blank">00:25:43.840</a></span> | <span class="t">and Hugging Face Transformers method but either way it will both of these will produce the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1549" target="_blank">00:25:49.680</a></span> | <span class="t">result whether you use that or the actual splayed library and what we'll get is three of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1556" target="_blank">00:25:56.080</a></span> | <span class="t">importance estimations the splayed vectors and then what we can do is calculate cosine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1562" target="_blank">00:26:02.560</a></span> | <span class="t">similarity between them. So here I'm just going to initialize a zeros array that is just to store</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1567" target="_blank">00:26:07.920</a></span> | <span class="t">the similarity scores that we're going to create using this here. So we run that let's have a look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1574" target="_blank">00:26:14.480</a></span> | <span class="t">at the similarity and we can see that obviously these in the diagonal here this is where we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1581" target="_blank">00:26:21.120</a></span> | <span class="t">comparing each of the vectors to itself so it scores pretty highly because obviously they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1587" target="_blank">00:26:27.200</a></span> | <span class="t">the same but then the ones that we see as being the most similar other than the you know themselves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1593" target="_blank">00:26:33.040</a></span> | <span class="t">is sentence zero and sentence one so this one here if we come up to here so basically these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1601" target="_blank">00:26:41.920</a></span> | <span class="t">two here are being viewed as the most similar and if we read those we can see that they are in fact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1607" target="_blank">00:26:47.200</a></span> | <span class="t">much more similar they have a lot more overlap in terms of the terms but it's not just about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1613" target="_blank">00:26:53.680</a></span> | <span class="t">the terms that we see here but also the terms that produce from the term expansion as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1618" target="_blank">00:26:58.640</a></span> | <span class="t">So that's how we would compare everything that's how we would actually use splayed to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1625" target="_blank">00:27:05.920</a></span> | <span class="t">embeddings and to actually compare those sparse vectors as well using cosine similarity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1631" target="_blank">00:27:11.840</a></span> | <span class="t">Now that's it for this introduction to learn sparse embeddings with splayed. Now using splayed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1638" target="_blank">00:27:18.000</a></span> | <span class="t">we can represent text with more efficient sparse vector embeddings that help us at the same time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1646" target="_blank">00:27:26.560</a></span> | <span class="t">deal with the vocabulary mismatch problem whilst enabling exact matching and drawing from some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1653" target="_blank">00:27:33.120</a></span> | <span class="t">the other benefits of using sparse vectors. But of course there's still a lot to be done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1658" target="_blank">00:27:38.640</a></span> | <span class="t">and there's more research and more efforts looking at how to mix both dense and sparse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1666" target="_blank">00:27:46.560</a></span> | <span class="t">vector embeddings using things like hybrid search as well as things like splayed and using both of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1672" target="_blank">00:27:52.400</a></span> | <span class="t">those together we can actually get really cool results. So I think this is just one step towards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1679" target="_blank">00:27:59.440</a></span> | <span class="t">making vector search and information retrieval way more accessible because we no longer need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1687" target="_blank">00:28:07.120</a></span> | <span class="t">fine-tune all these really big models in order to get the best possible performance but we can use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1693" target="_blank">00:28:13.840</a></span> | <span class="t">things like hybrid search and things like splayed in order to really just improve our performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1699" target="_blank">00:28:19.840</a></span> | <span class="t">with very little effort which is a really good thing to see. But that's it for this video I hope</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1706" target="_blank">00:28:26.960</a></span> | <span class="t">everything we've been through is interesting and useful but for now that's it so thank you very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1712" target="_blank">00:28:32.240</a></span> | <span class="t">much for watching and I'll see you again in the next one. Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1720" target="_blank">00:28:40.960</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1721" target="_blank">00:28:41.460</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0FQ2WmM0t3w&t=1721" target="_blank">00:28:41.960</a></span> | <span class="t">you</span></div></div></body></html>