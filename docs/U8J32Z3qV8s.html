<html><head><title>Stanford CS25: V1 I Mixture of Experts (MoE) paradigm and the Switch Transformer</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V1 I Mixture of Experts (MoE) paradigm and the Switch Transformer</h2><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s"><img src="https://i.ytimg.com/vi/U8J32Z3qV8s/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=7">0:7</a> Scaling Transformers through Sparsity<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=25">0:25</a> Overall Motivation<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=60">1:0</a> Scaling Laws for Neural Language Models<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=305">5:5</a> Switch Transformer<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=443">7:23</a> Improved Training Methodology<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=507">8:27</a> Differentiable Load Balancing<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=535">8:55</a> Selected Precision<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=625">10:25</a> The Initialization Scale<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=983">16:23</a> Multi-Stage Routing Procedure<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1235">20:35</a> What Is the Research Question<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1790">29:50</a> Perplexity versus Strength Time<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1853">30:53</a> Spot Scaling Laws<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2230">37:10</a> Data Parallelism<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2256">37:36</a> Model Parallelism<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2331">38:51</a> Expert and Data Parallelism<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2371">39:31</a> Model Partitioning<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2437">40:37</a> Mesh Abstraction<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2778">46:18</a> Fine-Tuning Properties of Sparse Models<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2828">47:8</a> Multilingual Training<br><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2865">47:45</a> Distillation<br><br><div style="text-align: left;"><a href="./U8J32Z3qV8s.html">Whisper Transcript</a> | <a href="./transcript_U8J32Z3qV8s.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Today, Erwin and I are going to be giving a talk on scaling transformers through sparsity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=10" target="_blank">00:00:10.460</a></span> | <span class="t">And the kind of sparsity we're going to be talking about today is the kind where each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=13" target="_blank">00:00:13.820</a></span> | <span class="t">input can get either a different set of weights or have a different amount of computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=18" target="_blank">00:00:18.720</a></span> | <span class="t">applied to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=19" target="_blank">00:00:19.720</a></span> | <span class="t">Erwin, do you want to start it off?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=22" target="_blank">00:00:22.960</a></span> | <span class="t">So, I guess the overall motivation for this line of work is that the community has realized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=32" target="_blank">00:00:32.360</a></span> | <span class="t">that scale is perhaps one of the most important axes to focus on for obtaining strong performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=40" target="_blank">00:00:40.240</a></span> | <span class="t">And there's almost like this ongoing arms race right now with different labs and different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=45" target="_blank">00:00:45.720</a></span> | <span class="t">institutions competing for training the largest models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=53" target="_blank">00:00:53.440</a></span> | <span class="t">And so, maybe this dates back from early 2020 with a paper from OpenAI called Scaling Laws</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=61" target="_blank">00:01:01.360</a></span> | <span class="t">for Neural Language Models, where they find that model performance follows a predictable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=68" target="_blank">00:01:08.600</a></span> | <span class="t">power law, scale as a power law with model size in terms of either compute or just parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=82" target="_blank">00:01:22.080</a></span> | <span class="t">And so, this scaling law generalizes over multiple orders of magnitude, and that gives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=88" target="_blank">00:01:28.620</a></span> | <span class="t">us the confidence that if we are to train very large models, we can expect a certain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=95" target="_blank">00:01:35.060</a></span> | <span class="t">performance just by extrapolating these scaling laws.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=100" target="_blank">00:01:40.120</a></span> | <span class="t">So, in that paper, they also find the interesting observation that basically larger models are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=110" target="_blank">00:01:50.240</a></span> | <span class="t">more sample efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=112" target="_blank">00:01:52.200</a></span> | <span class="t">And so, if you have a fixed compute budget, you can predict what is the size, what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=123" target="_blank">00:02:03.120</a></span> | <span class="t">the optimal model size for a fixed compute budget.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=126" target="_blank">00:02:06.440</a></span> | <span class="t">And the overall observation is that you'd rather train very large models for less tests</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=135" target="_blank">00:02:15.640</a></span> | <span class="t">than train smaller models for more training steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=139" target="_blank">00:02:19.680</a></span> | <span class="t">And so, these models are scaled through basically the paper focuses on dense models, where you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=148" target="_blank">00:02:28.640</a></span> | <span class="t">just increase the model dimensions, but they're not looking at sparsity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=154" target="_blank">00:02:34.480</a></span> | <span class="t">And so, sparsity is a new dimension that you can use to scale architectures, and this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=160" target="_blank">00:02:40.540</a></span> | <span class="t">sort of the focus of the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=165" target="_blank">00:02:45.240</a></span> | <span class="t">And so, the sparsity we're mentioning here is basically you will have sparsely activated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=172" target="_blank">00:02:52.960</a></span> | <span class="t">weights based on the network inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=176" target="_blank">00:02:56.560</a></span> | <span class="t">So, every input will go to a roughly similar amount of computation, but will be applied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=182" target="_blank">00:03:02.640</a></span> | <span class="t">different weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=185" target="_blank">00:03:05.840</a></span> | <span class="t">And so, this dates back to 1991 with a paper called Adaptive Mixtures of Local Experts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=194" target="_blank">00:03:14.040</a></span> | <span class="t">and was recently revisited by Noam Shazier and colleagues at Google Brain with LSTMs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=201" target="_blank">00:03:21.720</a></span> | <span class="t">where they replaced sort of the feed-forward networks in LSTMs with a mixture of experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=210" target="_blank">00:03:30.800</a></span> | <span class="t">And so, the way this works there roughly is that you will have multiple experts each implementing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=217" target="_blank">00:03:37.880</a></span> | <span class="t">a small network, or in that case, I think just a dense matrix multiplication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=226" target="_blank">00:03:46.040</a></span> | <span class="t">And so, you have an additional gating network shown in green here that outputs a probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=234" target="_blank">00:03:54.160</a></span> | <span class="t">distribution over experts that each token should be sent to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=240" target="_blank">00:04:00.600</a></span> | <span class="t">So, this probability distribution is computed as a softmax, and once you have it, you select</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=249" target="_blank">00:04:09.080</a></span> | <span class="t">a few experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=251" target="_blank">00:04:11.080</a></span> | <span class="t">So, there are different strategies, maybe we'll talk about it later on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=256" target="_blank">00:04:16.120</a></span> | <span class="t">And the output is simply sort of the weighted mixture of all selected experts' outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=262" target="_blank">00:04:22.440</a></span> | <span class="t">So, they've been pretty successful primarily in translation, but there were some complexities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=278" target="_blank">00:04:38.160</a></span> | <span class="t">that hindered their broader use in NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=281" target="_blank">00:04:41.880</a></span> | <span class="t">And so, the Switch Transformer paper addresses some of those, and we'll be discussing how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=289" target="_blank">00:04:49.880</a></span> | <span class="t">to fix training instabilities or reduce communication costs and reduce model complexity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=296" target="_blank">00:04:56.960</a></span> | <span class="t">All right, Barrett, do you want to go?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=301" target="_blank">00:05:01.920</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=302" target="_blank">00:05:02.920</a></span> | <span class="t">So, one kind of approach that we're going to have for sparsity is the Switch Transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=307" target="_blank">00:05:07.520</a></span> | <span class="t">which is kind of like a simplified mixture of expert variant along with some other improved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=313" target="_blank">00:05:13.080</a></span> | <span class="t">training and fine-tuning techniques that allow it to be stably trained and also perform better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=319" target="_blank">00:05:19.560</a></span> | <span class="t">when fine-tuned on a lot of downstream tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=322" target="_blank">00:05:22.880</a></span> | <span class="t">And so, yeah, so the Switch Transformer kind of model works as the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=327" target="_blank">00:05:27.320</a></span> | <span class="t">So, you have some transformer model that has self-attention and feed-forward layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=332" target="_blank">00:05:32.880</a></span> | <span class="t">And the idea is that we replace maybe one every two or one every four feed-forward layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=337" target="_blank">00:05:37.240</a></span> | <span class="t">with a Switch Transformer layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=339" target="_blank">00:05:39.960</a></span> | <span class="t">So, you can see on the left is like one kind of layer block, which is self-attention, then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=346" target="_blank">00:05:46.360</a></span> | <span class="t">add normalize, then a feed-forward layer, then add normalize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=349" target="_blank">00:05:49.400</a></span> | <span class="t">And in this case, we're replacing the normal feed-forward layer with the Switch layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=353" target="_blank">00:05:53.920</a></span> | <span class="t">And we can see an illustration of this on the right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=357" target="_blank">00:05:57.000</a></span> | <span class="t">So, on the right, we can see that the layer has two inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=361" target="_blank">00:06:01.320</a></span> | <span class="t">One is the token more, the other is the token parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=364" target="_blank">00:06:04.480</a></span> | <span class="t">And we can see that these embedding representations will get sent to a router, which is exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=369" target="_blank">00:06:09.400</a></span> | <span class="t">how it works in the mixture of expert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=370" target="_blank">00:06:10.960</a></span> | <span class="t">So, the router is basically just going to be getting a distribution over all of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=375" target="_blank">00:06:15.240</a></span> | <span class="t">experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=376" target="_blank">00:06:16.240</a></span> | <span class="t">So, in this case, we can see that the highest probability is going to the expert number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=380" target="_blank">00:06:20.760</a></span> | <span class="t">two out of the four experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=383" target="_blank">00:06:23.080</a></span> | <span class="t">And then the right token is actually having the most probability on the first feed-forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=387" target="_blank">00:06:27.960</a></span> | <span class="t">weight, which is like the first expert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=390" target="_blank">00:06:30.040</a></span> | <span class="t">So, yeah, we can see here that what we're going to do is in the Switch Transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=393" target="_blank">00:06:33.760</a></span> | <span class="t">which is very simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=394" target="_blank">00:06:34.760</a></span> | <span class="t">It's just send it to the highest probability expert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=398" target="_blank">00:06:38.040</a></span> | <span class="t">And so, here we can see where the adaptive computation lies, where we'll have four sets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=402" target="_blank">00:06:42.120</a></span> | <span class="t">of weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=403" target="_blank">00:06:43.480</a></span> | <span class="t">There's some shared weights and computation across all the tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=406" target="_blank">00:06:46.560</a></span> | <span class="t">For example, the self-attention layer is computed exactly the same for the more token and for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=410" target="_blank">00:06:50.800</a></span> | <span class="t">the parameters token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=412" target="_blank">00:06:52.920</a></span> | <span class="t">But in the sparse Switch layer, we can see that actually the inputs are, while having</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=416" target="_blank">00:06:56.720</a></span> | <span class="t">the same amount of floating point operations applied to them, actually have different weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=420" target="_blank">00:07:00.360</a></span> | <span class="t">matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=423" target="_blank">00:07:03.680</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=425" target="_blank">00:07:05.160</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=426" target="_blank">00:07:06.160</a></span> | <span class="t">So, that's the kind of high-level idea with Switch Transformer, is that instead of sending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=430" target="_blank">00:07:10.920</a></span> | <span class="t">a token to multiple different experts, which can also increase the communication costs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=434" target="_blank">00:07:14.940</a></span> | <span class="t">as I'll go into a little bit later, it also just significantly simplifies the algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=438" target="_blank">00:07:18.920</a></span> | <span class="t">by just only sending it to one expert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=442" target="_blank">00:07:22.120</a></span> | <span class="t">So, for the improved training methodology, we focused on three different things to help</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=446" target="_blank">00:07:26.940</a></span> | <span class="t">improve the training of sparse models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=449" target="_blank">00:07:29.560</a></span> | <span class="t">The first was selected precision, which allows these sparse models to be trained in lower</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=453" target="_blank">00:07:33.400</a></span> | <span class="t">precision formats, which is incredibly important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=456" target="_blank">00:07:36.560</a></span> | <span class="t">Most of the models we train, we really don't want to be using float 32, because it's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=460" target="_blank">00:07:40.640</a></span> | <span class="t">slower to compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=461" target="_blank">00:07:41.640</a></span> | <span class="t">And also, when you're communicating tensors across different processes and stuff, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=465" target="_blank">00:07:45.680</a></span> | <span class="t">twice as slow, just because there's twice as many things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=469" target="_blank">00:07:49.120</a></span> | <span class="t">Also, we have some initialization tricks and some training tricks as well for allowing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=473" target="_blank">00:07:53.160</a></span> | <span class="t">them to be trained more stably, especially as the models grow in size, which is like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=477" target="_blank">00:07:57.200</a></span> | <span class="t">a new initialization method, along with a change to the learning rate schedule.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=482" target="_blank">00:08:02.800</a></span> | <span class="t">And third, since that our models have so many more parameters, we do notice definitely different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=487" target="_blank">00:08:07.260</a></span> | <span class="t">overfitting dynamics, especially once we fine tune these models that have been pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=492" target="_blank">00:08:12.160</a></span> | <span class="t">on all of the internet on these small tasks with maybe only 50 to 100,000 examples, that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=496" target="_blank">00:08:16.880</a></span> | <span class="t">they can be much more prone to overfitting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=499" target="_blank">00:08:19.080</a></span> | <span class="t">So we also look at some custom regularization to help prevent some of the overfitting that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=504" target="_blank">00:08:24.720</a></span> | <span class="t">we observe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=506" target="_blank">00:08:26.400</a></span> | <span class="t">And finally, we also talk about this differentiable load balancing technique we make, which kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=511" target="_blank">00:08:31.520</a></span> | <span class="t">of allows each expert to roughly get the same amount of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=516" target="_blank">00:08:36.200</a></span> | <span class="t">Because this is very important, especially given that we want the stuff to be efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=519" target="_blank">00:08:39.480</a></span> | <span class="t">on hardware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=520" target="_blank">00:08:40.480</a></span> | <span class="t">We want roughly each expert to have similar amounts of tokens sent to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=524" target="_blank">00:08:44.080</a></span> | <span class="t">And so to kind of encourage this, we tack on an additional load balancing loss along</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=529" target="_blank">00:08:49.040</a></span> | <span class="t">with our cross-entropy loss that we're training with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=532" target="_blank">00:08:52.580</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=533" target="_blank">00:08:53.580</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=534" target="_blank">00:08:54.580</a></span> | <span class="t">So here, I'm going to go into selected precision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=536" target="_blank">00:08:56.520</a></span> | <span class="t">So yeah, again, so when we're training large models, it's really important that we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=540" target="_blank">00:09:00.080</a></span> | <span class="t">be able to train them in lower precision formats.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=542" target="_blank">00:09:02.080</a></span> | <span class="t">So instead of each weight being an activation, being 32 bits, we want to shrink it down to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=547" target="_blank">00:09:07.400</a></span> | <span class="t">16 bits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=548" target="_blank">00:09:08.400</a></span> | <span class="t">And we use the bfloat16 representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=551" target="_blank">00:09:11.220</a></span> | <span class="t">And what we found out of the gate is that these models are just unstable, especially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=556" target="_blank">00:09:16.240</a></span> | <span class="t">the sparse models are much more unstable than the dense models in terms of you'll train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=559" target="_blank">00:09:19.700</a></span> | <span class="t">it for 10,000, 20,000 steps, and then the losses would just diverge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=563" target="_blank">00:09:23.040</a></span> | <span class="t">This was something that we frequently encountered.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=565" target="_blank">00:09:25.620</a></span> | <span class="t">And so one key thing that we found is that basically, you need to be casting a part of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=570" target="_blank">00:09:30.740</a></span> | <span class="t">the computation in float32 for these models to be able to be trained stably.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=578" target="_blank">00:09:38.180</a></span> | <span class="t">And the key component that we found that you need to cast is the router computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=582" target="_blank">00:09:42.840</a></span> | <span class="t">And essentially, we can go into the technical details a little bit more later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=587" target="_blank">00:09:47.060</a></span> | <span class="t">But basically, any time that there's these exponentiation functions, it's very important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=591" target="_blank">00:09:51.140</a></span> | <span class="t">that we are having higher and higher precision because of round off errors that can then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=596" target="_blank">00:09:56.720</a></span> | <span class="t">drastically change the output of some kind of exponentiation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=601" target="_blank">00:10:01.480</a></span> | <span class="t">So for example, if you have an exponentiation function and you change it by 0.1 or 0.2 or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=606" target="_blank">00:10:06.580</a></span> | <span class="t">0.3, this can drastically change the output of exponentiating it, especially depending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=611" target="_blank">00:10:11.860</a></span> | <span class="t">on how large the input is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=614" target="_blank">00:10:14.080</a></span> | <span class="t">So yeah, so this was a very important thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=615" target="_blank">00:10:15.940</a></span> | <span class="t">And it basically doesn't change the compute at all and allows the models to just be significantly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=619" target="_blank">00:10:19.440</a></span> | <span class="t">more stable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=620" target="_blank">00:10:20.440</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=624" target="_blank">00:10:24.060</a></span> | <span class="t">So the second thing we looked at is also the initialization scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=627" target="_blank">00:10:27.220</a></span> | <span class="t">So like the standard way that we were initializing these models, we found to also just make the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=631" target="_blank">00:10:31.980</a></span> | <span class="t">models much more prone to being unstable and/or just performing worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=636" target="_blank">00:10:36.260</a></span> | <span class="t">So one thing that we did that we found was very effective was to just simply make the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=640" target="_blank">00:10:40.380</a></span> | <span class="t">initialization scale much smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=642" target="_blank">00:10:42.460</a></span> | <span class="t">And when we did this, we found that the quality just drastically improved, and it was like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=646" target="_blank">00:10:46.300</a></span> | <span class="t">a very simple fix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=648" target="_blank">00:10:48.940</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=651" target="_blank">00:10:51.840</a></span> | <span class="t">And the third thing I mentioned, where since we noticed that these models are much more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=655" target="_blank">00:10:55.300</a></span> | <span class="t">prone to overfitting, since they just have significantly more parameters, is that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=659" target="_blank">00:10:59.600</a></span> | <span class="t">also use much more dropout for the expert layers only.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=663" target="_blank">00:11:03.100</a></span> | <span class="t">So here we can see we have the T5 base, which is a dense model, and then we have a bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=668" target="_blank">00:11:08.320</a></span> | <span class="t">of different switch variants on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=670" target="_blank">00:11:10.120</a></span> | <span class="t">And we found to be the most effective on these four different fine-tuning tasks was just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=673" target="_blank">00:11:13.920</a></span> | <span class="t">to really significantly increase the dropout rate inside the expert layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=677" target="_blank">00:11:17.800</a></span> | <span class="t">And we found that this was pretty effective for combating the overfitting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=681" target="_blank">00:11:21.160</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=682" target="_blank">00:11:22.160</a></span> | <span class="t">Better?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=683" target="_blank">00:11:23.160</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=684" target="_blank">00:11:24.160</a></span> | <span class="t">We have a question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=685" target="_blank">00:11:25.160</a></span> | <span class="t">Oh, awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=686" target="_blank">00:11:26.160</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=687" target="_blank">00:11:27.160</a></span> | <span class="t">For one of the students.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=688" target="_blank">00:11:28.160</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=689" target="_blank">00:11:29.160</a></span> | <span class="t">OK, let me take a look.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=690" target="_blank">00:11:30.160</a></span> | <span class="t">Do you want to go ahead?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=691" target="_blank">00:11:31.160</a></span> | <span class="t">Yeah, I can ask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=692" target="_blank">00:11:32.160</a></span> | <span class="t">It was just in reference to the previous table where you have throughput and precision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=696" target="_blank">00:11:36.880</a></span> | <span class="t">It just seems surprising to me that you could match this 1390 number using selective precision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=703" target="_blank">00:11:43.320</a></span> | <span class="t">It seems like I would expect it to be something in between.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=706" target="_blank">00:11:46.960</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=707" target="_blank">00:11:47.960</a></span> | <span class="t">So it essentially comes down to the fact that there's maybe a little bit of noise sampled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=712" target="_blank">00:11:52.320</a></span> | <span class="t">with the speed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=713" target="_blank">00:11:53.440</a></span> | <span class="t">And the only part we're casting is the router, which is maybe such an insignificant portion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=718" target="_blank">00:11:58.720</a></span> | <span class="t">of the computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=720" target="_blank">00:12:00.240</a></span> | <span class="t">And there's zero communication there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=721" target="_blank">00:12:01.600</a></span> | <span class="t">That is essentially like a free operation in the network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=724" target="_blank">00:12:04.140</a></span> | <span class="t">So whether you cast it to VFLOW16 or FLOW32, it doesn't actually impact the speed at all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=728" target="_blank">00:12:08.720</a></span> | <span class="t">within the precision that we can actually measure the speed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=732" target="_blank">00:12:12.320</a></span> | <span class="t">And also, these architectures only use sparse layer once, one every four layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=739" target="_blank">00:12:19.240</a></span> | <span class="t">And so, yeah, essentially, the FLOW32 part is kind of very negligible in the entire architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=747" target="_blank">00:12:27.320</a></span> | <span class="t">It's like, for example, I think off the top of my head, it's like 1/40th the computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=752" target="_blank">00:12:32.360</a></span> | <span class="t">that would cost for you to do the first weight matrix multiply in a dense, ReLU dense layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=758" target="_blank">00:12:38.040</a></span> | <span class="t">or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=759" target="_blank">00:12:39.040</a></span> | <span class="t">So it's a very, very small part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=760" target="_blank">00:12:40.040</a></span> | <span class="t">And yeah, we're not using them very frequently, like Erwin mentioned as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=764" target="_blank">00:12:44.000</a></span> | <span class="t">Got it, OK, thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=768" target="_blank">00:12:48.800</a></span> | <span class="t">Yeah, and then just a quick point on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=771" target="_blank">00:12:51.480</a></span> | <span class="t">I won't go into some of the technical details, but yeah, we definitely-- since we're training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=774" target="_blank">00:12:54.960</a></span> | <span class="t">these things on hardware, we really-- I think a big part of the mixture of experts paradigm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=778" target="_blank">00:12:58.660</a></span> | <span class="t">is that these things are designed such that it maps really efficiently to hardware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=783" target="_blank">00:13:03.680</a></span> | <span class="t">So we want to be doing dense matrix multiplies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=786" target="_blank">00:13:06.360</a></span> | <span class="t">And for this to work really well, we also want to be able to have roughly equal amount</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=790" target="_blank">00:13:10.280</a></span> | <span class="t">of tokens going to each of the different experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=793" target="_blank">00:13:13.740</a></span> | <span class="t">And I think this isn't that sensitive to the load balancing formulation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=797" target="_blank">00:13:17.800</a></span> | <span class="t">Like, we tried a few things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=798" target="_blank">00:13:18.800</a></span> | <span class="t">A lot of them worked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=799" target="_blank">00:13:19.800</a></span> | <span class="t">But yeah, essentially, you definitely want some kind of load balancing loss added on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=803" target="_blank">00:13:23.640</a></span> | <span class="t">when using sparsity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=805" target="_blank">00:13:25.040</a></span> | <span class="t">Yeah, next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=807" target="_blank">00:13:27.640</a></span> | <span class="t">Yeah, Erwin, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=811" target="_blank">00:13:31.960</a></span> | <span class="t">Yeah, so the frameworks, the library we use rely on static shapes for-- OK, yeah, so XLA,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=835" target="_blank">00:13:55.020</a></span> | <span class="t">so the compiler for TensorFlow and MeshTensorFlow expects static shapes for tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=843" target="_blank">00:14:03.220</a></span> | <span class="t">However, the computations in switch transformers are dynamic because of the router, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=851" target="_blank">00:14:11.980</a></span> | <span class="t">Different inputs will be routed to different experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=855" target="_blank">00:14:15.700</a></span> | <span class="t">And so we need to specify ahead of time how many tokens will be sent to each expert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=862" target="_blank">00:14:22.580</a></span> | <span class="t">And so we will introduce this expert capacity hyperparameter to specify that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=869" target="_blank">00:14:29.120</a></span> | <span class="t">And that's going to be a static number which says how many tokens each expert can process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=877" target="_blank">00:14:37.960</a></span> | <span class="t">And so in practice, we instead parametrize this by having a quantity called the capacity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=883" target="_blank">00:14:43.300</a></span> | <span class="t">factor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=884" target="_blank">00:14:44.300</a></span> | <span class="t">And so we have an example here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=887" target="_blank">00:14:47.940</a></span> | <span class="t">So the bottom row is a bunch of tokens on one device.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=896" target="_blank">00:14:56.660</a></span> | <span class="t">And then you need to sort of route those tokens to multiple devices or multiple experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=902" target="_blank">00:15:02.880</a></span> | <span class="t">So if too many tokens are routed to a single expert, some tokens will be dropped because,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=908" target="_blank">00:15:08.740</a></span> | <span class="t">as we said, experts have a fixed capacity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=914" target="_blank">00:15:14.000</a></span> | <span class="t">So that's the example on the left where the capacity factor is one, and that basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=917" target="_blank">00:15:17.900</a></span> | <span class="t">means that there's no extra buffer for routing tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=927" target="_blank">00:15:27.180</a></span> | <span class="t">So instead of that, we can use the capacity factor that's larger than one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=930" target="_blank">00:15:30.780</a></span> | <span class="t">So on the right, you have an example with 1.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=934" target="_blank">00:15:34.740</a></span> | <span class="t">So that means that now each expert has three slots that can process three tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=942" target="_blank">00:15:42.580</a></span> | <span class="t">And so that prevents token dropping because we have more capacity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=946" target="_blank">00:15:46.920</a></span> | <span class="t">But the issue is that this means more expensive communication across devices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=962" target="_blank">00:16:02.860</a></span> | <span class="t">One thing that we also experimented with was this method called no token left behind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=967" target="_blank">00:16:07.820</a></span> | <span class="t">And the idea was the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=969" target="_blank">00:16:09.000</a></span> | <span class="t">So since we have to have a fixed batch size for each expert, and there can be token dropping,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=975" target="_blank">00:16:15.700</a></span> | <span class="t">we're thinking that, hey, yeah, having tokens dropped or having some tokens not having any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=979" target="_blank">00:16:19.860</a></span> | <span class="t">computation applied to it is probably hurting the model performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=983" target="_blank">00:16:23.740</a></span> | <span class="t">So what if we do a multistage routing procedure?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=986" target="_blank">00:16:26.060</a></span> | <span class="t">So first, you do the normal routing where it's like you send each token to its highest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=989" target="_blank">00:16:29.500</a></span> | <span class="t">probability expert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=990" target="_blank">00:16:30.900</a></span> | <span class="t">But then any dropped tokens, you then send to their second highest probability expert,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=996" target="_blank">00:16:36.460</a></span> | <span class="t">and so forth and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=997" target="_blank">00:16:37.460</a></span> | <span class="t">Or you can basically repeat this process to guarantee that no tokens are being dropped.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1001" target="_blank">00:16:41.540</a></span> | <span class="t">Interestingly, actually, this approach didn't empirically improve model performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1006" target="_blank">00:16:46.020</a></span> | <span class="t">If anything, it actually kind of hurt it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1008" target="_blank">00:16:48.420</a></span> | <span class="t">And we thought that was actually very interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1011" target="_blank">00:16:51.060</a></span> | <span class="t">And I think the intuition is that, you know, once the model learns it wants to send a token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1014" target="_blank">00:16:54.380</a></span> | <span class="t">to one expert, like it really wants to have that computation applied to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1018" target="_blank">00:16:58.060</a></span> | <span class="t">And just applying some other computation doesn't, you know, have at all the same property, along</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1023" target="_blank">00:17:03.460</a></span> | <span class="t">with it actually maybe being potentially detrimental.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1026" target="_blank">00:17:06.400</a></span> | <span class="t">So yeah, we thought that was pretty interesting, as we were very optimistic this would potentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1030" target="_blank">00:17:10.240</a></span> | <span class="t">you know, get improved performance, but it ended up not really making a difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1033" target="_blank">00:17:13.340</a></span> | <span class="t">And we found this quite surprising.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1035" target="_blank">00:17:15.380</a></span> | <span class="t">We have a question from…</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1041" target="_blank">00:17:21.220</a></span> | <span class="t">I think it will actually kind of like address literally the last point that you brought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1045" target="_blank">00:17:25.380</a></span> | <span class="t">up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1046" target="_blank">00:17:26.380</a></span> | <span class="t">I think when I think about like a mixture of experts, usually like they specialize in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1051" target="_blank">00:17:31.340</a></span> | <span class="t">like different things, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1053" target="_blank">00:17:33.140</a></span> | <span class="t">So I think it was like, just like a lot, like I was just wondering, like if you send it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1061" target="_blank">00:17:41.740</a></span> | <span class="t">to like the second best or whatever, like what if like all of your tokens would be particularly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1069" target="_blank">00:17:49.140</a></span> | <span class="t">good for like one expert, and then you only like process, let's say, like 20% of your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1075" target="_blank">00:17:55.000</a></span> | <span class="t">tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1077" target="_blank">00:17:57.000</a></span> | <span class="t">So that ends up being better than rerouting them to anything else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1082" target="_blank">00:18:02.080</a></span> | <span class="t">Exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1083" target="_blank">00:18:03.080</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1084" target="_blank">00:18:04.080</a></span> | <span class="t">So yeah, even if you're dropping a lot of tokens, it's not beneficial to be sending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1086" target="_blank">00:18:06.760</a></span> | <span class="t">them to the second, third or fourth best thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1089" target="_blank">00:18:09.440</a></span> | <span class="t">And one actually interesting property that we, you know, noticed about these models is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1092" target="_blank">00:18:12.380</a></span> | <span class="t">they're surprisingly robust to token dropping, especially during fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1096" target="_blank">00:18:16.980</a></span> | <span class="t">So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1097" target="_blank">00:18:17.980</a></span> | <span class="t">So in the standard paradigm, what we'll do is we'll pre-train this thing, we'll have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1099" target="_blank">00:18:19.680</a></span> | <span class="t">some load balancing loss, which makes the tokens pretty balanced actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1104" target="_blank">00:18:24.480</a></span> | <span class="t">But then during fine tuning, where it's like, we really want to fine tune it on a specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1107" target="_blank">00:18:27.640</a></span> | <span class="t">task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1108" target="_blank">00:18:28.640</a></span> | <span class="t">We actually studied this exact question and we were studying, does it help to have a load</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1111" target="_blank">00:18:31.780</a></span> | <span class="t">balancing loss during fine tuning or not?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1114" target="_blank">00:18:34.360</a></span> | <span class="t">And so if you have the load balancing loss, yeah, that kind of is encouraging, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1117" target="_blank">00:18:37.560</a></span> | <span class="t">for the specific task, we want to try to have, you know, all the experts be used versus turning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1121" target="_blank">00:18:41.240</a></span> | <span class="t">it off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1122" target="_blank">00:18:42.240</a></span> | <span class="t">Whereas there's definitely some, you know, prior specialization and it's actually much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1125" target="_blank">00:18:45.600</a></span> | <span class="t">better to just turn the auxiliary loss off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1127" target="_blank">00:18:47.760</a></span> | <span class="t">And even if it's like, you know, 60 to 70% of the tokens are being dropped, that actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1131" target="_blank">00:18:51.760</a></span> | <span class="t">performs much better than, you know, having all the tokens balanced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1135" target="_blank">00:18:55.520</a></span> | <span class="t">But doesn't a load balancing loss encourage basically all the experts to learn very similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1140" target="_blank">00:19:00.920</a></span> | <span class="t">weights and then just randomly assign your tokens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1145" target="_blank">00:19:05.320</a></span> | <span class="t">Because then it doesn't matter to which expert stuff is being sent to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1148" target="_blank">00:19:08.980</a></span> | <span class="t">So when we use the load balancing loss, like the routing mechanism is definitely learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1152" target="_blank">00:19:12.120</a></span> | <span class="t">So the model definitely is encouraged to, you know, choose an expert that it wants to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1156" target="_blank">00:19:16.520</a></span> | <span class="t">send it to for good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1157" target="_blank">00:19:17.520</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1158" target="_blank">00:19:18.520</a></span> | <span class="t">But like if all the experts learn the same weights, then the router learns basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1163" target="_blank">00:19:23.760</a></span> | <span class="t">oh, it doesn't matter where I send it to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1166" target="_blank">00:19:26.720</a></span> | <span class="t">So if you encourage load balancing, you encourage technically that like you want any loss to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1172" target="_blank">00:19:32.760</a></span> | <span class="t">fit with any expert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1173" target="_blank">00:19:33.760</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1174" target="_blank">00:19:34.760</a></span> | <span class="t">I mean, that's maybe the extreme behavior if you have a very high sort of load balancing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1179" target="_blank">00:19:39.320</a></span> | <span class="t">loss coefficient, but in practice that coefficient is kind of tuned and we observe that for,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1184" target="_blank">00:19:44.520</a></span> | <span class="t">you know, small enough values, the router still learns like semantic, like meaningful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1191" target="_blank">00:19:51.040</a></span> | <span class="t">routing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1192" target="_blank">00:19:52.040</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1193" target="_blank">00:19:53.040</a></span> | <span class="t">Because it's like a balance between this, like, you know, cross entropy loss and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1195" target="_blank">00:19:55.920</a></span> | <span class="t">load balancing loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1197" target="_blank">00:19:57.440</a></span> | <span class="t">And so on one hand, yeah, you definitely want to encourage the model to be balanced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1200" target="_blank">00:20:00.880</a></span> | <span class="t">Then on the other hand, you also want to just get good empirical performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1204" target="_blank">00:20:04.760</a></span> | <span class="t">And yeah, the model is able to definitely like on one hand, learn and specialize the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1208" target="_blank">00:20:08.680</a></span> | <span class="t">experts where they have different weights such that it's like, you know, definitely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1211" target="_blank">00:20:11.640</a></span> | <span class="t">it expects certain tokens to be sent to certain experts, but on the other hand, still be reasonably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1214" target="_blank">00:20:14.960</a></span> | <span class="t">balanced so that the models are efficiently run on like modern hardware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1218" target="_blank">00:20:18.800</a></span> | <span class="t">Exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1219" target="_blank">00:20:19.800</a></span> | <span class="t">We also have a question from the classroom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1224" target="_blank">00:20:24.080</a></span> | <span class="t">So the question that I want to ask is, it seems to me like this is a very experimental</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1228" target="_blank">00:20:28.480</a></span> | <span class="t">talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1229" target="_blank">00:20:29.480</a></span> | <span class="t">We're talking about floating point precision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1230" target="_blank">00:20:30.480</a></span> | <span class="t">We're talking about different approaches and currently work well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1231" target="_blank">00:20:31.480</a></span> | <span class="t">And whenever we're dealing with clients, there's a question of what is the research question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1237" target="_blank">00:20:37.440</a></span> | <span class="t">And I feel like I missed that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1239" target="_blank">00:20:39.040</a></span> | <span class="t">So what are we trying to answer with all these experiments?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1243" target="_blank">00:20:43.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1244" target="_blank">00:20:44.000</a></span> | <span class="t">I think the, I think the high level of research question is like, you know, can we, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1248" target="_blank">00:20:48.800</a></span> | <span class="t">create models that are, you know, like doing adaptive computation from the standpoint of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1253" target="_blank">00:20:53.920</a></span> | <span class="t">like, no, can we try to make models more simulate the dynamics that we think models should most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1258" target="_blank">00:20:58.760</a></span> | <span class="t">naturally use, which is different inputs to have different amounts of computation applied,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1263" target="_blank">00:21:03.040</a></span> | <span class="t">have different weights applied to them, you know, and basically all of this, basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1266" target="_blank">00:21:06.080</a></span> | <span class="t">we're trying to research and like figure out how can we create like a new framework for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1269" target="_blank">00:21:09.560</a></span> | <span class="t">these models to be trained as opposed to their dense counterparts that, you know, for every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1273" target="_blank">00:21:13.440</a></span> | <span class="t">input are always having the same exact computation applied.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1277" target="_blank">00:21:17.040</a></span> | <span class="t">So that's interesting because when you say the same exact computation applied, one might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1281" target="_blank">00:21:21.040</a></span> | <span class="t">imagine that like, to me, the immediate thing is about how long to deliberate about something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1286" target="_blank">00:21:26.960</a></span> | <span class="t">What I mean by that is if we want to have variable length computation, you could imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1291" target="_blank">00:21:31.360</a></span> | <span class="t">that I could have a short amount of computation or it could have much older computation, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1295" target="_blank">00:21:35.280</a></span> | <span class="t">there's like, you have like, why then do we instead consider the dimension of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1299" target="_blank">00:21:39.480</a></span> | <span class="t">computation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1300" target="_blank">00:21:40.480</a></span> | <span class="t">I mean, assuming of course that these experts do indeed learn different things, which I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1303" target="_blank">00:21:43.480</a></span> | <span class="t">think you'll get to in a minute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1305" target="_blank">00:21:45.320</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1306" target="_blank">00:21:46.320</a></span> | <span class="t">So why do we immediately jump to thinking about specialized experts as opposed to thinking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1310" target="_blank">00:21:50.880</a></span> | <span class="t">about variable length computation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1312" target="_blank">00:21:52.640</a></span> | <span class="t">So, yeah, so this is actually, we actually go into some variable length computation stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1317" target="_blank">00:21:57.240</a></span> | <span class="t">later in the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1318" target="_blank">00:21:58.240</a></span> | <span class="t">And I feel like they're both actually just important axes that should both be pushed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1321" target="_blank">00:22:01.720</a></span> | <span class="t">on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1322" target="_blank">00:22:02.720</a></span> | <span class="t">I think, I guess, yeah, I guess it's kind of, you know, yeah, I'm not afraid of my question,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1327" target="_blank">00:22:07.800</a></span> | <span class="t">but what I'm trying to understand is you're thinking about why did you decide to attack</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1330" target="_blank">00:22:10.760</a></span> | <span class="t">this one first?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1331" target="_blank">00:22:11.760</a></span> | <span class="t">I want to understand why your team chose to go this direction first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1334" target="_blank">00:22:14.560</a></span> | <span class="t">Yeah, absolutely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1335" target="_blank">00:22:15.720</a></span> | <span class="t">So I think that one empirically, it seems that sparsity has led to better empirical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1340" target="_blank">00:22:20.880</a></span> | <span class="t">results in the field of deep learning than adaptive computation so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1343" target="_blank">00:22:23.960</a></span> | <span class="t">And I think the way that we use these things maps really well to our modern hardware, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1348" target="_blank">00:22:28.320</a></span> | <span class="t">is also very promising.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1349" target="_blank">00:22:29.820</a></span> | <span class="t">And I think the way we were kind of looking at it as like sparsity is like a first step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1352" target="_blank">00:22:32.840</a></span> | <span class="t">towards doing more interesting and general adaptive computation where, and we're, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1357" target="_blank">00:22:37.320</a></span> | <span class="t">you know, cause I think it's like, you know, this stuff is complicated and typically starting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1361" target="_blank">00:22:41.000</a></span> | <span class="t">from something that works well is better than necessarily like, you know, you know, trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1365" target="_blank">00:22:45.720</a></span> | <span class="t">something that's not necessarily as proven out and then trying to like get it to work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1369" target="_blank">00:22:49.080</a></span> | <span class="t">really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1370" target="_blank">00:22:50.080</a></span> | <span class="t">So I think we're kind of starting from sparsity, which like, you know, Noam Shazier and others</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1373" target="_blank">00:22:53.360</a></span> | <span class="t">got to work really well in the context of LSTMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1375" target="_blank">00:22:55.600</a></span> | <span class="t">We were kind of interested in, you know, let's port some of this to transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1378" target="_blank">00:22:58.920</a></span> | <span class="t">Let's get it working really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1379" target="_blank">00:22:59.920</a></span> | <span class="t">And then let's slowly start expanding towards a lot of the other natural questions that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1383" target="_blank">00:23:03.080</a></span> | <span class="t">you mentioned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1384" target="_blank">00:23:04.080</a></span> | <span class="t">Whereas like, okay, whereas instead of, you know, different weights per core, let's also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1387" target="_blank">00:23:07.640</a></span> | <span class="t">maybe have different computation per core and all of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1390" target="_blank">00:23:10.320</a></span> | <span class="t">So that's, I guess how we were kind of building the natural, like, you know, buildup and progression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1394" target="_blank">00:23:14.240</a></span> | <span class="t">of our research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1395" target="_blank">00:23:15.240</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1396" target="_blank">00:23:16.240</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1397" target="_blank">00:23:17.240</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1398" target="_blank">00:23:18.240</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1399" target="_blank">00:23:19.240</a></span> | <span class="t">What do you think Erwin?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1401" target="_blank">00:23:21.280</a></span> | <span class="t">Anything else to add?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1402" target="_blank">00:23:22.280</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1403" target="_blank">00:23:23.280</a></span> | <span class="t">I mean, I guess I kind of see adaptive computation and sparsity as, you know, related, but separate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1411" target="_blank">00:23:31.520</a></span> | <span class="t">things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1412" target="_blank">00:23:32.520</a></span> | <span class="t">So, you know, sparsity is more like different parameters for each example and adaptive computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1416" target="_blank">00:23:36.960</a></span> | <span class="t">might be more different amount of flops and we have some of that with the token dropping,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1422" target="_blank">00:23:42.560</a></span> | <span class="t">but that's kind of, you know, that's not the main motivation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1429" target="_blank">00:23:49.200</a></span> | <span class="t">Definitely as Barrett mentioned, I would say, you know, no one really has figured out adaptive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1436" target="_blank">00:23:56.440</a></span> | <span class="t">computation yet for deep learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1439" target="_blank">00:23:59.200</a></span> | <span class="t">And one reason is because we have these, you know, accelerators, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1445" target="_blank">00:24:05.640</a></span> | <span class="t">Expect like sort of, you know, we need to work with like batch, like data parallelism,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1451" target="_blank">00:24:11.600</a></span> | <span class="t">right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1452" target="_blank">00:24:12.600</a></span> | <span class="t">So, and all of our accelerators and our frameworks use this SPMD paradigm where we're kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1459" target="_blank">00:24:19.320</a></span> | <span class="t">supposed to apply the same computation to examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1465" target="_blank">00:24:25.200</a></span> | <span class="t">And so if you look at the literature, you have, you know, works like universal transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1470" target="_blank">00:24:30.880</a></span> | <span class="t">where they replace the feed forward in the transformer by just a recurrent weight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1476" target="_blank">00:24:36.240</a></span> | <span class="t">And so it's kind of like an LSTM on each token and the LSTM can stop at different times based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1482" target="_blank">00:24:42.760</a></span> | <span class="t">on some criteria.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1485" target="_blank">00:24:45.440</a></span> | <span class="t">But the way these things are implemented is just through masking because it needs to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1491" target="_blank">00:24:51.320</a></span> | <span class="t">implemented in the SPMD programming style.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1495" target="_blank">00:24:55.520</a></span> | <span class="t">And so definitely sparsity was kind of like easier to get to work first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1499" target="_blank">00:24:59.800</a></span> | <span class="t">And also there were some prior results with LSTM, so yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1506" target="_blank">00:25:06.600</a></span> | <span class="t">In terms of like the first question, you know, sort of what's the research question here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1509" target="_blank">00:25:09.880</a></span> | <span class="t">is just like, oh, can we design more efficient models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1513" target="_blank">00:25:13.400</a></span> | <span class="t">And sparsity is this new axis that hasn't been explored that much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1517" target="_blank">00:25:17.240</a></span> | <span class="t">And yeah, I think that, you know, I'm happy with just that being the research question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1523" target="_blank">00:25:23.640</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1524" target="_blank">00:25:24.640</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1525" target="_blank">00:25:25.640</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1526" target="_blank">00:25:26.640</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1527" target="_blank">00:25:27.640</a></span> | <span class="t">So next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1528" target="_blank">00:25:28.640</a></span> | <span class="t">Yep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1529" target="_blank">00:25:29.640</a></span> | <span class="t">Oops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1530" target="_blank">00:25:30.640</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1531" target="_blank">00:25:31.640</a></span> | <span class="t">Again, so kind of putting it all together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1535" target="_blank">00:25:35.440</a></span> | <span class="t">So the switch transformer layer selects an expert, like just the top expert, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1540" target="_blank">00:25:40.160</a></span> | <span class="t">incorporates a bunch of the general sparse model improvements to, you know, allow it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1544" target="_blank">00:25:44.360</a></span> | <span class="t">to fine tune better, allow it to, you know, be more regularized, allow it to, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1549" target="_blank">00:25:49.840</a></span> | <span class="t">be trained with lower precision formats and a lot of like technical details to just get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1552" target="_blank">00:25:52.520</a></span> | <span class="t">them training and working well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1555" target="_blank">00:25:55.680</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1558" target="_blank">00:25:58.120</a></span> | <span class="t">So one thing that we also wanted to do was a comparison between like top one and top</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1562" target="_blank">00:26:02.200</a></span> | <span class="t">two routing since top two routing was kind of the, you know, most popular technique.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1567" target="_blank">00:26:07.760</a></span> | <span class="t">And so here we can see we have two different dense models trained of different sizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1570" target="_blank">00:26:10.400</a></span> | <span class="t">And we're going to be looking at like the, the pre-training like negative log perplexity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1574" target="_blank">00:26:14.640</a></span> | <span class="t">So yeah, the bigger the number, the better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1579" target="_blank">00:26:19.300</a></span> | <span class="t">So next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1580" target="_blank">00:26:20.900</a></span> | <span class="t">So, so, and what we're going to be doing is we're going to be studying them at different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1584" target="_blank">00:26:24.560</a></span> | <span class="t">capacity factors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1586" target="_blank">00:26:26.140</a></span> | <span class="t">So a capacity factor of 2.0 basically means that there's enough buffer for two tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1590" target="_blank">00:26:30.000</a></span> | <span class="t">to be sent to every single expert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1592" target="_blank">00:26:32.560</a></span> | <span class="t">And we're going to be comparing like top one versus top two routing and also comparing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1596" target="_blank">00:26:36.280</a></span> | <span class="t">their speeds along with their like time to get some like threshold quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1600" target="_blank">00:26:40.560</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1601" target="_blank">00:26:41.560</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1602" target="_blank">00:26:42.560</a></span> | <span class="t">So here we can see in the capacity factor 2.0 case that the MOE models outperform switch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1608" target="_blank">00:26:48.800</a></span> | <span class="t">transformer, which makes a lot of sense, like since switch transformer is only, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1612" target="_blank">00:26:52.800</a></span> | <span class="t">sending like a top one token to each expert, the mixture of expert is sending, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1617" target="_blank">00:26:57.920</a></span> | <span class="t">two tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1618" target="_blank">00:26:58.920</a></span> | <span class="t">So that makes sense that this extra buffer will be like disproportionately beneficial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1621" target="_blank">00:27:01.960</a></span> | <span class="t">for the mixture of expert models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1624" target="_blank">00:27:04.120</a></span> | <span class="t">And so we noticed that and next slide or next now, when we, so the really interesting parts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1631" target="_blank">00:27:11.020</a></span> | <span class="t">for the top one routing becomes when we lower the capacity factors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1634" target="_blank">00:27:14.940</a></span> | <span class="t">So having a high capacity factor is bad for many reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1637" target="_blank">00:27:17.800</a></span> | <span class="t">One of which is it really incurs more of these, you know, communication costs for sending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1642" target="_blank">00:27:22.320</a></span> | <span class="t">tokens to the correct experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1644" target="_blank">00:27:24.280</a></span> | <span class="t">It also incurs more compute costs and also incurs like a lot of memory overhead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1648" target="_blank">00:27:28.280</a></span> | <span class="t">So if you can get this lower, it's, it's usually like a very, very good thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1652" target="_blank">00:27:32.760</a></span> | <span class="t">And so what we see here is that switch transformer actually outperforms mixture of experts when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1657" target="_blank">00:27:37.460</a></span> | <span class="t">you have like a lower capacity factor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1660" target="_blank">00:27:40.400</a></span> | <span class="t">And we can see that the time to quality threshold, we you know, yeah, we, we get there much quicker.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1666" target="_blank">00:27:46.500</a></span> | <span class="t">And so even across the 2.0 and the 1.25 capacity factors, like the kind of Pareto optimal thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1671" target="_blank">00:27:51.460</a></span> | <span class="t">we saw in our setup is to use switch transformer at a lower capacity factor, just due to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1676" target="_blank">00:27:56.600</a></span> | <span class="t">fact that while the quality is worse, a little bit worse on a step basis, it's just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1680" target="_blank">00:28:00.440</a></span> | <span class="t">much faster to run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1681" target="_blank">00:28:01.520</a></span> | <span class="t">So it's kind of the Pareto optimal decision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1685" target="_blank">00:28:05.220</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1686" target="_blank">00:28:06.220</a></span> | <span class="t">And we can also be seeing that like for capacity factor 1.0, again, we can see that this really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1691" target="_blank">00:28:11.660</a></span> | <span class="t">disproportionately benefits switch transformer and is even better on a Pareto standpoint</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1697" target="_blank">00:28:17.020</a></span> | <span class="t">than the 1.25 capacity factors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1700" target="_blank">00:28:20.060</a></span> | <span class="t">And interestingly, since, you know, MOE also does like a little bit more computation, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1704" target="_blank">00:28:24.000</a></span> | <span class="t">can also just increase the amount of compute done elsewhere in the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1708" target="_blank">00:28:28.100</a></span> | <span class="t">And we can see that that's like a much more efficient allocation of compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1711" target="_blank">00:28:31.740</a></span> | <span class="t">So yeah, overall, our takeaway is that, yeah, lower capacity factors using op one routing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1717" target="_blank">00:28:37.240</a></span> | <span class="t">is more Pareto efficient than, you know, using like the top two routing at higher capacity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1722" target="_blank">00:28:42.220</a></span> | <span class="t">factors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1723" target="_blank">00:28:43.220</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1724" target="_blank">00:28:44.220</a></span> | <span class="t">Erwin, you can take it over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1727" target="_blank">00:28:47.980</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1728" target="_blank">00:28:48.980</a></span> | <span class="t">So next we'll look at how a switch transformer scales as a function of the number of exports</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1735" target="_blank">00:28:55.700</a></span> | <span class="t">in the switch layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1739" target="_blank">00:28:59.220</a></span> | <span class="t">And so on the right side here, you see a plot that shows perplexity versus training steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1746" target="_blank">00:29:06.140</a></span> | <span class="t">for different switch architectures, ranging from T5 base, which is basically no export</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1752" target="_blank">00:29:12.860</a></span> | <span class="t">or a single export up to 128 exports.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1757" target="_blank">00:29:17.940</a></span> | <span class="t">And so you see that as we increase the number of exports, which also increases the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1761" target="_blank">00:29:21.820</a></span> | <span class="t">of parameters, of sparse parameters, you get sort of speed ups, you know, you get increasing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1770" target="_blank">00:29:30.840</a></span> | <span class="t">speed ups over the dense baseline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1772" target="_blank">00:29:32.940</a></span> | <span class="t">And they're like sort of diminishing returns to, you know, multiplying to, you know, increasing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1779" target="_blank">00:29:39.000</a></span> | <span class="t">the number of exports as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1784" target="_blank">00:29:44.580</a></span> | <span class="t">So the previous figure was looking at perplexity versus training steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1790" target="_blank">00:29:50.780</a></span> | <span class="t">Here we look at perplexity versus strength time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1794" target="_blank">00:29:54.780</a></span> | <span class="t">So that includes, you know, all the, you know, additional communication costs when you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1800" target="_blank">00:30:00.900</a></span> | <span class="t">more exports or, you know, comparing to the dense baseline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1807" target="_blank">00:30:07.700</a></span> | <span class="t">And so this is for switch base or T5 base, and we observe up to 7x speedups over T5 base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1817" target="_blank">00:30:17.580</a></span> | <span class="t">And so, you know, just to maybe contextualize these numbers, like, you know, 7x speedups</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1824" target="_blank">00:30:24.740</a></span> | <span class="t">in deep learning are pretty hard to obtain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1826" target="_blank">00:30:26.620</a></span> | <span class="t">And so I think this is one of the, you know, one of the results that, you know, can spark</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1834" target="_blank">00:30:34.300</a></span> | <span class="t">a lot of interest in sparse models, even if it's only for pre-training for now, like just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1839" target="_blank">00:30:39.260</a></span> | <span class="t">having that number is like, you know, maybe there's a significant, there's something significant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1847" target="_blank">00:30:47.660</a></span> | <span class="t">that can be obtained here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1850" target="_blank">00:30:50.660</a></span> | <span class="t">Okay, so sparse scaling loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1855" target="_blank">00:30:55.660</a></span> | <span class="t">So here we'll look at sort of loss versus sparse model parameters, which are increased</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1863" target="_blank">00:31:03.580</a></span> | <span class="t">by increasing the number of exports.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1866" target="_blank">00:31:06.980</a></span> | <span class="t">And so similarly to the sort of, you know, normal scaling law paper, we observed that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1873" target="_blank">00:31:13.380</a></span> | <span class="t">as you increase the parameters, which the sparse parameters and keep the flops fixed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1883" target="_blank">00:31:23.020</a></span> | <span class="t">you get diminishing, like consistent gains, but diminishing gains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1887" target="_blank">00:31:27.900</a></span> | <span class="t">Okay, so now we're going to compare export parallelism and model parallelism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1894" target="_blank">00:31:34.700</a></span> | <span class="t">So we introduced sparsity or export parallelism as a new dimension to scale models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1902" target="_blank">00:31:42.340</a></span> | <span class="t">But of course, that's the other one for dense model, which is simply model parallelism where,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1908" target="_blank">00:31:48.660</a></span> | <span class="t">you know, model weights are partitioned across cores once they are above the maximum size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1914" target="_blank">00:31:54.900</a></span> | <span class="t">that you can fit on a single core.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1917" target="_blank">00:31:57.900</a></span> | <span class="t">All right, so, yeah, Bharath, I assume to the left is export parallelism here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1925" target="_blank">00:32:05.980</a></span> | <span class="t">Yeah, so essentially what we're doing is, yeah, we're kind of comparing a switch-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1931" target="_blank">00:32:11.020</a></span> | <span class="t">model versus the dense base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1933" target="_blank">00:32:13.660</a></span> | <span class="t">And we're also comparing against a larger dense model that has used model parallelism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1938" target="_blank">00:32:18.820</a></span> | <span class="t">And we can see that, you know, because basically when we want to scale a model size, we kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1942" target="_blank">00:32:22.380</a></span> | <span class="t">of have two axes that we can either go through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1944" target="_blank">00:32:24.340</a></span> | <span class="t">We can either increase the number of flops by scaling through model parallelism or increase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1948" target="_blank">00:32:28.820</a></span> | <span class="t">the number of parameters by scaling through sparsity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1951" target="_blank">00:32:31.620</a></span> | <span class="t">And so we can see that, you know, even compared to like, you know, a dense model that's been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1955" target="_blank">00:32:35.500</a></span> | <span class="t">scaled up through model parallelism, that sparsity is still at the scale, a more effective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1959" target="_blank">00:32:39.020</a></span> | <span class="t">way to scale up the model by, you know, still getting 2.5x speedups over this larger dense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1965" target="_blank">00:32:45.180</a></span> | <span class="t">model that was using model parallelism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1967" target="_blank">00:32:47.460</a></span> | <span class="t">Cool, so next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1970" target="_blank">00:32:50.060</a></span> | <span class="t">Yeah, basically here, T5 large is the dense model that uses model parallelism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1975" target="_blank">00:32:55.740</a></span> | <span class="t">Yeah, right, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1978" target="_blank">00:32:58.860</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1979" target="_blank">00:32:59.860</a></span> | <span class="t">Yeah, and so one thing that we also wanted to look at is like, you know, are these expert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1985" target="_blank">00:33:05.740</a></span> | <span class="t">models effective if you have like, you know, really small amount of computer, just a small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1989" target="_blank">00:33:09.220</a></span> | <span class="t">amount of experts?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1990" target="_blank">00:33:10.220</a></span> | <span class="t">So typically when we're designing these models, like we have one expert per core.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1994" target="_blank">00:33:14.580</a></span> | <span class="t">But if you don't have like a large cluster to run these things on, let's say you just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=1997" target="_blank">00:33:17.140</a></span> | <span class="t">have like a GPU with two cores or something, I guess having two experts more effective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2002" target="_blank">00:33:22.140</a></span> | <span class="t">than just like a dense model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2004" target="_blank">00:33:24.060</a></span> | <span class="t">And the answer is yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2005" target="_blank">00:33:25.220</a></span> | <span class="t">So we can see even pretty good scaling properties, even with like a tiny amount of experts, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2009" target="_blank">00:33:29.420</a></span> | <span class="t">is very, very promising for these models to be used even in like much lower compute regimes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2015" target="_blank">00:33:35.420</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2016" target="_blank">00:33:36.420</a></span> | <span class="t">Or when you want to go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2020" target="_blank">00:33:40.060</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2021" target="_blank">00:33:41.060</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2022" target="_blank">00:33:42.060</a></span> | <span class="t">And so look at, you know, what things look like when we use different types of parallelism,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2031" target="_blank">00:33:51.140</a></span> | <span class="t">namely expert parallelism to add experts, model parallelism to shard model weights across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2037" target="_blank">00:33:57.060</a></span> | <span class="t">cores and also data parallelism, which is sort of the dominant paradigm in deep learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2042" target="_blank">00:34:02.700</a></span> | <span class="t">at the moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2046" target="_blank">00:34:06.300</a></span> | <span class="t">And so, you know, I guess, you know, in the previous slides, we mostly talked about expert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2052" target="_blank">00:34:12.100</a></span> | <span class="t">parallelism, but of course, you know, dense models and large scale dense models use model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2057" target="_blank">00:34:17.220</a></span> | <span class="t">parallelism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2058" target="_blank">00:34:18.220</a></span> | <span class="t">So GP3 and these other large models, what they do is that they will simply shard model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2063" target="_blank">00:34:23.220</a></span> | <span class="t">weights across different cores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2065" target="_blank">00:34:25.300</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2066" target="_blank">00:34:26.300</a></span> | <span class="t">We have a question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2069" target="_blank">00:34:29.500</a></span> | <span class="t">Oh yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2071" target="_blank">00:34:31.740</a></span> | <span class="t">I just wanted to know, because I think there was like, I don't know if you're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2075" target="_blank">00:34:35.420</a></span> | <span class="t">address later, but I think somewhere in a paper, it said that the more experts you have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2080" target="_blank">00:34:40.580</a></span> | <span class="t">the more sample efficient it gets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2083" target="_blank">00:34:43.380</a></span> | <span class="t">And I was just like hoping, hoping that you could give us some intuition about that, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2087" target="_blank">00:34:47.700</a></span> | <span class="t">I don't understand why that would be the case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2092" target="_blank">00:34:52.260</a></span> | <span class="t">So I guess, yeah, maybe, so I guess like, you know, there's all of this work on larger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2099" target="_blank">00:34:59.700</a></span> | <span class="t">models are more sample efficient and larger in the context of the scaling law works means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2104" target="_blank">00:35:04.760</a></span> | <span class="t">like more parameters and more flops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2106" target="_blank">00:35:06.980</a></span> | <span class="t">As you increase the number of experts, there's more parameters, but not more flops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2109" target="_blank">00:35:09.900</a></span> | <span class="t">But the model is still like, you know, larger and like, you know, a similar sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2114" target="_blank">00:35:14.260</a></span> | <span class="t">So I guess like building on the intuition that larger models are more sample efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2118" target="_blank">00:35:18.100</a></span> | <span class="t">in my mind, it's not necessarily that surprising that these models with more experts that have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2124" target="_blank">00:35:24.020</a></span> | <span class="t">more parameters are more sample efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2127" target="_blank">00:35:27.500</a></span> | <span class="t">I guess that's my like kind of high level intuition for it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2130" target="_blank">00:35:30.100</a></span> | <span class="t">Yeah, I would say that's kind of expected that, you know, more experts leads to better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2137" target="_blank">00:35:37.480</a></span> | <span class="t">sample efficiency, especially if you look at training step, right, in a training time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2146" target="_blank">00:35:46.760</a></span> | <span class="t">Okay, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2152" target="_blank">00:35:52.040</a></span> | <span class="t">So where were we?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2153" target="_blank">00:35:53.700</a></span> | <span class="t">Yeah, so, yeah, so, okay, so we'll look at how model weights are split over cost for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2159" target="_blank">00:35:59.360</a></span> | <span class="t">different scenarios.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2161" target="_blank">00:36:01.560</a></span> | <span class="t">So data parallelism is the first one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2164" target="_blank">00:36:04.580</a></span> | <span class="t">So that's kind of the typical setup that deep learning uses, especially for not so large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2171" target="_blank">00:36:11.840</a></span> | <span class="t">networks which don't require model parallelism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2176" target="_blank">00:36:16.040</a></span> | <span class="t">And so let me, yeah, let me explain how, yeah, I'll just go to the final figure and I'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2182" target="_blank">00:36:22.920</a></span> | <span class="t">explain how to look at this figure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2187" target="_blank">00:36:27.360</a></span> | <span class="t">Okay, so we have 16 processes which are organized in the four by four mesh, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2193" target="_blank">00:36:33.980</a></span> | <span class="t">So each dotted line, each four by four dotted line here represents a different core.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2201" target="_blank">00:36:41.160</a></span> | <span class="t">And the first row studies how the model weights are split over cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2205" target="_blank">00:36:45.720</a></span> | <span class="t">And the second row illustrates how data, so literally examples and tokens are split over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2212" target="_blank">00:36:52.080</a></span> | <span class="t">cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2215" target="_blank">00:36:55.360</a></span> | <span class="t">And yeah, and then the final thing that's required to understand this figure is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2220" target="_blank">00:37:00.000</a></span> | <span class="t">each, yeah, each color of the shaded squares here identifies the unique weight matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2229" target="_blank">00:37:09.200</a></span> | <span class="t">Okay, so let's start with data parallelism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2233" target="_blank">00:37:13.140</a></span> | <span class="t">So for data parallelism, the same model weights are replicated across all cores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2240" target="_blank">00:37:20.800</a></span> | <span class="t">And the data is simply partitioned over cores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2243" target="_blank">00:37:23.920</a></span> | <span class="t">And so that's what this corresponds to, using the description of the caption, the explanation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2254" target="_blank">00:37:34.800</a></span> | <span class="t">of the caption I just gave.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2256" target="_blank">00:37:36.120</a></span> | <span class="t">So next we have model parallelism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2259" target="_blank">00:37:39.520</a></span> | <span class="t">That's kind of just like a theoretical example because in practice, people always use model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2264" target="_blank">00:37:44.440</a></span> | <span class="t">parallelism in conjunction with data parallelism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2268" target="_blank">00:37:48.000</a></span> | <span class="t">But so if you were to do only model parallelism, now you would have a single model weight that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2272" target="_blank">00:37:52.440</a></span> | <span class="t">is partitioned over all cores, and your data would just be replicated over all cores instead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2281" target="_blank">00:38:01.880</a></span> | <span class="t">So now we have model and data parallelism, and that's kind of the typical scenario for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2285" target="_blank">00:38:05.920</a></span> | <span class="t">large dense networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2288" target="_blank">00:38:08.040</a></span> | <span class="t">So in that case, model weights are partitioned among a subset of the cores, the subset of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2294" target="_blank">00:38:14.240</a></span> | <span class="t">cores that process different batches of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2296" target="_blank">00:38:16.920</a></span> | <span class="t">And so in that example here, we have sort of four, so the first sub-square here means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2302" target="_blank">00:38:22.760</a></span> | <span class="t">that the model weights are partitioned across four cores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2311" target="_blank">00:38:31.080</a></span> | <span class="t">And this is replicated sort of four times for the data parallelism dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2318" target="_blank">00:38:38.320</a></span> | <span class="t">On the data side, for model and data parallelism, yeah, the data here is replicated across model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2326" target="_blank">00:38:46.720</a></span> | <span class="t">parallel cores and partitioned across data parallel cores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2331" target="_blank">00:38:51.240</a></span> | <span class="t">So next we have expert and data parallelism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2335" target="_blank">00:38:55.320</a></span> | <span class="t">So in that scenario, that's kind of similar to data parallelism, but now each core will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2339" target="_blank">00:38:59.520</a></span> | <span class="t">hold a different model weight, which is illustrated by the different colors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2346" target="_blank">00:39:06.260</a></span> | <span class="t">And for the data side, the data is simply replicated, sorry, the data is partitioned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2352" target="_blank">00:39:12.400</a></span> | <span class="t">across all cores, just like in the data parallelism scenario.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2358" target="_blank">00:39:18.160</a></span> | <span class="t">And so finally, we have the rightmost column, which is, I guess, yeah, that's the setup</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2365" target="_blank">00:39:25.800</a></span> | <span class="t">used in the switch transformer paper for the larger models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2370" target="_blank">00:39:30.920</a></span> | <span class="t">And so here for the model partitioning, each expert is partitioned across multiple cores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2377" target="_blank">00:39:37.020</a></span> | <span class="t">So in that example, we have four experts, each partitioned across four cores, and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2382" target="_blank">00:39:42.820</a></span> | <span class="t">data is replicated across model parallel cores and partitioned across data parallel cores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2388" target="_blank">00:39:48.500</a></span> | <span class="t">So that's a little bit complex to understand, really, but the switch transformer paper has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2394" target="_blank">00:39:54.780</a></span> | <span class="t">a nice, the same figure with a nice caption to explain it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2400" target="_blank">00:40:00.360</a></span> | <span class="t">Yeah, maybe we can, about it, we can add something quickly about how this is implemented in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2409" target="_blank">00:40:09.820</a></span> | <span class="t">So there's this paper called Mesh Transformer, which kind of extends batch or data parallelism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2418" target="_blank">00:40:18.780</a></span> | <span class="t">to more general purpose SPMD style programming.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2423" target="_blank">00:40:23.900</a></span> | <span class="t">And so different labs have different frameworks, but this paper kind of lays the foundation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2428" target="_blank">00:40:28.420</a></span> | <span class="t">for general SPMD distributed computing, which is required for training large scale models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2437" target="_blank">00:40:37.080</a></span> | <span class="t">And so under the mesh abstraction, basically we have a mesh of processes, and so that mesh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2445" target="_blank">00:40:45.460</a></span> | <span class="t">has dimensions, name dimensions, and these name dimensions specify how the tensor dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2453" target="_blank">00:40:53.740</a></span> | <span class="t">will be partitioned or replicated across the mesh dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2458" target="_blank">00:40:58.400</a></span> | <span class="t">And so just that simple abstraction sort of supports data parallelism, also model parallelism,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2464" target="_blank">00:41:04.700</a></span> | <span class="t">and especially expert parallelism at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2467" target="_blank">00:41:07.980</a></span> | <span class="t">And so I invite whoever is interested to also check that paper, because that kind of lays</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2476" target="_blank">00:41:16.060</a></span> | <span class="t">the foundation for understanding these things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2478" target="_blank">00:41:18.300</a></span> | <span class="t">All right, Barry, do you want to go?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2482" target="_blank">00:41:22.180</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2483" target="_blank">00:41:23.180</a></span> | <span class="t">So next we are going to kind of talk about like how we take these parallelism strategies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2486" target="_blank">00:41:26.300</a></span> | <span class="t">and like kind of combine them together to make like a 1.6 trillion parameter sparse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2491" target="_blank">00:41:31.140</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2492" target="_blank">00:41:32.140</a></span> | <span class="t">So next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2495" target="_blank">00:41:35.220</a></span> | <span class="t">So what we ended up doing in this work was we trained two different very large sparse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2501" target="_blank">00:41:41.820</a></span> | <span class="t">models, and we compared them to the largest T5 model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2504" target="_blank">00:41:44.540</a></span> | <span class="t">So we can see the T5 XXL, which is a dense model, and it was the largest one trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2508" target="_blank">00:41:48.820</a></span> | <span class="t">in the T5 paper, and it has around 13 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2512" target="_blank">00:41:52.740</a></span> | <span class="t">And here we list a lot of the model dimensions like D model, DFF, which are just like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2516" target="_blank">00:41:56.140</a></span> | <span class="t">various sizes and shapes of the tensors and stuff, the number of layers, the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2520" target="_blank">00:42:00.900</a></span> | <span class="t">heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2521" target="_blank">00:42:01.900</a></span> | <span class="t">And importantly, we also mentioned the negative log perplexity at step 250k and at 500k.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2528" target="_blank">00:42:08.980</a></span> | <span class="t">And so yeah, so we designed two sparse models to test like how scaling versus sparsity versus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2536" target="_blank">00:42:16.740</a></span> | <span class="t">scaling versus sparsity and flops work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2539" target="_blank">00:42:19.600</a></span> | <span class="t">So first, let me talk about switch XXL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2541" target="_blank">00:42:21.660</a></span> | <span class="t">So that has the same amount of flops per token as T5 XXL, but has 64 experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2547" target="_blank">00:42:27.200</a></span> | <span class="t">And this leads it to have around 400 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2551" target="_blank">00:42:31.020</a></span> | <span class="t">And we can see that on a step basis, it actually performs quite well and outperforms the T5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2555" target="_blank">00:42:35.620</a></span> | <span class="t">XXL by like quite a good margin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2557" target="_blank">00:42:37.700</a></span> | <span class="t">Interestingly, though, are the third model we designed switch C, which has 1.6 trillion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2562" target="_blank">00:42:42.340</a></span> | <span class="t">parameters, but has a significantly fewer flops, almost 10 less flops per token than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2567" target="_blank">00:42:47.700</a></span> | <span class="t">either of the above two models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2568" target="_blank">00:42:48.900</a></span> | <span class="t">So it's really trading by reducing flops to have way more sparse parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2574" target="_blank">00:42:54.660</a></span> | <span class="t">And we can see on a step basis, the switch C model works well, but not, not as well as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2579" target="_blank">00:42:59.780</a></span> | <span class="t">actually the higher flop model, but on a, like a kind of a Pareto axis where we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2584" target="_blank">00:43:04.860</a></span> | <span class="t">looking at TPU hours on the X axis and not step the switch C model actually outperforms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2589" target="_blank">00:43:09.500</a></span> | <span class="t">them both by like a pretty large margin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2592" target="_blank">00:43:12.060</a></span> | <span class="t">So for pre-training performance, we're seeing that actually just like having a lot of sparsity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2595" target="_blank">00:43:15.900</a></span> | <span class="t">and less flops is actually, um, can be quite good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2600" target="_blank">00:43:20.060</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2601" target="_blank">00:43:21.060</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2602" target="_blank">00:43:22.060</a></span> | <span class="t">And so, yeah, this, so again, those two sparse models are kind of really trying to get at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2605" target="_blank">00:43:25.700</a></span> | <span class="t">this hypothesis that actually Noam Shazir had, which is, you know, that, you know, parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2610" target="_blank">00:43:30.060</a></span> | <span class="t">are good for more knowledge, reasoning and compute AKA flops is good for intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2617" target="_blank">00:43:37.020</a></span> | <span class="t">And so we're going to kind of try to get at that by taking these different sparse models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2619" target="_blank">00:43:39.900</a></span> | <span class="t">and then fine tuning them on, uh, different tasks, some of which require more like knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2624" target="_blank">00:43:44.140</a></span> | <span class="t">and then others, which require more of like reasoning, um, for whatever, like hand wavy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2628" target="_blank">00:43:48.100</a></span> | <span class="t">definition we want to give that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2631" target="_blank">00:43:51.060</a></span> | <span class="t">So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2632" target="_blank">00:43:52.060</a></span> | <span class="t">So for a fixed, Oh, go back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2633" target="_blank">00:43:53.380</a></span> | <span class="t">So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2634" target="_blank">00:43:54.380</a></span> | <span class="t">So for a fixed, Oh, can you go back to the previous slide?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2636" target="_blank">00:43:56.980</a></span> | <span class="t">Oh yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2637" target="_blank">00:43:57.980</a></span> | <span class="t">Sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2638" target="_blank">00:43:58.980</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2639" target="_blank">00:43:59.980</a></span> | <span class="t">So for a fixed quality on an upstream pre-training task, um, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2643" target="_blank">00:44:03.060</a></span> | <span class="t">Do parameters independently matter?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2645" target="_blank">00:44:05.360</a></span> | <span class="t">So we're going to look at two tasks here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2646" target="_blank">00:44:06.980</a></span> | <span class="t">One of which is super glue, which is kind of our like reasoning task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2649" target="_blank">00:44:09.860</a></span> | <span class="t">And then another is like trivia QA, which is like some knowledge task where it's like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2652" target="_blank">00:44:12.860</a></span> | <span class="t">you just give it a question, you have it output an answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2656" target="_blank">00:44:16.860</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2659" target="_blank">00:44:19.300</a></span> | <span class="t">And so here we're going to take a look at super glue quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2661" target="_blank">00:44:21.860</a></span> | <span class="t">So we can see on the X axis is the pre-training performance and the Y axis is the super glue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2666" target="_blank">00:44:26.620</a></span> | <span class="t">score after fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2668" target="_blank">00:44:28.980</a></span> | <span class="t">And interestingly, we can see definitely that the sparse models definitely are for a fixed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2674" target="_blank">00:44:34.260</a></span> | <span class="t">um, pre-training perplexity do worse on fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2677" target="_blank">00:44:37.660</a></span> | <span class="t">This can be especially noticed at like the upper right portion of the plot where the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2681" target="_blank">00:44:41.340</a></span> | <span class="t">dense models are definitely fine tuning better than the, their sparse counterpart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2687" target="_blank">00:44:47.140</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2688" target="_blank">00:44:48.140</a></span> | <span class="t">Interestingly, when we study it on the more knowledge, heavy tasks, the sparse model for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2692" target="_blank">00:44:52.940</a></span> | <span class="t">a fixed, uh, pre-training perplexity does disproportionately well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2697" target="_blank">00:44:57.100</a></span> | <span class="t">So, you know, for a model that roughly has the same perplexity, we're getting like really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2700" target="_blank">00:45:00.260</a></span> | <span class="t">large boosts for these knowledge, heavy tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2703" target="_blank">00:45:03.300</a></span> | <span class="t">So this is pretty interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2704" target="_blank">00:45:04.300</a></span> | <span class="t">And it also really, you know, show some of the dangers of comparing only on your pre-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2709" target="_blank">00:45:09.220</a></span> | <span class="t">metrics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2710" target="_blank">00:45:10.220</a></span> | <span class="t">So dense models, you know, can have the same exact pre-training metric, but very different,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2713" target="_blank">00:45:13.580</a></span> | <span class="t">um, you know, properties when fine tuning them on different tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2718" target="_blank">00:45:18.620</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2721" target="_blank">00:45:21.060</a></span> | <span class="t">And interestingly, so yeah, all of the switch models here are the, um, are, are just like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2726" target="_blank">00:45:26.220</a></span> | <span class="t">you know, various models that have still a good amount of flops, but the red model is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2730" target="_blank">00:45:30.780</a></span> | <span class="t">actually the 1.6 trillion parameter, uh, sparse model that has, you know, very few flops,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2737" target="_blank">00:45:37.500</a></span> | <span class="t">but a lot, a lot of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2738" target="_blank">00:45:38.500</a></span> | <span class="t">And we can see that as the red dot here, and it does actually disproportionately bad compared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2742" target="_blank">00:45:42.580</a></span> | <span class="t">to other sparse models that also have pretty good perplexities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2746" target="_blank">00:45:46.180</a></span> | <span class="t">And so, yeah, it's, uh, it's definitely very interesting and it shows that, you know, for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2749" target="_blank">00:45:49.540</a></span> | <span class="t">models during pre-training that have a lot of sparsity, they definitely suffer on some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2753" target="_blank">00:45:53.340</a></span> | <span class="t">of these more reasoning heavy metrics, but do disproportionately well for more of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2757" target="_blank">00:45:57.960</a></span> | <span class="t">knowledge, heavy tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2760" target="_blank">00:46:00.060</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2761" target="_blank">00:46:01.060</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2762" target="_blank">00:46:02.060</a></span> | <span class="t">And so here we can see it as just like a huge outlier for a pre-training perplexity doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2767" target="_blank">00:46:07.380</a></span> | <span class="t">like just incredibly well on this, uh, downstream question answering task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2773" target="_blank">00:46:13.420</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2774" target="_blank">00:46:14.420</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2775" target="_blank">00:46:15.420</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2776" target="_blank">00:46:16.420</a></span> | <span class="t">So also, you know, one thing that we were going to do is just look at the fine tuning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2779" target="_blank">00:46:19.760</a></span> | <span class="t">properties of sparse models across like a few scales and just see how they perform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2784" target="_blank">00:46:24.900</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2785" target="_blank">00:46:25.900</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2786" target="_blank">00:46:26.900</a></span> | <span class="t">And so here we try two different models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2789" target="_blank">00:46:29.220</a></span> | <span class="t">One is, um, T5 base, and then we make a flop match sparse counterpoint.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2793" target="_blank">00:46:33.460</a></span> | <span class="t">And when they say flop match, it's like, you know, each token will have the same amount</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2796" target="_blank">00:46:36.420</a></span> | <span class="t">of flops, but now we just have experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2798" target="_blank">00:46:38.600</a></span> | <span class="t">So we do this for both base and large, and we see that actually across almost all tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2802" target="_blank">00:46:42.140</a></span> | <span class="t">besides two arc tasks, the sparse models perform quite well, which is, which is definitely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2807" target="_blank">00:46:47.460</a></span> | <span class="t">promising.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2808" target="_blank">00:46:48.460</a></span> | <span class="t">So we are seeing that these models are pretty robust, they pre-train well, and then they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2811" target="_blank">00:46:51.380</a></span> | <span class="t">also fine tune well when scaled appropriately by scaling up both the flops and sparsity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2817" target="_blank">00:46:57.020</a></span> | <span class="t">Whereas, you know, the negative results we've really seen are like, yeah, when you just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2820" target="_blank">00:47:00.460</a></span> | <span class="t">have a huge amount of sparsity and not too many flops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2824" target="_blank">00:47:04.180</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2825" target="_blank">00:47:05.180</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2826" target="_blank">00:47:06.180</a></span> | <span class="t">And one also thing we wanted to look at was, uh, the multilingual training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2830" target="_blank">00:47:10.620</a></span> | <span class="t">So we were previously studying all of this on like English only, and we also wanted to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2834" target="_blank">00:47:14.300</a></span> | <span class="t">see how sparsity helps in the multilingual setting because, you know, we also felt like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2838" target="_blank">00:47:18.060</a></span> | <span class="t">this would be a very natural place for sparsity to work well, or potentially experts could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2841" target="_blank">00:47:21.540</a></span> | <span class="t">specialize across languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2843" target="_blank">00:47:23.060</a></span> | <span class="t">Um, and we do see strong results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2845" target="_blank">00:47:25.500</a></span> | <span class="t">So on 91% of the languages, I think of like around a hundred languages, we see over like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2850" target="_blank">00:47:30.500</a></span> | <span class="t">at least a 4x speedup over the MT5, um, dense model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2856" target="_blank">00:47:36.180</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2857" target="_blank">00:47:37.180</a></span> | <span class="t">Erwin, you want to go ahead?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2860" target="_blank">00:47:40.100</a></span> | <span class="t">Uh, no, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2862" target="_blank">00:47:42.860</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2863" target="_blank">00:47:43.860</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2864" target="_blank">00:47:44.860</a></span> | <span class="t">So another thing we wanted to talk about was, um, distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2867" target="_blank">00:47:47.300</a></span> | <span class="t">So one downside of these sparse models is that they'll have a lot more parameters, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2871" target="_blank">00:47:51.940</a></span> | <span class="t">means that, you know, if you're serving these things or something, you either need like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2875" target="_blank">00:47:55.140</a></span> | <span class="t">high throughput use cases, or you need to maybe distill it back down into like a smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2879" target="_blank">00:47:59.300</a></span> | <span class="t">dense model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2880" target="_blank">00:48:00.700</a></span> | <span class="t">So here, what we do is we look at like the T5 base and switch base, and we look at its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2883" target="_blank">00:48:03.980</a></span> | <span class="t">pre-training performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2885" target="_blank">00:48:05.620</a></span> | <span class="t">And then we go through, um, some ablations of different distillation techniques and find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2889" target="_blank">00:48:09.060</a></span> | <span class="t">that like with the best techniques, we can keep around 30% of the quality improvements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2894" target="_blank">00:48:14.180</a></span> | <span class="t">of sparsity while distilling it back down into its, uh, dense, um, counterpart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2900" target="_blank">00:48:20.460</a></span> | <span class="t">So next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2902" target="_blank">00:48:22.420</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2904" target="_blank">00:48:24.420</a></span> | <span class="t">And then we kind of study this across multiple scales.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2906" target="_blank">00:48:26.380</a></span> | <span class="t">And again, we see like around like 30 to 40% of the gains can be, um, like, you know, kept</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2912" target="_blank">00:48:32.340</a></span> | <span class="t">when going from a dense model and going from, you know, a sparse model and distilling it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2915" target="_blank">00:48:35.980</a></span> | <span class="t">back down until it gets flop match dense model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2918" target="_blank">00:48:38.500</a></span> | <span class="t">So you can get, you know, get rid of up to 99% of the parameters and still keep like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2922" target="_blank">00:48:42.620</a></span> | <span class="t">around 30% of the improvements, which is very promising.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2926" target="_blank">00:48:46.060</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2927" target="_blank">00:48:47.060</a></span> | <span class="t">Wait, I'm sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2928" target="_blank">00:48:48.060</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2929" target="_blank">00:48:49.060</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2930" target="_blank">00:48:50.060</a></span> | <span class="t">Sorry about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2931" target="_blank">00:48:51.060</a></span> | <span class="t">Can you say that last sentence again?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2932" target="_blank">00:48:52.940</a></span> | <span class="t">You said that you can keep the benefit 30% of the teachers benefit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2936" target="_blank">00:48:56.900</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2937" target="_blank">00:48:57.900</a></span> | <span class="t">Basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2938" target="_blank">00:48:58.900</a></span> | <span class="t">So yeah, you, you, you, yeah, exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2940" target="_blank">00:49:00.820</a></span> | <span class="t">So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2941" target="_blank">00:49:01.820</a></span> | <span class="t">So we're looking at like, yeah, you train a sparse model and then you just fill it back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2946" target="_blank">00:49:06.260</a></span> | <span class="t">down to a dense model and you're versus training a dense model from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2950" target="_blank">00:49:10.700</a></span> | <span class="t">And like you look at the gap between the sparse and dense model from scratch versus the, the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2955" target="_blank">00:49:15.380</a></span> | <span class="t">the gap between the dense and then the distilled dense model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2958" target="_blank">00:49:18.660</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2959" target="_blank">00:49:19.660</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2960" target="_blank">00:49:20.660</a></span> | <span class="t">Oh yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2961" target="_blank">00:49:21.660</a></span> | <span class="t">Oh yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2962" target="_blank">00:49:22.660</a></span> | <span class="t">Maybe let me just do like a quick high level summary again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2969" target="_blank">00:49:29.420</a></span> | <span class="t">So what we're, what we'll do is for our comparisons is we'll train a dense model from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2974" target="_blank">00:49:34.580</a></span> | <span class="t">We'll train a sparse model from scratch and then we'll also run a third experiment where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2978" target="_blank">00:49:38.740</a></span> | <span class="t">we distill that sparse model down into a dense model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2983" target="_blank">00:49:43.940</a></span> | <span class="t">What does distilling mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2985" target="_blank">00:49:45.740</a></span> | <span class="t">Like we're basically trying to match the like the teacher's logics, like the kind of standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2990" target="_blank">00:49:50.660</a></span> | <span class="t">thing of like, you know, like matching the, like either the logics or like the soft probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2994" target="_blank">00:49:54.940</a></span> | <span class="t">for each token or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2996" target="_blank">00:49:56.980</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=2997" target="_blank">00:49:57.980</a></span> | <span class="t">If I can jump in with my question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3005" target="_blank">00:50:05.420</a></span> | <span class="t">So what I'm struggling with is how do I interpret the linements as percent of teacher and performance?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3011" target="_blank">00:50:11.140</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3012" target="_blank">00:50:12.140</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3013" target="_blank">00:50:13.140</a></span> | <span class="t">So it's, it's basically looking at the, like the gap between the dense and sparse model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3017" target="_blank">00:50:17.820</a></span> | <span class="t">So we'll have the dense model gets some performance, we'll have the sparse model gets some performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3021" target="_blank">00:50:21.500</a></span> | <span class="t">and then the, the dense model that's still from the sparse model would be somewhere in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3025" target="_blank">00:50:25.920</a></span> | <span class="t">between that, that range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3028" target="_blank">00:50:28.020</a></span> | <span class="t">And we're basically saying it's 30% through that range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3031" target="_blank">00:50:31.280</a></span> | <span class="t">So it's like in like a zero to one interval, it's like 0.3 of the way from the dense to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3035" target="_blank">00:50:35.340</a></span> | <span class="t">the sparse model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3036" target="_blank">00:50:36.340</a></span> | <span class="t">I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3037" target="_blank">00:50:37.340</a></span> | <span class="t">So this is not saying that the percent of teacher performance does not mean that if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3040" target="_blank">00:50:40.540</a></span> | <span class="t">the teacher say gets, if we use the teacher's guesses or predictions as the ground truth,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3045" target="_blank">00:50:45.420</a></span> | <span class="t">this is not saying that the distilled model gets matches with the teacher, 33% of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3050" target="_blank">00:50:50.540</a></span> | <span class="t">time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3051" target="_blank">00:50:51.540</a></span> | <span class="t">No, no, exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3052" target="_blank">00:50:52.540</a></span> | <span class="t">It's basically saying you get like 30% of the quality improvements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3054" target="_blank">00:50:54.740</a></span> | <span class="t">Yeah, exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3055" target="_blank">00:50:55.740</a></span> | <span class="t">Okay, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3056" target="_blank">00:50:56.740</a></span> | <span class="t">And then if we can back up a slide, I had a different question, but I didn't want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3060" target="_blank">00:51:00.660</a></span> | <span class="t">interrupt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3061" target="_blank">00:51:01.660</a></span> | <span class="t">When we were talking about all of these different T5 bases, and then also on a few slides before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3064" target="_blank">00:51:04.900</a></span> | <span class="t">this, I don't know that much about T5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3066" target="_blank">00:51:06.980</a></span> | <span class="t">I'm curious, you know, when T5 is trained, is there a weight penalty in the loss function?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3073" target="_blank">00:51:13.860</a></span> | <span class="t">Is there a weight decay term?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3075" target="_blank">00:51:15.580</a></span> | <span class="t">No, there's no weight decay trained with any of those sparse or dense models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3079" target="_blank">00:51:19.780</a></span> | <span class="t">I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3080" target="_blank">00:51:20.780</a></span> | <span class="t">So out of curiosity then, how do dense models perform compared to the switch model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3085" target="_blank">00:51:25.500</a></span> | <span class="t">If you add some sort of weight regularization that incentivizes getting rid of useless weights?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3091" target="_blank">00:51:31.180</a></span> | <span class="t">Oh, so some kind of like maybe like L1 term or something like that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3095" target="_blank">00:51:35.620</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3096" target="_blank">00:51:36.620</a></span> | <span class="t">So I'm wondering like how much of, because here we're talking about the benefits of sparsity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3099" target="_blank">00:51:39.180</a></span> | <span class="t">and I'm wondering how much of this benefit from sparsity is due to the fact that just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3103" target="_blank">00:51:43.740</a></span> | <span class="t">some of this, I mean, effectively what the switch model is doing, if I understand correctly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3106" target="_blank">00:51:46.900</a></span> | <span class="t">maybe I don't, what I understand is that the switch model, the feed forward layer, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3110" target="_blank">00:51:50.500</a></span> | <span class="t">just like you fixing the weights to be zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3113" target="_blank">00:51:53.460</a></span> | <span class="t">That's what it means to be sparse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3115" target="_blank">00:51:55.860</a></span> | <span class="t">Well, actually, we're kind of really trying to like inject more weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3119" target="_blank">00:51:59.220</a></span> | <span class="t">So we're actually kind of trying to do, it's a little bit maybe like paradoxical, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3122" target="_blank">00:52:02.140</a></span> | <span class="t">we're saying switch transformer, but our idea is to be like, hey, we actually want to just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3125" target="_blank">00:52:05.700</a></span> | <span class="t">have significantly more weights, not less weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3128" target="_blank">00:52:08.580</a></span> | <span class="t">It's kind of like you would zero out weights, but within a much larger weight matrix, if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3133" target="_blank">00:52:13.660</a></span> | <span class="t">that makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3134" target="_blank">00:52:14.660</a></span> | <span class="t">I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3135" target="_blank">00:52:15.660</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3136" target="_blank">00:52:16.660</a></span> | <span class="t">And so to me, it seems like a relevant baseline to just ask what happens if I have the dense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3138" target="_blank">00:52:18.380</a></span> | <span class="t">matrix, but I incentivize it with, say, an L1 or L2 penalty on the weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3141" target="_blank">00:52:21.860</a></span> | <span class="t">And I would, I'd be curious to know how that compares.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3144" target="_blank">00:52:24.740</a></span> | <span class="t">Yeah, we didn't run this, but also that kind of gets rid of weights for the dense model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3148" target="_blank">00:52:28.580</a></span> | <span class="t">So if any-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3149" target="_blank">00:52:29.580</a></span> | <span class="t">Sure, sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3150" target="_blank">00:52:30.580</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3151" target="_blank">00:52:31.580</a></span> | <span class="t">So, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3152" target="_blank">00:52:32.580</a></span> | <span class="t">Also-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3153" target="_blank">00:52:33.580</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3154" target="_blank">00:52:34.580</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3155" target="_blank">00:52:35.580</a></span> | <span class="t">Also, to me, it's like, if you just add like an L1 penalty loss, you're not going to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3159" target="_blank">00:52:39.460</a></span> | <span class="t">structured sparsity, whereas like here we, you know, it's not random weights in your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3166" target="_blank">00:52:46.580</a></span> | <span class="t">giant weight matrix that are zeroed out, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3168" target="_blank">00:52:48.660</a></span> | <span class="t">It's like really like blocks depending, like blocks corresponding to each expo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3173" target="_blank">00:52:53.020</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3174" target="_blank">00:52:54.020</a></span> | <span class="t">And so-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3175" target="_blank">00:52:55.020</a></span> | <span class="t">So that structure allows the whole like communication stuff and that's-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3180" target="_blank">00:53:00.700</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3181" target="_blank">00:53:01.820</a></span> | <span class="t">That leverages the fact that you have multiple calls and so on, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3185" target="_blank">00:53:05.420</a></span> | <span class="t">I totally agree with that block structure and that's what I'm trying to say, is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3188" target="_blank">00:53:08.860</a></span> | <span class="t">the switch has this very rich, it's not just sparse, it also has this rich structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3192" target="_blank">00:53:12.780</a></span> | <span class="t">And what I'm trying to do in my mind is disentangle, is the sparsity what's offering an advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3197" target="_blank">00:53:17.420</a></span> | <span class="t">or is this additional structure that you built in, is that what is the performance gain?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3201" target="_blank">00:53:21.860</a></span> | <span class="t">So that's why I'm asking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3203" target="_blank">00:53:23.740</a></span> | <span class="t">So the block structure is what enables to leverage the fact that you have multiple calls.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3211" target="_blank">00:53:31.340</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3212" target="_blank">00:53:32.340</a></span> | <span class="t">Like if you didn't have that block structure, you'd still have to route to everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3216" target="_blank">00:53:36.980</a></span> | <span class="t">And so you have more communication costs and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3220" target="_blank">00:53:40.740</a></span> | <span class="t">And then your first question was what, sorry?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3223" target="_blank">00:53:43.020</a></span> | <span class="t">I'm not actually sure if there was a question, I guess what I'm trying to say is I'm trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3225" target="_blank">00:53:45.780</a></span> | <span class="t">to-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3226" target="_blank">00:53:46.780</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3227" target="_blank">00:53:47.780</a></span> | <span class="t">Yeah, anyways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3228" target="_blank">00:53:48.780</a></span> | <span class="t">But I agree, it's a little bit weird because sparsity kind of, there's a spectrum of meaning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3233" target="_blank">00:53:53.980</a></span> | <span class="t">for sparsity, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3234" target="_blank">00:53:54.980</a></span> | <span class="t">So it's like, for example, compression and like model pruning is a form of sparsity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3240" target="_blank">00:54:00.220</a></span> | <span class="t">but also a switch transformer and MOE also referred to as sparsity and that kind of related,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3247" target="_blank">00:54:07.340</a></span> | <span class="t">but definitely they're aiming at different things, so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3249" target="_blank">00:54:09.940</a></span> | <span class="t">This is a really interesting idea of it's sparse, but you have more parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3253" target="_blank">00:54:13.260</a></span> | <span class="t">I'll have to think about it more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3254" target="_blank">00:54:14.900</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3255" target="_blank">00:54:15.900</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3256" target="_blank">00:54:16.900</a></span> | <span class="t">So you have like sparse within this like giant weight matrix, which is-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3260" target="_blank">00:54:20.620</a></span> | <span class="t">Exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3261" target="_blank">00:54:21.620</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3262" target="_blank">00:54:22.620</a></span> | <span class="t">Yeah, yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3263" target="_blank">00:54:23.620</a></span> | <span class="t">I hadn't appreciated that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3264" target="_blank">00:54:24.620</a></span> | <span class="t">So I appreciate you pointing that out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3267" target="_blank">00:54:27.220</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3268" target="_blank">00:54:28.220</a></span> | <span class="t">I have a follow up question on distillation part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3271" target="_blank">00:54:31.980</a></span> | <span class="t">Yeah, of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3273" target="_blank">00:54:33.340</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3274" target="_blank">00:54:34.340</a></span> | <span class="t">So if you distill it back down, now you have like one technically, you're back to the dense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3278" target="_blank">00:54:38.700</a></span> | <span class="t">layer architecture, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3282" target="_blank">00:54:42.620</a></span> | <span class="t">So now the entire idea of expert is that certain tokens would be sent to different experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3288" target="_blank">00:54:48.140</a></span> | <span class="t">because they just like, I don't know, are more specialized in figuring something out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3291" target="_blank">00:54:51.940</a></span> | <span class="t">about this token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3292" target="_blank">00:54:52.940</a></span> | <span class="t">So now if you go back to this like dense layer, aren't you like basically only serving whichever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3304" target="_blank">00:55:04.140</a></span> | <span class="t">expert you base this dense layer on, like these tokens will probably perform well and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3309" target="_blank">00:55:09.020</a></span> | <span class="t">all the other tokens are kind of like left behind, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3313" target="_blank">00:55:13.140</a></span> | <span class="t">I'm actually, sorry, I don't think I'm fully understanding your question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3319" target="_blank">00:55:19.500</a></span> | <span class="t">So are you kind of getting at like we're distilling this on a specific data set?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3324" target="_blank">00:55:24.060</a></span> | <span class="t">So that-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3325" target="_blank">00:55:25.060</a></span> | <span class="t">No, I'm thinking of how to use that, like why-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3327" target="_blank">00:55:27.620</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3328" target="_blank">00:55:28.620</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3329" target="_blank">00:55:29.620</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3330" target="_blank">00:55:30.620</a></span> | <span class="t">So maybe concretely, like let's, so like for super glue, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3331" target="_blank">00:55:31.620</a></span> | <span class="t">Like let's say you want to serve a model that does super glue well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3334" target="_blank">00:55:34.380</a></span> | <span class="t">I think the idea is that like you distill the sparse model into a dense model on super</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3338" target="_blank">00:55:38.040</a></span> | <span class="t">glue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3339" target="_blank">00:55:39.040</a></span> | <span class="t">So then you kind of get this compressed dense model that now performs better than if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3342" target="_blank">00:55:42.740</a></span> | <span class="t">were to just train it from scratch or train it from like a pre-trained dense model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3346" target="_blank">00:55:46.900</a></span> | <span class="t">So then it's like-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3347" target="_blank">00:55:47.900</a></span> | <span class="t">Which model did you use though?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3348" target="_blank">00:55:48.900</a></span> | <span class="t">Say that again?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3350" target="_blank">00:55:50.900</a></span> | <span class="t">You have to pick one expert, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3352" target="_blank">00:55:52.740</a></span> | <span class="t">No, no, no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3353" target="_blank">00:55:53.740</a></span> | <span class="t">You can just distill all of the, again, because you're just matching the model outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3357" target="_blank">00:55:57.380</a></span> | <span class="t">So you can just treat the sparse model as kind of like a black box thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3360" target="_blank">00:56:00.180</a></span> | <span class="t">All we're doing is just trying to have the dense model match the actual like final like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3363" target="_blank">00:56:03.540</a></span> | <span class="t">token predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3364" target="_blank">00:56:04.540</a></span> | <span class="t">Oh God.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3365" target="_blank">00:56:05.540</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3366" target="_blank">00:56:06.540</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3367" target="_blank">00:56:07.540</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3368" target="_blank">00:56:08.540</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3369" target="_blank">00:56:09.540</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3370" target="_blank">00:56:10.540</a></span> | <span class="t">Sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3371" target="_blank">00:56:11.540</a></span> | <span class="t">I was not, I was not familiar with the idea of distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3372" target="_blank">00:56:12.540</a></span> | <span class="t">So I think that was like my current confusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3373" target="_blank">00:56:13.540</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3374" target="_blank">00:56:14.540</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3375" target="_blank">00:56:15.540</a></span> | <span class="t">Yeah, of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3376" target="_blank">00:56:16.540</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3377" target="_blank">00:56:17.540</a></span> | <span class="t">Um, I guess one motivation here is that, um, having experts can make solving a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3384" target="_blank">00:56:24.660</a></span> | <span class="t">more difficult because, um, it requires bigger topologies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3389" target="_blank">00:56:29.300</a></span> | <span class="t">Let's say you have eight experts, um, you need like, well, I guess you can have multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3395" target="_blank">00:56:35.140</a></span> | <span class="t">experts on fewer calls, but, um, you know, let's just say they're a little bit harder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3401" target="_blank">00:56:41.500</a></span> | <span class="t">to solve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3402" target="_blank">00:56:42.500</a></span> | <span class="t">And so if we can, you know, get the benefits from sparsity at pre-training and then use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3408" target="_blank">00:56:48.700</a></span> | <span class="t">distillation to a dense model for solving, uh, that can be, that can be beneficial.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3414" target="_blank">00:56:54.740</a></span> | <span class="t">So I think that was sort of the motivation for that, uh, experiment, right, Derek?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3421" target="_blank">00:57:01.340</a></span> | <span class="t">Yeah, exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3423" target="_blank">00:57:03.100</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3424" target="_blank">00:57:04.100</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3425" target="_blank">00:57:05.100</a></span> | <span class="t">Well, are we, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3426" target="_blank">00:57:06.100</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3427" target="_blank">00:57:07.100</a></span> | <span class="t">So kind of just wrapping up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3428" target="_blank">00:57:08.100</a></span> | <span class="t">Yeah, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3429" target="_blank">00:57:09.100</a></span> | <span class="t">No, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3430" target="_blank">00:57:10.100</a></span> | <span class="t">I just said, I think one more string kind of question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3431" target="_blank">00:57:11.100</a></span> | <span class="t">So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3432" target="_blank">00:57:12.100</a></span> | <span class="t">Oh yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3433" target="_blank">00:57:13.100</a></span> | <span class="t">Go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3434" target="_blank">00:57:14.100</a></span> | <span class="t">I feel free to ask it now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3435" target="_blank">00:57:15.100</a></span> | <span class="t">Oh yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3436" target="_blank">00:57:16.100</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3437" target="_blank">00:57:17.100</a></span> | <span class="t">Sounds good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3438" target="_blank">00:57:18.100</a></span> | <span class="t">Um, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3439" target="_blank">00:57:19.100</a></span> | <span class="t">Thanks guys for the talk so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3440" target="_blank">00:57:20.100</a></span> | <span class="t">Uh, just a quick question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3444" target="_blank">00:57:24.100</a></span> | <span class="t">Was wondering if you think there are any interesting directions around, uh, building models that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3449" target="_blank">00:57:29.220</a></span> | <span class="t">are like explicitly optimized for, for parallel training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3453" target="_blank">00:57:33.660</a></span> | <span class="t">Um, I guess like the, the MOE model seems like, you know, it does a really good job</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3458" target="_blank">00:57:38.420</a></span> | <span class="t">here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3459" target="_blank">00:57:39.420</a></span> | <span class="t">And also like at, at inference time, it's very useful to like, you know, have fewer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3463" target="_blank">00:57:43.820</a></span> | <span class="t">flops per, per computation, um, or per forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3469" target="_blank">00:57:49.720</a></span> | <span class="t">But, um, I guess, do you think that there are any interesting directions around distributed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3474" target="_blank">00:57:54.180</a></span> | <span class="t">training where you might have like models that are explicitly are architected to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3480" target="_blank">00:58:00.060</a></span> | <span class="t">a lot of, uh, parallel heads or, or other like features that are, you know, kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3486" target="_blank">00:58:06.500</a></span> | <span class="t">embarrassingly parallelizable or does just using like standard, you know, scale up the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3492" target="_blank">00:58:12.420</a></span> | <span class="t">models by adding more layers, uh, and then just, you know, get away with using model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3497" target="_blank">00:58:17.300</a></span> | <span class="t">and data parallelism work well enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3499" target="_blank">00:58:19.940</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3500" target="_blank">00:58:20.940</a></span> | <span class="t">So I think, so yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3501" target="_blank">00:58:21.940</a></span> | <span class="t">So let me just make sure I'm fully understanding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3503" target="_blank">00:58:23.440</a></span> | <span class="t">So yeah, I think also like, you know, right now, like even our models are definitely very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3506" target="_blank">00:58:26.780</a></span> | <span class="t">co-designed with the hardware and like the shapes and things, you know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3509" target="_blank">00:58:29.900</a></span> | <span class="t">Um, so yeah, I, I, I think at a high level, like, yes, I think there's a ton of interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3513" target="_blank">00:58:33.760</a></span> | <span class="t">research on like co-designing the hardware, the partitioning algorithms and the models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3519" target="_blank">00:58:39.020</a></span> | <span class="t">I think given, you know, that we have this kind of like SPMD mesh style partitioning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3523" target="_blank">00:58:43.540</a></span> | <span class="t">we are already kind of designing our models in ways that fit it really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3527" target="_blank">00:58:47.020</a></span> | <span class="t">So for example, when we want to scale up our model, one of the first dimensions we go to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3530" target="_blank">00:58:50.380</a></span> | <span class="t">scale up is the internal hidden dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3532" target="_blank">00:58:52.980</a></span> | <span class="t">Because there's some really nice properties of scaling up this dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3535" target="_blank">00:58:55.140</a></span> | <span class="t">It basically becomes like, kind of, you know, independent to some of the communication costs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3539" target="_blank">00:58:59.040</a></span> | <span class="t">It's really good when looking at the compute to memory operations on these, you know, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3542" target="_blank">00:59:02.820</a></span> | <span class="t">uh, compute devices and stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3544" target="_blank">00:59:04.940</a></span> | <span class="t">So yeah, exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3546" target="_blank">00:59:06.620</a></span> | <span class="t">Like I think when we're even designing these models, we're like really setting dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3549" target="_blank">00:59:09.620</a></span> | <span class="t">such that it maps well into hardware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3551" target="_blank">00:59:11.620</a></span> | <span class="t">Um, so it's almost like, you know, given that we have this model data parallelism, we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3555" target="_blank">00:59:15.340</a></span> | <span class="t">like actually designing models more for it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3558" target="_blank">00:59:18.300</a></span> | <span class="t">But I also think that there's a ton of new, interesting distributed algorithms and stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3562" target="_blank">00:59:22.100</a></span> | <span class="t">like that, which makes designing models very interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3564" target="_blank">00:59:24.020</a></span> | <span class="t">Like I think one thing that I think is really cool is like the Microsoft zero partitioning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3567" target="_blank">00:59:27.740</a></span> | <span class="t">too, which also adds some really new, like nice implications for like how to design and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3571" target="_blank">00:59:31.780</a></span> | <span class="t">scale models and stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3572" target="_blank">00:59:32.900</a></span> | <span class="t">So yeah, I think there's like, this is a very fruitful research direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3576" target="_blank">00:59:36.380</a></span> | <span class="t">Um, if that, if that kind of answered your question, yeah, no, that was super helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3581" target="_blank">00:59:41.340</a></span> | <span class="t">Interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3582" target="_blank">00:59:42.340</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3583" target="_blank">00:59:43.340</a></span> | <span class="t">Yeah, definitely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3584" target="_blank">00:59:44.340</a></span> | <span class="t">Like I'm very optimistic on the future of us, like designing the hardware, the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3588" target="_blank">00:59:48.020</a></span> | <span class="t">the partitioning strategies altogether, because really to get it to work well, you kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3590" target="_blank">00:59:50.820</a></span> | <span class="t">have to know about all three and like kind of, you know, intertwined the development</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3594" target="_blank">00:59:54.380</a></span> | <span class="t">of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3595" target="_blank">00:59:55.380</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3596" target="_blank">00:59:56.380</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3597" target="_blank">00:59:57.380</a></span> | <span class="t">That sounds awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3598" target="_blank">00:59:58.380</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3599" target="_blank">00:59:59.380</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3600" target="_blank">01:00:00.380</a></span> | <span class="t">So just to summarize, it's like, yeah, so switch transformer is like a nice simplification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3604" target="_blank">01:00:04.460</a></span> | <span class="t">over a mixture of experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3605" target="_blank">01:00:05.740</a></span> | <span class="t">And we're seeing that we get really strong speed up improvements on pre-training over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3609" target="_blank">01:00:09.900</a></span> | <span class="t">like a lot of the T5 models, which are very strong baselines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3613" target="_blank">01:00:13.140</a></span> | <span class="t">We're seeing that we can, you know, efficiently distill the sparse models back to dense ones</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3617" target="_blank">01:00:17.420</a></span> | <span class="t">and, you know, get improved both pre-training and fine tuning through some of these newer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3621" target="_blank">01:00:21.580</a></span> | <span class="t">techniques we talked about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3623" target="_blank">01:00:23.500</a></span> | <span class="t">And we're also seeing that the models are working on multilingual data and that we can,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3627" target="_blank">01:00:27.260</a></span> | <span class="t">you know, now easily successfully train up to, you know, 1.6 trillion parameter models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3631" target="_blank">01:00:31.700</a></span> | <span class="t">which is pretty promising and, um, next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3635" target="_blank">01:00:35.500</a></span> | <span class="t">And so we also wanted to go into two slides about some like newer work about actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3638" target="_blank">01:00:38.540</a></span> | <span class="t">using these kinds of models for computer vision, and actually also a little bit of how they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3642" target="_blank">01:00:42.340</a></span> | <span class="t">can be used to actually do some level of like adaptive computation where not only now each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3647" target="_blank">01:00:47.060</a></span> | <span class="t">input gets different weights, but also sometimes different inputs will have different amounts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3651" target="_blank">01:00:51.240</a></span> | <span class="t">of compute applied to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3653" target="_blank">01:00:53.420</a></span> | <span class="t">And so there was some really great work of doing this out of the Google Zurich team.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3658" target="_blank">01:00:58.340</a></span> | <span class="t">And yeah, there's just doing it for image classification and, you know, they're basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3661" target="_blank">01:01:01.420</a></span> | <span class="t">seeing a lot of the similar types of scaling properties where, you know, scaling up the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3664" target="_blank">01:01:04.780</a></span> | <span class="t">number of experts and using sparsity allows them to get good performances on image classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3671" target="_blank">01:01:11.420</a></span> | <span class="t">Next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3674" target="_blank">01:01:14.780</a></span> | <span class="t">And interestingly, one of the things they do is like, as we talk about the capacity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3677" target="_blank">01:01:17.740</a></span> | <span class="t">factor, so we were talking about values of like one, 1.25, 2.0, which means like at a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3682" target="_blank">01:01:22.100</a></span> | <span class="t">value of 2.0, there's buffer for, you know, two tokens per expert, but they actually study</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3687" target="_blank">01:01:27.260</a></span> | <span class="t">it going less than one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3688" target="_blank">01:01:28.420</a></span> | <span class="t">So that means that like at 0.5, that means there's only like room for half the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3691" target="_blank">01:01:31.660</a></span> | <span class="t">of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3693" target="_blank">01:01:33.140</a></span> | <span class="t">And the nice part is, is that they did this for image classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3695" target="_blank">01:01:35.900</a></span> | <span class="t">And also in images, there's just a lot of redundancy and they noticed that you can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3699" target="_blank">01:01:39.700</a></span> | <span class="t">get really good performance by only allowing like, you know, up to one 10th of the parts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3705" target="_blank">01:01:45.320</a></span> | <span class="t">of the image to be processed by a sparse layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3707" target="_blank">01:01:47.340</a></span> | <span class="t">So yeah, we think this is like a really nice direction too, in terms of combining sparsity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3711" target="_blank">01:01:51.540</a></span> | <span class="t">along with like adaptive computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3714" target="_blank">01:01:54.660</a></span> | <span class="t">And yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3715" target="_blank">01:01:55.660</a></span> | <span class="t">And yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3716" target="_blank">01:01:56.660</a></span> | <span class="t">Thanks so much for having us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3717" target="_blank">01:01:57.660</a></span> | <span class="t">That's the, that's the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3719" target="_blank">01:01:59.660</a></span> | <span class="t">So thank you, Barrett and, sorry, Arifan, for coming here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3728" target="_blank">01:02:08.300</a></span> | <span class="t">So I will just like ask a bunch of questions and then we can have like a, after the class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3737" target="_blank">01:02:17.180</a></span> | <span class="t">open question panel for the students.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3740" target="_blank">01:02:20.020</a></span> | <span class="t">So one thing is like, have you tried using like, like more like linear attention mechanisms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3743" target="_blank">01:02:23.700</a></span> | <span class="t">like reformers and like other stuff to like scale the computation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3751" target="_blank">01:02:31.020</a></span> | <span class="t">I personally haven't, I haven't personally done this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3754" target="_blank">01:02:34.620</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3755" target="_blank">01:02:35.620</a></span> | <span class="t">So, oh, you know, I guess we can maybe comment on how, you know, the attention, the cost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3764" target="_blank">01:02:44.540</a></span> | <span class="t">coming from the attention maps isn't the dominant cause in, in this large transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3772" target="_blank">01:02:52.640</a></span> | <span class="t">So you know, the motivation for using linear attention, like performance is that it reduces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3778" target="_blank">01:02:58.260</a></span> | <span class="t">the quadratic cost of attention maps, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3783" target="_blank">01:03:03.460</a></span> | <span class="t">But so far, I mean, at least, you know, in like sort of typical NLP setups, like superglue,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3789" target="_blank">01:03:09.060</a></span> | <span class="t">C4 and so on, as you scale the models, most of the memory comes from the model weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3797" target="_blank">01:03:17.260</a></span> | <span class="t">as opposed to attention, to the attention maps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3800" target="_blank">01:03:20.460</a></span> | <span class="t">That's also because, you know, using very long context or sequence length doesn't prove</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3807" target="_blank">01:03:27.180</a></span> | <span class="t">that fruitful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3808" target="_blank">01:03:28.580</a></span> | <span class="t">And so, you know, just, you know, working with the vanilla self-attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3814" target="_blank">01:03:34.220</a></span> | <span class="t">is, is a very strong baseline already.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3817" target="_blank">01:03:37.020</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3818" target="_blank">01:03:38.020</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3819" target="_blank">01:03:39.020</a></span> | <span class="t">So another question is like, do you think this like mechanism is even more scalable?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3823" target="_blank">01:03:43.660</a></span> | <span class="t">Like, can you go on and be like 10 trillion parameter models, stuff like that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3827" target="_blank">01:03:47.560</a></span> | <span class="t">Like what do you think?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3828" target="_blank">01:03:48.560</a></span> | <span class="t">Yeah, definitely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3829" target="_blank">01:03:49.560</a></span> | <span class="t">I think, yeah, totally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3830" target="_blank">01:03:50.560</a></span> | <span class="t">I think, honestly, the, one of the biggest constraints is that like, you know, and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3835" target="_blank">01:03:55.300</a></span> | <span class="t">isn't even necessarily a constraint, it's just like, you have to fit the parameter somewhere</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3839" target="_blank">01:03:59.860</a></span> | <span class="t">and there's just limited storage on devices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3841" target="_blank">01:04:01.780</a></span> | <span class="t">But if you get enough devices such that, you know, yeah, you can just partition the weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3845" target="_blank">01:04:05.420</a></span> | <span class="t">It's like, yeah, I don't see anything stopping it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3847" target="_blank">01:04:07.940</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3848" target="_blank">01:04:08.940</a></span> | <span class="t">So what do you think, like, personally, is your, like, the thing, like, with the direction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3854" target="_blank">01:04:14.120</a></span> | <span class="t">like, like scaling of transformers will go into, like, will there be more like works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3858" target="_blank">01:04:18.480</a></span> | <span class="t">that are trying to just like use this transformer, like mechanisms, like Mr. Experts, or do you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3863" target="_blank">01:04:23.340</a></span> | <span class="t">think there's like, you're going to be other things that the community needs?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3865" target="_blank">01:04:25.860</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3866" target="_blank">01:04:26.860</a></span> | <span class="t">I mean, I definitely think mixture of experts should find its way, or at least, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3869" target="_blank">01:04:29.780</a></span> | <span class="t">sparse players like switch transformer and stuff will definitely, I think, find their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3872" target="_blank">01:04:32.580</a></span> | <span class="t">way into like the future of large models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3874" target="_blank">01:04:34.120</a></span> | <span class="t">I think they really confer a lot of benefits and they're also very good in like high throughput</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3878" target="_blank">01:04:38.780</a></span> | <span class="t">applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3879" target="_blank">01:04:39.780</a></span> | <span class="t">So I think the one thing, like, so the one downside is on sparsity is like, if you look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3883" target="_blank">01:04:43.300</a></span> | <span class="t">at the performance per model weight, they're going to always be worse than bonds models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3887" target="_blank">01:04:47.700</a></span> | <span class="t">So it's like, if you really are constrained on like, I want to design the best model I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3891" target="_blank">01:04:51.120</a></span> | <span class="t">can to fit on as small of a device as I can, then they're probably not going to be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3895" target="_blank">01:04:55.140</a></span> | <span class="t">best solution because the sparse weights just aren't as good as just the dense weight that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3899" target="_blank">01:04:59.320</a></span> | <span class="t">being used for everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3901" target="_blank">01:05:01.020</a></span> | <span class="t">So I think it really depends on the application, but I'm very optimistic for when we're training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3904" target="_blank">01:05:04.660</a></span> | <span class="t">these models during pre-training with lots of data parallelism, and then we're serving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3907" target="_blank">01:05:07.740</a></span> | <span class="t">them in like medium to higher throughput examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3910" target="_blank">01:05:10.220</a></span> | <span class="t">I feel like they could actually just be a pretty big win.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3913" target="_blank">01:05:13.800</a></span> | <span class="t">So that that's kind of my thoughts on, on how I think sparsity will be used in terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3917" target="_blank">01:05:17.300</a></span> | <span class="t">of other things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3918" target="_blank">01:05:18.300</a></span> | <span class="t">Yeah, I think, I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3919" target="_blank">01:05:19.300</a></span> | <span class="t">There's a ton of exciting research, you know, from everything from, yeah, like a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3921" target="_blank">01:05:21.740</a></span> | <span class="t">the linear attention stuff, adaptive computation, new pre-training objectives, you know, yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3927" target="_blank">01:05:27.260</a></span> | <span class="t">it's hard to know what the future will look like, but yeah, a lot of exciting things to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3930" target="_blank">01:05:30.940</a></span> | <span class="t">look forward to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3931" target="_blank">01:05:31.940</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3932" target="_blank">01:05:32.940</a></span> | <span class="t">Sounds good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3933" target="_blank">01:05:33.940</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3934" target="_blank">01:05:34.940</a></span> | <span class="t">So we can just now have like a round of student questions, so we'll just stop the recording.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3937" target="_blank">01:05:37.780</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3938" target="_blank">01:05:38.780</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3939" target="_blank">01:05:39.780</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3940" target="_blank">01:05:40.780</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&t=3940" target="_blank">01:05:40.780</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>