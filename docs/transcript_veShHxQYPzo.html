<html><head><title>AI Engineer Summit 2023 — DAY 1 Livestream</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>AI Engineer Summit 2023 — DAY 1 Livestream</h2><a href="https://www.youtube.com/watch?v=veShHxQYPzo" target="_blank"><img src="https://i.ytimg.com/vi/veShHxQYPzo/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>I can't feel it, chasing the light is all we know. Chasing the light is all we know. Can't stop it, I won't let it go. Chasing the light is all we know. Can't stop it, I won't let it go. Like a fever, burning up here we're on fire. True believer, we're looking on the wild.</p><p>I can see it, on the horizon on the rocks. We can feel it, chasing the light is all we know. Like a fever, burning up here we're on fire. True believer, we're looking on the wild. I can see it, on the horizon on the rocks. We can feel it, chasing the light is all we know.</p><p>Ladies and gentlemen, the opening keynote presentations will commence in the ballroom starting in 15 minutes. Please make your way to the ballroom and find your seats. Thank you. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back.</p><p>We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back.</p><p>We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back.</p><p>We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back.</p><p>We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back.</p><p>We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back.</p><p>We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back.</p><p>We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. We'll be right back. Bye. We'll be right back. We'll be right back. We'll be right back. Bye. Bye.</p><p>Bye. We'll be right back. We'll be right back. We'll be right back. We'll be right back. Ladies and gentlemen, the opening keynote presentations will commence in the ballroom starting in 10 minutes. Please make your way to the ballroom and find your seats. Please make your way to the ballroom and find your way to the ballroom and find your way to the ballroom.</p><p>Please make your way to the ballroom and find your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom and find your way to the ballroom.</p><p>Please make your way to the ballroom and find your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom.</p><p>Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom.</p><p>Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom.</p><p>Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom.</p><p>Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom.</p><p>Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom.</p><p>Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom.</p><p>Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom. Please make your way to the ballroom and find your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Thank you. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Ladies and gentlemen, the opening keynote presentations are starting now. Please find your seats. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. The ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom. Please make your way to the ballroom.</p><p>Launch control. We have a go. Roger. Launch control. Ladies and gentlemen, please welcome to the stage the co-founder of the AI Engineer Summit, the managing partner of Software 3.0 LLC, and your host, Benjamin Dunphy. Engineers, founders, friends, sponsors. partners, colleagues, and friends. Welcome to the AI Engineer Summit 2023.</p><p>I am... Yeah. Yeah. I am deeply honored to have the opportunity to host you all at this event with my co-host, Swix. And I am especially delighted to kick off the presentation portion of the summit. We have curated two days of stage talk content for you from some of the top founders and engineers in this versioning new industry of AI engineering.</p><p>In just a few moments, Swix is going to come on stage to help set the context for those talks. But for my time on stage, I really want to know who is here. A few months ago, they were just an open source project. Mind you, the fastest growing open source project in history, but just an open source project.</p><p>Now, they are the presenting sponsor of the AI engineer summit. So needless to say, we can expect some big announcements when Torrin takes the stage in a few moments. Let's see who else is here. Superbase! Superbase is here. The social media experts who make product launches fun, and who make databases both easy and trustworthy, given that they are open source.</p><p>They are a diamond sponsor this year, and we are honored that they broke their no event sponsorships policy for this event. So Paul, Ant, let me know how that policy change works out for you. Fixie is here. Where is Fixie? I am looking forward to asking their booth sidekick if they can see what I am wearing yet, and if they can recommend any updates to my wardrobe.</p><p>We will call this sidekick, Derek. And if vision is not in the cards yet, Matt and Ben, I can make an intro to Logan Kilpatrick, and perhaps we can get a preview of that vision API. Microsoft is here. The company who helped to commercialize this movement by pumping $13 billion into open AI.</p><p>And many of you might be thinking, well, how can I get some of that gravy for my startup? Well, head over to their booth in the Carmel room, just past the elevators behind you, and have a chat. They have representatives from Microsoft for Startups who can help to open those doors for you.</p><p>And if you have any questions for the co-pilot team, well, Microsoft's booth is right next to GitHub. Oh, and happy belated birthday, Cloudflare, who is here. Round of applause. They have a... Cloudflare, you are now 13, so that means you're going to be going through some changes. But don't worry, there's plenty of folks who will be delighted to talk to you about those changes at your booth, just next to Microsoft and Carmel across the elevators.</p><p>Alessio Finelli is here. The VC half of the Latent Space podcast, but no less technical than Swix. Perhaps after watching some of today's talks, we can convince him to come back to the founder world. A little bit easier, right? But this time, Alessio, let's get you a software startup and leave the hardware to the experts.</p><p>Some of you got that joke. And who else is here? Well, of course, you. You're one of 500 people who were selected to attend the inaugural AI Engineer Summit. So give yourselves a round of applause. This means that you're not only an experienced software engineer, but that you're actively experimenting with, shipping to production, or have founded an AI-enhanced or AI-native apps and companies.</p><p>So I want you to remember this when you're interacting with one another. But we also want to recognize the many people who are not here today, but are watching the live stream. We thank you and appreciate you, and we hope that you enjoy the content on the live stream.</p><p>And be sure to watch tomorrow's opening keynote address for some exciting announcements for 2024, which will expand our participation limits. But for those of you who are here, we hope that you've been enjoying your conversations with other attendees so far. One of the reasons that I really love conferences is that it brings together experts from around the world, who are passionate about a singular niche subject.</p><p>And so most of your conversations are already high signal. But is there a way we can actually improve this? Many of you have downloaded our conference mobile app, Network, and have been enjoying its use. It includes the conference schedule, a full list of all of our expo partners, recommended restaurants, bars, and cafes within walking distance of the venue, and other event information.</p><p>But in addition to these features, we're pleased to announce AI-enhanced matching. Using our generative matching algorithm, which is a fancy way of saying, we use LLMs to match you with the right people, we can match you with people who help to solve your stated problems. We all have problems we need to solve.</p><p>That's why in your network profile, you can literally tell us what problem you're looking to have solved, and our GMA will connect to you with the right people. We call this app Network. You can download the app today at ai.engineer/network. The matching is currently limited to in-person attendees of this summit only, but we have one more big announcement that we'd like to make.</p><p>We are pleased to announce that we are open sourcing Network, both the event app and the matching algorithm. Our goal is to provide a simple, yet powerful, mobile event experience for attendees at any event around the world. And we hope that the matching algorithm can better assist to connect people at those events.</p><p>And we'd like to thank our lead engineer, Simon Stermer, for the mobile app, and Sweezetz Teller for our identity matching algorithm, and our infrastructure partners, dscope for auth, and Supabase for database and pgvector. So go talk to them about pgvector outside. So a round of applause for all of our developers, partners for this app.</p><p>So you can access the repo today at ai.engineer/network or github.com/ai.engineer/network. So with that, I'd love to welcome our first speaker to the stage. He's the co-host of the Latent Space Podcast and the co-founder of this very summit. Please join me in welcoming to the stage, SWIX. Thank you, everyone.</p><p>Hello, everyone. Hello, hello. Is this working? No. Okay. It sounds like it's working. Okay. Give it up for Ben. This conference would not be happening without him. A few logistical things. One, I'm carrying a magic trackpad because everyone has clickers. What if we had multiple dimensions? So we're going to experiment with this today.</p><p>And two, I'm using like AI, like fancy new everything, right? So this is Tome, and we're going to go two-dimensional with our slides as well. So I'm here to talk about the AI engineer. You're all here because you believe that there's some value to this idea. And then I just put like a ridiculous 1000x on this.</p><p>But I do think there is some meaning towards thinking about higher orders of magnitude towards raising your ambitions. And that's what I would like all of you to do today and to do with your friends back home. So, and obviously a lot of AI-generated art because, I mean, it's an AI conference.</p><p>You've got to do it. You've got to do it. First of all, I want to congratulate you on being here. I'm not talking about here location-wise, physically. I'm talking about here in terms of the point in time. Imagine if you were a mathematician. When was the best time to be born?</p><p>I would propose, around about 600 AD, this dude, Brahma Gupta, he invented zero. Pretty novel invention. It took us only 4,000 years to do that. But there's certain times where if you're in that field, you have to be there. That's the thing. If you're alive during that time, you have to be doing that thing.</p><p>Physics. When was the best time to be born? There's a right answer. 1905, 1927. And this conference kind of is inspired by the Solvay conference. That's Albert Einstein, Marie Curie, and a lot of people that you just saw in the Oppenheimer movie. Same thing. If you made cars, there was a right time, 1900 to 1930.</p><p>If you made personal computing products, 1980 to 2010. If you're a millennial, if you're very online, you ever get these memes like you're born too late to explore the earth, born too early to explore the stars. You're not too late. We are here. This is, based on demographics and history, the approximate timeline of all of humanity.</p><p>We know that we're roughly about 73% of all concurrent intelligences if we don't expand our own intelligences or go to other planets. So my argument and my message to you today is that you are just in time. And the timing is right to 1000x. I think a lot of my technology and industrial organization thinking is informed by Carlotta Perez, one of the most influential thinkers on tech revolutions.</p><p>So she wrote this book about the installation and deployment periods of tech cycles. And we're definitely going through one today. A lot of you on your mind here, I know you're here, but also mentally you're back home thinking, how much of this is a fad? How much of this is Web 3 again?</p><p>And we've seen this over and over. The historians greater than us have explored this over the industrial revolution, the age of railways, the age of heavy engineering and steel, oil, and most recently the tech revolution. Funny enough, I recently put a -- they all roughly span between 50 to 70 years, and if you're around in that time, that's the field to be pursuing.</p><p>So when did the AI revolution start? We're very lucky. It's very hard historically to place a start point on something that changes human civilization. We have a moment, 2012, Alex Nett. We're roughly 10 years on. And we can put numbers to it, right? So most of the time these curves are sort of theoretical.</p><p>They're just kind of like bleh. Like y-axis is just bleh. Here we can actually just put the amount of compute we're using towards training in models. There's a huge inflection. That's Alex Nett right on the blue dot over there. That's a huge inflection where we realized, gradually realized, it took too long to realize, but scale is starting to work.</p><p>And if you actually take this out, a lot of people have been taking this out, and I want you to take scaling seriously. There's three reasons why six is a magic number. There's a very famous investor who I shall not name. Imagine roughly six of the magnitude and more compute by the end of the decade and plan for that.</p><p>So there is more of this coming, linear projection-wise. And you can plan on a lot more investment in language models. John Carmack says there's six key insights towards AGI. And lastly, George Hatz has these really nice analogies. GPT-3 took about one person year of compute. GPT-4 took about 100 person years of compute.</p><p>You stretch it out to GPT-10, the difference between GPT-4 and GPT-10, again under the six-fold increments in GPT advancements. And that would be more compute than the equivalent compute of every human who ever lived. So just being in the right moment, you will get to live on top of these mega, mega trends that is greater than any single one of us.</p><p>And I think you're all here thinking about the AI engineer. And I put it in a very, very small local context of, hey, what's the org chart? And where do the ML engineers sit? Where do the ML researchers sit? Where do the software engineers sit? And what's the gap that's opening?</p><p>It's the AI engineer. It's very much of a demand and supply argument. There's something like 100,000 card-carrying data science machine learning engineers. And GitHub claims to have 100 million registered developers. I don't know what the real number is. You can debate 40 or 50 million to 100 million. It's orders of magnitude more.</p><p>So we think there's going to be much more AI engineers than ML engineers. There's all these reasons why. Same reasons that I mentioned in the blog post that you've all read. And also, why engineering and not just prompting is because LLMs themselves are not AGIs yet. We actually have to coordinate them in systems of software.</p><p>We have to write code around them and orchestrate them with code in order to do something useful. But we already know how to do code. So I want to spread it out a little bit more. I think that the conversation on AI engineer has a vague discrepancy. And I want to basically split it out into three areas of AI engineer.</p><p>Software engineer enhanced by AI tooling, like co-pilot. Software engineer building AI products, like mid-journey. AI product that replaces human engineer, potentially like auto-GBT and maybe replica ghost rider. So let's give these guys a name. And in case you're wondering like enhanced by versus replaces, I think about it very much like the self-driving car terms.</p><p>Like level two, level three. There's a difference between whether humans in the loop or humans as to fall back. So let's name it. Three major types of AI engineer. The AI enhanced engineer, for people who are enhanced by AI. People who build AI products, AI products engineer. And then the AI engineer agent, who is not human.</p><p>And naturally, of course, if you're interested in sort of progressing up the career ladder, there's AI enhanced engineer, then products engineer, and engineer agent. So this talk was really inspired by actually Amjad, who's speaking next, where he did a recent talk on with the AXNCD podcast, and Sam Altman, who actually sees 1,000 engineers in OpenAI every day.</p><p>And it's really a set of stackable 10 by 10 by 10 improvements. Over the course of the next two days, I think you'll be seeing a lot of the speakers will be working on different parts of the stack. So I really encourage you to think about where in your life this AI movement can improve and increase your productivity.</p><p>I'm very, very honored to have drawn from all over the world the leading lights of the AI engineering movement. We are a very small room today. I do think we can 100 and 1,000 X from here. And it's not just about tools and speakers, it's also about you. So I highly encourage you to take part in all the opportunities that we have for you to mix and mingle with each other, with the speakers, and with the sponsors as well.</p><p>So there's that. The final word I do want to offer you is effectively what I think, in terms of non-technical terms, the 1,000 X engineer could offer. My favorite advice for what a 10 X engineer could look like is an engineer that teaches 10 other people what they know.</p><p>That's not a technical term, but it is very useful. And there's all these scaling laws for networks, which I really keep in mind. So you can go from O of N to O of N squared to O to the power of N. But really what O of N is, is you attending all the talks and consuming all the content and letting people in with your Pac-Man rule.</p><p>O of N squared is helping others learn. My very first blog post was at exactly a conference like this where I was encouraged to write something. And of course it was on machine learning. And finally, going home and then building your own networks of AI engineers and helping to grow networks of learning as well.</p><p>So I hope you take that with you in your AI engineer journey. I hope that over the next few days you get a sense of what it's like to be at the start of an industry. And I'm just glad to be here with you. Thanks so much. All right.</p><p>Excited to be here. I agree with Swix and Ben that it feels like a moment. It feels like a historical moment here. My name is Amjad. I'm the co-founder of Replit, where we aspire to be the fastest way to get from an idea to a deployed software that you can scale.</p><p>So I'm going to take you back a little bit, not like Swix to the 600 AD, but perhaps to the start of computing. If my clicker works. It does not work. So next slide. We're going to get AGI before we get good presentation software. All right. Here we go.</p><p>All right. So very early computers. The ENIAC was the first Turing complete programmable Von Neumann machine computer. The way you programmed it is like you literally punched cards. Not physically, but you had a machine that sort of punched these cards. These are sort of binary code for the machine to interpret.</p><p>It was really hard. There wasn't really a software industry because this was really difficult. It automated some tasks that human computers did at the time. But it didn't create the software industry yet. But then we moved to text from punch cards. And we had first assembly. And then we had compilers and higher level languages such as C.</p><p>And then someone invented JavaScript. And it's all been downhill since then. But text editors were really -- or like text-based programming was at minimum a 10x improvement, if not a 100x improvement in programming. So we've had these moments where we've had orders of magnitude improvements in programming before. And then, you know, the IDE became a thing because, you know, we had large-scale software.</p><p>This is a screenshot from like 2017 or '18 when we added LSP to every programming environment on Replit. So anyone with an account can get IntelliSense. And we're really proud about that at the time. We're burning a lot of CPU doing sort of inference. And, you know, if you've run TypeScript server, that's like a lot of RAM.</p><p>But we're really proud that we're giving everyone in the world tools to create professional-grade software. About three, four years ago, we started kind of thinking about how AI could change software. It actually started much sooner than that. But with GPT-2, you know, you could sort of kind of, you know, give it some code and kind of complete part of it.</p><p>And we're like, okay, this thing is actually happening, and we better be part of it. And so we started building, and we built this product called Ghostwriter, which does auto-complete, chat, and all sorts of things inside the IDE. And in just those two years, I mean, the pace of progress across the industry, the tools, basically AI, you know, was deployed, and a lot of different engineers were using it.</p><p>The AI-enhanced engineer, as Wix kind of called it. Everyone is sort of using these tools. And so we have a world now where a lot of people are gaining a huge amount of productivity improvement. I don't think we're at a mode of magnitude improvement yet. We're probably in the 50, 80, perhaps 100% improvement for some people.</p><p>But we're still at the start of this. And we think that's going to be 10x, 100x, perhaps 1,000x over the next decade. The problem, however, Replis' mission has always been about access. Our mission is to empower the next billion developers. And so we really didn't want to create this world where some people have access to Ghostwriter and other people don't have access to it.</p><p>And we started thinking about, okay, what is it -- if you really take into heart everything that the AI engineer conference is about, that we're at a moment where software is changing, where AI is going to be part of the software stack, then you have to really step back a little bit and try to rethink how programming changes.</p><p>So our view is these programming add-ons such as Copilot and Coding and Ghostwriter and all these things. We're giving them cute names. We think that's not the way forward. We think that AI needs to be really infused in every programming interaction that you have. And it needs to be part of the default experience of Replit and, I'm sure, other products in the future.</p><p>That's why we're announcing today that we're giving AI for our millions of users that are coding on Replit. And so we think this is going to be the biggest deployment of AI-enhanced coding in the world. We're going to be burning as much GPU as we're burning CPU. So pray for us.</p><p>We have people all over the world coding on all sorts of devices. We have people coding on Android phones. And they're all going to get AI now. So they're all going to be AI-enhanced engineers. But, you know, as we showed, it's not just about AI-enhanced engineering. There's also product.</p><p>So AI being part of the software creation stack makes sense. But AI part of the call stack is also where a lot of value is created. So that's why we're also -- we have this new product called Model Farm. And Model Farm basically gives you access to models right into your IDE.</p><p>So all it takes is three lines of code to start doing inference. We launched with Google Cloud LLMs, but we're adding LLAMA pretty soon. We're adding Stable Diffusion. And if you're an LLM provider and want to work with us and provide this on our platform, we'd love to talk to you.</p><p>But basically, everyone will get -- there's some free tier here. Everyone will get free access at least until the end of the year to Model Farm so you can start doing inference and start building AI-based products. So next up, I'm going to bring up my colleague, the head of AI, Mikayla Katasta, to talk about how we train our own AI models.</p><p>And we have one more announcement for you coming up. Woo! Do you need a clicker? It's fine. All right. Thank you. All right. Hi, everyone. So today I'm going to be talking about how we're training LLM for code at Replit. And I will explain why this weird title. If you've been around Twitter, I think a bit more than a month ago, you must have read this study from Semi-Analysis.</p><p>And their point was, it's meaningless to work on small models, train on a limited amount of GPUs. And that came as a shock to us, because we had a very good success story back in May, where we started to train our models from scratch. And then, you know, Hamjad and I, and the AI team, started to think, are we really wasting our time here?</p><p>I'm going to try to convince this actually is not the case. So our code completion feature, or Replit, is powered by our own bespoke larger language model. We trained an open source code, both published on GitHub and also developed by the Replit user base. It's a very low latency feature.</p><p>So we try to find a different sweet spot compared to what you might be using with other plugins. We try to keep our P95 latency below 250 milliseconds, such as the developer experience is almost instantaneous. You don't even have to think about it, and the code is going to be completed for you.</p><p>At the model size that we were using, we have been state of the art across the past few months. And let's do a show of hands. Who has heard about our V1 model back in May? All right, that feels good. For a second I feel like an AI star.</p><p>Jokes aside, so we released Replit code V1 3B back in May. We got a lot of adoption, a lot of love, and also a lot of contribution. And that's one of the key reasons why we decided to give it back. Replit history has been built on the shoulders of giants, of all the people contributing to the open source space.</p><p>So we thought we should do exactly the same year. We should give back our model. And today, I'm going to be announcing Replit code V1.5 3B. So the evolution of the model that we released back in May. Let's go in detail as Amjad was saying. So the next 10 minutes, we're going to do a technical deep dive.</p><p>And I'm going to tell you how we built it and why it's so powerful. So first of all, we followed a slightly different recipe compared to the last time. If you recall, back in May, our V1 was a Lama-style code model, which means we followed a lot of the best recipes that Meta pioneered.</p><p>Now we went, you know, one level up, and we are training up to 300 tokens per parameter. So if you have been following a big history of LLMs, even two years ago, most of the models were under-trained. Pardon me for the word. It's not exactly -- technically speaking, it's not correct.</p><p>But the truth is, you know, mid-2022, the Chinchilla paper from the MIME came out, and it was like a big warning for the old field. Basically, what the paper tells us is that we were under-training our models. We should give them way more high-quality data. And in exchange, we could train smaller models.</p><p>So in a sense, we're amortizing training time for inference time. Spending more compute to train a smaller, more powerful model. And then at inference time, the latency will be lower. And that's the key insight that we're going to be carrying along, you know, this old keynote today. Now, differently from the V1, this time we also doubled the amount of high-quality data.</p><p>So we train it up to one trillion tokens of code. It's -- the data mixture is roughly 200 billion tokens across five epochs, plus a linear cooldown at the end that really allows us to squeeze the best possible performance for the model. And rapid code V1.5 this time supports 30 programming languages.</p><p>And we also added a mixture coming from Stack Exchange, posts that are oriented towards developers. So questions about coding, questions about software engineering, and so forth. So this is the basis of our data. Now let's go ahead and take a look inside of the dataset that we used. So we started from the Stack, which is an initiative led by BigCode.</p><p>It's a group, you know, under the Hagen-Phase umbrella. Very grateful about the work that these people have been doing. Basically, they have built a big pipeline, getting data from GitHub, selecting top repositories, cleaning up parts of the data, and then especially leaving only code that is licensed under permissive licenses, such as MIT, BSD, Apache 2, and so forth.</p><p>Out of this mixture, we selected 30 top languages. And then, really, the key secret ingredient here is how much time we spent working on the data. You must have been hearing this again and again. And every time you go to an LLM talk, there is a ground stage saying, "Hey, you should pay attention about data quality." I'm here to tell you exactly the same once again.</p><p>That's probably the most important thing that you could be spending your time on. Especially because the model I'm talking about today is trained from scratch. So this is not a fine tuning. All the models that we released have been trained from the very first token prepared by us. So it's extremely important to have high data quality.</p><p>So we took inspiration from the initial quality pipelines built by Codex, by the Pound paper. And then we applied way more heuristics there. So we're filtering for code that has been auto-generated, minified, non-parceable. Basically, all the code that you wouldn't want your model to recommend back to you. Because it's not something that you would be writing yourself.</p><p>We also removed toxic content. And all this pipeline have been built on Spark. So I'm trying to encourage you to also think of working on your own models. Because pretty much a lot of the base components are out there, available open source. So you could really build the whole pipeline to train and serve an LLM with a lot of open source components.</p><p>And as Wix was saying, we have seen this crazy acceleration in the last nine months. If you wanted to do this in 2022, good luck with that. It feels like we're a decade ahead compared to last year. So it's pretty amazing. And I didn't even expect in myself the speed to move this fast.</p><p>The other insight that we kind of pioneered for our V1 model, and turns out to be very powerful also for this new one. So when we released the V1, a few weeks after, coincidentally, a very interesting paper has been published called "Scaling Data Constraint Language Models." And I highly recommend it.</p><p>It's a great read, and it's probably one of the most interesting results in LLM, in my opinion. And this intuition allowed us to basically train the model to completion, rather than making trade-offs on the data quality. It allowed us to select a small, high-quality subset of data, and then repeat it several times.</p><p>The key finding of this paper is basically in these two plots. I'm going to be sharing the slides so you can go and check the links. And the idea is your loss curve, after you repeat data four or five times, is going to be comparable to training on a novel dataset.</p><p>Okay? Now, not only this is very useful, because it allows us to work only on high-quality data. It also allows us to work with data that is exclusively released under permissive license. Therefore, once again, for our 1.5 model, we're going to be releasing it open source, and it's going to be released with a commercially permissive license.</p><p>So you can use it. There you go. Just shoot us an email when you use it, because I'm very curious. If you're having a good time. So details about the model training. We change a few things here and there. It's a slightly larger model. It's a 3.3B. It's 4K context.</p><p>The old one was a 2K. We train a new domain-specific vocabulary, 32K, so a small one. It helps us to achieve even higher compression on the data. If you've been reading, again, about LLMs, you know that from a simplistic point of view, there are data compressors. Lots of data compressors.</p><p>So if your vocabulary allows you to pack even more data on fewer tokens, then you're basically bringing more signals to the model while you're training. And with this new vocabulary, we're squeezing a few percent extra, and it's a better vocabulary for code compared to what StarCoder or CodeLAM are using today.</p><p>We train the 128 H100 80GB GPUs, which are as rare as, you know, gold at this point. We have been on the Mosaic ML platform for a week. And to our knowledge, this is the first model officially announced to be trained on H100s and release open source. So we're very excited about it.</p><p>And we follow a list of LLM best practices. So, of course, we support flash attention. We have group query attention, which allows us to achieve better inference performance. Alibi position embedding, latest optimizers in the game. And that, you know, is really the reason why, at the end, you will see very exciting numbers that I don't want to spoil right away.</p><p>So let's start from the base model, and then there is surprise coming. This is the evaluation passed at one on UMineval. For those of you who have never heard about it, UMineval is a benchmark release back in 2021 by OpenAI, if I recall correctly. The format is the following.</p><p>You have a natural language description of a task in English, and then expect the model to generate a self-contained Python snippet that then is going to be tested with a test harness. So you generate code, and then you execute it, and you see if the values in output are exactly what you expect.</p><p>Now, an interesting evolution in the last few months in the field is we were not content on benchmarking exclusively on Python. So we're also doing that across several different programming languages. And this is coming from the multilingual code, EvalHarness, again, built by BigCode. And they also maintain a very interesting leaderboard.</p><p>So what they do is they take models across, you know, several companies and several open-source contributors. They run EvalHarness themselves, and then they compile these very interesting leaderboards. So you will find us there, I guess, in a few days. So from the left column, we have StartCoder 3B, which, as of yesterday, was a state-of-the-art model at the 3B parameter size across languages.</p><p>And today, our WIP 1.5 is basically optimal across every single language that you see on the list. But what gets me excited is not that much of the fact that we are more powerful than StartCoder, which has been released a few months ago. What got me hyped, you know, when we were training it is that we're very, very close to call Llama 7B.</p><p>So as a reminder, call Llama 7B is a Llama 2 model from Meta, the 7B version, which has been trained on 2 trillion tokens of natural language. And then it has an additional pre-training phase of 500 billion tokens exclusively on code. So it's a model that is twice the size, it's 2.5X more data, way more GPU compute.</p><p>So you see where I'm going, you know, we're getting very close. How do we surpass Code Llama? Here is the trick. This is the other model that we have been training in parallel, and this is the REPL-tune version. And it means the following: we further pre-trained it on 200 billion tokens of code, this time coming from our own developers.</p><p>So on REPLIT, when you create a public REPL, it's automatically published under IMAT license, so we use this code to further pre-train our model. And we extract, again, 30 billion tokens of code, same languages, same data filtering pipeline to retain only the top-quality ones. We do these three epochs, then we do also linear cooldown, and we are using, basically, the languages that are predominantly popular for REPLIT users.</p><p>So not the same list as we saw before. If you go on REPLIT, I would say 95% of the people are mostly writing Python and JavaScript. These are the cool languages of today. Another key insight is our cutoff for this model is literally a few weeks ago. So if there is a cool new library that everyone is writing software for in the last month, our model is going to be capable of generating code that follows that library.</p><p>And we are going to keep, basically, these models up to date so that we can follow the trends and we can make our developers more happy. Here is the table that I love. So we are back to this back-to-back comparison. And on the very left, we have our base model.</p><p>We didn't add StarCoder here for the sake of space. And also, the base model is already topping it on every other language, so it didn't make sense. Now we have Colama in between, and you can see why. We are, on pretty much every language, substantially better. So we have 36% on the OpenAI UMinevald benchmark.</p><p>As a reminder, when I was working on Palmcoder, for example, that was our Passed One result that we published in early 2022. That model was at 540 billion tokens. So almost 200x larger than this model. And it achieves exactly the same UMinevald Passed One performance. Same Code Da Vinci 001, if you go back to the paper, is getting exactly 36%.</p><p>So we were pretty much amazed when this happened. Now, why do we go through all this struggle of training our models? Not only because it's cool, you know, we love to do this stuff. But there is a rationale behind it. So we really want to go as fast as possible with the most powerful small model we could train.</p><p>And the reason is, all of our models are actually optimized for inference, rather than for being awesome at benchmarks. The fact that that happens gives us a lot of pride, and also makes us feel good when we do a wipe check with the model. And it performs as we expect, or even better.</p><p>But it turns out that our key result is, on a single model, with no batching, we're generating above 200 tokens per second. And we tune the architecture for speed in every possible way. We're training a smaller vocabulary, as I was saying before. We're using a flash of tension with a Triton kernel.</p><p>We're using the latest GQA. So every single aspect is there to make sure that we can go as fast as we can. And we optimize, basically, for the usage on the Triton inference server and acceleration framework, such as TENSOR RTLLM. They really squeeze, you know, the last drop for NBA GPUs.</p><p>But the other very interesting insight is, we work very hard, also, to make the model deployment go much faster. So if you ever, you know, had the bad luck to work with Kubernetes in your life, you know, you know how painful it can get, you know, to get your pod, and, you know, download all the dependencies, and build it, and yada, yada, you know.</p><p>So the very first time we brought this infrastructure up, it took 18 minutes to go, you know, from clicking until the model was deployed. Now, if you want to, you know, adapt to the load that the application is receiving, 18 minutes, you know, looks like an eternity. Like, if there is a traffic spike, good luck with that.</p><p>So one of our awesome engineers, Bradley, you're going to find him at the booth later today, brought this number from 18 minutes to just two minutes. There is a long list of tricks that he used. I'm not going to go through them, just talk to Brad. The cool insight here is the fact, now, whenever we get more load, we can react very quickly.</p><p>And that's how we serve a very large user base. So the moment that Amjad announced AI4ALL literally 10 minutes ago, we flipped the switch, and now code completion is in front of our users. And that's the way we made this happen. Now, I've been asked several times, guys, why are you losing your model open source?</p><p>You put so much effort. Maybe not. That's an advantage for a company. It turns out that the moment we did it, we got a lot of adoption. And apart from a lot of log, which always feels good, and it feels good to chat with other people in AI that are using what we build.</p><p>We also started to get fine-tuned versions, instruct-tuned versions of that. And we have seen a lot of people using our small model deployed in local, say with GJML, which goes super fast on Apple Silicon. And they built their own custom privacy-aware, like GitHub Copilot Alternative with Rapid V1. So we expect the same to happen with V1.5 in the next few days.</p><p>As we speak also, if you go on ag-in phase, the model is available. We're working on the README. Come to talk with Madhava the boot. It's the mastermind behind it, so it's going to tell you every single detail on how to make it run in production. And we're going to be here until tonight, so more than happy to play with the model together.</p><p>Now, in the last minute that I've left, I want to give a teaser of what we're going to be doing in the next few weeks. So we've aligned a few very exciting collaborations. The first one is with Glaive AI, and it's a company that is building synthetic datasets. And we're working on an IFT version of our model, so an instruct-fine-tuned version, over 210,000 coding instructions.</p><p>We're already seeing very exciting results. We want to triple-check them and, you know, follow our Twitters. And the moment that we're sure that this is performing as we expect, it's going to be out there, and you're going to be able to play with it. Second announcement. We're also collaborating with Morph Labs.</p><p>I think Jesse is here today, and he's going to run a session later explaining you exactly what this new format does. I'm going to give you a teaser, and then, you know, go to Jesse's talk, and he's going to explain you all the details. So we are design partners on the FIST format, which is fill in the syntax tree.</p><p>You might have heard of fill in the middle, this concept where you can take your file, split it in a half, and then, basically, if you're writing code in between, you can tell the LLM that the top of the file is your prefix, the bottom of the file is your suffix, and you give this context to the model so that it knows which part it should fill.</p><p>Now, we found that this format is even more powerful, is aware of the abstract syntax tree underlying the source code. We're seeing very promising results already. And, again, this will be out, you know, in just a matter of, like, a few days or weeks. Last thing. We have collaborations with the Perplexity AI guys.</p><p>You might have used their labs. So it's a place where the host models incredibly fast, and the Rapid B1.5 will appear there, and you can start to play with it and get a vibe check by tonight. Thanks, everyone. Ladies and gentlemen, please welcome to the stage the inventor of AutoGPT and his team, Torrin Bruce Richards.</p><p>Thank you, San Francisco, for the warm welcome. I'm Torrin, the creator of AutoGPT, and I'm excited to show you all what the brilliant minds at AutoGPT have been working on. over the past months. I'm going to hand off the stage now to Salen, one of our founding AI engineers.</p><p>Thank you, Torrin. Thank you. There you go. Thank you, Torrin. I want to talk about something that I think not many of you realize. I didn't realize this for a long time. We're not achieving the peak of our potentials. We can all work faster, we can work better, and we can do more with less time and less stress.</p><p>Let's take the spreadsheet for example. I don't know about you, but I've stared at this interface for hours on end. And I'm sick of it. How would you go about filling out this spreadsheet? It's the lead-generating name of the company, you've got the links. What you probably do is go on Google, you search, copy/paste, maybe go on LinkedIn, copy/paste, back to Google, over and over and over again for hours, going back to the same interface, going back to the same websites.</p><p>But what if instead of all that, you can just chat? And you get the same end result, a filled out spreadsheet with all the leads. All right, let me give you another example. We all have unread messages, right? Not because we're lazy, allegedly, but because we're overwhelmed. Now, how would you go about cleaning out your inbox?</p><p>You'd sit there for hours and hours, sending the same variation of the same email, but what if you could just chat? Last example, I promise. Actually, these emails will now be leads in your inbox instead of just unread emails. Last example. Say you're a company or a developer. You spend millions of dollars developing apps that take weeks, months, sometimes even years, sitting there, copy/pasting anyways, because you're probably using ChatGPT.</p><p>I know I am, copy/pasting. But what if, instead of all that effort, you just chat? I think you get the point. There's a reason you've heard of AutoGPT. AutoGPT inspired the minds of millions. It gave hope to what a world could look like where we all reach our full potential.</p><p>The light at the end of the tunnel. You could see the sparks of digital artificial intelligence. And in this world, everyone goes from using their minds mostly to execute menial tasks with only 10% of their brains being used for creative work, to becoming creative masterminds, orchestrating the peak potential of their lives.</p><p>And in this world, we're all AI engineers, whether you know it or not. And people have noticed, AutoGPT was the fastest repository to 100,000 stars. Every major news network picked up on this. Everyone understands what the potential of this is. And it kicked off a whole new field of development.</p><p>A whole new paradigm of augmenting humans to give them time back and live a more stress-free life. And even the major players in the space all realized how big of a deal it is. And now work on these agents. And so, I want to hand off to the primary open source developer at AutoGPT to talk a little bit more about the open source repo.</p><p>Thank you, Selen. Thank you, Selen. And hello, San Francisco. I think all of us being here is a real testament to the power of open source. And on that note, we have some really exciting news to share. Because just last week, our open source repo, AutoGPT, hit 150,000 stars on GitHub.</p><p>Of course, metrics are fun. But to me, it is so much more than just a number. It is the 150,000 people who took an interest in what we're doing and decided to click that button. So, if you start our repo, then thank you. It is also the 460+ contributors who took their time and effort submitting thousands of pull requests and issues.</p><p>In the process, and to all of them as well, thank you so much. It is also the 47,000 members of our online community. And all of the interesting and insightful interactions that they have given us. It has been a wild ride at times, but it has allowed us to do and learn so much in the past six months.</p><p>And I am extremely excited for what is to come based on that. Now, I have already said it, but we could not have done this without our community. And community matters. So, we are committed to fostering, to growing, and to empower this community, and to build the future together.</p><p>And I will hand it back to Slam to tell you what that means. Thank you, Puts. And we haven't stayed stagnant since the open source agent originally came out. We have continued to work on it, and we have continued to improve its capabilities and implement the latest cutting edge research.</p><p>But we have also been working on some other things. To show our commitment to the agent space and the open source ecosystem, we built a forge, which is a template for any agent creator to have a better time to develop their agents with a standardized template. We also built a dev tool UI to easily interact with and iteratively improve your agent using an intuitive interface.</p><p>All of these tools are built on top of the agent protocol from the AI engineer foundation and other industry standards to maximize compatibility and interoperability. Anyone who implements this protocol can use our benchmark, front-end dev tool, and other offerings built on top of this protocol. And while this dev tool template is in beta or in alpha, it has served our participants of the current hackathon we're running, where we have $30,000 in cash on the line.</p><p>And we've learned a lot from this. We've received a lot of great feedback. We've received a lot of bug fixes and insights that we're going to take into the future. One of those insights is that code is king. We've realized that coding agents are the fundamental agents of the world.</p><p>Let me tell you, the digital fabric, the fundamental digital fabric is code. Our goal is to build a generalist agent, yes. But code is the stepping stone to AGI. A motivated coder can get anything done, except forget a bed frame. Another thing that we've learned over time is that without a compass, you don't know where you're going.</p><p>You know, at the start of the repo, we were getting thousands of pull requests, and that's a pull request every two hours. We had no way to know whether the pull requests were good, and how do we even test these pull requests. We didn't have a real direction. It took time to test these, and it was unnecessarily costly.</p><p>And so we created a compass. We created a benchmark to direct the development of the open source repo and quantitatively know if we were improving. It's an easy way to know if your agents are improving down different categories, and people are currently benefiting from this for the virtual hackathon.</p><p>And this is just cool. We've been running this in our CI pipeline for the past couple months on different open source agents within the ecosystem. And what the tests have shown is that we're on the brink of something special. These agents have showed continual improvement, and don't worry, I wouldn't put this in a research paper.</p><p>It's very noisy. It's very messy. But there is a continuous trend from 35 to 55%. This is just a graph of the success rate on the benchmark over time, over the month of August. Another thing that we're committed to is safety. As the ecosystem grows, and as the capabilities of agents increase, there's always questions of trust and reliability.</p><p>And these are problems that AutoGPT is committed to. One of these problems is prompt injection, which will always be there. OWASP, one of the big security organizations, has talked about this and said this is one of the big problems that not just language models face, but also agents. It's essentially when agents visit a website, what all agents need to do, and the website has something malicious.</p><p>And then the LLM is like, "Alright, I need to be doing that now." And you can see that there in this example. Then there's this other category that I like to call innocently malicious, where agents are just bad sometimes. It's the truth. And in this example behind me, this person asked an open source agent to delete all the JSON files within a directory, a specific one.</p><p>And the agent ended up deleting all the JSON files on a laptop. And this is going to continue to be a problem. If we want agents to do the things that humans can do, they will need root access. And so within AutoGPT, we're committed to and think about these problems extensively.</p><p>And we've been working on a research paper to solve some of these issues. And in order for agents to be commercially viable and trusted, these safety problems need to be solved. You can't have a 99% success rate. It has to be 100%. That one email that's sent could be a lost contract or a lost lead.</p><p>And so this is fundamental, not just to the development of the open source agent, but to all agents out there. After all, our end goal is a digital AGI to augment all of humanity. And I'm going to invite Craig to announce some exciting news regarding some developments with AutoGPT.</p><p>Hello. So, it's been a wild journey from zero to here in six months. And we keep stressing this because it's so important to us. We're only here because of our community. Because of that shared passion in pushing the frontier of what AI agents can do. So, we're really excited to announce that Redpoint Ventures has invested $12 million in turning this vision to a reality.</p><p>Now, this isn't just funding. This is them showing their deep belief in our mission and their dedication to open source. That's why we went with them because they are so dedicated to staying open source. And that's really important to every single one of us working on this project. Now, this is where we need you.</p><p>With this funding, we want to grow our team and add more passionate individuals. So, join us. Message us. Join our Discord community. And let's all help make the world's best open source generalist agent. Together, we can redefine the future of work. Thank you. Thank you. Thank you. Ladies and gentlemen, please welcome to the stage our next speakers.</p><p>Applied AI engineer at OpenAI, Simone Fishman. And member of Developer Relations at OpenAI, Logan Kilpatrick. Cool. How's it going? Good. Hey, everyone. A little bit about us, too. So, you can think of OpenAI as a product and research company. We build awesome models. And then we think about what are some of the best ways to apply them to solve the biggest problems that humanity faces.</p><p>And so, there's this deployment pipeline. Logan and I sit at the end of this deployment pipeline. We work with people in the real world that are using OpenAI as models. We spend our time thinking about what are some of the best ways to use our models? What are some of the hardest problems that haven't been solved yet?</p><p>And how can we apply OpenAI technology to solve these? I'm on the apply team, and I'm an engineer. Yeah. And my name's Logan Kilpatrick, and I do developer relations stuff. So, helping people build fun and exciting products and services using our API. So, yeah. Folks, all from the title of the talk, we'll talk about multimodal stuff.</p><p>But I think it's important to start off with where we are today. And I think, you know, as we all know, people who have been building in the AI space for the last 6, 12, 18 months, 2023 has really been the year of chatbots. And I think it's been incredible to see how much people have actually been able to do, how much value you can create in the world with, like, just a simple chatbot.</p><p>And it still blows my mind to think about how rudimentary these systems are and how much more value there's going to be created in the next year, in the next decade. And that's why I'm excited for 2024, which I think is really going to be the, I don't know if I can trademark this, but the year of multimodal models.</p><p>It's a tongue twister, but also hopefully the domain is available, yearofmultimodals.com. No, don't buy it if it's available. Yeah, so I'm excited. OpenAI has a ton of multimodal capabilities that are in the works. Some folks might have already tried some of these in ChatGPT and the iOS app or the web app today.</p><p>Things like vision, taking in images, describing them. We'll show that later on. Also, the ability to generate images. We've had this historically with DALI 2, but DALI 3, really, if folks have tried it, it takes things to the next level. So excited to show some of that today as well.</p><p>Cool. So if you think of the way that multimodal capabilities are working right now, it's a little bit of a setup of islands where we have DALI that takes text and generates images. We have Whisper that takes an audio and generates text transcripts. We have GPTV with vision capabilities.</p><p>GPTV with vision capabilities that takes images and text and can reason over both at the same time. But right now, these are all very disparate things. However, you can think of text as a connective tissue between all of these models. And there's a lot of interesting things that we can build right now using that paradigm.</p><p>But what we're actually really excited for is a future in which there's unity between all these modalities. And this is where we're going. This is not where we're today. But you can think of models in the same way that like GPT can consume images and text simultaneously. Maybe in the future we'll consume even more modalities and we'll output even more modalities and we'll be able to reason about them at the same time.</p><p>However, we're not there yet. So today, Logan and I are going to show you just like some architecture patterns and some ways in which you can mimic this kind of situation with what we have available today and some of the patterns that you can start to think about as we move towards this future in which models can reason way beyond text.</p><p>As Simone and I were making these demos today, waiting until the last minute as always, it was really interesting to see that like really much of the work of making multimodal systems today is like how do you hook everything up together and connect the different modalities. And again, as Simone said, using text as sort of the bridge between different modalities.</p><p>But it's going to be super interesting to see like how much developer efficiency gains there are when you no longer have to do that and you really just have like a single model that can do text in, text out, video at some point, you know, speech in, speech out at some point.</p><p>So it'll be super cool to see when that's possible and make making demos even easier and simpler. Alright, well, we'll show you guys two demos today. And we'll talk about like some high level ideas and some high level concepts. And hopefully at the end of it, you'll be inspired to think about like what are some of the things that that maybe you're not able to build today, but you'll be able to build six months a year from now.</p><p>And how you should start thinking about your products as they are able to incorporate more modalities. Cool. So on to demo number one. This is a, it's a, it's a very, very simple DALI vision loop. Um, yeah, so yeah, sorry, um, excited to, to look at this demo. So Simone will, will pull up the demo and I'll, I'll sort of just walk through it.</p><p>But the basic idea is let's take a real image. Um, let's use GBTV, um, or GBT4 with, with image inputs to essentially create a nice human, uh, readable, understandable description of that image. Um, and then we'll put that into DALI3 and actually go and generate a synthetic version of that image.</p><p>Um, so this whole pipeline takes a little bit to run because, uh, it's not a production, um, system at the moment. Uh, but the nice part is, uh, we've got a couple of examples ready and we can, you want to kick one off live as well. We can let it run in the background.</p><p>So very, very, this is a, a fun, simple idea, but, uh, the, this is a photo that I took in the lobby downstairs. Uh, just when you walk into the, the hotel, uh, there are these, uh, kind of like, uh, Halloween themed painted ladies. Uh, and so what we did here is that we asked, uh, GBTV4 with vision to describe this image in detail.</p><p>Uh, and then we asked it to, uh, generate a description for DALI to, uh, generate a new image based on this. Um, uh, you can see it, it does an okay job. Here's the description of the image. Here's the prompted uses. Uh, it, it picks up on a lot of details like the, the RIP in the tombstone and the old dogs.</p><p>Uh, welcome, uh, I think here. And then it generates a whole new image, but there's a lot of details that are off. You know, like the, the, um, the marble is black and the, uh, the spiders are white. And so what we do next is that we pass the, yeah, it's close enough.</p><p>It's close. Uh, but we give the two images to GPT with vision again. Um, and we ask it to compare them, uh, and see what are some of the differences. And, uh, it, it picks up on a lot of the, the different details. And then we ask you to create a new, a new image based on these, uh, differences.</p><p>And it goes ahead. New image. You see, uh, all the black marble is gone. The spider is now larger and black. Uh, but you know, it, it, it matches something closely. And I think this, this is just to illustrate, I think there's a long way to go, but this is to illustrate the idea that there are plenty of tasks that we do right now in AI.</p><p>Where we, we need the human in the loop to be able to evaluate a visual output that a model produces, compare it with something else. Then like iterate on the instructions, pass that again to another model. And so that, that, that's a pipeline where we like thought that humans were very essential and that we're probably going to continue to be essential for some time.</p><p>And now that's something that the models can do by themselves. Um, and there's a couple of, uh, of, uh, interesting, uh, uh, patterns here. I think, I think one of them is describing images. That's powerful because now you have an image. Now you have text and you can reason about that text.</p><p>You can do a lot of things with that text. But another really powerful element is, uh, comparing images. Um, a, and, and, and spotting differences, like having like a final destination that you want to get to and like a current destination. And a, a, and that pattern of comparing things, you can apply to a lot of things.</p><p>So imagine, uh, talking outward where Logan and I were just chatting about like some other like ways that you can apply this. And, and, and Logan's idea was imagine you are, uh, curating, uh, your room and you're, you just moved to a new place. Uh, and you're an Instagram, you find some images of like a vibe that you like and, and like maybe some object.</p><p>And, and then you can grab that image. You can give that to GPT-4 with vision and you can tell like, okay, now like, like crawl through Amazon and find like all the lamps that matter. All the lamps that match this vibe that I want for my room. Um, I want this so badly.</p><p>Yeah. And so I can't do interior design. So it's like, I, I would love to be able to just be like, get me all the stuff that matches this specific vibe. It's, it's a hard problem right now. Yeah. Um, and a couple. Simone, can I make one other quick comment, which is just, I think also, you know, folks were, were laughing, you know, in, in, in good jest when this, when this third image came up, came up.</p><p>I think it's important to know that there's, there's like no prompt engineering or anything like that. That's happening. This is like the, the rawest output that you can get. This is a, a one hour demo version. So people can, uh, will hopefully go wild with this once it's available through the API and like ideally get much better results than, than we're seeing today.</p><p>Um, yeah. Yeah. Yeah. Probably using a bunch of techniques that other people talked about at the conference so far. So this is the, the very basic version of, of this demo. Yeah. And, and we wanted to keep it simple and minimal just to illustrate the, the, the power of the models.</p><p>This is as raw as you can get when it comes to the models that like, there is almost all the completion output. It's going straight into the model and, and I think there's like 50 lines of code. So like the majority of the power lifting is being done by the models here.</p><p>Um, and another quick example that I'll show you guys and then I'll try to do one live, uh, which will probably, uh, be tragic, but, uh, um, so this is, this is the backstage right here. Uh, uh, I just took this photo right before walking on stage. Uh, uh, uh, you can see that, uh, GPT with vision does a really good job actually of describing that.</p><p>Uh, the, there's the monitors and there's boxes and there's cables and there's one not. Uh, uh, and then this is the image that DALI generates, DALI 3. Uh, so you can see blue carpet, cables, boxes, all the elements. And then it goes on to spot the differences and it notices, for example, that in this image, there are all these vertical lights.</p><p>that are not present in the first image. It says that here, uh, lighting like all this like vertical lights on the walls and ceilings, which adds, uh, but then it rewrites the prompt and it gets rid of all the vertical lights. And it gets, and it adds the, uh, the curtain in the back, which wasn't present here, but it's present in the, the black curtain here.</p><p>Um, so a little, just little interesting things. It's still a long way to go, but like this, this new, this whole new, this opens a whole new box of interaction patterns. The, the, the fact that now you can reason visually. Um, cool. And, and let's give a shot to, uh, a live example.</p><p>So this, uh, this was a, a trail run that I did over the weekend, uh, up in Purisma Woods. Um, and so I was gonna do it from scratch. Um, hope that it works. I wanna go to another. There you go. Uh, cool. So the image depicts a, uh, serene and pictures, woodland setting.</p><p>Uh, the focus of the image is a wooden boardwalk or a footbridge that winds through the dense forest. Um, very detailed description, light filters through the trees. Uh, and I'm just passing that raw, just straight to DALI. Yeah, and if, if folks have seen what happens in the, the DALI, uh, mode in the chat GPT iOS app, for example, it's actually doing a little bit.</p><p>I, I don't, uh, know off the top of my head, like what the prompt is for that, but it's, it's doing some amount of prompt engineering. Like if folks have actually tried to use like our labs product before to make DALI images, you have to do that prompt engineering yourself.</p><p>Um, and I think that's been one of the limitations. Like if people used mid journey or other, um, other image models in the past, like it's just kind of hard to make good prompts that work well for these systems. So it's nice that the, uh, the model can, can take a stab at doing it for you.</p><p>It's telling us a lot of how the, the second image is a lot more beautiful and more detailed, which checks out. It's, it's also interesting to see, uh, just for folks to, to think about. It's interesting to see that like, it's still of these, um, image models. Like the main limitation as we're seeing this demo in real time is actually.</p><p>No. Of course. Of course. Thanks for going back to the slides. Next, go back to the slides. I'm going to leave it running. And then at the time, if we, if we have time, it'll probably work the second round. It worked the three times before this. Um, cool. Cool.</p><p>Okay. For the second demo, um, uh, we're going to take it a little bit further and we're going to do something, uh, with video. Uh, and the idea here is that there's a lot of video summarization demos out there that we've seen. Uh, the majority of them just take a transcript and then, uh, ask GPT-4 to summarize this transcript.</p><p>However, videos have a lot of, uh, information in them that is conveyed visually. And so, uh, what we're doing here is that we're taking frames from the video. Um, and then we're asking GPT-4 with Vision to describe all the frames. And then we are asking Whisper to transcribe the video.</p><p>And now we have this long textual representation of the video that not only includes all the audio information, but also includes visual information from the video. And then we're doing some exciting, like, mixes on that, uh, that Logan will tell you about. Yeah. I'm ready for the next slide.</p><p>Um, yeah. So for, for this demo, we're literally just taking the GPT-4 introduction, uh, video. If folks have seen on YouTube, it's a good video if you haven't seen it before. Um, so taking the video raw from YouTube. Uh, taking the video raw from YouTube, again, like Simone said, cutting up those, uh, the different frames from the video, putting those into, to GPT-4 with image input, getting the summaries, which you can see.</p><p>And I know it's really hard. Um, but literally just like actually saying what's, these are simple images. So it's easy to capture the, the depth of what's shown here. Um, taking those images and then going to the next piece, which is essentially a big, another, another wonderful dolly image, but a big description of, uh, of the transcript and then all of the image, essentially like image embeddings is the, is the easiest way of thinking about it.</p><p>So if you want to actually see the results of this QR code, bottom right hand corner is real. Um, you can scan it and see the resulting article. It's, it's pretty, it's pretty good. Um, it does a good job. And I think for, for me, you know, why this is exciting is cause you can sort of capture the, again, capture the depth of, uh, of what happens in a video.</p><p>So a dolly image to start and then a bunch of actual frames that like match up with the contextual representation of what's being talked about in the blog post. Um, and again, there's no hand. I, I couldn't open source the code cause it has a bunch of unreleased APIs, but no, no sort of magic behind the scenes stuff that's happening.</p><p>This is like a raw crappy prompt, um, to generate this, uh, this blog post, which I think is, again, I think it's really cool and, um, takes videos and makes them more accessible in the, in the text form. So I like it. Cool. Let's see if this finished. Uh, oh well.</p><p>Um, cool. Oh yeah, sure. Cool. Okay. Uh, so some, some, uh, concluding takeaways. Um, a start thinking multimodal. Uh, that's, that's something that new that's, that's happening these days. And, and if you have any crazy ideas that you think, wow, it would be really cool if, if technology could do this, uh, we'll probably be able to get there.</p><p>And, and the products that you'll be able to build six months from now, a year from now are going to be incredible. So start having this in mind as, as, as, as people who are building AI products and people who are building companies. Um, think of text as a, as a connecting teacher right now.</p><p>And, and I think this is a very powerful concept. And that's gonna continue to be the case for the near future. Uh, a, and there are many powerful patterns that are yet to be explored when it comes to multimodal stuff, especially when it comes to, to, uh, doing things with images.</p><p>Uh, so really excited to, uh, soon get this in the hands of all of you guys and, and to see why you all build with this. I think it's, uh, it's really exciting, uh, to see, uh, AI start to venture into the visual world. Yeah, agents with image input is going to be sick.</p><p>I can't wait. I feel like so much of the internet is requires that. Yeah. And we're excited. I think there's, there's a lot of stuff that's going to happen in the, in the near future. And, um, I think it's cool to be able to hopefully get a glimpse of, of what some of those use cases look like.</p><p>So anything else you want to say, Simone? That's good. All right. This is wonderful. Thank you all. Thank you all. And now, please welcome the founder and CEO of Lindy, Flo Crivello. I'm going to talk about the future that awaits us and not the super distant future either. Like I'm talking about five, ten years, certainly within our lifetimes.</p><p>The future that awaits us once agents have fully realized their potential. If I had to describe it in one sentence, I'd say that it's a world where a 25-year-old can have the same or more business impact as the Coca-Cola company. It sounds insane when you say it this way, but there's actually a precedent to it.</p><p>It's happened before with media. Consider what Oprah had to do to build her media empire. Right? She had to go and pitch a bunch of, a bunch of CNN executives in some stuffy room and raise money, hire a crew, find cameraman. And obviously, the internet and apps like YouTube have brought that friction down to zero.</p><p>And it's brought that friction down so much that it actually sounds like a joke. Consider the top YouTuber of Mr. Beast. He's got a much greater reach than Oprah. He actually has more of a reach than the Super Bowl. And it's just him and his laptop is how he got started.</p><p>So it happened before. Even weirder, Ryan's world. He started when he was three years old, making videos on YouTube of him unboxing and reviewing toys. Today, he's 12 years old and he's worth $100 million. My point is that once you bring the friction down to zero, and once you remove the gatekeepers, you don't just get the same kind of content except cheaper.</p><p>The nature of the content changes when you remove the gatekeepers. Right? Look at Oprah. Look at the Super Bowl. That makes sense. Right? It's like a talk show. It's like a sports game. Ryan's world and Mr. Beast is just weird. And so my point is that we're about to see the exact same transformation happen to the world of business at large.</p><p>We're going to take the friction down to zero. And as a result, we are going to see much weirder, more creative ideas come to life. You know, Lindy is my second startup. Before it, I had another one called TeamFlow. And I remember when I started it, I had a perhaps naive understanding of what starting a business entailed.</p><p>I thought it was all about building a cool product and bringing it to market. And then I found out that's actually the fun part. Right? Before you get there, people -- I see the fonders laughing in the audience. Before you get there, you've got to meet with lawyers and incorporate and meet with bankers and open a bank account and meet with VCs and raise money and meet with recruiters and hire a team.</p><p>And it goes on and on. And I mean, you guys know, once you have a business, it's not much easier. It just keeps going. So when that wave of generative AI came about, all these amazing products that you're seeing that generate copywriting for you, generate images, I was like, that's awesome.</p><p>But it doesn't solve my problem of it's just too darn painful to start a business. And also, the GDP isn't made of copywriters or illustrators. It's made of work and actions. So that's when I got interested in agentic AI. AI that can actually automate the many old parts of your life.</p><p>There's this amazing movie, Office Space, by the same guy who made Silicon Valley. and Silicon Valley, highly recommend. And there's this awful, depressing character in there named Milton. Milton. He spends his life in some basement doing God knows what. They call it filing TPS reports. And in the end, Milton does the most productive thing of his career, which is that he burns the building to the ground.</p><p>That's what we're going to do, figuratively. The cops are coming. You know, I take this as a symbol that no one is happy with this status quo. People are always worried about, oh, robots are stealing people's jobs. I think it's people who have been stealing robots' jobs. Do you want to be Milton?</p><p>I think it's a huge problem when you look at the data. The average manager in the U.S. spends 15 hours every week on this kind of administrative task. That's $459 billion every year just in the U.S. That's more than the GDP of Norway. So that's where we start. We build an AI employee.</p><p>And the first thing it does very well is it acts as your personal assistant. We call it Lindy. The good news is as we've dug into that problem space, we found out there are three big time wasters. And you know the ones. No surprise there. This is where you spend your life at work and you hate it.</p><p>So calendar, your email, your meetings. So the product we built, those are actual screenshots of the product. You can ask it to schedule your meetings for you. This example here is actually pretty cool because it demonstrates another ability of Lindy, which is she continuously learns from her interactions with you.</p><p>So here I was like, help me find half an hour every week with Eric. And she called it Flo Eric because previously I had asked her, find 30 minutes with Eric tomorrow. And she did that, but she named the meeting, meeting with Flo. And all my meetings are meeting with Flo.</p><p>So it's not very helpful. And I was like, no, I gave her a little bit of feedback. Call it Flo Eric. And she did so. She renamed the meeting. And she saved the preference for future instances. And generally, I can give any arbitrary preference of any complexity that I want to Lindy.</p><p>And she'll remember them and honor them. I can CC Lindy to my emails so that she helps me schedule them. And when you use Lindy, she can pre-draft your replies for you in your inbox, in your voice, for each individual recipient. Because you don't talk the same way to your partner as you do to your investors, hopefully.</p><p>So every morning I wake up, I open my Gmail, and I just have all the drafts ready for me to review. Lindy prepares me for my meetings. So I've asked her, hey, five minutes before every meeting, send me the Zoom link, the LinkedIn's of the people I'm meeting with, and the summary of my last few emails with them.</p><p>She just does that. The really crazy thing is that we ourselves didn't actually build any of these features. What we did is we built a universal framework, allowing an AI to pursue any arbitrary goal using any arbitrary tool. And some very complex and sophisticated behaviors come out of that, as we'll see later.</p><p>Now, my pet peeve, every time people go on stage and they talk about their AI products, they always talk about the good part. It always works. It's very cherry-picked. And so I'm going to break that pattern a little bit today, and I'm going to talk about a time when it didn't work.</p><p>A few weeks ago, I asked Lindy to help me work on my vocabulary, and every morning to send me a new interesting world. And so she does that. Every morning I wake up, I have a new world in my inbox. That's great. I start to use them. Until one morning, I received this world.</p><p>Pure liquidity. A captivating term, denoting the act of meandering through a conversation with no fixed direction. And I paused for a minute on that one. I was like, pure liquidity. So I was like, I didn't have heard of this one before. I just Googled it. And sure enough, it doesn't exist.</p><p>So then I went back and re-Googled every wheel that she sent me, and none of them existed. So if I've used any wheel that doesn't exist today, that's why. She's been poisoning my brain. But when it works, it works great. And what it means for you when it works is that your computing experience of the future isn't one when you're in a basement filing TPS reports all day.</p><p>It's not one where you're working on your computer. It's one where you're having a conversation with your computer. You're in flow state. You just focus on what you uniquely do best. And all the menial, awful parts of your work that you hate arrange themselves automatically for you. I don't know about you guys.</p><p>I think this is already awesome. I cannot wait for this to fully come to fruition. But it doesn't yet get you to the stage that I was talking about where a 25-year-old has more business impact than the Coca-Cola company. In order to get there, you have to go one step further.</p><p>And instead of having just one Lindy work for you as your assistant, you can have an entire society of Lindys working together on your business to pursue your goals. If you want to realize how powerful that can be, consider the fact that every single item around you in the room right now was made by a group of people.</p><p>Even the simplest of items, not one person can do it. In fact, there's this guy who ran this project called the toaster project. He wanted to see, can a single human make a very simple item like a toaster? He spent six months on it. It cost him $2,000, probably more like 50K if you include the value of his time.</p><p>And this is what he ended up with in the end. Oh, he could have gone to Amazon and bought a perfectly fine toaster for $25. I think this contrast is a good illustration of the difference in abilities between one person, six months, 50K, pretty bad looking toaster. And a group of people, $25, perfectly fine toaster.</p><p>I think the same thing happens to LLMs. You go to GPT-4, you ask it to do something like, hey, build an entire iOS app for me. Soup to nuts. Design it, publish it to the App Store, do everything. It can't. And then people conclude, oh, GPT-4 can't do it.</p><p>Right? You've got to wait for GPT-5, GPT-6, GPT-7. I think it's the same thing as if you went to some guy and you asked him, make a rocket for me. And you can't. And then you're like, oh, humans can't make rockets. And obviously they can. You've just got to let them work together.</p><p>So that's exactly what we built is a framework for multiple agents to work together in pursuit of your goals. This is what it looks like. The most awesome example that I know of is that we have created a society of Lindy's for Lindy to build herself. We need to build a lot of integrations for Lindy to work well with Slack, Twilio, Google Sheets, and so on and so forth.</p><p>Instead, we are building this society of Lindy's. At the top level, there's this tool creation Lindy that takes an instruction like, hey, build a Slack integration. Talks to this Lindy that goes online and finds the open API spec and the online web documentation. Talks to this Lindy, that's a manager Lindy, just splits up the task across many engineers.</p><p>The engineers work on the task. There's a specific engineer for the authentication code because there's a few gotchas here. And then they pass the work to a QA engineer in Lindy that tests the work. And if it doesn't work, sends it back to the software engineer. If it works, submits a PR.</p><p>This, for the record, is 70% or 80% of the way there. But I think it points to the future. So this is how you get to that future where a 25-year-old in his San Francisco studio can have more of a business impact than the Coca-Cola company. I think this is going to be the greatest equalizer of human history.</p><p>Today, the best CMO in the world probably works for Apple or Nike or Coca-Cola. Not too long from now, the best CMO in the world is going to be an AI CMO. The same goes for the best designer in the world, the best engineer in the world. They're all going to be AI designers, AI engineers.</p><p>They're going to work for you. They're going to work for you. They're going to work for you. We're all going to have the same lever of infinite strength to make change happen in the world. And the only question is going to be, can you use that lever? That's the only skill that's going to matter in the future.</p><p>Imagine if you weren't constrained anymore by time, by money, by your team, by your network. Imagine if you could build anything and it was just you, your laptop, and your Lindy's. So, thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. I don't know. I don't actually know. Am I audible? And now I am. That was day one keynotes, everyone.</p><p>How do you feel? Yeah? Give it up for everyone who presented. Very, very excited for everything that's already happened and about to come. There's obviously so much going on in the AI field and AI engineering fields. But I feel like oftentimes, I've always wanted to do this at a conference and now I help to run one.</p><p>So let's do it. I always wanted to do one of these. Marvel's a little bit past this view, but I think the model is kind of working. And most of you are familiar with the first two that we have. And any Latent Space listeners in the house? Yeah, a little bit, a little bit.</p><p>Highly recommend. We have a few other things that are planned and we'll be talking about them over the next few days. I think what's interesting about the AI field is so much stuff is being built. Some stuff is much further along and some stuff is in infancy. And we always want to try to encourage people to join in.</p><p>Like it's not too late. That's a lot of my message. You're just in time. It's not too late. So you can still 1,000x from here. So with that in mind, I want to introduce the next set of speakers. And the Marvel analogy is kind of blown up by Ant, CTO of Superbase, who's in the audience somewhere.</p><p>Because he said we're going to make some iPhone-level announcement. And I really idolized the way that he presented it. Things in threes. So we're going to present three small things. They're just in our infancy. So let's talk about it. So first up, we have Barb talking about the state of AI engineering.</p><p>Give it up. Can people hear me? Okay. Hi, everyone. Oh, I see some friends in the audience and a lot of new faces. So I'm Barb for those who I haven't met. I'm an investor at Amplify Partners, where we invest in very technical founders and platforms. And I focus a lot on data and AI.</p><p>And so we're going to talk about the state of AI engineering. So where are we right now? We're at the AI engineer summit. And even from today, you see how quickly the field is moving. I definitely don't need to tell this audience that. We're going to talk about that.</p><p>Everything from new state of the art models to changing and very rapidly changing tooling. And so we had a conversation and thought it would be helpful to take a step backwards and say, what is actually happening? How do we get a good sense of the state of AI engineering?</p><p>How do we get a good sense of what tools people are using? And how these things change over time? Especially given the really, really, really rapid advancements. So we put out a survey. It's still open. And you all are going to get the very first alpha view at the results of the survey.</p><p>841 people have filled it out about how they're using AI at work. As far as I'm aware, that's the largest survey of AI engineers that is out there. And this is the very first one. But after the inaugural one, we can track these over time. So the survey covers a lot.</p><p>And I don't have time to cover all the things. But we go over everything from demographics to use cases to what are people actually using in their stack, to some questions that people care about that are fun rapid fire, to who should we really celebrate in the community that is doing a really good job bringing people together and educating them through newsletters and podcasts.</p><p>On the demographics front, before I was an investor, I was a data scientist for most of my career. Then I worked a little bit in data infrastructure. Are there any other data scientists in the house by show of hands? Okay, I see some in the corner. How about software engineers as your formal job title?</p><p>Okay. How about AI engineers as your formal job title? Okay. Yeah. So software engineers actually beat out AI engineers. These were the top five roles. But we're at an AI engineering conference. We think that we're going to see a lot more of the title AI engineering. But it's both a job function and a skill set that we see across a bunch of different functions.</p><p>So, SWIX has talked about AI becoming more ubiquitous. Of the folks that we talked to who have over 10 years of software experience, 20% of them have less than one year of AI/ML experience, but they're getting into the field now. If I remember correctly, something like 40% have less than three years of AI/ML experience.</p><p>So, we're seeing this flood into combination of AI skill set and the AI engineering role. There are a lot of use cases we could talk about. But just to give some highlights, most people are using LLMs for more than one use case, for internal and external customer-facing tools. I think if we did this survey a few months ago, there would not have been as many folks who are using external products.</p><p>So, congrats to everyone that's doing the hard work to make that happen. Accuracy and cost are the most important when choosing a model. And serving cost and evaluation are what people said are the most challenging. There's a whole section on evaluation, and reading through the comments were pretty funny, because while folks had the opportunity to vote on human review and academic benchmarks, there were some write-ins like, "I evaluate based on vibes," or "based on my eyeballs." And so, it's just a commentary on how far we have to go there.</p><p>And finally, OpenAI's models are the most popular. Good job the OpenAIers in the crowd. But 80% are experimenting with more than one provider, and I'm including open source in that. We're not going to go through all of this, but we will share a survey that goes through all of this.</p><p>I'm just going to share some fun ones. I don't know what happened there, but as the cliffhanger, most AI engineers are using a vector database. And what I'm saying here that you can't really see is that there's a near-even split between folks using third-party and self-hosted for VectorDBs. So, it will be interesting to see where this goes over time.</p><p>For prompt management, I thought this comment was hilarious. Is prompt management a thing? I prototype using OpenAI Playground and then hard-code prompts into source code. Apparently, I missed something. And so, we're seeing a combination of folks using external tools, building internal tools -- that's the most popular thing -- and using internal spreadsheets.</p><p>But I think this is also an interesting question around kind of who in the stack owns this, and does it become more important or less important over time as the models get better? But as we're doing more AI and potentially have more prompts. And finally, we have a whole section of pretty fun questions around is the future open-source or third-party?</p><p>It was actually pretty even with open-source narrowly winning. And there aren't too many doomers in the AI engineering crowd. I think 12% of folks confidently said there's a 0% chance of -- 0% probability of doom. But you see that there's an interesting distribution there. Awesome. And I just want to shout out -- these were the top 10 of each category of newsletters, podcasts, and communities.</p><p>And the way that folks voted on this was if they felt like they've learned something from one of these in the past months. So, major shout out to this -- yeah, it's getting a lot of photos -- major shout out to the folks who are putting in a lot of work to educate and help build learning and space for AI engineering.</p><p>All right. If you want to take a look at all of this more in-depth, got a QR code for you and a link. You're the first people to see some early results of the AI engineering survey. I'm always open to feedback, to discussion, to questions you want to see next.</p><p>Some things -- like there were more people pre-training models than we expected -- are going to dictate what we continue to look at and what we continue to survey and share out. But I hope you get value out of the transparency and don't be a stranger. Awesome. Speak it to Sasha.</p><p>Thanks so much, Mark. Yeah. So, that's the first small launch that we're going to do, which is the definitive industry survey. The next thing I think about when building an industry -- That's the wrong mic. -- is open source communities. So, with that, Sasha Sheng, everyone. Hi, everyone. So, today I'm very excited to stand here in front of you to announce that we are starting a new organization called the AI Engineer Foundation.</p><p>Before -- Before I start, I just want to note that I have been an engineer for the last year. I've been an engineer for the last 10 years. So, standing in front of this audience is deeply uncomfortable for me. So, I'll try my best to explain what it is.</p><p>So, for AI Engineer Foundation, we exist to solve problems for AI engineers. Very on brand. And today, we're going to enumerate three problems to start with. First, every project is reinventing slightly different interfaces. Interfaces to popular LLM -- I apologize. Interface to popular LLMs have been implemented differently by different libraries.</p><p>And this is a problem for AI engineers because we're going to have to learn each interfaces independently. Secondly, a tooling gap. Development and monitoring tools lack interoperability. This is what I mean. Without standards, people build end-to-end apps that integrate with limited set of tools. This creates an ecosystem which is basically a bunch of verticalized silos that encourages churn.</p><p>more stable infrastructure, as well as a more modularized framework, as well as more collaboration. The problem is that, you know, we're not going to be able to do that. We're not going to be able to do that. But we're not going to be able to do that. We're not going to be able to do that.</p><p>However, on the other hand, with mutually agreed-upon standards, we find common points of shared interest so that AI engineers can find the best-in-class tools with familiar interfaces. And this creates an ecosystem as well that -- This creates an ecosystem that essentially encourages more stable infrastructure, as well as a more modularized framework, as well as more collaboration.</p><p>Problem three, venture-backed open-source lock-in, as well as -- We're preventing venture-backed open-source lock-in, as well as having to navigate relicensing challenges. So, some of you guys might know, in August, we have this announcement by HashiCorp. After nine years of Terraform being open-source, they were suddenly getting relicensed to be a non-open-source-compliant project.</p><p>And this created a lot of panic in the industry. And luckily, we have the Linux Foundation, who acted very quickly to create open-tofu to ensure that Terraform stays open-source. So, who we are as AI Engineer Foundation is -- everything we do is open-source. And we are a non-profit neutral body.</p><p>And we are building a strong AI engineer community. Our first project is called Agent Protocol. It's a simple API spec. And as of yesterday, it's starred about 300 on GitHub. And it's a unified interface for AI agent developers. It's currently a simple REST API. And by REST API, I really mean these nine endpoints, as well as a well-defined schema for data types.</p><p>And you can check out Agent Protocol at agentprotocol.ai. With Agent Protocol, new tools are suddenly available. So, the AutoGPT team recently launched the Arena Hacks Hackathon that is built on top of Agent Protocol. So, now we have evaluation benchmarks that, if you're an agent developer, you should totally still participate in this hackathon.</p><p>It's still ongoing to submit your agent to this leaderboard to see how they perform. There are three ways for you to stay engaged with us. If you have an open-source project and then you would like for other AI engineers to benefit from, you can submit your project to us.</p><p>And if you are a developer, you can stay engaged with us through our Discord community. And lastly, if you really resonate with the problems that we're trying to solve, and if you are financially able to do so, please sponsor us. More information can be found on the website, AIE.foundation.</p><p>May the source be open. Thank you. I don't know which we're using now, but I'm just going to stick to this for now. So, yeah, that's the foundation, everyone. Very, very early. I do think that we have to set these things in motion so that there's a place for you if you want to come join and collaborate in open-source community.</p><p>A lot of people ask me about my stuff. What am I doing? I'm just, you know, moving people around or, you know, suggesting projects and promoting people. I am working on a thing. I am not, like, super ready to, like, talk about it, but, you know, like, there's no other place to talk about it.</p><p>So it's small AI. Tim, if you want to roll the clip. And, basically, that's a cute logo. Thanks to Candy Code for that. And, essentially, it is what I've been pitching with an API gateway that actually helps AI engineers to code faster, make their code base simpler, and do a lot of things that, like, would otherwise take a lot of specialist knowledge, right?</p><p>I think a lot of AI engineering is about access from machine learning to product. If you have been on the -- and so that's the website. It's small AI. It just went live today. If you have been on the conference website yesterday, thanks to Sean Oliver for coding this up, you actually have seen the summit AI bot, which actually presents the information about the conference in a better way that's more personal to me than the website because sometimes I just want to see details about the speakers, the talks, whatever.</p><p>And the reason, like, why don't we do this more often, right? Like, the OpenAI docs don't have a chat bot. The line chain docs, they kind of launch the chat bot. It's on a different domain if you have to find it somewhere. It's not ubiquitous because it's a lot of work and a lot of code to write.</p><p>What we have found is that we've been able to fine tune data into models on production traffic. And that's effectively the kind of stuff I've been working on. That's a screenshot of the fine-tuning UI that OpenAI has launched. And this is an example of the kind of stuff that a fine-tuned smaller model can do that would eliminate a whole raft of sort of glue code that most people would be writing.</p><p>So that's what I'm working on. That's small AI. But I don't want to make it about me. This conference is definitely not, like, you know, like the small conference. Each of these things are new projects in their infancy that we're presenting alongside of the keynotes. And the image that I want to give is that this is a very new field.</p><p>There is room for you still. And please join us. Please promote your projects. This is a permissionless space as well. You don't need our permission. I never asked permission from OpenAI to get started on this crazy journey. And neither do you. So go forth and build. Thanks, everyone. Thanks, everyone.</p><p>Please welcome back to the stage the co-founders and hosts of the AIE Summit, SWIX and Benjamin Dunphy. I still don't have a mic, though. There we go. All right. Almost perfect. All right. How are we feeling? I am so excited by so many things. But particularly, I'm especially excited about the announcement of the AI Engineer Foundation.</p><p>You know, when SWIX and I were first talking about making this conference back in February, I initially proposed potentially we could do a foundation because, you know, this AI engineering phenomenon is going to change engineering. It's going to change it, we think, for the better. But a lot of people are going to struggle with it.</p><p>And we want to help out as much as possible. This is inevitable. We're going to make this happen. And we're going to help make it a much smoother transition. So I'm extremely excited about that. If you like the mission that was just announced, you can support it by getting a special edition tea.</p><p>I don't know if we can zoom in on that on the camera or not. This is the special edition tea. It is now set up just outside behind you. You can scan a QR code. No, no, no. This is what everyone gets. That's what everyone gets. Everyone's got one of those.</p><p>Those are cool. This is cooler. But there are only so many. And there's not that many. So here's what you need to do. There's a QR code out there. You scan it. And that takes you to a Stripe checkout link. You donate 50 bucks. And you get a t-shirt.</p><p>Simple, I know. But there's a lot of you in this. I think there's only like 100 t-shirts or something. So play nice. But yeah. So we're going to a break now. About 40 minutes. And then when that break is done, we'll come back for our second block of opening keynote presentations.</p><p>And then after that, we're going to have the topic tables. We did move the food and beverage. I know it says on the schedule, food and beverage. Happening about now. We move that to 7:00. I think it makes more sense since we just had lunch at like 1:00. So food will be coming.</p><p>Just not until about 7:00, 7:30 after the next block of sessions. What else did I want to say? This is just a preview of things to come. I mean, we had so many amazing announcements on the stage. So many amazing speakers. And we have a lot more packed in for you.</p><p>So we're here for all the speakers so far. Do you have anything else? Swix, did you want to say a few words before we break? No, let's eat. I mean, I'm sure these people are hungry and thirsty and want to chat. Well, 40 minutes from now. Yeah. Or a few hours from now.</p><p>Yeah. Yeah. All right. I haven't had lunch. Sorry. There's coffee. I don't have the mic. Thank you. Woo! Thank you. Woo! Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. So I could find strength to divide us. Gave it all we got and I know we did the best we could.</p><p>If I could go back under the mess, I would memorize your face before I go. But this is how we grow. Gotta give it up sometimes this goal. And knowing when to kill your pride. There's no one to blame. Nothing really stays the same. This is how we grow.</p><p>Sometimes we hold on to let go. Sometimes we hold on to let go of trust. Additionally, users, now that we all have access to ChatGBT and can really easily access these models, we have very high expectations when we're using AI features inside of products. We expect outputs to be crisp, exactly what we wanted.</p><p>We should expect to never see hallucinations. And in general, it should be fast and accurate. And so I want to go over three easy to implement tactics to get better and safer responses. And like I said, these can be used in your everyday when you're just using ChatGBT. Or if you're integrating AI onto your product, these will help go a long way to making sure that your outputs are better and that users are happier.</p><p>The first are called multi-persona prompting. This comes out of a research study from the University of Illinois. Essentially, what this method does is it calls on various agents to work on a specific task when you prompt it. And those agents are designed for that specific task. So, for example, if I was to prompt a model to help me write a book, multi-persona prompting would lead the model to get a publicist, an author, maybe the intended target audience of my book.</p><p>And they would work hand-in-hand in kind of a brainstorm mechanism with the AI leading this brainstorm. They'd go back and forth, throwing ideas off the wall, collaborating until they came to a final answer. And this prompting method is really cool. It's because you get to see the whole collaboration process.</p><p>And so it's very helpful in cases where you have a complex task at hand or it requires additional logic. I personally like using it for generative tasks. Next up is the according to method. What this does is it grounds prompts to a specific source. So, instead of just asking, you know, what part of the digestive tube do you expect starch to be digested, you can say that and then just add to the end according to Wikipedia.</p><p>So, adding according to specified source will increase the chance that the model goes to that specific source to retrieve the information. And this can help reduce hallucinations by up to 20%. So, this is really good if you have a fine-tuned model or a general model that you know that you're reaching to a very consistent data source for.</p><p>So, this is really good if you don't know how to do it. So, this is really good. This is really good. This is really good for your answers. This is out of Johns Hopkins University. It was published very recently. And last up, and arguably my favorite, is called Emotion Prom.</p><p>This was done by Microsoft and a few other universities. And what it basically looked at was how LLMs would react to emotional stimuli at the end of prompts. So, for example, if your boss tells you that this project is really important for your career or for a big client, you're probably going to take it much more seriously.</p><p>And this prompting method tries to tie into that cognitive behavior of humans. And it's really simple. All you have to do is add one of these emotional stimuli to the end of your normal prompt. And I'm sure you'll actually get better outputs. I've seen it done time and time again from everything from cover letters to generating change logs.</p><p>The outputs just seem to get better and more accurate. And the experiments show that this can lead to anywhere from an 8% increase to 115% increase, depending on the task at hand. And so, those are three really quick, easy-hit methods that you can use in ChatGPT or in the AI features in your product.</p><p>We have all these available as templates in PromptHub. You can just go there and copy them. It's PromptHub.us. You can use them there, run them through our playground, share them with your team, or you can have them via the links. And so, thanks for taking the time to watch this.</p><p>I hope that you've walked away with a couple of new methods that you can try out in your everyday. If you have any questions, feel free to reach out and be happy to chat about this stuff. Thanks. Hi, everyone. I'm going to go to the next video. I'm going to go to the next video.</p><p>I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video.</p><p>I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video.</p><p>I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video.</p><p>I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video.</p><p>I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video.</p><p>I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video.</p><p>I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. I'm going to go to the next video. Storyteller application. Storyteller is a client-server application.</p><p>The client is written using React and the server is a custom Fastify implementation. The main challenges were responsiveness, meaning getting results to the user as quickly as possible, quality and consistency. So when you start Storyteller, it's just a small screen that has a record topic button. And once you start pressing it, it starts recording.</p><p>The audio when you release gets sent to the server as a buffer. And there we transcribe it. For transcription, I'm using OpenAI Whisper. It is really quick for a short topic, 1.5 seconds. And once it becomes available, an event goes back to the client. So the client-server communication works through an event stream, server-send events that are being sent back.</p><p>The event arrives on the client, and the React state updates updating the screen. Okay, so then the user knows something is going on. In parallel, I start generating the Story Outline. For this, I use GPT-3 Turbo Instruct, which I found to be very fast. So it can generate a Story Outline in about 4 seconds.</p><p>And once we have that, we can start a bunch of other tasks in parallel. Generating the title, generating the image, and generating and narrating the audio story all happen in parallel. I'll go through those one by one now. First, the title is generated. For this, OpenAI GPT-3 Turbo Instruct is used again, giving a really quick result.</p><p>Once the title is available, it's being sent to the client again as an event and rendered there. In parallel, the image generation runs. First, there needs to be a prompt to actually generate the image. And here, consistency is important. So we pass in the whole story into a GPT-4 prompt, that then extracts relevant representative keywords for an image prompt from the story.</p><p>That image prompt is passed into Stability AI Stable Diffusion Excel, where an image is generated. The generated image is stored as a virtual file in the server. And then, an event is sent to the client with a path to that file. The client can then, through a regular URL request, just retrieve the image as part of an image tag.</p><p>And it shows up in the UI. Generating the full audio story is the most time-consuming piece of the puzzle. Here, we have a complex prompt that takes in the story and creates a structure with dialogue and speakers and extends the story. We use GPT-4 here with a low temperature to retain the story.</p><p>And the problem is it takes one and a half minutes, which is unacceptably long for an interactive client. So how can this be solved? The key idea is streaming the structure. That's a little bit more difficult than just streaming characters token by token. We need to always partially parse the structure and then determine if there is a new passage that we can actually narrate and synthesize speech for.</p><p>ModelFusion takes care of the partial parsing and returns an iterable over fragments of partially parsed results. But the application needs to decide what to do with them. Here, we determine which story part is finished so we can actually narrate it. So we narrate each story part as it's getting finished.</p><p>For each story part, we need to determine which voice we use to narrate it. The narrator has a predefined voice and for all the speakers where we already have voices, we can immediately proceed. However, when there's a new speaker, we need to figure out which voice to give it.</p><p>The first step for this is to generate a voice description for the speaker. Here's a GPT-3-5 Turbo prompt that gives us a structured result with gender and a voice description. And we then use that for retrieval where we beforehand embedded all the voices based on their descriptions and now can just retrieve them filtered by gender.</p><p>Here, for the speech synthesis, Element and 11labs are supported. Based on the voices that have been chosen, one of those providers is picked and the audio is synthesized. Similar to the images, we generate an audio file and we store it virtually in the server and then send the path to the client, which reconstructs the URL and just retrieves it as a media element.</p><p>Once the first audio is completed, the client can then start playing. And while this is ongoing, in the background, you're listening and in the background, the server continues to generate more and more parts. And that's it. So let's recap how the main challenge of responsiveness is addressed here. We have a loading state that has multiple parts that are updated as more results become available.</p><p>We use streaming and parallel processing in the backend to make results available as quickly as possible and you can start listening while the processing is still going on. And finally, models are being chosen such that the processing time for the generation, say, the story is minimized. Cool. I hope you enjoyed my talk.</p><p>Thank you for listening. And if you want to find out more, you can find Storyteller and also Model Fusion on GitHub at github.com/lgrammel/storyteller and github.com/lgrammel/model/fusion. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening.</p><p>Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening.</p><p>Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you for listening. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Hi, everyone. I'm Jeff. I'm Jeff. I want to share with you an interesting generative AI project that I recently did.</p><p>Not too long ago, I made a game with 100% AI-generated content. It's a simple game where you're wandering around lost in the forest and you go from scene to scene, having encounters that impact your vigor and your courage. The idea is that you want to find your home before you run out of courage.</p><p>There's 16 scenes in a 4x4 grid. And so, if you play a few times, you will have seen them all. And so, if you play a few times, you will have seen them all. Now, my favorite part of making this game was generating each scene and just seeing what AI would come up with.</p><p>And I thought, wouldn't it be cool to share that experience with the player. What if every time they went to a new scene, it was generated fresh for them. And every game would be unique and different this way. It would be a game of infinite exploration. That sounded so cool that I wanted to try to do it.</p><p>Now, the first thing that I would need to do is to generate each scene and have a consistent way of doing that. My scene definitions are JSON objects that describe what the scene is when you first find it, as well as when you come back to it later, and how that impacts your stats.</p><p>So, I started out by using OpenAI's completion endpoint and doing some prompt engineering. So, this is the prompt that I used. This is a very detailed prompt. It's rather long, but it worked really well. Most of the time, I would get scenes that had the right JSON format and the content was good.</p><p>It was fitting. It was varied. It was interesting. So, I was happy with this. But I wanted to make it even more reliable. And I decided to fine-tune a model. I used OpenAI's fine-tuning endpoint. And they recommend 50 to 100 examples. I generated 50 examples, just like these, and used them to fine-tune.</p><p>Now, the key is, I shortened the prompt. I simplified it. I took out any of the JSON and just generally described what I wanted, hoping that that information would be embedded in the training data. And I tried this out. I wasn't sure if it would work. And I tried it.</p><p>It only cost about a dollar or two. That includes generating all the examples and doing the fine-tuning. And when I tried it, I was very happy to find that it worked perfectly. Even though I didn't mention the JSON at all, it came out perfect because of what was in the examples.</p><p>And that meant I had less tokens in the prompt, which is faster and cheaper and just easier to work with. So I was really pleased with how this worked. The next step was to make the images. Now, I used a tool called Leonardo. Leonardo not only lets you generate images, they also let you create your own image models.</p><p>And this is great for a game because it means that you can have stylistically consistent images, which is exactly what I needed. So I spent a while using all the different parameters that Leonardo offers and working with the prompt to try and find an image that looks right and that I liked.</p><p>It turned out that using the description directly from the scene as the prompt made nice pictures, which I was surprised about since it had like second person and said things other than what was in there. But it worked out great. Now, the tricky part with fine-tuning an image model is that you need consistent images that have like the parts that should be the same are the same in all of your training data.</p><p>But the parts that you want to vary need to be varied. Otherwise, it will overfit and all of your images will look the same. But if you don't have that consistency between them, then it won't really know what you want and you won't get that good stylistic consistency. This was really tricky, especially in my case, I needed the perspective and the scale to be consistent from scene to scene.</p><p>Obviously, I needed them all to be set in the forest, and I wanted to have this overall tone and texture that looked the same. Some of my scenes have people in them, some have animals, some have buildings, some have nothing, and so it was hard to get that variety.</p><p>I ended up having to train a couple of models with different parameters, different sets of images, but I eventually found one that worked out. And to test it out, I generated a lot of images. I mean, a whole bunch. And you can see they all have similar features like the zigzag path down the middle.</p><p>Obviously, the trees and the look and everything looks the same. And yet, there's plenty of variety. Each one is unique and different, but still feels cohesive, which I am very pleased about. So now I had everything I needed to put it together and make the game. I made a simple asset server that had an AI pipeline starting by requesting a new scene from OpenAI's endpoint using my custom model.</p><p>Once I get that, I validate the JSON to make sure that it's got all the keys it needs. If it's good, I take the description and I send that to Leonardo. Leonardo makes an image from my custom model, gives it back to me. I put it all together and send it off.</p><p>Now, did this work? Well, let me show you. Here is an example scene that was created, and I'm very happy with it. I made a simple preview server so that I could scroll through a bunch of these scenes that I generated to make sure they worked. And it looked good.</p><p>So I made some changes to the game to request images each time the player went to a new scene. Now, there was a problem here. It takes 10, 20, sometimes 30 seconds to do this. And that wouldn't be good for the play experience. So what I did is I added some caching.</p><p>I pre-fill a bunch of these scenes. And then as scenes are taken out of it, I fill it back up again once it gets below a certain threshold. And that way, there's always a scene that's ready to go. With that, the game was ready. And I'm going to share it with you right now.</p><p>Now, keep in mind, everything that we see has never been seen before and will never be seen again. So this is the game. You always start out at this lamppost and you have to wander around and find your way home. Your stats are in the bottom left corner. As your vigor goes down, your speed goes down as well.</p><p>And as the courage goes down, the viewport will get smaller and smaller. Let's look around and explore. We're going to move down. And here's the first generated scene. This looks really cool. This is like you encounter a soft blue pulsating light coming from the organic formation scattered around the glade.</p><p>Your fear and tiredness lift and you feel rejuvenated and the vigor goes up, but I'm already at full. So that's really cool. Let's head off in this direction now. I won't read all of these, but this looks like a cool campfire scene, which is really neat. And I'm going to head down.</p><p>And what have we got here? There's a large dark cave over here at the end of the path somewhere. And it's daunting. So my courage is going down. Let's head this way instead. And now we've gotten into some fog, foggy trees. And hard to see. Let's go back. This is like a really windy road that we're going through.</p><p>Let's head down. Oh, I'm back where I started. Well, this is the game and it would continue on and on and on until you find your way home. And then you can just play again and it would be different every time. That's great. I just have a few closing thoughts.</p><p>One thing is that these images are low resolution. They're 512 pixels. And I could make them a higher resolution by adding an AI upscaler to my pipeline. It would add more time. It would add more time. So it's a trade off. Also, I could get more creative with adding something to the prompt to make a scene.</p><p>For example, I could let the user select a theme or maybe even get the time of day or the current weather at the location of where the user is set. And then the scenes could be generated to match where they are for a very immersive experience. And of course, I can use this same process on other projects.</p><p>That's all. I hope that you found this interesting and enjoyed watching it as much as I enjoyed putting it all together. Thank you so much. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Hello. And welcome to my talk on how we're thinking about the levels of code AI.</p><p>My name is Otto Kukic and I am the director of DevRel at Sourcegraph. At Sourcegraph, we're building Kodi, the only AI coding assistant that knows your entire codebase. To help educate our customers and users, as well as shape our thinking of code AI, we've been using a concept that we call levels of code AI internally.</p><p>These levels have really resonated with our community, so we wanted to publicize them and start a conversation with a broader developer community and we're better to do it than at the AI engineer summit. When we talk about code AI, we refer to software that builds software. Today, 92% of developers are using code AI tools, whereas this number was just 1% a year ago.</p><p>Our founder and CEO, Quinn Slack, has shared a bold prediction that in five years, 99% of code will be written by AI. While we await that future, let's talk about how we see the levels of code AI today. We see six distinct levels across three different categories. Human initiated, where humans are the primary coders.</p><p>AI initiated, where AI starts to take a proactive role in software development. AI led code, where AI has full autonomy over a code base. We'll contrast these levels of code with the SAE levels of autonomy for vehicles. Let's dive in. At level zero, the developer writes all code manually without any AI assistance.</p><p>The developer is responsible for writing, testing, and debugging a code base. AI does not generate or modify any part of the code base, but IDE features like symbol name, completion can provide a bit of assistance. This level reflects the traditional software development process before introducing any AI assistance into the development workflow.</p><p>A vehicle operating at level zero is fully reliant on the human driver for acceleration, steering, braking, and everything in between. At level one, the developer begins to use AI that can generate single lines or whole blocks of code based on developer intent. For example, a developer might write the signature of a function, and the AI will infer the context and generate the implementation details for said function.</p><p>At level one, the AI assistant has been trained on millions of lines of open source code and can leverage this to provide superior completions based on the developer's guidance. SAE level one vehicles still require the full attention of the human driver, but offer features such as cruise control or lane centering that make driving an easier, safer, and more comfortable experience.</p><p>At level two, the AI coding assistant has superior understanding and context of the code base it is interacting with. Where at level one, the context is broad and general, a level two AI coding assistant has specific context about the code base that it is working in. This allows the AI assistant to make better suggestions for code completions.</p><p>For example, if you were working in a Node.js code base and were using the Axiom library to handle HTTP requests, a level two AI assistant would provide autocomplete suggestions based on the Axiom library as opposed to a different node HTTP library like fetch or super agent. At SAE level two, we get partial automation.</p><p>The human driver is still in control and can override anything the car does at any time, but features such as traffic aware cruise control or automatic lane changes can make driving a much smoother experience. At level three, the developer provides high level requirements and the AI assistant delivers a code based solution.</p><p>The AI coding assistant goes beyond generating singular snippets of code to building out full components and even integrations with other pieces of software. Rather than writing the code themselves, a developer could instruct a level three code AI assistant to add a user authentication to an application that they are building and the coding assistant would generate all of the code required.</p><p>The coding assistant could then explain to the developer the code it wrote, how it works, and how it integrates with the rest of the application. SAE level three is also the first level where the vehicle itself takes on the primary role of driving, with the human driver being a fallback in case the vehicle cannot drive itself safely.</p><p>The vehicle can perform most of the driving tasks, but may encounter situations where it cannot adequately perform these tasks, so it's forced to give control back to the human driver. At level four, the code AI assistant can proactively handle coding tasks without developer oversight. Let's imagine a few scenarios where a level four code AI assistant would play a role.</p><p>A level four capable code AI assistant could continuously monitor your code changes and autonomously submit PRs to ensure your documentation stays up to date. Even better, the coding assistant could monitor bug reports from customers and submit PRs to fix the issues. The human developer could then simply review the pull requests and merge them.</p><p>Level four SAE vehicles can perform virtually all driving tasks under specific conditions. For example, Waymo operates a fleet of fully automated self-driving taxis in cities where they have high-quality mapping data and can provide a safe driving experience for passengers without human drivers. A customer simply hails a Waymo taxi using a mobile app, provides a destination, and the vehicle is responsible for taking the passenger to their final destination without any additional human input.</p><p>At level five, the AI assistant requires minimal human guidance on code generation and is capable of handling the entire software development lifecycle. The developer provides high-level requirements and specifications. The AI then designs the architecture, writes production quality code, handles deployment, and continuously improves the code base. The developer's role is to validate that the end product meets the stated requirements, but the developer does not necessarily look at the generated code.</p><p>The code AI assistant has complete autonomy to take code from concept to production. A self-driving car capable of level five driving automation can perform all driving tasks under all conditions, humans optional. The car is responsible for making all the decisions. At this level, a steering wheel or any ability for a human to override the car is unnecessary.</p><p>So there you have it, the six levels of code AI, or at least how we're thinking about them at Sourcegraph. Do you agree? Do you agree? Disagree? We'd love to hear your thoughts. Find us at booth G5 and let's chat. And if you'd like to try Kodi for yourself, get it for your IDE of choice at Kodi.dev.</p><p>Thank you and I'll see you on the show floor. Hey, I'm Matija and I'll show you how we created a GPT-powered full-stack web app generator and how it was used to create over 10,000 applications in one month. So first, we'll see what it is and then secondly, we'll check out how it works under the hood.</p><p>So let's get started. So everything happens on this web page and it's super simple. First, we have to enter the name of our application. Let's say we are building a simple to-do app. Second part is describe how it works in a couple of sentences. So we have a simple to-do app with one page listing all the tasks.</p><p>User can create tasks, change them, toggle them, edit them. Creativity level corresponds to GPT temperature. So we can go on the safe side and get less features or we can go a little bit crazy but also have more mistakes. So I will stick with this balanced one. And the last thing left to do is just to hit this generate button.</p><p>Bam. Here we can see the result of the generation. So we got a full-stack app in React, Node.js, Prisma and it's all glued together with a full-stack framework Wasp. So the secret of Wasp is that it relies on this single configuration file which describes your app in a high-level declarative manner.</p><p>So here, for example, we can see our auth in just a couple of lines, our routes, pages, our data model. So everything is here. And still here we can see our client code. For example, here's our React. And here we have our Node.js functions which are being executed on the backend.</p><p>So the last thing to do is just to download this app locally and run it with Wasp. So let's do that. So I downloaded the app locally. And now we just have to run it via Wasp Start. And there we have it. We can log in with username and password.</p><p>Now let's create a couple of tasks. Go to Mage. Become an AI engineer. Mark it as done. And now let's check it out in the database. So we have a database inspector that also comes with Wasp. And here we can see two rows. One for each task. And both are completed.</p><p>Is done is true. So let's try adding another task. Have fun. And let's check it out in the database again. We see it's not completed. Is done is false. But if we complete it. We can see now it's true. So there it is. We got a fully working full stack web app.</p><p>In React, Node.js and Wasp. You can also now deploy this app with a single CLI command. It's a regular React and Node.js app. So you can deploy it virtually anywhere. But we have a CLI helper in Wasp that makes it super easy to deploy to fly.io. And more providers are coming soon.</p><p>This is one of my favorite features. When we got Mage out, it was hardly the first AI coding agent. But it was among the first ones that could generate a full stack web app with almost no errors. When we released this and people started using it, we were getting two main questions.</p><p>How come this works so well? And secondly, how can you offer it for free? Is it that cheap? So let's answer them one by one. There are three main reasons for Mage's performance. First, it is specialized only for full stack web apps and nothing else. Only React, Node.js and Wasp.</p><p>That allows us to assume a lot upfront and makes everything easier and faster. Second, it makes use of a high level web framework, Wasp. That takes away a ton of boilerplate and makes it much easier for GPT to do its job. And lastly, Mage fixes the errors before it gives you the final result.</p><p>Again, because of the two points I mentioned previously, this is also a simpler problem than for the general AI coding agents. Let's dive a bit deeper. Let's go back to our generated app. Since Mage knows we are building a full stack web app and it's using Wasp for it, we can produce a lot of code upfront without even touching the OpenAI's API and asking GPT any questions.</p><p>For example, some of the config files, then also some of the authentication logic, which we can see right here, and global CSS and similar. So we call this step zero. Only then the code agent takes over. The code agent's work consists of three main phases: planning, generating the code, and fixing the errors.</p><p>So let's expand the generation log and explore each of the cases. Here, following the step zero, we can see the planning phase. Given our app description, Mage device needs to generate the following queries and actions: entities for data models and one page. After that, the generation step happens. Mage is actually implementing everything it planned for above.</p><p>And finally, here comes the error fixing phase. Mage can detect some of the common errors and fix it for itself. Here it failed to fix, so it had to try again. And finally, when it cannot detect any more errors, we are done. We can also see that all this took about 27,000 tokens.</p><p>The cool thing is that, while developing Mage, we identified the most common errors it consistently kept making. Like mixing up the default and named imports. Some of them we even ended up fixing with a simple heuristic, without involving GPT. That took care of 90% of all errors. Again, VOS framework with its high level configuration was of great help here.</p><p>Since it removed the tone of code and reduced the space for errors significantly. Now, let's take a look at another question we had. How much does it all cost? A typical app we created with Mage took about 2-3 minutes and 25-60,000 tokens. Which comes to about 10-20 cents. But, there is one trick we used.</p><p>We used GPT-3.5 and GPT-4 interchangeably for different stages. And that reduced the bill a lot. If we used only GPT-4 for everything, the cost would have been 10x more. So, 1-2 dollars per app. What we did is we used GPT-4 only for the planning stage. Which is the most complex step and one that requires the most creativity.</p><p>For the actual implementation, we could comfortably use GPT-3.5, which is both faster and cheaper. And that worked great. Again, the key here is that we provided a highly guided environment for the coding agent. Given VOSP's web-web abstractions. And that's why this approach worked. This is also the main difference between Mage and the other coding agents.</p><p>We tried another popular agent that uses the more free approach and relies more on the GPT itself. And the cost to make a similar app as we did with Mage was between 80 cents and 10 dollars. So, what should you use Mage for? And what should you expect? Is it going to magically produce any app you imagine or do you still have to put some work in?</p><p>At current stage, Mage serves as a really good and highly customized crowdstarter for full-stack web apps. At that level, it can operate with almost no or very little errors that you can easily detect and fix. Most of the people that tried it found it as a super easy way to get their app kickstarted with the mainstream pieces of stack such as React, Node, and Tailwind.</p><p>And that's how Mage got its popularity. I personally believe this is what the future of SaaS starters looks like. Tailored to your app instead of starting out with a generic boilerplate. As you would expect, the more you push it, the more errors it starts making. On the other hand, not giving enough information and just saying something like "make Facebook but yellow" can also be counter-effective.</p><p>So, what comes next? We created Mage as an experiment to see how well it can produce full-stack web apps with FOSP. And it works surprisingly well. The current main limitation of Mage comes from its simplicity. And the fact there is no interaction with the user beyond the initial prompt.</p><p>So, that's something we are looking to add next. A live debugging mode. Where you can, while still on the web page, interact with the agent and request changes and error fixes. Another thing that would be interesting to explore would be using an LLM that is fine-tuned for WASP and web development.</p><p>Although, that would also make it more expensive. Also, since WASP has such simple and humor-readable syntax, it's hard to predict how much benefit would fine-tuning bring. Still, it would be a cool thing to try out. And that's it! We saw what Mage was, how it works, and what is the secret sauce that made it both fast and affordable to create web apps.</p><p>So, thanks so much for watching. I had a lot of fun making this video with my helper. And I hope you also found it interesting. Please give Mage a try and let us know how it went. We are the same team that created WASP, which is a fully open-source web framework that makes it super easy to develop with React and Node.js.</p><p>Also, check out our repo and join our Discord for any questions and comments. Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you!</p><p>Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you!</p><p>Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you!</p><p>Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you!</p><p>Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Thank you! Bye! Bye! Bye! Bye! Bye!</p><p>Bye! Bye! Bye! Ladies and gentlemen, we're starting now, please take your seats! Bye! Bye! Bye! Bye! Bye! Bye! Bye! Bye! Bye! Bye! Bye! chain, Harrison Chase. Thank you guys for having me and thank you guys for being here. This is maybe one of the most famous screens of 2023 and yet I believe and I think we all believe and that's why we're all here that this is just the beginning of a lot of amazing things that we're all going to create.</p><p>Because as good as chat GPT is and as good as the language models that underlie them are, by themselves they're just the start. By themselves they don't know about current events, they cannot run the code that you write and they don't remember their previous interactions with you. In order to get to a future where we have truly personalized and actually helpful AI assistants, we're going to need to take these language models and use them as one part of a larger system and that's what I think a lot of us in here are trying to do.</p><p>These systems will be able to produce seemingly amazing and magical experiences, they'll understand the appropriate context and they'll be able to reason about it and respond appropriately. At Langchain we're trying to help teams close that gap between these magical experiences and the work that's actually required to get there and we believe that behind all of these seemingly magical product moments, there is an extraordinary feat of engineering and that's why it's awesome to be here at the AI engineering summit.</p><p>I'm going to talk a little bit about some of the approaches that we see work for developers when they're building these context aware reasoning applications that are going to power the future. First I'm going to talk about context. When I say context, I mean bringing relevant context to the language model so it can reason about what to do.</p><p>Bringing that context is really, really important because if you don't provide that context, no matter how good the language model is, it won't be able to figure out what to do. The first type of context and probably the most common type of context that we see people bringing to the language model, we see them bringing through this instruction prompting type of approach where they basically tell the language model how to respond to specific scenarios or specific inputs.</p><p>This is pretty straightforward and I think the way to think about it is if you have a new employee who shows up on the first day of work, you give them an employee handbook and it tells them how they should behave in certain scenarios. Equivalent that to kind of like this instruction prompting technique.</p><p>It's pretty straightforward, I think that's why people start with it, and as the models get better and better, this zero shot type of prompting is going to be able to carry a lot of the relevant context for how you expect the language model to behave. There are some cases where telling the language model is actually quite hard and it becomes better to give it some few-shot examples.</p><p>It becomes better to give it examples where you show the language model how to behave rather than just tell it how to behave. So I think a few concrete places where this works is where it's actually a little bit difficult to describe how exactly the language model should respond.</p><p>So tone, I think, is a good use case for this, and then also structured output is a good use case for this. You can give examples of the structured output format, you can give examples of the output tone a little bit more easily than you could describe in language my particular tone.</p><p>The structured output is a little bit, but I think as it starts to get more and more complicated, giving these really specific examples can help. The next type of context is maybe the most, you know, it pops to the mind most when you hear of context and when you hear about bringing context to the language model.</p><p>Contrasting this with the first two, retrieval augmented generation context uses context not to decide how to respond, but to decide what to base its response in. So, the kind of like canonical thing is you have a user question, you do some retrieval strategy, you get back some context, you pass that to the language model, and you say answer this question based on the context that's provided to you.</p><p>So this is a little bit different from the instructions. It's maybe the same as asking someone to take a test with like an open book test. You can look at the book, you can look at the answers, and in this case the answers are the text that you pass in to this context.</p><p>And then the fourth way that we see people providing context to language models is through fine-tuning, so updating the actual weights of the language model. This is still kind of like in its infancy, and I think we're starting to figure out how best to do this and what scenarios this is good to do in.</p><p>One of the things that we've seen is that this is good for the same use cases where few-shot examples are kind of good. It takes it to another extreme. And so for tone and structured data parsing, these are two use cases where we've seen it pretty beneficial to start doing some fine-tuning.</p><p>And the idea here is that, yeah, it can be helpful to have three examples of how your model should respond and what the tone there should be, but what if you could give it 10,000 examples and it updates its way accordingly? And so I think for those where the output is in a specific format, and again, you need more examples, you need to show it a lot more than you can tell it, this is where we see fine-tuning starting to become helpful, and I think we'll see that grow more and more over time.</p><p>So we've talked about context, and now I want to talk a little bit about the reasoning bit, and I think this is the most exciting and the most new bit of it as well. And so we've tried to think and categorize some of the approaches that we've seen to allow these applications to do this reasoning component.</p><p>And so we've listed a few of them out here and tried to discern a few different axes along which they kind of vary. So if we think about kind of like just plain old code, this is kind of like the way things were, you know, like a year ago, so a long, long time ago.</p><p>And so in code you kind of like -- it's all there, it's declared if it says what to run, it says what the outputs are, what steps to take, things like that. We start adding in a language model call, and so this is like the simplest form of these reasoning applications, and here you're using the language model to determine what the output should be, but that's it.</p><p>You're not using it to take actions yet, nothing fancy, you're just using it to determine what the output should be, and it's just a single language model call. So you're providing the context, and then you're returning the output to the user. If we take it up a little bit, then we start to get into a chain of language model calls, or a chain of language model call to API back to language model.</p><p>And so this can be -- this is again used to decide the steps of the output. And here there's multiple calls that are happening, and this can be used to break down more complex tasks into individual components. It can be used to insert knowledge dynamically in the middle of kind of like one language model call, then you go fetch some knowledge based on that language model call, and then you do another one.</p><p>But importantly here, the steps are known. You do this, and then you do this, and then you do this. And so it's a chain of events, and that starts to change a little bit when you use a router. And so in here, you're now using the language model call to start determining which steps to take.</p><p>So that's the big difference here. It's no longer just determining the output of the system, but it's determining which steps to take. And so you can use it to determine which prompts to use. So route between a prompt that's really good at math problems versus a prompt that's really good at writing English essays.</p><p>You can use it to route between language models. So one model might be better than another. You might want to use Claude because of its long context window. Or you might want to use GPT-4 because it's really good at reasoning. And so having the language model look at the question and decide whether it needs to reason or whether it wants to respond in a long-form fashion, you can determine which branches to go down.</p><p>Or I think another common use case is using it to determine which of several tools to take. So do I want to call this tool or do I want to call this tool? And what should the input to that tools be? And so we have this router here, and I think before going on to the next step, the main thing here that distinguishes it from that step is there's no kind of like cycles.</p><p>You don't kind of get these loops. You're just choosing kind of like which branch to go down. Once you start adding in these loops, this is where we see a lot more complex applications. These are things that we often see being called agents, kind of like out in the wild, and it's essentially kind of like a wow loop.</p><p>And then in that loop, you're doing a series of steps. And the language model is determining which steps to do. And then at some point, there's a point where it can choose whether to end the loop or not. And if it ends the loop, then you finish and return to the user.</p><p>Otherwise, you go back and continue the loop. And so here you get the language model deciding what the outputs are. It decides what steps to take. And you do have these cycles. The last thing, and I think this is largely what we would describe as kind of like what AutoGPT did that took the world by storm, is this idea of an agent where you kind of like remove a lot of the kind of like guardrails around what steps to take.</p><p>So here, the sequences of steps that are available are almost like determined by the LLM. And what I mean by this is that here is where you can start doing things like adding in tools that the language model can take. So if you guys are familiar with the Voyager paper, it starts adding in tools and building up a skill set of tools over time.</p><p>And so some of the actions that the language model can take are dynamically created. And then I think the other big thing here is that you remove some of the scaffolding from the state machines. So some of the -- if I go back a little bit -- so a lot of these kind of like cycles that we see in the wild break things down into discrete states.</p><p>The most common one that we see are kind of like plan, execute, and validate. So you ask the language model to plan what to do with it, then it goes to it, and then you validate it often with a language model call or something like that. And I think the big difference between that and then the autonomous agent style thing is that here, you are implicitly asking the agent to do all of those things in one go.</p><p>It should know when it should plan, it should know when it should validate, and it should know when it should kind of like determine what action to take. And you are asking it all to do that implicitly. You don't have these kind of like distinct sequences of steps laid out in the code.</p><p>And so this is a little bit about how we're thinking about it. I think the thing to -- the thing that I like to say when saying this as well, which goes back to the beginning, is that the main thing that we think is it's still extremely early on in the space.</p><p>We still think it's the beginning. And this could, you know, in three months be kind of irrelevant as the space progresses. So I would just keep that in mind. If we think about kind of like some of the magical experiences like this where it can reason over the relevant context, what is it going to take to kind of like build it under the hood?</p><p>What is the engineering that's going to go in to all these seemingly magical experiences? And so this is an example of what could be going under the hood of something like this. It's going to be a challenging experience to build these complex systems, and that's why we're building some of the tooling like this, what you see here, to help debug, understand, and iterate on these systems of the future.</p><p>And so what exactly are the challenges associated with building these complex context-aware reasoning applications? The first is kind of just the orchestration layer. So figuring out which of the different reasoning kind of like cognitive architectures you should be using. Should you be using a simple chain? Should you be using a router, a more complex agent?</p><p>And I think the thing to remember here is that it's not necessarily that one is better than the other or superior to the other. They all have kind of like their pros and cons and strengths and weaknesses. So chains are really good because you have more control over the sequence of steps that are taken.</p><p>Agents are better because they can more dynamically react to unexpected inputs and handle edge cases. And so being able to choose the right cognitive architecture that you want and being able to quickly experiment with a bunch of other ones are part of what inspired the initial release of LangChain and kind of how we aim to help people prototype these types of applications.</p><p>And then LangSmith, which is this thing here, provides a lot of visibility into what actually is going on. As these applications start to get more and more complex, understanding what exact sequences of tools are being used, what exact sequences of language model calls are being made becomes increasingly important.</p><p>Another big thing that we see people struggling with and spending a lot of time on is good old-fashioned data engineering. A lot of this comes down to providing the right context to language models, and the right context is often data. So you need to have ways to load that data, you need to have ways to transform that data, to transport that data, and then you often want to have observability into what exact data is getting passed around and where.</p><p>And so LangChain itself has a lot of open source kind of like modules for loading that data and transforming that data. And then LangSmith, we often see being really useful for debugging what exactly does that data look like by the time it's getting to the language model. Have you extracted the right documents from your vector store?</p><p>Have you transformed them and formatted in the right way where it's clear to the language model what's actually in them? These are all things that you're going to want to be able to debug so there's no little small errors or small issues that pop up. And then the third thing that we see a lot of people spending time on when building these applications is good old-fashioned prompt engineering.</p><p>So the main new thing here is language models. And the main way of interacting with language models is through prompts. And so being able to understand what exactly does the fully formatted prompt look like by the time it's going into the language model is really important. How are you combining the system instructions with maybe the few shot examples, any retrieved context, the chat history that you've got going on, any previous steps that the agent took, what does this all look by the time it gets to the language model?</p><p>And what does this look like in the middle of this complex application? It's easy enough to kind of like test and debug this if it's the first call, the first part of the system. But after it's already done three of these steps, if you want to kind of like debug what that prompt looks like, what that fully formatted prompt looks like, being able to do that it becomes increasingly difficult as the systems kind of scale up in their entangledness.</p><p>And so we've tried to make it really easy to hop into any kind of like particular language model call at any point in time, open it up in a playground like this so you can edit it directly and experiment with that prompt engineering and go kind of like change some of the instructions and see how it responds or swap out model providers so that you can see if another model provider does better.</p><p>Another big challenge with these language model applications and is probably worth a talk on its own is evaluation of them. And so I think evaluation is really hard for a few reasons. I think the two primary ones are a lack of data and a lack of good metrics. So comparing to traditional kind of like data science and machine learning with those, you generally started with a data set.</p><p>You needed that to build your model, and so then when it came time to evaluate it, you at least had those data points that you could look at and evaluate on. And I think that's a little bit different with a lot of these LLM applications because these models are fantastic zero-shot kind of like learners.</p><p>That's kind of like the whole exciting bit of them. And so you can get to a working MVP without building up kind of like any data set at all. And that's awesome, but that does make it a little bit of a challenge when it comes to evaluating them because you don't have these data points.</p><p>And so one of the things that we often encourage a lot of people to do and try to help them do as well is build up these data sets and iterate on those. And those can come from either labeling data points by hand or looking at production traffic and pulling things in or auto-generating things with LLMs.</p><p>The second big challenge in evaluation is lack of metrics. I think most traditional kind of like quantitative metrics don't perform super well for large unstructured outputs. A lot of what we see people doing is still doing a kind of like vibe check to kind of like see how the model is performing.</p><p>And as unsatisfying as that is, I still think that's probably the best way to gain kind of like intuition as to what's going on. And so a lot of what we try to do is make it really easy to observe the outputs and the inputs of the language models so that you can build up that intuition.</p><p>In terms of more quantitative and systematic metrics, we're very bullish on LLM-assisted evaluation, so using LLMs to evaluate the outputs. And then I think maybe the biggest thing that we see people doing in production is just keeping track of feedback, whether it be direct or indirect feedback. So do they leave kind of like a thumbs up or a thumbs down on your application?</p><p>That's an example of direct feedback where you're gathering that. An example of indirect feedback might be if they click on a link or that that might be a good thing that you provided a good suggestion. Or if they respond really confused to your chatbot, that might be a good indication that your chatbot actually did not perform well.</p><p>And so tracking these over time and doing A/B testing with that using kind of like traditional A/B testing software can be pretty impactful for gathering a sense online of how your model is doing. And then the last interesting thing that we're spending a lot of time thinking about is collaboration.</p><p>So as these systems get bigger and bigger, they're doubtless going to be a collaboration among a lot of people. And so who exactly is working on these systems? Is it all AI engineers? As we're here today, is it a combination of AI engineers and data engineers and data scientists and product managers?</p><p>And I think one of the interesting trends that we're seeing is it's still a little bit unclear what the best skill sets for this new AI engineer type role is. And there could very well be a bunch of different skill sets that are valuable. So going back to kind of like the two things that we see making up a lot of these applications, the context awareness and the reasoning bit.</p><p>The context awareness is bringing the right context to these applications. You often need kind of like a data engineering team to get in there and assist with that. The reasoning bit is often done through prompting, and oftentimes that's best done by non-technical people who can really outline the exact specification of the app that they're building, whether they be product managers or subject matter experts.</p><p>And so how do you enable collaboration between these two different types of folks? And what exactly does that look like? I don't think that's something that anyone kind of knows or definitely hasn't solved, but I think that's a really interesting trend that we're thinking a lot about going forward.</p><p>And so I think the main thing that I want to leave you all with is that the big thing that we believe is that it's still really, really early on in this journey. It's just the beginning. As crazy as things have been over the past year, they're hopefully going to get even crazier.</p><p>You saw an incredible demo of GPT-4V. Things like that are going to change it. And so we think behind all of these things, it's going to take a lot of engineering. And we're trying to build some of the tooling to help enable that. And I think you guys are all on the right track towards becoming those types of engineers by being at a conference like this.</p><p>So thank you, SWIX, for having me. Thank you guys for being here. Have a good rest of your day. Please welcome our next speaker, the founder of 567, Jason Liu. Hey guys. So I didn't know I was going to be one of the keynote speakers. So this is probably going to be the most reduced scope talk of today.</p><p>I'm talking about type hints. And in particular, I'm talking about how Pydantic might be all you need to build with language models. In particular, I want to talk about structured prompting, which is the idea that we can use objects to define what we want back out, rather than kind of praying to the LLM gods that the comma is in the right place and the bracket was closed.</p><p>So everyone here basically kind of knows or at least agrees that large language models are kind of eating software. But what this really means in production is 90% of the applications you build are just ones where you're asking the language model to output JSON or some structured output that you're parsing with a regular expression.</p><p>And that experience is pretty terrible. And the reason this is the case is because we really want language models to be backwards compatible with the existing software that we have. You know, code gen works. But a lot of the systems we have today are systems that we can't change.</p><p>And so, yeah, the idea is that although language models were introduced to us through ChatGPT, most of us are actually building systems and not chatbots. We want to process input data, integrate with existing systems via APIs or schemas that we might not have control over. And so the goal for today is effectively introduce OpenAI function calling, introduce PyDantic, then introduce Instructor and Marvin as a library to make using PyDantic to prompt language models much easier.</p><p>And what this gets us is, you know, better validation, makes your code a little bit cleaner, and then afterwards I'll talk over some design patterns that I've uncovered and some of the applications that we have. This is basically almost everyone's experience here, right? Like, you know, Riley Goodside had a tweet about asking to get JSON out of BARD, and the only way you could do it was to threaten to take a human life.</p><p>And that's not code I really want to commit into my repos. And then when you do ask for JSON, you know, maybe it works today, but maybe tomorrow, instead of getting JSON, you're going to get, like, okay, here you go, here's some JSON. And then again, you kind of pray that the JSON's parsed correctly.</p><p>And I don't know if you noticed, but here, user is a key for one query, and username is a key for another, and you would not really notice this unless you had, like, good logging in place. But really, this should not happen to begin with, right? Like, you shouldn't have to, like, read the logs to figure out that the passwords didn't match when you're signing up for an account.</p><p>And so what this means is our prompts and our schemas and our outputs are all strings. We're kind of writing code and text edit, rather than an IDE where you could, you know, get linting or type checking or syntax highlighting. And so OpenAI function calls somewhat fix this, right?</p><p>We get to define JSON schema of the output that we want, and OpenAI will do a better job in placing the JSON somewhere that you can reliably parse out. So instead of going from string to string to string, you get string to dict to string, and then you still have to call JSON loads.</p><p>And again, you're kind of praying that everything is in there. And a lot of this is kind of praying through the LLM gods. On top of that, like, if this code was committed to any repo I was managing, like, I would be pissed, right? Complex data structures are already difficult to define, and now you're working with the dictionary of JSON loads, and that also feels very unsafe because you get missing keys, missing values, and you get hallucinations, and maybe the keys are spelled wrong, you're missing an underscore, and you get all these issues.</p><p>And then you end up writing code like this. And this works for, like, name and age and email, and then you're checking if something is a bool by parsing a string, and it gets really messy. And what Python has done to solve this is use Pydantic. Pydantic is a library that do data model validation very similar to data classes.</p><p>It is powered by type hints. It has really great model and field validation. It has 70 million downloads a month, which means it's a library that everyone can trust and use and know that it's going to be maintained for a long period of time. And more importantly, it outputs JSON schema, which is how you communicate with open AI function calling.</p><p>And so the general idea is that we can define an object like delivery, say that the timestamp is a date time, and the dimensions is a tuple of events. And even if you pass in a string as a timestamp and a list of strings as tuples, everything is parsed out correctly.</p><p>This is all the code. We don't want it right. This is why there's 70 million downloads. More interestingly, timestamp and dimensions are now things that your IDE is aware of. They know the type of that. You get autocomplete and spell checking. Again, just more bug-free code. And so this really brings me to the idea of structured prompting, because now your prompt isn't a triple quoted string.</p><p>Your prompt is actual code that you can look at. You can review. And everyone has written a function that returns a data structure. Everyone knows how to manage code like this. Instead of doing the migration of JSON schemas in the one-shot examples, you know, I've done database migrations. I know how some of these things work.</p><p>And more importantly, we can program this way. And so that's why I built a library called Instructor a while ago. And the idea here is just to make open AI function calling super useful. So the idea is you import Instructor. You patch the completion API. Debatable if this is the best idea.</p><p>But ultimately, you define your pydantic object. You set that as the response model of that create call. And now you're guaranteed that that response model is the type of the entity that you extract. So again, you get a nice autocomplete. You get type safety. Really great. I would also want to mention that this only works for open AI function calling.</p><p>If you want to use a more comprehensive framework to do some of this pydantic work, I think Marvin is a really great library to try out. They give you access to more language models and more capabilities above this response. But the general idea here isn't that this is going to make your JSON come out better.</p><p>The idea is that when you define objects, you can find nested references. You can define methods of the behavior of that object. You can return instances of that object instead of dictionaries. And you're going to write cleaner code and code that's going to be easier to maintain as they're passed through different systems.</p><p>So here you have, for example, a base model, but you can add a method if you want to. You can define the same class but with an address key. You can then define new classes like best friend and friends, which is a list of user details. If I was to write this in JSON schema to make a post request, it would be very unmanageable.</p><p>But this makes it a lot easier. On top of that, when you have doc strings, the doc strings are now a part of that JSON schema that is sent to open AI. And this is because the model now represents both the prompt, the data, and the behavior all in one.</p><p>You want good doc strings, you want good field descriptors, and it's all part of the JSON schema that you send. And now your code quality, your prompt quality, your data quality are all in sync. There's this one thing you want to manage and one thing you want to review.</p><p>And what that really means is that you need to have good variable names, good descriptions, and good documentation. And this is something we should have anyways. You can also do some really cool things with Pydantic without language models. For example, you can define a validator. Here I define a function that takes in a value, I check that there is a string in that value, and if it's not, I return a lowercase version of that, because that might be how I want to parse my data.</p><p>And when you construct this object, you get an error back out. We're not going to fix it, but we get a validation error, something that we can catch reliably and understand. But then if you introduce language models, you can just import the LLM validator. And now you can have something that says, like, don't say mean things.</p><p>And then when you construct an object that has something that says that the meaning of life is the evil and steal things, you're going to get a validation error and an error message. And this error message, the statement is objectable, is actually coming out of a language model API call.</p><p>It's using Instructor under the hood to define that. But it's not enough to actually just point out these errors. You also want to fix that. And so the easy way of doing that in Instructor is to just add max retries. Now what we do is we'll append the message that you had before, but then we can also capture all the validations in one shot, send it back to the language model, and try again.</p><p>But the idea here that this isn't prompt change, this isn't constitutional AI, here we just have validation, error handling, and then re-asking. And these are just separate systems in code that we can manage. If you want something to be less than 10 characters, there's a character count validator. If you want to make sure that a name is in a database, you can just add a post request if you want to.</p><p>But this is just classical code again. This is the backwards compatibility of language models. But we can also do a lot more, right? Structured prompts, get you structured outputs. But ideally, the structure actually helps you structure your thoughts. So here's another example. It's really important for us to give language models the ability to have an escape hatch and say that it doesn't know something or can't find something.</p><p>And right now, most people will say something like, return I don't know in all caps, check if I don't know all caps in string. Right? Sometimes it doesn't say that. It's very difficult to manage. But here, you see that I've defined user details with an optional role. That could be none.</p><p>But the entity I want to extract is just maybe a user. It has a result that's maybe a user. And then an error and an error message. And so I can write code that looks like this. I get this object back out. It's a little bit more complicated. But now I can kind of program with language models in a way that feels more like programming and less like chaining, for example.</p><p>Right? We can also define reusable components. Here I've defined a work time and a leisure time as both a time range. And the time range has a start time and an end time. If I find that this is not being parsed correctly, what I could do is actually add chain of thought directly in the time range component.</p><p>And now I have modularity in some of these features. And you can imagine having a system where, in production, you disable that chain of thought field. And then in testing, you add that to figure out what's the latency or performance trade-offs. You could also extract arbitrary values. Here I define a property called key and value.</p><p>And then I want to extract a list of properties. You might want to add a prompt that says make sure the keys are consistent over those properties. We can also add validators to make sure that's the case. And then re-ask when that's not the case. If I want, you know, only five properties, I could add an index to the property key and just say, well, now count them out.</p><p>And when you count to five, stop. And you're going to get much more reliable outputs. Some of the things that I find really interesting with this kind of method is prompting data structures. Here I have user details. Age name as before. But now I define an ID and a friends array, which is a list of IDs.</p><p>And if you prompt that well enough, you can basically extract, like a network out of your data. So, you know, we've seen that structured prompting kind of gives you really useful components that you can reuse and make modular. And the idea again here is that we want to model both the prompt, the data, and the behavior.</p><p>Here I haven't mentioned too many methods that you could act on this object. But the idea is almost like, you know, when we go from C to C++, the thing we get is object-oriented programming, and that makes a lot of things easier. And we've learned our lessons with object-oriented programming.</p><p>And so if we do the right track, I think we're going to get a lot more productive development out of these language models. And the second thing is that these language models now can output data structures. You can pull up your old lead code textbooks or whatever and actually figure out how to traverse these graphs, for example, process this data in a useful way.</p><p>And so now they can represent knowledge, workflows, and even plans that you can just dispatch to a classical computer system. You can create the data that you want to send to Airflow rather than doing this for loop, hoping it terminates. And so now I think about six minutes, so I'll go over some advanced applications.</p><p>These are actually fairly simple. I have some more documentation if you want to see that later on. But let's go over some of these examples. So the first one is RAG. I think when we first started out, a lot of these systems end up being systems where we embed the user query, make a vector database search, return the results, and then hope that those are good enough.</p><p>But in practice, you might have multiple backends to search from. Maybe you want to rewrite the user query. Maybe you want to decompose that user query. If you want to ask something like what was something that was recent, you need to have time filters. And so you could define that as a data structure.</p><p>Right? The search type is email or video. Search has a title, a query, a before date, and a type. And then you can just implement the execute method that says, you know, if type is video, do this. If email, do that. Really simple. And then what you want to extract back out is multiple searches.</p><p>Like, give me a list of search queries. And then you can write some like async iota, map across these things. And now, because all that prompting is embedded in the data structure, your prompt that you send to open AI is very simple. Your helpful assistant, segment the search queries.</p><p>And then what you get back out is this ability to just have an object that you can program with in a way that you've managed sort of like all your life. Right? Something very straightforward. But you can also do something more interesting. You can then plan. Right? Before we talked about like extracting a social network, but you can actually just produce the entire DAG.</p><p>Here, I had the same graph structure. All right? It's an ID, a question, and a list of dependencies, where I have a lot of information in the description here. And that's basically the prompt. And what I want back out is a query plan. So now, if you send it to a query planner that says, like, you're a helpful query planner, like, build out this query, you can ask something like, what is the difference in populations of Canada and Texas home country?</p><p>And then what you can see is, you know what, like, if I'm good at elite code, I could query the first two in parallel because there are no dependencies, and then wait for dependency three to merge, and then wait for four to merge those two. But this requires one language model call, and now it's just traditional RAG.</p><p>And if you have an IR system, you get to skip this for loop of agent queries. You know, an example that was really popular on Twitter recently was extracting knowledge graphs. You know, same thing here. Here, what I've done is I've made sure that the data structure I model is as close as possible to the graph visualization API.</p><p>What that gets me is really, really simple code that does, basically, the creation and visualization of a graph. I've just defined things one-to-one to the API, and now what I can do is if I ask for something that's very simple, like, you know, give me the description of quantum mechanics, you can get a graph out.</p><p>That's basically in, like, 40 lines of code, because what you've done is you've modeled the data structure the graph is needs to make the visualization. And we're kind of trying to couple that a lot more. This is a more advanced example, so don't feel bad if you can't follow this one.</p><p>But here, what I've done is I've done a question answer is a question and an answer, and the answer is a list of facts. And what a fact is, is it's a fact as a statement and a substring quote from the original text. I want multiple quotes as a substring of the original text.</p><p>And then what my validators do is it says, you know what, for every quote you give me, validate that it exists in the text chunk. If it's not there, throw out the fact. And then the validator for question and answer says, only show me facts that have at least one substring quote from the original document.</p><p>So now I'm trying to encapsulate some of the business logic of not hallucinating, not by asking it to not hallucinate, but actually trying to figure out what is the paraphrasing detection algorithm to identify what the quotes were. And what this means is instead of being able to say that the answer was in page seven, you can say the answer was this sentence, that sentence, and something else.</p><p>And I know they exist in the text chunks. And so I think what we end up finding is that as language models get more interesting and more capable, we're only going to be limited in the creativity that we can have to actually prompt these things, right? Like you can have instructions per object, you can have like recursive structures, right?</p><p>It goes into domain modeling more than it goes to prompt engineering. And again, now we can use the code that we've always used. If you want more examples, I have a bunch of examples here on different kinds of applications that I've had with some of my consulting clients. Yeah, I think these are some really useful ones.</p><p>And I'll go to the next slide, which is... This doesn't have the QR code. That's fine. The updated slide has a QR code, but instead you can just visit jxnl.github.io/instructor. I also want to call out that we're also experimenting with a lot of different UIs to do this structured evaluation, right?</p><p>Where you might want to figure out whether or not one response was mean, but you also want to figure out what the distribution of floats was for a different attribute and be able to write evals against that. And I think there's a lot of really interesting open work to be done, right?</p><p>Like right now we're doing very simple things around extracting graphs out of documents. You can imagine a world where we have multimodal, in which case you could be extracting bounding boxes, right? Like one application I'm really excited about is being able to say, give an image, draw the bounding box for every image, and the search query I would need to go on Amazon to buy this product.</p><p>And then you can really instantly build a UI that just says, you know, for every bounding box, render a modal, right? You can have like generative UI over images, over audio. I think in general it's going to be a very exciting space to play more with structured outputs. Thank you.</p><p>Ladies and gentlemen, please join me in welcoming our next guest, Senior Applied Scientist at Amazon, Eugene Yen. Thank you. Thank you everyone. I'm Eugene Yen and today I want to share with you about some building blocks for LLM systems and products. Like many of you here, I'm trying to figure out how to effectively use these LLMs in production.</p><p>So a few months ago, to clarify my thinking, I wrote some patterns about building LLM systems and products, and the community seemed to like it. There's Jason asking for this to be seminar. So here you go, Jason. Today, I'm going to focus on four of those patterns: evaluations, retrieval-augmented generation, guardrails, and collecting feedback.</p><p>All the slides will be made available after this talk. So I ask you to just focus. Buckle up, hang on tight, because we'll be going really fast. All right, let's start with evals, or what I really consider the foundation of it all. Why do we need evals? Well, evals help us understand if our prompt engineering, our retrieval augmentation, our fine-tuning, is it doing anything at all?</p><p>Right? Consider eval-driven development, where evals guide how you build your system and product. We can also think of evals as test cases, right, where we run these evals before deploying any new changes. It makes us feel safe. And finally, if managers at OpenAI take the time to write evals or give feedback on them.</p><p>You know it's pretty important. But building evals is hard. Here are some things I've seen folks trip up on. Firstly, we don't have a consistent approach to evals. If you think about more conventional machine learning, regression, we have root mean square error, classification, precision recall, even ranking, NDCG. All these metrics are pretty straightforward, and there's usually only one way to compute them.</p><p>But what about for LLMs? Well, we have this benchmark whereby we write a prompt, there's a multiple choice question, we evaluate the model's ability to get it right. MMLU is an example that's widely used where it assesses LLMs on knowledge and reasonability, you know, computer science questions, math, US history, et cetera.</p><p>But there's no consistent way to run MMLU. Less than a week ago, Avin and Sayash from Princeton, evaluating LLMs is a minefield. They ask, are we assessing prompt sensitivity? Are we assessing the LLM? Or are we assessing our prompt to get the LLM to give us what we want?</p><p>On the same day, Entrophic noted that the simple MCQ may not be as simple as it seems. Simple formatting changes, such as different parentheses, lead to different changes in accuracy. And no one is, there's no consistent way to do this. As a result, it makes it really difficult to compare models based on these academic benchmarks.</p><p>Now, speaking of academic benchmarks, we may have outgrown some of them. For example, this task of summarization. On the top, you see the human evaluation scores on the reference summaries. And on the bottom, you see the evaluation scores for the automated summaries. You don't have to go through all the numbers there, but the point is that all the numbers on the bottom are already higher than the numbers on top.</p><p>Here's another one that's more recent on the XSUM dataset, extreme summarization, where you see that all the human evaluation scores are lower than instruct GPT. And that's not even GPT-4. Now, finally, with all these benchmarks being so easily available, we sometimes forget to ask ourselves, hey, is it a fit for our task?</p><p>If you think about it, does MMLU really apply to your task? Maybe, if you're building a college-level chatbot, right? But here's Linus reminding us that we should be measuring our apps on our task and not just rely on academic evals. So how do we do evals? Well, I think as an industry, we're still figuring it out.</p><p>Bar pointed out it's the number one challenge out there, and we hear so many people talk about evals. I think there are some tenants emerging. Firstly, I think we should build evals for our specific task. And it's okay to start small. It may seem daunting, but it's okay to start small.</p><p>How small? Well, here's Technium. You know, he releases a lot of open source models. He starts with an eval set of 40 questions for his domain expert task. 40 evals. That's all it takes, and it can go very far. Second, we should try to simplify the task as much as we can.</p><p>You know, while LLMs are very flexible, I think we have better chance if we try to make it more specific. For example, if you're using an LLM for content-moration task, you can fall back to simple precision and recall. How often is it catching toxicity? How often is it catching bias?</p><p>How often is it catching hallucination? Next, if it's something broader like writing SQL or extracting JSON, you know, you can try to run the SQL and see if it returns the expected result. That's very deterministic. Or you can check the extracted JSON keys and check if the JSON keys and the values match what you expect.</p><p>These are still fairly easy to evaluate because we have expected answers. But if your task is more open-ended, such as dialogue, you may have to rely on a strong LLM to evaluate the output. However, this can be really expensive. Here's Jerry saying, you know, 60 evals, GPT-4, it costs him a lot.</p><p>Finally, even if you have automated evals, I think we shouldn't discount the value of eyeballing the output. Here's Jonathan from Mosaic. I don't believe that any of these evals capture what we care about. They have a prompt to generate games for a three-year-old and a seven-year-old, and it was more effective for them to actually just eyeball the output as it trains throughout the epochs.</p><p>Okay, that's it for evals. Now, retrieval-on-metageneration. I don't think I have to convince you all here why we need retrieval-on-metageneration, but, you know, it lets us add knowledge to our model as input context where we don't have to rely solely on the model's knowledge. And second, it's far practical, right?</p><p>It's cheaper and precise and continuously fine-tuning to our new knowledge. But retrieving the right documents is really hard. Nonetheless, we have great speakers, Jerry and Anton, sharing about this topic tomorrow, so I won't go into the challenges of retrieval here. Instead, I'd like to focus on the LLM side of things, right, and discuss some of the challenges that remain even if we have retrieval-on-metageneration.</p><p>The first of all is that LLMs can't really see all the documents you retrieve. Here's an interesting experiment, right? The task is retrieval-omited question-and-answering, you know, historical queries on Google, and hand-annotated answers from Wikipedia. As part of the context, they provide 20 documents. Each of these documents are at most 100 tokens long, so that means 2,000 tokens maximum.</p><p>And one of these documents contain the answer, and the rest are simply distractors. So the question they had was this: How would the position of the document containing the answer affect question-answering? Now, some of you may have seen this before, don't spoil it for the rest. If the answer is in the first retrieved document, accuracy is the highest.</p><p>If it's in the last, accuracy is decent. But if it's somewhere in the middle, it's actually worse accuracy than having no retrieval-on-metageneration. So what does this mean? It means that even if context window sizes are growing, we shouldn't allow our retrieval to get worse. Getting the most relevant documents to rank highly still matters, regardless of how big the context size is.</p><p>And also, even if the answer is in the context and in the top position, accuracy is only 75%. So that means even with perfect retrieval, you can still expect some mistakes. So another gotcha is that LLMs can't really tell if the retrieved context is irrelevant. Here's a simple example.</p><p>So here are 20 top sci-fi movies, and you can think of this as movies that I like. And I asked the LLM if I would like Twilight. So for folks not familiar with Twilight, you know, it's romantic fantasy, girl, vampire, werewolf, something like that. But I think I've never watched it before.</p><p>But I have a really important instruction. If it doesn't think I would like Twilight because I've watched all these sci-fi movies, it should reply with not applicable. And this is pretty important in recommendations. We don't want to make bad recommendations. So here's what happened. First, it notes that Twilight is a different genre and not quite sci-fi, which is fantastic, right?</p><p>But then it suggests ET because of interspecies relationships. I mean, I'm not sure how I feel about that. Yeah, I mean, how would you feel if you got this for a movie recommendation? The point is, these LLMs are so fine-tuned to be helpful, and it's really smart. And they try their best to give an answer, but sometimes it's really hard to get them to say something that's not relevant, especially something that's fuzzy like this, right?</p><p>So, how do we best address these limitations in RAC? Well, I think that there are a lot of great ideas in the field of information retrieval. Search and recommendations have been trying to figure out how to show the most relevant documents on top, and I think it worked really well.</p><p>And there's a lot that we can learn from them. Second, LLMs may not know that the retrieved document is irrelevant. I think it helps to include a threshold to exclude irrelevant documents. So, in the Twilight and sci-fi movie example, I bet we could do something like just measuring item distance between those two, and if it's too far, we don't go to the next step.</p><p>Next, guardrails. So, guardrails are really important in production. We want to make sure what we deploy is safe. What's safe? What's safe? We can look at OpenAI's moderation API, hate, harassment, self-harm, all that good stuff. But another thing that I also think about a lot is guardrails on factual consistency, or we call that hallucinations.</p><p>I think it's really important so that you don't have trust-busting experiences. You can also think of these as evals for hallucination. Fortunately, or unfortunately, the field of summarization has been trying to tackle this for a very long time, and we can take a leave from that playbook. So, one approach to this is via the natural language inference task.</p><p>In a nutshell, given a premise and a hypothesis, we classify if the hypothesis is true or false. So, given a premise, John likes all fruits, the hypothesis that John likes apples is true, therefore it's entailment, because there's not enough information to confirm if John eats apples daily, it's neutral.</p><p>And finally, John dislikes apples, it's clearly false, therefore contradiction. Do you see how we can apply this to document summarization? The premise is the document, and this hypothesis is the summary. And it just works. Now, when doing this, though, it helps to apply at the sentence instead of the entire document level.</p><p>So, in this example here, the last sentence in the summary is incorrect. So, if we run the NLI task on the entire document and summary, it's going to say that the entire summary is correct. But if you run it at the sentence level, it's able to tell you that the last sentence in the summary is incorrect.</p><p>And they include a really nice ablation study, right, where they check the granularity of the document. As we got finer and finer, from document to paragraph to sentence, the accuracy of detecting factual inconsistency goes up. That's pretty amazing. Now, another approach is sampling, right? And here's an example from Chef CheckGPD.</p><p>Given an input document, we generate a summary multiple times. Now, we check if those summaries are similar to each other, ngram overlap, bird score, et cetera. The assumption is that if the summaries are very different, it probably means that they're not grounded on the context document and therefore likely hallucinating.</p><p>But if they're quite similar, you can assume that they're grounded effectively and therefore factual. And the final approach is asking a strong LLM. You know, conceptually, it's simple. Given an input document and summary, they get the LLM to return a summary score. And this LLM has to be pretty strong.</p><p>And we have seen that strong LLMs are actually quite expensive. But in the case of factual consistency, I've seen similar, simple, simpler methods outperform LLM-based approaches at a far lower cost. So, try to keep things simple if you can. Okay, now to close the loop, let's touch briefly about collecting feedback.</p><p>And I'm going to need audience help here. So, why is collecting feedback important? Because we want to understand what our customers like and don't like. And then the magic thing here is that collecting feedback helps you build your evals and fine-tuning dataset. New models come and go every day, but your evals and fine-tuning dataset, that's your transferable asset that you can always use.</p><p>So, but collecting feedback from users is not as easy as it seems. So, explicit feedback can be sparse. Sparse means very low in number. And explicit feedback is feedback we ask users for. So, here's a quick thought experiment. How many of you here use ChatGPT? Okay, I see a lot of you.</p><p>How many of you here actually click the thumbs up and thumbs down button? Accidentally. Okay, but these are the beta testers, right? But you can see it's very small in number. So, even if you include this thumbs up, thumbs down button, you may not be getting the feedback you expect.</p><p>Now, if the issue with explicit feedback is sparsity, then the issue with implicit feedback is noise. So, implicit feedback is the feedback you get as users organically use your product, right? You don't have to ask them for feedback, but you get this feedback. So, here's the same example. How often do you click the copy code button?</p><p>The rest of you just type it out like a madman? Okay. But does clicking the copy code button mean that the code is correct? In this case, no. End rows is not a valid argument for Panda's read parquet. But if we were to consider all code snippets that were copied as positive feedback, we would have a lot of bad data in our training.</p><p>So, think about that. So, how do we collect feedback? I don't have any good answers, but here are two apps I've seen do it really well. First one, GitHub Copilot, or any kind of coding assistant, right? For people not familiar with it, you type some functional signature, some comments, and it suggests code.</p><p>You can either accept the code, reject the code, move on to the next suggestion. We do this dozens of times a day. Imagine how much feedback they get from this, right? Here's a golden dataset. Another example is Mint Journey. For folks not familiar, Mint Journey, you write a prompt, it suggests four images.</p><p>And then, based on those images, you can either rerun the prompt, you can either vary the prompt, that's what the V stands for, or you can either upscale the image, that's what the U stands for. But do you know what an AI engineer sees? Rerunning the prompt is negative reward, where the user doesn't like any of the images.</p><p>Varying the image is a small positive reward, where the user is saying, "This one has potential, but tweak it slightly." And choosing the upscale image is a large positive reward, where the user likes it and just wants to use it. So, think about this. Think about how you can build in this implicit feedback data flywheel into your products that you quickly understand what users like and don't like.</p><p>Oh, sorry. You can take your phone out. All slides available after the talk. So, that's all I wanted to share. If you remember anything from this talk, I hope it's these three things. You need automated evals. You need automated evals. Just annotate 30 or 100 examples and start from there, right?</p><p>And then figure out how to automate it. It will help you iterate faster, right? On your prompt engineering, on your retrieval augmentation, on your fine-tuning, help you deploy safer. I mean, this is a huge conference of engineers. I don't think I have to explain to you the need for testing.</p><p>Eyeballing doesn't scale. It's good as a final vibe check, but it just doesn't scale. Every time you update the prompt, you just want to run your evals immediately, right? I run hundreds of... I run tens of experiments every day, and the only way I can do this is with automated evals.</p><p>Second, reuse your existing systems as much as you can. There's no need to reinvent the wheel. BM25, metadata matching can get you pretty far, and so do the techniques from recommendation systems, right? Two-stage retrieval and ranking, filtering, etc. All these information retrieval techniques are optimised to rank the most relevant items on top, so don't forget about them.</p><p>And finally, UX plays a large role in LLM products. I think that a big chunk of GitHub Copilot and ChatGPT is UX. It allows you to use the LLMs in your context without calling an API. You can use an IDE using a chat window. Similarly, UX makes it far more effective for you to collect user feedback.</p><p>Okay, that's all I had. Thank you, and keep on building. Our next speaker is AI lead at Notion. Please welcome Linus Lee. Hi, everyone. I'm Linus. I'm here to talk about embedding. I'm grateful to be here at the inaugural AI engineer conference. Who learned something new today? Yeah. Before I talk about that, a little bit about myself.</p><p>If you don't know me already, I am Linus. I work on AI at Notion for the last year or so. Before that, I did a lot of independent work, prototyping, experimenting with, trying out different things with language models, with traditional LLP, things like TF, IDF, BM25, to build interesting interfaces for reading and writing.</p><p>In particular, I worked a lot with embedding models and latent spaces of models, which is what I'll be talking about today. But before I do that, I want to take a moment to say it's been almost a year since Notion launched Notion AI. Our public beta was first announced in around November 2022.</p><p>So as we get close to a year, we've been steadily launching new and interesting features inside Notion AI. From November, we have AI autofill inside databases, translation, and things coming soon, though not today, so keep an eye on the space. And obviously, we're hiring, just like everybody else here.</p><p>We're looking for AI engineers, product engineers, machine learning engineers to tackle the full gamut of problems that people have been talking about today. Agents, tool use, evaluations, data, training, and all the interface stuff that we'll see today and tomorrow. So if you're interested, please grab me and we'll have a little chat.</p><p>Now, it wouldn't be Alan's talk without talking about latent spaces, so let's talk about it. One of the problems that I find always motivated by is the problem of steering language models. And I always say that prompting language models feels a lot like you're steering a car from the backseat with a pool noodle.</p><p>Like, yes, technically, you have some control over the motion of the vehicle. It's like there's some connection. But like, you're not really in the driver's seat. The control isn't really there. It's not really direct. There's like three layers of indirection between you and what the vehicle's doing. And that, to me, trying to prompt a model, especially smaller, more efficient models that we can use for production with just tokens, just prompts, feels a lot like there's too many layers of indirection.</p><p>And even though models are getting better at understanding prompts, I think there's always going to be this fundamental barrier between indirect control of models with just prompts and getting the model to do what we want them to do. And so perhaps we can get a closer layer of control, a more direct layer of control, by looking inside the model, which is where we look at latent spaces.</p><p>Latent spaces arise, I think, most famously inside embedding models. If you embed some piece of text, that vector of 1536 numbers or 1024 numbers is inside a high-dimensional vector space. That's a latent space. But also you can look at latent spaces inside activation spaces of models, inside token embeddings, inside image models, and obviously other model architectures like autoencoder.</p><p>adapters. Today, we're going to be looking at embedding models, but I think a lot of the general takeaways apply to other models, and I think there's a lot of fascinating research work happening inside other models as well. When you look at an embedding, you kind of see this, right?</p><p>You see like rows and rows of numbers. If you ever debug some kind of an embedding pipeline and you print out the embedding, you can kind of tell it has like a thousand numbers, but it's just looking at like a matrix screen of numbers raining down. But in theory, there's a lot of information actually packed inside those embeddings.</p><p>If you get an embedding of a piece of text or image, these latent spaces, these embeddings represent, in theory, the most salient features of a text or the image that the model is using to lower its loss or do its task. And so maybe if we can disentangle some meaningful attributes or features out of these embeddings, if we can look at them a little more closely and interpret them a little better, maybe we can build more expressive interfaces that let them control the model by interfering or intervening inside the model.</p><p>Another way to say that is that embeddings show us what the model sees in a sample of input. So maybe we can read out what it sees and try to understand better what the model's doing. And maybe we can even control the embedding, intermediate activations to see what the model can generate.</p><p>So let's see some of that. So some of this some of you might have seen before, but I promise there's some new stuff at the end, so hang tight. So here's some sentence that I have. It's a sentence about this novel, one of my favorite novels, named Diaspora. It's a science fiction novel by Greg Egan that explores evolution and existence, post-human artificial intelligences, something to do with alien civilizations and the questioning the nature of reality and consciousness, which you might be doing a lot given all the things that are happening.</p><p>And so I have trained this model that can generate some embeddings out of this text. So if I hit the center, it's going to give us an embedding. But it's an embedding of length 2048, and so it's quite large. But it's just a row of numbers, right? But then I have a decoder half of this model that can take this embedding and try to reconstruct the original input that may have produced this embedding.</p><p>So in this case, it took the original sentence. There's some variation. You can tell it's not exactly the same length, maybe. But it's mostly reconstructed the original sentence, including the specific details like the title of the book and so on. So we have an encoder that's going from text to embedding, and a decoder that's going from embedding back to text.</p><p>And now we can start to do things with the embedding to vary it a little bit. And see what the decoder might see if we make some modifications to the embedding. So here, I've tried to kind of blur the embedding and sample some points around the embedding with this blur radius.</p><p>And you can see the text that's generated from those blurry embeddings, they're a little off. Like, this is not the correct title. The title's kind of gone here. It still kept the name Greg, but it's a different person, and so there's kind of a semantic blur that's happened here.</p><p>But this is kind of boring. This is not really useful. What's a little more useful is trying to actually manipulate things in more meaningful directions. So now we have the same taste of text. And now here I have a bunch of controls. So maybe I want to find a direction in this embedding space.</p><p>Here I've computed a direction where if you push an embedding in that direction, that's going to represent a shorter piece of text of roughly the same topic. And so I pick this direction, and I hit go, and it'll try to push the embedding of this text in that direction and decode them out.</p><p>And you can tell they're a little bit shorter if I push it a little bit further, even. So now I'm taking that shorter direction and moving a little farther along it and sampling, generating text out of those embeddings again. And they're even a little bit shorter. But they've still kept the general kind of idea, general topic.</p><p>And with that kind of building block, you can build really interesting interfaces. For example, I can plop this piece of text down here, and maybe I want to generate a couple of shorter versions. So this is a little bit shorter. This is even more short. But maybe I like this version.</p><p>So I'm going to clone this over here. And I'm going to make the sentiment of the sentence a little more negative. And you can start to explore the latent space of this embedding model, this language model, by actually moving around in a kind of spatial canvas interface, which is just kind of interesting.</p><p>Another thing you can do with this kind of embedding model is, now that we have a vague sense that there are specific directions in this space that mean specific things, we can start to more directly look at a text and ask the model, hey, where does this piece of text lie along your length direction or along your negative sentiment direction?</p><p>So this is the original text that we've been playing with. It's pretty objective, like a Wikipedia-style piece of text. Here I've asked ChatGPT to take the original text and make it sound a lot more pessimistic. So things like the futile quest for meaning and plunging deeper into the abyss of nihilism.</p><p>And if I embed both of these, what I'm asking the model to do here is embed both of these things in the embedding space of the model and then project those embeddings down onto each of these directions. So one way to read this table is that this default piece of text is at this point in this negative direction, which by itself doesn't mean anything, but it's clearly less than this.</p><p>So this piece of text is much further along the negative sentiment axis inside this model. When you look at other properties, like how much of the artistic kind of topic does it talk about, it's roughly the same. The length is roughly the same. Maybe the negative sentiment text is a bit more elaborate in its vocabulary.</p><p>And so you can start to project these things into these meaningful directions and say, what are the features of the models, what are the attributes of the models finding in the text that we're feeding it? Another way you could test out some of these ideas is by mixing embeddings.</p><p>And so here I'm going to embed both of these pieces of text. This one's the one that we've been playing with. This one is the beginning of a short story that I wrote once. It's about this town in the Mediterranean coast that's calm and a little bit old. And both of these have been embedded.</p><p>And so I'm going to say, this is a 2,000-dimensional embedding. I'm going to say, give me a new embedding that's just the first 1,000 or so dimensions from the one embedding, and then take the last 1,000 dimensions of the second embedding and just slam them together and have this new embedding.</p><p>And naively, you wouldn't really think that that would amount too much. That would be kind of gibberish. But actually, if you generate some samples from it, you can see in a bit, you get a sentence that's kind of a semantic mix of both. You have structural similarities to both of those things.</p><p>Like you have this structure where there's a quoted kind of title of a book in the beginning. There's topical similarities. There's punctuation similarities, tone similarities. And so this is an example of interpolating in latent space. The last thing you may have seen on Twitter is about, okay, I have this un-embedding model and I have kind of an un-embedding model.</p><p>That works pretty well. Can I use this un-embedding model and somehow fine-tune it or otherwise adapt it so we can read out text from other kinds of embedding spaces? So this is the same sentence we've been using, but now when I hit this run button, it's going to embed this text not using my embedding model, but using OpenAI's text-to-eta2.</p><p>And then there's a linear adapter that I've trained so that my decoder model can read out not from my embedding model but from OpenAI's embedding space. So I'm going to embed it. It's going to try to decode out the text from given just the OpenAI embedding. And you can see, okay, it's not as perfect, but there's a surprising amount of detail that we've recovered out of just the embedding with no reference to the source text.</p><p>So you can see this proper noun, diaspora, it's surprisingly still in there. This feature where there's a quoted title of a book is in there. It's roughly about the same topic, things like the rogue AI. Sometimes when I rerun this, there's also references to the author where the name is roughly correct.</p><p>So even surprising features like proper nouns, punctuation, things like the quotes, general structure and topic, obviously, those are recoverable given just the embedding because of the amount of detail that these high-capacity embedding spaces have. But not only can you do this in the text space, you can also do this in image space.</p><p>So here I have a few prepared files. Let's start with me. And for dumb technical reasons, I have to put two of them in. And then let's try to interpolate in this image space. So this is now using clips embedding space. I'm going to try to generate, say, like six images in between me and the Notion avatar version of me, the cartoon version of me.</p><p>If the back end will warm up, cold starting models is sometimes difficult. There we go. So now it's generating six images, bridging, kind of interpolating between the photographic version of me and the cartoon version of me. And again, it's not perfect, but you can see here, on the left, it's quite photographic.</p><p>And then as you move further down this interpolation, you're seeing more kind of cartoony features appear. And it's actually quite a surprisingly smooth transition. Another thing you can do on top of this is you can do text manipulations as well because clip is a multimodal text and image model.</p><p>And so I can say, let's add some text. I'm going to subtract the vector for a photo of a smiling man. And instead, I'm going to add the vector for a photo of a very sad, crying man. And then I'll embed these pieces of text. And empirically, I find that for text I have to be a little more careful, so I'm going to dial down how much of those vectors I'm adding and subtracting.</p><p>And then generate 6 again. And... It's taking a bit. Okay. I'm really sad. And you can do even more fun things. Like, you can try to add -- like, here's a photo of a beach. I'm going to try to add some beach in this. This time maybe just generate 4 for the sake of time.</p><p>Or maybe there's a bug and it won't let me generate. So in all these demos that I've done, both in the text and image domain -- okay, the beach didn't quite survive the latent space arithmetic. But in all these demos, the only thing I'm doing is calculating vectors, calculating embeddings for examples.</p><p>And embedding them and just adding them together with some normalization. And it's surprising that just by doing that, you can try to manipulate interesting features in text and images. And with this, you can also do things like add style and subject at the same time. You can -- this is a cool image that I thought I generated when I made my first demo.</p><p>And then you can also do some pretty smooth transitions between landscape imagery. So -- that's interesting. In all these prototypes, one principle that I've tried to reiterate to myself is that oftentimes when you're studying this very complex, sophisticated models, you don't necessarily have the ability to look inside and say, "Okay, what's happening?" Not even getting an intuitive understanding -- even getting an intuitive understanding of what is the model thinking, what is the model looking at, can be difficult.</p><p>And I think these are some of the ways that I've tried to render these invisible parts of the model a little bit more visible, to let you a little bit more directly observe exactly what the model is -- the representations the model is operating in. And sometimes you can also take those and directly interact or let humans directly interact with the representations to explore what these spaces represent.</p><p>And I think there's a ton of interesting, pretty groundbreaking research that's happening here. On the left here is the Othello world model paper, which is fascinating. Neurons in a haystack. And then on the right is a very, very recent -- I had to add this in last minute because it's super relevant.</p><p>In a lot of these examples, I've calculated these feature dimensions by just giving examples and calculating centroids between them. But here, anthropics and new work along with other work from Conjecture and other labs have found unsupervised ways to try to automatically discover these dimensions inside models. So that's super exciting.</p><p>And in general, I'm really excited to see latent spaces that appear to encode, you know, by some definition interpretable, controllable representations of the models input and output. I want to talk a little bit in the last few minutes about the models that I'm using. The text model is a custom model.</p><p>I won't talk -- I won't go into too much detail, but it's fine-tuned from T5 checkpoint as a denoising autoencoder. It's an encoder/decoder transformer with some modifications that you can see in the code. So here's a general transformer. Encoder on the left, decoder on the right. I have some pooling layers to get an embedding.</p><p>This is like a normal T5 embedding model stack. And then on the right, I have this special kind of gated layer that pulls from the embedding to decode from the embedding. You can look at the code. It's a little more easy to understand. But we take this model, and we can adapt it to other models as well, as you saw with the OpenAI embedding recovery.</p><p>And so on the left is the normal trading regime where you have an encoder, you get an embedding, and you try to reconstruct the text. On the right, we just train this linear adapter layer to go from embedding of a different model to then reconstruct the text with a normal decoder.</p><p>And today, I'm excited to share that these models that I've been dealing with, that you may have asked about before, are open on Hugging Face. So you can go download them and try them out now. These are the links. On the left is the Hugging Face models, and then there's a Colab notebook that lets you get started really quickly and try to do things like interpolation and interpretation of these features.</p><p>And so if you find any interesting results with these, please let me know. And if you have any questions, also reach out, and I'll be able to help you out. The image model that I was using at the end was CacaoBrains Carlo. Excited to see Korea stepping up there.</p><p>In this model, this model is an unclip model, which is trained kind of like the way that DALI 2 was trained as a diffusion model that's trained to invert clip embedding. So go from clip embedding of images back to text. And that lets us do similar things as the text model that we used.</p><p>In all this prototyping, I think a general principle, if you have one takeaway from this talk, it's that when you're working with these really complex models and kind of inscrutable pieces of data, if you can get something into a thing that feels like it can fit in your hand, that you can play with, that you can concretely see and observe and interact with, can be directly manipulated, visualized, all these things, all the tools and prototypes that you can build around these things, I think help us get a deeper understanding of how these models work and how we can improve them.</p><p>And in that way I think models, language models and image models, generative models are a really interesting laboratory for knowledge, for studying how these different kinds of modalities can be represented. And Brad Victor said, "The purpose of a thinking medium is to bring thought outside the head to represent these concepts in a form that can be seen with the senses and manipulated with the body.</p><p>In this way the medium is literally an extension of the mind." And I think that's a great poetic way to kind of describe the philosophy that I've approached a lot of my prototyping with. So, if you follow some of these principles and try to dig deeper in what the models are actually looking at, build interfaces around them, I think more humane interfaces to knowledge are possible.</p><p>I'm really excited to see that future. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. And now we welcome Dr. Brian Bischoff, head of AI at Hex, and Dr. Chris White, CTO at Prefect, in a fireside chat moderated by Brittany Walker, principal at CRV. Thank you. Thank you so much, everyone, for being here. I'm really excited to be moderating this panel between two of my favorite people working in AI.</p><p>I'm Brittany. I'm a principal at CRV, which is an early-stage venture capital firm, investing primarily in seed and Series A startups. Chris, why don't you give us a little bit about yourself? Test one, two. Everybody can hear me? All right. My name's Chris. I'm currently the CTO at Prefect.</p><p>We're a workflow orchestration company. We build a workflow orchestration dev tool and cell and remote orchestration as a service. A little background. So I started kind of my journey into startup land and eventually AI and data, got a Ph.D. in math focused on non-convex optimization, which I'm sure a lot of people hear into.</p><p>And then eventually, you know, data science and then into the kind of dev tool space, which is where I'm at now. Awesome. And Brian, fill us in on your side. I'm Brian. I lead AI at Hex. Hex is a data science notebook platform, sort of like the best place to do data science workflows.</p><p>I was going to say I started my journey by getting a math Ph.D., but he kind of already took that one. It's kind of awkward. Yeah. I've been doing data science and machine learning for about a decade and, yeah, currently find myself doing AI, as they call it these days.</p><p>Awesome. So both of you are at relatively early stage startups. And as we all know, early stage startups have a number of competing priorities, everything from hiring to fundraising to building products. And one might say it would be a lot to kind of take a moment and just say, what is this AI thing?</p><p>What the fuck do we do with this? And so I'm wondering, how did you decide that AI was something that you really needed to invest in when you already had, you know, established business, growing well, lots of users, lots of customers, presumably placing a lot of demands on your time?</p><p>So, Chris, I would love to hear from you on how you guys thought about that choice. Yeah, so there are a couple of different dimensions to it for us. So we are, you know, a workflow orchestration company, and our main user persona are data engineers and data scientists, but there's nothing inherent about our tool that requires you to have that type of use case.</p><p>And so one thing, one dimension for us is, right, we assumed that a big component of AI use cases were going to be data driven, right? Like semantic search or, like, retrieval, summarization, these sorts of things. So just we wanted to make sure that, you know, we had a seat at the table to understand how people were productionizing these things and, like, were there any new ETL considerations when you're, you know, moving data between maybe vector databases or something?</p><p>So that was one thing. Another one that I think is interesting is when I look at AI going into production, I see, basically, a remote API that is expensive, brittle, and non-deterministic, and that's just a data API to me. And so, right, if we can orchestrate these workflows that are building applications for data engineers, presumably a lot of that's going to translate over.</p><p>And so, and I mean, last, like, you know, I'm sure the reason most people are here now is, you know, it was fun, and so we just wanted to learn in the open. So we did end up just kind of creating a new repo called Marvin that I think Jason mentioned in his last talk, just to kind of keep up, you know, be incentivized to keep up.</p><p>And, Brian, you were literally brought on board to Hex to focus on this stuff. Would love to hear more about how that decision was made and how you've spent your time on it. Yeah, I think a couple of things. One is that data science is this unique interface between sort of, like, business acumen, creativity, and, like, pretty, like, difficult sometimes programming.</p><p>And it turns out that, like, the opportunity to unlock more creativity and more business acumen as part of that workflow is a really unique opportunity. I think a lot of data people, the favorite part of their job is not remembering Matplotlib syntax. And so the opportunity to sort of, like, take away that tedium is a really exciting place to be.</p><p>Also, realistically, any data platform that isn't integrating AI is almost certainly going to be dooming themselves to the now. And sort of it will be table stakes pretty soon. And so I think missing that opportunity would be pretty criminal. Yeah, I totally agree with that. So you decided that you were going to go ahead and do this.</p><p>You were going to go all in on AI. What criteria did you evaluate when you were determining how you were going to build out these features or products? Did you optimize for how quickly you could get to market, how hard it would be to build, ability to work within your existing resources?</p><p>What criteria did you consider when you were saying, okay, this is how we're actually going to take hold of this thing? So for us, I guess there's two different angles. There's the kind of just pure open source Marvin project. It is a product, but not one that we sell, just one that we maintain.</p><p>And then we do have some AI features built into our actual core product. And I think they have slightly different success criteria. So for Marvin, it's mainly just getting to see how people are experimenting with LLMs and just talking to users directly. It just kind of gives us that avenue and that audience.</p><p>And so that's just been really useful and insightful for us. So we just get on the phone. I mean, our head of AI gets, you know, talks to users at least, you know, a couple times a day. And then for our core product, so one way that I love to think about DevTools and think about what we build is failure mode.</p><p>So, like, I like to think of choosing tools for what happens when they fail. Can I quickly recover from that failure and understand it? And so a lot of our features are geared towards that sort of kind of discoverability. And so for AI, it's kind of the same thing.</p><p>It's like quick error summaries shown on the dashboard for quick triage. And then measuring success there is, like, relatively straightforward, right? It's, like, how quickly are users kind of getting to the pages they want and how quickly are they debugging their workflows? So, like, very quantifiable. Yeah, we'd love to hear from you, too.</p><p>Yeah, my team's charter is relatively simple. It's make using Hex feel magical. And so ultimately we're constantly thinking about sort of what the user is trying to do in Hex during their data science workflow and making that as low friction as absolutely possible and giving them more enhancement opportunities.</p><p>So a simple example is I don't know how many times you all have had a very long SQL query that's made up of a bunch of CTEs and it's a giant pain in the ass to work with. So we build an explode feature. It takes a single SQL query, breaks it into a bunch of different cells, and they're chained together in our platform.</p><p>This is, like, such a trivial thing to build, but it's something that I've wanted for eight years. Like, I've done this so many times. It's so annoying. And so thinking like that makes it really easy to make tradeoffs in terms of what is important and what we should focus on.</p><p>And so in terms of, like, how we think about, yeah, like, where our positioning is, it's really just how do we make things feel magical and feel smooth and comfortable. And how did you reallocate resources beyond, you know, they obviously hired you. That was a great step in the right direction.</p><p>But what else did you do to actually get up and running in terms of operationalizing some of this stuff? Yeah, I think we kept things pretty slim and we continue to keep things pretty slim. We started with one hacker. He built out a very simple prototype that seemed like it showed promise.</p><p>And then we started building out the team. We scaled the team to a couple people and we've always remained as slim as possible while building out new features. These days, I have a roadmap long enough for 20 engineers and we continue to stay around five. And that's not an accident.</p><p>Basically, like, ruthless prioritization is definitely an advantage. And, Chris, you guys wound up hiring a guy as well, right? Yeah, we hired a great guy. His name's Adam. So he definitely owns most of our AI, but also, right, like, anyone at the company that wants to participate. And so there was one engineer that got really into it and is, for, you know, all intents and purposes, is, like, effectively switched to Adam's team and is now doing AI full-time.</p><p>Yeah, so you guys are really dedicating a lot to solving this problem, including the hiring of two people on your side and one on Brian's. So presumably, you're going to be looking for a return on that investment. So how do you think about what a successful implementation of an AI-based feature or product looks like?</p><p>For us, I would say that already we've hit that success criterion. So now the question is, like, further investment or just kind of keep going with the way that we're doing it. But so big thing was time to value in the core product. That we can just easily see has definitely happened with just the few sprinkles of AI that we put in.</p><p>So we'll just kind of keep pushing on that. And then, kind of like I said in the beginning, just getting involved in those conversations, those really early conversations about companies looking to put AI in production. And we've been having those on the regular now. So I would say, like, already feels like it was well worth the investment.</p><p>What about you guys? You obviously just had a big launch the other day, too. Curious how you thought about success for that. Yeah. Once again, it's sort of like how frequently do our users reach for this tool? Ultimately, Magic is a tool that we've given to our users to try to make them more efficient and have a better experience using Hex.</p><p>And so if they're constantly interacting with Magic, if they're using it in every cell and every project, then that's a good sign that we're succeeding. And so to make that possible, we really have to make sure that Magic has something to help with all parts of our platform. We have a pretty complicated platform that can do a lot.</p><p>And so finding applications of AI in every single aspect of that platform has been one of our sort of, like, you know, north stars. And very intentionally so to make sure that we're, you know, making our platform feel smooth at all times. Awesome. Well, let's move on to the next section.</p><p>We're going to talk about how you guys actually built some of these features and products. Since we're all here at the AI Engineer Summit, I assume we all have an interest in actually getting stuff done and putting it into prod. So when you were making some of these initial determinations, Brian, how did you guys determine what to build versus buy?</p><p>Yeah. So from day one, I think one of the first questions I asked when I joined is what they were doing for evaluation. And you might say, like, okay, yeah, we've heard a lot about evaluation today. But I would like to remind everyone here that that was February. And the reason that I was asking that question already in February is because I've been working in machine learning for a long time where evaluation sort of, like, gives you the opportunity to do a good job.</p><p>And if you've done a poor job of objective framing and done a poor job of evaluation, you don't have much hope. And so I think the first thing that we really looked into is evals. And back then there was not 25 companies starting evals. There are now more than 25.</p><p>But ultimately we made the call to build. And I'm very confident that that was the right call for a few reasons. One, evals should be as close to production as possible, is literally, like, using prod when possible. And so to do that, you have to have very deep hooks into your platform when you're moving at the speed that we try to move.</p><p>That's hard for a SaaS company to do. On the flip side, we chose to not build our own vector database. I've been, you know, doing semantic search with vectors for six, seven years now. And I've used open source tools like Face and Pinecone back when it was more primitive.</p><p>Unfortunately, a lot of those tools are very complicated. And so having set up vector databases before, I didn't want to go down that journey. So we ended up working with LanceDB and sort of built a very custom implementation of vector retrieval that really fits our use case. That was highly nuanced and highly complicated.</p><p>But it's what we needed to make our RAG pipeline really effective. So we spent a lot of effort on that. So ultimately, just sort of, where is the complexity worth the squeeze? Totally. And Chris, what about you guys? How did you do that? So I have a couple of different kind of things that we decided on here, and some of which are still in the works.</p><p>Vector databases, a million percent agree with that. Like, we would never build our own. We haven't had as much need of one, I think, as Hex. But we've done a lot with both Chroma and with Lance, but neither in production yet. So none of those use cases are in prod.</p><p>And so the way that I've, the exposure that I've seen about people actually integrating AI into, you know, their workflows and things is, there's a lot of experimentation that happens. And then you kind of want to get out of that experimental framework maybe by just looking at all of the prompts that you were using and then just using those directly yourself with no framework in the middle.</p><p>And then once you're kind of in that mode, like I was saying before, like, you're just, at the end of the day, you're interacting with an API, and there's lots of tooling for that. And so I kind of see a lot of the decisions, at least, that we had to confront on build versus buy is, like, it's another just kind of tool in our stack.</p><p>Do we already have the sufficient dev tooling to support it, make sure it's observable, monitorable, and all this? And we did, so we didn't do any buying. It was all build. Yeah. And as you mentioned, I think, you know, you talked about many, many eval startups. I think we're all familiar with the broad landscape of vector databases as well.</p><p>Are there any pieces of your infrastructure stack that you wish people were building or you wish people were kind of tackling in a different way than what you've seen out there so far, either one of you? Yeah. I mean, I think it would have been a hard sell to sell me on an eval platform.</p><p>I think there was some opportunity to sell me on, like, an observability platform for LLMs. I've looked at quite a few, and I will admit to being an alumni of weights and biases, so I have some bias. But that being said, I think there is still a golden opportunity for a really fantastic, like, experimentation plus observability platform.</p><p>One thing that I'm watching quite carefully is Rivet by Ironclad. It's an open source library, and I think the way that they have approached the experimentation and iteration is really fantastic, and I'm really excited about that. If I see something like that get laced really well into observability, that's something that I'd be excited about.</p><p>Anything to add on your side? I think, like, small addition to what Brian said, which is just more focus on kind of the machine-to-machine layer of the tooling, and so I think a lot, you know, right at the end of the day, the input is always kind of this natural language string, and that makes a lot of sense, but the output, making it more of a guaranteed-typed output, like with function calling and other things, I think is one step in the journey of integrating AI actually into back-end processes and machine-to-machine processes, and so any focus in that area is where, you know, my interest gets peaked for sure.</p><p>Yeah, totally. Okay, so you have all your people, and you have all your tools, and then you're obviously completely good to go and fully in production. JK, we all know it doesn't work that way. What challenges did you run into along the way, maybe ones that you didn't expect or that were larger obstacles than you would have thought?</p><p>So I don't, integrating AI into our core product, I would say from a tooling and developer perspective and, you know, productionizing perspective, none. Culturally, though, I would say we definitely hit, you know, some challenges, which is that when we first, we're like, all right, let's start to incorporate some AI and do some ideation here, right?</p><p>A lot of engineers just started to throw everything at it, like, we should, it should do everything. It can monitor itself for, like, all of this stuff, and it was like, all right, all right, everyone needs to kind of, like, backtrack, and so just that internal conversation of, like, you know, getting buy-in on, like, very specific focus areas, which, you know, at the end of the day, where we are focused is that just removal of user friction, whether it's through design or just through, like, quicker surfacing of information that AI just, like, lets you do in a more guaranteed way.</p><p>But, yeah, restraining the enthusiasm was the biggest challenge for sure, and it still exists to this day. Everyone wants to be an AI engineer, right? Yeah, exactly. Yeah. What about you guys? Did you have similar or different issues? That's interesting. It's, like, a similar flavor. It's a little different instantiation, which is to say that, like, you know, I've never met an engineer that's good at estimating how long things take, and I would say that, like, that is somehow exacerbated with AI features because then you, your first few experiments show such great promise so quickly, but then the long tail feels even longer than most engineering corner case triage, just such a long journey between we got this to work for a few cases and we think we can make it work to it's bulletproof is even more of a challenging journey.</p><p>And I, yeah, this, like, over enthusiasm, I think, yeah, slightly different instantiation, but similar flavor. Whenever you're on that journey, how are you testing and tracking along the way, if at all, which is totally, yeah. Yeah, I mean, to be a broken record, a lot of, like, robust evals, like, trying really hard to codify things into evaluations, trying really hard to codify, like, if someone comes to us and says, wouldn't it be great if magic could do X, we sort of pursue that conversation a little bit further and say, like, okay, what would you expect magic to do with this prompt?</p><p>What would you expect magic to do in this case? And kind of get them to kind of, like, vet that out and then sort of using this, like, barometer of could a new data scientist at your company with very little context do that? And sort of that, like, you know, cutting edge around what's feasible and what's possible.</p><p>Yeah, that makes a lot of sense. And, you know, one of the reasons I was excited to have the two of you up here together is because, you know, while Prefect has some elements of AI in the core product, as you mentioned, probably you're best known for the Marvin project that you guys have put out there, which is kind of a standalone project, which is a really interesting phenomenon that I'll say that I've kind of observed in this current wave of AI, which is, you know, companies that maybe weren't doing AI previously launching entirely separate brands, essentially, alongside the core product.</p><p>So we'd love to understand more of what were your user experience considerations when you were building out, you know, Marvin as a separate product versus Prefect. What freedom did that allow you? What restrictions did you still have? Yeah, that's a good question. So a few different angles there. I think one kind of philosophical angle is, you know, we try to do things that maximize our ability to learn without having to go full commitment.</p><p>And so I think starting a new open source repo, like, right, we definitely have some ties to it now. We have to maintain it. But past that, it's not all that high of a cost. But, like, if it, you know, it's all upside, basically. If no one notices it, no big deal.</p><p>We learned a little bit more about how to, you know, write APIs that, you know, interface with all of the different LLMs, for example, or something like that. Or if it does take off, which, you know, it basically did for us, we got to meet all these new people who are working on interesting things like AI and data adjacent.</p><p>But for the core product, this was maybe more, I guess, kind of interesting. And, Brian, I'd be curious to hear about how much you had to, like, really focus some of your prompts to the use case that you cared about. So Prefect is a general purpose orchestrator. And so the reason I say that again is our use case scope is, like, technically infinite.</p><p>And so helping people write code to do completely arbitrary things is definitely not a value add we're going to have over the engineers at OpenAI or at GitHub or something else. So we knew that we couldn't invest in, like, that way of integrating AI. And so then the next question was, like, okay, so then what are just the marginal ads?</p><p>And that's kind of where we landed, you know, where we are today. But there was, we did put energy initially, like, can we put this directly in, like, the SDK or something like that? And just very quickly realized that it was just too large of scope. And at that point, you might as well just have the user do it themselves.</p><p>And, like, we're not adding anything to that workflow. Yeah. Yeah, and on the flip side, you know, Magic has been kind of a part of Hex, basically, it seems like, since inception from the outside. Obviously, we've all seen, again, a number of text-to-SQL players out there. We can make arguments about whether or not those should exist as standalone companies.</p><p>But I'm curious, you know, how you guys had to think about UX considerations when you were building out Magic in the context of the existing Hex product? Ultimately, I've been really fortunate to kind of, like, work with a great design team who sort of, they're just excellent. But the question about, like, how does Magic feel?</p><p>Magic is not its own product. I think that's one thing that's been important from early on. Magic is not a product. Magic is an augmentation of our product. So it is a collection of features that makes the product easier and more comfortable to use. That is an easy sort of thing to keep in mind when deciding how to design because it allows us to say, okay, like, we don't want this to distract from the core product experience.</p><p>I can tell a story. We had one sprint where we would design something called Crystal Ball. And Crystal Ball was a really sick product. It did exactly what we wanted to do, and it felt wonderful. However, ultimately, it drew the user away from the core Hex experience. And very quickly, our CEO rightly was like, I feel like this is kind of splitting Magic out into its own little ecosystem.</p><p>And that made it kind of clear that that might be the wrong direction to go. So even though Crystal Ball did feel really good and had a really incredible capability behind it, and frankly, the design on Crystal Ball was beautiful, the problem with that was it pulled us away from what we were really trying to do, which was make Hex better for all of our users.</p><p>Every Hex, like, consumer should be able to benefit from Magic features, and that was starting to split that. And so we literally killed Crystal Ball despite it being a really cool experience for that reason. So genuinely, we've really stuck to the, like, it's one platform, and Magic augments it.</p><p>Yeah, that makes a lot of sense. And obviously, you know, Hex already had a relatively sizable user base at the time you guys launched this. So I'm curious, how did you think about the rollout, like, just in terms of what users you gave it to and what timeline, what marketing did you do, all of those types of considerations?</p><p>Yeah, generally, we start with a private beta, and then we, as quickly as possible, expand that to a sort of, like, public beta. Our goal is to find people that are, like, engaged with the product, and they are prepared for some of the limitations of AI tools. Stochasticity has come up many times, and ultimately, we're expecting the user to work with a stochastic thing.</p><p>Also, they're working with something very complex, which is data science workflows. So we're looking for people that are pretty technical in the early days. Then we want to keep scaling and scaling to include the rest of the distribution in terms of technical capabilities so that we can make sure that it's really serving all of our users.</p><p>And on the flip side, again, you had a little bit maybe more flexibility with the rollout, just given it was a new repo. I'm curious if that was different, similar to what Brian's talked about. Well, so, yeah, well, the repo, no. It was, we hacked on it, you know, we had fun with it.</p><p>We got it to a place where we felt proud of it, and then we clicked make public and then tweeted about it, and that was, like, the end of that. So that was just pure fun. But for integrating AI into our core product, I mean, this isn't particularly deep, but, you know, it's one of those things that I'm sure everyone here is thinking about and we'll continue to talk about, which is, for us, a large part of our customer base are, like, large enterprises and financial services and also healthcare.</p><p>And so, like, very, very security conscious, and so we definitely had to make sure that this was, like, a very opt-in type of feature. But, like, you know, we still want to have little, like, tool tips, like, hey, if you click this, but also if you click this, we will send a couple of bits of data, you know, to a third-party provider, so.</p><p>Yeah. Yeah. And post-rollout, just to go to kind of the last logical part of the conversation here, how have you guys thought about continuing to kind of measure the outputs? I mean, Brian, you're the big evals guy up here, so I'm sure that'll be the answer, but I would love to hear more about how you think about that measurement and in terms of both the model itself, but also in terms of, you know, the model in the context of the product, which I think is also something that people, you know, need to think about.</p><p>Yeah. So I recently learned that there's a more friendly term than dogfooding, which is drinking your own champagne. And so I'll say I drink a lot of champagne. I use magic every day, all through the day. One of the fun things about trying to analyze product performance is that you normally do that via data science.</p><p>And so I have this fun thing where I'm using magic to analyze magic, and I put a lot of effort into trying to understand where it's succeeding and where it's failing, both through traditional product analytics, guided by using the product itself. And so there's a very Ouroboros feeling, but ultimately good old-fashioned data science.</p><p>Love to hear it, and appropriate with where you've come from. Yeah. What about you guys? For us, it's, you know, I definitely don't have as much experience as Brian on that side of it. But for a while, one thing we were doing when it was pure just like prompt input, string output with no typing interface whatsoever is then using that and then writing tests that, again, used in LLM to do comparisons and semantic comparisons.</p><p>And, like, right, there's obviously problems with that, but, like, it also kind of works. But so then when we moved in kind of the typing world where, like, Marvin is for, like, guaranteed typed outputs, essentially, it definitely becomes a lot easier to test in that world, which is, you know, one reason that that's kind of the soapbox that I get on when I talk about LLM tooling, like, bringing it into the back end is just like having these typed handshakes.</p><p>Because, you know, you can write prompts where you know what the output should be, and it should have a certain type, and that's a very easy thing to test most of the time. Yeah, yeah, totally. And one of the things I think has been, you know, most fascinating about this wave of software, and, Brian, you alluded to this a little bit earlier with your comments around, you know, being stochastic, essentially, is that it's not deterministic, right?</p><p>And also, I think that AI-based software doesn't have to be static, either. It can be, you know, dynamic in a way that maybe traditional software isn't quite as much, and there's, you know, improvements that come along maybe on the UX side of things, but also the model. We've heard a lot of people talk about techniques like fine-tuning, techniques like RLHF, RLAIF, all sorts of, you know, approaches to kind of continuing to improve the model itself in the context of the product over time.</p><p>So, I'm curious about how you think about measuring that improvement as you continue to hopefully, you know, collect data and refine your understanding of the end user. Totally. There was a paper that came out in, like, June-ish or something that was, like, kind of splashy. It was from Matai from Spark, and it was like, oh, like, the models are degrading over time, even when they say they're not.</p><p>And, like, what I thought was interesting was for, like, the people that are doing this stuff in prod, we already knew that. Like, my evals failed the first day they switched to the new endpoint. I didn't even switch the endpoint over, and suddenly my evals were failing. So, I think there is a certain amount of, like, when you're building these stuff, these things in a production environment, you're keeping a very close eye on the performance over time, and you're building evals in this very robust way.</p><p>And I've said evals enough time for this conversation already, but I think the thing that I keep coming back to is, what do you care about in terms of your performance? Boil your cases down to traditional methods of evaluation. We don't need latent distance distributions and KL divergence between those distributions.</p><p>We don't need that. It turns out, like, blue scores of similarity aren't very good for LLMM outputs. This has been known for three, four years now. So, take your task. Understand what it means in a very clear, you know, human way. Boil it down to binary yes or nos.</p><p>And run your evals. And to the people that say, like, my task is too complicated, I can't tell if it's right or wrong, I have to use something more latent, I would challenge you to try harder. The tasks that I'm evaluating are quite nuanced and quite complicated, and it hasn't always been easy for me to come up with binary evaluations, but you keep hunting and you eventually find things.</p><p>You talked about type checking, and you talk about, like, type handshakes, and that's something that, like, a lot of people in ML have been preaching the gospel of composability for five years now. You know, these are not new ideas. They're just maybe new to some of the people that are thinking about evals today.</p><p>Yeah. Well, so, moral of the story is try harder, essentially. That's what I take away. Chris, did you have anything to add there? I think the only thing I'd add is I don't have much take on actually how someone should do it or what they could consider, but I think, you know, you just described a highly non-deterministic, very dynamic experimentation workflow, and, like, those are the sorts of things that just, like, our core product is meant for.</p><p>And so, like, experimenting with those, like, just knowing the structure of them is maybe the best way to say it is what fascinates me more than the actual, like, details of what metrics you might be using. Yeah. Well, you know, I think the other reason I was really excited to do this panel is because we have kind of maybe two sides of the same coin as it relates to being an AI engineer here, right?</p><p>One person coming from more of a traditional ML background, one person coming from more of a traditional engineering background, and both of you building these AI-based products. So I wanted to give you a second, if you have any last questions to ask of each other. Yeah. So you work in this, like, data workflow space, and, like, I've thought a lot about composability and, like, data workflows, and I've long been a fan of sort of, like, workflow-centric ML.</p><p>And so what I'd love to hear is sort of, like, when you think about building these agent pipelines, which are starting to get more into the, like, DAGs and the sort of, like, structured chains of response and request, what is the, like, one thing that, like, every AI engineer building agents should know from your sphere that will make it easier for them to build agents?</p><p>So, oh, that's a really good question. I don't, I think the main thing is something that I kind of alluded to earlier, which is think about failure modes. I think that is the biggest thing. So, like, runaway processes, capturing potential oddities in outputs or inputs as early as possible with some observability layer.</p><p>And so the earlier you can get that wiring in, I think, the better. And then caching is, like, this is the only time I will ever say this. It's definitely your friend in some of these situations. But it's also the root of all evils, so you've got to kind of, you know, balance that.</p><p>But, yeah, I think just thinking about the observability and debuggability layer, especially with some of the kind of black boxy and, like, people who are pushing it and actually having, like, immediate eval of the returned code or something, like, having that monitoring layer, I think, is just key. Yeah.</p><p>Chris, I know you've asked Brian a bunch during this panel, but anything else you want to ask? Yeah, I mean, I'm just really curious, you know, I'm sure everybody asks you this, but the hallucination problem. Like, how, you know, obviously your users can just confront it directly if it looks weird.</p><p>They can see that it looks weird or it errors out. But just how do you think about it as the person building that interface for your users? Yeah. Someone recently asked me for, like, references on hallucination. And I was like, what are some good references on hallucination? And I Googled around, and I found that generally the advice that people are giving is to fix hallucination, basically rag harder, just, like, make a better retrieval augmented pipeline.</p><p>And when I said that and I looked at myself, I was like, honestly, that's, like, kind of how we solved it. Like, our reduction in hallucination for magic, which is not an easy problem, was that we had to think a little bit more carefully about retrieval augmented generation. And in particular, the retrieval is not something that you'll find in any book, even the book that I just published.</p><p>Like, even in there, I don't talk about this particular retrieval mechanism, but it took us some additional thinking, but we got there. Yeah. So, again, moral of the story, try harder. Yeah. Just think carefully. Yeah. All right. Last thing, just to wrap up, what is your hot take of the day for closing out the AI Engineer Summit today?</p><p>I definitely stopped building chat interfaces. I think chat is a product, AI is a tool, and so finding ways to, once again, I know that I've said this before, but, like, improve on the machine. The machine interfaces so that developers can actually benefit and use AI more directly, as opposed to building chat everywhere.</p><p>Love that. Mine is a little bit mean-spirited, so I apologize in advance. I think a lot of the work that's in front of you as you're building out AI capabilities is going to be incredibly boring. And I think you should be prepared for that. The capability is really exciting.</p><p>The possibilities are amazing. And it's always been like this in ML. The journey feels very tedious. It's worth it in the end. It's so fun, but there's a lot of data engineering work in front of you, and I think people haven't yet appreciated how important that is. Yeah. No, I think it's very real and very fair take, as all of us try to start, hopefully, moving into production with a bunch of this stuff.</p><p>That's where the rubber meets the road, right? Well, that's all for us, I think. Thank you so much, the two of you, for coming up here with me. All right. I'm just going to say a few words of closing. Thank you all for the lovely panel. So now it's your turn to talk.</p><p>So we have placed a bunch of flag signs out in the ballroom. This is to help facilitate discussion. So gravitate towards the topics that you want to discuss. And what topics are these? These are multi-modality, AI UX, agents, prompt engineering, code generation, LLM tooling. Come on, clicker. Retrieval augmented generation, OSS models, hire and pitch, Langchain and Lama index, and GPUs and infra.</p><p>The LLM tooling, Langchain and Lama index, and AI UX will be found in Carmel, not the Lama index, that's across their booth, where the Microsoft Cloudflare, GitHub and Weights and Biases booth are. And there's additional food in Carmel as well with these topics. There's also Vector Village, a little further past Carmel.</p><p>That's in Monterey. And those are ad hoc demos. You can connect your laptop, make a presentation. There's also whiteboards for you to brainstorm. And then obviously there's more lounge and table seating. We have a cash bar. We maxed out our budget so far. But don't worry, tomorrow we have a hosted bar courtesy of Decibel VC.</p><p>So thanks for making this an incredible opening day. Please enjoy the topic tables until 9.30 p.m. And then we'll see you tomorrow morning at 9 a.m. for Breakfast and Expo with the opening keynote from Mario Rodriguez at GitHub starting at 9.45. Thank you all so much. We'll see you tomorrow.</p><p>We'll see you tomorrow morning at 9.30 p.m. We'll see you tomorrow morning at 9.30 p.m. We'll see you tomorrow morning at 9.30 p.m.</p></div></div></body></html>