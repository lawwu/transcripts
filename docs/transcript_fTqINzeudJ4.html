<html><head><title>China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner</h2><a href="https://www.youtube.com/watch?v=fTqINzeudJ4" target="_blank"><img src="https://i.ytimg.com/vi/fTqINzeudJ4/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=fTqINzeudJ4&t=0 target="_blank"">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=fTqINzeudJ4&t=235 target="_blank"">3:55</a> Open-Source Models in China<br><a href="https://www.youtube.com/watch?v=fTqINzeudJ4&t=1074 target="_blank"">17:54</a> Future of American Open-Source Models<br><a href="https://www.youtube.com/watch?v=fTqINzeudJ4&t=1590 target="_blank"">26:30</a> Compute Arms Race<br><a href="https://www.youtube.com/watch?v=fTqINzeudJ4&t=2825 target="_blank"">47:5</a> China, Tariffs, and Reordering of Global Trade<br><h3>Transcript</h3><div class='max-width'><p>15% on all goods coming from Europe, 0% on US goods going to Europe, so an opening up of Europe markets, paying us 15% and on top of that, getting commitments like $750 billion, almost a trillion dollars of energy purchases from the US. Or look at Japan, which they announced last week, another huge market.</p><p>Again, similar, they're going to pay tariffs to the United States, no tariffs imposed on the United States, and they're going to invest $550 billion into the US in a way the President gets to direct. So I just think we need to give the President credit where credit is due.</p><p>Everybody said this was going to lead to retaliation, to trade wars, was going to be disastrous for the US. And all we've seen so far is deals, deals, deals, deals. And I have to say, if this was the CEO of one of our companies, let's say we had a board meeting at the start of the year and he outlined these plans.</p><p>And we said, hey, we're really nervous about this. This is a high risk, high reward strategy. It's either going to backfire and we're going to fire you, or it's going to work really well and we give you a bonus. If we're measuring them halfway through the year, I would say that he's in line for a bonus.</p><p>All right, the Summer Pod's back in action. Good to see you guys. We have our good friend, Sunny Madra, the CEO of Grok joining from, I don't know, Sunny, it looks like some fancy hotel in Saudi Arabia in the middle of the night. Good to see you. You picked it up.</p><p>You picked it up. You have a good eye for the Middle East. And Bill, you got off your boat catching bluefin tuna. You look like you're in some office somewhere, so good to see you. I am borrowing Mitch Lasky's incredible podcast set up in our Woodside office with the high-def SLR camera.</p><p>Nice. You're looking good. I appreciate it. Sunny, of course, our good buddy Sunny is the CEO of Grok. I don't know, Sunny, probably a few hundred million of revenues, maybe doubling year over year. I just saw something. You're rumored to be raising 600 million at a $6 billion valuation.</p><p>Maybe that has something to do with you being over here in Saudi Arabia. And of course, you're hosting all the open source models in your inference clouds around the world. Is that about right? Yeah. You got it right. You touched on all the key points. We don't comment on speculation, but you touched on some good points.</p><p>Well, it's great to see you in DC last week. Our good friend David Sachs is really on a heater. First, it was the Crypto Summit a couple of weeks ago. Of course, the Genius Act got passed, which really teed up these stable coins. And now the Clarity Act around market structures making its way through Congress.</p><p>And then, of course, last week was the AI Summit, where the president laid out a multi-pronged strategic plan for American AI that both extends the government's investment and leadership, but also accelerates the distribution of the American AI stack around the world. Both of those things, both the crypto and the AI Summit that he put together, I thought are key contributions, really, to the next generation of American technology leadership around the world.</p><p>It's amazing to see that much progress in six months. Congrats to Sachs. And the rest of the team, Kratios, Dean Ball, Sriram, they really all kind of brought the heat with that. Yeah, for sure. Of course, the AI Action Plan was focused on maintaining global AI leadership, particularly over China.</p><p>And I hate to say it, but we have really been our own worst enemy. It seems like excess regulation on everything from energy production to model development to semiconductor chip distribution. It's really been a bit of an unforced error by the US over the course of the last 24 months, and handed a lot of momentum to China, and I think really threatened our leadership.</p><p>So this is kind of a 180 to get America back on track. We've underestimated Huawei and the Chinese AI development, model development, really at every step of the way. So I want to kick off today talking about recent developments with the base models and reasoning models coming out of China.</p><p>Because we had this freak out moment earlier in the year with DeepSeq that we all remember. Remember, Nvidia stock plummets, everybody in Washington's talking about it. But since then, people kind of forgot about DeepSeq, but the reality is China's been on a roll. I mean, they're dominating the global landscape for open source models.</p><p>We've seen six to seven high quality open source model providers, many of the fastest growing in the world. And this at a time when American open source, right, Lama 4 has been sputtering a bit, really losing its mojo around the world. So Quinn, the open source model out of Alibaba has passed, I think, 400 million downloads.</p><p>Of course, that's released like these other models under the Apache 2.0 open source license. So very open, as Bill's talked about. But Sonny, you tweeted, and one of the reasons I wanted to get you on the pod this week is you tweeted that all of these open source models are really coming together in China.</p><p>They're leveraging one another. They can distill and generate synthetic data on each other's work. And you went so far as to suggest this might allow them to pass the best proprietary models coming out of the US yet this year, maybe by Q4 of this year. So why don't we dig in there?</p><p>What is your theory of the case? Why is China doing so well in open source? And should US model companies like OpenAI and Anthropic be concerned? Yeah. So let's kind of tie it into, I think, three important things that we see happening. The first one being, you know, the Chinese and the president addressed this at the AI summit, right?</p><p>He addressed the point around using copyrighted work and he says, you know, he used a great example. If you read a book and you use it, you're not violating the copyright there. And so he addressed that concern. And that was one of the major things that, you know, a lot of people didn't talk about, but I think it's important for the model makers.</p><p>And so the Chinese just have been able to work around that because of, you know, their position on IP and what we're really seeing here. And I think, you know, Bill teed it up even better off of my tweet, which is, you know, they're able to compound. So what you're seeing very quickly is both the open source nature, the open weights nature, allow them to basically compound on each other.</p><p>So instead of working in silos and instead of having to create giant training clusters separately, they can basically take each other's work, build on top of it, almost consider it like a remix of someone's model K2, sort of a well-known remix of what DeepSeq had done. And now we're starting to see that happen really fast and we're seeing two dimensions of it quickly.</p><p>One we're seeing the leading edge models getting quicker and we're then seeing them distill down smaller, you know, turbo models really, really fast as well. There was a release today of a Quen, you know, 30 billion parameter model, which is performing as good as GPT-40. So think about that, right?</p><p>And GPT-40 was, you know, world-class not that long ago. So those are the reasons that we're really seeing an acceleration right now. You know, I want to dig into this model development in particular, you know, a year ago, we were talking about these models being these, you know, stochastic parrots, you know, and we really had to compress the entire internet.</p><p>So you go back to GPT-4, you know, and you're compressing, you know, the entire internet, but now we really don't need to do it because we've trained them to use tools like the internet, right? They're true reasoning engines. When I ask a question today, you know, it doesn't just spit out an answer immediately, it goes and uses the tool and it searches the internet.</p><p>So if you don't have to compress all this Wikipedia information, I don't know, take a subject like World War II, you just need to know how to go out and use the internet to find the information, to summarize it in real time. How has that changed the pace of progress and the balance between open and closed?</p><p>Yeah, and so it's spot on, right, Brad, what we see now, and you see it when you use these reasoning models, and what I suggest everyone do is when you're using a reasoning model, you can usually expand out its thought process. So when you ask a question, it'll say, oh, the person is asking a question about this, what should I do?</p><p>Let me go and maybe search the internet, let me go do a few different things, it can have a lot of different tools. And so the push has been towards, you know, really, really strong reasoning models. And, you know, we have to give credit there, OpenAI really started that with O1, that was really the first reasoning model.</p><p>that was put out there. But I think on the back of the research and the back of it, you know, everyone talked about, and, you know, the pod's good friend, Noam Brown was the leader on that program. Everyone's been able to look at that and say, let's reframe the problem.</p><p>And this allows us to build stronger reasoning models that don't have to compress, like you said, all the internet's information. And once they're coupled with strong tools, you start getting these really, really incredible results that don't even just show up in benchmarks, because none of the benchmarks really allow you to use a tool to answer the results.</p><p>Right, exactly. And if they did, we're going to see a whole bunch of new set of results that happen there. Hey, Bill, just in many ways, I think this validates what you were arguing over the last six to 12 months. You said this was likely to happen. Everybody knows you're one of the biggest proponents of open source in the world.</p><p>And you were telling people, don't make, you know, the Chinese are going to use open source to their advantage. And now we're seeing that in a really profound way. Why is China so successful here? And what can we learn from them? Well, we've talked about it in the past.</p><p>I won't dwell on it. But China got excited about open source about 20 years ago. It's not a new thing that's happened. And you can imagine when most of the world accuses you of IP theft, that embracing something like Linux and all the other software, open source products, seems very appealing, right?</p><p>And so I think it became kind of a common way of operating within China. And, you know, it's a country that hasn't prioritized IP protection the way we have, you know, around patents. And I can make an argument that, you know, there's way more prosperity if ideas are shared instead of protected.</p><p>But I don't know. The one thing I don't know is in the current AI situation, you know, was the government promoting open source and encouraging it? Or did it just develop through competitive forces? Right. But now you have, you know, a scenario where these companies, you know, are, you know, are, first of all, there's new ones popping up.</p><p>And so, you know, I almost feel like an idiot when Kimmy comes out and moonshot and then this week, I guess, I don't even know how to pronounce it, Zipu, you know, releases a model. And I go on PitchBook and I look it up and they've already raised $1.4 billion.</p><p>So it shouldn't have been a secret, but I didn't know about it, you know. And all of a sudden they're in the leader tables on open router and like, oh, my God, you know, they're coming out of everywhere. And what it shows you is just that when you have a competitive dynamic where every single player, and I think there may be seven or eight dig pocketed players with open models in China, they all learn from each other extremely fast.</p><p>And in this case, unlike software, you can use one model to distill the other and make it better. So it's almost like an accelerated, you know, form of that. And you just get massive, quick co-evolution. And I came up with a little analogy for people. I'll try and do it quickly.</p><p>But imagine you had two communities. They're both farming communities. And let's say there's 10 to 20 farms in each. And in one community, they come into the farmer's market once a week, and they just compete by selling their products. But then they go back. In the other community, when they come into farmer's market, they also, in addition to competing and selling, they're forced.</p><p>I don't know who would force them, but they're forced to share all their best practices from that week with everybody. And everyone does it. And everyone shares their best practices. And then if you ran that exercise over two years or whatever, obviously, the community where the best practices are shared across all farms is going to have a higher global output for the community than the one where you've just got proprietary ideas driving, you know, the individual and just competition without the ideas sharing.</p><p>And that may, like, even me saying that may cause some people to scream, that's socialism or, like, you know, they may not understand open source. Or how it works or how it works or why. But I do think you end up with a higher fitness level for a community that's behaving that way.</p><p>Overall, you may end up with a lot less chance of a breakout monopolist like we've had in many of the sectors in American technology. Well, I mean, let's just assume the Chinese government is, in fact, encouraging this in whatever ways, right? I mean, if you look at the release of the AI action plan last week, the Trump administration, you know, had a section.</p><p>They did. Which is about encouraging open source and open weight models in the US saying that these could become standards in some businesses and academic workloads. And it's important they're built on the American AI stack. So, you know, as an aside, the Chinese quickly followed, you know, the American AI plan.</p><p>I think they released theirs a couple of days later where they called for the establishment of a global AI cooperation organization, which I thought, again, is interesting. So, you know, Bill, how do you feel about, you know, like we haven't seen that much traction in the US labs on open source.</p><p>Obviously, Lama has been probably the market leader there, but this is for both of you, you know, handicap for me, if you will, how you think this plays out. First, Sonny, maybe you start. What do you see at Grok? Do you see a lot of demand for these Chinese open source models?</p><p>And if so, what would it take for an American open source model to catch up? Yeah, one of the things that we should pull up is the chart of intelligence to price. And one of the things that you see with the leading open source models now, which are the Chinese, is 90% of the quality in terms of intelligence, but at a 90% price discount.</p><p>And I think whenever you offer that to anybody, you're going to see people want to use that, whether it's individual developers or enterprises. And so we're seeing that around the world. Let me interrupt there real quick. So if you're looking at this chart, right, in the top right of that chart, you'll see a cluster of these Chinese open source companies, right?</p><p>And the vertical axes here being really intelligence, the horizontal axes from left to right being the cost per million tokens. And so you really want to be in the top right of that model, high intelligence, low cost. And what this chart shows is that, to Sonny's point, you can get 90% of the intelligence, right, for 10% or 20% of the cost.</p><p>And the result, I assume, Sonny, is that you're seeing huge demand at Grok and in Saudi Arabia, where you are right now, for these Chinese open source models. We are. And then now, just taking that forward, what do people want? They want some accountability. And that's what you'll get.</p><p>That's what you sort of got out of Linux and, say, Red Hat, right? As great as Linux was, and Bill was touching on it, the majority of the enterprise was using a distribution, which they could go and point to someone if they needed something. And so I think the world wants models that they can get from companies that they can go to.</p><p>And so to answer your question on what happens, I think if we look at Q4 this year or Q1 next year, I'd be willing to say, you know, a top three worldwide model will be a U.S.-based open source model. And, you know, we've got two big efforts happening there.</p><p>We know we have the open AI, open source, which, you know, a lot of people have been working on, and open AI. And even, you know, Sam himself has commented on that, and it's released later this summer. And then we have all the efforts by Meta. And if you take – you combine both of those things together, I don't think you end up with something that ends up further down the list in terms of intelligence and or price.</p><p>One thing I wanted to highlight about the China situation that I think might inform the U.S. situation, I was having a conversation with this extremely young AI entrepreneur that I know. And he was pouring over the Zipu – I hope I'm pronouncing that right – paper. And he asked me some information.</p><p>I went on PitchBook and sent it to him who had funded it. And he asked me, he says, why is Alibaba funding all these things when they've got their own model? And because they're in several of the other competitive plays. And it reminded me of, you know, a lot of the points that I've made about open source is, like, if you're not – If you're not confident you're going to win on offense, you want to play defense.</p><p>And so, for any large tech company, commoditizing a potential threat is actually quite valuable. You know, you look at what Facebook did with the Open Compute Initiative inside of their data centers. And so, you know, it may just be – very well be that Alibaba just wants to make sure there's no ByteDance, you know, that equivalent in the AI space.</p><p>And that would be pretty rational. And the reason I think that's an interesting data point when you think about the U.S. And, you know, there are several big tech companies that seem to be not on the bleeding edge of AI. You know, you got Microsoft maybe. I mean, they have access to OpenAI right now, but they might lose that or whatever.</p><p>You've got Amazon. You've got Apple. You know, if I'm at any of those companies, I'd be funding a Open Source competitor, you know, rather than funding, you know, like Amazon with Anthropoc. I think you're in a much better position to encourage Open Source. And so, I actually think we may – But, Bill, are there a bunch of Open Source startup models in the U.S.?</p><p>That's where I was going next. I think you're going to see new entrants pop up that try to co-evolve with the Chinese models. You know, is Linux American? Is Linux Chinese? No one thinks of it as having a domicile, right? And so, this is just me predicting. I don't – you know, I just think you're going to see – just like you saw Kimmy and Zipu pop up.</p><p>I wouldn't be shocked if you see other new entrants pop up that are trying to be, like, sanctioned or, you know, cleaned. You know, use Red Hat as an example, Sonny, version of these things. Because I think if you start with access to those Chinese models, it wouldn't take you long to move into a near place.</p><p>And you wouldn't have to spend the kind of money the foundational model companies have. And then you may see a big fight around regulatory capture where someone tries to say that's not allowed or whatnot. But I do expect to see that. And if I'm the Mistral team, if you're not distilling on these Chinese models, I don't know what you're doing right now.</p><p>But I don't have any data on that front. OpenAI is rumored to be launching their open source model any day. Sonny, what would they have to do? So to your point, you predicted, go back to the intelligence and pricing chart, right? So if OpenAI was in the top right of that chart, i.e.</p><p>if they're able to deliver something at the intelligence of, let's call it Quinn, and they're also able to deliver it to market at, you know, 20% of the cost, it would seem to me that actors around the world, certainly actor, it would be, you know, part of the American AI action plan.</p><p>We had won everybody in the world to use that model. And do you believe they have a shot at out competing, being the upper right, you know, by the end of the year? And if so, do you think that will be the outcome? Like these companies that are using Quinn on Grok, do you think they would prefer to use OpenAI so long as it was equally capable and equally priced performance?</p><p>So two things that we see is brand and, you know, the U.S. domiciled or, you know, someone that they can kind of point at, that wins. And so if that shows up, it will win. Because if you're a company and, you know, at some point you have to, you know, get your teams to sign off on what is it that you're using?</p><p>What are the risks associated with it? And like, you know, who is liable if something goes wrong? And so I sort of feel like with OpenAI's release and, you know, Meta charges back or even if some of these startups emerge that, you know, we can point at, I think we'll see a huge shift back towards those models versus the Chinese ones.</p><p>Yeah. And we really don't know at this point, unless you guys have some inside knowledge, like when Meta makes their second push with all these hires that they've made, are they going to remain committed to open source or even be more open? We don't, we really don't know yet at this point.</p><p>Right. You certainly seen some of those rumors out there. You know, I've seen rumors that on Twitter that they were debating whether or not they should back away from open source. My hunch is that that's a misread. My hunch is that they're going to stay very committed to open source, but they may complement it with a proprietary model.</p><p>That would be my guess, as opposed to scrapping open source altogether. And Brad, can I throw one thing out there? I think part of the pitch to get everyone there is that it's open because everyone that they're pulling over are coming from closed places. And so if you're really passionate about the work you're doing and you're passionate about where this is going to go and there's only, there's only one company that can fund that to that scale and do it open is, is those guys.</p><p>So I think it's part of the pitch. Yeah. Some of the researchers like have a religious belief in it, which was evident in the, in the interview with the Deep Seek founder. Like it's, it's, it's oddly like higher on their Maslow, you know, hierarchy than the money. Of course they're getting the money.</p><p>Of course, yeah, this one is both. This is, I think a really important point I want to come back to Sonny is that you're seeing massive demand for these Chinese open source models today, precisely because enterprises around the world can utilize them. They have 90% of their capabilities at 20% of the cost, 10 or 20% of the cost, you know, so it turns out if you deliver something really powerful and really cheap, that's more important to these players than American values aligned.</p><p>Right. But if you gave them something that was super powerful and super cheap and aligned with Western values, right? That you think that would be the winning formula for an American open AI model to top the, the distribution leaderboards around the world. I think, yeah, on a place like Open Router, you know, where you can see where this is happening, I see, we would see it rise to the top in a few days.</p><p>That's music to David Sachs' ears because, you know, clearly in the strategic plan, they're worried about Chinese open source models dominating globally. And if you just watch the pace of releases, the quality of the releases out of China, the cycle time on the innovation in the open source community out of China, it's faster and better at the moment.</p><p>So, and the only real big development, you know, we may see some of these new startups, Bill, we may see a reboot out of Meta, but the one that has, I think, everybody really holding their breath and hoping that we see something really capable and powerful is this open source model that's been promised out of OpenAI.</p><p>Now, of course, Elon says he's committed to open source as well. Grok 4 is a great model, is impressive what they put out there. Any idea, Sonny, about the open source plans out of X? Yeah. So I think like Elon has been, you know, pretty said it on Twitter that they'll always open source one generation back.</p><p>And so while they were on three, you know, they should have gotten to two and now they're on four. So I think the thinking is that they will get there. You know, my only guess would be is that, you know, right now if they were to open source two or even three, it's so far behind that, you know, what's the purpose in doing it?</p><p>It may not even be utilized and you'll maybe end up having to deal with just, you know, a bunch of internet or Twitter FUD. Just coming back to OpenAI, I will tell you, it's like one of those things that really you rarely see in the enterprise. It's almost like the demand for like a, you know, like a Tesla Roadster or something or Model Y before it came out.</p><p>Everybody asked for it. It's like, that's the model that everybody wants to use right now. And so, you know, we can't wait till it comes live and, you know, when it's on Grok, when it's all over the world. I think it's going to be a real big one for everyone.</p><p>And while we have you, Sonny, you know, there's really just been this explosion in the compute arms race, right? There was a big debate. You were on the pod a year ago. We had with Bill, you know, had we topped out on compute demand? You know, were we entering an overbuild a la Cisco 2000?</p><p>And it's really been remarkable. I think now that's very clear. Just a couple of tweets out of Elon and Sam Altman the last couple of weeks on this compute demand has really caught my attention. If you look at this one out of Elon talking about the X.AI goal is 50 million in units of H100 equivalent.</p><p>And Clark Tang on my team tweeted something that broke that down, which showed that that reflected something like, you know, 4 million total GPUs and an energy footprint of like 11 gigawatts. Right? And then Sam Altman comes out and talks about their deal down in Abilene for four and a half gigawatts and the fact that they were going to come in well above the $500 billion estimate that they had promised to the government.</p><p>So these compute clusters now that are being talked about being built over the next five years, these are massively bigger than what we were even talking about a year ago. And I think they reflect this move toward inference time reasoning, agent to agent interaction, reasoning engines. You know, Jensen's comment on the pod last year, the inference was going to 1 billion X and the consequence, you know, what we're going to need in terms of compute power to power all that.</p><p>Maybe just reflect, you're in the middle of all this, you're building out your own inference clouds around the world. Is this a lot of hyperbole and chest pounding or do you actually see the dollars going into the ground in places like Saudi Arabia and around the US? Yeah, I'm going to just quantify it with Google for a second.</p><p>So, you know, in the, in the, in the, in the Mary Meeker bond deck, you know, they have a slide there that shows Google went from 5 trillion tokens a month to 480 trillion tokens a month. And they had just put some press out that they crossed like, you know, 800 trillion.</p><p>And I saw something today that crossed a quadrillion and I had to look that up. That's a thousand trillion. And so in, in a course of a year and a few months, they've gone from 5 trillion to a thousand trillion. So that's 200 X right there. Wow. I mean, that just shows it to you without having to look at anything else.</p><p>Yeah. The amount of demand. Every single search query on the planet today is now an inference transaction. Correct. And so, you know, and you see it, you know, in Anthropic with, you know, their continued fundraisers going, you know, through the roof. That's happening because they're, they're seeing the amount of token consumption.</p><p>And so anywhere we lay infrastructure, we fire up a rack. It becomes fully consumed within a few hours. And so. You're talking GROC. It was GROC, yeah. So you have demand far outstripping supply, even at GROC. Yep. We do. So, Bill, you see these fundraising announcements that are being discussed.</p><p>Just CNBC's reporting tonight. I think that Iconic is going to lead a $5 billion round into Anthropic at $170 billion. In the case of Anthropic, it's, you know, $170 billion on rumored $5 billion in revenue. X has been rumored to be raising at $150 to $200 billion. So, you know, you've never in the history of venture, you've never seen fundraisers like this.</p><p>One of the topics that is being hotly debated on the Twitter is that there's massive intervening dilution in these rounds, Bill, because of the employee option grants or the employee RSUs that are needing to be granted to keep the employees, you know, in these businesses. So, just observing this a little bit from afar, I don't think you guys, you're a direct investor in any of these major labs.</p><p>What do you see? What do you observe? What are your warning signs, you know, about the size and the demand that you see in these rounds? Well, I, yeah, I've never seen anything like it. I mean, I saw, you know, through the Uber and Lyft situation, I saw a precursor to this, but these dollars are even bigger.</p><p>And the amount of money that these companies are willing to lose in a year. You know, I still think it's particularly interesting that Google has to compete with OpenAI because OpenAI is going to lose $7 billion this year and Google won't. Like, they would never allow themselves to do that.</p><p>And this started back in that previous era where, for the first time ever, you saw private companies have a competitive advantage in that they can be more risk-seeking with capital than the public companies are allowed to be. Yeah, yeah. But I think in the past 12 months, we've seen OpenAI, Meta, and certainly X, you know, move into this place.</p><p>I call them the cost is no object, the CINO group, where they're just putting out press lease after press lease and opening data center after data center. And, you know, there are other people, I think, you know, you look at Microsoft, you know, choosing not to extend their CapEx budget.</p><p>You look at that Amazon example when we spoke to the LeFont brothers at Code 2 where they're not keeping up with the NVIDIA purchases relative to their AWS share. And so there are a few companies that are back on their feet and there's a few companies that are really pushing the gas pedal.</p><p>And I can't, there's a few in the middle, I can't tell whether Anthropic has the audacity and the means to raise enough money to start building data centers themselves. They haven't so far. But it's, you know, it's a sport of kings, you know, it's a sport of kings. It's like there's never been this amount of money spent right now in video and others building in the stack like like Dell that we spoke to a few weeks ago, like they're the winners in a pick and shovel game that's got this amount of aggressiveness.</p><p>It's maybe maybe SK Hynix, too. I don't know. Sonny, do you see, you know, going back to the well-worn cliche that every glut or that every shortage ultimately ends in a glut? Do you have any evidence on the horizon where you see supply outstripping demand? No, I mean, I was going to ask this to Bill as he was just saying it, Bill, like on a daily basis, are you consuming more tokens or are you consuming more, you know, more, you know, just traditional, you know, web lookups, right?</p><p>And I'd be willing to guess you're consuming more tokens. Oh, it's insane. And tokens are increasing. Like when you're using those reasoning models, you don't see all the tokens, right? They don't publish it. Oh, right. Totally. Yeah, it's 10 to 100x more than your very first AI search. Yeah, for sure.</p><p>Absolutely. The one thing I will, so you're right. I'm doing that. Thank you. Go ahead. You finish. No, no, no, no. And I just wanted to say, even yesterday, Anthropic had to put this, you know, press release plus, you know, product change out saying, hey, we've got to throttle everybody.</p><p>Right? And this is like, because we have all these people using way too much of our services. So, I think, you know, if you have intelligent models, right? And you have the capacity for it. It's one of those things people are consuming. Jeevan's paradox or whatever. Yeah. Sorry, go ahead, Bill.</p><p>I'm going to offer one caveat to this super exciting line of thought, which is because of the amount of venture capital out there, companies are not pricing the cost. Like no one, none of the model companies, I don't think anyone, even in the verticals, like no one's pricing the cost because they're pricing to take market share.</p><p>And, you know, you and I, Sonny, we had a previous discussion about the unlimited pricing and inference has variable costs. So, is that even sustainable? Right? And even when people say to me, oh, well, we're going to run out of power. You wouldn't run out of power if you just took the price up.</p><p>Like the thing that throttles demand is price. But no one here is raising price. Like Anthropic doesn't need to throttle. They just raise price. But they're not willing to do that because they're afraid to lose share. And so, everyone's pricing to share. That means they're pricing under cost. There's rumors of even some of the best known brands in AI having negative gross margin.</p><p>And so, I don't know when that settles out, but that will create a bump in the supply demand curve if that ever has to be, you know, fixed. But for now, it doesn't. Can I ask a question there? Yeah. Yeah. No, no. Which is going back to the point that we made on open source.</p><p>But if you know in the back of your mind that there's something that's 90% as good, but it's 90% cheaper. How does that factor in? Because we've also never had that factor as we're going through this growth curve. I mean, almost since we started the pod, you know, I've routinely highlighted that the steepness of that price curve on, you know, as it kind of becomes, you know, less cutting edge is something I've never seen before.</p><p>I've never ever seen it. I've never ever seen it. And I'm sure that a lot of people sit around and say, well, it's okay if I'm losing money here because, you know, six months from now, I'll just use the older model. And we also talked in the past about how in the internet age, all the startups began with Oracle and Sun, and eventually they all moved to Linux and MySQL.</p><p>And so there was a, there was a, we got to win at all costs phase. And then there was a phase where you started worrying about cost and optimization. And so one day, one day we'll likely, you know, make that, make that move. And, and, and I, a few of the companies I've talked to that are running inference at scale, they are already starting to think that way.</p><p>Like they're looking, they're looking at it, you know, from that, from that lens. But, but it was, and, and I think that's why Grock and Cerebris are doing so well, but Sonny, give us, give us an, you know, an example. I would imagine that the windsurf of the world and the cursors of the world and all these folks who are building these, you know, these coding agents, they've got massive, you know, demand for their applications, right?</p><p>But they're paying through the nose to Anthropic or to these underlying proprietary model providers to be able to do that. So what's the dynamic that you see there? Do you see them running to implement CLEN or some of these cheaper models? Yeah. Without kind of getting into specific, any one of them, but like, you know, multiple folks are building their own models based off open source.</p><p>Right. So they could just go distill any one of these models. Correct. Right. And, you know, given that they're these very lenient Apache licenses and eliminate the entire cost of Sonnet that sits under it for 70% of use cases. Yeah. And like Bill said, turn that into a premium offering, right?</p><p>And say that's the, you know, the gold and, you know, the silver and the bronze are built off, you know, something that's, like I said, one tenths the price. That seems to me, Bill, you know, if I had to forecast, you know, if I'm open AI, I'm running a consumer business with really high gross margins.</p><p>Right. Because consumers are less sensitive to what they're paying. Their intensity of use is lower. Whereas if somebody's writing code, you know, there's the variable intensity is high. And so, you know, for them, it would make sense to launch an open source model back to where we started the pod and price it really low to, you know, drive share knowing, you know, reminds me a little bit of Amazon back in the day.</p><p>Amazon had this monopoly retail business they could use to subsidize AWS, gain share for a decade, and then begin to take price. That would be a rational strategy for open AI to follow. So you take the profitable consumer business, you use it to subsidize, you know, the market sharing other applications that you hope to build.</p><p>It's certainly a reasonable strategy. I'm not inside that company. You have way more knowledge than I do. But, you know, I pay the $200 a month and I do every one of my searches on 4.5, and I'm probably negative, I would think. And so I do think there'll be some rationalization where these models kind of self-pick which one they're running based on what you need.</p><p>I probably don't need to be using 4.5. And move to more of a consumption logic. Yeah. Move more to more of a consumption logic. They're already doing that in parts of their enterprise business. And, you know, as they've transitioned to more of this consumption logic, I think it's led to some real unlocks, you know, for the business.</p><p>I think that makes it harder if you're in the lab game and you don't have a consumer product and you don't own an application that you can drive high gross margins. I think then gets back to this question, how long can you run your business for share, right? Hoping that someday, because, you know, they all exist at the beneficence of the capital markets and the capital markets are willing to provide an incredible amount of capital to these businesses today.</p><p>They sure are. But you and I have, we've lived through these periods where that disappears quickly. And can I can I put something there just to kind of hear your feedback on it, guys, which is look at Google and the TPU. And Google is clearly, you know, by these numbers that we're seeing, right, putting out more tokens than anyone else.</p><p>Right. Do they do you guys believe they have a strategic advantage because they have their own hardware? They're not having to pay, you know, 80 percent margin on something that they can generate tokens with. Like, how do you guys look at that business and say clearly they look to be sort of the largest, at least openly saying the largest processor of tokens?</p><p>I think, look, the key data point in that case, which I don't have the data, but I'd be glad to repeat it if someone shared it with us, is how many non Google applications are running on the TPUs? Like how many third party customers are using, because what I've heard or what the, you know, the general perception is, is that that most of their proprietary TPU transactions are their own applications.</p><p>Yep. But that's probably where most of their tokens are being processed anyways at this point, Bill, right? Like transcribing YouTube videos and, you know, all you can Google meet, you can turn it on and all those searches. I was just inferring in your question, maybe I shouldn't have been, that they'll have an advantage for Google Cloud.</p><p>And in order for that to be true, they need to have this crossover moment. One quick thing, Brad, on OpenAI, you know, I've been writing a book, which I've talked about frequently, and I've been quite, although I guess there's some privacy things now you need to be worried about, but I've been quite open with OpenAI about the book and doing research, you know, along the way.</p><p>It knows a tremendous amount about my book right now, and I can ask follow-up questions without having to put the whole book back in the prompt again because of that. So incredible. And so I would continue to believe that OpenAI's most likely chance to long-term success comes from switching costs and lock-in more than it will come from staying on the edge of the model race.</p><p>And the pricing power that comes with that brand dominance, right? No doubt. No doubt. Because the fact of the matter is, you said you're paying $200, you're getting more than $200 in value. I don't know what the price is, but I know that if it was variable, you would pay a hell of a lot more money to use that service to it.</p><p>I would, but the lock-in once one of these systems starts to truly understand you and have all your historic knowledge, I think the switching costs will be very high at that moment in time. Sonny, back to your question about the TPU and the advantage of that vertical integration, you know, to Google.</p><p>I think it's too early to know. What I would tell you is that they've absolutely made some changes, I think, over the course of the last three to four months to accelerate the business. You've seen the news about OpenAI leveraging TPUs for some of the inference demand that they have.</p><p>Ultimately, what I've said all along about Google is that in many ways the best position company in the world, but a lot of their advantage, right, is no longer much of an advantage, right? And namely that, you know, they were the dominant place where the consumer started every single query.</p><p>And we know today that's just not true. And, you know, when people are looking for answers, Bill's book is not in Google. Bill's book is in, you know, ChatGPT. Actually, it's in Google Docs, but you're right. You're right. The knowledge of it. The knowledge of it. The knowledge of it and the interaction and the token generation.</p><p>So my only point is this, the battle for the consumer is ultimately where the value accrues, Sonny, not who runs, you know, what hardware. And so Google's dominance, it's been the greatest business in the history of capitalism for 20 years because they owned the consumer. They owned the verb in something that was extraordinarily high margin.</p><p>And all I would say is the first real threat in 20 years came about in the ChatGPT moment. It's continued to accelerate. I think ChatGPT will cross a billion weeklies like maybe this year, you know, probably this year, I would guess. And so that to me has always been the case, right, for OpenAI.</p><p>And when you look at the rest of these frontier labs, the case you've made throughout this pod, about seven of these models in China, being able to open source, distill off one another, drive up intelligence, drive down costs. What that tells me is the model layers being increasingly commoditized and that there's not going to be a lot of intrinsic value in that intelligence layer, that operating layer.</p><p>You're going to have to build applications that guys like Bill Gurley and you and I are using every day. And that's where the battle is on the consumer side. You're going to have the exact same battle when it comes to coding agents and general enterprise applications. And I've said there, I think it's going to be more of a, you know, heterogeneous world.</p><p>I think there are going to be lots of players that compete. I think the margins will be lower in that world. But it may very well be that the tide is going up so much here, right? And the whole world is transforming so much around this that you're still going to have lots of players who do incredibly well.</p><p>I have to say I'm surprised if you would have told any of us a year ago or 18 months ago, right, that the combined enterprise value of OpenAI and Anthropic together would be over half a trillion dollars. And you throw X.AI in there, it'd be a trillion dollars, you know, across the three of them roughly.</p><p>It's bigger and faster. And the compute demand is higher than any of us, I think, anticipated. Sonny, hopefully we can keep you on here for a bit. We're just going to wrap up with a, you know, a topic that I think has dominated really the conversation in the markets over the course of last year.</p><p>And that's been about tariffs and kind of the reordering of global trade. Bill, I know you had some questions, some thoughts about it, and I'm happy to dig in and talk about it as well. Well, I mean, I would really just love to hear from you, Brad. The markets got very nervous when the, what was it, Liberation Day, when the kind of unpredictability of how big some of the numbers were and what that might mean and whether we were walking away from the notion of comparative advantage and, and I think the markets got spooked.</p><p>And, you know, I think you turn around and look at where the markets are today and, and we've really gone through an evolution of how the Wall Street is interpreting the, both the initial launch of the tariffs and the reality of where they're landing. So, how would you, how would you describe that and, and why do you think the markets are getting very comfortable with the, where, where they're landing?</p><p>I mean, not only comfortable, we're at all time highs, you know, and, and April 2nd, I was going, you know, on CNBC saying the nuclear Navarro was going to be a disaster and I'm out. Right. And that's this, this, the, the, the amazing thing about this administration is there's really, you know, it's a team of rivals within the White House.</p><p>And you had Bessett and Lutnick who were basically outlining this plan for, you know, let's call it 10 to 15 to 20% tariffs across the board. That would amount to about $300 billion in total tariffs up from 75 billion in 2024. But you had Navarro who was basically saying, we're going to replace the Internal Revenue Service.</p><p>We're going to get rid of the income tax and we're going to have 2 trillion of tariffs. Okay. And I was very clear. And I think the market was very clear. We all voted with our wallets. And we said, until the president tells us whether it's door one or door two, we're out.</p><p>Yep. The market shot first and asked questions later. And that's where you saw that huge drawdown in the market. The NASDAQ was down 21%, right? At its trough this year. Now the NASDAQ's up over 10. It's a 30% move in about 60 or 70 days, which is extraordinary, even by the historical patterns that we've seen over the course of the last five years.</p><p>But let me back up here for a second. I think the consensus view of all economists, right? 90% of economists is that tariffs are going to be bad. They're going to be a tax that gets paid by the US consumer. There was a small group led by, you know, Scott Besant and Kevin Hassett at the National Economic Council that said, no, it's actually going to be different this time.</p><p>And the reason it's going to be different this time, their theory argued, was that the world had become dependent upon exporting to the United States. So that the total trade deficit to the United States of goods and services was about 915 billion last year, a $1.2 trillion goods deficit.</p><p>And that basically meant that China was selling a lot more to the United States than they were buying of US goods. And so what Besant and Hassett postulated was that these countries have no choice. If we impose a tariff on them, so long as it's not draconian, 70%, 80%, what Navarro was talking about.</p><p>If we impose a 15% tariff on them, they have to eat it. The producers have to eat it because otherwise they're going to end up laying off millions of people in Vietnam and China and these countries. And politically, they can't afford to lay these folks off. So that was their theory of the case.</p><p>The consensus economists said, no way is that true. You're going to see massive inflation. But what have you seen? You have not seen the inflation percolate through. I will caveat this by saying yet. Okay. So here we are in July. The consensus economists said it would have already happened.</p><p>And the National Economic Council was out with a paper last week that deconstructed core PCE. So that's the best proxy the Fed watches for inflation since the start of the year. And it showed-- this is really interesting. Import prices have been going up at a slower rate than domestically produced goods.</p><p>Okay. So this is the exact opposite of what you would have expected from tariffs. Of course, you would have expected the imported prices would have been going up more than domestic goods. And so we'll show these charts and we'll put the link to this paper. People ought to take a look at that.</p><p>But to me, when I look at the president's-- the deals he's landing, the deal he just got announced yesterday with the European Union. 15% on all goods coming from Europe. 0% on US goods going to Europe. So an opening up of Europe markets, paying us 15%. And on top of that, getting commitments like $750 billion, almost $1 trillion of energy purchases from the US.</p><p>Or look at Japan, which they announced last week, another huge market. Again, similar. They're going to pay tariffs to the United States, no tariffs imposed on the United States. And they're going to invest $550 billion into the US in a way the president gets to direct. So I would love-- I would-- I just think we need to give the president credit where credit is due.</p><p>Everybody said this was going to lead to retaliation, to trade wars, was going to be disastrous for the US. And all we've seen so far is deals, deals, deals, deals. And I have to say, if this was the CEO of one of our companies, right? Let's say we had a board meeting at the start of the year, and he outlined these plans.</p><p>And we said, hey, we're really nervous about this. You know, this is a high-risk, high-reward strategy. It's either going to backfire, and we're going to fire you. Or it's going to work really well, and we give you a bonus. If we're measuring them halfway through the year, I would say that he's in line for a bonus based upon the trillions of dollars that are going to be coming into the United States.</p><p>And the fact that we now have a $300 to a $350 billion recurring stream of revenues into Treasury in the form of these tariffs which are being paid. And I think Besant said last week, in the month of June, we had our first surplus in the United States, monthly surplus, since 2015, right?</p><p>Because of the tariff revenues that are coming in. So I will say this. I was on the fence. I knew the nuclear tariffs, the trillion or two trillion. I knew that was a disaster. I said if we landed the plane where Besant wanted to come in at $300 billion, I thought there was a decent chance that those prices could be passed on in the home countries.</p><p>And so far, it looks like that's the case. Do you have any concerns? What's the-- anything you're watching out for? Well, I think the number one thing is core PCE did bottom last year, and it started to tick up. And so we have to keep our eye on inflation.</p><p>And of course, I think it's almost impossible to conceive that we would have totally reordered the entire global trading system on the first pass with zero mistakes. So we're going to have some goods and some products that consumers or US consumers are going to end up paying the taxes, and we're going to have to go back and fix some of these things.</p><p>But I will say that it's turning out massively better than consensus criticisms. I mean, remember Larry Summers at the CO2 event? I mean, this was just a few months ago, and he was saying this is the biggest economic disaster of his career. And I don't think you can describe it that way.</p><p>The markets are-- the voting machine is telling you-- and these are a lot of sophisticated investors. The voting machine is telling you that no retaliations, no trade wars, all these deals getting done works for the US economy. And I will tell you that the Atlanta now Fed tracker, which tracks real-time GDP, has now ticked up back to 3%.</p><p>So after going down a lot in April, it's bounced way back up. So economic activity appears to be going up. And then one final thing here. Remember, one of the key reasons for doing this, right, wasn't just because we were-- I mean, the EU conceded in the trade negotiations that our relationship was unfairly balanced in the direction of the EU.</p><p>I mean, they said that's the starting point. So we have to rebalance it. But on top of that, a key reason for doing this was to support the domestic production of critical national industries and to make our supply chains more resilient. Chips, data centers, energy production. At Altimeter, we just led the Series A in a company-- I don't even know if we've announced it, but I'll announce it here-- which is an all-American producer of rare earth magnets, right?</p><p>So these are now viable investments because of the tariffs and the resolve of the government to re-onshore these critical supply chains. So, I mean, that's a huge benefit that you would be willing to pay something for. But we're getting the benefit. And on top of that, we're getting paid.</p><p>Yeah. Can I add one thing, guys? Go for it. Not on the economic side, but running the supply chain at Brock? Yeah. Look, we're fortunate in that the majority of our supply chain is US-centric, including our chips. But we still have small, discrete components. And Brad, exactly what you were saying is happening is that the producers and manufacturers of those things are coming and you're negotiating.</p><p>And we've had pretty significant negotiation with those folks. And that was, I think, like you said, wasn't anticipated. The other thing is, it hasn't been static. The one thing-- again, I'll go back to the administration. They moved. Like, we've seen our tariff schedule move around probably six to eight times since this all has started.</p><p>Because they keep evaluating. They understand. They listen. And I think that's also a function of how this administration operates. And we want to give them credit for that. Like, people can go and share with them, hey, these things are not available. We can't replace them right away. And I think that that's also helping with that last thing you said with the investment you're making is companies get established onshore to take advantage of what these tariffs are causing.</p><p>Sonny, has your supply chain planning and that dynamic nature, has that started to settle down? Do you see this-- you know, are we reaching the end of the tariff negotiations such that everything can kind of settle down and operate? It's gone from week to week or even day to day when it first started and trying to figure out what happened to like now we're looking at it monthly.</p><p>So it's definitely settling. And Brad, we still have a big thing looming with a China discussion, correct? Yeah. You know, listen. So we landed Europe. We've landed Japan. We're going to have, you know, the long list is going to be coming out. But when you look at our big trading partners, the EU, we do, I think, $900 billion of trade with them a year.</p><p>With China, it's about $600 billion. But we have about a $300 billion goods trade deficit with both Europe and China. So they were the two big ones. China is the big enchilada because it's not just trade with China, right? It's strategic, it's national security, and it's trade. And it's the AI race.</p><p>We know that, you know, the rare earth ban on Chinese magnets was devastating to US industry. We know the retaliation that, you know, where H20s were cut off in terms of NVIDIA's chips back to China. So reading the tea leaves, I'll go out on a limb and I'll say the consensus still believes like that the China thing is going to be a problem or that it will be small.</p><p>I think this president wants to do the biggest deal ever done with China. I don't think he's dogmatic at all. I don't think he's some big China hawk. He said at the AI summit last week, I'm a deal junkie. I like to do deals. You can't be the biggest deal maker in the world without doing a big deal with China.</p><p>All right. Right. And I, you know, if you just look at today, the Chinese reciprocated in a way I think the US government was looking for. They said, hey, we'll postpone all of our retaliatory tariffs. They've invited the president to China. The president has suggested he's going to go to China the first week of September, sometime between September and November.</p><p>I think there is a very, very big deal that's going to get done with China that's going to reorient the relationship in a big way. And let me just like tease this. The president said earlier this year something that caught all of our attention. He said, you know, if I could wave a magic wand, I would cut the defense budget in half for the United States, for China and for Russia.</p><p>We've never heard a US president in history utter anything like that. That'd be amazing. That is what I would call an extraordinarily flexible mindset. And if you go into this deal negotiation with China with that sort of flexible mindset, I think it could include all of the above. Rare earths, chips, maybe even military cooperation, certainly a rebalancing of trade.</p><p>I think China's-- listen, we entered the year with China paying 15% in tariffs. That was pre-Trump. They were paying 15% to the United States in tariffs. So I don't think we're going below 15%. I think they're going to pay-- And Trump did that in Trump 45. In Trump 1.</p><p>In Trump 1, right. So I think that they will continue to pay at least 15%, but I think it's going to be much more structured, much more nuanced. You know, Besson's been very clear. China has to rebalance to domestic consumption and away from an export economy that's really sticking it to the US in terms of the trade deficit.</p><p>I think China gets that. I think they want that for their own country. I think they're willing to do that. I think the United States understands that can't happen overnight. It has to happen over a period of years. I think that there's going to be a big Chinese deal done before the year's out.</p><p>So let's close with this, Brad. You've often on the pod been willing to speak about your own temperature for the US markets and whether you're net long or net short. And you've, on this podcast, been more enthusiastic than I've ever seen you, both about AI, but this China theory that you have, I think, would cause the markets to rip, if you're correct.</p><p>But you also said we're at all-time highs, you know, and so you want to buy low and sell high. So where's your head hanging? We did a pod, I think, around May 2nd. Well, we did the pod in March, and I said, we're out of the market. Right. I remember.</p><p>Right. And you said you're early, and Liberation Day came, and we were happy to be out of the market. On May 2nd, I said, we're all back in, because the Besant consensus has won. It's going to be $300 billion. We're going to land the plane. And I outlined a flight path.</p><p>I said, you can land the plane, no inflation, you get rate cuts, and it's kind of off to the races. And we're up 30% off of that bottom in the NASDAQ since then. So 30% is a huge move. Huge. But when you telescope out, we're up 10% for the year, okay?</p><p>And if I had told you guys on day one of this year, here's what's going to happen. We're going to rebalance global trade, and we're going to land the plane around $300 billion. We're going to have the economy grow at 3% accelerating. We're going to have no inflation heading toward rate cuts by the end of the year.</p><p>That is the backdrop. And we're going to have all of this AI demand and accelerating demand for AI compute. I would have said the market can be up at least 15% for the year. I think we've captured a lot of the return for the year, Bill. I will tell you this, though.</p><p>We see tons of opportunities. And so I would say that we're also bullish on what we see happening in AI. Sonny's going to raise a huge new round here. We're happy to be investors with Sonny as well. And it's extraordinary to watch what Sonny's- No, it's serious. If he wasn't supposed to disclose that, he can edit it out.</p><p>He hasn't, but his face is turning all red. Have a good week. Thank you, Sonny, for coming on with us today. Hope to see you both, too. Thank you so much. Thanks for having me. Yeah. Thanks, guys. Bye-bye. Take care, guys. As a reminder to everybody, just our opinions, not investment advice.</p></div></div></body></html>