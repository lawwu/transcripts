<html><head><title>Storyteller: Building Multi-modal Apps with TS & ModelFusion - Lars Grammel, PhD</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Storyteller: Building Multi-modal Apps with TS & ModelFusion - Lars Grammel, PhD</h2><a href="https://www.youtube.com/watch?v=KnndhPSVF3E"><img src="https://i.ytimg.com/vi/KnndhPSVF3E/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./KnndhPSVF3E.html">Whisper Transcript</a> | <a href="./transcript_KnndhPSVF3E.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hey everyone, I'm presenting Storyteller, an app for generating short audio stories for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=18" target="_blank">00:00:18.200</a></span> | <span class="t">preschool kids. Storyteller is implemented using TypeScript and Model Fusion, an AI orchestration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=25" target="_blank">00:00:25.000</a></span> | <span class="t">library that I've been developing. It generates audio stories that are about two minutes long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=31" target="_blank">00:00:31.080</a></span> | <span class="t">and all it needs is a voice input. Here is an example of the kind of story it generates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=36" target="_blank">00:00:36.580</a></span> | <span class="t">to give you an idea. One day while they were playing, Benny noticed something strange. The</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=42" target="_blank">00:00:42.540</a></span> | <span class="t">forest wasn't as vibrant as before. The leaves were turning brown and the animals seemed less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=47" target="_blank">00:00:47.740</a></span> | <span class="t">cheerful. Worried, Benny asked his friends what was wrong. Friends, why do the trees look so sad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=53" target="_blank">00:00:53.100</a></span> | <span class="t">and why are you all so quiet today? Benny, the forest is in trouble. The trees are dying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=57" target="_blank">00:00:57.940</a></span> | <span class="t">and we don't know what to do. How does this work? Let's dive into the details of the Storyteller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=63" target="_blank">00:01:03.180</a></span> | <span class="t">application. Storyteller is a client server application. The client is written using React</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=69" target="_blank">00:01:09.640</a></span> | <span class="t">and the server is a custom Fastify implementation. The main challenges were responsiveness, meaning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=75" target="_blank">00:01:15.580</a></span> | <span class="t">getting results to the user as quickly as possible, quality and consistency. So when you start Storyteller,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=85" target="_blank">00:01:25.660</a></span> | <span class="t">it's just a small screen that has a record topic button. And once you start pressing it, it starts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=91" target="_blank">00:01:31.700</a></span> | <span class="t">recording. The audio when you release gets sent to the server as a buffer and there we transcribe it. For</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=99" target="_blank">00:01:39.740</a></span> | <span class="t">transcription, I'm using OpenAI Whisper. It is really quick for a short topic, 1.5 seconds. And once it becomes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=107" target="_blank">00:01:47.900</a></span> | <span class="t">available, an event goes back to the client. So the client server communication works through an event</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=115" target="_blank">00:01:55.460</a></span> | <span class="t">stream, server sent events, that are being sent back. The event arrives on the client and the React state updates,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=126" target="_blank">00:02:06.140</a></span> | <span class="t">it's updating the screen. Okay, so then the user knows something is going on. In parallel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=131" target="_blank">00:02:11.340</a></span> | <span class="t">I start generating the Story Outline. For this, I use GPT-3 Turbo Instruct, which I found to be very fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=139" target="_blank">00:02:19.020</a></span> | <span class="t">So it can generate a Story Outline in about 4 seconds. And once we have that, we can start a bunch of other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=146" target="_blank">00:02:26.060</a></span> | <span class="t">tasks in parallel. Generating the title, generating the image, and generating and narrating the audio story</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=155" target="_blank">00:02:35.420</a></span> | <span class="t">it will all happen in parallel. I'll go through those one by one now. First, the title is generated. For</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=163" target="_blank">00:02:43.900</a></span> | <span class="t">this, OpenAI GPT-3 Turbo Instruct is used again, giving a really quick result. Once the title is available,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=171" target="_blank">00:02:51.500</a></span> | <span class="t">it's being sent to the client again as an event and rendered there. In parallel, the image generation runs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=180" target="_blank">00:03:00.140</a></span> | <span class="t">First, there needs to be a prompt to actually generate the image. And here, consistency is important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=186" target="_blank">00:03:06.780</a></span> | <span class="t">So we pass in the whole story into a GPT-4 prompt that then extracts relevant representative keywords</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=193" target="_blank">00:03:13.980</a></span> | <span class="t">for an image from the story. That image prompt is passed into Stability AI Stable Diffusion XL where an image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=204" target="_blank">00:03:24.460</a></span> | <span class="t">is generated. The generated image is stored as a virtual file in the server. And then an event is sent to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=213" target="_blank">00:03:33.900</a></span> | <span class="t">client with a path to the client with a path to the file. The client can then, through a regular URL request,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=220" target="_blank">00:03:40.220</a></span> | <span class="t">just retrieve the image as part of an image tag. And it shows up in the UI. Generating the full audio story is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=231" target="_blank">00:03:51.020</a></span> | <span class="t">the most time-consuming piece of the puzzle. Here, we have a complex prompt that takes in the story and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=238" target="_blank">00:03:58.540</a></span> | <span class="t">creates a structure with dialogue and speakers and extends the story. We use GPT-4 here with a low</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=247" target="_blank">00:04:07.500</a></span> | <span class="t">temperature to retain the story. And the problem is it takes one and a half minutes, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=252" target="_blank">00:04:12.940</a></span> | <span class="t">unacceptably long for an interactive client. So how can this be solved? The key idea is streaming the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=262" target="_blank">00:04:22.300</a></span> | <span class="t">structure. That's a little bit more difficult than just streaming characters token by token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=267" target="_blank">00:04:27.340</a></span> | <span class="t">We need to always partially pass the structure and then determine if there is a new passage that we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=275" target="_blank">00:04:35.580</a></span> | <span class="t">actually narrate and synthesize speech for. Model Fusion takes care of the partial parsing and returns an iterable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=284" target="_blank">00:04:44.940</a></span> | <span class="t">over fragments of partially parsed results, but the application needs to decide what to do with them. Here, we determine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=292" target="_blank">00:04:52.540</a></span> | <span class="t">which story part is finished so we can actually narrate it. So we narrate each story part as it's getting finished.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=302" target="_blank">00:05:02.940</a></span> | <span class="t">For each story part, we need to determine which voice we use to narrate it. The narrator has a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=310" target="_blank">00:05:10.220</a></span> | <span class="t">predefined voice and for all the speakers where we already have voices, we can immediately proceed. However,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=317" target="_blank">00:05:17.020</a></span> | <span class="t">when there's a new speaker, we need to figure out which voice to give it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=320" target="_blank">00:05:20.940</a></span> | <span class="t">The first step for this is to generate a voice description for the speaker. Here's a GPT-3-5 turbo prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=331" target="_blank">00:05:31.100</a></span> | <span class="t">that gives us a structured result with gender and a voice description and we then use that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=335" target="_blank">00:05:35.980</a></span> | <span class="t">for retrieval where we beforehand embedded all the voices based on their descriptions and now can just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=343" target="_blank">00:05:43.020</a></span> | <span class="t">retrieve them filtered by gender. Then a voice is selected making sure there are no speakers with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=349" target="_blank">00:05:49.580</a></span> | <span class="t">same voice and finally we can generate the audio. Here for the speech synthesis element and 11labs are supported</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=359" target="_blank">00:05:59.420</a></span> | <span class="t">based on the voices that have been chosen. One of those providers is picked and the audio synthesized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=364" target="_blank">00:06:04.860</a></span> | <span class="t">Similar to the images, we generate an audio file and we store it virtually in the server and then send</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=373" target="_blank">00:06:13.500</a></span> | <span class="t">the path to the client which reconstructs the URL and just retrieves it as a media element. Once the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=381" target="_blank">00:06:21.740</a></span> | <span class="t">audio is completed, the client can start playing. While this is ongoing, in the background you're listening and in the background the server continues to generate more and more parts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=395" target="_blank">00:06:35.820</a></span> | <span class="t">And that's it. So let's recap how the main challenge of responsiveness is addressed here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=401" target="_blank">00:06:41.260</a></span> | <span class="t">We have a loading state that has multiple parts that are updated as more results become available.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=408" target="_blank">00:06:48.300</a></span> | <span class="t">We use streaming and parallel processing in the backend to make results available as quickly as possible and you can start listening while the processing is still going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=418" target="_blank">00:06:58.460</a></span> | <span class="t">And finally, models are being chosen such that the processing time for like the generation, say the story, is minimized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=427" target="_blank">00:07:07.660</a></span> | <span class="t">So cool. I hope you enjoyed my talk. Thank you for listening. And if you want to find out more, you can find Storyteller and also Model Fusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=KnndhPSVF3E&t=435" target="_blank">00:07:15.020</a></span> | <span class="t">on GitHub at GitHub.com L Grammar Storyteller and GitHub.com L Grammar Model Fusion.</span></div></div></body></html>