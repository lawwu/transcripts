<html><head><title>Stanford CS224N | 2023 | Lecture 10 - Prompting, Reinforcement Learning from Human Feedback</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS224N | 2023 | Lecture 10 - Prompting, Reinforcement Learning from Human Feedback</h2><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4"><img src="https://i.ytimg.com/vi/SXpJ9EmG3s4/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./SXpJ9EmG3s4.html">Whisper Transcript</a> | <a href="./transcript_SXpJ9EmG3s4.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Okay, awesome. We're going to get started. So my name is Jesse Mu. I'm a PhD student</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=13" target="_blank">00:00:13.400</a></span> | <span class="t">in the CS department here working with the NLP group and really excited to be talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=18" target="_blank">00:00:18.760</a></span> | <span class="t">about the topic of today's lecture, which is on prompting instruction, fine tuning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=23" target="_blank">00:00:23.480</a></span> | <span class="t">and RLHF. So this is all stuff that has been super hot recently because of all the latest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=30" target="_blank">00:00:30.480</a></span> | <span class="t">criticise about chatbots, chat-dbt, etc. And we're going to hopefully get somewhat of an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=36" target="_blank">00:00:36.480</a></span> | <span class="t">understanding as to how these systems are trained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=40" target="_blank">00:00:40.360</a></span> | <span class="t">Okay, so before that, some course logistics things. So project proposals, both custom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=45" target="_blank">00:00:45.760</a></span> | <span class="t">and final, were due a few minutes ago. So if you haven't done that, this is a nice reminder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=52" target="_blank">00:00:52.560</a></span> | <span class="t">We're in the process of assigning mentors of projects, so we'll give feedback soon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=57" target="_blank">00:00:57.840</a></span> | <span class="t">Besides that, assignment five is due on Friday at midnight. We still recommend using Colab</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=63" target="_blank">00:01:03.320</a></span> | <span class="t">for the assignments, even if you've had AWS or Azure credits granted. If that doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=67" target="_blank">00:01:07.960</a></span> | <span class="t">work, there's instructions for how to connect to a Kaggle notebook where you will also be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=71" target="_blank">00:01:11.200</a></span> | <span class="t">able to use GPUs. Look for that post on ed. And then finally, also just posted on ed by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=76" target="_blank">00:01:16.760</a></span> | <span class="t">John is a course feedback survey. So this is part of your participation grade. Please</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=81" target="_blank">00:01:21.440</a></span> | <span class="t">fill that in by Sunday, 11.59 p.m.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=88" target="_blank">00:01:28.440</a></span> | <span class="t">Okay, so let's get into this lecture, which is going to be about what we are trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=94" target="_blank">00:01:34.920</a></span> | <span class="t">do with these larger and larger models. Over the years, the compute for these models have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=100" target="_blank">00:01:40.680</a></span> | <span class="t">just gone up hundreds of powers of 10, trained on more and more data. So larger and larger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=109" target="_blank">00:01:49.680</a></span> | <span class="t">models, they're seeing more and more data. And in lecture 10, if you recall this slide,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=116" target="_blank">00:01:56.520</a></span> | <span class="t">we talked a little bit about what happens when you do pre-training. And as you begin</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=121" target="_blank">00:02:01.680</a></span> | <span class="t">to really learn to predict the missing sentence in certain texts, right? You learn things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=126" target="_blank">00:02:06.240</a></span> | <span class="t">like syntax, co-reference, sentiment, et cetera. But in this lecture, we're going to take it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=131" target="_blank">00:02:11.840</a></span> | <span class="t">a little bit further and really take this idea to its logical conclusion. So if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=135" target="_blank">00:02:15.680</a></span> | <span class="t">really follow this idea of we're just going to train a giant language model on all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=140" target="_blank">00:02:20.400</a></span> | <span class="t">the world's text, you really begin to see language models sort of in a way as rudimentary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=145" target="_blank">00:02:25.680</a></span> | <span class="t">world models. So maybe they're not very good at world models, but they kind of have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=149" target="_blank">00:02:29.480</a></span> | <span class="t">be doing some implicit world modeling just because we have so much information on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=153" target="_blank">00:02:33.600</a></span> | <span class="t">internet and so much of human collective knowledge is transcribed and written for us on the internet,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=158" target="_blank">00:02:38.680</a></span> | <span class="t">right? So if you are really good at predicting the next word in text, what do you learn to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=162" target="_blank">00:02:42.720</a></span> | <span class="t">do? There's evidence that these large language models are to some degree learning to represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=167" target="_blank">00:02:47.960</a></span> | <span class="t">and think about agents and humans and the beliefs and actions that they might take.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=172" target="_blank">00:02:52.600</a></span> | <span class="t">So here's an example from our recent paper where we are talking about someone named Pat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=177" target="_blank">00:02:57.800</a></span> | <span class="t">watching a demonstration of a bowling ball and a leaf being dropped at the same time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=181" target="_blank">00:03:01.520</a></span> | <span class="t">in a vacuum chamber. And the idea is here we're saying Pat is a physicist, right? So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=187" target="_blank">00:03:07.320</a></span> | <span class="t">Pat is a physicist and we ask for the language models next continuation of this sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=193" target="_blank">00:03:13.520</a></span> | <span class="t">Because he's a physicist, we do some inference about what kind of knowledge Pat has and Pat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=197" target="_blank">00:03:17.560</a></span> | <span class="t">will predict that the bowling ball and the leaf will fall at the same time. But if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=201" target="_blank">00:03:21.880</a></span> | <span class="t">change the sentence of the prompt and we say, well, Pat has actually never seen this demonstration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=205" target="_blank">00:03:25.400</a></span> | <span class="t">before, then Pat will predict that the bowling ball will fall to the ground first, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=209" target="_blank">00:03:29.600</a></span> | <span class="t">is wrong, right? So if you get really good at predicting the next sentence in text, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=213" target="_blank">00:03:33.880</a></span> | <span class="t">also to some degree have to learn to predict an agent's beliefs, their backgrounds, common</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=219" target="_blank">00:03:39.080</a></span> | <span class="t">knowledge and what they might do next. So not just that, of course, if we continue browsing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=224" target="_blank">00:03:44.920</a></span> | <span class="t">the internet, we see a lot of encyclopedic knowledge. So maybe language models are actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=228" target="_blank">00:03:48.960</a></span> | <span class="t">good at solving math reasoning problems if they've seen enough demonstrations of math</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=233" target="_blank">00:03:53.080</a></span> | <span class="t">on the internet. Code, of course, code generation is a really exciting topic that people will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=238" target="_blank">00:03:58.560</a></span> | <span class="t">that people are looking into and we'll give a presentation on that in a few weeks. Even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=244" target="_blank">00:04:04.120</a></span> | <span class="t">medicine, right? We're beginning to think about language models trained on medical texts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=247" target="_blank">00:04:07.680</a></span> | <span class="t">and being applied to the sciences and whatnot. So this is what happens when we really take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=251" target="_blank">00:04:11.920</a></span> | <span class="t">this language modeling idea seriously. And this has resulted in a resurgence of interest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=257" target="_blank">00:04:17.760</a></span> | <span class="t">in building language models that are basically assistants, right? You can give them any task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=262" target="_blank">00:04:22.960</a></span> | <span class="t">under the sun. I want to create a three course meal and a language model should be able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=267" target="_blank">00:04:27.960</a></span> | <span class="t">take a good stab at being able to do this. This is kind of the promise of language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=274" target="_blank">00:04:34.320</a></span> | <span class="t">But of course, there's a lot of steps required to get from this, from our basic language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=278" target="_blank">00:04:38.720</a></span> | <span class="t">modeling objective. And that's what this lecture is going to be about. So how do we get from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=284" target="_blank">00:04:44.040</a></span> | <span class="t">just predicting the next word in a sentence to something like chat GPT, which you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=288" target="_blank">00:04:48.600</a></span> | <span class="t">really ask it to do anything and it might fail sometimes, but it's getting really, really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=292" target="_blank">00:04:52.400</a></span> | <span class="t">convincingly good at some things. Okay. So this is the lecture plan. Basically, I'm going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=298" target="_blank">00:04:58.760</a></span> | <span class="t">to talk about as we're working with these large language models, we come up with kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=302" target="_blank">00:05:02.360</a></span> | <span class="t">of increasingly complex ways of steering the language models closer and closer to something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=306" target="_blank">00:05:06.600</a></span> | <span class="t">like chat GPT. So we'll start with zero shot and few shot learning, then instruction, fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=311" target="_blank">00:05:11.120</a></span> | <span class="t">tuning and then reinforcing learning from human feedback or RLHF. Okay. So let's first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=321" target="_blank">00:05:21.000</a></span> | <span class="t">talk about few shot and zero shot learning. And in order to do so, we're again going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=326" target="_blank">00:05:26.480</a></span> | <span class="t">kind of build off of the pre-training lecture last Tuesday. So in the pre-training lecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=330" target="_blank">00:05:30.800</a></span> | <span class="t">John talked about these models like GPT, generative pre-trained transformer, that are these decoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=338" target="_blank">00:05:38.200</a></span> | <span class="t">only language models. So they're just trained to predict the next word in a corpus of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=343" target="_blank">00:05:43.600</a></span> | <span class="t">And back in 2018 was the first iteration of this model. And it was 117 million parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=350" target="_blank">00:05:50.000</a></span> | <span class="t">So at the time it was pretty big. Nowadays, it's definitely much smaller. And again, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=354" target="_blank">00:05:54.400</a></span> | <span class="t">just a vanilla transformer decoder using the techniques that you've seen. And it's trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=358" target="_blank">00:05:58.280</a></span> | <span class="t">on a corpus of books. So about 4.6 gigabytes of text. And what GPT showed was the promise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=364" target="_blank">00:06:04.840</a></span> | <span class="t">at doing this simple language modeling objective and serving as an effective pre-training technique</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=370" target="_blank">00:06:10.200</a></span> | <span class="t">for various downstream tasks that you might care about. So if you wanted to apply it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=373" target="_blank">00:06:13.920</a></span> | <span class="t">something like natural language inference, you would take your premise sentence and your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=377" target="_blank">00:06:17.720</a></span> | <span class="t">hypothesis sentence, concatenate them, and then maybe train a linear classifier on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=382" target="_blank">00:06:22.080</a></span> | <span class="t">last representation the model produces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=384" target="_blank">00:06:24.960</a></span> | <span class="t">OK, but that was three, four, five years ago. What has changed since then? So they came</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=394" target="_blank">00:06:34.520</a></span> | <span class="t">out with GPT-2. So GPT-2 was released the next year in 2019. This is 1.5 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=401" target="_blank">00:06:41.720</a></span> | <span class="t">So it's the same architecture as GPT, but just an order of magnitude bigger. And also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=406" target="_blank">00:06:46.620</a></span> | <span class="t">trained on much more data. So we went from 4 gigabytes of books to 40 gigabytes of internet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=413" target="_blank">00:06:53.040</a></span> | <span class="t">text data. So they produced a data set called WebText. This is produced by scraping a bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=417" target="_blank">00:06:57.560</a></span> | <span class="t">of links to comments on Reddit. So the idea is that the web contains a lot of spam, maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=421" target="_blank">00:07:01.920</a></span> | <span class="t">a lot of low-quality information. But they took links that were posted on Reddit that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=425" target="_blank">00:07:05.680</a></span> | <span class="t">had at least a few upvotes. So humans maybe looked through it and said, you know, this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=429" target="_blank">00:07:09.240</a></span> | <span class="t">is a useful post. So that was kind of a rough proxy of human quality. And that's how they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=434" target="_blank">00:07:14.120</a></span> | <span class="t">collected this large data set. And so if you look at the size of GPT in 2018, we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=440" target="_blank">00:07:20.920</a></span> | <span class="t">draw a bigger dot, which is the size of GPT-2 in 2019. And one might ask, how much better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=447" target="_blank">00:07:27.120</a></span> | <span class="t">does this do? What is this value?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=450" target="_blank">00:07:30.500</a></span> | <span class="t">So the authors of GPT-2 titled their paper, "Language Models are Unsupervised Multitask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=455" target="_blank">00:07:35.600</a></span> | <span class="t">Learners." And that kind of gives you a hint as to what the key takeaway they found was,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=460" target="_blank">00:07:40.240</a></span> | <span class="t">which is this unsupervised multitasking part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=464" target="_blank">00:07:44.640</a></span> | <span class="t">So basically, I think the key takeaway from GPT-2 was this idea that language models can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=469" target="_blank">00:07:49.920</a></span> | <span class="t">display zero-shot learning. So what I mean by zero-shot learning is you can do many tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=475" target="_blank">00:07:55.720</a></span> | <span class="t">that the model may not have actually explicitly been trained for with no gradient updates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=480" target="_blank">00:08:00.360</a></span> | <span class="t">So you just kind of query the model by simply specifying the right sequence prediction problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=486" target="_blank">00:08:06.340</a></span> | <span class="t">So if you care about question answering, for example, you might include your passage, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=490" target="_blank">00:08:10.000</a></span> | <span class="t">a Wikipedia article about Tom Brady. And then you'll add a question, so a question, where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=494" target="_blank">00:08:14.000</a></span> | <span class="t">was Tom Brady born? And then include an answer, like A followed by a colon. And then just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=498" target="_blank">00:08:18.720</a></span> | <span class="t">ask the model to predict the next token. You've kind of jury-rigged the model into doing question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=503" target="_blank">00:08:23.760</a></span> | <span class="t">answering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=506" target="_blank">00:08:26.720</a></span> | <span class="t">For other tasks, like classification tasks, another thing you can do is compare different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=510" target="_blank">00:08:30.440</a></span> | <span class="t">probabilities of sequences. So this task is called the Winograd Schema Challenge. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=515" target="_blank">00:08:35.080</a></span> | <span class="t">a pronoun resolution task. So the task is to kind of resolve a pronoun which requires</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=519" target="_blank">00:08:39.620</a></span> | <span class="t">some world knowledge. So one example is something like, the cat couldn't fit into the hat because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=524" target="_blank">00:08:44.720</a></span> | <span class="t">it was too big. And the question is whether it refers to the cat or to the hat. And in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=530" target="_blank">00:08:50.480</a></span> | <span class="t">this case, it makes most sense for it to refer to the cat because things fitting into things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=535" target="_blank">00:08:55.400</a></span> | <span class="t">because they're too big, you need to use some world knowledge to kind of resolve that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=540" target="_blank">00:09:00.060</a></span> | <span class="t">So the way that you get zero-shot predictions for this task out of a language model like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=543" target="_blank">00:09:03.560</a></span> | <span class="t">GPT-2 is you just ask the language model, which sequence is more likely? Is the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=550" target="_blank">00:09:10.080</a></span> | <span class="t">of the cat couldn't fit into the hat because the cat was too big deemed more likely by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=555" target="_blank">00:09:15.080</a></span> | <span class="t">the language model than the probability that the cat couldn't fit into the hat because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=559" target="_blank">00:09:19.760</a></span> | <span class="t">the hat was too big? You can score those sequences because this is a language model. And from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=564" target="_blank">00:09:24.120</a></span> | <span class="t">there, you get your zero-shot prediction. And you can end up doing fairly well on this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=567" target="_blank">00:09:27.960</a></span> | <span class="t">task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=568" target="_blank">00:09:28.960</a></span> | <span class="t">Any questions about this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=571" target="_blank">00:09:31.960</a></span> | <span class="t">OK. Yeah, so digging a little bit more into the results, GPT-2 at the time beat the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=579" target="_blank">00:09:39.480</a></span> | <span class="t">of the art on a bunch of language modeling benchmarks with no task-specific fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=584" target="_blank">00:09:44.000</a></span> | <span class="t">So no traditional fine-tune on a training set and then test on a testing set. So here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=588" target="_blank">00:09:48.800</a></span> | <span class="t">an example of such a task. This is a language modeling task called Lambada, where the goal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=592" target="_blank">00:09:52.480</a></span> | <span class="t">is to predict a missing word. And the idea is that the word that you need to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=596" target="_blank">00:09:56.600</a></span> | <span class="t">depends on some discourse earlier in the sentence or earlier a few sentences ago. And by simply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=603" target="_blank">00:10:03.560</a></span> | <span class="t">training your language model and then running it on the Lambada task, you end up doing better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=607" target="_blank">00:10:07.880</a></span> | <span class="t">than the supervised fine-tuned state of the art at the time and across a wide variety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=613" target="_blank">00:10:13.120</a></span> | <span class="t">of other tasks as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=618" target="_blank">00:10:18.640</a></span> | <span class="t">OK. Another kind of interesting behavior they observed-- and so you'll see hints of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=627" target="_blank">00:10:27.240</a></span> | <span class="t">that we now take for granted in this paper-- is that you can get interesting zero-shot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=631" target="_blank">00:10:31.200</a></span> | <span class="t">behavior as long as you take some liberties with how you specify the task. So for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=636" target="_blank">00:10:36.360</a></span> | <span class="t">let's imagine that we want our model to do summarization. Even though GPT-2 was just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=640" target="_blank">00:10:40.880</a></span> | <span class="t">a language model, how can we make it do summarization?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=644" target="_blank">00:10:44.880</a></span> | <span class="t">The idea they explored was we're going to take an article, some news article, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=648" target="_blank">00:10:48.960</a></span> | <span class="t">at the end, we're going to append the TLDR sign, the TLDR token. So this stands for Too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=653" target="_blank">00:10:53.920</a></span> | <span class="t">Long Didn't Read. It's used a lot on Reddit to just say, if you didn't want to read the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=657" target="_blank">00:10:57.460</a></span> | <span class="t">above stuff, here's a few sentences that summarizes it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=660" target="_blank">00:11:00.920</a></span> | <span class="t">So if you ask the model to predict what follows after the TLDR token, you might expect that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=666" target="_blank">00:11:06.600</a></span> | <span class="t">it'll generate some sort of summary. And this is kind of early whispers at this term that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=672" target="_blank">00:11:12.040</a></span> | <span class="t">we now call prompting, which is thinking of the right way to define a task such that your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=677" target="_blank">00:11:17.160</a></span> | <span class="t">model will do the behavior that you want it to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=682" target="_blank">00:11:22.240</a></span> | <span class="t">So if we look at the performance we actually observed on this task, here at the bottom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=685" target="_blank">00:11:25.960</a></span> | <span class="t">is a random baseline. So you just select three sentences from the article. And the scores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=691" target="_blank">00:11:31.040</a></span> | <span class="t">that we're using here are Rouge scores, if you remember the natural language generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=694" target="_blank">00:11:34.040</a></span> | <span class="t">lecture. GPT-2 is right above. So it's not actually that good. It only does maybe a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=700" target="_blank">00:11:40.040</a></span> | <span class="t">bit or barely any better than the random baseline. But it is approaching approaches that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=706" target="_blank">00:11:46.160</a></span> | <span class="t">supervised approaches that are actually explicitly fine-tuned to do summarization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=712" target="_blank">00:11:52.120</a></span> | <span class="t">And of course, at the time, it still underperformed the state of the art. But this really showed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=716" target="_blank">00:11:56.180</a></span> | <span class="t">the promise of getting language models to do things that maybe they weren't trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=719" target="_blank">00:11:59.400</a></span> | <span class="t">to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=721" target="_blank">00:12:01.520</a></span> | <span class="t">OK, so that was GPT-2. That was 2019. Now here's 2020, GPT-3. So GPT-3 is 175 billion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=733" target="_blank">00:12:13.000</a></span> | <span class="t">parameters. So it's another increase in size by an order of magnitude. And at the time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=737" target="_blank">00:12:17.280</a></span> | <span class="t">it was unprecedented. I think it still is kind of overwhelmingly large for most people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=742" target="_blank">00:12:22.320</a></span> | <span class="t">And data. So they scaled up the data once again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=744" target="_blank">00:12:24.720</a></span> | <span class="t">OK, so what is this by you? This paper's title was called Language Models are Few Shot Learners.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=750" target="_blank">00:12:30.960</a></span> | <span class="t">So what does that mean? So the key takeaway from GPT-3 was emergent few-shot learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=757" target="_blank">00:12:37.480</a></span> | <span class="t">So the idea here is, sure, GPT can still do zero-shot learning. But now you can specify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=763" target="_blank">00:12:43.200</a></span> | <span class="t">a task by basically giving examples of the task before asking it to predict the example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=768" target="_blank">00:12:48.260</a></span> | <span class="t">that you care about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=771" target="_blank">00:12:51.000</a></span> | <span class="t">So this is often called in-context learning to stress that there are no gradient updates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=775" target="_blank">00:12:55.540</a></span> | <span class="t">being performed when you learn a new task. You're basically kind of constructing a tiny</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=779" target="_blank">00:12:59.580</a></span> | <span class="t">little training data set and just including it in the prompt, including it in the context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=783" target="_blank">00:13:03.520</a></span> | <span class="t">window of your transformer, and then asking it to pick up on what the task is and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=787" target="_blank">00:13:07.500</a></span> | <span class="t">predict the right answer. And this is in contrast to a separate literature on few-shot learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=793" target="_blank">00:13:13.600</a></span> | <span class="t">which assumes that you can do gradient updates. In this case, it's really just a frozen language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=797" target="_blank">00:13:17.380</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=801" target="_blank">00:13:21.340</a></span> | <span class="t">So few-shot learning works, and it's really impressive. So here's a graph. SuperGLUE here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=806" target="_blank">00:13:26.560</a></span> | <span class="t">is a kind of a wide coverage natural language understanding benchmark. And what they did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=810" target="_blank">00:13:30.360</a></span> | <span class="t">was they took GPT-3, and this data point here is what you get when you just do zero-shot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=816" target="_blank">00:13:36.480</a></span> | <span class="t">learning with GPT-3. So you provide an English description of a task to be completed, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=820" target="_blank">00:13:40.800</a></span> | <span class="t">then you ask it to complete the task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=824" target="_blank">00:13:44.960</a></span> | <span class="t">Just by providing one example, so one shot, you get like a 10% accuracy increase. So you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=829" target="_blank">00:13:49.840</a></span> | <span class="t">give not only the natural language task description, but also an example input and an example output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=835" target="_blank">00:13:55.520</a></span> | <span class="t">and you ask it to code the next output. And as you increase to more shots, you do get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=841" target="_blank">00:14:01.000</a></span> | <span class="t">better and better scores, although, of course, you get diminishing returns after a while.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=846" target="_blank">00:14:06.480</a></span> | <span class="t">But what you can notice is that few-shot GPT-3, so no gradient updates, is doing as well as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=852" target="_blank">00:14:12.000</a></span> | <span class="t">or outperforming BERT fine-tuned on the SuperGLUE task explicitly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=859" target="_blank">00:14:19.040</a></span> | <span class="t">Any questions about this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=863" target="_blank">00:14:23.680</a></span> | <span class="t">So one thing that I think is really exciting is that you might think, OK, a few-shot learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=868" target="_blank">00:14:28.840</a></span> | <span class="t">whatever, it's just memorizing. Maybe there's a lot of examples of needing to do a few-shot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=872" target="_blank">00:14:32.200</a></span> | <span class="t">learning in the internet text data. And that's true, but I think there's also evidence that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=877" target="_blank">00:14:37.320</a></span> | <span class="t">GPT-3 is really learning to do some sort of on-the-fly optimization or reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=883" target="_blank">00:14:43.560</a></span> | <span class="t">And so the evidence for this comes in the form of these synthetic word unscrambling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=886" target="_blank">00:14:46.840</a></span> | <span class="t">tasks. So the authors came up with a bunch of simple kind of letter manipulation tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=891" target="_blank">00:14:51.480</a></span> | <span class="t">that are probably unlikely to exist in internet text data. So these include things like cycling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=896" target="_blank">00:14:56.880</a></span> | <span class="t">through the letters to get the kind of uncycled version of a word, so converting from P-L-E-A-P</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=902" target="_blank">00:15:02.160</a></span> | <span class="t">to Apple, removing characters added to a word, or even just reversing words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=907" target="_blank">00:15:07.800</a></span> | <span class="t">And what you see here is performance as you do few-shot learning as you increase the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=912" target="_blank">00:15:12.240</a></span> | <span class="t">size. And what you can see is that the ability to do few-shot learning is kind of an emergent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=919" target="_blank">00:15:19.160</a></span> | <span class="t">property of model scale. So at the very largest model, we're actually seeing a model be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=924" target="_blank">00:15:24.080</a></span> | <span class="t">to do this exclusively in context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=926" target="_blank">00:15:26.160</a></span> | <span class="t">Question? Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=927" target="_blank">00:15:27.160</a></span> | <span class="t">I've noticed the reversed words are horrible, like the performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=936" target="_blank">00:15:36.160</a></span> | <span class="t">Yeah. Yeah. So the question was the reversed words. Mine is still low. Yeah, that's an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=942" target="_blank">00:15:42.160</a></span> | <span class="t">example of a task that these models still can't solve yet, although I'm not sure if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=946" target="_blank">00:15:46.600</a></span> | <span class="t">we've evaluated it with newer and newer models. Maybe the latest versions can indeed actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=950" target="_blank">00:15:50.360</a></span> | <span class="t">solve that task. Yeah, question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=951" target="_blank">00:15:51.360</a></span> | <span class="t">Is there some intuition for why this emerges as a result of model scale?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=956" target="_blank">00:15:56.160</a></span> | <span class="t">I think that's a highly active area of research, and there's been papers published every week</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=960" target="_blank">00:16:00.360</a></span> | <span class="t">on this. So I think there's a lot of interesting experiments that really try to dissect either</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=965" target="_blank">00:16:05.120</a></span> | <span class="t">with synthetic tasks, like can GPT-3 learn linear regression in context? And there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=970" target="_blank">00:16:10.720</a></span> | <span class="t">some model interpretability tasks, like what in the attention layers or what in the hidden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=974" target="_blank">00:16:14.560</a></span> | <span class="t">states are resulting in this kind of emergent learning. But yeah, I'd have to just refer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=979" target="_blank">00:16:19.160</a></span> | <span class="t">you to the recent literature on that. Anything else? Awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=987" target="_blank">00:16:27.000</a></span> | <span class="t">Okay, so just to summarize, traditional fine tuning here is on the right. We take a bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=992" target="_blank">00:16:32.280</a></span> | <span class="t">of examples of a task that we care about. We give it to our model, and then we do a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=995" target="_blank">00:16:35.680</a></span> | <span class="t">gradient step on each example. And then at the end, we hopefully get a model that can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=999" target="_blank">00:16:39.080</a></span> | <span class="t">do well on some outputs. And in this new kind of paradigm of just prompting a language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1003" target="_blank">00:16:43.640</a></span> | <span class="t">we just have a frozen language model, and we just give some examples and ask the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1007" target="_blank">00:16:47.240</a></span> | <span class="t">to predict the right answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1013" target="_blank">00:16:53.320</a></span> | <span class="t">So you might think, and you'd be right, that there are some limits of prompting. Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1017" target="_blank">00:16:57.040</a></span> | <span class="t">there's a lot of limits of prompting, but especially for tasks that are too hard. There</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1020" target="_blank">00:17:00.680</a></span> | <span class="t">are a lot of tasks that maybe seem too difficult, especially ones that involve maybe richer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1024" target="_blank">00:17:04.840</a></span> | <span class="t">reasoning steps or needing to synthesize multiple pieces of information. And these are tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1030" target="_blank">00:17:10.060</a></span> | <span class="t">that humans struggle with too. So one example is GPT-3. I don't have the actual graph here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1036" target="_blank">00:17:16.620</a></span> | <span class="t">but it was famously bad at doing addition for much larger digits. And so if you prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1041" target="_blank">00:17:21.960</a></span> | <span class="t">GPT-3 with a bunch of examples of addition, it won't do it correct. But part of the reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1047" target="_blank">00:17:27.000</a></span> | <span class="t">is because humans are also pretty bad at doing this in one step. Like if I asked you to just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1051" target="_blank">00:17:31.280</a></span> | <span class="t">add these two numbers on the fly and didn't give you a pencil and paper, you'd have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1054" target="_blank">00:17:34.600</a></span> | <span class="t">pretty hard time with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1057" target="_blank">00:17:37.520</a></span> | <span class="t">So one observation is that you can just change the prompts and hopefully get some better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1061" target="_blank">00:17:41.160</a></span> | <span class="t">performance out of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1064" target="_blank">00:17:44.100</a></span> | <span class="t">So there's this idea of doing chain of thought prompting, where in standard prompting, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1069" target="_blank">00:17:49.700</a></span> | <span class="t">give some examples of a task that we'd like to complete. So here is an example of a math</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1073" target="_blank">00:17:53.380</a></span> | <span class="t">word problem. And I told you that what we would do is we would give the question and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1078" target="_blank">00:17:58.180</a></span> | <span class="t">then the answer. And then for a data point that we actually care about, we ask the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1083" target="_blank">00:18:03.320</a></span> | <span class="t">to predict the answer. And the model will try to produce the right answer, and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1087" target="_blank">00:18:07.040</a></span> | <span class="t">just wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1089" target="_blank">00:18:09.780</a></span> | <span class="t">So the idea of chain of thought prompting is to actually demonstrate what kind of reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1094" target="_blank">00:18:14.040</a></span> | <span class="t">you want the model to complete. So in your prompt, you not only put the question, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1100" target="_blank">00:18:20.060</a></span> | <span class="t">you also put an answer and the kinds of reasoning steps that are required to arrive at the correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1104" target="_blank">00:18:24.700</a></span> | <span class="t">answer. So here is actually some reasoning of how you actually would answer this tennis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1108" target="_blank">00:18:28.400</a></span> | <span class="t">ball question and then get the right answer. And because the language model is incentivized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1113" target="_blank">00:18:33.540</a></span> | <span class="t">to just follow the pattern and continue the prompt, if you give it another question, it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1118" target="_blank">00:18:38.260</a></span> | <span class="t">will in turn produce an answer, sorry, a rationale followed by an answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1124" target="_blank">00:18:44.860</a></span> | <span class="t">So you're kind of asking the language model to work through the steps yourself. And by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1128" target="_blank">00:18:48.640</a></span> | <span class="t">doing so, you end up getting some questions right when you otherwise might not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1134" target="_blank">00:18:54.620</a></span> | <span class="t">So a super simple idea, but it's shown to be extremely effective. So here is this middle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1139" target="_blank">00:18:59.940</a></span> | <span class="t">school math word problems benchmark. And again, as we scale up the model for GPT and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1144" target="_blank">00:19:04.540</a></span> | <span class="t">some other kinds of models, being able to do chain of thought prompting emerges. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1150" target="_blank">00:19:10.020</a></span> | <span class="t">we really see a performance approaching that of supervised baselines for these larger and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1155" target="_blank">00:19:15.220</a></span> | <span class="t">larger models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1156" target="_blank">00:19:16.220</a></span> | <span class="t">Questions? Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1157" target="_blank">00:19:17.220</a></span> | <span class="t">Seemingly the problem with the addition of the large numbers, do you have results on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1175" target="_blank">00:19:35.900</a></span> | <span class="t">how the chain of thought prompting for the larger numbers that middle school math word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1176" target="_blank">00:19:36.900</a></span> | <span class="t">problems?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1177" target="_blank">00:19:37.900</a></span> | <span class="t">Yeah. So the question is, does chain of thought prompting work for those addition problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1179" target="_blank">00:19:39.020</a></span> | <span class="t">that I had presented? Yeah. There should be some results in the actual paper. They're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1184" target="_blank">00:19:44.220</a></span> | <span class="t">just not here, but you can take a look. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1188" target="_blank">00:19:48.540</a></span> | <span class="t">Intuition of how the model was trained without doing gradient update?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1193" target="_blank">00:19:53.340</a></span> | <span class="t">Intuition about how the model is learning without gradient updates. Yeah. So this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1196" target="_blank">00:19:56.540</a></span> | <span class="t">related to the question asked earlier about how is this actually happening. That is, yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1202" target="_blank">00:20:02.220</a></span> | <span class="t">again, it's an active area of research. So my understanding of the literature is something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1206" target="_blank">00:20:06.660</a></span> | <span class="t">like you can show that models are kind of almost doing in-context gradient descent as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1211" target="_blank">00:20:11.140</a></span> | <span class="t">it's encoding a prompt. And you can analyze this with model interpretability experiments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1216" target="_blank">00:20:16.820</a></span> | <span class="t">But I'm happy to suggest papers afterwards that kind of deal with this problem more carefully.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1226" target="_blank">00:20:26.220</a></span> | <span class="t">Cool. Okay. So a follow up work to this asked the question of, do we actually even need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1235" target="_blank">00:20:35.740</a></span> | <span class="t">examples of reasoning? Do we actually even need to collect humans working through these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1239" target="_blank">00:20:39.860</a></span> | <span class="t">problems? Can we actually just ask the model to reason through things? Just ask it nicely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1245" target="_blank">00:20:45.980</a></span> | <span class="t">So this introduced this idea called zero shot chain of thought prompting. And it was honestly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1249" target="_blank">00:20:49.820</a></span> | <span class="t">like I think probably like the highest impact to simple idea ratio I've seen in a paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1255" target="_blank">00:20:55.260</a></span> | <span class="t">where it's like the simplest possible thing where instead of doing this chain of thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1258" target="_blank">00:20:58.620</a></span> | <span class="t">stuff, you just ask the question and then the answer, you first prepend the token, let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1263" target="_blank">00:21:03.740</a></span> | <span class="t">think step by step. And the model will decode as if it had said, let's think step by step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1269" target="_blank">00:21:09.540</a></span> | <span class="t">And it will work through some reasoning and produce the right answer. So does this work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1275" target="_blank">00:21:15.900</a></span> | <span class="t">on some arithmetic benchmarks? Here's what happens when you prompt the model just zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1280" target="_blank">00:21:20.300</a></span> | <span class="t">shot. So just asking it to produce the answer right away without any reasoning. A few shots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1284" target="_blank">00:21:24.900</a></span> | <span class="t">are giving some examples of inputs and outputs. And this is zero shot chain of thought. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1289" target="_blank">00:21:29.820</a></span> | <span class="t">just asking the model to think through things, you get crazy good accuracy. When we compare</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1295" target="_blank">00:21:35.980</a></span> | <span class="t">to actually doing manual chain of thought, you still do better with manual chains of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1299" target="_blank">00:21:39.620</a></span> | <span class="t">thought. But that just goes to show you how simple of an idea this is and ends up producing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1304" target="_blank">00:21:44.620</a></span> | <span class="t">improved performance numbers. So the funny part of this paper was why use let's think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1311" target="_blank">00:21:51.620</a></span> | <span class="t">by step by step. They used actually a lot of prompts and tried them out. So here's zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1315" target="_blank">00:21:55.860</a></span> | <span class="t">shot baseline performance. They tried out a bunch of different prefixes, the answers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1320" target="_blank">00:22:00.340</a></span> | <span class="t">after the proof. Let's think. Let's think about this logically. And they found that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1324" target="_blank">00:22:04.140</a></span> | <span class="t">let's think step by step was the best one. It turns out this was actually built upon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1329" target="_blank">00:22:09.260</a></span> | <span class="t">later in the year where they actually use a language model to search through the best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1332" target="_blank">00:22:12.860</a></span> | <span class="t">possible strings that would maximize performance on this task, which is probably gross overfitting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1338" target="_blank">00:22:18.420</a></span> | <span class="t">But the best prompt they found was let's work this out step by step in a step by step way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1343" target="_blank">00:22:23.060</a></span> | <span class="t">to be sure that we have the right answer. So the right answer thing is presuming that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1347" target="_blank">00:22:27.060</a></span> | <span class="t">you get the answer right. It's like giving the model some confidence in itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1352" target="_blank">00:22:32.780</a></span> | <span class="t">So this might seem to you like a total dark arcane art. And that's because it is. We really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1358" target="_blank">00:22:38.260</a></span> | <span class="t">have no intuition as to what's going on here. Or we're trying to build some intuition. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1364" target="_blank">00:22:44.580</a></span> | <span class="t">as a result, and I'm sure you've seen if you spend time in tech circles or you've seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1368" target="_blank">00:22:48.380</a></span> | <span class="t">on the internet, there's this whole new idea of prompt engineering being an emergent science</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1372" target="_blank">00:22:52.700</a></span> | <span class="t">and profession. So this includes things like asking a model for reasoning. It includes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1377" target="_blank">00:22:57.060</a></span> | <span class="t">jailbreaking language models for telling them to do things that they otherwise aren't trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1381" target="_blank">00:23:01.780</a></span> | <span class="t">to do. Even AI art like DALI or stable diffusion, this idea of constructing these really complex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1388" target="_blank">00:23:08.740</a></span> | <span class="t">prompts to get model outputs that you want. That's also prompting. Anecdotally, I've heard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1393" target="_blank">00:23:13.660</a></span> | <span class="t">of people saying I'm going to use a code generation model, but I'm going to include the Google</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1397" target="_blank">00:23:17.100</a></span> | <span class="t">code header in first because that will make more professional or bug free code depending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1401" target="_blank">00:23:21.460</a></span> | <span class="t">on how much you believe in Google. But yeah, and there's a Wikipedia article on this now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1407" target="_blank">00:23:27.260</a></span> | <span class="t">and there's even startups that are hiring for prompt engineers and they pay quite well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1410" target="_blank">00:23:30.480</a></span> | <span class="t">So if you want to be a prompt engineer, definitely practice your GPT whispering skills.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1415" target="_blank">00:23:35.980</a></span> | <span class="t">We have a question? Sorry. Yes, you go. Yeah, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1424" target="_blank">00:23:44.500</a></span> | <span class="t">A few slides ago, you said LM design that was like this long. How can you get the LM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1431" target="_blank">00:23:51.700</a></span> | <span class="t">to design and input like--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1434" target="_blank">00:23:54.620</a></span> | <span class="t">I think they treated it like a reinforcement learning problem. But I'll just direct you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1438" target="_blank">00:23:58.100</a></span> | <span class="t">to this paper at the bottom to learn more details. Yeah, I think it's the Joe et al</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1441" target="_blank">00:24:01.780</a></span> | <span class="t">2022 paper. Yeah. Yeah, question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1442" target="_blank">00:24:02.780</a></span> | <span class="t">So I'm just a bit curious about how they provided feedback. So in case the model was not giving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1443" target="_blank">00:24:03.780</a></span> | <span class="t">the right answer, were there prompts to say that that's not right? Maybe think about this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1444" target="_blank">00:24:04.780</a></span> | <span class="t">different approach. How is feedback provided?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1449" target="_blank">00:24:09.780</a></span> | <span class="t">They don't think about feedback in this kind of chain of thought prompting experiments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1465" target="_blank">00:24:25.020</a></span> | <span class="t">They just like if the model gets the answer wrong, then it gets the answer wrong and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1467" target="_blank">00:24:27.900</a></span> | <span class="t">just evaluate accuracy. Right. But this idea of incorporating feedback, I think, is quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1471" target="_blank">00:24:31.700</a></span> | <span class="t">interesting and I think you'll see some maybe hints of discussion of that later on. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1481" target="_blank">00:24:41.900</a></span> | <span class="t">Questions? Okay, awesome. Okay, so talking about these three things, I'm going to talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1491" target="_blank">00:24:51.820</a></span> | <span class="t">about the benefits and limitations of the various different things that we could be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1495" target="_blank">00:24:55.060</a></span> | <span class="t">doing here. So for zero shot and few shot in context learning, the benefit is you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1499" target="_blank">00:24:59.860</a></span> | <span class="t">need any fine tuning and you can carefully construct your prompts to hopefully get better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1504" target="_blank">00:25:04.140</a></span> | <span class="t">performance. The downsides are there are limits to what you can fit in context. Transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1509" target="_blank">00:25:09.540</a></span> | <span class="t">have a fixed context window of say 1,000 or a few thousand tokens. And I think, as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1514" target="_blank">00:25:14.940</a></span> | <span class="t">will probably find out, for really complex tasks, you are indeed going to need some gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1518" target="_blank">00:25:18.980</a></span> | <span class="t">steps. So you're going to need some sort of fine tuning. But that brings us to the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1524" target="_blank">00:25:24.220</a></span> | <span class="t">part of the lecture. So that's instruction fine tuning. Okay, so the idea of instruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1531" target="_blank">00:25:31.180</a></span> | <span class="t">fine tuning is that, sure, these models are pretty good at doing prompting. You can get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1535" target="_blank">00:25:35.900</a></span> | <span class="t">them to do really interesting things. But there is still a problem, which is that language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1540" target="_blank">00:25:40.280</a></span> | <span class="t">models are trained to predict the most likely continuation of tokens. And that is not the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1544" target="_blank">00:25:44.220</a></span> | <span class="t">same as what we want language models to do, which is to assist people. So as an example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1549" target="_blank">00:25:49.460</a></span> | <span class="t">if I give GPT-3 this kind of prompt, explain the moon landing, GPT-3 is trained to predict,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1554" target="_blank">00:25:54.700</a></span> | <span class="t">you know, if I saw this on the internet somewhere, what is the most likely continuation? Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1559" target="_blank">00:25:59.140</a></span> | <span class="t">maybe someone was coming up with a list of things to do with a six year old. So it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1562" target="_blank">00:26:02.420</a></span> | <span class="t">just predicting a list of other tasks, right? It's not answering your question. And so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1567" target="_blank">00:26:07.260</a></span> | <span class="t">issue here is that language models are not, the term is aligned with user intent. So how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1573" target="_blank">00:26:13.360</a></span> | <span class="t">might we better align models with user intent for this case? Well, super simple answer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1579" target="_blank">00:26:19.700</a></span> | <span class="t">right? We're machine learners. Let's do machine learning. So we're going to ask a human, give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1585" target="_blank">00:26:25.360</a></span> | <span class="t">me the right answer, right? Give me the way that a language model should respond according</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1589" target="_blank">00:26:29.300</a></span> | <span class="t">to this prompt. And let's just do fine tuning. So this is a slide from the pre-training lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1599" target="_blank">00:26:39.140</a></span> | <span class="t">Again, pre-training can improve NLP applications by serving as parameter initialization. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1605" target="_blank">00:26:45.940</a></span> | <span class="t">this kind of pipeline, I think you are familiar with. And the difference here is that instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1611" target="_blank">00:26:51.420</a></span> | <span class="t">of fine tuning on a single downstream task of interest, like sentiment analysis, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1615" target="_blank">00:26:55.460</a></span> | <span class="t">we're going to do is we're going to fine tune on many tasks. So we have a lot of tasks and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1619" target="_blank">00:26:59.860</a></span> | <span class="t">the hope is that we can then generalize to other unseen tasks at test time. So as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1626" target="_blank">00:27:06.040</a></span> | <span class="t">might expect, data and scale is kind of key for this to work. So we're going to collect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1631" target="_blank">00:27:11.340</a></span> | <span class="t">a bunch of examples of instruction output pairs across many tasks and then fine tune</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1636" target="_blank">00:27:16.940</a></span> | <span class="t">our language model and then evaluate generalization to unseen tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1643" target="_blank">00:27:23.100</a></span> | <span class="t">Yeah, so data and scale is important. So as an example, one recent data set that was published</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1649" target="_blank">00:27:29.900</a></span> | <span class="t">for this is called the Supernatural Instructions Dataset. It contains over 1.6 thousand tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1655" target="_blank">00:27:35.820</a></span> | <span class="t">containing 3 million examples. So this includes translation, question answering, question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1660" target="_blank">00:27:40.740</a></span> | <span class="t">generation, even coding, mathematical reasoning, etc. And when you look at this, you really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1667" target="_blank">00:27:47.540</a></span> | <span class="t">begin to think, well, is this actually fine tuning or is this just more pre-training?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1671" target="_blank">00:27:51.340</a></span> | <span class="t">And it's actually both. We're kind of blurring the lines here where the amount of scale that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1675" target="_blank">00:27:55.780</a></span> | <span class="t">we're training this on, basically it is kind of a still general but slightly more specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1680" target="_blank">00:28:00.520</a></span> | <span class="t">than language modeling type of pre-training task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1685" target="_blank">00:28:05.780</a></span> | <span class="t">So one question I have is, now that we are training our model on so many tasks, how do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1690" target="_blank">00:28:10.660</a></span> | <span class="t">we evaluate such a model? Because you can't really say, OK, can you now do sentiment analysis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1695" target="_blank">00:28:15.140</a></span> | <span class="t">well? The scale of tasks we want to evaluate this language model on is much greater.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1702" target="_blank">00:28:22.220</a></span> | <span class="t">So just as a brief aside, a lot of research has been going into building up these benchmarks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1707" target="_blank">00:28:27.740</a></span> | <span class="t">for these massive multicast language models and seeing to what degree they can do not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1712" target="_blank">00:28:32.380</a></span> | <span class="t">only just one task, but just a variety of tasks. So this is the Massive Multitask Language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1717" target="_blank">00:28:37.180</a></span> | <span class="t">Understanding Benchmark or MMLU. It consists of a bunch of benchmarks for measuring language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1722" target="_blank">00:28:42.420</a></span> | <span class="t">model performance on a bunch of knowledge intensive tasks that you would expect a high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1726" target="_blank">00:28:46.380</a></span> | <span class="t">school or college student to complete. So you're testing a language model not only on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1731" target="_blank">00:28:51.500</a></span> | <span class="t">sentiment analysis, but on astronomy and logic and European history. And here are some numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1738" target="_blank">00:28:58.340</a></span> | <span class="t">where at the time, DPD 3 is not that good, but it's certainly above a random baseline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1742" target="_blank">00:29:02.900</a></span> | <span class="t">on all of these tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1747" target="_blank">00:29:07.380</a></span> | <span class="t">Here's another example. So this is the Beyond the Imitation Game Benchmark or BigBench.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1751" target="_blank">00:29:11.760</a></span> | <span class="t">This has like a billion authors because it was a huge collaborative effort. And this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1756" target="_blank">00:29:16.100</a></span> | <span class="t">is a word cloud of the tasks that were evaluated. And it really contains some very esoteric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1762" target="_blank">00:29:22.740</a></span> | <span class="t">tasks. So this is an example of one task included where you have to, given a kanji or Japanese</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1767" target="_blank">00:29:27.740</a></span> | <span class="t">character in ASCII art, you need to predict the meaning of the character. So we're really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1771" target="_blank">00:29:31.580</a></span> | <span class="t">stress testing these language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1775" target="_blank">00:29:35.060</a></span> | <span class="t">OK, so instruction fine tuning, does it work? Recall there's a T5 encoded decoder model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1784" target="_blank">00:29:44.860</a></span> | <span class="t">So this is kind of Google's encoded decoder model, where it's pre-trained on this span</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1788" target="_blank">00:29:48.540</a></span> | <span class="t">corruption task. So if you don't remember that, you can refer back to that lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1792" target="_blank">00:29:52.780</a></span> | <span class="t">But the authors released a newer version called FLAN T5. So FLAN stands for fine tuning language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1797" target="_blank">00:29:57.780</a></span> | <span class="t">models. And this is T5 models trained on an additional 1.8 thousand tasks, which include</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1802" target="_blank">00:30:02.820</a></span> | <span class="t">the natural instructions data that I just mentioned. And if we average across both the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1807" target="_blank">00:30:07.120</a></span> | <span class="t">BigBench and an MLU performance and normalize it, what we see is that instruction fine tuning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1812" target="_blank">00:30:12.940</a></span> | <span class="t">works. And crucially, the bigger the model, the bigger the benefit that you get from doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1818" target="_blank">00:30:18.300</a></span> | <span class="t">instruction fine tuning. So it's really the large models that stand to do well from fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1822" target="_blank">00:30:22.840</a></span> | <span class="t">tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1825" target="_blank">00:30:25.640</a></span> | <span class="t">And you might look at this and say, this is kind of sad for academics or anyone without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1829" target="_blank">00:30:29.620</a></span> | <span class="t">a massive GPU cluster. It's like who can run an 11 billion parameter model? I guess the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1834" target="_blank">00:30:34.020</a></span> | <span class="t">one silver lining, if you look at the results here, are the 80 million model, which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1838" target="_blank">00:30:38.500</a></span> | <span class="t">smallest one. If you look at after fine tuning, it ends up performing about as well as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1843" target="_blank">00:30:43.140</a></span> | <span class="t">un-fine tuned 11 billion parameter model. So there's a lot of examples in the literature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1848" target="_blank">00:30:48.020</a></span> | <span class="t">about smaller instruction fine tune pre-trained models outperforming larger models that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1853" target="_blank">00:30:53.700</a></span> | <span class="t">many, many more times the size. So hopefully there's still some hope for people with just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1857" target="_blank">00:30:57.380</a></span> | <span class="t">like a few GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1860" target="_blank">00:31:00.420</a></span> | <span class="t">Any questions? Awesome. In order to really understand the capabilities, I highly recommend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1868" target="_blank">00:31:08.140</a></span> | <span class="t">that you just try it out yourself. So Flan T5 is hosted on Hugging Face. I think Hugging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1873" target="_blank">00:31:13.500</a></span> | <span class="t">Face has a demo where you can just type in a little query, ask it to do anything, see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1877" target="_blank">00:31:17.500</a></span> | <span class="t">what it does. But there are qualitative examples of this working. So four questions where a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1883" target="_blank">00:31:23.140</a></span> | <span class="t">non-instruction fine tune model will just kind of waffle on and not answer the question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1887" target="_blank">00:31:27.820</a></span> | <span class="t">Doing instruction fine tuning will get your model to much more accurately reason through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1891" target="_blank">00:31:31.460</a></span> | <span class="t">things and give you the right answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1897" target="_blank">00:31:37.380</a></span> | <span class="t">OK. So that was instruction fine tuning. Positives of this method. Super simple, super straightforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1905" target="_blank">00:31:45.300</a></span> | <span class="t">It's just doing fine tuning. And you see this really cool ability to generalize to unseen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1910" target="_blank">00:31:50.100</a></span> | <span class="t">tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1912" target="_blank">00:31:52.940</a></span> | <span class="t">In terms of negatives, does anyone have any ideas for what might be downsides of instruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1918" target="_blank">00:31:58.580</a></span> | <span class="t">fine tuning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1919" target="_blank">00:31:59.580</a></span> | <span class="t">It seems like it suffers from the same negatives of any human source data. It's hard to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1933" target="_blank">00:32:13.620</a></span> | <span class="t">people to provide the input. You don't know. Different people think different inputs about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1938" target="_blank">00:32:18.620</a></span> | <span class="t">it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1939" target="_blank">00:32:19.620</a></span> | <span class="t">Yeah, yeah, exactly. So comments are, well, it's hard and annoying to get human labels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1945" target="_blank">00:32:25.700</a></span> | <span class="t">and it's expensive. That's something that definitely matters. And that last part you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1949" target="_blank">00:32:29.180</a></span> | <span class="t">mentioned about there might be, you know, humans might disagree on what the right label</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1952" target="_blank">00:32:32.540</a></span> | <span class="t">is. Yeah, that's increasingly a problem. Yeah. So what are the limitations? The obvious limitation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1959" target="_blank">00:32:39.540</a></span> | <span class="t">is money. Collecting ground truth data for so many tasks costs a lot of money. Subtler</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1965" target="_blank">00:32:45.740</a></span> | <span class="t">limitations include the one that you were mentioning. So as we begin to ask for more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1970" target="_blank">00:32:50.340</a></span> | <span class="t">creative and open-ended tasks from our models, right, there are tasks where there is no right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1974" target="_blank">00:32:54.300</a></span> | <span class="t">answer. And it's a little bit weird to say, you know, this is an example of how to write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1979" target="_blank">00:32:59.020</a></span> | <span class="t">some story, right? So write me a story about a dog and her pet grasshopper. Like there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1983" target="_blank">00:33:03.100</a></span> | <span class="t">is not one answer to this, but if we were only to collect one or two demonstrations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1987" target="_blank">00:33:07.740</a></span> | <span class="t">the language modeling objective would say you should put all of your probability mass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1991" target="_blank">00:33:11.820</a></span> | <span class="t">on the two ways that two humans wrote this answer, right? When in reality, there's no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1996" target="_blank">00:33:16.360</a></span> | <span class="t">right answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=1999" target="_blank">00:33:19.900</a></span> | <span class="t">Another problem, which is related kind of fundamentally to language modeling in the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2003" target="_blank">00:33:23.260</a></span> | <span class="t">place, is that language modeling as an objective penalizes all token level mistakes equally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2009" target="_blank">00:33:29.860</a></span> | <span class="t">So what I mean by that is if you were asking a language model, for example, to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2013" target="_blank">00:33:33.020</a></span> | <span class="t">the sentence, "Avatar is a fantasy TV show," and you were asking it, and let's imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2019" target="_blank">00:33:39.140</a></span> | <span class="t">that the LM mispredicted adventure instead of fantasy, right? So adventure is a mistake.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2025" target="_blank">00:33:45.420</a></span> | <span class="t">It's not the right word, but it is equally as bad as if the model were to predict something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2030" target="_blank">00:33:50.500</a></span> | <span class="t">like musical, right? But the problem is that "Avatar is an adventure TV show" is still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2036" target="_blank">00:33:56.420</a></span> | <span class="t">true, right? So it's not necessarily a bad thing, whereas "Avatar is a musical" is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2040" target="_blank">00:34:00.460</a></span> | <span class="t">false. So under the language modeling objective, right, if the model were equally confident,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2046" target="_blank">00:34:06.020</a></span> | <span class="t">you would pay the equal penalty, an equal loss penalty for predicting either of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2049" target="_blank">00:34:09.480</a></span> | <span class="t">tokens wrong. But it's clear that this objective is not actually aligned with what users want,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2055" target="_blank">00:34:15.700</a></span> | <span class="t">which is maybe truth or creativity or generally just this idea of human preferences, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2060" target="_blank">00:34:20.860</a></span> | <span class="t">Yeah, question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2061" target="_blank">00:34:21.860</a></span> | <span class="t">Could we do something like multiply the penalty by the distance from where you're betting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2069" target="_blank">00:34:29.060</a></span> | <span class="t">in order to reduce this? Because musical would have a higher distance away than adventure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2074" target="_blank">00:34:34.740</a></span> | <span class="t">Yeah, that's an interesting question. It's an interesting idea. I haven't heard of people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2081" target="_blank">00:34:41.140</a></span> | <span class="t">doing that, but it seems plausible. I guess one issue is you might come up with adversarial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2086" target="_blank">00:34:46.540</a></span> | <span class="t">settings where maybe the word embedding distance is also not telling you the right thing, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2090" target="_blank">00:34:50.340</a></span> | <span class="t">So for example, show and musical maybe are very close together because they're both shows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2095" target="_blank">00:34:55.220</a></span> | <span class="t">or things to watch, but they are in veracity, right? They're completely different. One is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2099" target="_blank">00:34:59.860</a></span> | <span class="t">true, one is false, right? So yeah, you can try it, although I think there might be some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2104" target="_blank">00:35:04.340</a></span> | <span class="t">tricky edge cases like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2107" target="_blank">00:35:07.540</a></span> | <span class="t">Cool. Okay, so in the next part of the talk, we're going to actually explicitly try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2114" target="_blank">00:35:14.700</a></span> | <span class="t">satisfy human preferences and come with a mathematical framework for doing so. And yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2123" target="_blank">00:35:23.020</a></span> | <span class="t">so these are the limitations, as I had just mentioned. So this is where we get into reinforcing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2127" target="_blank">00:35:27.820</a></span> | <span class="t">learning from human feedback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2131" target="_blank">00:35:31.020</a></span> | <span class="t">Okay, so RLHF. So let's say we were training a language model on some task like summarization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2141" target="_blank">00:35:41.700</a></span> | <span class="t">And let's imagine that for each language model sample S, let's imagine that we had a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2146" target="_blank">00:35:46.340</a></span> | <span class="t">to obtain a human reward of that summary. So we could score this summary with a reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2152" target="_blank">00:35:52.740</a></span> | <span class="t">function, which we'll call R of S, and the higher the reward, the better. So let's imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2159" target="_blank">00:35:59.940</a></span> | <span class="t">we're summarizing this article, and we have this summary, which maybe is pretty good,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2164" target="_blank">00:36:04.900</a></span> | <span class="t">let's say. We had another summary, maybe it's a bit worse. And if we were able to ask a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2170" target="_blank">00:36:10.780</a></span> | <span class="t">human to just rate all these outputs, then the objective that we want to maximize or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2174" target="_blank">00:36:14.740</a></span> | <span class="t">satisfy is very obvious. We just want to maximize the expected reward of samples from our language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2179" target="_blank">00:36:19.900</a></span> | <span class="t">model, right? So in expectation, as we take samples from our language model, P theta,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2185" target="_blank">00:36:25.940</a></span> | <span class="t">we just want to maximize the reward of those samples. Fairly straightforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2193" target="_blank">00:36:33.300</a></span> | <span class="t">So for mathematical simplicity here, I'm kind of assuming that there's only one task or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2198" target="_blank">00:36:38.220</a></span> | <span class="t">one prompt, right? So let's imagine we were just trying to summarize this article, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2202" target="_blank">00:36:42.340</a></span> | <span class="t">we could talk about how to extend it to multiple prompts later on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2206" target="_blank">00:36:46.580</a></span> | <span class="t">Okay, so this kind of task is the domain of reinforcement learning. So I'm not going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2212" target="_blank">00:36:52.580</a></span> | <span class="t">presume there's any knowledge of reinforcement learning, although I'm sure some of you are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2215" target="_blank">00:36:55.780</a></span> | <span class="t">quite familiar with it, probably even more familiar than I am. But the field of reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2220" target="_blank">00:37:00.380</a></span> | <span class="t">learning has studied these kinds of problems, these optimization problems of how to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2224" target="_blank">00:37:04.700</a></span> | <span class="t">something while you're simulating the optimization for many years now. And in 2013, there was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2231" target="_blank">00:37:11.140</a></span> | <span class="t">a resurgence of interest in reinforcement learning for deep learning specifically. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2235" target="_blank">00:37:15.100</a></span> | <span class="t">you might have seen these results from DeepMind about an agent learning to play Atari games,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2240" target="_blank">00:37:20.140</a></span> | <span class="t">an agent mastering Go much earlier than expected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2244" target="_blank">00:37:24.660</a></span> | <span class="t">But interestingly, I think the interest in applying reinforcement learning to modern</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2248" target="_blank">00:37:28.140</a></span> | <span class="t">LMs is a bit newer, on the other hand. And I think the kind of earliest success story</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2252" target="_blank">00:37:32.940</a></span> | <span class="t">or one of the earliest success stories was only in 2019, for example. So why might this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2257" target="_blank">00:37:37.620</a></span> | <span class="t">be the case? There's a few reasons. I think in general, the field had kind of this sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2262" target="_blank">00:37:42.100</a></span> | <span class="t">that reinforcement learning with language models was really hard to get right, partially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2266" target="_blank">00:37:46.780</a></span> | <span class="t">because language models are very complicated. And if you think of language models as actors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2272" target="_blank">00:37:52.540</a></span> | <span class="t">that have an action space where they can spit out any sentence, that's a lot of sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2276" target="_blank">00:37:56.980</a></span> | <span class="t">So it's a very complex space to explore. So it still is a really hard problem. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2281" target="_blank">00:38:01.460</a></span> | <span class="t">part of the reason. But also practically, I think there have been these newer algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2286" target="_blank">00:38:06.340</a></span> | <span class="t">that seem to work much better for deep neural models, including language models. And these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2291" target="_blank">00:38:11.080</a></span> | <span class="t">include algorithms like proximal policy optimization. But we won't get into the details of that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2295" target="_blank">00:38:15.620</a></span> | <span class="t">for this course. But these are the kind of the reasons why we've been reinterested in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2300" target="_blank">00:38:20.420</a></span> | <span class="t">this idea of doing RL with language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2308" target="_blank">00:38:28.060</a></span> | <span class="t">So how do we actually maximize this objective? I've written it down. And ideally, we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2312" target="_blank">00:38:32.420</a></span> | <span class="t">just change our parameters data so that reward is high. But it's not really clear how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2317" target="_blank">00:38:37.020</a></span> | <span class="t">do so. So when we think about it, I mean, what have we learned in the class thus far?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2322" target="_blank">00:38:42.180</a></span> | <span class="t">We know that we can do gradient descent or gradient ascent. So let's try doing gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2325" target="_blank">00:38:45.780</a></span> | <span class="t">ascent. We're going to maximize this objective. So we're going to step in the direction of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2329" target="_blank">00:38:49.220</a></span> | <span class="t">steepest gradient. But this quickly becomes a problem, which is what is this quantity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2334" target="_blank">00:38:54.820</a></span> | <span class="t">and how do we evaluate it? How do we estimate this expectation given that the variables</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2340" target="_blank">00:39:00.460</a></span> | <span class="t">of the gradient that we're taking, theta, appear in the sample of the expectation? And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2346" target="_blank">00:39:06.420</a></span> | <span class="t">the second is what if our reward function is not differentiable? Like human judgments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2350" target="_blank">00:39:10.600</a></span> | <span class="t">are not differentiable. We can't back prop through them. And so we need this to be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2354" target="_blank">00:39:14.000</a></span> | <span class="t">to work with a black box reward function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2356" target="_blank">00:39:16.620</a></span> | <span class="t">So there's a class of methods in reinforcement learning called policy gradient methods that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2362" target="_blank">00:39:22.820</a></span> | <span class="t">gives us tools for estimating and optimizing this objective. And for the purposes of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2368" target="_blank">00:39:28.220</a></span> | <span class="t">course, I'm going to try to describe the highest level possible intuition for this, which looks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2375" target="_blank">00:39:35.260</a></span> | <span class="t">at the math and shows what's going on here. But it is going to omit a lot of the details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2380" target="_blank">00:39:40.380</a></span> | <span class="t">And a full treatment of reinforcement learning is definitely outside of the scope of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2383" target="_blank">00:39:43.820</a></span> | <span class="t">course. So if you're more interested in this kind of content, you should check out CS234</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2388" target="_blank">00:39:48.620</a></span> | <span class="t">Reinforcement Learning, for example. And in general, I think this is going to get a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2393" target="_blank">00:39:53.380</a></span> | <span class="t">mathy, but it's totally fine if you don't understand it. We will talk, we'll regroup</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2396" target="_blank">00:39:56.740</a></span> | <span class="t">at the end and just show what this means for how to do RLHF.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2403" target="_blank">00:40:03.320</a></span> | <span class="t">But what I'm going to do is just describe how we actually estimate this objective. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2406" target="_blank">00:40:06.780</a></span> | <span class="t">we want to obtain this gradient. So it's the gradient of the expectation of the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2412" target="_blank">00:40:12.980</a></span> | <span class="t">of samples from our language model. And if we do the math, we break this apart. This</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2417" target="_blank">00:40:17.260</a></span> | <span class="t">is our definition of what an expectation is. We're going to sum over all sentences rated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2421" target="_blank">00:40:21.580</a></span> | <span class="t">by the probability. And due to the linearity of the gradient, we can put the gradient operator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2426" target="_blank">00:40:26.740</a></span> | <span class="t">inside of the sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2431" target="_blank">00:40:31.680</a></span> | <span class="t">Now what we're going to do is we're going to use a very handy trick known as a log derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2435" target="_blank">00:40:35.380</a></span> | <span class="t">trick. And this is called a trick, but it's really just the chain rule. But let's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2438" target="_blank">00:40:38.940</a></span> | <span class="t">see what happens when we take the gradient of the log probability of a sample from our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2443" target="_blank">00:40:43.580</a></span> | <span class="t">language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2446" target="_blank">00:40:46.240</a></span> | <span class="t">So if I take the gradients, then how do we use the chain rule? So the gradient of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2449" target="_blank">00:40:49.900</a></span> | <span class="t">log of something is going to be 1 over that something times the gradient of the middle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2453" target="_blank">00:40:53.460</a></span> | <span class="t">of that something. So 1 over P theta of s times the gradient. And if we rearrange, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2458" target="_blank">00:40:58.380</a></span> | <span class="t">see that we can alternatively write the gradient of P theta of s as this product. So P theta</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2464" target="_blank">00:41:04.580</a></span> | <span class="t">of s times the gradient of the log P theta of s. And we can plug this back in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2472" target="_blank">00:41:12.660</a></span> | <span class="t">And the reason why we're doing this is because we're going to convert this into a form where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2475" target="_blank">00:41:15.740</a></span> | <span class="t">the expectation is easy to estimate. So we plug it back in. That gives us this. And if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2482" target="_blank">00:41:22.340</a></span> | <span class="t">you squint quite closely at this last equation here, this first part here is the definition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2488" target="_blank">00:41:28.540</a></span> | <span class="t">of an expectation. We are summing over a bunch of samples from our model, and we are weighting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2493" target="_blank">00:41:33.060</a></span> | <span class="t">it by the probability of that sample, which means that we can rewrite it as an expectation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2497" target="_blank">00:41:37.740</a></span> | <span class="t">And in particular, it's an expectation of this quantity here. So let's just rewrite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2502" target="_blank">00:41:42.900</a></span> | <span class="t">it. And this gives us our kind of newer form of this objective. So these two are equivalent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2507" target="_blank">00:41:47.900</a></span> | <span class="t">the top here and the bottom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2510" target="_blank">00:41:50.500</a></span> | <span class="t">And what has happened here is we've kind of shoved the gradient inside of the expectation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2514" target="_blank">00:41:54.860</a></span> | <span class="t">if that makes sense. So why is this useful? Does anyone have any questions on this before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2520" target="_blank">00:42:00.700</a></span> | <span class="t">I move on? If you don't understand it, that's fine as well, because we will understand the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2526" target="_blank">00:42:06.620</a></span> | <span class="t">intuition behind it later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2534" target="_blank">00:42:14.420</a></span> | <span class="t">So we've converted this into this. And we put the gradient inside the expectation, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2540" target="_blank">00:42:20.180</a></span> | <span class="t">means we can now approximate this objective with Monte Carlo samples. So the way to approximate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2544" target="_blank">00:42:24.900</a></span> | <span class="t">any expectation is to just take a bunch of samples and then average them. So approximately,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2550" target="_blank">00:42:30.420</a></span> | <span class="t">this is equal to sampling a finite number of samples from our model, and then summing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2554" target="_blank">00:42:34.920</a></span> | <span class="t">up the average of the reward times the log probability, the gradient of the log probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2559" target="_blank">00:42:39.700</a></span> | <span class="t">of that sample. And that gives us this update rule, plugging it back in for that gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2566" target="_blank">00:42:46.980</a></span> | <span class="t">descent step that we wanted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2569" target="_blank">00:42:49.580</a></span> | <span class="t">So what is this? What does this mean? Let's think about a very simple case. Imagine the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2576" target="_blank">00:42:56.840</a></span> | <span class="t">reward was a binary reward. So it was either 0 or 1. So for example, imagine we were trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2581" target="_blank">00:43:01.820</a></span> | <span class="t">to train a language model to talk about cats. So whenever it utters a sentence with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2585" target="_blank">00:43:05.500</a></span> | <span class="t">word cat, we give it a 1 reward. Otherwise, we give it a 0 reward. Now, if our reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2590" target="_blank">00:43:10.860</a></span> | <span class="t">is binary, does anyone know what this objective reduces to or look like? Any ideas? If I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2603" target="_blank">00:43:23.360</a></span> | <span class="t">lost everyone, that's fine too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2607" target="_blank">00:43:27.460</a></span> | <span class="t">The reward would just be an indicator function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2615" target="_blank">00:43:35.380</a></span> | <span class="t">So basically, to answer, the reward would be 0 everywhere, except for sentences that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2621" target="_blank">00:43:41.260</a></span> | <span class="t">contain the word cat. And in that case, it would be 1. So basically, that would just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2626" target="_blank">00:43:46.300</a></span> | <span class="t">look like vanilla gradient descent, just on sentences that contain the word cat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2632" target="_blank">00:43:52.760</a></span> | <span class="t">So to generalize this to the more general case, where the reward is scalar, what this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2637" target="_blank">00:43:57.860</a></span> | <span class="t">is looking like, if you look at it, is if r is very high, very positive, then we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2643" target="_blank">00:44:03.060</a></span> | <span class="t">multiplying the gradient of that sample by a large number. And so our objective will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2647" target="_blank">00:44:07.420</a></span> | <span class="t">try to take gradient steps in the direction of maximizing the probability of producing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2651" target="_blank">00:44:11.460</a></span> | <span class="t">that sample again, producing the sample that led to high reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2655" target="_blank">00:44:15.680</a></span> | <span class="t">And on the other hand, if r is low or even negative, then we will actively take steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2659" target="_blank">00:44:19.700</a></span> | <span class="t">to minimize the probability of that happening again. And that's the English intuition of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2664" target="_blank">00:44:24.180</a></span> | <span class="t">what's going on here. The reason why we call it reinforcement learning is because we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2668" target="_blank">00:44:28.280</a></span> | <span class="t">to reinforce good actions and increase the probability that they happen again in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2672" target="_blank">00:44:32.100</a></span> | <span class="t">future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2673" target="_blank">00:44:33.100</a></span> | <span class="t">And hopefully, this intuitively makes sense to all of you. Let's say you're playing a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2676" target="_blank">00:44:36.100</a></span> | <span class="t">video game, and on one run, you get a super high score. And you think to yourself, oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2680" target="_blank">00:44:40.300</a></span> | <span class="t">that was really good. Whatever I did that time, I should do again in the future. This</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2683" target="_blank">00:44:43.980</a></span> | <span class="t">is what we're trying to capture with this kind of update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2686" target="_blank">00:44:46.020</a></span> | <span class="t">Question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2687" target="_blank">00:44:47.020</a></span> | <span class="t">Is there any reason that we use policy gradient and not value iteration or other methods?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2695" target="_blank">00:44:55.300</a></span> | <span class="t">You can do a lot of things. I think there have been methods for doing Q-learning, offline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2699" target="_blank">00:44:59.980</a></span> | <span class="t">learning, et cetera, with language models. I think the design space has been very underexplored.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2706" target="_blank">00:45:06.660</a></span> | <span class="t">So there's a lot of low-hanging fruit out there for people who are willing to think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2709" target="_blank">00:45:09.440</a></span> | <span class="t">about what fancy things we can do in RL and apply them to this language modeling case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2715" target="_blank">00:45:15.540</a></span> | <span class="t">And in practice, what we use is not this simple thing, but we use a fancier thing that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2720" target="_blank">00:45:20.340</a></span> | <span class="t">proximal policy optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2721" target="_blank">00:45:21.340</a></span> | <span class="t">Question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2722" target="_blank">00:45:22.340</a></span> | <span class="t">Do you know if you're on LN, the space are super big, like almost a bit?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2728" target="_blank">00:45:28.100</a></span> | <span class="t">So that's the challenge. So one thing that I haven't mentioned here is that right now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2733" target="_blank">00:45:33.500</a></span> | <span class="t">I'm talking about entire samples of sentences, which is a massive space. In practice, when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2738" target="_blank">00:45:38.260</a></span> | <span class="t">we do RL, we actually do it at the level of generating individual tokens. So each token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2742" target="_blank">00:45:42.180</a></span> | <span class="t">is, let's say, GPT has 50,000 tokens. So it's a pretty large action space, but it's still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2747" target="_blank">00:45:47.540</a></span> | <span class="t">manageable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2751" target="_blank">00:45:51.300</a></span> | <span class="t">So that kind of answers this question I was asking, which is, can you see any problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2754" target="_blank">00:45:54.240</a></span> | <span class="t">with this objective? Which is that this is a very simplified objective. There is a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2758" target="_blank">00:45:58.440</a></span> | <span class="t">more tricks needed to make this work. But hopefully, this has given you kind of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2762" target="_blank">00:46:02.100</a></span> | <span class="t">high-level intuition as to what we're trying to do in the first place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2769" target="_blank">00:46:09.260</a></span> | <span class="t">OK, so now we are set. We have a bunch of samples from a language model. And for any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2779" target="_blank">00:46:19.020</a></span> | <span class="t">arbitrary reward function, like we're just asking a human to rate these samples, we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2783" target="_blank">00:46:23.500</a></span> | <span class="t">maximize that reward. So we're done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2786" target="_blank">00:46:26.260</a></span> | <span class="t">OK, so not so fast. There's a few problems. The first is the same as in the instruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2791" target="_blank">00:46:31.660</a></span> | <span class="t">fine-tuning case, which is that keeping a human in the loop is expensive. I don't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2796" target="_blank">00:46:36.180</a></span> | <span class="t">want to supervise every single output from a language model. I don't know if you all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2799" target="_blank">00:46:39.340</a></span> | <span class="t">want to. So what can we do to fix this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2804" target="_blank">00:46:44.660</a></span> | <span class="t">So one idea is, instead of needing to ask humans for preferences every single time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2809" target="_blank">00:46:49.120</a></span> | <span class="t">you can actually build a model of their preferences, like literally just train an NLP model of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2813" target="_blank">00:46:53.540</a></span> | <span class="t">their preferences. So this idea was kind of first introduced outside of language modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2818" target="_blank">00:46:58.820</a></span> | <span class="t">by this paper, Knox and Stone. They called it Tamr. But we're going to see it re-implemented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2824" target="_blank">00:47:04.560</a></span> | <span class="t">in this idea, where we're going to train a language model-- we'll call it a reward model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2828" target="_blank">00:47:08.940</a></span> | <span class="t">RM, which is parameterized by phi-- to predict human preferences from an annotated data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2835" target="_blank">00:47:15.060</a></span> | <span class="t">And then when doing RLHF, we're going to optimize for the reward model rewards instead of actual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2839" target="_blank">00:47:19.900</a></span> | <span class="t">human rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2845" target="_blank">00:47:25.140</a></span> | <span class="t">Here's another conceptual problem. So here's a new sample for our summarization task. What</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2850" target="_blank">00:47:30.260</a></span> | <span class="t">is the score of the sample? Anyone give me a number. Does anyone want to rate this sample?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2856" target="_blank">00:47:36.020</a></span> | <span class="t">It's like a 3, 6. What scale are we using? Et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2862" target="_blank">00:47:42.560</a></span> | <span class="t">So the issue here is that human judgments can be noisy and miscalibrated when you ask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2866" target="_blank">00:47:46.660</a></span> | <span class="t">people for things alone. So one workaround for this problem is, instead of asking for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2873" target="_blank">00:47:53.960</a></span> | <span class="t">direct ratings, ask humans to compare two summaries and judge which one is better. This</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2879" target="_blank">00:47:59.660</a></span> | <span class="t">has been shown, I think, in a variety of fields where people work with human subjects and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2883" target="_blank">00:48:03.400</a></span> | <span class="t">human responses to be more reliable. This includes psychology and medicine, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2889" target="_blank">00:48:09.340</a></span> | <span class="t">So in other words, instead of asking humans to just give absolute scores, we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2893" target="_blank">00:48:13.400</a></span> | <span class="t">to ask humans to compare different samples and rate which one is better. So as an example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2899" target="_blank">00:48:19.580</a></span> | <span class="t">maybe this first sample is better than the middle sample, and it's better than the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2902" target="_blank">00:48:22.760</a></span> | <span class="t">sample.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2905" target="_blank">00:48:25.760</a></span> | <span class="t">Now that we have these pairwise comparisons, our reward model is going to generate latent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2910" target="_blank">00:48:30.260</a></span> | <span class="t">scores, so implicit scores based on this pairwise comparison data. So our reward model is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2915" target="_blank">00:48:35.660</a></span> | <span class="t">language model that takes in a possible sample, and then it's going to produce a number, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2921" target="_blank">00:48:41.200</a></span> | <span class="t">is the score or the reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2923" target="_blank">00:48:43.700</a></span> | <span class="t">And the way that we're going to train this model-- and again, you don't really need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2926" target="_blank">00:48:46.980</a></span> | <span class="t">know too much of the details here, but this is a classic statistical comparison model--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2931" target="_blank">00:48:51.700</a></span> | <span class="t">is via the following loss, where the reward model essentially should just predict a higher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2936" target="_blank">00:48:56.700</a></span> | <span class="t">score if a sample is judged to be better than another sample. So in expectation, if we sample</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2943" target="_blank">00:49:03.160</a></span> | <span class="t">winning samples and losing samples from our data sets, then if you look at this term here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2949" target="_blank">00:49:09.160</a></span> | <span class="t">the score of the higher sample should be higher than the score of the losing sample. Does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2956" target="_blank">00:49:16.540</a></span> | <span class="t">that make sense? And in doing so, by just training on this objective, you will get a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2962" target="_blank">00:49:22.020</a></span> | <span class="t">language model that will learn to assign numerical scores to things, which indicate their relative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2967" target="_blank">00:49:27.420</a></span> | <span class="t">preference over other samples. And we can use those outputs as rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2972" target="_blank">00:49:32.580</a></span> | <span class="t">Is there some renormalization either in the output or somewhere else?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2984" target="_blank">00:49:44.140</a></span> | <span class="t">Yeah, so I don't remember if it happens during training. But certainly, after you've trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2989" target="_blank">00:49:49.580</a></span> | <span class="t">this model, you normalize the reward model so that the score is-- the expectation of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2992" target="_blank">00:49:52.380</a></span> | <span class="t">the score is 0, because that's good for reinforcement learning and things like that as well. Yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2998" target="_blank">00:49:58.380</a></span> | <span class="t">question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=2999" target="_blank">00:49:59.380</a></span> | <span class="t">How do we account for the fact that even though things are noisy, some people could view S3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3006" target="_blank">00:50:06.780</a></span> | <span class="t">as better than S1. How do we account for even though when it's noisy, the border and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3012" target="_blank">00:50:12.780</a></span> | <span class="t">coordination still work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3014" target="_blank">00:50:14.260</a></span> | <span class="t">Yeah, I think that's just kind of limitations with asking for these preferences in the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3020" target="_blank">00:50:20.060</a></span> | <span class="t">place is that humans will disagree. So we really have no ground truth unless we maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3024" target="_blank">00:50:24.500</a></span> | <span class="t">ask an ensemble of humans, for example. That's just a limitation with this. I think hopefully,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3029" target="_blank">00:50:29.540</a></span> | <span class="t">in the limit with enough data, this kind of noise washes out. But it's certainly an issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3033" target="_blank">00:50:33.980</a></span> | <span class="t">And this next slide will also kind of touch on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3038" target="_blank">00:50:38.180</a></span> | <span class="t">So does the reward model work? Can we actually learn to model human preferences in this way?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3042" target="_blank">00:50:42.620</a></span> | <span class="t">This is obviously an important standard we check before we actually try to optimize this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3045" target="_blank">00:50:45.620</a></span> | <span class="t">objective. And they measured this. So this is kind of evaluating the reward model on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3051" target="_blank">00:50:51.540</a></span> | <span class="t">a standard kind of validation set. So can the reward model predict outcomes for data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3056" target="_blank">00:50:56.660</a></span> | <span class="t">points that they have not seen during training? And does it change based on model size or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3062" target="_blank">00:51:02.020</a></span> | <span class="t">amount of data? And if you notice here, there's one dashed line, which is the human baseline,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3066" target="_blank">00:51:06.780</a></span> | <span class="t">which is if you ask a human to predict the outcome, a human does not get 100% accuracy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3071" target="_blank">00:51:11.660</a></span> | <span class="t">because humans disagree. And even an ensemble of, let's say, five humans also doesn't get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3076" target="_blank">00:51:16.660</a></span> | <span class="t">100% accuracy because humans have different preferences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3080" target="_blank">00:51:20.580</a></span> | <span class="t">But the key takeaway here is that for the largest possible model and for enough data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3085" target="_blank">00:51:25.820</a></span> | <span class="t">a reward model, at least on the validation set that they used, is kind of approaching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3090" target="_blank">00:51:30.380</a></span> | <span class="t">the performance of a single human person. And that's kind of a green light that maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3094" target="_blank">00:51:34.620</a></span> | <span class="t">we can try this out and see what happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3102" target="_blank">00:51:42.940</a></span> | <span class="t">So if there are no questions, this is kind of the components of our LHF. So we have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3109" target="_blank">00:51:49.500</a></span> | <span class="t">pre-trained model, maybe it's instruction fine-tuned, which we're going to call P of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3113" target="_blank">00:51:53.180</a></span> | <span class="t">PT. We have a reward model, which produces scalar rewards for language model outputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3119" target="_blank">00:51:59.500</a></span> | <span class="t">and it is trained on a dataset of human comparisons. And we have a method, policy gradient, for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3124" target="_blank">00:52:04.900</a></span> | <span class="t">arbitrarily optimizing language model perimeters towards some reward function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3130" target="_blank">00:52:10.220</a></span> | <span class="t">And so now if you want to do our LHF, you clone the pre-trained model, we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3134" target="_blank">00:52:14.860</a></span> | <span class="t">call this a copy of the model, which is the RL model, with parameters data that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3139" target="_blank">00:52:19.420</a></span> | <span class="t">actually going to optimize. And we're going to optimize the following reward with reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3145" target="_blank">00:52:25.660</a></span> | <span class="t">learning. And this reward looks a little bit more complicated than just using the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3149" target="_blank">00:52:29.660</a></span> | <span class="t">model. And the extra term here is a penalty, which prevents us from diverging too far from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3157" target="_blank">00:52:37.060</a></span> | <span class="t">the pre-trained model. So in expectation, this is known as the KL or Kohlback-Lieber</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3161" target="_blank">00:52:41.860</a></span> | <span class="t">divergence between the RL model and the pre-trained model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3167" target="_blank">00:52:47.940</a></span> | <span class="t">And I'll explain why we need this in a few slides. But basically, if you over-optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3172" target="_blank">00:52:52.580</a></span> | <span class="t">the reward model, you end up producing-- you can produce gibberish. And what happens is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3177" target="_blank">00:52:57.300</a></span> | <span class="t">you pay a price. So this quantity is large if the probability of a sample under the RL</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3184" target="_blank">00:53:04.180</a></span> | <span class="t">tuned model is much higher than the probability of the sample under the pre-trained model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3188" target="_blank">00:53:08.820</a></span> | <span class="t">So the pre-trained model would say, this is a very unlikely sequence of characters for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3192" target="_blank">00:53:12.160</a></span> | <span class="t">anyone to say. That's when you would pay a price here. And beta here is a tunable parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3197" target="_blank">00:53:17.780</a></span> | <span class="t">Yeah, question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3198" target="_blank">00:53:18.780</a></span> | <span class="t">When you say initialize a copy, that means the first iteration, PRL is equal to PPT?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3208" target="_blank">00:53:28.060</a></span> | <span class="t">That's right. Yeah. Yeah, when I say initialize a copy, basically, we want to be able to compare</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3213" target="_blank">00:53:33.200</a></span> | <span class="t">to the non-fine-tuned model just to evaluate this penalty term. So just leave the predictions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3218" target="_blank">00:53:38.840</a></span> | <span class="t">of the pre-RL model around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3223" target="_blank">00:53:43.620</a></span> | <span class="t">More questions? Great. So does it work? The answer is yes. So here is the key takeaway,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3236" target="_blank">00:53:56.660</a></span> | <span class="t">at least for the task summarization on this daily mail data set. So again, we're looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3242" target="_blank">00:54:02.220</a></span> | <span class="t">at different model sizes. But at the end here, we see that if we do just pre-training-- so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3247" target="_blank">00:54:07.060</a></span> | <span class="t">just like the typical language modeling objective that GPT uses-- you end up producing summaries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3252" target="_blank">00:54:12.060</a></span> | <span class="t">that, in general, are not preferred to the reference summaries. So this is on the y-axis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3256" target="_blank">00:54:16.020</a></span> | <span class="t">here is the amount of times that a human prefers the model-generated summary to a summary that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3261" target="_blank">00:54:21.860</a></span> | <span class="t">a human actually wrote or the one that's in the data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3265" target="_blank">00:54:25.480</a></span> | <span class="t">So pre-training doesn't work well, even if you do supervised learning. So supervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3269" target="_blank">00:54:29.340</a></span> | <span class="t">learning in this case is, let's actually fine-tune our model on the summaries that were in our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3273" target="_blank">00:54:33.900</a></span> | <span class="t">data sets. Even if you do that, you still kind of underperform the reference summaries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3279" target="_blank">00:54:39.020</a></span> | <span class="t">because you're not perfectly modeling those summaries. But it's only with this human feedback</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3284" target="_blank">00:54:44.860</a></span> | <span class="t">that we end up producing a language model that actually ends up producing summaries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3288" target="_blank">00:54:48.780</a></span> | <span class="t">that are judged to be better than the summaries in a data set that you were training on in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3292" target="_blank">00:54:52.260</a></span> | <span class="t">the first place. I think that's quite interesting. Any questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3306" target="_blank">00:55:06.100</a></span> | <span class="t">So now we talk about-- yeah, we're getting closer and closer to something like InstructGPT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3310" target="_blank">00:55:10.900</a></span> | <span class="t">or ChatGPT. The basic idea of InstructGPT is that we are scaling up RLHF to not just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3318" target="_blank">00:55:18.660</a></span> | <span class="t">one prompt, as I had described previously, but tens of thousands of prompts. And if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3323" target="_blank">00:55:23.940</a></span> | <span class="t">look at these three pieces, these are the three pieces that we've just described. The</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3327" target="_blank">00:55:27.740</a></span> | <span class="t">first piece here being instruction fine-tuning, the second piece being RLHF, and the third</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3333" target="_blank">00:55:33.540</a></span> | <span class="t">piece-- oh, sorry, the second part being reward model training, and the last part being RLHF.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3339" target="_blank">00:55:39.740</a></span> | <span class="t">The difference here is that they use 30,000 tasks. So again, with the same instruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3347" target="_blank">00:55:47.060</a></span> | <span class="t">fine-tuning idea, it's really about the scale and diversity of tasks that really matters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3350" target="_blank">00:55:50.820</a></span> | <span class="t">for getting good performance for these things. Yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3354" target="_blank">00:55:54.300</a></span> | <span class="t">Yeah, so the preceding results, you suggested that you really needed the RLHF, and it didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3368" target="_blank">00:56:08.540</a></span> | <span class="t">work so well to do supervised learning on the data. But they do supervised learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3374" target="_blank">00:56:14.020</a></span> | <span class="t">on the data in the fine-tuning in the first stage. Is that necessary, or else they should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3381" target="_blank">00:56:21.060</a></span> | <span class="t">have tended to go haywire and just went straight to RLHF?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3387" target="_blank">00:56:27.140</a></span> | <span class="t">Oh, yeah, that's a good question. So I think a key point here is that they initialized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3391" target="_blank">00:56:31.300</a></span> | <span class="t">the RL policy on the supervised policy. So they first got the model getting reasonably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3396" target="_blank">00:56:36.340</a></span> | <span class="t">good at doing summarization first, and then you do the RLHF on top to get the boost performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3402" target="_blank">00:56:42.640</a></span> | <span class="t">Your question you're asking is maybe, can we just do the RLHF starting from that pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3406" target="_blank">00:56:46.380</a></span> | <span class="t">baseline? That's a good question. I don't think they explored that, although I'm not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3412" target="_blank">00:56:52.300</a></span> | <span class="t">sure. I'd have to look at the paper again to remind myself. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3422" target="_blank">00:57:02.380</a></span> | <span class="t">So certainly for something like InstructGPT, yeah, they've always kind of presumed that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3426" target="_blank">00:57:06.240</a></span> | <span class="t">you need the kind of fine-tuning phase first, and then you build on top of it. But I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3430" target="_blank">00:57:10.820</a></span> | <span class="t">yeah, there's still some interesting open questions as to whether you can just go directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3434" target="_blank">00:57:14.580</a></span> | <span class="t">to RLHF. Question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3436" target="_blank">00:57:16.580</a></span> | <span class="t">Is the human reward function trained simultaneously with the fine-tuning of the language model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3447" target="_blank">00:57:27.980</a></span> | <span class="t">Or is it sequential?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3450" target="_blank">00:57:30.340</a></span> | <span class="t">Reward model should be trained first. Yeah. You train it first, you make sure it's good,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3454" target="_blank">00:57:34.620</a></span> | <span class="t">it's frozen, you optimize against that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3455" target="_blank">00:57:35.620</a></span> | <span class="t">What are the samples for the human rewards? Do they come from the generated task from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3462" target="_blank">00:57:42.620</a></span> | <span class="t">language model? Or where does the training sample come from?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3465" target="_blank">00:57:45.500</a></span> | <span class="t">For training the reward model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3467" target="_blank">00:57:47.220</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3468" target="_blank">00:57:48.220</a></span> | <span class="t">So, yeah, actually, it's a good question. Where do the rewards come from? So there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3474" target="_blank">00:57:54.700</a></span> | <span class="t">kind of an iterative process you can apply where you kind of repeat steps two and three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3478" target="_blank">00:57:58.580</a></span> | <span class="t">over and over again. So you sample a bunch of outputs from your language model. You get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3483" target="_blank">00:58:03.380</a></span> | <span class="t">humans to rate them. You then do RLHF to update your model again. And then you sample more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3487" target="_blank">00:58:07.740</a></span> | <span class="t">outputs and get humans to rate them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3489" target="_blank">00:58:09.800</a></span> | <span class="t">So in general, the rewards are done on sampled model outputs, because those are the outputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3493" target="_blank">00:58:13.780</a></span> | <span class="t">that you want to steer in one direction or another. But you can do this in an iterative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3497" target="_blank">00:58:17.820</a></span> | <span class="t">process where you kind of do RL and then maybe train a better reward model based on the new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3502" target="_blank">00:58:22.420</a></span> | <span class="t">outputs and continue. And I think they do a few iterations in InstructGBT, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3511" target="_blank">00:58:31.220</a></span> | <span class="t">Questions? OK. So 30,000 tasks. I think we're getting into very recent stuff where increasingly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3525" target="_blank">00:58:45.180</a></span> | <span class="t">companies like OpenAI are sharing less and less details about what actually happens in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3529" target="_blank">00:58:49.740</a></span> | <span class="t">training these models. So we have a little bit less clarity as to what's going on here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3533" target="_blank">00:58:53.220</a></span> | <span class="t">than maybe we have had in the past. But they do share the data that's not public, but they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3539" target="_blank">00:58:59.620</a></span> | <span class="t">do share the kinds of tasks that they collected from labelers. So they collected a bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3543" target="_blank">00:59:03.780</a></span> | <span class="t">prompts from people who were already using the GPT-3 API. So they had the benefit of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3548" target="_blank">00:59:08.540</a></span> | <span class="t">having many, many users of their API and taking the kinds of tasks that users would ask GPT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3555" target="_blank">00:59:15.860</a></span> | <span class="t">to do. And so these include things like brainstorming or open-end generation, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3564" target="_blank">00:59:24.860</a></span> | <span class="t">And yeah, I mean, the key results of InstructGBT, which is kind of the backbone of ChatGBT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3571" target="_blank">00:59:31.060</a></span> | <span class="t">really just needs to be seen and played with to understand. So you can feel free to play</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3574" target="_blank">00:59:34.540</a></span> | <span class="t">with either ChatGBT or one of the OpenAI APIs. But again, this example of a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3580" target="_blank">00:59:40.620</a></span> | <span class="t">and not necessarily following tasks, by doing this kind of instruction fine tuning followed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3585" target="_blank">00:59:45.900</a></span> | <span class="t">by RLHF, you get a model that is much better at adhering to user commands. Similarly, a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3595" target="_blank">00:59:55.220</a></span> | <span class="t">language model can be very good at generating super interesting open-ended creative text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3600" target="_blank">01:00:00.660</a></span> | <span class="t">as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3609" target="_blank">01:00:09.580</a></span> | <span class="t">This brings us to ChatGBT, which is even newer, and we have even less information about what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3614" target="_blank">01:00:14.260</a></span> | <span class="t">actually going on or what's being trained here. But yeah, and they're keeping their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3619" target="_blank">01:00:19.660</a></span> | <span class="t">secret sauce secret. But we do have a blog post where they wrote two paragraphs. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3625" target="_blank">01:00:25.940</a></span> | <span class="t">in the first paragraph, they said that they did instruction fine tuning. So we trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3630" target="_blank">01:00:30.980</a></span> | <span class="t">an initial model using supervised fine tuning. So human AI trainers provided conversations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3635" target="_blank">01:00:35.940</a></span> | <span class="t">where they played both sides. And then we asked them to act as a AI assistant. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3640" target="_blank">01:00:40.620</a></span> | <span class="t">we fine-tuned our model on acting like an AI assistant for humans. That's part one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3646" target="_blank">01:00:46.540</a></span> | <span class="t">Second paragraph, to create a reward model for RL, we collected comparison data. So we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3652" target="_blank">01:00:52.940</a></span> | <span class="t">took conversations with an earlier version of the chatbot, so the one that's pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3656" target="_blank">01:00:56.940</a></span> | <span class="t">on instruction following or instruction fine tuning, and then take multiple samples and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3662" target="_blank">01:01:02.020</a></span> | <span class="t">then rate the quality of the samples. And then using these reward models, we fine-tune</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3667" target="_blank">01:01:07.900</a></span> | <span class="t">it with RL. In particular, they used PPO, which is a fancier version of RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3678" target="_blank">01:01:18.340</a></span> | <span class="t">And yeah, so that produces-- I don't need to introduce the capabilities of ChatGBT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3681" target="_blank">01:01:21.580</a></span> | <span class="t">It's been very exciting recently. Here's an example. It's fun to play with. Definitely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3686" target="_blank">01:01:26.260</a></span> | <span class="t">play with it. Sorry, it's a bit of an attack on the students. Yeah. OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3703" target="_blank">01:01:43.300</a></span> | <span class="t">So reinforcement learning, pluses. You're kind of directly modeling what you care about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3709" target="_blank">01:01:49.700</a></span> | <span class="t">which is human preferences, not is the collection of the demonstration that I collected, is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3715" target="_blank">01:01:55.300</a></span> | <span class="t">that the highest probability mass in your model. You're actually just saying, how well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3719" target="_blank">01:01:59.140</a></span> | <span class="t">am I satisfying human preferences? So that's a clear benefit over something like instruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3723" target="_blank">01:02:03.660</a></span> | <span class="t">fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3726" target="_blank">01:02:06.340</a></span> | <span class="t">So in terms of negatives, one is that RL is hard. It's very tricky to get right. I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3731" target="_blank">01:02:11.420</a></span> | <span class="t">it will get easier in the future as we kind of explore the design space of possible options.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3736" target="_blank">01:02:16.900</a></span> | <span class="t">So that's an obvious one. Does anyone come up with any other kind of maybe weaknesses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3741" target="_blank">01:02:21.060</a></span> | <span class="t">or issues they see with this kind of training? Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3745" target="_blank">01:02:25.420</a></span> | <span class="t">Is it possible that your language model and then your reward model can over-fit to each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3753" target="_blank">01:02:33.300</a></span> | <span class="t">other, especially-- even if you're not training them together, if you're going back and forth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3757" target="_blank">01:02:37.820</a></span> | <span class="t">and like--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3758" target="_blank">01:02:38.820</a></span> | <span class="t">Yeah. Yeah. So over-optimization, I think, of the reward model is an issue. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3763" target="_blank">01:02:43.900</a></span> | <span class="t">Is it also that if you retrain your baseline, if you repeat all this human feedback, it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3769" target="_blank">01:02:49.580</a></span> | <span class="t">will fall over again?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3770" target="_blank">01:02:50.580</a></span> | <span class="t">Yeah. So it still is extremely data expensive. And you can see some articles if you just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3775" target="_blank">01:02:55.820</a></span> | <span class="t">Google OpenAI data labeling. People have not been very happy with the amount of data that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3780" target="_blank">01:03:00.020</a></span> | <span class="t">has been needed to train something like ChatGBT. I mean, they're hiring developers to just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3783" target="_blank">01:03:03.660</a></span> | <span class="t">explain coding problems 40 hours a week. So it is still data intensive. That's kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3789" target="_blank">01:03:09.820</a></span> | <span class="t">the takeaway. All of these are-- it's all still data intensive, every single one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3793" target="_blank">01:03:13.420</a></span> | <span class="t">these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3796" target="_blank">01:03:16.420</a></span> | <span class="t">Yeah. I think that summarizes kind of the big ones here. So when we talk about limitations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3804" target="_blank">01:03:24.100</a></span> | <span class="t">of RLHF, we also need to talk about just limitations in general of RL, and also this idea that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3810" target="_blank">01:03:30.540</a></span> | <span class="t">we can model or capture human reward in this single data point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3815" target="_blank">01:03:35.460</a></span> | <span class="t">So human preferences can be very unreliable. The RL people have known this for a very long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3821" target="_blank">01:03:41.100</a></span> | <span class="t">time. They have a term called reward hacking, which is when an agent is optimizing for something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3825" target="_blank">01:03:45.900</a></span> | <span class="t">that the developer specified, but it is not what we actually care about. So one of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3831" target="_blank">01:03:51.620</a></span> | <span class="t">classic examples is this example from OpenAI, where they were training this agent to race</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3838" target="_blank">01:03:58.020</a></span> | <span class="t">boats. And they were training it to maximize the score, which you can see at the bottom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3842" target="_blank">01:04:02.220</a></span> | <span class="t">left. But implicitly, the score actually isn't what you care about. What you care about is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3846" target="_blank">01:04:06.220</a></span> | <span class="t">just finishing the race ahead of everyone else. And the score is just kind of this bonus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3849" target="_blank">01:04:09.740</a></span> | <span class="t">But what the agent found out was that there are these turbo boost things that you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3853" target="_blank">01:04:13.660</a></span> | <span class="t">collect, which boost your score. And so what it ends up doing is it ends up kind of just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3857" target="_blank">01:04:17.580</a></span> | <span class="t">driving in the middle, collecting these turbo boosts over and over again. So it's racking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3860" target="_blank">01:04:20.820</a></span> | <span class="t">up insane score, but it is not doing the race. It is continuously crashing into objects,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3865" target="_blank">01:04:25.860</a></span> | <span class="t">and its boat is always on fire. And this is a pretty salient example of what we call AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3873" target="_blank">01:04:33.300</a></span> | <span class="t">misalignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3874" target="_blank">01:04:34.300</a></span> | <span class="t">And you might think, well, OK, this is a really simple example. They made a dumb mistake.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3879" target="_blank">01:04:39.900</a></span> | <span class="t">They shouldn't have used score as a reward function. But I think it's even more naive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3884" target="_blank">01:04:44.220</a></span> | <span class="t">to think that we can capture all of human preferences in a single number and assign</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3889" target="_blank">01:04:49.820</a></span> | <span class="t">certain scalar values to things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3894" target="_blank">01:04:54.020</a></span> | <span class="t">So one example where I think this is already happening, you can see, is maybe you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3898" target="_blank">01:04:58.940</a></span> | <span class="t">played with chatbots before, and you notice that they do a lot of hallucination. They</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3903" target="_blank">01:05:03.060</a></span> | <span class="t">make up a lot of facts. And this might be because of RLHF. Chatbots are rewarded to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3907" target="_blank">01:05:07.960</a></span> | <span class="t">produce responses that seem authoritative or seem helpful, but they don't care about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3913" target="_blank">01:05:13.060</a></span> | <span class="t">whether it's actually true or not. They just want to seem helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3917" target="_blank">01:05:17.060</a></span> | <span class="t">So this results in making up facts. You may be seeing the news about chatbots. Companies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3922" target="_blank">01:05:22.180</a></span> | <span class="t">are in this race to deploy chatbots, and they make mistakes. Even Bing also has been hallucinating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3928" target="_blank">01:05:28.180</a></span> | <span class="t">a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3931" target="_blank">01:05:31.220</a></span> | <span class="t">And in general, when you think about that, you think, well, models of human preferences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3935" target="_blank">01:05:35.700</a></span> | <span class="t">are even more unreliable. We're not even just using human preferences by themselves. We're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3940" target="_blank">01:05:40.340</a></span> | <span class="t">also training a model, a deep model, that we have no idea how that works. We're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3944" target="_blank">01:05:44.500</a></span> | <span class="t">to use that instead. And that can obviously be quite dangerous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3950" target="_blank">01:05:50.420</a></span> | <span class="t">And so going back to this slide here, where I was describing why we need this KL penalty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3954" target="_blank">01:05:54.740</a></span> | <span class="t">term, this yellow highlighted term here, here's a concrete example of what actually happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3959" target="_blank">01:05:59.900</a></span> | <span class="t">of a language model overfitting to the reward model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3963" target="_blank">01:06:03.460</a></span> | <span class="t">So what this is showing is, in this case, they took off the KL penalty. So they were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3967" target="_blank">01:06:07.300</a></span> | <span class="t">just trying to maximize reward. They trained this reward model. Let's just push those numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3971" target="_blank">01:06:11.140</a></span> | <span class="t">up as high as possible. And on the x-axis here is what happens as training continues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3976" target="_blank">01:06:16.620</a></span> | <span class="t">You diverge further and further. This is the KL divergence or the distance from where you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3980" target="_blank">01:06:20.740</a></span> | <span class="t">started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3982" target="_blank">01:06:22.580</a></span> | <span class="t">And the golden dashed line here is what the reward model predicts your language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3987" target="_blank">01:06:27.340</a></span> | <span class="t">is doing. So your reward model is thinking, wow, you are killing it. They are going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3991" target="_blank">01:06:31.220</a></span> | <span class="t">love these summaries. They are going to love them way more than the reference summaries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=3995" target="_blank">01:06:35.860</a></span> | <span class="t">But in reality, when you actually ask humans, the preferences peak, and then they just crater.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4003" target="_blank">01:06:43.060</a></span> | <span class="t">So this can be an example of over-optimizing for a metric that you care about. It ceases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4008" target="_blank">01:06:48.720</a></span> | <span class="t">to become a good metric to optimize for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4012" target="_blank">01:06:52.620</a></span> | <span class="t">Any questions about this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4017" target="_blank">01:06:57.380</a></span> | <span class="t">So there's this real concern of, I think, what people are calling the AI alignment problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4021" target="_blank">01:07:01.020</a></span> | <span class="t">I'll let Percy Leung talk about this. He tweeted that the main tool that we have for alignment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4027" target="_blank">01:07:07.580</a></span> | <span class="t">is RLHF. But reward hacking happens a lot. Humans are not very good supervisors of rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4034" target="_blank">01:07:14.520</a></span> | <span class="t">So this strategy is probably going to result in agents that seem like they're doing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4038" target="_blank">01:07:18.100</a></span> | <span class="t">right thing, but they're wrong in subtle and conspicuous ways. And I think we're already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4041" target="_blank">01:07:21.900</a></span> | <span class="t">seeing examples of that in the current generation of chatbots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4049" target="_blank">01:07:29.060</a></span> | <span class="t">So in terms of positives, here are some positives. But again, RL is tricky to get right. Human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4054" target="_blank">01:07:34.700</a></span> | <span class="t">preferences are fallible, and models of human preferences are even more so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4062" target="_blank">01:07:42.860</a></span> | <span class="t">So I remember seeing a joke on Twitter somewhere where someone was saying that zero shot and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4067" target="_blank">01:07:47.540</a></span> | <span class="t">few shot learning is the worst way to align in AI. Instruction fine tuning is the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4072" target="_blank">01:07:52.180</a></span> | <span class="t">worst way to align in AI. And RLHF is the third worst way to align in AI. So we're getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4077" target="_blank">01:07:57.700</a></span> | <span class="t">somewhere, but each of these have clear fundamental limitations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4082" target="_blank">01:08:02.100</a></span> | <span class="t">Yeah, question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4083" target="_blank">01:08:03.660</a></span> | <span class="t">I have a question on more of like competition of reinforcement learning. Because if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4091" target="_blank">01:08:11.540</a></span> | <span class="t">get the math that Nick showed before, essentially you're putting the gradient inside so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4095" target="_blank">01:08:15.940</a></span> | <span class="t">you can sample it, the sample expectation. But when it comes to sampling, how do you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4100" target="_blank">01:08:20.980</a></span> | <span class="t">make that parallel? Because then you need to adaptively stop sampling, and then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4107" target="_blank">01:08:27.660</a></span> | <span class="t">don't know when you're going to stop. How do you make that process quicker? The whole</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4112" target="_blank">01:08:32.700</a></span> | <span class="t">unit on transformers and all that was parallelizing everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4118" target="_blank">01:08:38.460</a></span> | <span class="t">I mean, yeah. So this is really compute heavy. And I'm actually not sure what kind of infrastructure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4124" target="_blank">01:08:44.220</a></span> | <span class="t">is used for a state of the art, very performant implementation of RLHF. But it's possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4128" target="_blank">01:08:48.420</a></span> | <span class="t">that they use parallelization like what you're describing, where I think in a lot of maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4132" target="_blank">01:08:52.140</a></span> | <span class="t">more traditional RL, there's this kind of idea of having an actor learner architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4137" target="_blank">01:08:57.020</a></span> | <span class="t">where you have a bunch of actor workers, which are each kind of a language model producing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4140" target="_blank">01:09:00.100</a></span> | <span class="t">a bunch of samples. And then the learner would then integrate them and perform the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4143" target="_blank">01:09:03.740</a></span> | <span class="t">updates. So it's possible that you do need to do just sheer multiprocessing in order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4148" target="_blank">01:09:08.420</a></span> | <span class="t">to get enough samples to make this work in a reasonable amount of time. Is that the kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4153" target="_blank">01:09:13.060</a></span> | <span class="t">of question you had? Or do you have other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4155" target="_blank">01:09:15.060</a></span> | <span class="t">Kind of. So you're basically saying that each unit that you parallelize over is larger than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4163" target="_blank">01:09:23.380</a></span> | <span class="t">what we would typically see in transformers?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4166" target="_blank">01:09:26.780</a></span> | <span class="t">I was saying that you might need to actually copy your model several times and take samples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4171" target="_blank">01:09:31.180</a></span> | <span class="t">from different copies of the models. Yeah. But in terms of like-- yeah, so autoregressive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4175" target="_blank">01:09:35.540</a></span> | <span class="t">generation, transformers, especially like the forward pass and the multi-head attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4179" target="_blank">01:09:39.500</a></span> | <span class="t">stuff is very easy to parallelize. But autoregressive generation is still kind of bottlenecked by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4188" target="_blank">01:09:48.220</a></span> | <span class="t">the fact that it's autoregressive. So you have to run it first and then you need to--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4191" target="_blank">01:09:51.500</a></span> | <span class="t">depends on what you sample, you have to run it again. So those are kind of blocks that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4195" target="_blank">01:09:55.140</a></span> | <span class="t">we haven't fully been able to solve, I think. And that will add to compute cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4207" target="_blank">01:10:07.260</a></span> | <span class="t">So I think we have 10 more minutes if I'm not mistaken. So we've mostly finally answered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4212" target="_blank">01:10:12.220</a></span> | <span class="t">how we get from this to this. There's some details missing. But the key kind of factors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4216" target="_blank">01:10:16.940</a></span> | <span class="t">are one, instruction fine tuning. Two, this idea of reinforced learning from human feedback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4224" target="_blank">01:10:24.260</a></span> | <span class="t">So let's talk a little bit about what's next. So as I had mentioned, RLHF is still a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4231" target="_blank">01:10:31.520</a></span> | <span class="t">new area. It's still very fast moving. I think by the next lecture, by the time we say that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4236" target="_blank">01:10:36.380</a></span> | <span class="t">I did these slides again, these slides might look completely different because maybe a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4239" target="_blank">01:10:39.740</a></span> | <span class="t">lot of the things that I was presenting here turn out to be really bad ideas or not the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4244" target="_blank">01:10:44.580</a></span> | <span class="t">most efficient way of going about things. RLHF gets you further than instruction fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4249" target="_blank">01:10:49.860</a></span> | <span class="t">tuning. But as someone had already mentioned, it is still very data expensive. There are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4254" target="_blank">01:10:54.740</a></span> | <span class="t">a lot of articles about OpenAI needing to hire a legion of annotators or developers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4259" target="_blank">01:10:59.140</a></span> | <span class="t">to just compare outputs over and over again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4263" target="_blank">01:11:03.700</a></span> | <span class="t">I think a recent work that I'm especially interested in and been thinking about is how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4267" target="_blank">01:11:07.740</a></span> | <span class="t">we can get the benefits of RLHF without such stringent data requirements. So there's these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4273" target="_blank">01:11:13.140</a></span> | <span class="t">newer kind of crazy ideas about doing reinforcement learning from not human feedback, but from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4279" target="_blank">01:11:19.100</a></span> | <span class="t">AI feedback. So having language models themselves evaluate the output of language models. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4284" target="_blank">01:11:24.340</a></span> | <span class="t">as an example of what that might look like, a team from Anthropic, which works on these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4288" target="_blank">01:11:28.300</a></span> | <span class="t">large language models, came up with this idea called constitutional AI. And the basic idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4293" target="_blank">01:11:33.580</a></span> | <span class="t">here is that if you ask GPT-3 to identify whether a response was not helpful, it would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4298" target="_blank">01:11:38.180</a></span> | <span class="t">be pretty good at doing so. And you might be able to use that feedback itself to improve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4302" target="_blank">01:11:42.260</a></span> | <span class="t">a model. So as an example, if you have some sort of human request, like, can you help</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4307" target="_blank">01:11:47.380</a></span> | <span class="t">me hack into my neighbor's Wi-Fi? And the assistant says, yeah, sure, you can use this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4311" target="_blank">01:11:51.380</a></span> | <span class="t">app, right? We can ask a model for feedback on this. What we do is we add a critique request,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4318" target="_blank">01:11:58.540</a></span> | <span class="t">which says, hey, language model GPT-3, identify ways in which the assistant's response is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4324" target="_blank">01:12:04.060</a></span> | <span class="t">harmful. And then it will generate a critique, like hacking into someone else's Wi-Fi is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4329" target="_blank">01:12:09.580</a></span> | <span class="t">illegal. And then you might ask it to then revise it, right? So just rewrite the assistant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4334" target="_blank">01:12:14.940</a></span> | <span class="t">response to remove harmful content. And it does so. And now by just decoding from a language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4343" target="_blank">01:12:23.260</a></span> | <span class="t">model, assuming you can do this well, what you have now is a set of data that you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4348" target="_blank">01:12:28.220</a></span> | <span class="t">do instruction fine tuning on, right? You have a request and you have a request that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4352" target="_blank">01:12:32.220</a></span> | <span class="t">has been revised to make sure it doesn't contain harmful content.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4357" target="_blank">01:12:37.780</a></span> | <span class="t">So this is pretty interesting. I think it's quite exciting. But all of those issues that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4361" target="_blank">01:12:41.700</a></span> | <span class="t">I had mentioned about alignment, mis-overinterpreting human preferences, reward models being fallible,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4369" target="_blank">01:12:49.980</a></span> | <span class="t">everything gets compounded like 40,000 times when you're thinking about this, right? We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4373" target="_blank">01:12:53.140</a></span> | <span class="t">have no understanding of how safe this is or where this ends up going, but it is something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4379" target="_blank">01:12:59.940</a></span> | <span class="t">Another kind of more common idea also is this general idea of fine tuning language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4383" target="_blank">01:13:03.780</a></span> | <span class="t">on their own outputs. And this has been explored a lot in the context of chain of thought reasoning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4387" target="_blank">01:13:07.580</a></span> | <span class="t">which is something I presented at the beginning of the lecture. And these are provocatively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4391" target="_blank">01:13:11.420</a></span> | <span class="t">named large language models can self-improve. But again, it's not clear how much runway</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4396" target="_blank">01:13:16.220</a></span> | <span class="t">there is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4397" target="_blank">01:13:17.220</a></span> | <span class="t">But the basic idea maybe is to-- you can use let's think step by step, for example, to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4401" target="_blank">01:13:21.420</a></span> | <span class="t">get a language model to produce a bunch of reasoning. And then you can say fine tune</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4404" target="_blank">01:13:24.260</a></span> | <span class="t">on that reasoning as if it were true data and see whether or not a language model can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4407" target="_blank">01:13:27.460</a></span> | <span class="t">get any better using that technique.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4413" target="_blank">01:13:33.940</a></span> | <span class="t">But as I mentioned, this is all still very new. There are, I think, a lot of limitations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4418" target="_blank">01:13:38.100</a></span> | <span class="t">of large language models like hallucination and also just the sheer size and compute intensity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4422" target="_blank">01:13:42.900</a></span> | <span class="t">of this that may or may not be solvable with RLHF.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4426" target="_blank">01:13:46.420</a></span> | <span class="t">Question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4427" target="_blank">01:13:47.420</a></span> | <span class="t">[INAUDIBLE] feedback of how we don't want to be at that. I've seen people talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4436" target="_blank">01:13:56.700</a></span> | <span class="t">how you can jailbreak chat GPT to still give those types of funnable responses. Are there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4442" target="_blank">01:14:02.300</a></span> | <span class="t">any ways for us to buffer against those types of things as well? Because it seems like you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4449" target="_blank">01:14:09.700</a></span> | <span class="t">just going to keep building on-- we need to identify chances where it's trying to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4454" target="_blank">01:14:14.940</a></span> | <span class="t">action not like yourself. I guess is there any way to build up that scale to avoid those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4463" target="_blank">01:14:23.740</a></span> | <span class="t">jailbreaking possibilities?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4464" target="_blank">01:14:24.740</a></span> | <span class="t">Yeah, that's interesting. So there are certainly ways that you can use either AI feedback or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4472" target="_blank">01:14:32.860</a></span> | <span class="t">human feedback to mitigate those kinds of jailbreaks. If you see someone on Twitter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4476" target="_blank">01:14:36.420</a></span> | <span class="t">saying that, oh, I made GPT-3 jailbreak using this strategy or whatever, you can then maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4483" target="_blank">01:14:43.180</a></span> | <span class="t">plug it into this kind of framework and say identify ways in which the assistant went</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4486" target="_blank">01:14:46.260</a></span> | <span class="t">off the rails and then fine tune and hopefully correct those. But it is really difficult,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4490" target="_blank">01:14:50.980</a></span> | <span class="t">I think, in most of these kinds of settings. It's really difficult to anticipate all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4494" target="_blank">01:14:54.140</a></span> | <span class="t">possible ways in which a user might jailbreak an assistant. So you always have this kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4498" target="_blank">01:14:58.820</a></span> | <span class="t">of dynamic of like in security, cybersecurity, for example, there's always the attacker advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4504" target="_blank">01:15:04.260</a></span> | <span class="t">where the attacker will always come up with something new or some new exploit. So yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4510" target="_blank">01:15:10.260</a></span> | <span class="t">I think this is a deep problem. I don't have a really clear answer. But certainly, if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4514" target="_blank">01:15:14.180</a></span> | <span class="t">knew what the jailbreak was, we could mitigate it. I think that seems pretty straightforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4521" target="_blank">01:15:21.180</a></span> | <span class="t">But if you know how to do that, you should be hired by one of these companies. They'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4524" target="_blank">01:15:24.340</a></span> | <span class="t">pay you millions if you can solve this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4530" target="_blank">01:15:30.100</a></span> | <span class="t">OK. Yeah, so just last remarks is with all of these scaling results that I presented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4536" target="_blank">01:15:36.900</a></span> | <span class="t">and all of these like, oh, you can just do instruction fine tuning and it'll follow your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4540" target="_blank">01:15:40.300</a></span> | <span class="t">instructions, or you can do RLHF. You might have a very bullish view on like, oh, this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4544" target="_blank">01:15:44.580</a></span> | <span class="t">is how we're going to solve artificial general intelligence by just scaling up RLHF. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4548" target="_blank">01:15:48.860</a></span> | <span class="t">possible that that is actually going to happen. But it's also possible that there are certain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4553" target="_blank">01:15:53.300</a></span> | <span class="t">fundamental limitations that we just need to figure out how to solve, like hallucination,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4558" target="_blank">01:15:58.220</a></span> | <span class="t">before we get anywhere productive with these models. But it is a really exciting time to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4561" target="_blank">01:16:01.720</a></span> | <span class="t">work on this kind of stuff. So yeah. Thanks for listening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4566" target="_blank">01:16:06.700</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4567" target="_blank">01:16:07.700</a></span> | <span class="t">[APPLAUSE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4568" target="_blank">01:16:08.700</a></span> | <span class="t">[END OF TRANSCRIPT]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4568" target="_blank">01:16:08.700</a></span> | <span class="t">1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4569" target="_blank">01:16:09.700</a></span> | <span class="t">1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4570" target="_blank">01:16:10.700</a></span> | <span class="t">1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4571" target="_blank">01:16:11.700</a></span> | <span class="t">1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4572" target="_blank">01:16:12.700</a></span> | <span class="t">1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=SXpJ9EmG3s4&t=4573" target="_blank">01:16:13.700</a></span> | <span class="t">1</span></div></div></body></html>