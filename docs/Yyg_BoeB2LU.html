<html><head><title>A Practical Guide to Efficient AI: Shelby Heinecke</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>A Practical Guide to Efficient AI: Shelby Heinecke</h2><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU"><img src="https://i.ytimg.com/vi_webp/Yyg_BoeB2LU/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./Yyg_BoeB2LU.html">Whisper Transcript</a> | <a href="./transcript_Yyg_BoeB2LU.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">I go to a lot of AI conferences. I go to AI research conferences, I go to you know more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=19" target="_blank">00:00:19.140</a></span> | <span class="t">general tech conferences, and what I absolutely love about this conference is that it's really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=23" target="_blank">00:00:23.520</a></span> | <span class="t">about the builders and it's really about the techniques that we need to get AI into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=27" target="_blank">00:00:27.960</a></span> | <span class="t">hands of our customers. And so we're all here in the AI space, we're all AI practitioners</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=33" target="_blank">00:00:33.080</a></span> | <span class="t">here, and we know that AI is developing at an unprecedented pace. It's pretty hard to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=39" target="_blank">00:00:39.040</a></span> | <span class="t">keep up with it, right? Every week, there's a new model, a new capability, a new feature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=44" target="_blank">00:00:44.260</a></span> | <span class="t">so much to keep up with. And when we see these new models, capabilities, and features, they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=51" target="_blank">00:00:51.480</a></span> | <span class="t">often shown to us as a demo or a prototype. And as builders and engineers here today, we all know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=59" target="_blank">00:00:59.480</a></span> | <span class="t">there is a big difference between a demo and a prototype and scaling and productionizing AI. So one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=66" target="_blank">00:01:06.600</a></span> | <span class="t">of the biggest ways that we can bridge that gap, we can go from having cool awesome demos to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=72" target="_blank">00:01:12.440</a></span> | <span class="t">bringing that to customers is with efficiency. So if we have techniques for making our AI efficient, we can get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=79" target="_blank">00:01:19.160</a></span> | <span class="t">closer to productization. And so that's what I want to tell you about today. I'm going to tell you about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=83" target="_blank">00:01:23.400</a></span> | <span class="t">some practical ways you can take away today to start making your AI models more efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=89" target="_blank">00:01:29.160</a></span> | <span class="t">So let me introduce myself. So I'm Shelby, and I lead an AI research team at Salesforce.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=96" target="_blank">00:01:36.600</a></span> | <span class="t">My team ships AI today. So we deliver, for example, LLM solutions to our data platform at Salesforce. The data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=105" target="_blank">00:01:45.560</a></span> | <span class="t">platform is the foundation of all of Salesforce that scale. Now, while we're delivering AI today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=112" target="_blank">00:01:52.440</a></span> | <span class="t">we're also envisioning what we'll need for tomorrow. And to do that, we've released over 15 cutting edge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=119" target="_blank">00:01:59.400</a></span> | <span class="t">research papers and agents, LLMs on device AI and more. And we've also released over six open source repos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=128" target="_blank">00:02:08.760</a></span> | <span class="t">So my team has released these repos, I'm going to talk about one of them today, so you'll get to see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=132" target="_blank">00:02:12.840</a></span> | <span class="t">And this is all in vain of pushing AI forward and getting the AI that we'll need for tomorrow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=141" target="_blank">00:02:21.240</a></span> | <span class="t">Now, a little bit about my personal background. I have a PhD in machine learning. So I focused on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=146" target="_blank">00:02:26.680</a></span> | <span class="t">developing learning algorithms that are sample and communication efficient. And I have a bachelor's and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=152" target="_blank">00:02:32.760</a></span> | <span class="t">master's in math as well. So if you're interested in learning more about my team, my background,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=157" target="_blank">00:02:37.720</a></span> | <span class="t">and want to connect on LinkedIn, feel free to scan the QR code. I'm always happy to chat with you all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=161" target="_blank">00:02:41.720</a></span> | <span class="t">Now, what about Salesforce? This is AI in the Fortune 500 section. Let's talk about Salesforce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=169" target="_blank">00:02:49.320</a></span> | <span class="t">and what we're doing. Salesforce has been deploying AI for 10 years, everyone, 10 years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=176" target="_blank">00:02:56.040</a></span> | <span class="t">So the AI research team was founded in 2014. And since then, Salesforce has accumulated over 300 AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=184" target="_blank">00:03:04.280</a></span> | <span class="t">patents, over 227 AI research papers, all in the last 10 years. And you can see here the map of all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=192" target="_blank">00:03:12.120</a></span> | <span class="t">deployments that have taken place. Okay. And at Salesforce, trust is our number one value. So we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=198" target="_blank">00:03:18.520</a></span> | <span class="t">don't just deliver, we don't just build and deliver AI in isolation. We build and deliver trusted AI. That</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=204" target="_blank">00:03:24.680</a></span> | <span class="t">is a key. So to do that, we're a part of six ethical AI councils. And we're also involved in the White</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=211" target="_blank">00:03:31.000</a></span> | <span class="t">House commitment for trusted AI. So I want to zoom in here past two years, that's where all the AI action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=218" target="_blank">00:03:38.680</a></span> | <span class="t">has been happening, right? The past two years, let's look at 2022 and 2023. What's Salesforce been up to?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=224" target="_blank">00:03:44.760</a></span> | <span class="t">Well, we've been deploying a lot of LLM products, right? If you look here, you'll see, you'll see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=230" target="_blank">00:03:50.520</a></span> | <span class="t">co-gen based products, you'll see service GPT, Einstein GPT, Tableau GPT. That's very similar to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=237" target="_blank">00:03:57.160</a></span> | <span class="t">the rest of the tech industry, right? Like if we zoom out the rest of the tech industry, we're deploying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=241" target="_blank">00:04:01.400</a></span> | <span class="t">LLM products. And now for us to do it at Salesforce, efficiency is key. Think about Salesforce scale,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=247" target="_blank">00:04:07.720</a></span> | <span class="t">think about Fortune 500 scale that we're talking here. Efficiency is key.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=253" target="_blank">00:04:13.080</a></span> | <span class="t">And we're all in the same boat here. We're all working on the same deployment environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=257" target="_blank">00:04:17.400</a></span> | <span class="t">So let's review that a little bit. When we've got an AI model, we're mostly deploying, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=261" target="_blank">00:04:21.880</a></span> | <span class="t">a lot of times we're deploying on a cloud, right? Private or public cloud. We're paying for resource</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=266" target="_blank">00:04:26.600</a></span> | <span class="t">consumption. We're paying for, you know, we're paying for GPU, CPU, disk space. We're paying for all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=271" target="_blank">00:04:31.800</a></span> | <span class="t">that. So we've got to keep that in mind, right? When we deploy, we're paying for that cost to serve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=275" target="_blank">00:04:35.560</a></span> | <span class="t">Or now we're seeing even on-prem solutions. Maybe we have an on-prem, maybe you have an on-prem cluster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=281" target="_blank">00:04:41.240</a></span> | <span class="t">So not only are you paying for that, you've got restricted GPUs to work within.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=285" target="_blank">00:04:45.320</a></span> | <span class="t">And more recently, and this is pretty exciting, small devices. We're seeing LLMs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=291" target="_blank">00:04:51.320</a></span> | <span class="t">being feasible on small devices. So if you guys were paying attention to the news in the past couple of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=297" target="_blank">00:04:57.240</a></span> | <span class="t">weeks, you'll see, you'll remember that Apple has announced their LLM on their newer devices. So this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=302" target="_blank">00:05:02.680</a></span> | <span class="t">is so exciting. And so if we're seeing it on iPhones and small devices like that, we can think maybe LLMs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=310" target="_blank">00:05:10.520</a></span> | <span class="t">and LMMs, multimodal models on tablets, on laptops, on edge devices. Now that's an even more challenging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=317" target="_blank">00:05:17.480</a></span> | <span class="t">situation, right? Small devices have even worse hardware, have even more resource constraints.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=322" target="_blank">00:05:22.760</a></span> | <span class="t">The point here is that when you're deploying AI models, you're deploying in these constrained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=327" target="_blank">00:05:27.960</a></span> | <span class="t">environments. We're never in a situation where we have infinite resources. So efficiency is going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=333" target="_blank">00:05:33.080</a></span> | <span class="t">key. So how do we make AI more efficient? So that's what I want to talk to you today about. I've summarized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=341" target="_blank">00:05:41.240</a></span> | <span class="t">it into five dimensions, five orthogonal directions that I would love for you to consider as you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=346" target="_blank">00:05:46.760</a></span> | <span class="t">as you're thinking about building your AI for customers and deploying. The first, and this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=351" target="_blank">00:05:51.560</a></span> | <span class="t">just scratching the surface. This is just scratching the surface, but I'm hoping these five dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=355" target="_blank">00:05:55.720</a></span> | <span class="t">will be easy for you to remember. The first is picking efficient architectures from the very beginning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=361" target="_blank">00:06:01.880</a></span> | <span class="t">from the very beginning. So this includes picking small models. I'm going to talk about that today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=366" target="_blank">00:06:06.440</a></span> | <span class="t">This includes using sophisticated architectures such as mixture of experts, for example. And if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=374" target="_blank">00:06:14.120</a></span> | <span class="t">building your architecture from scratch, it includes choosing efficient attention mechanisms and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=380" target="_blank">00:06:20.040</a></span> | <span class="t">So there's a lot we can say there. Today, I'm just going to touch on a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=382" target="_blank">00:06:22.920</a></span> | <span class="t">Moving on to the second one, efficient pre-training. Now, not a lot of us are doing pre-training. It's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=387" target="_blank">00:06:27.960</a></span> | <span class="t">really expensive thing to do. But if you're doing it, you know the GPU costs. You want to use mixed precision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=393" target="_blank">00:06:33.720</a></span> | <span class="t">training, scaling methods, among other methods here. So definitely make your pre-training efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=397" target="_blank">00:06:37.800</a></span> | <span class="t">Now, efficient fine-tuning. This is the case a lot of us are in today, efficient fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=404" target="_blank">00:06:44.200</a></span> | <span class="t">It's you want to pick methods that are not optimizing all of the weights, every single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=408" target="_blank">00:06:48.760</a></span> | <span class="t">weight, full fine-tuning. You want to pick methods that are that are only optimizing a subset of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=415" target="_blank">00:06:55.000</a></span> | <span class="t">weights. So think about Laura, QLaura and so on. And fourth, the fourth dimension, efficient inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=421" target="_blank">00:07:01.720</a></span> | <span class="t">So you've got your model. It's pre-trained. It's fine-tuned. You're ready to, you're almost ready to serve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=426" target="_blank">00:07:06.200</a></span> | <span class="t">it. How can we do that efficiently? We're paying for costs to serve, right? So with that, you want to consider</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=431" target="_blank">00:07:11.400</a></span> | <span class="t">post-training quantization, which I'll get into today, and speculative decoding. And there's many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=436" target="_blank">00:07:16.200</a></span> | <span class="t">others to cover as well. And finally, prompting. Prompting. We got to think about that. Prompts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=441" target="_blank">00:07:21.480</a></span> | <span class="t">you know, consume memory. They also directly affect latency as well. So you want your prompts to be as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=447" target="_blank">00:07:27.400</a></span> | <span class="t">concise as possible. Concise as possible. So think about template formatting and prompt compression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=453" target="_blank">00:07:33.480</a></span> | <span class="t">Now, with our limited time here today together, I'm going to dive into two crucial directions that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=459" target="_blank">00:07:39.800</a></span> | <span class="t">can take away with you and apply right away. The first direction is around efficient architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=465" target="_blank">00:07:45.560</a></span> | <span class="t">selection. I want to tell you about the power of small models. Small models are coming back, guys. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=470" target="_blank">00:07:50.360</a></span> | <span class="t">went big models. Small models are super efficient. We'll talk about it. Second, I want to go into efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=476" target="_blank">00:07:56.440</a></span> | <span class="t">inference. I want to tell you about post-training quantization. This is something that you could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=479" target="_blank">00:07:59.960</a></span> | <span class="t">you could actually apply at the end of the day on your model. So efficient and so quick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=483" target="_blank">00:08:03.720</a></span> | <span class="t">So let's get started with small LLMs, the power of these small LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=489" target="_blank">00:08:09.400</a></span> | <span class="t">So let's think about the past two years. As I mentioned, every week, new model, new feature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=498" target="_blank">00:08:18.040</a></span> | <span class="t">When we look at these LLMs that have been released, they're mostly pretty big. They're mostly pretty big.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=504" target="_blank">00:08:24.120</a></span> | <span class="t">So here are just a few. These are older models, but I just wanted to prove a point here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=507" target="_blank">00:08:27.960</a></span> | <span class="t">If we look at the Palm model, for example, 540 billion parameters, right? So parameters, again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=513" target="_blank">00:08:33.160</a></span> | <span class="t">everyone is the number of weights in that deep neural network. 540 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=517" target="_blank">00:08:37.960</a></span> | <span class="t">These other models, Bloom, Yum, 176 billion parameters, 100 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=524" target="_blank">00:08:44.840</a></span> | <span class="t">So those parameters have got to be stored in memory. They're all going to be used in computation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=529" target="_blank">00:08:49.560</a></span> | <span class="t">GPU computation, CPU computation. They're going to take up space. Long story short, these huge models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=535" target="_blank">00:08:55.480</a></span> | <span class="t">are resource hungry. They're going to take a lot of resources to train, certainly to pre-train, to fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=540" target="_blank">00:09:00.840</a></span> | <span class="t">tune and to serve. Now, in parallel, let's think over the past several months, we're seeing these smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=548" target="_blank">00:09:08.520</a></span> | <span class="t">models emerge. And when I think about small LLMs, I'm thinking models that are 13 billion parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=556" target="_blank">00:09:16.280</a></span> | <span class="t">or less. We're seeing these emerge, and for very good reason. They're emerging for good reason.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=560" target="_blank">00:09:20.280</a></span> | <span class="t">There's so many benefits to these smaller models. So as you can imagine, with less parameters, with less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=567" target="_blank">00:09:27.560</a></span> | <span class="t">weights, they consume less RAM, they consume less GPU, less CPU, less disk space, and they're just faster to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=574" target="_blank">00:09:34.440</a></span> | <span class="t">fine-tune. They're super resource efficient. This is exactly what we're looking for today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=578" target="_blank">00:09:38.680</a></span> | <span class="t">They're also low latency. Fewer weights means the forward pass is faster. There's just fewer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=584" target="_blank">00:09:44.120</a></span> | <span class="t">weights to go through, right? And both of those together, the resource efficiency, the low latency,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=590" target="_blank">00:09:50.120</a></span> | <span class="t">makes them perfect for additional deployment options. So not only can you take these small LLMs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=595" target="_blank">00:09:55.480</a></span> | <span class="t">and deploy them on the cloud, on on-prem, they can also be deployed on mobile if they're small enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=601" target="_blank">00:10:01.400</a></span> | <span class="t">They can be deployed on laptops for personal models. They can be deployed on edge devices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=605" target="_blank">00:10:05.640</a></span> | <span class="t">They're super nimble and super useful. So let me tell you about how... So what I want to do today is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=610" target="_blank">00:10:10.200</a></span> | <span class="t">tell you about a few small state-of-the-art LLMs to keep in mind as you're building your solutions for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=616" target="_blank">00:10:16.120</a></span> | <span class="t">your customers. So the first one I'm going to tell you about is Fi3. You guys may have heard of this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=623" target="_blank">00:10:23.080</a></span> | <span class="t">This is a 3.8 billion model, super, super small. And as I'm talking about small models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=629" target="_blank">00:10:29.240</a></span> | <span class="t">I showed you these 540 billion parameter model. Now we're talking about a 3.8 billion parameter model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=634" target="_blank">00:10:34.680</a></span> | <span class="t">Your first question might be, "What is the performance? Is the performance good?" So interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=639" target="_blank">00:10:39.880</a></span> | <span class="t">So Fi3 is actually a pretty... It's a very strong performing model. So as you can see here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=645" target="_blank">00:10:45.000</a></span> | <span class="t">I took this clip right from their technical report. Feel free to check it out. As you can see here, Fi3 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=649" target="_blank">00:10:49.880</a></span> | <span class="t">outperforming a very, very well-known 7B model, a model that's almost twice its size. So this 3B model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=657" target="_blank">00:10:57.160</a></span> | <span class="t">is pretty powerful for being so small. And now with that model being powerful, we're seeing even smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=665" target="_blank">00:11:05.240</a></span> | <span class="t">models emerge, even smaller models, because even smaller models will fit on edge devices,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=669" target="_blank">00:11:09.240</a></span> | <span class="t">on mobile, and so on. So what I want to point out to you is mobile LLM. It has less than 1B. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=675" target="_blank">00:11:15.480</a></span> | <span class="t">this has 350 million parameters, 350 million, not even a billion parameters. So super, super tiny.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=682" target="_blank">00:11:22.920</a></span> | <span class="t">And here's the key after fine tuning, after fine tuning, it's on par with the 7B model on tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=690" target="_blank">00:11:30.360</a></span> | <span class="t">So this is one of the takeaways I want to share with you is that the power of these small models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=694" target="_blank">00:11:34.280</a></span> | <span class="t">models, the way you use them is important. They're great after fine tuning. They are very competitive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=701" target="_blank">00:11:41.640</a></span> | <span class="t">That is what this is showing. And finally, I want to bring up a model that's really interesting for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=706" target="_blank">00:11:46.440</a></span> | <span class="t">function calling. So this Octopus model is a fine tuned model. It's fine tuned Gemma 2B. It's fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=713" target="_blank">00:11:53.560</a></span> | <span class="t">tuned on Android tasks. And again, a 2B model, they are showing after fine tuning, it's outperforming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=720" target="_blank">00:12:00.760</a></span> | <span class="t">GPT-4, Llama 7B on these Android tasks. So super, super promising. So definitely check out these small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=727" target="_blank">00:12:07.400</a></span> | <span class="t">LLMs. They have a ton of potential. And finally, I will go to our next topic, which is quantization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=734" target="_blank">00:12:14.200</a></span> | <span class="t">This is about inference. So what is quantization? Quantization is actually not a new topic. It's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=741" target="_blank">00:12:21.400</a></span> | <span class="t">a new topic. What's new is applying it to LLMs and LMMs. So the idea of quantization is to take a big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=748" target="_blank">00:12:28.280</a></span> | <span class="t">number and to map it to a smaller number. So what we want to do for quantization for LLMs is we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=755" target="_blank">00:12:35.720</a></span> | <span class="t">reduce the precision of the weights. So typically weights and LLMs, depending on the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=759" target="_blank">00:12:39.880</a></span> | <span class="t">could be 32-bit or 16-bit floats. What we want to do with what quantization does is reduces that 32 or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=767" target="_blank">00:12:47.880</a></span> | <span class="t">16 down to 8, down to 84 bits, down to actually you can specify just a smaller number of bits, reducing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=774" target="_blank">00:12:54.120</a></span> | <span class="t">precision of all those weights. So as you can imagine, that's hugely, hugely beneficial, massive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=779" target="_blank">00:12:59.640</a></span> | <span class="t">efficiency gains. So as you can see here, it's as you can imagine, if each weight was originally 32-bit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=785" target="_blank">00:13:05.160</a></span> | <span class="t">taking up 32-bit space, now we reduce it to 4-bit, it's going to take up a lot less space, it's going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=790" target="_blank">00:13:10.040</a></span> | <span class="t">consume a lot less memory, and it's going to consume a lot less CPU and GPU. So as you can see here really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=794" target="_blank">00:13:14.680</a></span> | <span class="t">quickly, just some models looking at these Llama models, 7B, 13B, 70B, the original, you can see the disk space it was taking up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=802" target="_blank">00:13:22.120</a></span> | <span class="t">After 4-bit quantization, it's taking up a fraction of the disk space. And now, what about latency?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=809" target="_blank">00:13:29.000</a></span> | <span class="t">So as the resource consumption comes down, the latency improves. So as we can see here in this study on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=815" target="_blank">00:13:35.080</a></span> | <span class="t">large multimodal models, so these are large LLMs, 16-bit was their original encoding, originally 16-bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=823" target="_blank">00:13:43.800</a></span> | <span class="t">Now if you look at the 4-bit quantization, you can see that the latency measured here as time to first token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=830" target="_blank">00:13:50.120</a></span> | <span class="t">has decreased. So lots of benefits. So reduced resource consumption faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=836" target="_blank">00:13:56.600</a></span> | <span class="t">Again though, the most important question is, is the performance still there? Are we making a huge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=842" target="_blank">00:14:02.040</a></span> | <span class="t">trade-off with this? And the good news is, no. This is pretty amazing. Quantization generally has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=849" target="_blank">00:14:09.240</a></span> | <span class="t">negligible effects on performance. So I want to show you that. So look here at this chart, at this graph,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=856" target="_blank">00:14:16.440</a></span> | <span class="t">and again, we're looking at LLMs. And you can see here, on this particular task, this well-known vision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=863" target="_blank">00:14:23.000</a></span> | <span class="t">language task, we took the LLMs and we measured performance on 16-bit and then 18-bit quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=869" target="_blank">00:14:29.880</a></span> | <span class="t">and 4-bit quantization. And as you can see, essentially no movement. 4-bit quantization was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=874" target="_blank">00:14:34.760</a></span> | <span class="t">essentially free. We could just quantize it with 4-bit and just enjoy reduced latency, enjoy reduced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=885" target="_blank">00:14:45.240</a></span> | <span class="t">resource consumption, enjoy improved latency, and no effect to performance, retained performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=890" target="_blank">00:14:50.920</a></span> | <span class="t">However, you can take this too far. There is a way to take this too far. So as you can see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=895" target="_blank">00:14:55.240</a></span> | <span class="t">when we quantize down to 3-bits, performance did drop. So evaluating your quantized model is super</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=901" target="_blank">00:15:01.320</a></span> | <span class="t">important. So don't just assume 4-bit is the answer. You definitely want to measure your quantized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=907" target="_blank">00:15:07.160</a></span> | <span class="t">performance. So really quickly, so you can get started on this today, you can quantize any of your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=913" target="_blank">00:15:13.160</a></span> | <span class="t">models, whether it's ML models, LLMs, LLMs, and so on. I want to just highlight a couple of frameworks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=918" target="_blank">00:15:18.920</a></span> | <span class="t">that are really awesome for that. So LLMACPP is one of the most well-known frameworks right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=923" target="_blank">00:15:23.480</a></span> | <span class="t">It's gaining a lot of traction. It has quantization from 16-bit all the way down to 1.5-bit. So pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=929" target="_blank">00:15:29.560</a></span> | <span class="t">crazy. Wide adoption. So actually you may not even need to quantize the models that you're using. Just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=934" target="_blank">00:15:34.840</a></span> | <span class="t">check hugging face. A lot of people are, as they're releasing their models, they're going, they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=937" target="_blank">00:15:37.960</a></span> | <span class="t">releasing the LLMACPP compatible quantization models too. So pretty awesome. And there's Python and Java</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=944" target="_blank">00:15:44.440</a></span> | <span class="t">wrappers. Second thing I just want to quickly mention, Onyx Runtime. Onyx has been around for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=950" target="_blank">00:15:50.440</a></span> | <span class="t">some time. If you've been around since the ML days, Onyx was around in the ML days. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=955" target="_blank">00:15:55.240</a></span> | <span class="t">they have some 8-bit quantization. And the beauty of Onyx is that it's compatible across so many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=961" target="_blank">00:16:01.560</a></span> | <span class="t">programming languages. So definitely take a look at these and there's a bunch of other ones you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=966" target="_blank">00:16:06.920</a></span> | <span class="t">consider too. Now, final point here, as we mentioned with quantization, now you have your quantized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=974" target="_blank">00:16:14.040</a></span> | <span class="t">model. I mentioned before, it is still important to evaluate your quantized model before you deploy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=978" target="_blank">00:16:18.040</a></span> | <span class="t">it. So I want to introduce you one of the open source repos that my team just developed. We just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=982" target="_blank">00:16:22.920</a></span> | <span class="t">released this maybe like a week ago. It's called Mobile AI Bench. And the point of this is an open</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=989" target="_blank">00:16:29.000</a></span> | <span class="t">source framework for you to evaluate your quantized models. Okay, so this is going to give you some rigor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=995" target="_blank">00:16:35.240</a></span> | <span class="t">before you actually deploy that quantized model, just to make sure that it is performing as expected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=1001" target="_blank">00:16:41.080</a></span> | <span class="t">So it's going to streamline evaluation, your evaluation across text tasks, trust and safety.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=1006" target="_blank">00:16:46.200</a></span> | <span class="t">That's really important. Make sure trust and safety doesn't degrade with quantization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=1009" target="_blank">00:16:49.800</a></span> | <span class="t">Vision language. Now, if you're interested in deploying your quantized models to device,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=1014" target="_blank">00:16:54.600</a></span> | <span class="t">we even have an iOS app right now. An iOS app that you can use that will measure the latency of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=1019" target="_blank">00:16:59.800</a></span> | <span class="t">quantized model and even measure the hardware usage. So you can even check like battery drainage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=1024" target="_blank">00:17:04.920</a></span> | <span class="t">for deploying these models. So feel free to check out our open source repo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=1029" target="_blank">00:17:09.000</a></span> | <span class="t">And with that, that wraps up the content for today. It was absolutely great being here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=1034" target="_blank">00:17:14.760</a></span> | <span class="t">So again, remember these five dimensions of AI efficiency as you're building and deploying your models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=1041" target="_blank">00:17:21.880</a></span> | <span class="t">Thank you so much. And if you're interested, feel free to check out these QR codes. Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Yyg_BoeB2LU&t=1055" target="_blank">00:17:35.560</a></span> | <span class="t">Thank you.</span></div></div></body></html>