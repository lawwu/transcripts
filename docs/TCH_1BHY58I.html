<html><head><title>Building makemore Part 2: MLP</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Building makemore Part 2: MLP</h2><a href="https://www.youtube.com/watch?v=TCH_1BHY58I"><img src="https://i.ytimg.com/vi_webp/TCH_1BHY58I/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=0">0:0</a> intro<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=108">1:48</a> Bengio et al. 2003 (MLP language model) paper walkthrough<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=543">9:3</a> (re-)building our training dataset<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=739">12:19</a> implementing the embedding lookup table<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1115">18:35</a> implementing the hidden layer + internals of torch.Tensor: storage, views<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1755">29:15</a> implementing the output layer<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1793">29:53</a> implementing the negative log likelihood loss<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1937">32:17</a> summary of the full network<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1969">32:49</a> introducing F.cross_entropy and why<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2276">37:56</a> implementing the training loop, overfitting one batch<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2485">41:25</a> training on the full dataset, minibatches<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2740">45:40</a> finding a good initial learning rate<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3200">53:20</a> splitting up the dataset into train/val/test splits and why<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3649">60:49</a> experiment: larger hidden layer<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3927">65:27</a> visualizing the character embeddings<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4036">67:16</a> experiment: larger embedding size<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4306">71:46</a> summary of our final code, conclusion<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4404">73:24</a> sampling from the model<br><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4495">74:55</a> google collab (new!!) notebook advertisement<br><br><div style="text-align: left;"><a href="./TCH_1BHY58I.html">Whisper Transcript</a> | <a href="./transcript_TCH_1BHY58I.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi everyone. Today we are continuing our implementation of Makemore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=5" target="_blank">00:00:05.000</a></span> | <span class="t">Now, in the last lecture, we implemented the bigram language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=8" target="_blank">00:00:08.000</a></span> | <span class="t">and we implemented it both using counts and also using a super simple neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=13" target="_blank">00:00:13.000</a></span> | <span class="t">that has a single linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=15" target="_blank">00:00:15.000</a></span> | <span class="t">Now, this is the Jupyter Notebook that we built out last lecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=20" target="_blank">00:00:20.000</a></span> | <span class="t">and we saw that the way we approached this is that we looked at only the single previous character,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=24" target="_blank">00:00:24.000</a></span> | <span class="t">and we predicted the distribution for the character that would go next in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=29" target="_blank">00:00:29.000</a></span> | <span class="t">And we did that by taking counts and normalizing them into probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=33" target="_blank">00:00:33.000</a></span> | <span class="t">so that each row here sums to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=36" target="_blank">00:00:36.000</a></span> | <span class="t">Now, this is all well and good if you only have one character of previous context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=41" target="_blank">00:00:41.000</a></span> | <span class="t">And this works, and it's approachable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=44" target="_blank">00:00:44.000</a></span> | <span class="t">The problem with this model, of course, is that the predictions from this model are not very good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=49" target="_blank">00:00:49.000</a></span> | <span class="t">because you only take one character of context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=51" target="_blank">00:00:51.000</a></span> | <span class="t">So the model didn't produce very name-like sounding things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=55" target="_blank">00:00:55.000</a></span> | <span class="t">Now, the problem with this approach, though, is that if we are to take more context into account</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=61" target="_blank">00:01:01.000</a></span> | <span class="t">when predicting the next character in a sequence, things quickly blow up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=64" target="_blank">00:01:04.000</a></span> | <span class="t">And this table, the size of this table, grows, and in fact it grows exponentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=69" target="_blank">00:01:09.000</a></span> | <span class="t">with the length of the context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=71" target="_blank">00:01:11.000</a></span> | <span class="t">Because if we only take a single character at a time, that's 27 possibilities of context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=75" target="_blank">00:01:15.000</a></span> | <span class="t">But if we take two characters in the past and try to predict the third one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=79" target="_blank">00:01:19.000</a></span> | <span class="t">suddenly the number of rows in this matrix, you can look at it that way, is 27 times 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=85" target="_blank">00:01:25.000</a></span> | <span class="t">So there's 729 possibilities for what could have come in the context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=90" target="_blank">00:01:30.000</a></span> | <span class="t">If we take three characters as the context, suddenly we have 20,000 possibilities of context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=97" target="_blank">00:01:37.000</a></span> | <span class="t">And so that's just way too many rows of this matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=101" target="_blank">00:01:41.000</a></span> | <span class="t">It's way too few counts for each possibility.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=105" target="_blank">00:01:45.000</a></span> | <span class="t">And the whole thing just kind of explodes and doesn't work very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=109" target="_blank">00:01:49.000</a></span> | <span class="t">So that's why today we're going to move on to this bullet point here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=112" target="_blank">00:01:52.000</a></span> | <span class="t">And we're going to implement a multilayer perceptron model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=115" target="_blank">00:01:55.000</a></span> | <span class="t">to predict the next character in a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=118" target="_blank">00:01:58.000</a></span> | <span class="t">And this modeling approach that we're going to adopt follows this paper, Ben-Ju et al., 2003.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=124" target="_blank">00:02:04.000</a></span> | <span class="t">So I have the paper pulled up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=126" target="_blank">00:02:06.000</a></span> | <span class="t">Now, this isn't the very first paper that proposed the use of multilayer perceptrons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=131" target="_blank">00:02:11.000</a></span> | <span class="t">or neural networks to predict the next character or token in a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=134" target="_blank">00:02:14.000</a></span> | <span class="t">But it's definitely one that was very influential around that time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=138" target="_blank">00:02:18.000</a></span> | <span class="t">It is very often cited to stand in for this idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=141" target="_blank">00:02:21.000</a></span> | <span class="t">And I think it's a very nice write-up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=143" target="_blank">00:02:23.000</a></span> | <span class="t">And so this is the paper that we're going to first look at and then implement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=147" target="_blank">00:02:27.000</a></span> | <span class="t">Now, this paper has 19 pages, so we don't have time to go into the full detail of this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=152" target="_blank">00:02:32.000</a></span> | <span class="t">But I invite you to read it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=153" target="_blank">00:02:33.000</a></span> | <span class="t">It's very readable, interesting, and has a lot of interesting ideas in it as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=157" target="_blank">00:02:37.000</a></span> | <span class="t">In the introduction, they described the exact same problem I just described.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=160" target="_blank">00:02:40.000</a></span> | <span class="t">And then to address it, they proposed the following model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=164" target="_blank">00:02:44.000</a></span> | <span class="t">Now, keep in mind that we are building a character-level language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=168" target="_blank">00:02:48.000</a></span> | <span class="t">So we're working on the level of characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=170" target="_blank">00:02:50.000</a></span> | <span class="t">In this paper, they have a vocabulary of 17,000 possible words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=174" target="_blank">00:02:54.000</a></span> | <span class="t">and they instead build a word-level language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=177" target="_blank">00:02:57.000</a></span> | <span class="t">But we're going to still stick with the characters, but we'll take the same modeling approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=181" target="_blank">00:03:01.000</a></span> | <span class="t">Now, what they do is basically they propose to take every one of these words, 17,000 words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=187" target="_blank">00:03:07.000</a></span> | <span class="t">and they're going to associate to each word a, say, 30-dimensional feature vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=193" target="_blank">00:03:13.000</a></span> | <span class="t">So every word is now embedded into a 30-dimensional space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=198" target="_blank">00:03:18.000</a></span> | <span class="t">You can think of it that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=199" target="_blank">00:03:19.000</a></span> | <span class="t">So we have 17,000 points or vectors in a 30-dimensional space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=204" target="_blank">00:03:24.000</a></span> | <span class="t">and you might imagine that's very crowded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=206" target="_blank">00:03:26.000</a></span> | <span class="t">That's a lot of points for a very small space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=209" target="_blank">00:03:29.000</a></span> | <span class="t">Now, in the beginning, these words are initialized completely randomly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=213" target="_blank">00:03:33.000</a></span> | <span class="t">so they're spread out at random.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=215" target="_blank">00:03:35.000</a></span> | <span class="t">But then we're going to tune these embeddings of these words using backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=220" target="_blank">00:03:40.000</a></span> | <span class="t">So during the course of training of this neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=222" target="_blank">00:03:42.000</a></span> | <span class="t">these points or vectors are going to basically move around in this space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=226" target="_blank">00:03:46.000</a></span> | <span class="t">And you might imagine that, for example, words that have very similar meanings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=229" target="_blank">00:03:49.000</a></span> | <span class="t">or that are indeed synonyms of each other might end up in a very similar part of the space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=234" target="_blank">00:03:54.000</a></span> | <span class="t">and conversely, words that mean very different things would go somewhere else in the space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=239" target="_blank">00:03:59.000</a></span> | <span class="t">Now, their modeling approach otherwise is identical to ours.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=243" target="_blank">00:04:03.000</a></span> | <span class="t">They are using a multilayer neural network to predict the next word, given the previous words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=247" target="_blank">00:04:07.000</a></span> | <span class="t">and to train the neural network, they are maximizing the log likelihood of the training data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=251" target="_blank">00:04:11.000</a></span> | <span class="t">just like we did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=253" target="_blank">00:04:13.000</a></span> | <span class="t">So the modeling approach itself is identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=256" target="_blank">00:04:16.000</a></span> | <span class="t">Now, here they have a concrete example of this intuition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=259" target="_blank">00:04:19.000</a></span> | <span class="t">Why does it work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=260" target="_blank">00:04:20.000</a></span> | <span class="t">Basically, suppose that, for example, you are trying to predict a dog was running in a blank.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=266" target="_blank">00:04:26.000</a></span> | <span class="t">Now, suppose that the exact phrase "a dog was running in a" has never occurred in the training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=272" target="_blank">00:04:32.000</a></span> | <span class="t">And here you are at sort of test time later, when the model is deployed somewhere,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=276" target="_blank">00:04:36.000</a></span> | <span class="t">and it's trying to make a sentence, and it's saying "a dog was running in a blank."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=281" target="_blank">00:04:41.000</a></span> | <span class="t">And because it's never encountered this exact phrase in the training set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=285" target="_blank">00:04:45.000</a></span> | <span class="t">you're out of distribution, as we say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=287" target="_blank">00:04:47.000</a></span> | <span class="t">Like, you don't have fundamentally any reason to suspect what might come next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=294" target="_blank">00:04:54.000</a></span> | <span class="t">But this approach actually allows you to get around that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=297" target="_blank">00:04:57.000</a></span> | <span class="t">because maybe you didn't see the exact phrase "a dog was running in a" something,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=301" target="_blank">00:05:01.000</a></span> | <span class="t">but maybe you've seen similar phrases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=303" target="_blank">00:05:03.000</a></span> | <span class="t">Maybe you've seen the phrase "the dog was running in a blank."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=306" target="_blank">00:05:06.000</a></span> | <span class="t">And maybe your network has learned that "a" and "the" are, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=310" target="_blank">00:05:10.000</a></span> | <span class="t">frequently are interchangeable with each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=312" target="_blank">00:05:12.000</a></span> | <span class="t">And so maybe it took the embedding for "a" and the embedding for "the,"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=316" target="_blank">00:05:16.000</a></span> | <span class="t">and it actually put them, like, nearby each other in the space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=319" target="_blank">00:05:19.000</a></span> | <span class="t">And so you can transfer knowledge through that embedding, and you can generalize in that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=323" target="_blank">00:05:23.000</a></span> | <span class="t">Similarly, the network could know that cats and dogs are animals,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=327" target="_blank">00:05:27.000</a></span> | <span class="t">and they co-occur in lots of very similar contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=330" target="_blank">00:05:30.000</a></span> | <span class="t">So even though you haven't seen this exact phrase,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=332" target="_blank">00:05:32.000</a></span> | <span class="t">or you haven't seen exactly "walking" or "running,"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=335" target="_blank">00:05:35.000</a></span> | <span class="t">you can, through the embedding space, transfer knowledge,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=339" target="_blank">00:05:39.000</a></span> | <span class="t">and you can generalize to novel scenarios.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=342" target="_blank">00:05:42.000</a></span> | <span class="t">So let's now scroll down to the diagram of the neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=345" target="_blank">00:05:45.000</a></span> | <span class="t">They have a nice diagram here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=347" target="_blank">00:05:47.000</a></span> | <span class="t">And in this example, we are taking three previous words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=351" target="_blank">00:05:51.000</a></span> | <span class="t">and we are trying to predict the fourth word in a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=356" target="_blank">00:05:56.000</a></span> | <span class="t">Now, these three previous words, as I mentioned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=359" target="_blank">00:05:59.000</a></span> | <span class="t">they have a vocabulary of 17,000 possible words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=363" target="_blank">00:06:03.000</a></span> | <span class="t">So every one of these basically are the index of the incoming word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=369" target="_blank">00:06:09.000</a></span> | <span class="t">And because there are 17,000 words, this is an integer between 0 and 16,999.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=377" target="_blank">00:06:17.000</a></span> | <span class="t">Now, there's also a lookup table that they call C.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=381" target="_blank">00:06:21.000</a></span> | <span class="t">This lookup table is a matrix that is 17,000 by, say, 30.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=386" target="_blank">00:06:26.000</a></span> | <span class="t">And basically what we're doing here is we're treating this as a lookup table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=389" target="_blank">00:06:29.000</a></span> | <span class="t">And so every index is plucking out a row of this embedding matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=395" target="_blank">00:06:35.000</a></span> | <span class="t">so that each index is converted to the 30-dimensional vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=399" target="_blank">00:06:39.000</a></span> | <span class="t">that corresponds to the embedding vector for that word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=403" target="_blank">00:06:43.000</a></span> | <span class="t">So here we have the input layer of 30 neurons for three words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=408" target="_blank">00:06:48.000</a></span> | <span class="t">making up 90 neurons in total.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=411" target="_blank">00:06:51.000</a></span> | <span class="t">And here they're saying that this matrix C is shared across all the words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=415" target="_blank">00:06:55.000</a></span> | <span class="t">So we're always indexing into the same matrix C over and over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=419" target="_blank">00:06:59.000</a></span> | <span class="t">for each one of these words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=422" target="_blank">00:07:02.000</a></span> | <span class="t">Next up is the hidden layer of this neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=425" target="_blank">00:07:05.000</a></span> | <span class="t">The size of this hidden neural layer of this neural net is a hyperparameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=429" target="_blank">00:07:09.000</a></span> | <span class="t">So we use the word hyperparameter when it's kind of like a design choice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=432" target="_blank">00:07:12.000</a></span> | <span class="t">up to the designer of the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=434" target="_blank">00:07:14.000</a></span> | <span class="t">And this can be as large as you'd like or as small as you'd like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=437" target="_blank">00:07:17.000</a></span> | <span class="t">So, for example, the size could be 100.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=439" target="_blank">00:07:19.000</a></span> | <span class="t">And we are going to go over multiple choices of the size of this hidden layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=443" target="_blank">00:07:23.000</a></span> | <span class="t">and we're going to evaluate how well they work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=446" target="_blank">00:07:26.000</a></span> | <span class="t">So say there were 100 neurons here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=448" target="_blank">00:07:28.000</a></span> | <span class="t">All of them would be fully connected to the 90 words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=451" target="_blank">00:07:31.000</a></span> | <span class="t">or 90 numbers that make up these three words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=455" target="_blank">00:07:35.000</a></span> | <span class="t">So this is a fully connected layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=458" target="_blank">00:07:38.000</a></span> | <span class="t">Then there's a 10-inch-long linearity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=460" target="_blank">00:07:40.000</a></span> | <span class="t">And then there's this output layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=462" target="_blank">00:07:42.000</a></span> | <span class="t">And because there are 17,000 possible words that could come next,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=466" target="_blank">00:07:46.000</a></span> | <span class="t">this layer has 17,000 neurons,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=469" target="_blank">00:07:49.000</a></span> | <span class="t">and all of them are fully connected to all of these neurons in the hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=475" target="_blank">00:07:55.000</a></span> | <span class="t">So there's a lot of parameters here because there's a lot of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=478" target="_blank">00:07:58.000</a></span> | <span class="t">So most computation is here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=480" target="_blank">00:08:00.000</a></span> | <span class="t">This is the expensive layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=482" target="_blank">00:08:02.000</a></span> | <span class="t">Now, there are 17,000 logits here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=485" target="_blank">00:08:05.000</a></span> | <span class="t">So on top of there, we have the softmax layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=487" target="_blank">00:08:07.000</a></span> | <span class="t">which we've seen in our previous video as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=489" target="_blank">00:08:09.000</a></span> | <span class="t">So every one of these logits is exponentiated,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=492" target="_blank">00:08:12.000</a></span> | <span class="t">and then everything is normalized to sum to 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=495" target="_blank">00:08:15.000</a></span> | <span class="t">to have a nice probability distribution for the next word in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=500" target="_blank">00:08:20.000</a></span> | <span class="t">Now, of course, during training, we actually have the label.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=503" target="_blank">00:08:23.000</a></span> | <span class="t">We have the identity of the next word in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=506" target="_blank">00:08:26.000</a></span> | <span class="t">That word or its index is used to pluck out the probability of that word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=513" target="_blank">00:08:33.000</a></span> | <span class="t">and then we are maximizing the probability of that word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=517" target="_blank">00:08:37.000</a></span> | <span class="t">with respect to the parameters of this neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=520" target="_blank">00:08:40.000</a></span> | <span class="t">So the parameters are the weights and biases of this output layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=524" target="_blank">00:08:44.000</a></span> | <span class="t">The weights and biases of this hidden layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=526" target="_blank">00:08:46.000</a></span> | <span class="t">and the embedding lookup table C,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=529" target="_blank">00:08:49.000</a></span> | <span class="t">and all of that is optimized using backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=532" target="_blank">00:08:52.000</a></span> | <span class="t">And these dashed arrows, ignore those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=535" target="_blank">00:08:55.000</a></span> | <span class="t">That represents a variation of a neural net</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=537" target="_blank">00:08:57.000</a></span> | <span class="t">that we are not going to explore in this video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=540" target="_blank">00:09:00.000</a></span> | <span class="t">So that's the setup, and now let's implement it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=542" target="_blank">00:09:02.000</a></span> | <span class="t">Okay, so I started a brand new notebook for this lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=545" target="_blank">00:09:05.000</a></span> | <span class="t">We are importing PyTorch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=547" target="_blank">00:09:07.000</a></span> | <span class="t">and we are importing Matplotlib so we can create figures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=550" target="_blank">00:09:10.000</a></span> | <span class="t">Then I am reading all the names into a list of words like I did before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=555" target="_blank">00:09:15.000</a></span> | <span class="t">and I'm showing the first eight right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=558" target="_blank">00:09:18.000</a></span> | <span class="t">Keep in mind that we have 32,000 in total.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=561" target="_blank">00:09:21.000</a></span> | <span class="t">These are just the first eight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=563" target="_blank">00:09:23.000</a></span> | <span class="t">And then here I'm building out the vocabulary of characters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=565" target="_blank">00:09:25.000</a></span> | <span class="t">and all the mappings from the characters as strings to integers and vice versa.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=571" target="_blank">00:09:31.000</a></span> | <span class="t">Now, the first thing we want to do is we want to compile the dataset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=574" target="_blank">00:09:34.000</a></span> | <span class="t">for the neural network, and I had to rewrite this code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=578" target="_blank">00:09:38.000</a></span> | <span class="t">I'll show you in a second what it looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=581" target="_blank">00:09:41.000</a></span> | <span class="t">So this is the code that I created for the dataset creation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=584" target="_blank">00:09:44.000</a></span> | <span class="t">So let me first run it, and then I'll briefly explain how this works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=588" target="_blank">00:09:48.000</a></span> | <span class="t">So first we're going to define something called block size,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=591" target="_blank">00:09:51.000</a></span> | <span class="t">and this is basically the context length of how many characters do we take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=595" target="_blank">00:09:55.000</a></span> | <span class="t">to predict the next one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=597" target="_blank">00:09:57.000</a></span> | <span class="t">So here in this example, we're taking three characters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=599" target="_blank">00:09:59.000</a></span> | <span class="t">to predict the fourth one, so we have a block size of three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=602" target="_blank">00:10:02.000</a></span> | <span class="t">That's the size of the block that supports the prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=606" target="_blank">00:10:06.000</a></span> | <span class="t">Then here I'm building out the x and y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=610" target="_blank">00:10:10.000</a></span> | <span class="t">The x are the input to the neural net, and the y are the labels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=614" target="_blank">00:10:14.000</a></span> | <span class="t">for each example inside x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=617" target="_blank">00:10:17.000</a></span> | <span class="t">Then I'm iterating over the first five words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=620" target="_blank">00:10:20.000</a></span> | <span class="t">I'm doing the first five just for efficiency while we are developing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=623" target="_blank">00:10:23.000</a></span> | <span class="t">all the code, but then later we are going to come here and erase this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=626" target="_blank">00:10:26.000</a></span> | <span class="t">so that we use the entire training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=629" target="_blank">00:10:29.000</a></span> | <span class="t">So here I'm printing the word "Emma," and here I'm basically showing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=633" target="_blank">00:10:33.000</a></span> | <span class="t">the examples that we can generate, the five examples that we can generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=637" target="_blank">00:10:37.000</a></span> | <span class="t">out of the single word "Emma."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=641" target="_blank">00:10:41.000</a></span> | <span class="t">So when we are given the context of just dot, dot, dot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=645" target="_blank">00:10:45.000</a></span> | <span class="t">the first character in a sequence is E.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=648" target="_blank">00:10:48.000</a></span> | <span class="t">In this context, the label is M.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=650" target="_blank">00:10:50.000</a></span> | <span class="t">When the context is this, the label is M, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=654" target="_blank">00:10:54.000</a></span> | <span class="t">So the way I build this out is first I start with a padded context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=657" target="_blank">00:10:57.000</a></span> | <span class="t">of just zero tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=660" target="_blank">00:11:00.000</a></span> | <span class="t">Then I iterate over all the characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=662" target="_blank">00:11:02.000</a></span> | <span class="t">I get the character in the sequence, and I basically build out the array y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=666" target="_blank">00:11:06.000</a></span> | <span class="t">of this current character, and the array x, which stores the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=670" target="_blank">00:11:10.000</a></span> | <span class="t">running context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=672" target="_blank">00:11:12.000</a></span> | <span class="t">Then here, see, I print everything, and here I crop the context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=676" target="_blank">00:11:16.000</a></span> | <span class="t">and enter the new character in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=679" target="_blank">00:11:19.000</a></span> | <span class="t">So this is kind of like a rolling window of context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=683" target="_blank">00:11:23.000</a></span> | <span class="t">Now we can change the block size here to, for example, four,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=686" target="_blank">00:11:26.000</a></span> | <span class="t">and in that case we would be predicting the fifth character</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=689" target="_blank">00:11:29.000</a></span> | <span class="t">in the previous four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=690" target="_blank">00:11:30.000</a></span> | <span class="t">Or it can be five, and then it would look like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=694" target="_blank">00:11:34.000</a></span> | <span class="t">Or it can be, say, ten, and then it would look something like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=698" target="_blank">00:11:38.000</a></span> | <span class="t">We're taking ten characters to predict the eleventh one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=701" target="_blank">00:11:41.000</a></span> | <span class="t">and we're always padding with dots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=703" target="_blank">00:11:43.000</a></span> | <span class="t">So let me bring this back to three just so that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=707" target="_blank">00:11:47.000</a></span> | <span class="t">what we have here in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=710" target="_blank">00:11:50.000</a></span> | <span class="t">And finally, the data set right now looks as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=713" target="_blank">00:11:53.000</a></span> | <span class="t">From these five words, we have created a data set of 32 examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=718" target="_blank">00:11:58.000</a></span> | <span class="t">and each input to the neural net is three integers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=721" target="_blank">00:12:01.000</a></span> | <span class="t">and we have a label that is also an integer, y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=725" target="_blank">00:12:05.000</a></span> | <span class="t">So x looks like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=727" target="_blank">00:12:07.000</a></span> | <span class="t">These are the individual examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=729" target="_blank">00:12:09.000</a></span> | <span class="t">And then y are the labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=732" target="_blank">00:12:12.000</a></span> | <span class="t">So given this, let's now write a neural network that takes these x's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=738" target="_blank">00:12:18.000</a></span> | <span class="t">and predicts the y's.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=739" target="_blank">00:12:19.000</a></span> | <span class="t">First, let's build the embedding lookup table C.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=743" target="_blank">00:12:23.000</a></span> | <span class="t">So we have 27 possible characters, and we're going to embed them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=746" target="_blank">00:12:26.000</a></span> | <span class="t">in a lower-dimensional space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=748" target="_blank">00:12:28.000</a></span> | <span class="t">In the paper, they have 17,000 words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=751" target="_blank">00:12:31.000</a></span> | <span class="t">and they embed them in spaces as small-dimensional as 30.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=756" target="_blank">00:12:36.000</a></span> | <span class="t">So they cram 17,000 words into 30-dimensional space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=760" target="_blank">00:12:40.000</a></span> | <span class="t">In our case, we have only 27 possible characters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=763" target="_blank">00:12:43.000</a></span> | <span class="t">so let's cram them in something as small as, to start with,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=766" target="_blank">00:12:46.000</a></span> | <span class="t">for example, a two-dimensional space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=768" target="_blank">00:12:48.000</a></span> | <span class="t">So this lookup table will be random numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=771" target="_blank">00:12:51.000</a></span> | <span class="t">and we'll have 27 rows, and we'll have two columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=776" target="_blank">00:12:56.000</a></span> | <span class="t">So each one of 27 characters will have a two-dimensional embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=782" target="_blank">00:13:02.000</a></span> | <span class="t">So that's our matrix C of embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=785" target="_blank">00:13:05.000</a></span> | <span class="t">in the beginning, initialized randomly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=788" target="_blank">00:13:08.000</a></span> | <span class="t">Now, before we embed all of the integers inside the input x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=791" target="_blank">00:13:11.000</a></span> | <span class="t">using this lookup table C,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=794" target="_blank">00:13:14.000</a></span> | <span class="t">let me actually just try to embed a single individual integer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=797" target="_blank">00:13:17.000</a></span> | <span class="t">like, say, 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=799" target="_blank">00:13:19.000</a></span> | <span class="t">So we get a sense of how this works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=802" target="_blank">00:13:22.000</a></span> | <span class="t">Now, one way this works, of course, is we can just take the C,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=805" target="_blank">00:13:25.000</a></span> | <span class="t">and we can index into row 5,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=808" target="_blank">00:13:28.000</a></span> | <span class="t">and that gives us a vector, the fifth row of C.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=811" target="_blank">00:13:31.000</a></span> | <span class="t">And this is one way to do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=814" target="_blank">00:13:34.000</a></span> | <span class="t">The other way that I presented in the previous lecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=817" target="_blank">00:13:37.000</a></span> | <span class="t">is actually seemingly different, but actually identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=820" target="_blank">00:13:40.000</a></span> | <span class="t">So in the previous lecture, what we did is we took these integers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=823" target="_blank">00:13:43.000</a></span> | <span class="t">and we used the one-hot encoding to first encode them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=826" target="_blank">00:13:46.000</a></span> | <span class="t">So f.one-hot, we want to encode integer 5,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=830" target="_blank">00:13:50.000</a></span> | <span class="t">and we want to tell it that the number of classes is 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=833" target="_blank">00:13:53.000</a></span> | <span class="t">So that's the 26-dimensional vector of all zeros,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=836" target="_blank">00:13:56.000</a></span> | <span class="t">except the fifth bit is turned on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=839" target="_blank">00:13:59.000</a></span> | <span class="t">Now, this actually doesn't work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=842" target="_blank">00:14:02.000</a></span> | <span class="t">The reason is that this input actually must be a torstadt tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=847" target="_blank">00:14:07.000</a></span> | <span class="t">And I'm making some of these errors intentionally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=849" target="_blank">00:14:09.000</a></span> | <span class="t">just so you get to see some errors and how to fix them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=852" target="_blank">00:14:12.000</a></span> | <span class="t">So this must be a tensor, not an int.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=854" target="_blank">00:14:14.000</a></span> | <span class="t">Fairly straightforward to fix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=856" target="_blank">00:14:16.000</a></span> | <span class="t">We get a one-hot vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=858" target="_blank">00:14:18.000</a></span> | <span class="t">The fifth dimension is 1, and the shape of this is 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=862" target="_blank">00:14:22.000</a></span> | <span class="t">And now notice that, just as I briefly alluded to in a previous video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=866" target="_blank">00:14:26.000</a></span> | <span class="t">if we take this one-hot vector and we multiply it by C,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=873" target="_blank">00:14:33.000</a></span> | <span class="t">then what would you expect?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=877" target="_blank">00:14:37.000</a></span> | <span class="t">Well, number one, first you'd expect an error,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=881" target="_blank">00:14:41.000</a></span> | <span class="t">because expected scalar type long, but found float.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=886" target="_blank">00:14:46.000</a></span> | <span class="t">So a little bit confusing, but the problem here is that one-hot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=890" target="_blank">00:14:50.000</a></span> | <span class="t">the data type of it, is long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=894" target="_blank">00:14:54.000</a></span> | <span class="t">It's a 64-bit integer, but this is a float tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=897" target="_blank">00:14:57.000</a></span> | <span class="t">And so PyTorch doesn't know how to multiply an int with a float,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=901" target="_blank">00:15:01.000</a></span> | <span class="t">and that's why we had to explicitly cast this to a float,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=904" target="_blank">00:15:04.000</a></span> | <span class="t">so that we can multiply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=906" target="_blank">00:15:06.000</a></span> | <span class="t">Now, the output actually here is identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=911" target="_blank">00:15:11.000</a></span> | <span class="t">And it's identical because of the way the matrix multiplication here works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=915" target="_blank">00:15:15.000</a></span> | <span class="t">We have the one-hot vector multiplying columns of C,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=920" target="_blank">00:15:20.000</a></span> | <span class="t">and because of all the zeros, they actually end up masking out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=923" target="_blank">00:15:23.000</a></span> | <span class="t">everything in C except for the fifth row, which is plucked out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=927" target="_blank">00:15:27.000</a></span> | <span class="t">And so we actually arrive at the same result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=930" target="_blank">00:15:30.000</a></span> | <span class="t">And that tells you that here we can interpret this first piece here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=934" target="_blank">00:15:34.000</a></span> | <span class="t">this embedding of the integer, we can either think of it as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=937" target="_blank">00:15:37.000</a></span> | <span class="t">the integer indexing into lookup table C,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=940" target="_blank">00:15:40.000</a></span> | <span class="t">but equivalently we can also think of this little piece here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=943" target="_blank">00:15:43.000</a></span> | <span class="t">as a first layer of this bigger neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=946" target="_blank">00:15:46.000</a></span> | <span class="t">This layer here has neurons that have no nonlinearity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=950" target="_blank">00:15:50.000</a></span> | <span class="t">There's no tanh. They're just linear neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=952" target="_blank">00:15:52.000</a></span> | <span class="t">And their weight matrix is C.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=955" target="_blank">00:15:55.000</a></span> | <span class="t">And then we are encoding integers into one-hot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=958" target="_blank">00:15:58.000</a></span> | <span class="t">and feeding those into a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=960" target="_blank">00:16:00.000</a></span> | <span class="t">And this first layer basically embeds them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=963" target="_blank">00:16:03.000</a></span> | <span class="t">Those are two equivalent ways of doing the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=966" target="_blank">00:16:06.000</a></span> | <span class="t">We're just going to index because it's much, much faster,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=968" target="_blank">00:16:08.000</a></span> | <span class="t">and we're going to discard this interpretation of one-hot inputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=972" target="_blank">00:16:12.000</a></span> | <span class="t">into neural nets, and we're just going to index integers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=975" target="_blank">00:16:15.000</a></span> | <span class="t">and use embedding tables.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=977" target="_blank">00:16:17.000</a></span> | <span class="t">Now, embedding a single integer like 5 is easy enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=981" target="_blank">00:16:21.000</a></span> | <span class="t">We can simply ask PyTorch to retrieve the fifth row of C,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=984" target="_blank">00:16:24.000</a></span> | <span class="t">or the row index 5 of C.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=988" target="_blank">00:16:28.000</a></span> | <span class="t">But how do we simultaneously embed all of these 32 by 3 integers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=992" target="_blank">00:16:32.000</a></span> | <span class="t">stored in array X?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=995" target="_blank">00:16:35.000</a></span> | <span class="t">Luckily, PyTorch indexing is fairly flexible and quite powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=998" target="_blank">00:16:38.000</a></span> | <span class="t">So it doesn't just work to ask for a single element 5 like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1004" target="_blank">00:16:44.000</a></span> | <span class="t">You can actually index using lists.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1006" target="_blank">00:16:46.000</a></span> | <span class="t">So, for example, we can get the rows 5, 6, and 7,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1009" target="_blank">00:16:49.000</a></span> | <span class="t">and this will just work like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1011" target="_blank">00:16:51.000</a></span> | <span class="t">We can index with a list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1013" target="_blank">00:16:53.000</a></span> | <span class="t">It doesn't just have to be a list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1015" target="_blank">00:16:55.000</a></span> | <span class="t">It can also be actually a tensor of integers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1018" target="_blank">00:16:58.000</a></span> | <span class="t">and we can index with that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1020" target="_blank">00:17:00.000</a></span> | <span class="t">So this is an integer tensor of 5, 6, 7,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1023" target="_blank">00:17:03.000</a></span> | <span class="t">and this will just work as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1026" target="_blank">00:17:06.000</a></span> | <span class="t">In fact, we can also, for example, repeat row 7</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1029" target="_blank">00:17:09.000</a></span> | <span class="t">and retrieve it multiple times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1031" target="_blank">00:17:11.000</a></span> | <span class="t">and that same index will just get embedded multiple times here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1036" target="_blank">00:17:16.000</a></span> | <span class="t">So here we are indexing with a one-dimensional tensor of integers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1040" target="_blank">00:17:20.000</a></span> | <span class="t">but it turns out that you can also index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1042" target="_blank">00:17:22.000</a></span> | <span class="t">with multi-dimensional tensors of integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1045" target="_blank">00:17:25.000</a></span> | <span class="t">Here we have a two-dimensional tensor of integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1048" target="_blank">00:17:28.000</a></span> | <span class="t">So we can simply just do C at x, and this just works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1054" target="_blank">00:17:34.000</a></span> | <span class="t">And the shape of this is 32 by 3, which is the original shape,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1059" target="_blank">00:17:39.000</a></span> | <span class="t">and now for every one of those 32 by 3 integers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1062" target="_blank">00:17:42.000</a></span> | <span class="t">we've retrieved the embedding vector here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1065" target="_blank">00:17:45.000</a></span> | <span class="t">So basically we have that as an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1069" target="_blank">00:17:49.000</a></span> | <span class="t">The 13th--or example index 13, the second dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1075" target="_blank">00:17:55.000</a></span> | <span class="t">is the integer 1, as an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1078" target="_blank">00:17:58.000</a></span> | <span class="t">And so here, if we do C of x, which gives us that array,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1083" target="_blank">00:18:03.000</a></span> | <span class="t">and then we index into 13 by 2 of that array,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1087" target="_blank">00:18:07.000</a></span> | <span class="t">then we get the embedding here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1090" target="_blank">00:18:10.000</a></span> | <span class="t">And you can verify that C at 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1094" target="_blank">00:18:14.000</a></span> | <span class="t">which is the integer at that location,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1096" target="_blank">00:18:16.000</a></span> | <span class="t">is indeed equal to this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1099" target="_blank">00:18:19.000</a></span> | <span class="t">You see they're equal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1101" target="_blank">00:18:21.000</a></span> | <span class="t">So basically, long story short, PyTorch indexing is awesome,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1105" target="_blank">00:18:25.000</a></span> | <span class="t">and to embed simultaneously all of the integers in x,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1109" target="_blank">00:18:29.000</a></span> | <span class="t">we can simply do C of x, and that is our embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1113" target="_blank">00:18:33.000</a></span> | <span class="t">and that just works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1115" target="_blank">00:18:35.000</a></span> | <span class="t">Now let's construct this layer here, the hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1118" target="_blank">00:18:38.000</a></span> | <span class="t">So we have that w1, as I'll call it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1122" target="_blank">00:18:42.000</a></span> | <span class="t">are these weights, which we will initialize randomly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1126" target="_blank">00:18:46.000</a></span> | <span class="t">Now the number of inputs to this layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1128" target="_blank">00:18:48.000</a></span> | <span class="t">is going to be 3 times 2, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1131" target="_blank">00:18:51.000</a></span> | <span class="t">Because we have two-dimensional embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1132" target="_blank">00:18:52.000</a></span> | <span class="t">and we have three of them, so the number of inputs is 6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1136" target="_blank">00:18:56.000</a></span> | <span class="t">And the number of neurons in this layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1138" target="_blank">00:18:58.000</a></span> | <span class="t">is a variable up to us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1140" target="_blank">00:19:00.000</a></span> | <span class="t">Let's use 100 neurons as an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1143" target="_blank">00:19:03.000</a></span> | <span class="t">And then biases will be also initialized randomly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1146" target="_blank">00:19:06.000</a></span> | <span class="t">as an example, and we just need 100 of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1151" target="_blank">00:19:11.000</a></span> | <span class="t">Now the problem with this is we can't simply--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1154" target="_blank">00:19:14.000</a></span> | <span class="t">normally we would take the input--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1155" target="_blank">00:19:15.000</a></span> | <span class="t">in this case, that's embedding--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1157" target="_blank">00:19:17.000</a></span> | <span class="t">and we'd like to multiply it with these weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1160" target="_blank">00:19:20.000</a></span> | <span class="t">and then we would like to add the bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1162" target="_blank">00:19:22.000</a></span> | <span class="t">This is roughly what we want to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1164" target="_blank">00:19:24.000</a></span> | <span class="t">But the problem here is that these embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1166" target="_blank">00:19:26.000</a></span> | <span class="t">are stacked up in the dimensions of this input tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1169" target="_blank">00:19:29.000</a></span> | <span class="t">So this will not work, this matrix multiplication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1172" target="_blank">00:19:32.000</a></span> | <span class="t">because this is a shape 32 by 3 by 2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1174" target="_blank">00:19:34.000</a></span> | <span class="t">and I can't multiply that by 6 by 100.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1177" target="_blank">00:19:37.000</a></span> | <span class="t">So somehow we need to concatenate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1179" target="_blank">00:19:39.000</a></span> | <span class="t">these inputs here together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1181" target="_blank">00:19:41.000</a></span> | <span class="t">so that we can do something along these lines,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1183" target="_blank">00:19:43.000</a></span> | <span class="t">which currently does not work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1185" target="_blank">00:19:45.000</a></span> | <span class="t">So how do we transform this 32 by 3 by 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1188" target="_blank">00:19:48.000</a></span> | <span class="t">into a 32 by 6</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1190" target="_blank">00:19:50.000</a></span> | <span class="t">so that we can actually perform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1192" target="_blank">00:19:52.000</a></span> | <span class="t">this multiplication over here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1194" target="_blank">00:19:54.000</a></span> | <span class="t">I'd like to show you that there are usually many ways</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1197" target="_blank">00:19:57.000</a></span> | <span class="t">of implementing what you'd like to do in Torch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1200" target="_blank">00:20:00.000</a></span> | <span class="t">and some of them will be faster, better, shorter, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1204" target="_blank">00:20:04.000</a></span> | <span class="t">And that's because Torch is a very large library,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1207" target="_blank">00:20:07.000</a></span> | <span class="t">and it's got lots and lots of functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1209" target="_blank">00:20:09.000</a></span> | <span class="t">So if we just go to the documentation and click on Torch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1211" target="_blank">00:20:11.000</a></span> | <span class="t">you'll see that my slider here is very tiny,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1214" target="_blank">00:20:14.000</a></span> | <span class="t">and that's because there are so many functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1216" target="_blank">00:20:16.000</a></span> | <span class="t">that you can call on these tensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1218" target="_blank">00:20:18.000</a></span> | <span class="t">to transform them, create them, multiply them, add them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1221" target="_blank">00:20:21.000</a></span> | <span class="t">perform all kinds of different operations on them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1224" target="_blank">00:20:24.000</a></span> | <span class="t">And so this is kind of like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1228" target="_blank">00:20:28.000</a></span> | <span class="t">the space of possibility, if you will.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1231" target="_blank">00:20:31.000</a></span> | <span class="t">Now, one of the things that you can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1233" target="_blank">00:20:33.000</a></span> | <span class="t">is we can Ctrl+F for concatenate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1236" target="_blank">00:20:36.000</a></span> | <span class="t">and we see that there's a function, Torch.cat,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1238" target="_blank">00:20:38.000</a></span> | <span class="t">short for concatenate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1240" target="_blank">00:20:40.000</a></span> | <span class="t">And this concatenates a given sequence of tensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1243" target="_blank">00:20:43.000</a></span> | <span class="t">in a given dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1245" target="_blank">00:20:45.000</a></span> | <span class="t">and these tensors must have the same shape, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1248" target="_blank">00:20:48.000</a></span> | <span class="t">So we can use the concatenate operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1250" target="_blank">00:20:50.000</a></span> | <span class="t">to, in a naive way, concatenate these three embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1254" target="_blank">00:20:54.000</a></span> | <span class="t">for each input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1256" target="_blank">00:20:56.000</a></span> | <span class="t">So in this case, we have m of the shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1260" target="_blank">00:21:00.000</a></span> | <span class="t">And really what we want to do is we want to retrieve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1263" target="_blank">00:21:03.000</a></span> | <span class="t">these three parts and concatenate them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1265" target="_blank">00:21:05.000</a></span> | <span class="t">So we want to grab all the examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1268" target="_blank">00:21:08.000</a></span> | <span class="t">We want to grab first the 0th index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1274" target="_blank">00:21:14.000</a></span> | <span class="t">and then all of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1277" target="_blank">00:21:17.000</a></span> | <span class="t">So this plucks out the 32x2 embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1282" target="_blank">00:21:22.000</a></span> | <span class="t">of just the first word here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1286" target="_blank">00:21:26.000</a></span> | <span class="t">And so basically we want this guy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1288" target="_blank">00:21:28.000</a></span> | <span class="t">we want the first dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1290" target="_blank">00:21:30.000</a></span> | <span class="t">and we want the second dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1292" target="_blank">00:21:32.000</a></span> | <span class="t">And these are the three pieces individually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1296" target="_blank">00:21:36.000</a></span> | <span class="t">And then we want to treat this as a sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1298" target="_blank">00:21:38.000</a></span> | <span class="t">and we want to Torch.cat on that sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1301" target="_blank">00:21:41.000</a></span> | <span class="t">So this is the list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1303" target="_blank">00:21:43.000</a></span> | <span class="t">Torch.cat takes a sequence of tensors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1307" target="_blank">00:21:47.000</a></span> | <span class="t">and then we have to tell it along which dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1309" target="_blank">00:21:49.000</a></span> | <span class="t">to concatenate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1311" target="_blank">00:21:51.000</a></span> | <span class="t">So in this case, all of these are 32x2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1313" target="_blank">00:21:53.000</a></span> | <span class="t">and we want to concatenate not across dimension 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1316" target="_blank">00:21:56.000</a></span> | <span class="t">but across dimension 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1318" target="_blank">00:21:58.000</a></span> | <span class="t">So passing in 1 gives us the result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1321" target="_blank">00:22:01.000</a></span> | <span class="t">that the shape of this is 32x6, exactly as we'd like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1325" target="_blank">00:22:05.000</a></span> | <span class="t">So that basically took 32 and squashed these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1328" target="_blank">00:22:08.000</a></span> | <span class="t">by concatenating them into 32x6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1331" target="_blank">00:22:11.000</a></span> | <span class="t">Now, this is kind of ugly because this code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1333" target="_blank">00:22:13.000</a></span> | <span class="t">would not generalize if we want to later change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1336" target="_blank">00:22:16.000</a></span> | <span class="t">the block size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1337" target="_blank">00:22:17.000</a></span> | <span class="t">Right now we have three inputs, three words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1340" target="_blank">00:22:20.000</a></span> | <span class="t">But what if we had five?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1342" target="_blank">00:22:22.000</a></span> | <span class="t">Then here we would have to change the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1344" target="_blank">00:22:24.000</a></span> | <span class="t">because I'm indexing directly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1346" target="_blank">00:22:26.000</a></span> | <span class="t">Well, Torch comes to rescue again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1348" target="_blank">00:22:28.000</a></span> | <span class="t">because there turns out to be a function called unbind,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1351" target="_blank">00:22:31.000</a></span> | <span class="t">and it removes a tensor dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1355" target="_blank">00:22:35.000</a></span> | <span class="t">So it removes a tensor dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1357" target="_blank">00:22:37.000</a></span> | <span class="t">returns a tuple of all slices along a given dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1360" target="_blank">00:22:40.000</a></span> | <span class="t">without it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1361" target="_blank">00:22:41.000</a></span> | <span class="t">So this is exactly what we need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1364" target="_blank">00:22:44.000</a></span> | <span class="t">And basically, when we call Torch.unbind of m</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1371" target="_blank">00:22:51.000</a></span> | <span class="t">and pass in dimension 1, index 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1376" target="_blank">00:22:56.000</a></span> | <span class="t">this gives us a list of tensors exactly equivalent to this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1382" target="_blank">00:23:02.000</a></span> | <span class="t">So running this gives us a line 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1387" target="_blank">00:23:07.000</a></span> | <span class="t">and it's exactly this list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1389" target="_blank">00:23:09.000</a></span> | <span class="t">And so we can call Torch.cat on it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1392" target="_blank">00:23:12.000</a></span> | <span class="t">and along the first dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1394" target="_blank">00:23:14.000</a></span> | <span class="t">And this works, and this shape is the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1399" target="_blank">00:23:19.000</a></span> | <span class="t">But now it doesn't matter if we have block size 3 or 5 or 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1403" target="_blank">00:23:23.000</a></span> | <span class="t">This will just work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1404" target="_blank">00:23:24.000</a></span> | <span class="t">So this is one way to do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1406" target="_blank">00:23:26.000</a></span> | <span class="t">But it turns out that in this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1408" target="_blank">00:23:28.000</a></span> | <span class="t">there's actually a significantly better and more efficient way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1411" target="_blank">00:23:31.000</a></span> | <span class="t">And this gives me an opportunity to hint</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1413" target="_blank">00:23:33.000</a></span> | <span class="t">at some of the internals of Torch.tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1416" target="_blank">00:23:36.000</a></span> | <span class="t">So let's create an array here of elements from 0 to 17.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1422" target="_blank">00:23:42.000</a></span> | <span class="t">And the shape of this is just 18.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1425" target="_blank">00:23:45.000</a></span> | <span class="t">It's a single vector of 18 numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1427" target="_blank">00:23:47.000</a></span> | <span class="t">It turns out that we can very quickly re-represent this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1431" target="_blank">00:23:51.000</a></span> | <span class="t">as different sized n-dimensional tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1434" target="_blank">00:23:54.000</a></span> | <span class="t">We do this by calling a view.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1437" target="_blank">00:23:57.000</a></span> | <span class="t">And we can say that actually this is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1439" target="_blank">00:23:59.000</a></span> | <span class="t">a single vector of 18.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1441" target="_blank">00:24:01.000</a></span> | <span class="t">This is a 2 by 9 tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1444" target="_blank">00:24:04.000</a></span> | <span class="t">Or alternatively, this is a 9 by 2 tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1448" target="_blank">00:24:08.000</a></span> | <span class="t">Or this is actually a 3 by 3 by 2 tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1452" target="_blank">00:24:12.000</a></span> | <span class="t">As long as the total number of elements here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1454" target="_blank">00:24:14.000</a></span> | <span class="t">multiply to be the same, this will just work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1458" target="_blank">00:24:18.000</a></span> | <span class="t">And in PyTorch, this operation, calling that view,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1462" target="_blank">00:24:22.000</a></span> | <span class="t">is extremely efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1464" target="_blank">00:24:24.000</a></span> | <span class="t">And the reason for that is that in each tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1467" target="_blank">00:24:27.000</a></span> | <span class="t">there's something called the underlying storage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1470" target="_blank">00:24:30.000</a></span> | <span class="t">And the storage is just the numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1473" target="_blank">00:24:33.000</a></span> | <span class="t">always as a one-dimensional vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1475" target="_blank">00:24:35.000</a></span> | <span class="t">And this is how this tensor is represented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1477" target="_blank">00:24:37.000</a></span> | <span class="t">in the computer memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1478" target="_blank">00:24:38.000</a></span> | <span class="t">It's always a one-dimensional vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1481" target="_blank">00:24:41.000</a></span> | <span class="t">But when we call that view,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1484" target="_blank">00:24:44.000</a></span> | <span class="t">we are manipulating some of the attributes of that tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1487" target="_blank">00:24:47.000</a></span> | <span class="t">that dictate how this one-dimensional sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1490" target="_blank">00:24:50.000</a></span> | <span class="t">is interpreted to be an n-dimensional tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1493" target="_blank">00:24:53.000</a></span> | <span class="t">And so what's happening here is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1495" target="_blank">00:24:55.000</a></span> | <span class="t">no memory is being changed, copied, moved, or created</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1498" target="_blank">00:24:58.000</a></span> | <span class="t">when we call that view.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1499" target="_blank">00:24:59.000</a></span> | <span class="t">The storage is identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1501" target="_blank">00:25:01.000</a></span> | <span class="t">But when you call that view,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1503" target="_blank">00:25:03.000</a></span> | <span class="t">some of the internal attributes of the view of this tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1507" target="_blank">00:25:07.000</a></span> | <span class="t">are being manipulated and changed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1509" target="_blank">00:25:09.000</a></span> | <span class="t">In particular, there's something called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1511" target="_blank">00:25:11.000</a></span> | <span class="t">storage offset, strides, and shapes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1513" target="_blank">00:25:13.000</a></span> | <span class="t">And those are manipulated so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1515" target="_blank">00:25:15.000</a></span> | <span class="t">this one-dimensional sequence of bytes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1517" target="_blank">00:25:17.000</a></span> | <span class="t">is seen as different n-dimensional arrays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1520" target="_blank">00:25:20.000</a></span> | <span class="t">There's a blog post here from Eric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1522" target="_blank">00:25:22.000</a></span> | <span class="t">called PyTorch Internals,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1524" target="_blank">00:25:24.000</a></span> | <span class="t">where he goes into some of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1526" target="_blank">00:25:26.000</a></span> | <span class="t">with respect to tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1527" target="_blank">00:25:27.000</a></span> | <span class="t">and how the view of a tensor is represented.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1530" target="_blank">00:25:30.000</a></span> | <span class="t">And this is really just like a logical construct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1533" target="_blank">00:25:33.000</a></span> | <span class="t">of representing the physical memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1535" target="_blank">00:25:35.000</a></span> | <span class="t">And so this is a pretty good blog post</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1538" target="_blank">00:25:38.000</a></span> | <span class="t">that you can go into.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1539" target="_blank">00:25:39.000</a></span> | <span class="t">I might also create an entire video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1541" target="_blank">00:25:41.000</a></span> | <span class="t">on the internals of TorchTensor and how this works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1544" target="_blank">00:25:44.000</a></span> | <span class="t">For here, we just note that this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1546" target="_blank">00:25:46.000</a></span> | <span class="t">an extremely efficient operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1548" target="_blank">00:25:48.000</a></span> | <span class="t">And if I delete this and come back to our EMP,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1553" target="_blank">00:25:53.000</a></span> | <span class="t">we see that the shape of our EMP is 32x3x2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1556" target="_blank">00:25:56.000</a></span> | <span class="t">But we can simply ask for PyTorch to view this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1560" target="_blank">00:26:00.000</a></span> | <span class="t">instead as a 32x6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1562" target="_blank">00:26:02.000</a></span> | <span class="t">And the way this gets flattened into a 32x6 array</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1567" target="_blank">00:26:07.000</a></span> | <span class="t">just happens that these two get stacked up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1572" target="_blank">00:26:12.000</a></span> | <span class="t">in a single row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1574" target="_blank">00:26:14.000</a></span> | <span class="t">And so that's basically the concatenation operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1576" target="_blank">00:26:16.000</a></span> | <span class="t">that we're after.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1577" target="_blank">00:26:17.000</a></span> | <span class="t">And you can verify that this actually gives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1579" target="_blank">00:26:19.000</a></span> | <span class="t">the exact same result as what we had before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1582" target="_blank">00:26:22.000</a></span> | <span class="t">So this is an element y=,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1584" target="_blank">00:26:24.000</a></span> | <span class="t">and you can see that all the elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1585" target="_blank">00:26:25.000</a></span> | <span class="t">of these two tensors are the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1588" target="_blank">00:26:28.000</a></span> | <span class="t">And so we get the exact same result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1591" target="_blank">00:26:31.000</a></span> | <span class="t">So long story short, we can actually just come here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1594" target="_blank">00:26:34.000</a></span> | <span class="t">and if we just view this as a 32x6 instead,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1600" target="_blank">00:26:40.000</a></span> | <span class="t">this multiplication will work and give us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1602" target="_blank">00:26:42.000</a></span> | <span class="t">the hidden states that we're after.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1604" target="_blank">00:26:44.000</a></span> | <span class="t">So if this is h, then h-shape is now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1608" target="_blank">00:26:48.000</a></span> | <span class="t">the 100-dimensional activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1611" target="_blank">00:26:51.000</a></span> | <span class="t">for every one of our 32 examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1613" target="_blank">00:26:53.000</a></span> | <span class="t">And this gives the desired result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1615" target="_blank">00:26:55.000</a></span> | <span class="t">Let me do two things here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1617" target="_blank">00:26:57.000</a></span> | <span class="t">Number one, let's not use 32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1619" target="_blank">00:26:59.000</a></span> | <span class="t">We can, for example, do something like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1622" target="_blank">00:27:02.000</a></span> | <span class="t">EMP.shape(0) so that we don't hard-code these numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1627" target="_blank">00:27:07.000</a></span> | <span class="t">And this would work for any size of this EMP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1630" target="_blank">00:27:10.000</a></span> | <span class="t">Or alternatively, we can also do -1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1632" target="_blank">00:27:12.000</a></span> | <span class="t">When we do -1, PyTorch will infer what this should be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1636" target="_blank">00:27:16.000</a></span> | <span class="t">Because the number of elements must be the same,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1638" target="_blank">00:27:18.000</a></span> | <span class="t">and we're saying that this is 6,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1640" target="_blank">00:27:20.000</a></span> | <span class="t">PyTorch will derive that this must be 32,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1642" target="_blank">00:27:22.000</a></span> | <span class="t">or whatever else it is if EMP is of different size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1646" target="_blank">00:27:26.000</a></span> | <span class="t">The other thing is here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1649" target="_blank">00:27:29.000</a></span> | <span class="t">one more thing I'd like to point out is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1652" target="_blank">00:27:32.000</a></span> | <span class="t">here when we do the concatenation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1655" target="_blank">00:27:35.000</a></span> | <span class="t">this actually is much less efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1657" target="_blank">00:27:37.000</a></span> | <span class="t">because this concatenation would create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1660" target="_blank">00:27:40.000</a></span> | <span class="t">a whole new tensor with a whole new storage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1662" target="_blank">00:27:42.000</a></span> | <span class="t">So new memory is being created</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1664" target="_blank">00:27:44.000</a></span> | <span class="t">because there's no way to concatenate tensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1666" target="_blank">00:27:46.000</a></span> | <span class="t">just by manipulating the view attributes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1668" target="_blank">00:27:48.000</a></span> | <span class="t">So this is inefficient and creates all kinds of new memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1672" target="_blank">00:27:52.000</a></span> | <span class="t">So let me delete this now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1675" target="_blank">00:27:55.000</a></span> | <span class="t">We don't need this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1677" target="_blank">00:27:57.000</a></span> | <span class="t">And here to calculate h, we want to also dot 10h</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1681" target="_blank">00:28:01.000</a></span> | <span class="t">of this to get our h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1686" target="_blank">00:28:06.000</a></span> | <span class="t">So these are now numbers between -1 and 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1689" target="_blank">00:28:09.000</a></span> | <span class="t">because of the 10h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1691" target="_blank">00:28:11.000</a></span> | <span class="t">And we have that the shape is 32 by 100.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1694" target="_blank">00:28:14.000</a></span> | <span class="t">And that is basically this hidden layer of activations here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1697" target="_blank">00:28:17.000</a></span> | <span class="t">for every one of our 32 examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1700" target="_blank">00:28:20.000</a></span> | <span class="t">Now there's one more thing I've lost over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1702" target="_blank">00:28:22.000</a></span> | <span class="t">that we have to be very careful with,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1704" target="_blank">00:28:24.000</a></span> | <span class="t">and that's this plus here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1706" target="_blank">00:28:26.000</a></span> | <span class="t">In particular, we want to make sure that the broadcasting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1709" target="_blank">00:28:29.000</a></span> | <span class="t">will do what we like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1711" target="_blank">00:28:31.000</a></span> | <span class="t">The shape of this is 32 by 100,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1713" target="_blank">00:28:33.000</a></span> | <span class="t">and v1's shape is 100.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1716" target="_blank">00:28:36.000</a></span> | <span class="t">So we see that the addition here will broadcast these two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1719" target="_blank">00:28:39.000</a></span> | <span class="t">And in particular, we have 32 by 100 broadcasting to 100.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1724" target="_blank">00:28:44.000</a></span> | <span class="t">So broadcasting will align on the right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1727" target="_blank">00:28:47.000</a></span> | <span class="t">create a fake dimension here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1729" target="_blank">00:28:49.000</a></span> | <span class="t">So this will become a 1 by 100 row vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1732" target="_blank">00:28:52.000</a></span> | <span class="t">And then it will copy vertically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1735" target="_blank">00:28:55.000</a></span> | <span class="t">for every one of these rows of 32</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1737" target="_blank">00:28:57.000</a></span> | <span class="t">and do an element-wise addition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1739" target="_blank">00:28:59.000</a></span> | <span class="t">So in this case, the correct thing will be happening</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1741" target="_blank">00:29:01.000</a></span> | <span class="t">because the same bias vector will be added</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1744" target="_blank">00:29:04.000</a></span> | <span class="t">to all the rows of this matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1748" target="_blank">00:29:08.000</a></span> | <span class="t">So that is correct. That's what we'd like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1750" target="_blank">00:29:10.000</a></span> | <span class="t">And it's always good practice to just make sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1753" target="_blank">00:29:13.000</a></span> | <span class="t">so that you don't shoot yourself in the foot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1755" target="_blank">00:29:15.000</a></span> | <span class="t">And finally, let's create the final layer here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1758" target="_blank">00:29:18.000</a></span> | <span class="t">So let's create w2 and v2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1763" target="_blank">00:29:23.000</a></span> | <span class="t">The input now is 100,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1765" target="_blank">00:29:25.000</a></span> | <span class="t">and the output number of neurons will be for us 27</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1768" target="_blank">00:29:28.000</a></span> | <span class="t">because we have 27 possible characters that come next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1772" target="_blank">00:29:32.000</a></span> | <span class="t">So the biases will be 27 as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1775" target="_blank">00:29:35.000</a></span> | <span class="t">So therefore, the logits, which are the outputs of this neural net,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1778" target="_blank">00:29:38.000</a></span> | <span class="t">are going to be h multiplied by w2 plus v2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1787" target="_blank">00:29:47.000</a></span> | <span class="t">Logits.shape is 32 by 27,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1790" target="_blank">00:29:50.000</a></span> | <span class="t">and the logits look good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1793" target="_blank">00:29:53.000</a></span> | <span class="t">Now, exactly as we saw in the previous video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1795" target="_blank">00:29:55.000</a></span> | <span class="t">we want to take these logits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1796" target="_blank">00:29:56.000</a></span> | <span class="t">and we want to first exponentiate them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1798" target="_blank">00:29:58.000</a></span> | <span class="t">to get our fake counts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1800" target="_blank">00:30:00.000</a></span> | <span class="t">and then we want to normalize them into a probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1803" target="_blank">00:30:03.000</a></span> | <span class="t">So prob is counts divide,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1805" target="_blank">00:30:05.000</a></span> | <span class="t">and now counts.sum along the first dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1810" target="_blank">00:30:10.000</a></span> | <span class="t">and keep them as true, exactly as in the previous video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1814" target="_blank">00:30:14.000</a></span> | <span class="t">And so prob.shape now is 32 by 27,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1820" target="_blank">00:30:20.000</a></span> | <span class="t">and you'll see that every row of prob sums to 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1825" target="_blank">00:30:25.000</a></span> | <span class="t">so it's normalized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1827" target="_blank">00:30:27.000</a></span> | <span class="t">So that gives us the probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1829" target="_blank">00:30:29.000</a></span> | <span class="t">Now, of course, we have the actual letter that comes next,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1831" target="_blank">00:30:31.000</a></span> | <span class="t">and that comes from this array y,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1834" target="_blank">00:30:34.000</a></span> | <span class="t">which we created during the dataset creation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1838" target="_blank">00:30:38.000</a></span> | <span class="t">So y is this last piece here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1840" target="_blank">00:30:40.000</a></span> | <span class="t">which is the identity of the next character in the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1842" target="_blank">00:30:42.000</a></span> | <span class="t">that we'd like to now predict.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1845" target="_blank">00:30:45.000</a></span> | <span class="t">So what we'd like to do now is, just as in the previous video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1848" target="_blank">00:30:48.000</a></span> | <span class="t">we'd like to index into the rows of prob,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1850" target="_blank">00:30:50.000</a></span> | <span class="t">and in each row, we'd like to pluck out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1852" target="_blank">00:30:52.000</a></span> | <span class="t">the probability assigned to the correct character,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1855" target="_blank">00:30:55.000</a></span> | <span class="t">as given here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1857" target="_blank">00:30:57.000</a></span> | <span class="t">So first, we have torch.arrange of 32,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1860" target="_blank">00:31:00.000</a></span> | <span class="t">which is kind of like an iterator over numbers from 0 to 31,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1866" target="_blank">00:31:06.000</a></span> | <span class="t">and then we can index into prob in the following way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1869" target="_blank">00:31:09.000</a></span> | <span class="t">Prob in torch.arrange of 32, which iterates the rows,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1874" target="_blank">00:31:14.000</a></span> | <span class="t">and then in each row, we'd like to grab this column,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1877" target="_blank">00:31:17.000</a></span> | <span class="t">as given by y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1879" target="_blank">00:31:19.000</a></span> | <span class="t">So this gives the current probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1881" target="_blank">00:31:21.000</a></span> | <span class="t">as assigned by this neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1883" target="_blank">00:31:23.000</a></span> | <span class="t">with this setting of its weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1885" target="_blank">00:31:25.000</a></span> | <span class="t">to the correct character in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1888" target="_blank">00:31:28.000</a></span> | <span class="t">And you can see here that this looks okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1890" target="_blank">00:31:30.000</a></span> | <span class="t">for some of these characters, like this is basically 0.2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1893" target="_blank">00:31:33.000</a></span> | <span class="t">but it doesn't look very good at all for many other characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1896" target="_blank">00:31:36.000</a></span> | <span class="t">Like this is 0.0701 probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1900" target="_blank">00:31:40.000</a></span> | <span class="t">and so the network thinks that some of these are extremely unlikely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1903" target="_blank">00:31:43.000</a></span> | <span class="t">But of course, we haven't trained the neural network yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1906" target="_blank">00:31:46.000</a></span> | <span class="t">This will improve, and ideally, all of these numbers here, of course, are 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1911" target="_blank">00:31:51.000</a></span> | <span class="t">because then we are correctly predicting the next character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1914" target="_blank">00:31:54.000</a></span> | <span class="t">Now, just as in the previous video, we want to take these probabilities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1917" target="_blank">00:31:57.000</a></span> | <span class="t">we want to look at the log probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1919" target="_blank">00:31:59.000</a></span> | <span class="t">and then we want to look at the average log probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1922" target="_blank">00:32:02.000</a></span> | <span class="t">and the negative of it to create the negative log likelihood loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1927" target="_blank">00:32:07.000</a></span> | <span class="t">So the loss here is 17,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1930" target="_blank">00:32:10.000</a></span> | <span class="t">and this is the loss that we'd like to minimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1932" target="_blank">00:32:12.000</a></span> | <span class="t">to get the network to predict the correct character in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1936" target="_blank">00:32:16.000</a></span> | <span class="t">Okay, so I rewrote everything here and made it a bit more respectable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1940" target="_blank">00:32:20.000</a></span> | <span class="t">So here's our dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1942" target="_blank">00:32:22.000</a></span> | <span class="t">Here's all the parameters that we defined.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1944" target="_blank">00:32:24.000</a></span> | <span class="t">I'm now using a generator to make it reproducible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1947" target="_blank">00:32:27.000</a></span> | <span class="t">I clustered all the parameters into a single list of parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1950" target="_blank">00:32:30.000</a></span> | <span class="t">so that, for example, it's easy to count them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1953" target="_blank">00:32:33.000</a></span> | <span class="t">and see that in total we currently have about 3,400 parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1956" target="_blank">00:32:36.000</a></span> | <span class="t">And this is the forward pass as we developed it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1959" target="_blank">00:32:39.000</a></span> | <span class="t">and we arrive at a single number here, the loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1962" target="_blank">00:32:42.000</a></span> | <span class="t">that is currently expressing how well this neural network works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1966" target="_blank">00:32:46.000</a></span> | <span class="t">with the current setting of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1968" target="_blank">00:32:48.000</a></span> | <span class="t">Now I would like to make it even more respectable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1971" target="_blank">00:32:51.000</a></span> | <span class="t">So in particular, see these lines here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1973" target="_blank">00:32:53.000</a></span> | <span class="t">where we take the logits and we calculate the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1977" target="_blank">00:32:57.000</a></span> | <span class="t">We're not actually reinventing the wheel here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1980" target="_blank">00:33:00.000</a></span> | <span class="t">This is just classification, and many people use classification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1984" target="_blank">00:33:04.000</a></span> | <span class="t">and that's why there is a functional.crossentropy function in PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1988" target="_blank">00:33:08.000</a></span> | <span class="t">to calculate this much more efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1991" target="_blank">00:33:11.000</a></span> | <span class="t">So we could just simply call f.crossentropy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1993" target="_blank">00:33:13.000</a></span> | <span class="t">and we can pass in the logits, and we can pass in the array of targets, y,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=1998" target="_blank">00:33:18.000</a></span> | <span class="t">and this calculates the exact same loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2002" target="_blank">00:33:22.000</a></span> | <span class="t">So in fact, we can simply put this here and erase these three lines,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2007" target="_blank">00:33:27.000</a></span> | <span class="t">and we're going to get the exact same result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2010" target="_blank">00:33:30.000</a></span> | <span class="t">Now there are actually many good reasons to prefer f.crossentropy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2013" target="_blank">00:33:33.000</a></span> | <span class="t">over rolling your own implementation like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2016" target="_blank">00:33:36.000</a></span> | <span class="t">I did this for educational reasons, but you'd never use this in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2019" target="_blank">00:33:39.000</a></span> | <span class="t">Why is that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2020" target="_blank">00:33:40.000</a></span> | <span class="t">Number one, when you use f.crossentropy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2023" target="_blank">00:33:43.000</a></span> | <span class="t">PyTorch will not actually create all these intermediate tensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2026" target="_blank">00:33:46.000</a></span> | <span class="t">because these are all new tensors in memory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2029" target="_blank">00:33:49.000</a></span> | <span class="t">and all this is fairly inefficient to run like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2032" target="_blank">00:33:52.000</a></span> | <span class="t">Instead, PyTorch will cluster up all these operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2035" target="_blank">00:33:55.000</a></span> | <span class="t">and very often have fused kernels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2038" target="_blank">00:33:58.000</a></span> | <span class="t">that very efficiently evaluate these expressions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2041" target="_blank">00:34:01.000</a></span> | <span class="t">that are sort of like clustered mathematical operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2044" target="_blank">00:34:04.000</a></span> | <span class="t">Number two, the backward pass can be made much more efficient,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2047" target="_blank">00:34:07.000</a></span> | <span class="t">and not just because it's a fused kernel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2050" target="_blank">00:34:10.000</a></span> | <span class="t">but also analytically and mathematically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2053" target="_blank">00:34:13.000</a></span> | <span class="t">it's often a very much simpler backward pass to implement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2057" target="_blank">00:34:17.000</a></span> | <span class="t">We actually saw this with micrograd.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2059" target="_blank">00:34:19.000</a></span> | <span class="t">You see here when we implemented 10H,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2062" target="_blank">00:34:22.000</a></span> | <span class="t">the forward pass of this operation to calculate the 10H</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2065" target="_blank">00:34:25.000</a></span> | <span class="t">was actually a fairly complicated mathematical expression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2068" target="_blank">00:34:28.000</a></span> | <span class="t">But because it's a clustered mathematical expression,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2071" target="_blank">00:34:31.000</a></span> | <span class="t">when we did the backward pass, we didn't individually backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2074" target="_blank">00:34:34.000</a></span> | <span class="t">through the x and the 2 times and the -1 and division, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2078" target="_blank">00:34:38.000</a></span> | <span class="t">We just said it's 1 - t^2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2080" target="_blank">00:34:40.000</a></span> | <span class="t">and that's a much simpler mathematical expression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2083" target="_blank">00:34:43.000</a></span> | <span class="t">And we were able to do this because we're able to reuse calculations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2086" target="_blank">00:34:46.000</a></span> | <span class="t">and because we are able to mathematically and analytically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2089" target="_blank">00:34:49.000</a></span> | <span class="t">derive the derivative, and often that expression simplifies mathematically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2093" target="_blank">00:34:53.000</a></span> | <span class="t">and so there's much less to implement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2096" target="_blank">00:34:56.000</a></span> | <span class="t">So not only can it be made more efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2098" target="_blank">00:34:58.000</a></span> | <span class="t">because it runs in a fused kernel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2100" target="_blank">00:35:00.000</a></span> | <span class="t">but also because the expressions can take a much simpler form mathematically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2105" target="_blank">00:35:05.000</a></span> | <span class="t">So that's number one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2107" target="_blank">00:35:07.000</a></span> | <span class="t">Number two, under the hood,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2109" target="_blank">00:35:09.000</a></span> | <span class="t">F dot cross entropy can also be significantly more numerically well-behaved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2115" target="_blank">00:35:15.000</a></span> | <span class="t">Let me show you an example of how this works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2119" target="_blank">00:35:19.000</a></span> | <span class="t">Suppose we have a logit of -2, 3, -3, 0, and 5,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2124" target="_blank">00:35:24.000</a></span> | <span class="t">and then we are taking the exponent of it and normalizing it to sum to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2127" target="_blank">00:35:27.000</a></span> | <span class="t">So when logits take on this value, everything is well and good,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2131" target="_blank">00:35:31.000</a></span> | <span class="t">and we get a nice probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2133" target="_blank">00:35:33.000</a></span> | <span class="t">Now consider what happens when some of these logits take on more extreme values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2137" target="_blank">00:35:37.000</a></span> | <span class="t">and that can happen during optimization of the neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2140" target="_blank">00:35:40.000</a></span> | <span class="t">Suppose that some of these numbers grow very negative,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2143" target="_blank">00:35:43.000</a></span> | <span class="t">like say -100, then actually everything will come out fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2147" target="_blank">00:35:47.000</a></span> | <span class="t">We still get probabilities that are well-behaved,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2152" target="_blank">00:35:52.000</a></span> | <span class="t">and they sum to 1, and everything is great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2154" target="_blank">00:35:54.000</a></span> | <span class="t">But because of the way the X works,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2156" target="_blank">00:35:56.000</a></span> | <span class="t">if you have very positive logits, like say +100 in here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2160" target="_blank">00:36:00.000</a></span> | <span class="t">you actually start to run into trouble, and we get not a number here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2164" target="_blank">00:36:04.000</a></span> | <span class="t">And the reason for that is that these counts have an inf here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2170" target="_blank">00:36:10.000</a></span> | <span class="t">So if you pass in a very negative number to exp,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2173" target="_blank">00:36:13.000</a></span> | <span class="t">you just get a very small number, very near 0, and that's fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2179" target="_blank">00:36:19.000</a></span> | <span class="t">But if you pass in a very positive number,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2181" target="_blank">00:36:21.000</a></span> | <span class="t">suddenly we run out of range in our floating-point number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2185" target="_blank">00:36:25.000</a></span> | <span class="t">that represents these counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2188" target="_blank">00:36:28.000</a></span> | <span class="t">So basically we're taking e and we're raising it to the power of 100,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2192" target="_blank">00:36:32.000</a></span> | <span class="t">and that gives us inf, because we've run out of dynamic range</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2195" target="_blank">00:36:35.000</a></span> | <span class="t">on this floating-point number that is count.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2198" target="_blank">00:36:38.000</a></span> | <span class="t">And so we cannot pass very large logits through this expression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2204" target="_blank">00:36:44.000</a></span> | <span class="t">Now let me reset these numbers to something reasonable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2207" target="_blank">00:36:47.000</a></span> | <span class="t">The way PyTorch solved this is that -</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2210" target="_blank">00:36:50.000</a></span> | <span class="t">you see how we have a well-behaved result here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2213" target="_blank">00:36:53.000</a></span> | <span class="t">It turns out that because of the normalization here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2216" target="_blank">00:36:56.000</a></span> | <span class="t">you can actually offset logits by any arbitrary constant value that you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2220" target="_blank">00:37:00.000</a></span> | <span class="t">So if I add 1 here, you actually get the exact same result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2224" target="_blank">00:37:04.000</a></span> | <span class="t">Or if I add 2, or if I subtract 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2228" target="_blank">00:37:08.000</a></span> | <span class="t">any offset will produce the exact same probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2232" target="_blank">00:37:12.000</a></span> | <span class="t">So because negative numbers are okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2235" target="_blank">00:37:15.000</a></span> | <span class="t">but positive numbers can actually overflow this exp,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2238" target="_blank">00:37:18.000</a></span> | <span class="t">what PyTorch does is it internally calculates the maximum value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2241" target="_blank">00:37:21.000</a></span> | <span class="t">that occurs in the logits, and it subtracts it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2244" target="_blank">00:37:24.000</a></span> | <span class="t">So in this case, it would subtract 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2247" target="_blank">00:37:27.000</a></span> | <span class="t">And so therefore the greatest number in logits will become 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2250" target="_blank">00:37:30.000</a></span> | <span class="t">and all the other numbers will become some negative numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2253" target="_blank">00:37:33.000</a></span> | <span class="t">And then the result of this is always well-behaved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2256" target="_blank">00:37:36.000</a></span> | <span class="t">So even if we have 100 here, previously, not good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2260" target="_blank">00:37:40.000</a></span> | <span class="t">But because PyTorch will subtract 100, this will work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2264" target="_blank">00:37:44.000</a></span> | <span class="t">And so there's many good reasons to call cross-entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2268" target="_blank">00:37:48.000</a></span> | <span class="t">Number one, the forward pass can be much more efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2271" target="_blank">00:37:51.000</a></span> | <span class="t">The backward pass can be much more efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2273" target="_blank">00:37:53.000</a></span> | <span class="t">And also things can be much more numerically well-behaved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2276" target="_blank">00:37:56.000</a></span> | <span class="t">Okay, so let's now set up the training of this neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2279" target="_blank">00:37:59.000</a></span> | <span class="t">We have the forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2282" target="_blank">00:38:02.000</a></span> | <span class="t">We don't need these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2285" target="_blank">00:38:05.000</a></span> | <span class="t">Instead, we have that loss is equal to the cross-entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2288" target="_blank">00:38:08.000</a></span> | <span class="t">That's the forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2290" target="_blank">00:38:10.000</a></span> | <span class="t">Then we need the backward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2292" target="_blank">00:38:12.000</a></span> | <span class="t">First, we want to set the gradients to be 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2295" target="_blank">00:38:15.000</a></span> | <span class="t">For p-in parameters, we want to make sure that p.grad is none,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2298" target="_blank">00:38:18.000</a></span> | <span class="t">which is the same as setting it to 0 in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2301" target="_blank">00:38:21.000</a></span> | <span class="t">And then loss.backward to populate those gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2304" target="_blank">00:38:24.000</a></span> | <span class="t">Once we have the gradients, we can do the parameter update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2307" target="_blank">00:38:27.000</a></span> | <span class="t">So for p-in parameters, we want to take all the data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2311" target="_blank">00:38:31.000</a></span> | <span class="t">and we want to nudge it learning rate times p.grad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2317" target="_blank">00:38:37.000</a></span> | <span class="t">And then we want to repeat this a few times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2324" target="_blank">00:38:44.000</a></span> | <span class="t">And let's print the loss here as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2329" target="_blank">00:38:49.000</a></span> | <span class="t">Now, this won't suffice, and it will create an error,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2331" target="_blank">00:38:51.000</a></span> | <span class="t">because we also have to go for p-in parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2334" target="_blank">00:38:54.000</a></span> | <span class="t">and we have to make sure that p.requires grad is set to true in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2340" target="_blank">00:39:00.000</a></span> | <span class="t">And this should just work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2344" target="_blank">00:39:04.000</a></span> | <span class="t">Okay, so we started off with loss of 17, and we're decreasing it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2348" target="_blank">00:39:08.000</a></span> | <span class="t">Let's run longer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2350" target="_blank">00:39:10.000</a></span> | <span class="t">And you see how the loss decreases a lot here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2353" target="_blank">00:39:13.000</a></span> | <span class="t">So if we just run for 1,000 times, we get a very, very low loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2361" target="_blank">00:39:21.000</a></span> | <span class="t">and that means that we're making very good predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2363" target="_blank">00:39:23.000</a></span> | <span class="t">Now, the reason that this is so straightforward right now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2367" target="_blank">00:39:27.000</a></span> | <span class="t">is because we're only overfitting 32 examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2372" target="_blank">00:39:32.000</a></span> | <span class="t">So we only have 32 examples of the first five words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2376" target="_blank">00:39:36.000</a></span> | <span class="t">and therefore it's very easy to make this neural net fit only these 32 examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2381" target="_blank">00:39:41.000</a></span> | <span class="t">because we have 3,400 parameters and only 32 examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2386" target="_blank">00:39:46.000</a></span> | <span class="t">So we're doing what's called overfitting a single batch of the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2390" target="_blank">00:39:50.000</a></span> | <span class="t">and getting a very low loss and good predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2394" target="_blank">00:39:54.000</a></span> | <span class="t">But that's just because we have so many parameters for so few examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2397" target="_blank">00:39:57.000</a></span> | <span class="t">so it's easy to make this be very low.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2400" target="_blank">00:40:00.000</a></span> | <span class="t">Now, we're not able to achieve exactly zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2402" target="_blank">00:40:02.000</a></span> | <span class="t">and the reason for that is we can, for example, look at logits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2406" target="_blank">00:40:06.000</a></span> | <span class="t">which are being predicted,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2408" target="_blank">00:40:08.000</a></span> | <span class="t">and we can look at the max along the first dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2413" target="_blank">00:40:13.000</a></span> | <span class="t">And in PyTorch, max reports both the actual values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2417" target="_blank">00:40:17.000</a></span> | <span class="t">that take on the maximum number but also the indices of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2422" target="_blank">00:40:22.000</a></span> | <span class="t">And you'll see that the indices are very close to the labels,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2426" target="_blank">00:40:26.000</a></span> | <span class="t">but in some cases, they differ.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2428" target="_blank">00:40:28.000</a></span> | <span class="t">For example, in this very first example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2431" target="_blank">00:40:31.000</a></span> | <span class="t">the predicted index is 19, but the label is 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2435" target="_blank">00:40:35.000</a></span> | <span class="t">And we're not able to make loss be zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2437" target="_blank">00:40:37.000</a></span> | <span class="t">and fundamentally that's because here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2440" target="_blank">00:40:40.000</a></span> | <span class="t">the very first or the zeroth index is the example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2443" target="_blank">00:40:43.000</a></span> | <span class="t">where dot, dot, dot is supposed to predict E,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2445" target="_blank">00:40:45.000</a></span> | <span class="t">but you see how dot, dot, dot is also supposed to predict an O,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2449" target="_blank">00:40:49.000</a></span> | <span class="t">and dot, dot, dot is also supposed to predict an I and an S as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2453" target="_blank">00:40:53.000</a></span> | <span class="t">And so basically E, O, A, or S are all possible outcomes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2457" target="_blank">00:40:57.000</a></span> | <span class="t">in a training set for the exact same input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2460" target="_blank">00:41:00.000</a></span> | <span class="t">So we're not able to completely overfit and make the loss be exactly zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2466" target="_blank">00:41:06.000</a></span> | <span class="t">but we're getting very close in the cases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2469" target="_blank">00:41:09.000</a></span> | <span class="t">where there's a unique input for a unique output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2472" target="_blank">00:41:12.000</a></span> | <span class="t">In those cases, we do what's called overfit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2475" target="_blank">00:41:15.000</a></span> | <span class="t">and we basically get the exact same and the exact correct result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2479" target="_blank">00:41:19.000</a></span> | <span class="t">So now all we have to do is we just need to make sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2482" target="_blank">00:41:22.000</a></span> | <span class="t">that we read in the full dataset and optimize the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2485" target="_blank">00:41:25.000</a></span> | <span class="t">Okay, so let's swing back up where we created the dataset,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2489" target="_blank">00:41:29.000</a></span> | <span class="t">and we see that here we only use the first five words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2492" target="_blank">00:41:32.000</a></span> | <span class="t">So let me now erase this, and let me erase the print statements,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2495" target="_blank">00:41:35.000</a></span> | <span class="t">otherwise we'd be printing way too much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2498" target="_blank">00:41:38.000</a></span> | <span class="t">And so when we process the full dataset of all the words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2501" target="_blank">00:41:41.000</a></span> | <span class="t">we now have 228,000 examples instead of just 32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2505" target="_blank">00:41:45.000</a></span> | <span class="t">So let's now scroll back down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2507" target="_blank">00:41:47.000</a></span> | <span class="t">The dataset is much larger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2509" target="_blank">00:41:49.000</a></span> | <span class="t">We initialize the weights, the same number of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2512" target="_blank">00:41:52.000</a></span> | <span class="t">They all require gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2514" target="_blank">00:41:54.000</a></span> | <span class="t">And then let's push this print I lost that item to be here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2518" target="_blank">00:41:58.000</a></span> | <span class="t">and let's just see how the optimization goes if we run this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2524" target="_blank">00:42:04.000</a></span> | <span class="t">Okay, so we started with a fairly high loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2526" target="_blank">00:42:06.000</a></span> | <span class="t">and then as we're optimizing, the loss is coming down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2532" target="_blank">00:42:12.000</a></span> | <span class="t">But you'll notice that it takes quite a bit of time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2534" target="_blank">00:42:14.000</a></span> | <span class="t">for every single iteration, so let's actually address that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2537" target="_blank">00:42:17.000</a></span> | <span class="t">because we're doing way too much work forwarding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2539" target="_blank">00:42:19.000</a></span> | <span class="t">and backwarding 228,000 examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2542" target="_blank">00:42:22.000</a></span> | <span class="t">In practice, what people usually do is they perform forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2546" target="_blank">00:42:26.000</a></span> | <span class="t">and backward pass and update on many batches of the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2549" target="_blank">00:42:29.000</a></span> | <span class="t">So what we will want to do is we want to randomly select</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2552" target="_blank">00:42:32.000</a></span> | <span class="t">some portion of the dataset, and that's a mini batch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2555" target="_blank">00:42:35.000</a></span> | <span class="t">and then only forward, backward, and update on that little mini batch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2558" target="_blank">00:42:38.000</a></span> | <span class="t">and then we iterate on those mini batches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2562" target="_blank">00:42:42.000</a></span> | <span class="t">So in PyTorch, we can, for example, use torch.randint.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2565" target="_blank">00:42:45.000</a></span> | <span class="t">We can generate numbers between 0 and 5 and make 32 of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2572" target="_blank">00:42:52.000</a></span> | <span class="t">I believe the size has to be a tuple in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2577" target="_blank">00:42:57.000</a></span> | <span class="t">So we can have a tuple, 32 of numbers between 0 and 5,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2582" target="_blank">00:43:02.000</a></span> | <span class="t">but actually we want x.shape of 0 here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2585" target="_blank">00:43:05.000</a></span> | <span class="t">And so this creates integers that index into our dataset,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2589" target="_blank">00:43:09.000</a></span> | <span class="t">and there's 32 of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2591" target="_blank">00:43:11.000</a></span> | <span class="t">So if our mini batch size is 32, then we can come here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2594" target="_blank">00:43:14.000</a></span> | <span class="t">and we can first do mini batch construct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2600" target="_blank">00:43:20.000</a></span> | <span class="t">So integers that we want to optimize in this single iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2605" target="_blank">00:43:25.000</a></span> | <span class="t">are in the ix, and then we want to index into x with ix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2612" target="_blank">00:43:32.000</a></span> | <span class="t">to only grab those rows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2614" target="_blank">00:43:34.000</a></span> | <span class="t">So we're only getting 32 rows of x, and therefore embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2617" target="_blank">00:43:37.000</a></span> | <span class="t">will again be 32 by 3 by 2, not 200,000 by 3 by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2623" target="_blank">00:43:43.000</a></span> | <span class="t">And then this ix has to be used not just to index into x,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2627" target="_blank">00:43:47.000</a></span> | <span class="t">but also to index into y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2630" target="_blank">00:43:50.000</a></span> | <span class="t">And now this should be mini batches, and this should be much, much faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2634" target="_blank">00:43:54.000</a></span> | <span class="t">So it's instant almost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2638" target="_blank">00:43:58.000</a></span> | <span class="t">So this way we can run many, many examples nearly instantly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2643" target="_blank">00:44:03.000</a></span> | <span class="t">and decrease the loss much, much faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2646" target="_blank">00:44:06.000</a></span> | <span class="t">Now because we're only dealing with mini batches,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2648" target="_blank">00:44:08.000</a></span> | <span class="t">the quality of our gradient is lower, so the direction is not as reliable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2652" target="_blank">00:44:12.000</a></span> | <span class="t">It's not the actual gradient direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2655" target="_blank">00:44:15.000</a></span> | <span class="t">But the gradient direction is good enough,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2657" target="_blank">00:44:17.000</a></span> | <span class="t">even when it's estimating on only 32 examples, that it is useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2662" target="_blank">00:44:22.000</a></span> | <span class="t">And so it's much better to have an approximate gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2665" target="_blank">00:44:25.000</a></span> | <span class="t">and just make more steps than it is to evaluate the exact gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2669" target="_blank">00:44:29.000</a></span> | <span class="t">and take fewer steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2671" target="_blank">00:44:31.000</a></span> | <span class="t">So that's why in practice this works quite well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2675" target="_blank">00:44:35.000</a></span> | <span class="t">So let's now continue the optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2678" target="_blank">00:44:38.000</a></span> | <span class="t">Let me take out this lost.item from here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2682" target="_blank">00:44:42.000</a></span> | <span class="t">and place it over here at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2686" target="_blank">00:44:46.000</a></span> | <span class="t">Okay, so we're hovering around 2.5 or so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2690" target="_blank">00:44:50.000</a></span> | <span class="t">However, this is only the loss for that mini batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2692" target="_blank">00:44:52.000</a></span> | <span class="t">So let's actually evaluate the loss here for all of x and for all of y,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2700" target="_blank">00:45:00.000</a></span> | <span class="t">just so we have a full sense of exactly how well the model is doing right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2705" target="_blank">00:45:05.000</a></span> | <span class="t">So right now we're at about 2.7 on the entire training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2709" target="_blank">00:45:09.000</a></span> | <span class="t">So let's run the optimization for a while.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2713" target="_blank">00:45:13.000</a></span> | <span class="t">We're at 2.6, 2.57, 2.53.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2722" target="_blank">00:45:22.000</a></span> | <span class="t">Okay, so one issue, of course, is we don't know if we're stepping too slow or too fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2729" target="_blank">00:45:29.000</a></span> | <span class="t">So this point one, I just guessed it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2731" target="_blank">00:45:31.000</a></span> | <span class="t">So one question is, how do you determine this learning rate?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2735" target="_blank">00:45:35.000</a></span> | <span class="t">And how do we gain confidence that we're stepping in the right sort of speed?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2740" target="_blank">00:45:40.000</a></span> | <span class="t">So I'll show you one way to determine a reasonable learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2743" target="_blank">00:45:43.000</a></span> | <span class="t">It works as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2744" target="_blank">00:45:44.000</a></span> | <span class="t">Let's reset our parameters to the initial settings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2751" target="_blank">00:45:51.000</a></span> | <span class="t">And now let's print in every step, but let's only do 10 steps or so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2758" target="_blank">00:45:58.000</a></span> | <span class="t">or maybe 100 steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2761" target="_blank">00:46:01.000</a></span> | <span class="t">We want to find a very reasonable search range, if you will.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2765" target="_blank">00:46:05.000</a></span> | <span class="t">So, for example, if this is very low, then we see that the loss is barely decreasing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2772" target="_blank">00:46:12.000</a></span> | <span class="t">So that's not -- that's too low, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2775" target="_blank">00:46:15.000</a></span> | <span class="t">So let's try this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2778" target="_blank">00:46:18.000</a></span> | <span class="t">Okay, so we're decreasing the loss, but not very quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2781" target="_blank">00:46:21.000</a></span> | <span class="t">So that's a pretty good low range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2783" target="_blank">00:46:23.000</a></span> | <span class="t">Now let's reset it again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2785" target="_blank">00:46:25.000</a></span> | <span class="t">And now let's try to find the place at which the loss kind of explodes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2789" target="_blank">00:46:29.000</a></span> | <span class="t">So maybe at negative 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2793" target="_blank">00:46:33.000</a></span> | <span class="t">Okay, we see that we're minimizing the loss, but you see how it's kind of unstable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2797" target="_blank">00:46:37.000</a></span> | <span class="t">It goes up and down quite a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2800" target="_blank">00:46:40.000</a></span> | <span class="t">So negative 1 is probably like a fast learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2803" target="_blank">00:46:43.000</a></span> | <span class="t">Let's try negative 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2806" target="_blank">00:46:46.000</a></span> | <span class="t">Okay, so this isn't optimizing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2808" target="_blank">00:46:48.000</a></span> | <span class="t">This is not working very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2809" target="_blank">00:46:49.000</a></span> | <span class="t">So negative 10 is way too big.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2811" target="_blank">00:46:51.000</a></span> | <span class="t">Negative 1 was already kind of big.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2815" target="_blank">00:46:55.000</a></span> | <span class="t">So, therefore, negative 1 was like somewhat reasonable if I reset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2820" target="_blank">00:47:00.000</a></span> | <span class="t">So I'm thinking that the right learning rate is somewhere between negative 0.001 and negative 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2828" target="_blank">00:47:08.000</a></span> | <span class="t">So the way we can do this here is we can use torch.learnspace.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2833" target="_blank">00:47:13.000</a></span> | <span class="t">And we want to basically do something like this, between 0 and 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2836" target="_blank">00:47:16.000</a></span> | <span class="t">But -- oh, number of steps is one more parameter that's required.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2842" target="_blank">00:47:22.000</a></span> | <span class="t">Let's do 1,000 steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2844" target="_blank">00:47:24.000</a></span> | <span class="t">This creates 1,000 numbers between 0.001 and 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2849" target="_blank">00:47:29.000</a></span> | <span class="t">But it doesn't really make sense to step between these linearly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2852" target="_blank">00:47:32.000</a></span> | <span class="t">So instead, let me create learning rate exponent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2856" target="_blank">00:47:36.000</a></span> | <span class="t">And instead of 0.001, this will be a negative 3, and this will be a 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2861" target="_blank">00:47:41.000</a></span> | <span class="t">And then the actual LRs that we want to search over are going to be 10 to the power of LRE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2868" target="_blank">00:47:48.000</a></span> | <span class="t">So now what we're doing is we're stepping linearly between the exponents of these learning rates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2872" target="_blank">00:47:52.000</a></span> | <span class="t">This is 0.001, and this is 1, because 10 to the power of 0 is 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2878" target="_blank">00:47:58.000</a></span> | <span class="t">And therefore, we are spaced exponentially in this interval.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2882" target="_blank">00:48:02.000</a></span> | <span class="t">So these are the candidate learning rates that we want to sort of like search over, roughly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2888" target="_blank">00:48:08.000</a></span> | <span class="t">So now what we're going to do is here we are going to run the optimization for 1,000 steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2895" target="_blank">00:48:15.000</a></span> | <span class="t">And instead of using a fixed number, we are going to use learning rate indexing into here, LRs of i, and make this i.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2906" target="_blank">00:48:26.000</a></span> | <span class="t">So basically, let me reset this to be, again, starting from random,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2911" target="_blank">00:48:31.000</a></span> | <span class="t">creating these learning rates between 0.001 and 1, but exponentially stepped.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2920" target="_blank">00:48:40.000</a></span> | <span class="t">And here what we're doing is we're iterating 1,000 times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2924" target="_blank">00:48:44.000</a></span> | <span class="t">We're going to use the learning rate that's in the beginning very, very low.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2929" target="_blank">00:48:49.000</a></span> | <span class="t">In the beginning, it's going to be 0.001, but by the end, it's going to be 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2934" target="_blank">00:48:54.000</a></span> | <span class="t">And we're going to step with that learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2937" target="_blank">00:48:57.000</a></span> | <span class="t">And now what we want to do is we want to keep track of the learning rates that we used,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2946" target="_blank">00:49:06.000</a></span> | <span class="t">and we want to look at the losses that resulted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2950" target="_blank">00:49:10.000</a></span> | <span class="t">And so here, let me track stats.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2954" target="_blank">00:49:14.000</a></span> | <span class="t">So LRI.append LR and LOSI.append LOS.item.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2963" target="_blank">00:49:23.000</a></span> | <span class="t">So again, reset everything and then run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2970" target="_blank">00:49:30.000</a></span> | <span class="t">And so basically, we started with a very low learning rate, and we went all the way up to a learning rate of -1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2976" target="_blank">00:49:36.000</a></span> | <span class="t">And now what we can do is we can plt.plot, and we can plot the two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2980" target="_blank">00:49:40.000</a></span> | <span class="t">So we can plot the learning rates on the x-axis and the losses we saw on the y-axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2986" target="_blank">00:49:46.000</a></span> | <span class="t">And often, you're going to find that your plot looks something like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2990" target="_blank">00:49:50.000</a></span> | <span class="t">where in the beginning, you had very low learning rates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2993" target="_blank">00:49:53.000</a></span> | <span class="t">So basically, anything--barely anything happened.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=2997" target="_blank">00:49:57.000</a></span> | <span class="t">Then we got to, like, a nice spot here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3000" target="_blank">00:50:00.000</a></span> | <span class="t">And then as we increased the learning rate enough, we basically started to be kind of unstable here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3006" target="_blank">00:50:06.000</a></span> | <span class="t">So a good learning rate turns out to be somewhere around here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3010" target="_blank">00:50:10.000</a></span> | <span class="t">And because we have LRI here, we actually may want to do not LR--not the learning rate, but the exponent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3022" target="_blank">00:50:22.000</a></span> | <span class="t">So that would be the LRE at i is maybe what we want to log.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3026" target="_blank">00:50:26.000</a></span> | <span class="t">So let me reset this and redo that calculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3030" target="_blank">00:50:30.000</a></span> | <span class="t">But now on the x-axis, we have the exponent of the learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3035" target="_blank">00:50:35.000</a></span> | <span class="t">And so we can see the exponent of the learning rate that is good to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3038" target="_blank">00:50:38.000</a></span> | <span class="t">It would be sort of like roughly in the valley here, because here the learning rates are just way too low.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3043" target="_blank">00:50:43.000</a></span> | <span class="t">And then here, we expect relatively good learning rates somewhere here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3047" target="_blank">00:50:47.000</a></span> | <span class="t">And then here, things are starting to explode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3049" target="_blank">00:50:49.000</a></span> | <span class="t">So somewhere around -1 as the exponent of the learning rate is a pretty good setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3054" target="_blank">00:50:54.000</a></span> | <span class="t">And 10 to the -1 is 0.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3057" target="_blank">00:50:57.000</a></span> | <span class="t">So 0.1 was actually a fairly good learning rate around here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3062" target="_blank">00:51:02.000</a></span> | <span class="t">And that's what we had in the initial setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3065" target="_blank">00:51:05.000</a></span> | <span class="t">But that's roughly how you would determine it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3068" target="_blank">00:51:08.000</a></span> | <span class="t">And so here now we can take out the tracking of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3073" target="_blank">00:51:13.000</a></span> | <span class="t">And we can just simply set LR to be 10 to the -1, or basically otherwise 0.1, as it was before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3081" target="_blank">00:51:21.000</a></span> | <span class="t">And now we have some confidence that this is actually a fairly good learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3084" target="_blank">00:51:24.000</a></span> | <span class="t">And so now what we can do is we can crank up the iterations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3088" target="_blank">00:51:28.000</a></span> | <span class="t">We can reset our optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3091" target="_blank">00:51:31.000</a></span> | <span class="t">And we can run for a pretty long time using this learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3096" target="_blank">00:51:36.000</a></span> | <span class="t">Oops, and we don't want to print.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3098" target="_blank">00:51:38.000</a></span> | <span class="t">It's way too much printing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3100" target="_blank">00:51:40.000</a></span> | <span class="t">So let me again reset and run 10,000 steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3108" target="_blank">00:51:48.000</a></span> | <span class="t">Okay, so we're at 2.48, roughly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3112" target="_blank">00:51:52.000</a></span> | <span class="t">Let's run another 10,000 steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3119" target="_blank">00:51:59.000</a></span> | <span class="t">2.46.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3120" target="_blank">00:52:00.000</a></span> | <span class="t">And now let's do one learning rate decay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3122" target="_blank">00:52:02.000</a></span> | <span class="t">What this means is we're going to take our learning rate and we're going to 10x lower it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3127" target="_blank">00:52:07.000</a></span> | <span class="t">And so we're at the late stages of training, potentially, and we may want to go a bit slower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3132" target="_blank">00:52:12.000</a></span> | <span class="t">Let's do one more, actually, at 0.1, just to see if we're making a dent here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3139" target="_blank">00:52:19.000</a></span> | <span class="t">Okay, we're still making a dent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3140" target="_blank">00:52:20.000</a></span> | <span class="t">And by the way, the bigram loss that we achieved last video was 2.45.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3145" target="_blank">00:52:25.000</a></span> | <span class="t">So we've already surpassed the bigram model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3149" target="_blank">00:52:29.000</a></span> | <span class="t">And once I get a sense that this is actually kind of starting to plateau off, people like to do, as I mentioned, this learning rate decay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3155" target="_blank">00:52:35.000</a></span> | <span class="t">So let's try to decay the loss, the learning rate, I mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3162" target="_blank">00:52:42.000</a></span> | <span class="t">And we achieve at about 2.3 now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3166" target="_blank">00:52:46.000</a></span> | <span class="t">Obviously, this is janky and not exactly how you would train it in production, but this is roughly what you're going through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3172" target="_blank">00:52:52.000</a></span> | <span class="t">You first find a decent learning rate using the approach that I showed you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3175" target="_blank">00:52:55.000</a></span> | <span class="t">Then you start with that learning rate and you train for a while.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3179" target="_blank">00:52:59.000</a></span> | <span class="t">And then at the end, people like to do a learning rate decay, where you decay the learning rate by, say, a factor of 10, and you do a few more steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3185" target="_blank">00:53:05.000</a></span> | <span class="t">And then you get a trained network, roughly speaking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3188" target="_blank">00:53:08.000</a></span> | <span class="t">So we've achieved 2.3 and dramatically improved on the bigram language model using this simple neural net, as described here, using these 3,400 parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3200" target="_blank">00:53:20.000</a></span> | <span class="t">Now, there's something we have to be careful with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3202" target="_blank">00:53:22.000</a></span> | <span class="t">I said that we have a better model because we are achieving a lower loss, 2.3, much lower than 2.45 with the bigram model previously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3210" target="_blank">00:53:30.000</a></span> | <span class="t">Now, that's not exactly true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3212" target="_blank">00:53:32.000</a></span> | <span class="t">And the reason that's not true is that this is actually a fairly small model, but these models can get larger and larger if you keep adding neurons and parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3223" target="_blank">00:53:43.000</a></span> | <span class="t">So you can imagine that we don't potentially have a thousand parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3226" target="_blank">00:53:46.000</a></span> | <span class="t">We could have 10,000 or 100,000 or millions of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3229" target="_blank">00:53:49.000</a></span> | <span class="t">And as the capacity of the neural network grows, it becomes more and more capable of overfitting your training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3236" target="_blank">00:53:56.000</a></span> | <span class="t">What that means is that the loss on the training set, on the data that you're training on, will become very, very low, as low as zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3244" target="_blank">00:54:04.000</a></span> | <span class="t">But all that the model is doing is memorizing your training set verbatim.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3248" target="_blank">00:54:08.000</a></span> | <span class="t">So if you take that model and it looks like it's working really well, but you try to sample from it, you will basically only get examples exactly as they are in the training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3257" target="_blank">00:54:17.000</a></span> | <span class="t">You won't get any new data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3259" target="_blank">00:54:19.000</a></span> | <span class="t">In addition to that, if you try to evaluate the loss on some withheld names or other words, you will actually see that the loss on those can be very high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3268" target="_blank">00:54:28.000</a></span> | <span class="t">And so basically, it's not a good model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3271" target="_blank">00:54:31.000</a></span> | <span class="t">So the standard in the field is to split up your data set into three splits, as we call them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3276" target="_blank">00:54:36.000</a></span> | <span class="t">We have the training split, the dev split or the validation split, and the test split.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3282" target="_blank">00:54:42.000</a></span> | <span class="t">So training split, dev or validation split, and test split.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3291" target="_blank">00:54:51.000</a></span> | <span class="t">And typically, this would be, say, 80% of your data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3295" target="_blank">00:54:55.000</a></span> | <span class="t">This could be 10% and this 10%, roughly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3298" target="_blank">00:54:58.000</a></span> | <span class="t">So you have these three splits of the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3301" target="_blank">00:55:01.000</a></span> | <span class="t">Now, these 80% of the data set, the training set, is used to optimize the parameters of the model, just like we're doing here, using gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3310" target="_blank">00:55:10.000</a></span> | <span class="t">These 10% of the examples, the dev or validation split, they're used for development over all the hyperparameters of your model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3319" target="_blank">00:55:19.000</a></span> | <span class="t">So hyperparameters are, for example, the size of this hidden layer, the size of the embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3324" target="_blank">00:55:24.000</a></span> | <span class="t">So this is 100 or a 2 for us, but we could try different things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3328" target="_blank">00:55:28.000</a></span> | <span class="t">The strength of the regularization, which we aren't using yet so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3332" target="_blank">00:55:32.000</a></span> | <span class="t">So there's lots of different hyperparameters and settings that go into defining a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3336" target="_blank">00:55:36.000</a></span> | <span class="t">And you can try many different variations of them and see whichever one works best on your validation split.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3343" target="_blank">00:55:43.000</a></span> | <span class="t">So this is used to train the parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3346" target="_blank">00:55:46.000</a></span> | <span class="t">This is used to train the hyperparameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3349" target="_blank">00:55:49.000</a></span> | <span class="t">And test split is used to evaluate, basically, the performance of the model at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3354" target="_blank">00:55:54.000</a></span> | <span class="t">So we're only evaluating the loss on the test split very, very sparingly and very few times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3359" target="_blank">00:55:59.000</a></span> | <span class="t">because every single time you evaluate your test loss and you learn something from it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3364" target="_blank">00:56:04.000</a></span> | <span class="t">you are basically starting to also train on the test split.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3368" target="_blank">00:56:08.000</a></span> | <span class="t">So you are only allowed to test the loss on the test set very, very few times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3374" target="_blank">00:56:14.000</a></span> | <span class="t">Otherwise, you risk overfitting to it as well as you experiment on your model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3379" target="_blank">00:56:19.000</a></span> | <span class="t">So let's also split up our training data into train, dev, and test.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3384" target="_blank">00:56:24.000</a></span> | <span class="t">And then we are going to train on train and only evaluate on test very, very sparingly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3389" target="_blank">00:56:29.000</a></span> | <span class="t">Okay, so here we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3391" target="_blank">00:56:31.000</a></span> | <span class="t">Here is where we took all the words and put them into x and y tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3396" target="_blank">00:56:36.000</a></span> | <span class="t">So instead, let me create a new cell here, and let me just copy/paste some code here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3401" target="_blank">00:56:41.000</a></span> | <span class="t">because I don't think it's that complex, but we're going to try to save a little bit of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3407" target="_blank">00:56:47.000</a></span> | <span class="t">I'm converting this to be a function now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3410" target="_blank">00:56:50.000</a></span> | <span class="t">And this function takes some list of words and builds the arrays x and y for those words only.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3416" target="_blank">00:56:56.000</a></span> | <span class="t">And then here, I am shuffling up all the words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3420" target="_blank">00:57:00.000</a></span> | <span class="t">So these are the input words that we get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3422" target="_blank">00:57:02.000</a></span> | <span class="t">We are randomly shuffling them all up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3425" target="_blank">00:57:05.000</a></span> | <span class="t">And then we're going to set n1 to be the number of examples that is 80% of the words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3432" target="_blank">00:57:12.000</a></span> | <span class="t">and n2 to be 90% of the way of the words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3436" target="_blank">00:57:16.000</a></span> | <span class="t">So basically, if length of words is 32,000, n1 is--well, sorry, I should probably run this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3444" target="_blank">00:57:24.000</a></span> | <span class="t">n1 is 25,000, and n2 is 28,000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3448" target="_blank">00:57:28.000</a></span> | <span class="t">And so here we see that I'm calling buildDataSet to build a training set x and y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3454" target="_blank">00:57:34.000</a></span> | <span class="t">by indexing into up to n1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3456" target="_blank">00:57:36.000</a></span> | <span class="t">So we're going to have only 25,000 training words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3460" target="_blank">00:57:40.000</a></span> | <span class="t">And then we're going to have roughly n2 minus n1, 3,000 validation examples or dev examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3470" target="_blank">00:57:50.000</a></span> | <span class="t">And we're going to have length of words basically minus n2 or 3,204 examples here for the test set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3483" target="_blank">00:58:03.000</a></span> | <span class="t">So now we have x's and y's for all those three splits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3493" target="_blank">00:58:13.000</a></span> | <span class="t">Oh yeah, I'm printing their size here inside the function as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3499" target="_blank">00:58:19.000</a></span> | <span class="t">But here we don't have words, but these are already the individual examples made from those words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3505" target="_blank">00:58:25.000</a></span> | <span class="t">So let's now scroll down here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3507" target="_blank">00:58:27.000</a></span> | <span class="t">And the data set now for training is more like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3513" target="_blank">00:58:33.000</a></span> | <span class="t">And then when we reset the network, when we're training, we're only going to be training using x train,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3524" target="_blank">00:58:44.000</a></span> | <span class="t">x train, and y train.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3528" target="_blank">00:58:48.000</a></span> | <span class="t">So that's the only thing we're training on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3538" target="_blank">00:58:58.000</a></span> | <span class="t">Let's see where we are on the single batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3542" target="_blank">00:59:02.000</a></span> | <span class="t">Let's now train maybe a few more steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3548" target="_blank">00:59:08.000</a></span> | <span class="t">Training neural networks can take a while.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3550" target="_blank">00:59:10.000</a></span> | <span class="t">Usually you don't do it inline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3551" target="_blank">00:59:11.000</a></span> | <span class="t">You launch a bunch of jobs and you wait for them to finish.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3554" target="_blank">00:59:14.000</a></span> | <span class="t">It can take multiple days and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3557" target="_blank">00:59:17.000</a></span> | <span class="t">But basically this is a very small network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3561" target="_blank">00:59:21.000</a></span> | <span class="t">Okay, so the loss is pretty good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3564" target="_blank">00:59:24.000</a></span> | <span class="t">Oh, we accidentally used a learning rate that is way too low.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3568" target="_blank">00:59:28.000</a></span> | <span class="t">So let me actually come back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3570" target="_blank">00:59:30.000</a></span> | <span class="t">We used the decay learning rate of 0.01.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3575" target="_blank">00:59:35.000</a></span> | <span class="t">So this will train much faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3577" target="_blank">00:59:37.000</a></span> | <span class="t">And then here when we evaluate, let's use the dev set here, x dev and y dev to evaluate the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3587" target="_blank">00:59:47.000</a></span> | <span class="t">Okay, and let's now decay the learning rate and only do say 10,000 examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3595" target="_blank">00:59:55.000</a></span> | <span class="t">And let's evaluate the dev loss once here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3599" target="_blank">00:59:59.000</a></span> | <span class="t">Okay, so we're getting about 2.3 on dev.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3601" target="_blank">01:00:01.000</a></span> | <span class="t">And so the neural network when it was training did not see these dev examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3605" target="_blank">01:00:05.000</a></span> | <span class="t">It hasn't optimized on them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3607" target="_blank">01:00:07.000</a></span> | <span class="t">And yet when we evaluate the loss on these dev, we actually get a pretty decent loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3612" target="_blank">01:00:12.000</a></span> | <span class="t">And so we can also look at what the loss is on all of training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3619" target="_blank">01:00:19.000</a></span> | <span class="t">Oops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3620" target="_blank">01:00:20.000</a></span> | <span class="t">And so we see that the training and the dev loss are about equal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3624" target="_blank">01:00:24.000</a></span> | <span class="t">So we're not overfitting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3626" target="_blank">01:00:26.000</a></span> | <span class="t">This model is not powerful enough to just be purely memorizing the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3630" target="_blank">01:00:30.000</a></span> | <span class="t">And so far we are what's called underfitting because the training loss and the dev or test losses are roughly equal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3637" target="_blank">01:00:37.000</a></span> | <span class="t">So what that typically means is that our network is very tiny, very small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3642" target="_blank">01:00:42.000</a></span> | <span class="t">And we expect to make performance improvements by scaling up the size of this neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3647" target="_blank">01:00:47.000</a></span> | <span class="t">So let's do that now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3648" target="_blank">01:00:48.000</a></span> | <span class="t">So let's come over here and let's increase the size of the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3652" target="_blank">01:00:52.000</a></span> | <span class="t">The easiest way to do this is we can come here to the hidden layer, which currently is 100 neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3656" target="_blank">01:00:56.000</a></span> | <span class="t">And let's just bump this up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3658" target="_blank">01:00:58.000</a></span> | <span class="t">So let's do 300 neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3660" target="_blank">01:01:00.000</a></span> | <span class="t">And then this is also 300 biases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3663" target="_blank">01:01:03.000</a></span> | <span class="t">And here we have 300 inputs into the final layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3667" target="_blank">01:01:07.000</a></span> | <span class="t">So let's initialize our neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3670" target="_blank">01:01:10.000</a></span> | <span class="t">We now have 10,000 parameters instead of 3,000 parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3675" target="_blank">01:01:15.000</a></span> | <span class="t">And then we're not using this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3678" target="_blank">01:01:18.000</a></span> | <span class="t">And then here what I'd like to do is I'd like to actually keep track of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3687" target="_blank">01:01:27.000</a></span> | <span class="t">Okay, let's just do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3689" target="_blank">01:01:29.000</a></span> | <span class="t">Let's keep stats again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3691" target="_blank">01:01:31.000</a></span> | <span class="t">And here when we're keeping track of the loss, let's just also keep track of the steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3698" target="_blank">01:01:38.000</a></span> | <span class="t">And let's just have an eye here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3701" target="_blank">01:01:41.000</a></span> | <span class="t">And let's train on 30,000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3704" target="_blank">01:01:44.000</a></span> | <span class="t">Or rather say, let's try 30,000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3708" target="_blank">01:01:48.000</a></span> | <span class="t">And we are at 0.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3711" target="_blank">01:01:51.000</a></span> | <span class="t">And we should be able to run this and optimize the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3717" target="_blank">01:01:57.000</a></span> | <span class="t">And then here basically I want to plt.plot the steps against the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3729" target="_blank">01:02:09.000</a></span> | <span class="t">So these are the x's and the y's.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3731" target="_blank">01:02:11.000</a></span> | <span class="t">And this is the loss function and how it's being optimized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3736" target="_blank">01:02:16.000</a></span> | <span class="t">Now, you see that there's quite a bit of thickness to this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3739" target="_blank">01:02:19.000</a></span> | <span class="t">And that's because we are optimizing over these mini-batches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3742" target="_blank">01:02:22.000</a></span> | <span class="t">And the mini-batches create a little bit of noise in this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3746" target="_blank">01:02:26.000</a></span> | <span class="t">Where are we in the dev set?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3748" target="_blank">01:02:28.000</a></span> | <span class="t">We are at 2.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3749" target="_blank">01:02:29.000</a></span> | <span class="t">So we still haven't optimized this neural net very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3752" target="_blank">01:02:32.000</a></span> | <span class="t">And that's probably because we made it bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3754" target="_blank">01:02:34.000</a></span> | <span class="t">It might take longer for this neural net to converge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3757" target="_blank">01:02:37.000</a></span> | <span class="t">And so let's continue training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3763" target="_blank">01:02:43.000</a></span> | <span class="t">Yeah, let's just continue training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3766" target="_blank">01:02:46.000</a></span> | <span class="t">One possibility is that the batch size is so low that we just have way too much noise in the training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3772" target="_blank">01:02:52.000</a></span> | <span class="t">And we may want to increase the batch size so that we have a bit more correct gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3777" target="_blank">01:02:57.000</a></span> | <span class="t">And we're not thrashing too much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3779" target="_blank">01:02:59.000</a></span> | <span class="t">And we can actually optimize more properly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3788" target="_blank">01:03:08.000</a></span> | <span class="t">This will now become meaningless because we've reinitialized these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3793" target="_blank">01:03:13.000</a></span> | <span class="t">So yeah, this looks not pleasing right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3796" target="_blank">01:03:16.000</a></span> | <span class="t">But there probably is a tiny improvement, but it's so hard to tell.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3800" target="_blank">01:03:20.000</a></span> | <span class="t">Let's go again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3802" target="_blank">01:03:22.000</a></span> | <span class="t">2.52.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3805" target="_blank">01:03:25.000</a></span> | <span class="t">Let's try to decrease the learning rate by a factor of two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3830" target="_blank">01:03:50.000</a></span> | <span class="t">Okay, we're at 2.32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3832" target="_blank">01:03:52.000</a></span> | <span class="t">Let's continue training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3845" target="_blank">01:04:05.000</a></span> | <span class="t">We basically expect to see a lower loss than what we had before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3848" target="_blank">01:04:08.000</a></span> | <span class="t">Because now we have a much, much bigger model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3850" target="_blank">01:04:10.000</a></span> | <span class="t">And we were underfitting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3852" target="_blank">01:04:12.000</a></span> | <span class="t">So we'd expect that increasing the size of the model should help the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3856" target="_blank">01:04:16.000</a></span> | <span class="t">2.32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3857" target="_blank">01:04:17.000</a></span> | <span class="t">Okay, so that's not happening too well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3859" target="_blank">01:04:19.000</a></span> | <span class="t">Now, one other concern is that even though we've made the 10H layer here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3863" target="_blank">01:04:23.000</a></span> | <span class="t">or the hidden layer, much, much bigger,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3865" target="_blank">01:04:25.000</a></span> | <span class="t">it could be that the bottleneck of the network right now are these embeddings that are two-dimensional.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3870" target="_blank">01:04:30.000</a></span> | <span class="t">It can be that we're just cramming way too many characters into just two dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3874" target="_blank">01:04:34.000</a></span> | <span class="t">And the neural net is not able to really use that space effectively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3878" target="_blank">01:04:38.000</a></span> | <span class="t">And that is sort of like the bottleneck to our network's performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3882" target="_blank">01:04:42.000</a></span> | <span class="t">Okay, 2.23.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3884" target="_blank">01:04:44.000</a></span> | <span class="t">So just by decreasing the learning rate, I was able to make quite a bit of progress.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3887" target="_blank">01:04:47.000</a></span> | <span class="t">Let's run this one more time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3892" target="_blank">01:04:52.000</a></span> | <span class="t">And then evaluate the training and the dev loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3897" target="_blank">01:04:57.000</a></span> | <span class="t">Now, one more thing after training that I'd like to do is I'd like to visualize the embedding vectors for these characters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3906" target="_blank">01:05:06.000</a></span> | <span class="t">before we scale up the embedding size from 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3910" target="_blank">01:05:10.000</a></span> | <span class="t">Because we'd like to make this bottleneck potentially go away.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3914" target="_blank">01:05:14.000</a></span> | <span class="t">And once I make this greater than 2, we won't be able to visualize them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3917" target="_blank">01:05:17.000</a></span> | <span class="t">So here, okay, we're at 2.23 and 2.24.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3921" target="_blank">01:05:21.000</a></span> | <span class="t">So we're not improving much more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3924" target="_blank">01:05:24.000</a></span> | <span class="t">And maybe the bottleneck now is the character embedding size, which is 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3928" target="_blank">01:05:28.000</a></span> | <span class="t">So here I have a bunch of code that will create a figure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3931" target="_blank">01:05:31.000</a></span> | <span class="t">And then we're going to visualize the embeddings that were trained by the neural net on these characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3938" target="_blank">01:05:38.000</a></span> | <span class="t">Because right now the embedding size is just 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3940" target="_blank">01:05:40.000</a></span> | <span class="t">So we can visualize all the characters with the x and the y coordinates as the two embedding locations for each of these characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3947" target="_blank">01:05:47.000</a></span> | <span class="t">And so here are the x coordinates and the y coordinates, which are the columns of C.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3953" target="_blank">01:05:53.000</a></span> | <span class="t">And then for each one, I also include the text of the little character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3958" target="_blank">01:05:58.000</a></span> | <span class="t">So here what we see is actually kind of interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3962" target="_blank">01:06:02.000</a></span> | <span class="t">The network has basically learned to separate out the characters and cluster them a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3967" target="_blank">01:06:07.000</a></span> | <span class="t">So, for example, you see how the vowels, A, E, I, O, U, are clustered up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3972" target="_blank">01:06:12.000</a></span> | <span class="t">So what that's telling us is that the neural net treats these as very similar, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3976" target="_blank">01:06:16.000</a></span> | <span class="t">Because when they feed into the neural net, the embedding for all these characters is very similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3982" target="_blank">01:06:22.000</a></span> | <span class="t">And so the neural net thinks that they're very similar and kind of like interchangeable, if that makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3989" target="_blank">01:06:29.000</a></span> | <span class="t">Then the points that are like really far away are, for example, Q.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3993" target="_blank">01:06:33.000</a></span> | <span class="t">Q is kind of treated as an exception, and Q has a very special embedding vector, so to speak.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=3998" target="_blank">01:06:38.000</a></span> | <span class="t">Similarly, dot, which is a special character, is all the way out here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4002" target="_blank">01:06:42.000</a></span> | <span class="t">And a lot of the other letters are sort of like clustered up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4006" target="_blank">01:06:46.000</a></span> | <span class="t">And so it's kind of interesting that there's a little bit of structure here after the training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4011" target="_blank">01:06:51.000</a></span> | <span class="t">And it's definitely not random, and these embeddings make sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4015" target="_blank">01:06:55.000</a></span> | <span class="t">So we're now going to scale up the embedding size and won't be able to visualize it directly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4020" target="_blank">01:07:00.000</a></span> | <span class="t">But we expect that because we're underfitting and we made this layer much bigger and did not sufficiently improve the loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4028" target="_blank">01:07:08.000</a></span> | <span class="t">we're thinking that the constraint to better performance right now could be these embedding vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4035" target="_blank">01:07:15.000</a></span> | <span class="t">So let's make them bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4036" target="_blank">01:07:16.000</a></span> | <span class="t">OK, so let's scroll up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4038" target="_blank">01:07:18.000</a></span> | <span class="t">And now we don't have two-dimensional embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4040" target="_blank">01:07:20.000</a></span> | <span class="t">We are going to have, say, 10-dimensional embeddings for each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4045" target="_blank">01:07:25.000</a></span> | <span class="t">Then this layer will receive 3 times 10, so 30 inputs will go into the hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4056" target="_blank">01:07:36.000</a></span> | <span class="t">Let's also make the hidden layer a bit smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4057" target="_blank">01:07:37.000</a></span> | <span class="t">So instead of 300, let's just do 200 neurons in that hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4062" target="_blank">01:07:42.000</a></span> | <span class="t">So now the total number of elements will be slightly bigger at 11,000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4067" target="_blank">01:07:47.000</a></span> | <span class="t">And then here we have to be a bit careful because, OK, the learning rate, we set to 0.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4073" target="_blank">01:07:53.000</a></span> | <span class="t">Here we are hardcoding 6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4075" target="_blank">01:07:55.000</a></span> | <span class="t">And obviously if you're working in production, you don't want to be hardcoding magic numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4079" target="_blank">01:07:59.000</a></span> | <span class="t">But instead of 6, this should now be 30.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4084" target="_blank">01:08:04.000</a></span> | <span class="t">And let's run for 50,000 iterations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4087" target="_blank">01:08:07.000</a></span> | <span class="t">And let me split out the initialization here outside so that when we run this cell multiple times, it's not going to wipe out our loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4097" target="_blank">01:08:17.000</a></span> | <span class="t">In addition to that, here, instead of logging the lost.item, let's actually log the -- let's do log10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4108" target="_blank">01:08:28.000</a></span> | <span class="t">I believe that's a function of the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4112" target="_blank">01:08:32.000</a></span> | <span class="t">And I'll show you why in a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4114" target="_blank">01:08:34.000</a></span> | <span class="t">Let's optimize this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4117" target="_blank">01:08:37.000</a></span> | <span class="t">Basically I'd like to plot the log loss instead of the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4120" target="_blank">01:08:40.000</a></span> | <span class="t">Because when you plot the loss, many times it can have this hockey stick appearance and log squashes it in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4127" target="_blank">01:08:47.000</a></span> | <span class="t">So it just kind of looks nicer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4129" target="_blank">01:08:49.000</a></span> | <span class="t">So the x-axis is step i, and the y-axis will be the loss i.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4141" target="_blank">01:09:01.000</a></span> | <span class="t">And then here this is 30.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4143" target="_blank">01:09:03.000</a></span> | <span class="t">Ideally we wouldn't be hardcoding these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4149" target="_blank">01:09:09.000</a></span> | <span class="t">Because let's look at the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4151" target="_blank">01:09:11.000</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4152" target="_blank">01:09:12.000</a></span> | <span class="t">It's, again, very thick because the mini batch size is very small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4155" target="_blank">01:09:15.000</a></span> | <span class="t">But the total loss over the training set is 2.3, and the test -- or the dev set is 2.38 as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4162" target="_blank">01:09:22.000</a></span> | <span class="t">So so far so good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4163" target="_blank">01:09:23.000</a></span> | <span class="t">Let's try to now decrease the learning rate by a factor of 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4169" target="_blank">01:09:29.000</a></span> | <span class="t">And train for another 50,000 iterations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4175" target="_blank">01:09:35.000</a></span> | <span class="t">We'd hope that we would be able to beat 2.32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4183" target="_blank">01:09:43.000</a></span> | <span class="t">But, again, we're just kind of, like, doing this very haphazardly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4186" target="_blank">01:09:46.000</a></span> | <span class="t">So I don't actually have confidence that our learning rate is set very well, that our learning rate decay, which we just do at random, is set very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4195" target="_blank">01:09:55.000</a></span> | <span class="t">And so the optimization here is kind of suspect, to be honest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4199" target="_blank">01:09:59.000</a></span> | <span class="t">And this is not how you would do it typically in production.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4201" target="_blank">01:10:01.000</a></span> | <span class="t">In production you would create parameters or hyperparameters out of all these settings, and then you would run lots of experiments and see whichever ones are working well for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4211" target="_blank">01:10:11.000</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4212" target="_blank">01:10:12.000</a></span> | <span class="t">So we have 2.17 now and 2.2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4216" target="_blank">01:10:16.000</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4217" target="_blank">01:10:17.000</a></span> | <span class="t">So you see how the training and the validation performance are starting to slightly slowly depart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4223" target="_blank">01:10:23.000</a></span> | <span class="t">So maybe we're getting the sense that the neural net is getting good enough or that number of parameters is large enough that we are slowly starting to overfit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4234" target="_blank">01:10:34.000</a></span> | <span class="t">Let's maybe run one more iteration of this and see where we get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4241" target="_blank">01:10:41.000</a></span> | <span class="t">But, yeah, basically you would be running lots of experiments and then you are slowly scrutinizing whichever ones give you the best depth performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4248" target="_blank">01:10:48.000</a></span> | <span class="t">And then once you find all the hyperparameters that make your depth performance good, you take that model and you evaluate the test set performance a single time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4257" target="_blank">01:10:57.000</a></span> | <span class="t">And that's the number that you report in your paper or wherever else you want to talk about and brag about your model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4265" target="_blank">01:11:05.000</a></span> | <span class="t">So let's then rerun the plot and rerun the train and dev.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4271" target="_blank">01:11:11.000</a></span> | <span class="t">And because we're getting lower loss now, it is the case that the embedding size of these was holding us back very likely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4280" target="_blank">01:11:20.000</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4281" target="_blank">01:11:21.000</a></span> | <span class="t">So 2.16, 2.19 is what we're roughly getting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4284" target="_blank">01:11:24.000</a></span> | <span class="t">So there's many ways to go from here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4287" target="_blank">01:11:27.000</a></span> | <span class="t">We can continue tuning the optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4290" target="_blank">01:11:30.000</a></span> | <span class="t">We can continue, for example, playing with the size of the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4293" target="_blank">01:11:33.000</a></span> | <span class="t">Or we can increase the number of words or characters in our case that we are taking as an input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4299" target="_blank">01:11:39.000</a></span> | <span class="t">So instead of just three characters, we could be taking more characters as an input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4303" target="_blank">01:11:43.000</a></span> | <span class="t">And that could further improve the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4306" target="_blank">01:11:46.000</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4307" target="_blank">01:11:47.000</a></span> | <span class="t">So I changed the code slightly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4308" target="_blank">01:11:48.000</a></span> | <span class="t">So we have here 200,000 steps of the optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4311" target="_blank">01:11:51.000</a></span> | <span class="t">And in the first 100,000, we're using a learning rate of .1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4314" target="_blank">01:11:54.000</a></span> | <span class="t">And then in the next 100,000, we're using a learning rate of .01.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4318" target="_blank">01:11:58.000</a></span> | <span class="t">This is the loss that I achieve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4320" target="_blank">01:12:00.000</a></span> | <span class="t">And these are the performance on the training and validation loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4323" target="_blank">01:12:03.000</a></span> | <span class="t">And in particular, the best validation loss I've been able to obtain in the last 30 minutes or so is 2.17.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4330" target="_blank">01:12:10.000</a></span> | <span class="t">So now I invite you to beat this number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4332" target="_blank">01:12:12.000</a></span> | <span class="t">And you have quite a few knobs available to you to, I think, surpass this number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4336" target="_blank">01:12:16.000</a></span> | <span class="t">So number one, you can, of course, change the number of neurons in the hidden layer of this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4341" target="_blank">01:12:21.000</a></span> | <span class="t">You can change the dimensionality of the embedding lookup table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4345" target="_blank">01:12:25.000</a></span> | <span class="t">You can change the number of characters that are feeding in as an input, as the context into this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4351" target="_blank">01:12:31.000</a></span> | <span class="t">And then, of course, you can change the details of the optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4354" target="_blank">01:12:34.000</a></span> | <span class="t">How long are we running?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4356" target="_blank">01:12:36.000</a></span> | <span class="t">What is the learning rate?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4357" target="_blank">01:12:37.000</a></span> | <span class="t">How does it change over time?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4359" target="_blank">01:12:39.000</a></span> | <span class="t">How does it decay?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4361" target="_blank">01:12:41.000</a></span> | <span class="t">You can change the batch size, and you may be able to actually achieve a much better convergence speed in terms of how many seconds or minutes it takes to train the model and get your result in terms of really good loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4375" target="_blank">01:12:55.000</a></span> | <span class="t">And then, of course, I actually invite you to read this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4378" target="_blank">01:12:58.000</a></span> | <span class="t">It is 19 pages, but at this point, you should actually be able to read a good chunk of this paper and understand pretty good chunks of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4386" target="_blank">01:13:06.000</a></span> | <span class="t">And this paper also has quite a few ideas for improvements that you can play with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4391" target="_blank">01:13:11.000</a></span> | <span class="t">So all of those are knobs available to you, and you should be able to beat this number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4395" target="_blank">01:13:15.000</a></span> | <span class="t">I'm leaving that as an exercise to the reader.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4397" target="_blank">01:13:17.000</a></span> | <span class="t">And that's it for now, and I'll see you next time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4404" target="_blank">01:13:24.000</a></span> | <span class="t">Before we wrap up, I also wanted to show how you would sample from the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4408" target="_blank">01:13:28.000</a></span> | <span class="t">So we're going to generate 20 samples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4411" target="_blank">01:13:31.000</a></span> | <span class="t">At first, we begin with all dots, so that's the context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4415" target="_blank">01:13:35.000</a></span> | <span class="t">And then until we generate the zeroth character again, we're going to embed the current context using the embedding table C.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4426" target="_blank">01:13:46.000</a></span> | <span class="t">Now, usually here, the first dimension was the size of the training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4430" target="_blank">01:13:50.000</a></span> | <span class="t">But here, we're only working with a single example that we're generating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4433" target="_blank">01:13:53.000</a></span> | <span class="t">So this is just dimension one, just for simplicity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4438" target="_blank">01:13:58.000</a></span> | <span class="t">And so this embedding then gets projected into the end state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4442" target="_blank">01:14:02.000</a></span> | <span class="t">You get the logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4443" target="_blank">01:14:03.000</a></span> | <span class="t">Now we calculate the probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4445" target="_blank">01:14:05.000</a></span> | <span class="t">For that, you can use f.softmax of logits, and that just basically exponentiates logits and makes them sum to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4453" target="_blank">01:14:13.000</a></span> | <span class="t">And similar to cross entropy, it is careful that there's no overflows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4458" target="_blank">01:14:18.000</a></span> | <span class="t">Once we have the probabilities, we sample from them using torsion multinomial to get our next index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4463" target="_blank">01:14:23.000</a></span> | <span class="t">And then we shift the context window to append the index and record it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4468" target="_blank">01:14:28.000</a></span> | <span class="t">And then we can just decode all the integers to strings and print them out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4473" target="_blank">01:14:33.000</a></span> | <span class="t">And so these are some example samples, and you can see that the model now works much better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4477" target="_blank">01:14:37.000</a></span> | <span class="t">So the words here are much more word-like or name-like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4481" target="_blank">01:14:41.000</a></span> | <span class="t">So we have things like Ham, Joe's, Lila.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4488" target="_blank">01:14:48.000</a></span> | <span class="t">It's starting to sound a little bit more name-like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4491" target="_blank">01:14:51.000</a></span> | <span class="t">So we're definitely making progress, but we can still improve on this model quite a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4495" target="_blank">01:14:55.000</a></span> | <span class="t">Okay, sorry, there's some bonus content.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4497" target="_blank">01:14:57.000</a></span> | <span class="t">I wanted to mention that I want to make these notebooks more accessible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4501" target="_blank">01:15:01.000</a></span> | <span class="t">And so I don't want you to have to install Jupyter Notebooks and Torch and everything else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4505" target="_blank">01:15:05.000</a></span> | <span class="t">So I will be sharing a link to a Google Colab.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4509" target="_blank">01:15:09.000</a></span> | <span class="t">And the Google Colab will look like a notebook in your browser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4513" target="_blank">01:15:13.000</a></span> | <span class="t">And you can just go to a URL, and you'll be able to execute all of the code that you saw in the Google Colab.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4519" target="_blank">01:15:19.000</a></span> | <span class="t">And so this is me executing the code in this lecture, and I shortened it a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4524" target="_blank">01:15:24.000</a></span> | <span class="t">But basically, you're able to train the exact same network and then plot and sample from the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4529" target="_blank">01:15:29.000</a></span> | <span class="t">And everything is ready for you to tinker with the numbers right there in your browser, no installation necessary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&t=4535" target="_blank">01:15:35.000</a></span> | <span class="t">So I just wanted to point that out, and the link to this will be in the video description.</span></div></div></body></html>