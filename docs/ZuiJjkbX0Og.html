<html><head><title>How LLMs work for Web Devs: GPT in 600 lines of Vanilla JS - Ishan Anand</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>How LLMs work for Web Devs: GPT in 600 lines of Vanilla JS - Ishan Anand</h2><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og"><img src="https://i.ytimg.com/vi_webp/ZuiJjkbX0Og/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./ZuiJjkbX0Og.html">Whisper Transcript</a> | <a href="./transcript_ZuiJjkbX0Og.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Okay, thank you for coming bright and early, 9:00 a.m. at the start of the conference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=24" target="_blank">00:00:24.420</a></span> | <span class="t">for LLMs for Web Devs, GPT in 600 lines of vanilla JavaScript. I think you guys are going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=32" target="_blank">00:00:32.740</a></span> | <span class="t">to have a great conference. I was here last year. I thoroughly enjoyed it. And I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=37" target="_blank">00:00:37.300</a></span> | <span class="t">this is a great way to kick things off. If you're just coming to this field or this conference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=42" target="_blank">00:00:42.580</a></span> | <span class="t">without any background in machine learning, this is your missing AI degree that will help</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=47" target="_blank">00:00:47.900</a></span> | <span class="t">make the rest of the conference, I think, hopefully a lot more valuable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=52" target="_blank">00:00:52.260</a></span> | <span class="t">So if you're just joining us and came in, you can go to spreadsheets are all you need.ai.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=59" target="_blank">00:00:59.260</a></span> | <span class="t">And there's a Discord link in the upper menu. Click on that, and then go to the AI Engineer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=65" target="_blank">00:01:05.040</a></span> | <span class="t">World's Fair 2025 room. And then there's a link to download the GPT-2 model weights in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=72" target="_blank">00:01:12.000</a></span> | <span class="t">the pinned message at the top. It takes a little while to download that, so I would download</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=76" target="_blank">00:01:16.040</a></span> | <span class="t">that and get started. Because the WiFi might be a little bit slow. We're going to be using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=81" target="_blank">00:01:21.420</a></span> | <span class="t">that in a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=83" target="_blank">00:01:23.040</a></span> | <span class="t">Okay. We're going to do something special today. It is a talk. And I will be doing a lot of talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=89" target="_blank">00:01:29.880</a></span> | <span class="t">at you. And we'll be running code, so it is a workshop. But our mission today is to break</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=97" target="_blank">00:01:37.040</a></span> | <span class="t">Clark's third law. You might be familiar with the science fiction author, Arthur C. Clark,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=104" target="_blank">00:01:44.040</a></span> | <span class="t">and his famous maxim that any sufficiently advanced technology is indistinguishable from magic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=109" target="_blank">00:01:49.820</a></span> | <span class="t">And nowhere is that more true or relevant today than when it comes to large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=116" target="_blank">00:01:56.040</a></span> | <span class="t">These are seemingly magical machines that can produce lifelike text, automate tasks as agents,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=124" target="_blank">00:02:04.040</a></span> | <span class="t">and maybe even replace humans in certain contexts. And if you ask somebody how these work or you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=130" target="_blank">00:02:10.900</a></span> | <span class="t">go online, you're liable to get the impression that you need to have semesters of linear algebra</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=137" target="_blank">00:02:17.680</a></span> | <span class="t">and calculus before you can begin taking your first machine learning class and understand how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=142" target="_blank">00:02:22.760</a></span> | <span class="t">these work. And yes, that's true if you want to be a machine learning engineer. But if you just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=148" target="_blank">00:02:28.760</a></span> | <span class="t">want to understand how these work and you're a builder on top of them, I'm here to tell you that is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=154" target="_blank">00:02:34.200</a></span> | <span class="t">true. Do not believe them. You don't need all that sophistication if you just want to have a really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=159" target="_blank">00:02:39.800</a></span> | <span class="t">accurate model of how a transformer works. I know because I was here last year. I gave a talk called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=167" target="_blank">00:02:47.720</a></span> | <span class="t">Spreadsheets are all you need where I showed an Excel worksheet that implemented all of GPT2 small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=173" target="_blank">00:02:53.720</a></span> | <span class="t">entirely in pure Excel functions. And then I took that spreadsheet and I turned it into a class that I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=179" target="_blank">00:02:59.560</a></span> | <span class="t">taught online where I took people tab by tab through how the entire model works. And not everyone was even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=187" target="_blank">00:03:07.720</a></span> | <span class="t">an engineer. So one of my favorites is this guy. This Joe here is a CFO. He's just naturally good at Excel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=195" target="_blank">00:03:15.720</a></span> | <span class="t">And that gave him everything he needed combined with the sheet to understand how a large language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=201" target="_blank">00:03:21.000</a></span> | <span class="t">works on the inside. And he says, this is great. I had no experience in machine learning or AI concepts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=207" target="_blank">00:03:27.160</a></span> | <span class="t">before. Yet he was able to walk away with a very good understanding of how they work. So what I'm going to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=211" target="_blank">00:03:31.720</a></span> | <span class="t">today is compress that class, which is about eight hours down to these two hours today, and explain how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=217" target="_blank">00:03:37.720</a></span> | <span class="t">the transformer works. And I show up today or this year at the conference with this. Instead of Excel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=225" target="_blank">00:03:45.720</a></span> | <span class="t">because not every one of us have a job that requires us to be really good at Excel, we're using a vanilla</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=231" target="_blank">00:03:51.720</a></span> | <span class="t">JavaScript implementation. Because a lot of folks who come to AI engineering as a field have a web development or full-stack</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=239" target="_blank">00:03:59.720</a></span> | <span class="t">JavaScript background. And so if that's your background, you are perfect the way you are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=245" target="_blank">00:04:05.720</a></span> | <span class="t">You don't need to learn Python if you just want to understand how a model works. And you're still going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=249" target="_blank">00:04:09.720</a></span> | <span class="t">to use TypeScript and Next.js around the model, but you still want to have a good understanding of how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=255" target="_blank">00:04:15.720</a></span> | <span class="t">So today's approach is I'm going to give you the background to understand the code. And we're going to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=261" target="_blank">00:04:21.720</a></span> | <span class="t">a brief walkthrough of it. I'm going to focus more on the why than on the what. So we'll take a look at the code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=269" target="_blank">00:04:29.720</a></span> | <span class="t">but I'm going to spend a lot of time building intuition and background to understand the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=275" target="_blank">00:04:35.720</a></span> | <span class="t">the code. And then instead of complex equations, I'm going to use analogies and examples to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=281" target="_blank">00:04:41.720</a></span> | <span class="t">it more tangible. Okay. And the background you need to have is, first of all, just motivation. And a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=287" target="_blank">00:04:47.720</a></span> | <span class="t">curiosity to understand how these work on the inside. Any science, technology, engineering background is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=293" target="_blank">00:04:53.720</a></span> | <span class="t">sufficient. Prior programming experience, especially in JavaScript, but you don't need to know React or Vue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=299" target="_blank">00:04:59.720</a></span> | <span class="t">We're going to just use vanilla JavaScript. And then some awareness, I would call it, of linear algebra,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=305" target="_blank">00:05:05.720</a></span> | <span class="t">meaning you just need to know what a matrix multiplication is. You don't need to be a hot shot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=311" target="_blank">00:05:11.720</a></span> | <span class="t">JavaScript ninja. You don't need prior AI or ML background. And you don't need, you know, deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=317" target="_blank">00:05:17.720</a></span> | <span class="t">calculus or linear algebra fluency. Okay. And the key resources for today are going to be our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=323" target="_blank">00:05:23.720</a></span> | <span class="t">JavaScript implementation of GPT-2. Use Chrome on the desktop. And if you go to spreadsheets are all you need,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=331" target="_blank">00:05:31.720</a></span> | <span class="t">.ai -- and I apologize, that is a long domain name -- slash GPT-2, it will load up this implementation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=338" target="_blank">00:05:38.720</a></span> | <span class="t">and it will run locally in your browser. There's a Discord server where I've dropped some links.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=343" target="_blank">00:05:43.720</a></span> | <span class="t">And if you've got questions, feel free to drop them in there as well. Okay. So this is a simplified diagram of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=352" target="_blank">00:05:52.720</a></span> | <span class="t">GPT-2. It does not look like your classic transformer diagram intentionally. And it will serve as our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=359" target="_blank">00:05:59.720</a></span> | <span class="t">roadmap for what we're going to do throughout today's workshop. I'm going to start by just giving you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=365" target="_blank">00:06:05.720</a></span> | <span class="t">some background on LLMs and our JavaScript implementation of GPT-2, how to get it running,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=371" target="_blank">00:06:11.720</a></span> | <span class="t">how to get it started. And then we're going to focus on these three areas for the most of it. And that's tokenization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=379" target="_blank">00:06:19.720</a></span> | <span class="t">embeddings, and then the language head. And I've focused on those because that's the input and the output of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=386" target="_blank">00:06:26.720</a></span> | <span class="t">And those are the most important to have the background and understanding if you're going to be building systems around and on top of LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=394" target="_blank">00:06:34.720</a></span> | <span class="t">We will cover the inside number crunching part. That's attention and the multilayer perceptron at a pretty high level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=401" target="_blank">00:06:41.720</a></span> | <span class="t">I'll go a little bit more into the multilayer perceptron because then I can explain about backpropagation and it serves as kind of a foundation to understand how the model learns, which is an important concept.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=413" target="_blank">00:06:53.720</a></span> | <span class="t">And then finally, I'll talk about the difference between GPT-2 and ChatGPT. So they were separated by three or four years. What were those innovations that made the model so much seemingly more smarter than GPT-2?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=428" target="_blank">00:07:08.720</a></span> | <span class="t">Hint, it wasn't necessarily anything algorithmic. Okay, so let's start with a quick tour of our JavaScript implementation of GPT-2 and then some background on LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=440" target="_blank">00:07:20.720</a></span> | <span class="t">So this is what you get when you load up GPT-2, slash GPT-2 on the spreadsheets-are-all-you-need website.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=453" target="_blank">00:07:33.720</a></span> | <span class="t">If you scroll down, the first thing you're going to want to do, there's a link here that says download the GPT-2 small CSV.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=461" target="_blank">00:07:41.720</a></span> | <span class="t">So the first thing you want to do is go to this page on GitHub. This is all the model parameters of GPT-2 small in a bunch of CSV.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=469" target="_blank">00:07:49.720</a></span> | <span class="t">It's a giant zip file. You're going to download it and unzip it. And when you hear about, you know, this model is like a billion parameters or 70 billion parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=481" target="_blank">00:08:01.720</a></span> | <span class="t">it's all just a bunch of giant numbers. And you can think of it as a giant spreadsheet. And that's literally what the zip file is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=487" target="_blank">00:08:07.720</a></span> | <span class="t">Once you've downloaded that zip file and opened it up, what you want to do is select all of those files and drag it into this section here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=498" target="_blank">00:08:18.720</a></span> | <span class="t">When it's done loading all those files, it'll look like this. It'll say, ready, all model parameters loaded. When it's not loaded, it'll be in red.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=507" target="_blank">00:08:27.720</a></span> | <span class="t">And it'll say, you know, please add the files. And what it's doing is it's actually loading all those GPT-2 parameters locally into your index.db database.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=517" target="_blank">00:08:37.720</a></span> | <span class="t">And you can see here, it's basically about 1.5 gigabytes. Now, Chrome will let you do that if you've got sufficient disk space on your hard drive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=526" target="_blank">00:08:46.720</a></span> | <span class="t">But the benefit of doing this is now the entire model is running locally in vanilla JavaScript on your browser. In fact, to run and debug this, you don't need anything else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=536" target="_blank">00:08:56.720</a></span> | <span class="t">You could pull the internet connection and it should still work. And the way this is set up is similar to a Python notebook, if you've encountered one of those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=545" target="_blank">00:09:05.720</a></span> | <span class="t">We have these cells. And every cell has a play button, which will run what's inside it. And there are two types of cells. One type of cell is just JavaScript code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=554" target="_blank">00:09:14.720</a></span> | <span class="t">And you can open these and expand these if you want. It's just vanilla JavaScript code. Our matrices, for example, are just simple 2D JavaScript arrays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=562" target="_blank">00:09:22.720</a></span> | <span class="t">And if I hit play, it will execute this code. And you can see there's a message right there. The other type of cell is one like this. It's kind of like a table or spreadsheet interface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=576" target="_blank">00:09:36.720</a></span> | <span class="t">It runs a formula and then shows you the result. So it's a way to actually run code and see the results very immediately. And these formulas are just raw JavaScript with a little syntactic sugar to figure out, you know, if you reference something, it knows what previous table it was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=591" target="_blank">00:09:51.720</a></span> | <span class="t">Let me give you an example of that. So right here is an example. So here, this get final tokens is just JavaScript code we defined earlier. And this prompt to tokens is literally the DOM ID. And this, let's zoom this one out. This brackets is basically syntactic sugar saying, go grab the DOM table that has this ID. So prompt to tokens is up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=620" target="_blank">00:10:20.720</a></span> | <span class="t">And it's just grabbing this thing and passing that into that function. So it's all straight vanilla JavaScript. But the real benefit is being able to debug a model right here without leaving your browser. So what I'm going to do, I can do -- who here has done like console debugging before, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=639" target="_blank">00:10:39.720</a></span> | <span class="t">So you can do console debugging right here. So if I say console.log matches, and I open up my DevTools inspector, let's put that side by side. There we go. And I go to the console. And then I rerun this. There we go. Wait for the layout shifts. There we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=668" target="_blank">00:11:08.720</a></span> | <span class="t">Separate into words is right there. So if I rerun this, you can see right here, it might be a little bit hard. But if you look here, you can see I've basically got my console.log statement here. If I wanted to know what that variable is doing, but I can go even further. I can just type the word debugger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=687" target="_blank">00:11:27.720</a></span> | <span class="t">And then I rerun it. And boom. I'm actually stepping through a large language model that was once considered too dangerous to release, right with ever leaving my browser. So every part of this model, if you were like, I really want to understand how this works, you can step right through it in a familiar language and the browser, a very familiar IDE. So I'm going to remove that debugger statement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=714" target="_blank">00:11:54.720</a></span> | <span class="t">So it doesn't get in our way later. And then there we go. OK. So that's a quick tour of how to run our JavaScript implementation of GPT-2. OK. Next up, large language models. So a large language model has a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=742" target="_blank">00:12:22.720</a></span> | <span class="t">So a large language model has a really simple job to do. We give it a passage of text and it simply predicts the next word. Technically the next token, as we'll talk about. So I might give it the text, Mike is quick, he moves, and it'll just output a single word quickly. It does not by nature naturally give you paragraphs of text. So if we want to get more text, what we do is we take that output, and then we append the word we just got out, which was quickly, and then we append the word we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=770" target="_blank">00:12:50.720</a></span> | <span class="t">got out, which was quickly, and put it at the end of the thing we originally put in. So then we take that additional text, and now we ask it to run through again with this much longer piece, and say, what is the next word now? And it says, oh, it's and. Then we take that, append it to the original input, and keep going, and ask it what the next thing is. And this is how we generate paragraphs of text or code from a model. It is what they call an autoregressive model. You simply take the output, put it back to the input, and rerun it. Now, this is why, if you use our JavaScript implementation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=798" target="_blank">00:13:18.720</a></span> | <span class="t">all it does is predict the next word. Because that is the core function. If you understand that, you understand how the rest of it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=805" target="_blank">00:13:25.720</a></span> | <span class="t">So we've said that large language models have this core action of completing passages of text, and they're trained to complete sentences like this one. Mike is quick, he moves, and as a human, you probably understand, you know, a possible completion is the word quickly, or maybe the word fast, or around. But how do we get a computer to do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=826" target="_blank">00:13:46.720</a></span> | <span class="t">Well, here's a fill-in-the-blank problem that computers are really good at. Two plus two equals four. It's a math problem. Computers are really good at math. And you can make these equations really complex, and computers can still do them really fast. So in effect, what researchers have figured out how to do is take what is a word problem and turn it into a math problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=848" target="_blank">00:14:08.720</a></span> | <span class="t">In order to do that, they have to go through a series of steps. First, what they have to do is they have to map the words in our text to numbers. Here I've shown it as just a one-to-one mapping. So Mike goes to 89. Is goes to 9. But in practice, as we'll see, it's a long list of numbers called an embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=866" target="_blank">00:14:26.720</a></span> | <span class="t">And then we do our number crunching on them. Here I've drawn this as just simple arithmetic. It's much more complex than that. But it's actually almost as simple as that. It's just a lot of multiplication, addition. There's an exponentiation in there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=880" target="_blank">00:14:40.720</a></span> | <span class="t">But it's not math you probably haven't seen before. It's just a lot of it tediously put together. And then after all that arithmetic, we get a result. Again, it'll be a long list of numbers. Here I've simplified for now, just saying a single number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=896" target="_blank">00:14:56.720</a></span> | <span class="t">And we look at the resulting number that comes back. And that number is going to be what it says the next predicted word is going to be. But we need to translate that back to a word, because it's a number. So then we do the reverse of what we did at the beginning, instead of going from words to numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=908" target="_blank">00:15:08.720</a></span> | <span class="t">We go from numbers to words. And of course, the number we get back, numbers are continuous, words are discrete, doesn't necessarily always cleanly map. We'll get some number like here. For example, hypothetically, we get 231.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=924" target="_blank">00:15:24.720</a></span> | <span class="t">There's nothing in our dictionary that maps to it. But the closest word in our dictionary is quickly, which is at 232. But fast is kind of close to 240. So what we're going to do is we're going to wait the probability distribution of these tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=938" target="_blank">00:15:38.720</a></span> | <span class="t">according to how close they are to the predicted number that came out of our model. And that turns into our probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=945" target="_blank">00:15:45.720</a></span> | <span class="t">So then we run a random number generator, and then we pick according to that distribution. One thing I want to emphasize is we add the random number generator in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=954" target="_blank">00:15:54.720</a></span> | <span class="t">We could always just simply take the closest word, and that's called greedy or temperature zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=961" target="_blank">00:16:01.720</a></span> | <span class="t">OK. So that gives us this view. You get some text. We turn that text into tokens. We turn those tokens into numbers. And then we do some number crunching on them. And then we turn those numbers into text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=974" target="_blank">00:16:14.720</a></span> | <span class="t">And that gives us our next predicted token. OK. The model we are going to be studying today is GPT-2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=983" target="_blank">00:16:23.720</a></span> | <span class="t">GPT-2 small, specifically. There's actually multiple versions of GPT-2 that were released. And that came out in 2019.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=990" target="_blank">00:16:30.720</a></span> | <span class="t">So about four years before GPT-4 and three years before ChatGPT. But don't let that fool you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=997" target="_blank">00:16:37.720</a></span> | <span class="t">GPT-2 small, you know. This was a model that was considered too dangerous to release when it first came out. And it was state-of-the-art. More importantly, it is actually the foundation of most of the state-of-the-art models you have probably used today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1011" target="_blank">00:16:51.720</a></span> | <span class="t">And you don't have to take my word for it. This is a research lab, Luther AI, saying basically the recipe for building a large language model has not fundamentally changed since the transform was introduced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1022" target="_blank">00:17:02.720</a></span> | <span class="t">And only slightly tweaked from the language models by OpenAI GPT-1 and GPT-2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1027" target="_blank">00:17:07.720</a></span> | <span class="t">And then in this article, they actually go on to list what the changes are between GPT-2 and a state-of-the-art model at the time, which was LAMA-2 when this was written, which was last year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1040" target="_blank">00:17:20.720</a></span> | <span class="t">So the way to think about this -- and this is a helpful family tree chart of different large language models -- is that most of the ones you're familiar with at the top of this tree -- might be hard to see -- is ChatGPT, LAMA, Bard/Gemini, GPT-4, Claude.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1055" target="_blank">00:17:35.720</a></span> | <span class="t">They all inherit from GPT-2. GPT-2 is its granddaddy. If you understand GPT-2, you are 80% of the way to understanding how a state-of-the-art model works under the hood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1067" target="_blank">00:17:47.720</a></span> | <span class="t">Okay, so now let's dive into the first stage of our model. So that's tokenization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1074" target="_blank">00:17:54.720</a></span> | <span class="t">Okay, this is where we take the input text and we split it into subword units called tokens. In the example that I like to use -- Mike is quick, he moves -- unfortunately, every single word is a single token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1089" target="_blank">00:18:09.720</a></span> | <span class="t">is a single token. But it is not unusual for a word to be two, three, or more tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1096" target="_blank">00:18:16.720</a></span> | <span class="t">And then these tokens all have IDs that are just lists or positions in the dictionary, as you see here underneath.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1102" target="_blank">00:18:22.720</a></span> | <span class="t">So let me show you what this looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1105" target="_blank">00:18:25.720</a></span> | <span class="t">I'll wait for the Wi-Fi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1112" target="_blank">00:18:32.720</a></span> | <span class="t">Okay. Well, we get more people in the room, we get more Wi-Fi issues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1132" target="_blank">00:18:52.720</a></span> | <span class="t">So what I want to illustrate for you is that you can take a word like -- oh, you know what I can do? I do have a backup. Let's do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1147" target="_blank">00:19:07.720</a></span> | <span class="t">Okay, this is a version that runs locally. So what you can do -- so you can try this once we get Wi-Fi back. I'm going to take the word "reindeer" and the word "re-injury" and then I'm going to run them up until right here, which is the final tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1166" target="_blank">00:19:26.720</a></span> | <span class="t">So these are the tokens for the input prompt we just put in here. And you can see the word "re-injury" was turned into multiple tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1180" target="_blank">00:19:40.720</a></span> | <span class="t">There's a space, which is part of the token itself, R-E-I-N, and then jury, J-U-R-Y, and they have separate token IDs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1188" target="_blank">00:19:48.720</a></span> | <span class="t">The thing I want you to pay attention to is reindeer also starts as R-E-I-N, right? But it got split into three tokens, space, R-E-I-N-D, E-E-R.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1199" target="_blank">00:19:59.720</a></span> | <span class="t">So this is not like basic string parsing. Something more complex is going on. And so the natural question is, well, why the heck are we doing this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1207" target="_blank">00:20:07.720</a></span> | <span class="t">Why don't we do something simpler? Why don't we do, say, word-based tokenization? We just take every word in the dictionary and give it a number, like dog is one, cat is two, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1218" target="_blank">00:20:18.720</a></span> | <span class="t">So that has a couple of problems. First is it can't handle unknown or misspelled words. And there are some models, early models, that had an unk for unknown token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1226" target="_blank">00:20:26.720</a></span> | <span class="t">But when you're grabbing all the text on the Internet, you might encounter things you didn't expect. Examples could be languages that you weren't planning for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1234" target="_blank">00:20:34.720</a></span> | <span class="t">One of the early models, GPT-2, in fact, they tried to take foreign languages out of it. And then they magically discovered some snuck in and it was actually good at translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1243" target="_blank">00:20:43.720</a></span> | <span class="t">They wouldn't have had that if a lot of words were just simply that were not English, were just thrown out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1248" target="_blank">00:20:48.720</a></span> | <span class="t">Another example is when they did summarization with it, they realized they can put the too-long-didn't-read acronym, TL:DR. And if they didn't have a token for that, it would have been thrown away and it would have lost that ability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1261" target="_blank">00:21:01.720</a></span> | <span class="t">The other problem is that you're going to increase the vocabulary size, which is going to increase the size of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1267" target="_blank">00:21:07.720</a></span> | <span class="t">It will need more parameters if you're going to have more vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1270" target="_blank">00:21:10.720</a></span> | <span class="t">English alone is 170,000 words. For perspective, GPT-2's vocabulary is only about 50,000, so it's a third of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1277" target="_blank">00:21:17.720</a></span> | <span class="t">And then if you add additional languages on that and you're doing word-based tokenization, it would get even larger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1282" target="_blank">00:21:22.720</a></span> | <span class="t">in essence, if you do this, you get more memory, more compute, or maybe less performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1288" target="_blank">00:21:28.720</a></span> | <span class="t">So then you're like, well, I'm a developer. I'm used to say something like ASCII. Why don't I just do character-based tokenization?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1294" target="_blank">00:21:34.720</a></span> | <span class="t">I say A is 1, B is 2, and do it that way. Well, the first problem is it's going to increase the sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1301" target="_blank">00:21:41.720</a></span> | <span class="t">So you can see this inside the model. As you go through the model, after you get your prompt, right here, you can see here is where the embeddings -- we'll talk about this in a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1311" target="_blank">00:21:51.720</a></span> | <span class="t">But you can see Mike is quick, period, he moves. Each of these rows, this matrix, has a height that is the size of the number of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1320" target="_blank">00:22:00.720</a></span> | <span class="t">And that persists through the entire model. So as I keep going, we see, again, this six-height matrix. It's going to keep going.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1329" target="_blank">00:22:09.720</a></span> | <span class="t">If we made every single character its own token, this is going to get a lot larger. Right now, it's just, what, six tokens high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1337" target="_blank">00:22:17.720</a></span> | <span class="t">But if I made M, I, K, E, each of these characters their own token, this is going to get a much larger matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1342" target="_blank">00:22:22.720</a></span> | <span class="t">So it will be more memory, more compute to process. The other issue is there's low semantic correlation in characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1349" target="_blank">00:22:29.720</a></span> | <span class="t">They don't carry a lot of meaning. And a good example is this chain letter that went around a few decades ago on the Internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1354" target="_blank">00:22:34.720</a></span> | <span class="t">And it says, according to research at Cambridge University, it doesn't matter in what order the letters in a word are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1360" target="_blank">00:22:40.720</a></span> | <span class="t">The only important thing is that the first and last letter be at the right place. And all the letters are jumbled, but you can still read it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1366" target="_blank">00:22:46.720</a></span> | <span class="t">And the point is that you don't read characters. You actually read subword units yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1371" target="_blank">00:22:51.720</a></span> | <span class="t">And so if there's less semantic correlation, it's going to be more work for the model to do during training to erase that character boundary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1378" target="_blank">00:22:58.720</a></span> | <span class="t">boundary and get the pieces that really matter. So if character tokenization is too small and word tokenization is too big,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1386" target="_blank">00:23:06.720</a></span> | <span class="t">Goldilocks says let's do something in between, which is subword tokenization. And that's this algorithm called by parent coding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1392" target="_blank">00:23:12.720</a></span> | <span class="t">So it's got two phases. The first is the learning phase where you take a large -- they call it a corpus of text that's gathered from the Internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1399" target="_blank">00:23:19.720</a></span> | <span class="t">And then we put it through this learning algorithm we'll describe in a second. And then we get out of it a vocabulary, a dictionary of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1407" target="_blank">00:23:27.720</a></span> | <span class="t">And then later when we're processing the model and asking it to generate text, if we give it some input words, we have to re-translate it into those tokens that were used during training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1417" target="_blank">00:23:37.720</a></span> | <span class="t">So we take the input words, we take the vocabulary, and we get out tokens. This is the research paper that introduced the algorithm to machine learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1427" target="_blank">00:23:47.720</a></span> | <span class="t">to machine learning. But it turns out this algorithm is from the 90s. It's actually a compression algorithm, as we'll talk about in a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1434" target="_blank">00:23:54.720</a></span> | <span class="t">And it even has some Python code you can copy and paste and run. The goal of the algorithm really is to take the text that's going to be trained on and figure out the most efficient way to represent it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1445" target="_blank">00:24:05.720</a></span> | <span class="t">That's really what tokenization is trying to do. And so here's the example from the paper. And what I'm going to do first is I'm going to use this dot to separate out the characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1456" target="_blank">00:24:16.720</a></span> | <span class="t">That's going to tell us, essentially, each token will be separated by those dots so we can see them individually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1461" target="_blank">00:24:21.720</a></span> | <span class="t">At the end of this process, you'll see that there are less dots, meaning that we've got more tokens -- sorry, more number of tokens in our vocabulary, but less number of tokens being used to represent the corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1473" target="_blank">00:24:33.720</a></span> | <span class="t">And in this corpus, we're going to pretend that when we scraped the internet, this is all we came up with. There are only four words: low, lower, newest, and widest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1482" target="_blank">00:24:42.720</a></span> | <span class="t">And you'll notice some of them appear more than once. In fact, they all do. But the reason I'm doing it this way is I want the frequency of the word to be represented.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1489" target="_blank">00:24:49.720</a></span> | <span class="t">Right? I'm trying to compress all the words on the internet. And if a word is more frequent, I want to know that, because if it's more frequent, I want to give it more representation or a more efficient representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1502" target="_blank">00:25:02.720</a></span> | <span class="t">So that dot is going to separate out our tokens. And then I'm going to put the underscore to indicate the space character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1508" target="_blank">00:25:08.720</a></span> | <span class="t">One caveat -- in this example, the space character is at the end of the token. In GPT-2, it's actually at the beginning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1514" target="_blank">00:25:14.720</a></span> | <span class="t">And then we're going to start with our vocabulary of just our characters, A through Z, all lowercase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1520" target="_blank">00:25:20.720</a></span> | <span class="t">We're assuming that we're in a lowercase-only world here for now. Those are our initial tokens. That's our vocabulary on the right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1526" target="_blank">00:25:26.720</a></span> | <span class="t">And then the first thing we're going to do is we're going to count all the adjacent tokens, all the adjacent characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1533" target="_blank">00:25:33.720</a></span> | <span class="t">Right now, there are no tokens other than characters. And we can see E and S occurs nine times. Right? Six times in newest and three times in widest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1542" target="_blank">00:25:42.720</a></span> | <span class="t">So I'm going to put a table together, and I put E next to S has a frequency of nine. And then I'm going to do this for all the possible pairs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1549" target="_blank">00:25:49.720</a></span> | <span class="t">And then I'm going to take the most frequent pair, and I'm going to say that's a new token. Why am I doing that? Because if I do that, I can take E and S, I can put them together, and I can pretend they're their own character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1560" target="_blank">00:26:00.720</a></span> | <span class="t">I'll call it a token. And then I can go back to my corpus. And every place there was an E and S, I'm going to replace it with my new ES token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1569" target="_blank">00:26:09.720</a></span> | <span class="t">So now I've shrunk the number of tokens that need to represent the stuff here on the left. Now I've taken what were separate tokens and combined them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1577" target="_blank">00:26:17.720</a></span> | <span class="t">So I'm using less tokens to represent all this text. And then I can repeat the process. I can just say ES next to T occurs nine times, and I can do a whole other table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1587" target="_blank">00:26:27.720</a></span> | <span class="t">Now ES is itself, now its own character, its own token. So it can be paired with other things that I'm counting. And I can see that ES with T occurs nine times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1597" target="_blank">00:26:37.720</a></span> | <span class="t">So I add that to my vocabulary of tokens, and I go back and I compress my corpus again. Then I keep going, and I just simply repeat this process, looking for whatever is the most frequent at each pass, and making it a token, and then recompressing the corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1612" target="_blank">00:26:52.720</a></span> | <span class="t">And after 10 passes, you get something like this. Here on the right is our vocabulary of tokens. And then here on the left, you can see we have shrunk the number of tokens used to represent the corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1624" target="_blank">00:27:04.720</a></span> | <span class="t">There are less of these dots, right, than we saw before and originally. Now the most frequent words became their own tokens, right? Low and newest. And even the words that did not get their own full token representation, we've now represented them a lot more efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1644" target="_blank">00:27:24.720</a></span> | <span class="t">And you notice that there are some common subword units like low became tokens, and EST became tokens. So it managed to map to some of the morphemes, the parts of speech we use as humans as tokens, but that is just a coincidence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1659" target="_blank">00:27:39.720</a></span> | <span class="t">Right now the model has no understanding of semantic meaning. Okay, so this is the learning algorithm. Now the tokenization algorithm is really similar. I'm not going to go through it in full detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1671" target="_blank">00:27:51.720</a></span> | <span class="t">I have a video on my YouTube channel where I do go through it in full detail. But for time, I'm just going to talk about it at a high level. It's essentially similar to the learning algorithm, except we're doing it on individual tokens as they come in, asking ourselves what would it look like if this word were part of the tokenization process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1689" target="_blank">00:28:09.720</a></span> | <span class="t">So let me show you what that is like. This is a helpful jump to here. Let's go to tokenization. So the first thing we do is we take our prompt and we separate it into words. Now this is just a regular expression that came from the OpenAI source code when they open sourced it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1703" target="_blank">00:28:23.720</a></span> | <span class="t">And we're just parsing it according to this. And it's going to basically take out punctuation and spaces. So we get these as our words. So Mike is quick, period gets its own token, he and moves. Right now we're not tokens, we're just separating words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1717" target="_blank">00:28:37.720</a></span> | <span class="t">But do note that the spaces are assigned as part of this separation. So he and moves have a space in front of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1726" target="_blank">00:28:46.720</a></span> | <span class="t">The next thing we're going to do is we're going to fetch vocab, BPE. This is a file that came from OpenAI when they trained the model and its tokenization. This is their dictionary of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1735" target="_blank">00:28:55.720</a></span> | <span class="t">So when they did their training in BPE, the most common left token with a right token was space with a T. The second most was space followed by an A and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1746" target="_blank">00:29:06.720</a></span> | <span class="t">I've added two extra columns, rank and score, just to figure out where a pair of tokens is in terms relative to the others, how important they were.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1755" target="_blank">00:29:15.720</a></span> | <span class="t">Then this is just a helpful map of tokens to their IDs. And then finally, there's this, which is prompt to tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1764" target="_blank">00:29:24.720</a></span> | <span class="t">Now, this is not how you'd really want to write a tokenizer. I've set it up to look similar to the Excel version of the matrix so you can watch the video and still be able to watch it and understand it, no matter which version of the Excel sheet or the JavaScript version you're looking at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1780" target="_blank">00:29:40.720</a></span> | <span class="t">But this kind of illustrates the process. So here is quick. We're breaking it apart right here. Let me get presentify. There we go. We're breaking it apart into characters here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1793" target="_blank">00:29:53.720</a></span> | <span class="t">All right, quick. And then what we're doing is we're looking at each pair of characters. So Q with U, U with I, I with C, C with K. And we're saying, for each one, where does it fall in that rank of tokens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1807" target="_blank">00:30:07.720</a></span> | <span class="t">And we're going to take the most popular one. In this case, it's I and C. And so we've rewritten this as a string of tokens, except the I and C were put together as one token. And then we're simply going to repeat the process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1822" target="_blank">00:30:22.720</a></span> | <span class="t">We're going to repeat the process. And then we're going to repeat the process. And then we're going to repeat the process and keep going. And we're combined. Then Q and U get combined. Then I, C, K get combined. And finally, it gets to the point where we realize, oh, this is in my token vocabulary. And it turns it into a token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1834" target="_blank">00:30:34.720</a></span> | <span class="t">I have a video if you want to see this in depth and in detail. But for now, just think of it as the same part of the learning process, really just run small. Now, tokenization is actually kind of considered by a lot of experts to be a necessary evil.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1851" target="_blank">00:30:51.720</a></span> | <span class="t">Some of the problems that you sometimes encounter with models, although the root causes in tokenization, it can sometimes make them worse. So the common one is how many Rs are there in strawberry?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1862" target="_blank">00:31:02.720</a></span> | <span class="t">Models don't actually see the Rs. And I love this post from Riley Goodside, where if you remember the matrix, the character says, oh, I don't see the numbers. I just see what's inside there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1875" target="_blank">00:31:15.720</a></span> | <span class="t">And that's kind of like what it is to the model. You know, it doesn't see any of the letters. So what he's done here in this image is strawberry is tokenized multiple different ways, depending on whether it's uppercase versus lowercase, whether there's a space in front of it or not,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1890" target="_blank">00:31:30.720</a></span> | <span class="t">and whether there's a quote in front of it or not. So the model, you know, these six words are not all strawberry. They're all six different token patterns. And so it makes it a lot harder and a lot more work for the model to understand it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1904" target="_blank">00:31:44.720</a></span> | <span class="t">If you think about it, you don't actually see the letters either. If somebody asked you how many letters are in a word, you'd have to stop and think and parse it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1911" target="_blank">00:31:51.720</a></span> | <span class="t">I said at the beginning, you know, you don't do word tokenization. You don't do character tokenization. That's not a hard, fast rule. That's an empirical rule. And there are research models that have done both.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1923" target="_blank">00:32:03.720</a></span> | <span class="t">So just know that, you know, that's, you know, maybe a few years from now that we character based tokenization will be more popular. This is an example of one. The last thing I want to leave you with is that tokenization doesn't have to be just about text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1938" target="_blank">00:32:18.720</a></span> | <span class="t">It can also be about other things. So here's an example of the vision transformer. They use patches of images as tokens. Waymo uses trajectories in space to prevent collisions as tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1951" target="_blank">00:32:31.720</a></span> | <span class="t">Okay. Let's briefly check how we're doing. If this will load. There we go. So this is actually pinned at the top. 9:30. Oh, we're right. We're just a minute behind. Okay. So this is -- you guys can keep me honest. In the Discord room at the top is a spreadsheet. I'm, of course, using a spreadsheet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=1980" target="_blank">00:33:00.720</a></span> | <span class="t">to make sure we stay on track and on time. Okay. So -- oh, we need to do this now. There we go. Next up is embeddings. So now we're in the second phase of the input. These are token and position embeddings. Okay. So at the beginning of this workshop, I talked about how we map words into numbers. And I simplified this process by saying what we're doing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2010" target="_blank">00:33:30.720</a></span> | <span class="t">is we're taking, say, the word "Mike" and we're turning it into a single number like 89. But, of course, that's not what we're really doing. We're actually going to turn it into a long list of numbers called an embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2022" target="_blank">00:33:42.720</a></span> | <span class="t">So even the period, for example, gets 768 of these numbers. And in the case of GPT-2, the dimensionality, that list size, is 768. Every single word gets 768 numbers. And you can see that if we go to the token embedding section.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2041" target="_blank">00:34:01.720</a></span> | <span class="t">And then these -- we'll get to that table in a second. These are the embeddings of our input prompt. And you can type -- by the way, I didn't mention this before -- you can type anything here in this input prompt and it should parse it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2051" target="_blank">00:34:11.720</a></span> | <span class="t">Although it doesn't handle foreign language as well because there's a character mismatch on import. But here's Mike is quick period. And each of these get 768 numbers. So what you can do is if I take row one, column 768, there's where our list of 768 numbers ends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2070" target="_blank">00:34:30.720</a></span> | <span class="t">So every single one of these gets the same number of dimensions for how it gets represented. And it might be a little confusing because we mapped words into tokens with numbers. We've had token IDs. And then we have these embeddings as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2087" target="_blank">00:34:47.720</a></span> | <span class="t">And the analogy I like to use is imagine you are going to go look for a house to rent or a house to buy. And the street addresses -- and you build this table of street addresses and square feet, bedrooms and bathrooms and price.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2099" target="_blank">00:34:59.720</a></span> | <span class="t">The street addresses are identifiers. They're kind of like the token IDs. They tell you where to find something, where to find a house. But they don't tell you anything about what's inside. They don't tell you about what you care about, what the meaning is. It doesn't tell you the square feet, the bedrooms, the bathrooms, or the price.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2114" target="_blank">00:35:14.720</a></span> | <span class="t">And the embedding values, that's their job. They're to take the token and tell you something about what it means. And the identifier or the token ID is just to give it a position in the dictionary or effectively a numerical name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2126" target="_blank">00:35:26.720</a></span> | <span class="t">And what we're really doing with the embedding values is we're trying to build a map for words where we put similar words grouped together. So here I've shown a two-dimensional map. But of course, we're in a 768 hyper-dimensional area.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2141" target="_blank">00:35:41.720</a></span> | <span class="t">But the idea is basically the same. Take the words happy and glad in this map, this word island. You know, those are happy words. So I'm going to put them up here. And then I'm going to take words like dog and cat, and I'll put them in another part of this word island because they're animals. They don't relate as much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2158" target="_blank">00:35:58.720</a></span> | <span class="t">And then, you know, the word sad, well, it doesn't have the same meaning as happy and glad. But it's still an emotion. So I want it closer to the emotions, happy emotions, part of the island, into this, maybe the sad province, right next to the happy province.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2174" target="_blank">00:36:14.720</a></span> | <span class="t">So they're kind of on the same half of the island, but they're not directly in the same spot because happy and sad have different meanings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2180" target="_blank">00:36:20.720</a></span> | <span class="t">So here's a simple two-dimensional example of the benefit of doing this, which is that you can start doing word arithmetic and word math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2190" target="_blank">00:36:30.720</a></span> | <span class="t">So we're going to imagine that we've built a two-dimensional embedding where the first column is authority and the second column is gender. And we take the token man and we'll just arbitrarily say man has authority of one and a gender of one, a woman, authority of one and a gender of two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2205" target="_blank">00:36:45.720</a></span> | <span class="t">A king has more authority than a man, so two for authority, gender of one because still a man, and then queen, authority of two, a gender of two. And we can plot this out in a plane like as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2217" target="_blank">00:36:57.720</a></span> | <span class="t">So queen, for example, is at position two, two. And then we can actually build relationships just from doing vector math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2225" target="_blank">00:37:05.720</a></span> | <span class="t">So, for example, if we take king, we subtract man, we add woman, and then we just do regular arithmetic column by column.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2233" target="_blank">00:37:13.720</a></span> | <span class="t">So two minus one plus one, one minus one plus two gives us two, two. So king minus man plus woman is two, two. But of course, that is the same thing as queen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2244" target="_blank">00:37:24.720</a></span> | <span class="t">So we're saying king minus man plus woman equals queen. And we can think of this as an analogy. King is to man as queen is to woman. And if we take out the queen and just leave it as a blank, we have our first kind of word problem we can convert to a math problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2262" target="_blank">00:37:42.720</a></span> | <span class="t">If you give me any three words and I had an embedding for them, I can figure out what the fourth word in this relationship is simply from vector math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2270" target="_blank">00:37:50.720</a></span> | <span class="t">And that is what Word2vec, which was the most famous word embedding, was able to do. It learned a bunch of relationships in a series of papers, not just one paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2282" target="_blank">00:38:02.720</a></span> | <span class="t">So, for example, France is to Paris, as Italy is to Rome, and Japan is to Tokyo. Einstein is to scientist, as Messi is to midfielder, or Mozart is to violinist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2294" target="_blank">00:38:14.720</a></span> | <span class="t">Japan is to sushi, as Germany is to bratwurst. It didn't get all of them right, but clearly it's learning something about relationships.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2301" target="_blank">00:38:21.720</a></span> | <span class="t">And I want to be clear, this is actually just the same thing as being good at clustering. So imagine I've got on my word island, I've got all my countries over here. Right? France, Japan, Italy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2312" target="_blank">00:38:32.720</a></span> | <span class="t">And then I've got all my capitals, Paris, Tokyo, Rome, over here in the word island. Well, every single one of them has the same vector relationship in space between each other if the clustering was really good and tight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2325" target="_blank">00:38:45.720</a></span> | <span class="t">And if somebody comes in and says, hey, what's the capital of Canada? Well, I just use that same direction, I add that same vector to it, I can say, oh, well, it's Ottawa.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2338" target="_blank">00:38:58.720</a></span> | <span class="t">So in practice, real world embeddings are different than what I've shown here with this contrived example. First of all, we have many more columns or dimensions. So instead of simply two, we have hundreds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2349" target="_blank">00:39:09.720</a></span> | <span class="t">GPT-2 is 768. Your state-of-the-art model these days has a lot more. The other key difference is I made up this thing saying that the first dimension or column is authority, the second one is gender.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2362" target="_blank">00:39:22.720</a></span> | <span class="t">We don't know what they mean. The columns are completely uninterpretable. The model just seems to pick them. And the values themselves, correspondingly, therefore, are not interpretable either.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2372" target="_blank">00:39:32.720</a></span> | <span class="t">And that might sound useless, but it's actually useful for at least getting similarity, for example. So let's go back to our housing analogy. Imagine I took off the top column, the labels, essentially, top row of what each column meant, and I just gave you the IDs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2387" target="_blank">00:39:47.720</a></span> | <span class="t">If you went to, say, 47 Ivy Lane, and you said, I like this house, I want to see more like it, you could still find those. Because what you could do is you could notice that with 58 Sun, Av, and 15 Luna Lane, they all have roughly the same values in the first column, 2400, 2400, 2400, and the same values for the third, fourth column as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2409" target="_blank">00:40:09.720</a></span> | <span class="t">So you're like, if I like this house, I can find the others like it, even though I don't know what these columns mean. So the question you're probably asking yourself is, well, where the heck do these values come from? How do we know what they mean? Or how does the model pick it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2424" target="_blank">00:40:24.720</a></span> | <span class="t">Well, the slightly unsatisfying answer is that the embeddings are just simply learned by the neural network model during training. So now let's just talk about what training looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2433" target="_blank">00:40:33.720</a></span> | <span class="t">So in training, what we do is we grab a bunch of text from the internet, we take out passages like this one, Mike is quick, he moves quickly. So quickly was in the original passage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2442" target="_blank">00:40:42.720</a></span> | <span class="t">And then we chop off the last token or the last word, and we have a randomly initialized model. All the values of the weights and parameters are completely random, including the embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2452" target="_blank">00:40:52.720</a></span> | <span class="t">And then we run it through the whole model, and we say, what do you predict is going to be the next word? And it's random, so it comes up with something nonsensical, like Mike is quick, he moves haircut.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2461" target="_blank">00:41:01.720</a></span> | <span class="t">And then we use this algorithm called backpropagation. And we say, hey, backpropagation, the correct answer was quickly. Can you adjust the parameters to get closer to that value?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2471" target="_blank">00:41:11.720</a></span> | <span class="t">And it will go and tell us for every single parameter how to slightly move it to get it closer to producing the right answer of quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2478" target="_blank">00:41:18.720</a></span> | <span class="t">And then we rerun the model, and eventually it says, Mike is quick, he moves quickly. And then we do this not just for one single passage of text, we're doing this for many passages at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2488" target="_blank">00:41:28.720</a></span> | <span class="t">And one great benefit of this is we don't have to teach the model anything explicitly. It's kind of learning from unsupervised text. We're just gathering that was naturally there on the Internet, and it's learning things like grammar, names, capitals of countries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2506" target="_blank">00:41:46.720</a></span> | <span class="t">But that seems a bit mysterious. And we can kind of get an intuitive sense for what it's doing if you think about it as learning from word statistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2515" target="_blank">00:41:55.720</a></span> | <span class="t">So imagine you've got passages like this one about the words ice and steam. It was so cold, the puddle had turned to ice. Steam rose from the still hot cup of coffee.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2525" target="_blank">00:42:05.720</a></span> | <span class="t">And what you notice is that the words ice and cold tend to co-occur with each other. And the words steam and hot tend to co-occur with each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2534" target="_blank">00:42:14.720</a></span> | <span class="t">So if you were an alien coming down from another planet, you had no idea what our language was. And you looked at this, you would say, I don't know what ice, cold, steam, and hot are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2543" target="_blank">00:42:23.720</a></span> | <span class="t">But I know that ice is probably cold more than steam is. Because ice and cold co-occur more often than steam and cold do. And I know that steam is hot because ice and hot don't co-occur as much as steam and hot do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2557" target="_blank">00:42:37.720</a></span> | <span class="t">So there must be some relationship. They must have that meaning. And this is called the distributional hypothesis. And the phrase that you'll often hear people quote is, you shall know a word by the company it keeps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2569" target="_blank">00:42:49.720</a></span> | <span class="t">Which is basically that a word is partially defined by its context. Or said another way, words that have similar meanings can be replaced with each other in similar contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2578" target="_blank">00:42:58.720</a></span> | <span class="t">So if you know how they're distributed in their statistical representation, you have a sense of what similar words might be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2584" target="_blank">00:43:04.720</a></span> | <span class="t">And in fact, in the full version of my class, we actually build our own primitive embeddings from Wikipedia pages, using a very simplified version of not Word2Vec, but another algorithm called GloVe, which is based on just using a co-occurrence matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2599" target="_blank">00:43:19.720</a></span> | <span class="t">So roughly what that process looks like is you would count how often the words co-occur within some window size inside your corpus of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2607" target="_blank">00:43:27.720</a></span> | <span class="t">You'd analyze every single word, and you'd look in this case, three words to the left, three words to the right, and you'd say, these are the words that co-occur with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2613" target="_blank">00:43:33.720</a></span> | <span class="t">And then you'd build a big giant table, a matrix, and you'd say, let me compare every word to every other word, and I'm going to count how often they co-occur in all my text with each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2623" target="_blank">00:43:43.720</a></span> | <span class="t">So here in this example, Word2 and Word3 co-occur, let's say, five times in our corpus of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2630" target="_blank">00:43:50.720</a></span> | <span class="t">And then what you can think of the embedding as, instead of taking this table, which is all possible words by all possible words, and if we're in English, that's about 170,000 words, or in the case of BPE, it's 50,000 tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2643" target="_blank">00:44:03.720</a></span> | <span class="t">You can imagine it compressing the columns of the matrix. So instead of having 170,000 columns, it's got whatever your embedding dimension is, 768.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2653" target="_blank">00:44:13.720</a></span> | <span class="t">It's still all possible words high, but it's now a lot smaller in the number of columns. So it's basically a compressed co-occurrence matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2662" target="_blank">00:44:22.720</a></span> | <span class="t">So the actual table OpenAI gives us from training, the embedding table, is really just this thing on the right. It is basically every single word or token and then the representation of its dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2674" target="_blank">00:44:34.720</a></span> | <span class="t">And you can think of it as they just took a co-occurrence matrix and they shrunk it. The reason I like this mental model is it helps motivate certain things about embeddings that might seem a bit weird.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2685" target="_blank">00:44:45.720</a></span> | <span class="t">So a classic one is, how do you measure how similar two embeddings are? You might naively think we just look at Euclidean space, you know, as the crow flies, how far apart they are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2694" target="_blank">00:44:54.720</a></span> | <span class="t">But that's not what we do. We use something called cosine similarity. How many people have heard of cosine similarity?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2700" target="_blank">00:45:00.720</a></span> | <span class="t">Yeah, so, you know, cosine similarity is not an intuitive measurement of similarity. And what it is, is we take the angle of the two points, and then we take the cosine of that angle, and that's how we say how similar two embeddings are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2713" target="_blank">00:45:13.720</a></span> | <span class="t">So if you remember your trigonometry, if they're opposed, you know, cosine goes from negative one to one. So if they're opposed, they get negative one value. So that's when two vectors are like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2723" target="_blank">00:45:23.720</a></span> | <span class="t">If they're unrelated, it's like this, they're orthogonal. And if they're similar, they're pointing the same place, the cosine will be one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2730" target="_blank">00:45:30.720</a></span> | <span class="t">Oh, and by the way, this opposite is not opposite in probably your intuitive conventional sense. So happy and sad, you might think would be opposites. But actually, they're more similar to each other than any other set of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2741" target="_blank">00:45:41.720</a></span> | <span class="t">If you think about how often like happy and sad probably occur in songs and poems and other types of contexts. They're both emotions. So they're actually more similar than they would be opposite.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2749" target="_blank">00:45:49.720</a></span> | <span class="t">In fact, if you take all the GPT-2 embeddings and you compare them against each other, very few are negative, and most of those are close to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2756" target="_blank">00:45:56.720</a></span> | <span class="t">But going back to kind of learning from word statistics, the key thing we care about is the relative co-occurrence of different words against each other. We don't care about the raw co-occurrence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2770" target="_blank">00:46:10.720</a></span> | <span class="t">So let's, for example, imagine there's this section of our co-occurrence matrix, which is the basis for the embeddings. We're comparing three words -- one, two, three -- against word 10 and 11.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2780" target="_blank">00:46:20.720</a></span> | <span class="t">And word 1 occurs with word 10 and word 10 times each. Word 2 occurs with word 10 and 11 50 times each. Word 3 on the other end is 5 and 20 times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2791" target="_blank">00:46:31.720</a></span> | <span class="t">And I'm going to plot these as vectors, where the horizontal axis is how many times it occurs with word 10, and the vertical is how many times with word 11.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2799" target="_blank">00:46:39.720</a></span> | <span class="t">And what we notice is something interesting. Word 1 and 2 essentially have the same meaning. As far as word 10 and word 11 can tell, the relative probability between them is the same -- one to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2812" target="_blank">00:46:52.720</a></span> | <span class="t">It's just that word 2 happens to be more common, right? It's five times more common. But if we plotted this in vector space, what you see is that word 2 and word 1 are really far apart in Euclidean distance, but they have the same angle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2827" target="_blank">00:47:07.720</a></span> | <span class="t">Now, word 3 is actually closer in Euclidean space. But relative to word 10 and word 11 has a different meaning. And so that's a motivation for why we're looking at the angle, right, which seems to more accurately capture the meaning and not necessarily popularity, which is what Euclidean space would capture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2844" target="_blank">00:47:24.720</a></span> | <span class="t">Okay, so how do we actually use this? Well, these embeddings were learned during training. So OpenAI gives us this model_wte matrix. And the way it's set up is that it is our vocabulary --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2855" target="_blank">00:47:35.720</a></span> | <span class="t">size tall. So 50,257 tokens is how many tokens GPT-2 uses. So there's a row for every single token. And then each row is just simply the embedding for that token. So there's a row for dog. And that row is the 768 numbers that represent the semantic meaning of dog.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2875" target="_blank">00:47:55.720</a></span> | <span class="t">So let's go to our example here. So let's take is. What is the token ID for is? It is 318. So this is our model_wte. This is one of those CSV files we dragged in. So it fetches it and displays it. So let's go to row 318. There's actually an off by one. But I'll do that anyways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2897" target="_blank">00:48:17.720</a></span> | <span class="t">So you can see -- we'll come back to that in a second. And let's go to our final token embeddings right here. So as you can see, it's .0097, .010. And you'll see that matches what you have here. .0097 in the 319th row. But if we were 0 index, it would be 318. .001.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2919" target="_blank">00:48:39.720</a></span> | <span class="t">So all this code is doing right here is it's grabbing the token ID and just pulling out the corresponding row and plopping it into this table here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2929" target="_blank">00:48:49.720</a></span> | <span class="t">So it's a very simple operation. That's why this is what? Basically 15 lines of JavaScript to just grab one thing out of another and then put it here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2940" target="_blank">00:49:00.720</a></span> | <span class="t">So all this is doing is just taking those token IDs and looking them up and putting them in this table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2946" target="_blank">00:49:06.720</a></span> | <span class="t">Last thing I want to say before we leave token embeddings is, as before with tokens, I said it doesn't just have to be a text. The same thing is true for embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2955" target="_blank">00:49:15.720</a></span> | <span class="t">So the famous example is clip, which was the basis for a lot of the image generators that you probably have tried or used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2961" target="_blank">00:49:21.720</a></span> | <span class="t">And instead of just comparing words against words, you can think of it as it's comparing words against images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2967" target="_blank">00:49:27.720</a></span> | <span class="t">So if you look at all the images on the internet and you look at that alt text and it sees dog and a bunch of images with dogs in it, you can get it to learn that relationship.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2976" target="_blank">00:49:36.720</a></span> | <span class="t">So later on, you can pass an image and it can say this thing is a dog.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2981" target="_blank">00:49:41.720</a></span> | <span class="t">Okay, let's go to position embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2984" target="_blank">00:49:44.720</a></span> | <span class="t">Okay, so now we're still at the top with input and we're talking about embeddings, but now we're talking about different kind of embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2992" target="_blank">00:49:52.720</a></span> | <span class="t">And the key thing to remember is that in English word order matters, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=2997" target="_blank">00:49:57.720</a></span> | <span class="t">The dog chases the cat is something different than the cat chases the dog.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3002" target="_blank">00:50:02.720</a></span> | <span class="t">Now in math, two plus three equals five, but three plus two equals five.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3008" target="_blank">00:50:08.720</a></span> | <span class="t">Anything after that equal sign has no idea what the order was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3011" target="_blank">00:50:11.720</a></span> | <span class="t">Our problem is that when we mapped from a word domain, an English domain, a language domain, we were in a domain where order typically matters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3022" target="_blank">00:50:22.720</a></span> | <span class="t">We went to a number domain where order typically does not matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3025" target="_blank">00:50:25.720</a></span> | <span class="t">And even though I've drawn this with simplified arithmetic, there are parts of the model that are also commutative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3031" target="_blank">00:50:31.720</a></span> | <span class="t">So what could happen in essence is you can change the order of the words, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3036" target="_blank">00:50:36.720</a></span> | <span class="t">And now it could mean something different or it could be completely gibberish.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3040" target="_blank">00:50:40.720</a></span> | <span class="t">But anything after that equal sign can't tell the difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3043" target="_blank">00:50:43.720</a></span> | <span class="t">So it has no sense of position of these words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3046" target="_blank">00:50:46.720</a></span> | <span class="t">And that's going to be really hard to understand the meaning of the sentence or the phrase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3050" target="_blank">00:50:50.720</a></span> | <span class="t">So what we're going to do is we're going to add some sense of position to the embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3054" target="_blank">00:50:54.720</a></span> | <span class="t">What we're going to do is we're going to just take, for example, one token like woman and say woman at position zero in the prompt is going to basically mean the same thing as woman at any other position in the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3065" target="_blank">00:51:05.720</a></span> | <span class="t">So let's give woman at one position other than zero a slight offset, a slightly different position to represent that it's roughly the same meaning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3075" target="_blank">00:51:15.720</a></span> | <span class="t">It's just woman when it occurs at position one and a different spot for woman at position two and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3081" target="_blank">00:51:21.720</a></span> | <span class="t">And in general, we'll use the position in the prompt to define a small offset that we're going to slightly move the position of the token in the embedding space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3091" target="_blank">00:51:31.720</a></span> | <span class="t">In the original famous attention is all you need paper, they use the sine and cosine formulas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3097" target="_blank">00:51:37.720</a></span> | <span class="t">You don't have to look through the whole thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3099" target="_blank">00:51:39.720</a></span> | <span class="t">The key thing I want you to pay attention to is it's just sine and a cosine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3102" target="_blank">00:51:42.720</a></span> | <span class="t">And if you remember your trigonometry, sine and cosine goes from negative one to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3105" target="_blank">00:51:45.720</a></span> | <span class="t">So what we're effectively doing is we are building this circle right around this with a limited diameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3112" target="_blank">00:51:52.720</a></span> | <span class="t">And we're saying I'm oscillating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3115" target="_blank">00:51:55.720</a></span> | <span class="t">And that's the other thing to remember for trigonometry is sine and cosine oscillate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3118" target="_blank">00:51:58.720</a></span> | <span class="t">I'm just oscillating the position of this thing in space based on its position in the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3123" target="_blank">00:52:03.720</a></span> | <span class="t">I'm keeping it roughly in that same area, but it's a little cloud that's all the different versions of woman just in different parts of the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3129" target="_blank">00:52:09.720</a></span> | <span class="t">In GPT-2, interestingly enough, they didn't use that same technique.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3134" target="_blank">00:52:14.720</a></span> | <span class="t">They let the model learn the embeddings on its own, which still blows my mind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3140" target="_blank">00:52:20.720</a></span> | <span class="t">So how we use this is we have another matrix that OpenAI gives us when they open source GPT-2, which is the position matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3149" target="_blank">00:52:29.720</a></span> | <span class="t">And this time it is 1024 high, which is our maximum context length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3154" target="_blank">00:52:34.720</a></span> | <span class="t">It's an early model, so it's not very large context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3157" target="_blank">00:52:37.720</a></span> | <span class="t">And then each of the rows is, again, the embedding dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3161" target="_blank">00:52:41.720</a></span> | <span class="t">And what we're going to do is these are position offsets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3164" target="_blank">00:52:44.720</a></span> | <span class="t">So we're going to add these offsets in each row to each value in our embedding dimension to offset it to represent its position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3171" target="_blank">00:52:51.720</a></span> | <span class="t">So you can think of it like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3173" target="_blank">00:52:53.720</a></span> | <span class="t">We start with our embedding values from the token embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3177" target="_blank">00:52:57.720</a></span> | <span class="t">And then we're basically doing a matrix add.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3180" target="_blank">00:53:00.720</a></span> | <span class="t">So we're taking each of these elements here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3183" target="_blank">00:53:03.720</a></span> | <span class="t">So I take this element, add it to this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3185" target="_blank">00:53:05.720</a></span> | <span class="t">This element, add it to this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3186" target="_blank">00:53:06.720</a></span> | <span class="t">This one, add it to that one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3188" target="_blank">00:53:08.720</a></span> | <span class="t">We're just simple element-wise add.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3190" target="_blank">00:53:10.720</a></span> | <span class="t">And that gets our position embeddings for every single one of our input tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3196" target="_blank">00:53:16.720</a></span> | <span class="t">So let me show you where that is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3201" target="_blank">00:53:21.720</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3202" target="_blank">00:53:22.720</a></span> | <span class="t">So the first thing we do is we fetch this model_wpe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3206" target="_blank">00:53:26.720</a></span> | <span class="t">That, again, comes from OpenAI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3208" target="_blank">00:53:28.720</a></span> | <span class="t">You can see this is 1024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3210" target="_blank">00:53:30.720</a></span> | <span class="t">So if we go to row, 1024, column one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3214" target="_blank">00:53:34.720</a></span> | <span class="t">There it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3215" target="_blank">00:53:35.720</a></span> | <span class="t">That's our max context length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3217" target="_blank">00:53:37.720</a></span> | <span class="t">So beyond that, the model has no idea how to understand that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3221" target="_blank">00:53:41.720</a></span> | <span class="t">You can ignore this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3222" target="_blank">00:53:42.720</a></span> | <span class="t">And this code for positional embed is really just a matrix add.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3226" target="_blank">00:53:46.720</a></span> | <span class="t">The only thing I have to do is make sure, well, the input prompt is going to be less than 1024 tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3232" target="_blank">00:53:52.720</a></span> | <span class="t">So it just needs to stop when it gets to the end of the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3235" target="_blank">00:53:55.720</a></span> | <span class="t">But that gives us our positional embeddings here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3237" target="_blank">00:53:57.720</a></span> | <span class="t">So we're just passing to this our token embeddings we had from the previous step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3241" target="_blank">00:54:01.720</a></span> | <span class="t">And then our model_wpe table, which came fetched from the CSV file.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3246" target="_blank">00:54:06.720</a></span> | <span class="t">It just simply adds those together and we get these positional embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3252" target="_blank">00:54:12.720</a></span> | <span class="t">Here's kind of an illustration of the action of GPT-2 and its positional embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3257" target="_blank">00:54:17.720</a></span> | <span class="t">So these numbers you see right after, like happy3, happy4, aren't the actual tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3262" target="_blank">00:54:22.720</a></span> | <span class="t">What I've done is I've plotted the word happy, the token happy rather, and I've put it at different positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3268" target="_blank">00:54:28.720</a></span> | <span class="t">I've put it at position3, position4, position5, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3271" target="_blank">00:54:31.720</a></span> | <span class="t">And then I took two other words, happy and glad, and I just put them in space as reference points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3275" target="_blank">00:54:35.720</a></span> | <span class="t">And you can see it's doing what we described earlier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3277" target="_blank">00:54:37.720</a></span> | <span class="t">It's basically just keeping it roughly in the same position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3280" target="_blank">00:54:40.720</a></span> | <span class="t">It's slightly offsetting it depending on where it is in the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3285" target="_blank">00:54:45.720</a></span> | <span class="t">One key thing you should know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3288" target="_blank">00:54:48.720</a></span> | <span class="t">Of all the changes to modern LLMs from GPT-2, probably one of the most common and biggest ones is that they do not use these types of positional embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3297" target="_blank">00:54:57.720</a></span> | <span class="t">They do something called rope.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3299" target="_blank">00:54:59.720</a></span> | <span class="t">So if you look at a modern LM, this is probably the first thing you'll see that's different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3304" target="_blank">00:55:04.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3305" target="_blank">00:55:05.720</a></span> | <span class="t">Let's do a time check.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3311" target="_blank">00:55:11.720</a></span> | <span class="t">We are five minutes behind, but we've got 10 minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3318" target="_blank">00:55:18.720</a></span> | <span class="t">I will take a break for a question or two, if anybody wants to ask one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3326" target="_blank">00:55:26.720</a></span> | <span class="t">Any questions so far?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3328" target="_blank">00:55:28.720</a></span> | <span class="t">Yeah, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3329" target="_blank">00:55:29.720</a></span> | <span class="t">Oh, there's a microphone right there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3336" target="_blank">00:55:36.720</a></span> | <span class="t">You were mentioning that with GPT-2, they weren't using the sine and cosine kind of cloud.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3341" target="_blank">00:55:41.720</a></span> | <span class="t">What were they actually doing to come up with the word position embeddings?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3345" target="_blank">00:55:45.720</a></span> | <span class="t">That's the crazy thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3346" target="_blank">00:55:46.720</a></span> | <span class="t">That's the crazy thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3348" target="_blank">00:55:48.720</a></span> | <span class="t">They let the model learn it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3350" target="_blank">00:55:50.720</a></span> | <span class="t">So the big change they did is, think of it this way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3356" target="_blank">00:55:56.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3357" target="_blank">00:55:57.720</a></span> | <span class="t">If we go back to our diagram of how embeddings are learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3363" target="_blank">00:56:03.720</a></span> | <span class="t">Right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3364" target="_blank">00:56:04.720</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3365" target="_blank">00:56:05.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3366" target="_blank">00:56:06.720</a></span> | <span class="t">So in the original transformer, the position embeddings were not represented as learnable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3382" target="_blank">00:56:22.720</a></span> | <span class="t">weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3383" target="_blank">00:56:23.720</a></span> | <span class="t">They were sine and cosine functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3385" target="_blank">00:56:25.720</a></span> | <span class="t">All they did is they said during back propagation, let's learn those as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3390" target="_blank">00:56:30.720</a></span> | <span class="t">And that's how it learned them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3392" target="_blank">00:56:32.720</a></span> | <span class="t">So they did not, they just simply said, hey, let's not hard code those values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3396" target="_blank">00:56:36.720</a></span> | <span class="t">And let's make that other thing now a parameter of the model learns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3403" target="_blank">00:56:43.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3404" target="_blank">00:56:44.720</a></span> | <span class="t">Good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3405" target="_blank">00:56:45.720</a></span> | <span class="t">And as I said, it's still kind of mind blowing that that worked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3410" target="_blank">00:56:50.720</a></span> | <span class="t">But as we now know, these things can learn a ton.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3414" target="_blank">00:56:54.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3415" target="_blank">00:56:55.720</a></span> | <span class="t">Let's talk about attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3419" target="_blank">00:56:59.720</a></span> | <span class="t">Okay, there we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3424" target="_blank">00:57:04.720</a></span> | <span class="t">So now we're getting into the heart of the number crunching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3427" target="_blank">00:57:07.720</a></span> | <span class="t">And this one's going to be a little more cursory understanding and explanation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3431" target="_blank">00:57:11.720</a></span> | <span class="t">But I still think it's important to understand what it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3433" target="_blank">00:57:13.720</a></span> | <span class="t">And we're going to start with attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3437" target="_blank">00:57:17.720</a></span> | <span class="t">And now we're inside what are often called the layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3440" target="_blank">00:57:20.720</a></span> | <span class="t">I like to call them the blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3442" target="_blank">00:57:22.720</a></span> | <span class="t">That's a less common, but other people use the term blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3444" target="_blank">00:57:24.720</a></span> | <span class="t">The reason I use the word blocks is when I teach this, it's usually people coming to it the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3449" target="_blank">00:57:29.720</a></span> | <span class="t">first time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3450" target="_blank">00:57:30.720</a></span> | <span class="t">And the word layer gets used in other contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3453" target="_blank">00:57:33.720</a></span> | <span class="t">Later we're going to talk about the multi-layer perceptron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3455" target="_blank">00:57:35.720</a></span> | <span class="t">And it can be confusing when you're first coming to something and the same word has different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3459" target="_blank">00:57:39.720</a></span> | <span class="t">meanings depending on the context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3460" target="_blank">00:57:40.720</a></span> | <span class="t">But know that when you talk to most people, when they talk about how many layers in a model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3464" target="_blank">00:57:44.720</a></span> | <span class="t">they're talking about what I call how many blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3467" target="_blank">00:57:47.720</a></span> | <span class="t">And if you start getting to this part of the code, you'll notice that inside the blocks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3472" target="_blank">00:57:52.720</a></span> | <span class="t">actually if you go right here, you can see these are all labeled with steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3479" target="_blank">00:57:59.720</a></span> | <span class="t">Step 1, 2, 4, 9, 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3481" target="_blank">00:58:01.720</a></span> | <span class="t">So this step number is arbitrary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3484" target="_blank">00:58:04.720</a></span> | <span class="t">You can do it in fewer steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3486" target="_blank">00:58:06.720</a></span> | <span class="t">You can do it in more steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3487" target="_blank">00:58:07.720</a></span> | <span class="t">This is what I happened to pick when I was implementing it in Excel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3490" target="_blank">00:58:10.720</a></span> | <span class="t">And I kept the same mapping so all my material would translate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3494" target="_blank">00:58:14.720</a></span> | <span class="t">Attention is steps 4 through 9.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3496" target="_blank">00:58:16.720</a></span> | <span class="t">So there's a lot of steps in here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3498" target="_blank">00:58:18.720</a></span> | <span class="t">But the key operations inside the blocks are multi-head attention and the multi-layer perceptron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3503" target="_blank">00:58:23.720</a></span> | <span class="t">So let's talk about attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3504" target="_blank">00:58:24.720</a></span> | <span class="t">And I'll talk about it mostly conceptually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3508" target="_blank">00:58:28.720</a></span> | <span class="t">The way to think about attention is we're going to let the tokens or words talk to each other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3513" target="_blank">00:58:33.720</a></span> | <span class="t">so they can convey their meaning to all the other words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3517" target="_blank">00:58:37.720</a></span> | <span class="t">So, for example, he is a pronoun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3519" target="_blank">00:58:39.720</a></span> | <span class="t">It's antecedent Mike is in the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3522" target="_blank">00:58:42.720</a></span> | <span class="t">Maybe it needs to find that guy and realize, oh, Mike is my antecedent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3526" target="_blank">00:58:46.720</a></span> | <span class="t">As opposed to, say, if there was the name Sally, that's unlikely to be the match of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3530" target="_blank">00:58:50.720</a></span> | <span class="t">But there's other kinds of ways words can communicate to disambiguate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3534" target="_blank">00:58:54.720</a></span> | <span class="t">So, for example, the word quick in English has four different meanings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3538" target="_blank">00:58:58.720</a></span> | <span class="t">It can mean moving fast in physical space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3540" target="_blank">00:59:00.720</a></span> | <span class="t">But it can also mean bright, as in quick of wit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3543" target="_blank">00:59:03.720</a></span> | <span class="t">It can be a body part, as in the quick of your fingernail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3546" target="_blank">00:59:06.720</a></span> | <span class="t">And in Shakespeare in English, it can be alive, as in the phrase the quick and the dead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3552" target="_blank">00:59:12.720</a></span> | <span class="t">And knowing that this word moves here helps the model understand that, oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3557" target="_blank">00:59:17.720</a></span> | <span class="t">we're probably talking about quick when it's physical space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3560" target="_blank">00:59:20.720</a></span> | <span class="t">It helps to predict what the next word could be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3562" target="_blank">00:59:22.720</a></span> | <span class="t">It could be fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3563" target="_blank">00:59:23.720</a></span> | <span class="t">It could be moves around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3565" target="_blank">00:59:25.720</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3566" target="_blank">00:59:26.720</a></span> | <span class="t">But it's not, you know, a body part or your fingernail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3569" target="_blank">00:59:29.720</a></span> | <span class="t">And the way I like to think about attention is we've got these tokens, these words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3576" target="_blank">00:59:36.720</a></span> | <span class="t">And they're sitting in this embedding space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3578" target="_blank">00:59:38.720</a></span> | <span class="t">And I like to imagine there's kind of a weird gravity, like celestial mechanics, where each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3583" target="_blank">00:59:43.720</a></span> | <span class="t">of these tokens in attention now suddenly look at what position they're at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3586" target="_blank">00:59:46.720</a></span> | <span class="t">And they're able to push and pull each other relative to a kind of gravity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3591" target="_blank">00:59:51.720</a></span> | <span class="t">And if you remember, gravity is mass times distance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3594" target="_blank">00:59:54.720</a></span> | <span class="t">So, you've probably heard of query, key, and value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3596" target="_blank">00:59:56.720</a></span> | <span class="t">And there's kind of this file cabin analogy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3598" target="_blank">00:59:58.720</a></span> | <span class="t">But I feel like it doesn't capture kind of all the level of interaction between the tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3603" target="_blank">01:00:03.720</a></span> | <span class="t">that's really happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3604" target="_blank">01:00:04.720</a></span> | <span class="t">And what's happening is, you know, if you remember, gravity is mass times distance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3608" target="_blank">01:00:08.720</a></span> | <span class="t">The distance I like to think of is a measure of relevance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3611" target="_blank">01:00:11.720</a></span> | <span class="t">So, quick and moves, whenever they see each other, they're like, oh, yeah, you and you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3616" target="_blank">01:00:16.720</a></span> | <span class="t">we should talk to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3618" target="_blank">01:00:18.720</a></span> | <span class="t">But quick and the period, they probably don't need to talk to each other a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3621" target="_blank">01:00:21.720</a></span> | <span class="t">And that's kind of like distance in terms of gravity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3624" target="_blank">01:00:24.720</a></span> | <span class="t">And then I like to think of the value as being kind of like mass, the kind of action they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3627" target="_blank">01:00:27.720</a></span> | <span class="t">going to exert on each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3630" target="_blank">01:00:30.720</a></span> | <span class="t">And what's happening is, let's go back to what we talked about with embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3633" target="_blank">01:00:33.720</a></span> | <span class="t">We've got moves, which is sitting somewhere in the embedding space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3636" target="_blank">01:00:36.720</a></span> | <span class="t">And if you remember that co-occurrence matrix, moves has been used in a lot of sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3641" target="_blank">01:00:41.720</a></span> | <span class="t">In some sentences, moves was used to describe rabbits, right, or cheetahs, or animals or things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3648" target="_blank">01:00:48.720</a></span> | <span class="t">that moved fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3649" target="_blank">01:00:49.720</a></span> | <span class="t">So there's some other point in this embedding space that we don't have yet that represents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3654" target="_blank">01:00:54.720</a></span> | <span class="t">moves in a fast context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3656" target="_blank">01:00:56.720</a></span> | <span class="t">And then moves was used in some sentences to describe slugs or penguins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3661" target="_blank">01:01:01.720</a></span> | <span class="t">And so moves in that case was implying slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3664" target="_blank">01:01:04.720</a></span> | <span class="t">But the embedding for moves, unfortunately, has to capture all of those meanings together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3669" target="_blank">01:01:09.720</a></span> | <span class="t">But now that we know quick is here, we can change that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3672" target="_blank">01:01:12.720</a></span> | <span class="t">We can say, oh, I'm going to shift the position of moves from the regular generic version of moves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3679" target="_blank">01:01:19.720</a></span> | <span class="t">to the moves fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3682" target="_blank">01:01:22.720</a></span> | <span class="t">So I've kind of disambiguated the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3684" target="_blank">01:01:24.720</a></span> | <span class="t">I've shifted its position in space to capture its meaning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3687" target="_blank">01:01:27.720</a></span> | <span class="t">I'm not going to go through all the steps of attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3690" target="_blank">01:01:30.720</a></span> | <span class="t">But I think the most salient part of attention to see is step seven.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3697" target="_blank">01:01:37.720</a></span> | <span class="t">It's the most famous thing that people usually show.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3704" target="_blank">01:01:44.720</a></span> | <span class="t">move right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3705" target="_blank">01:01:45.720</a></span> | <span class="t">So you can see it says Mike is quick on the horizontal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3709" target="_blank">01:01:49.720</a></span> | <span class="t">And then Mike is quick, period.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3711" target="_blank">01:01:51.720</a></span> | <span class="t">He moves on the vertical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3713" target="_blank">01:01:53.720</a></span> | <span class="t">So what this is is you can see how much relevance or attention each word is paying to every other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3720" target="_blank">01:02:00.720</a></span> | <span class="t">word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3721" target="_blank">01:02:01.720</a></span> | <span class="t">And by the way, what you're seeing here is just the first head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3723" target="_blank">01:02:03.720</a></span> | <span class="t">There are multiple of these heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3724" target="_blank">01:02:04.720</a></span> | <span class="t">So if you scroll this to the right, 64 spaces, you'll see another matrix that looks like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3729" target="_blank">01:02:09.720</a></span> | <span class="t">And there's a couple of things to notice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3731" target="_blank">01:02:11.720</a></span> | <span class="t">The biggest thing to notice here is that the upper triangle is all zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3737" target="_blank">01:02:17.720</a></span> | <span class="t">And that's because in transformers like GPT-2 and decoder-based transformers, we have this rule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3745" target="_blank">01:02:25.720</a></span> | <span class="t">that no token can look forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3747" target="_blank">01:02:27.720</a></span> | <span class="t">They can only look at the tokens before it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3749" target="_blank">01:02:29.720</a></span> | <span class="t">Those are the ones that can influence it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3750" target="_blank">01:02:30.720</a></span> | <span class="t">And then the other key property is each of these values, each row, sums up to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3755" target="_blank">01:02:35.720</a></span> | <span class="t">So you can think about the percentage of attention each word is paying to the other tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3759" target="_blank">01:02:39.720</a></span> | <span class="t">So here, for example, moves, 16% of its attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3763" target="_blank">01:02:43.720</a></span> | <span class="t">Here in the first head of, in this case, the last block is paying 16% of its attention to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3770" target="_blank">01:02:50.720</a></span> | <span class="t">word Mike, 23% of its attention to is, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3775" target="_blank">01:02:55.720</a></span> | <span class="t">Okay, next up, the multilayer perceptron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3779" target="_blank">01:02:59.720</a></span> | <span class="t">Okay, so now this is the second major operation inside each block or layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3784" target="_blank">01:03:04.720</a></span> | <span class="t">And the reason I want to cover this in a little more detail is I want to explain what neural network is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3789" target="_blank">01:03:09.720</a></span> | <span class="t">And it helps give a little more understanding to how the model actually learns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3792" target="_blank">01:03:12.720</a></span> | <span class="t">The key algorithm called backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3794" target="_blank">01:03:14.720</a></span> | <span class="t">So if you haven't seen a neural network before, it is a computational model inspired by the human brain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3800" target="_blank">01:03:20.720</a></span> | <span class="t">It is not a direct mimic or simulation of how the brain works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3803" target="_blank">01:03:23.720</a></span> | <span class="t">Inside the brain, we have these things called neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3806" target="_blank">01:03:26.720</a></span> | <span class="t">These neurons are all connected to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3809" target="_blank">01:03:29.720</a></span> | <span class="t">And you've got a bunch of connections incoming from other neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3811" target="_blank">01:03:31.720</a></span> | <span class="t">And you've got a bunch of connections outgoing to other neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3814" target="_blank">01:03:34.720</a></span> | <span class="t">And then in between, you have this axon right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3819" target="_blank">01:03:39.720</a></span> | <span class="t">And the axon has an all or nothing activation behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3824" target="_blank">01:03:44.720</a></span> | <span class="t">If there's a sufficient amount of pattern of input that shows up, the axon will activate and it will send a signal out to its output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3832" target="_blank">01:03:52.720</a></span> | <span class="t">But if the activation doesn't have enough meet some threshold, you'll have these failed initiations and no signal will be sent to the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3840" target="_blank">01:04:00.720</a></span> | <span class="t">It will be completely silent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3841" target="_blank">01:04:01.720</a></span> | <span class="t">As far as the other output neurons connected, this neuron is not firing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3846" target="_blank">01:04:06.720</a></span> | <span class="t">The signal wasn't getting through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3848" target="_blank">01:04:08.720</a></span> | <span class="t">And so we model this mathematically with this diagram where we've got a bunch of inputs, x1 through xn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3855" target="_blank">01:04:15.720</a></span> | <span class="t">And these are just numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3856" target="_blank">01:04:16.720</a></span> | <span class="t">These will be our embedding dimension numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3858" target="_blank">01:04:18.720</a></span> | <span class="t">And then we've got another series of numbers called weights, w1, w2 through wn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3863" target="_blank">01:04:23.720</a></span> | <span class="t">And we're simply multiplying the x's times the w's, adding them together, adding an additional number called a bias term.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3870" target="_blank">01:04:30.720</a></span> | <span class="t">And then we put it through an activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3873" target="_blank">01:04:33.720</a></span> | <span class="t">And this activation function is designed to roughly mimic what happens in the brain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3877" target="_blank">01:04:37.720</a></span> | <span class="t">The easiest one to understand is this one, ReLU, which is basically saying when I multiply and add all the inputs coming in against their weights, if the result is negative, then I do nothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3888" target="_blank">01:04:48.720</a></span> | <span class="t">It comes out as zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3889" target="_blank">01:04:49.720</a></span> | <span class="t">If it's positive, then I just pass it through as is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3892" target="_blank">01:04:52.720</a></span> | <span class="t">And there's a whole zoo of these activation functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3896" target="_blank">01:04:56.720</a></span> | <span class="t">So then what we do is we take these neurons and we stitch them together into a network of neurons, hence an artificial neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3904" target="_blank">01:05:04.720</a></span> | <span class="t">Now there's a lot of ways you can stitch these together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3907" target="_blank">01:05:07.720</a></span> | <span class="t">In the case of transformers in GPT-2, we do it in a pattern called the multilayer perceptron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3914" target="_blank">01:05:14.720</a></span> | <span class="t">You will also see it referred to as a fully connected network and a feed forward neural network or just simply the neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3922" target="_blank">01:05:22.720</a></span> | <span class="t">It is called by all these terms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3924" target="_blank">01:05:24.720</a></span> | <span class="t">These are not directly identical terms, but they all overlap.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3927" target="_blank">01:05:27.720</a></span> | <span class="t">And the way the MLP pattern looks is you have your neurons arranged in these columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3933" target="_blank">01:05:33.720</a></span> | <span class="t">of neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3934" target="_blank">01:05:34.720</a></span> | <span class="t">And these are called layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3936" target="_blank">01:05:36.720</a></span> | <span class="t">And each layer in the multilayer perceptron has a node.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3940" target="_blank">01:05:40.720</a></span> | <span class="t">And nodes in each layer are fully connected to every other node in its preceding input, but no other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3946" target="_blank">01:05:46.720</a></span> | <span class="t">So this node right here can see all of its input nodes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3950" target="_blank">01:05:50.720</a></span> | <span class="t">It's all connected to all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3951" target="_blank">01:05:51.720</a></span> | <span class="t">But it cannot talk to any of these directly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3953" target="_blank">01:05:53.720</a></span> | <span class="t">Everything that it gets is mediated through this intermediate layer between it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3957" target="_blank">01:05:57.720</a></span> | <span class="t">And these layers between the input and the output are simply just called hidden layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3963" target="_blank">01:06:03.720</a></span> | <span class="t">The last thing maybe you should know as background on this is that neural networks can be more efficient to write as a matrix multiplication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3970" target="_blank">01:06:10.720</a></span> | <span class="t">So this process, we've got two neurons with a set of weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3973" target="_blank">01:06:13.720</a></span> | <span class="t">So w11 times x1, w21 times x2 plus b.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3979" target="_blank">01:06:19.720</a></span> | <span class="t">This can be written as a matrix multiplication where you just separate all the weights into one matrix, all the inputs into one matrix, and all the biases into one matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3987" target="_blank">01:06:27.720</a></span> | <span class="t">And then you can write it as this large w times x plus b equals the representation of the same thing of running a bunch of these neurons together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=3996" target="_blank">01:06:36.720</a></span> | <span class="t">If you don't know your matrix multiplication, I like this website, which has a nice interactive visual demonstration of what matrix multiplication looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4005" target="_blank">01:06:45.720</a></span> | <span class="t">So you hit this, and then you keep going through step, and you can kind of convince yourself that what I showed here matches what's on that web page.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4013" target="_blank">01:06:53.720</a></span> | <span class="t">So the key property, though, and why MLPs are so important is that they are universal, trainable approximators to any function purely from its input and output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4024" target="_blank">01:07:04.720</a></span> | <span class="t">With enough neurons, an MLP can approximate almost any function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4030" target="_blank">01:07:10.720</a></span> | <span class="t">So let's just take a simple example like a parabola.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4033" target="_blank">01:07:13.720</a></span> | <span class="t">And we're going to imagine we're going to use a simple neural network with a ReLU activation to try and approximate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4038" target="_blank">01:07:18.720</a></span> | <span class="t">We'll have one input node, one output, because we have an x going into a y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4042" target="_blank">01:07:22.720</a></span> | <span class="t">And then let's just use two nodes in our hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4045" target="_blank">01:07:25.720</a></span> | <span class="t">And those two nodes will use a ReLU activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4047" target="_blank">01:07:27.720</a></span> | <span class="t">Well, without doing the math, you can kind of imagine, just by matching shapes, how you might do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4052" target="_blank">01:07:32.720</a></span> | <span class="t">I'll take the ReLU, and I can take this part of the ReLU, and I can match it to the right half of my parabola.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4059" target="_blank">01:07:39.720</a></span> | <span class="t">And then I can take another ReLU, I can flip it, and then I can match it to the left half.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4065" target="_blank">01:07:45.720</a></span> | <span class="t">And then I can add them together, and I've got some kind of approximation to my parabola,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4069" target="_blank">01:07:49.720</a></span> | <span class="t">at least on this domain of x that we're looking at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4072" target="_blank">01:07:52.720</a></span> | <span class="t">And that's what I've done in this example here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4076" target="_blank">01:07:56.720</a></span> | <span class="t">which, let's see if the Wi-Fi behaves for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4086" target="_blank">01:08:06.720</a></span> | <span class="t">There we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4087" target="_blank">01:08:07.720</a></span> | <span class="t">So here you can actually do this simple neural network, and you can try to match it to a parabola,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4092" target="_blank">01:08:12.720</a></span> | <span class="t">and you can interactively move this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4094" target="_blank">01:08:14.720</a></span> | <span class="t">And you can see, and this is a measure of error up here, called mean square error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4098" target="_blank">01:08:18.720</a></span> | <span class="t">And you can try and see how good you can get your level of error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4101" target="_blank">01:08:21.720</a></span> | <span class="t">And you can see the action here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4102" target="_blank">01:08:22.720</a></span> | <span class="t">We're basically changing the different line pieces that we're using that are made out of ReLUs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4108" target="_blank">01:08:28.720</a></span> | <span class="t">to try and match this parabola.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4110" target="_blank">01:08:30.720</a></span> | <span class="t">This can kind of give you a feel for what the model is actually trying to do when it's trying to match a function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4116" target="_blank">01:08:36.720</a></span> | <span class="t">And you can think of this like more neurons means more lines, which means a better approximation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4123" target="_blank">01:08:43.720</a></span> | <span class="t">So with eight neurons, the parabola looks like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4126" target="_blank">01:08:46.720</a></span> | <span class="t">With 20 neurons, it looks like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4128" target="_blank">01:08:48.720</a></span> | <span class="t">And with 200 neurons, you can barely tell the difference, at least at this scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4132" target="_blank">01:08:52.720</a></span> | <span class="t">But the other key thing is that we don't have to use trial and error to find this out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4137" target="_blank">01:08:57.720</a></span> | <span class="t">especially once you get 200 neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4139" target="_blank">01:08:59.720</a></span> | <span class="t">Imagine doing what I was doing with that fiddling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4141" target="_blank">01:09:01.720</a></span> | <span class="t">Oh, so this is a theorem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4144" target="_blank">01:09:04.720</a></span> | <span class="t">With enough neurons that you can approximate almost any function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4147" target="_blank">01:09:07.720</a></span> | <span class="t">It's called the universal approximation theorem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4149" target="_blank">01:09:09.720</a></span> | <span class="t">But the key thing is you combine that with a special algorithm called backpropagation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4153" target="_blank">01:09:13.720</a></span> | <span class="t">which lets us learn any function purely from its inputs and outputs without having to twiddle those knobs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4160" target="_blank">01:09:20.720</a></span> | <span class="t">It'll do it for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4162" target="_blank">01:09:22.720</a></span> | <span class="t">And that's important because what we're going to ask this multilayer perceptron to do is the core mechanism job of a transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4169" target="_blank">01:09:29.720</a></span> | <span class="t">which is I'm going to give it a word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4171" target="_blank">01:09:31.720</a></span> | <span class="t">I'm going to give it the embedding of a token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4173" target="_blank">01:09:33.720</a></span> | <span class="t">And I'm going to ask it, predict what the embedding of the next token is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4177" target="_blank">01:09:37.720</a></span> | <span class="t">I don't know what that function is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4179" target="_blank">01:09:39.720</a></span> | <span class="t">But I can grab text on the internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4181" target="_blank">01:09:41.720</a></span> | <span class="t">And I can grab one word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4182" target="_blank">01:09:42.720</a></span> | <span class="t">And I can grab the word that comes after it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4184" target="_blank">01:09:44.720</a></span> | <span class="t">And I can give it to the perceptron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4185" target="_blank">01:09:45.720</a></span> | <span class="t">And I can say, learn from this input and output what that mapping function is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4189" target="_blank">01:09:49.720</a></span> | <span class="t">As crazy as it might be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4190" target="_blank">01:09:50.720</a></span> | <span class="t">And what will happen is backpropagation will look at the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4193" target="_blank">01:09:53.720</a></span> | <span class="t">It'll look at the output we got from the model when it was initially randomized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4197" target="_blank">01:09:57.720</a></span> | <span class="t">It'll look at the ground truth from what came pulled from the internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4201" target="_blank">01:10:01.720</a></span> | <span class="t">And it will look at how we need to adjust the parameters and weights to change the perceptron to get more accurate at making that prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4207" target="_blank">01:10:07.720</a></span> | <span class="t">And after enough iterations, it'll get better and better and actually begin to start matching the function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4213" target="_blank">01:10:13.720</a></span> | <span class="t">The canonical analogy to understand what's happening in backpropagation, also known as, for our purposes, gradient descent, is a lost hiker trying to get down a foggy mountain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4224" target="_blank">01:10:24.720</a></span> | <span class="t">And you're at the top of this foggy mountain as a hiker.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4227" target="_blank">01:10:27.720</a></span> | <span class="t">I've actually been in this situation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4228" target="_blank">01:10:28.720</a></span> | <span class="t">And you can't see any of the landscape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4231" target="_blank">01:10:31.720</a></span> | <span class="t">So you don't know which direction to get off the mountain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4234" target="_blank">01:10:34.720</a></span> | <span class="t">Well, the one thing you can do is you can look down at the ground.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4238" target="_blank">01:10:38.720</a></span> | <span class="t">And you can say, oh, whichever way is going down, that's going to be the area towards getting off the mountain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4246" target="_blank">01:10:46.720</a></span> | <span class="t">By the way, actually, in real life, I have tried this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4248" target="_blank">01:10:48.720</a></span> | <span class="t">It does not work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4249" target="_blank">01:10:49.720</a></span> | <span class="t">This is how I got lost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4251" target="_blank">01:10:51.720</a></span> | <span class="t">So this is a hiker representing -- the hiker in this analogy represents the model parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4258" target="_blank">01:10:58.720</a></span> | <span class="t">It's in some space, but we don't know where to move the model parameters to get the least amount of error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4264" target="_blank">01:11:04.720</a></span> | <span class="t">And the mountain represents the error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4266" target="_blank">01:11:06.720</a></span> | <span class="t">It represents how wrong we are at the current position of where the hiker is or where the parameters are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4273" target="_blank">01:11:13.720</a></span> | <span class="t">And the mountain is foggy because we can tell the amount of error when I give it an input and it makes a prediction for the next token and we compare it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4280" target="_blank">01:11:20.720</a></span> | <span class="t">We can say, oh, it's wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4281" target="_blank">01:11:21.720</a></span> | <span class="t">But we don't know how to shift the model parameters to get lower error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4286" target="_blank">01:11:26.720</a></span> | <span class="t">But calculus will give us that slope.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4288" target="_blank">01:11:28.720</a></span> | <span class="t">It will tell us where the elevation is going down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4291" target="_blank">01:11:31.720</a></span> | <span class="t">It won't tell us what the whole mountain looks like, but it will just say where you are standing right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4295" target="_blank">01:11:35.720</a></span> | <span class="t">Go in this direction and you'll decrease the amount of error you've got.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4299" target="_blank">01:11:39.720</a></span> | <span class="t">And you use that to find your way down the mountain, so to speak, of the parameters and find a minima.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4306" target="_blank">01:11:46.720</a></span> | <span class="t">So that brings us to the MLP stage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4309" target="_blank">01:11:49.720</a></span> | <span class="t">It's steps 13, 14, and 15 in the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4313" target="_blank">01:11:53.720</a></span> | <span class="t">So let's go back here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4318" target="_blank">01:11:58.720</a></span> | <span class="t">So you can see all the formulas here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4322" target="_blank">01:12:02.720</a></span> | <span class="t">What I'm going to do is I'm going to show you them in slide form.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4326" target="_blank">01:12:06.720</a></span> | <span class="t">And I'm going to graphically show what's happening in the GPT-2 MLP stage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4331" target="_blank">01:12:11.720</a></span> | <span class="t">So GPT-2's MLP has only one hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4335" target="_blank">01:12:15.720</a></span> | <span class="t">The input layer right here has 768 of these X inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4340" target="_blank">01:12:20.720</a></span> | <span class="t">Why?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4341" target="_blank">01:12:21.720</a></span> | <span class="t">Because we're going to give it an embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4343" target="_blank">01:12:23.720</a></span> | <span class="t">We're going to say, here's the embedding predicted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4345" target="_blank">01:12:25.720</a></span> | <span class="t">So I'm going to give it 768 numbers of the preceding token that I want it to predict afterwards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4351" target="_blank">01:12:31.720</a></span> | <span class="t">And then its output layer is 768 numbers for the predicted embedding token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4356" target="_blank">01:12:36.720</a></span> | <span class="t">So input and output are 768.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4357" target="_blank">01:12:37.720</a></span> | <span class="t">They are our embedding dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4359" target="_blank">01:12:39.720</a></span> | <span class="t">And then it's got one hidden layer, which is bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4362" target="_blank">01:12:42.720</a></span> | <span class="t">This ratio of four times the embedding dimension turns out to be empirically useful and lots of models do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4367" target="_blank">01:12:47.720</a></span> | <span class="t">But I don't know if you could figure that out just from first principles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4371" target="_blank">01:12:51.720</a></span> | <span class="t">It's kind of empirically been determined.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4373" target="_blank">01:12:53.720</a></span> | <span class="t">And we have three steps here for applying our weights and bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4378" target="_blank">01:12:58.720</a></span> | <span class="t">Applying our activation function, which in this case is the gelu activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4382" target="_blank">01:13:02.720</a></span> | <span class="t">And then we project that back down to our embedding dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4385" target="_blank">01:13:05.720</a></span> | <span class="t">So you can think of it this way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4387" target="_blank">01:13:07.720</a></span> | <span class="t">We take our embeddings from a previous step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4390" target="_blank">01:13:10.720</a></span> | <span class="t">And then what we're doing is we're taking those embedding values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4393" target="_blank">01:13:13.720</a></span> | <span class="t">And we're going to send each embedding value into its position inside the MLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4398" target="_blank">01:13:18.720</a></span> | <span class="t">And then we run it through the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4401" target="_blank">01:13:21.720</a></span> | <span class="t">And then these are now the embedding values that come out are the embedding values of the predicted token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4408" target="_blank">01:13:28.720</a></span> | <span class="t">And then we take the next token in our prompt and then run it through the MLP again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4413" target="_blank">01:13:33.720</a></span> | <span class="t">In practice, you do this in parallel, but conceptually you can think of it this way as happening one after the other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4418" target="_blank">01:13:38.720</a></span> | <span class="t">Okay, so what's happening in these three steps is really just a combination of matrix, add, and multiply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4425" target="_blank">01:13:45.720</a></span> | <span class="t">So we take the result of our previous step, which is step 12, which I have not gone into.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4430" target="_blank">01:13:50.720</a></span> | <span class="t">And then we have some learned weight matrix, which you see is MLP FC weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4435" target="_blank">01:13:55.720</a></span> | <span class="t">That's how they decided to name it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4437" target="_blank">01:13:57.720</a></span> | <span class="t">And we multiply those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4439" target="_blank">01:13:59.720</a></span> | <span class="t">So here's our matrix multiply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4441" target="_blank">01:14:01.720</a></span> | <span class="t">And then we add it to another bias matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4444" target="_blank">01:14:04.720</a></span> | <span class="t">So written as matrix multiplication, step 13 is just step 12 times some weight matrix plus some learned bias matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4452" target="_blank">01:14:12.720</a></span> | <span class="t">Then we apply a GELU activation function, which I showed the diagram earlier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4457" target="_blank">01:14:17.720</a></span> | <span class="t">The details are not really important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4459" target="_blank">01:14:19.720</a></span> | <span class="t">And then we then do our projection, which is the remaining step to get down to the 768.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4465" target="_blank">01:14:25.720</a></span> | <span class="t">So we take the result of the previous step, which was the activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4469" target="_blank">01:14:29.720</a></span> | <span class="t">We apply a different learned weight matrix, a new set of weights that gets learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4474" target="_blank">01:14:34.720</a></span> | <span class="t">And that's due with a matrix multiply and then another matrix add.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4477" target="_blank">01:14:37.720</a></span> | <span class="t">So step 15 is just step 14 times some weight matrix plus some projection matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4488" target="_blank">01:14:48.720</a></span> | <span class="t">Before we leave backpropagation, you might remember that I talked about how embeddings are learned by the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4495" target="_blank">01:14:55.720</a></span> | <span class="t">Both the token and in the case of GPT-2, they started learning the position embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4499" target="_blank">01:14:59.720</a></span> | <span class="t">So the key thing I want you to remember is that backpropagation is a generic optimization algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4504" target="_blank">01:15:04.720</a></span> | <span class="t">It is not just for the weights and biases of the MLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4507" target="_blank">01:15:07.720</a></span> | <span class="t">It can be used for other parts of the transformer too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4510" target="_blank">01:15:10.720</a></span> | <span class="t">And in fact, it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4511" target="_blank">01:15:11.720</a></span> | <span class="t">It's used for the token embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4512" target="_blank">01:15:12.720</a></span> | <span class="t">It's used for position embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4514" target="_blank">01:15:14.720</a></span> | <span class="t">It's used for all the parameters and attention, the queries, keys, and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4517" target="_blank">01:15:17.720</a></span> | <span class="t">If you've heard that term, all use backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4520" target="_blank">01:15:20.720</a></span> | <span class="t">Even other parts, layer normalization and the like, use backpropagation to get optimized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4525" target="_blank">01:15:25.720</a></span> | <span class="t">And the analogy I want you to think about is backpropagation and optimizing a model this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4530" target="_blank">01:15:30.720</a></span> | <span class="t">way is like a chef imitating a dish.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4533" target="_blank">01:15:33.720</a></span> | <span class="t">If you remember those cooking shows, they tell the chef, here are your ingredients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4538" target="_blank">01:15:38.720</a></span> | <span class="t">Here's the final dish.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4539" target="_blank">01:15:39.720</a></span> | <span class="t">And you've got to taste it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4540" target="_blank">01:15:40.720</a></span> | <span class="t">And maybe we give you some tools that you've got to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4542" target="_blank">01:15:42.720</a></span> | <span class="t">And you've got to make this dish.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4543" target="_blank">01:15:43.720</a></span> | <span class="t">And then they have them compete against somebody else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4546" target="_blank">01:15:46.720</a></span> | <span class="t">We kind of do the same thing with the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4548" target="_blank">01:15:48.720</a></span> | <span class="t">We basically give it some input text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4550" target="_blank">01:15:50.720</a></span> | <span class="t">We tell it, here's the next token afterwards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4552" target="_blank">01:15:52.720</a></span> | <span class="t">We want you to imitate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4553" target="_blank">01:15:53.720</a></span> | <span class="t">And then we define the steps in the architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4556" target="_blank">01:15:56.720</a></span> | <span class="t">We want you to do attention, then MP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4558" target="_blank">01:15:58.720</a></span> | <span class="t">And then you, the chef, backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4560" target="_blank">01:16:00.720</a></span> | <span class="t">Decide how much to mix of each ingredient at each step to get the desired output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4565" target="_blank">01:16:05.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4566" target="_blank">01:16:06.720</a></span> | <span class="t">Iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4567" target="_blank">01:16:07.720</a></span> | <span class="t">How are we doing on time?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4568" target="_blank">01:16:08.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4569" target="_blank">01:16:09.720</a></span> | <span class="t">Good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4570" target="_blank">01:16:10.720</a></span> | <span class="t">So, I've got this in our simplified diagram, which is 12x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4574" target="_blank">01:16:14.720</a></span> | <span class="t">Which represents that what happens is we run attention and we run the perceptron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4579" target="_blank">01:16:19.720</a></span> | <span class="t">But we ask the model to continue to refine iteratively its prediction for what the next model is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4585" target="_blank">01:16:25.720</a></span> | <span class="t">It has all the tokens talk to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4588" target="_blank">01:16:28.720</a></span> | <span class="t">It makes the next token prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4589" target="_blank">01:16:29.720</a></span> | <span class="t">And then it does it again and again and again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4591" target="_blank">01:16:31.720</a></span> | <span class="t">So, it goes through each of these steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4594" target="_blank">01:16:34.720</a></span> | <span class="t">In the case of GPT2 small, it's 12 times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4597" target="_blank">01:16:37.720</a></span> | <span class="t">In the case of your modern state-of-the-art model, it's going to be many more times than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4601" target="_blank">01:16:41.720</a></span> | <span class="t">that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4602" target="_blank">01:16:42.720</a></span> | <span class="t">The key thing I want you to remember though is that each block is performing identical operations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4607" target="_blank">01:16:47.720</a></span> | <span class="t">but the weights are different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4609" target="_blank">01:16:49.720</a></span> | <span class="t">Each one has different parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4611" target="_blank">01:16:51.720</a></span> | <span class="t">And you can see this if you look at the code here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4614" target="_blank">01:16:54.720</a></span> | <span class="t">You see, for example, here we're grabbing the weights of, in this case, the MLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4618" target="_blank">01:16:58.720</a></span> | <span class="t">And you see that it's got MLP_C.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4621" target="_blank">01:17:01.720</a></span> | <span class="t">This is just the name for that stage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4622" target="_blank">01:17:02.720</a></span> | <span class="t">But the key thing I want you to pay attention to is this H11.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4625" target="_blank">01:17:05.720</a></span> | <span class="t">That basically is saying hidden block or hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4628" target="_blank">01:17:08.720</a></span> | <span class="t">This is grabbing the hidden layer MLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4632" target="_blank">01:17:12.720</a></span> | <span class="t">Sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4633" target="_blank">01:17:13.720</a></span> | <span class="t">The MLP's weight matrix for the 11th layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4637" target="_blank">01:17:17.720</a></span> | <span class="t">If we were doing it for the first block, it would say H0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4641" target="_blank">01:17:21.720</a></span> | <span class="t">So, inside the implementation of this code, we go to the iteration step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4646" target="_blank">01:17:26.720</a></span> | <span class="t">All this kind of messy code is doing is it's grabbing the DOM objects for the blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4652" target="_blank">01:17:32.720</a></span> | <span class="t">And then it's going into each of the formulas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4654" target="_blank">01:17:34.720</a></span> | <span class="t">And it's doing a string replace.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4656" target="_blank">01:17:36.720</a></span> | <span class="t">And it's just changing that H value to each one for every iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4660" target="_blank">01:17:40.720</a></span> | <span class="t">And then reruns the entire set again, just to simulate what would actually be happening in a model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4665" target="_blank">01:17:45.720</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4666" target="_blank">01:17:46.720</a></span> | <span class="t">Last step is the language head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4669" target="_blank">01:17:49.720</a></span> | <span class="t">So, this is when we finally get to take our predicted token embedding and turn it back into a token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4676" target="_blank">01:17:56.720</a></span> | <span class="t">So, what we do is we take the last MLP's of the last block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4681" target="_blank">01:18:01.720</a></span> | <span class="t">So, this one here, we got our most refined prediction for what the token embedding is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4690" target="_blank">01:18:10.720</a></span> | <span class="t">And then we're going to turn that into a token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4693" target="_blank">01:18:13.720</a></span> | <span class="t">So, it's going to go through an operation called layer norm, which we haven't gone through in detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4697" target="_blank">01:18:17.720</a></span> | <span class="t">But what we get out of this step is the embedding of the predicted next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4701" target="_blank">01:18:21.720</a></span> | <span class="t">This is what it's saying the next token is going to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4703" target="_blank">01:18:23.720</a></span> | <span class="t">But it's represented as 768 numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4705" target="_blank">01:18:25.720</a></span> | <span class="t">It's not represented as a token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4707" target="_blank">01:18:27.720</a></span> | <span class="t">So, if you translate it back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4708" target="_blank">01:18:28.720</a></span> | <span class="t">So, the way we're going to do this is we're going to go back to that matrix we had.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4711" target="_blank">01:18:31.720</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4712" target="_blank">01:18:32.720</a></span> | <span class="t">Our dictionary of tokens to embeddings, model_wte.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4715" target="_blank">01:18:35.720</a></span> | <span class="t">And we're going to take that and we're going to multiply it times our predicted next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4721" target="_blank">01:18:41.720</a></span> | <span class="t">And we get this matrix here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4723" target="_blank">01:18:43.720</a></span> | <span class="t">So, one thing to remember, it's very helpful to somebody to think about the dimensions of these things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4727" target="_blank">01:18:47.720</a></span> | <span class="t">So, our token embeddings was 50,257 tall.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4732" target="_blank">01:18:52.720</a></span> | <span class="t">It represents a row for every one of our vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4735" target="_blank">01:18:55.720</a></span> | <span class="t">And then it's 768 dimensions wide for the embeddings for each dimension, embeddings for each token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4741" target="_blank">01:19:01.720</a></span> | <span class="t">And when we multiply it times this column, which is 768 dimensions representing embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4747" target="_blank">01:19:07.720</a></span> | <span class="t">what we're basically doing is we're getting this, which is a column of 50,257, but only one wide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4754" target="_blank">01:19:14.720</a></span> | <span class="t">Each one of those, right, is a dot product of the predicted embedding against one of the known token embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4760" target="_blank">01:19:20.720</a></span> | <span class="t">What we have is 50,000 scores for how similar the embedding we got is to each of the embeddings in our dictionary of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4769" target="_blank">01:19:29.720</a></span> | <span class="t">So, you can think of these as they're called logits or logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4772" target="_blank">01:19:32.720</a></span> | <span class="t">They are really token scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4774" target="_blank">01:19:34.720</a></span> | <span class="t">How close is the embedding we got to our dictionary of tokens that we have?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4779" target="_blank">01:19:39.720</a></span> | <span class="t">And so, the more similar a prediction in a token, the higher that entry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4784" target="_blank">01:19:44.720</a></span> | <span class="t">To turn this into a probability distribution, we have a problem because these are just numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4789" target="_blank">01:19:49.720</a></span> | <span class="t">A probability distribution has to sum up to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4792" target="_blank">01:19:52.720</a></span> | <span class="t">So, then we put it through a special normalization operation called a softmax, and that will make sure they all sum to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4799" target="_blank">01:19:59.720</a></span> | <span class="t">And then we can interpret this as entirely a probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4802" target="_blank">01:20:02.720</a></span> | <span class="t">Each one of those normalized token scores will then basically represent the probability of that token in the representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4811" target="_blank">01:20:11.720</a></span> | <span class="t">So, you can see that here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4816" target="_blank">01:20:16.720</a></span> | <span class="t">So, here are our logits, right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4819" target="_blank">01:20:19.720</a></span> | <span class="t">And so, the logit, you know, for this negative 135.9 represents a score of some kind of the similarity of the very first token in our dictionary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4831" target="_blank">01:20:31.720</a></span> | <span class="t">against whatever this predicted embedding was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4834" target="_blank">01:20:34.720</a></span> | <span class="t">And then, the code for predicted token is not actually doing what's doing a probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4841" target="_blank">01:20:41.720</a></span> | <span class="t">It's doing what is called greedy sampling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4844" target="_blank">01:20:44.720</a></span> | <span class="t">It's temperature zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4845" target="_blank">01:20:45.720</a></span> | <span class="t">It always picks the highest probability token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4848" target="_blank">01:20:48.720</a></span> | <span class="t">And that is done so that when you're using this, you can compare it against the same GPT-2 you'd get from OpenAI's code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4855" target="_blank">01:20:55.720</a></span> | <span class="t">or from Hugging Face Transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4857" target="_blank">01:20:57.720</a></span> | <span class="t">You can compare like for like, and you'll get the same result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4860" target="_blank">01:21:00.720</a></span> | <span class="t">In fact, if you-- do I still have this up?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4865" target="_blank">01:21:05.720</a></span> | <span class="t">Yeah, right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4866" target="_blank">01:21:06.720</a></span> | <span class="t">If I do, Mike is quick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4869" target="_blank">01:21:09.720</a></span> | <span class="t">He moves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4871" target="_blank">01:21:11.720</a></span> | <span class="t">So, this is using Hugging Face Transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4874" target="_blank">01:21:14.720</a></span> | <span class="t">We might have an issue if we've got network being slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4878" target="_blank">01:21:18.720</a></span> | <span class="t">Let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4879" target="_blank">01:21:19.720</a></span> | <span class="t">So, we've entered the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4880" target="_blank">01:21:20.720</a></span> | <span class="t">Yeah, the network's being a little slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4882" target="_blank">01:21:22.720</a></span> | <span class="t">We'll come back to this guy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4883" target="_blank">01:21:23.720</a></span> | <span class="t">It will give us the same value of quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4885" target="_blank">01:21:25.720</a></span> | <span class="t">Now, there are other ways to sample than just simply taking a-- running a random number generator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4890" target="_blank">01:21:30.720</a></span> | <span class="t">and using the probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4892" target="_blank">01:21:32.720</a></span> | <span class="t">One of those is called top K, which is you say, I don't want the really unlikely words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4897" target="_blank">01:21:37.720</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4898" target="_blank">01:21:38.720</a></span> | <span class="t">If it's Mike is quick, he moves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4899" target="_blank">01:21:39.720</a></span> | <span class="t">I don't want to accidentally end up with haircut.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4901" target="_blank">01:21:41.720</a></span> | <span class="t">You know, even if it's 1% of the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4903" target="_blank">01:21:43.720</a></span> | <span class="t">So, what you do is you define a cutoff and you say, okay, the top 10 tokens, give me those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4908" target="_blank">01:21:48.720</a></span> | <span class="t">And then you renormalize the probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4910" target="_blank">01:21:50.720</a></span> | <span class="t">Another way is called nucleus or top P sampling, which is instead of saying, give me just 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4915" target="_blank">01:21:55.720</a></span> | <span class="t">tokens, just give me as many tokens as it takes to get 80% or 90% total probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4921" target="_blank">01:22:01.720</a></span> | <span class="t">So, I get most of the likely tokens and then I renormalize to those 90%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4927" target="_blank">01:22:07.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4928" target="_blank">01:22:08.720</a></span> | <span class="t">Let's see how we're doing on time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4930" target="_blank">01:22:10.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4931" target="_blank">01:22:11.720</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4932" target="_blank">01:22:12.720</a></span> | <span class="t">So, lastly, actually, did this come back?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4934" target="_blank">01:22:14.720</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4935" target="_blank">01:22:15.720</a></span> | <span class="t">Came back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4936" target="_blank">01:22:16.720</a></span> | <span class="t">You can see it says Mike is quick, he moves, there is quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4940" target="_blank">01:22:20.720</a></span> | <span class="t">So, if you run GPT2 small using, you know, hugging face transformers, you should get the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4945" target="_blank">01:22:25.720</a></span> | <span class="t">answer, that next token, as you get here, Mike is quick, he moves, and you get this final result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4950" target="_blank">01:22:30.720</a></span> | <span class="t">right here, predicted token of quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4952" target="_blank">01:22:32.720</a></span> | <span class="t">And then it will tell us what the token ID was and what the maximum logit turned out to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4956" target="_blank">01:22:36.720</a></span> | <span class="t">from the previous table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4957" target="_blank">01:22:37.720</a></span> | <span class="t">So, this negative 129, so what is this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4960" target="_blank">01:22:40.720</a></span> | <span class="t">2952.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4961" target="_blank">01:22:41.720</a></span> | <span class="t">So, let's go here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4963" target="_blank">01:22:43.720</a></span> | <span class="t">Row 2952, column one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4968" target="_blank">01:22:48.720</a></span> | <span class="t">I think there will be an off by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4970" target="_blank">01:22:50.720</a></span> | <span class="t">Yeah, there it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4972" target="_blank">01:22:52.720</a></span> | <span class="t">2953, because this is one index, because it's like a spreadsheet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4975" target="_blank">01:22:55.720</a></span> | <span class="t">So, you see the negative 129.44.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4978" target="_blank">01:22:58.720</a></span> | <span class="t">That was the highest logit in the entire column.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4982" target="_blank">01:23:02.720</a></span> | <span class="t">And all the code that it's running here is doing, next is just going to pick what the most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4987" target="_blank">01:23:07.720</a></span> | <span class="t">highest value was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4988" target="_blank">01:23:08.720</a></span> | <span class="t">And when it finds it, 2952, it converts it back to our token dictionary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4992" target="_blank">01:23:12.720</a></span> | <span class="t">So, you can see there's that same value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4995" target="_blank">01:23:15.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=4996" target="_blank">01:23:16.720</a></span> | <span class="t">Now, let's talk about chat GPT versus GPT2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5001" target="_blank">01:23:21.720</a></span> | <span class="t">So, GPT2 was definitely groundbreaking when it first came out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5005" target="_blank">01:23:25.720</a></span> | <span class="t">It was famously considered too dangerous to release.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5008" target="_blank">01:23:28.720</a></span> | <span class="t">But chat GPT was, you know, earth shattering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5012" target="_blank">01:23:32.720</a></span> | <span class="t">What were the intervening, you know, what were the additional innovations in those intervening</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5018" target="_blank">01:23:38.720</a></span> | <span class="t">years?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5019" target="_blank">01:23:39.720</a></span> | <span class="t">Well, for the most part, it was a lot of the same architecture, just more scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5026" target="_blank">01:23:46.720</a></span> | <span class="t">So, if you looked at a modern transformer, you would probably see a lot of the same parts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5029" target="_blank">01:23:49.720</a></span> | <span class="t">but bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5030" target="_blank">01:23:50.720</a></span> | <span class="t">And some of the parts might be upgraded or switched out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5032" target="_blank">01:23:52.720</a></span> | <span class="t">So, certain pieces might have changed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5034" target="_blank">01:23:54.720</a></span> | <span class="t">So, attention mechanisms changed and other things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5037" target="_blank">01:23:57.720</a></span> | <span class="t">But the biggest difference you should know about is that the job and the training were actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5043" target="_blank">01:24:03.720</a></span> | <span class="t">changed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5044" target="_blank">01:24:04.720</a></span> | <span class="t">The key thing to understand is that predicting the next word is not the same as being a chatbot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5049" target="_blank">01:24:09.720</a></span> | <span class="t">Our GPT2 -- actually, I'll show you this here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5052" target="_blank">01:24:12.720</a></span> | <span class="t">Our GPT2 is basically trained to predict next words from looking at text on the internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5059" target="_blank">01:24:19.720</a></span> | <span class="t">So, it is a next word predictor only for internet text, but not for being a helpful assistant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5064" target="_blank">01:24:24.720</a></span> | <span class="t">So, if I give you this example, like first name, let's see if we'll come back quickly enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5069" target="_blank">01:24:29.720</a></span> | <span class="t">It says, first name, colon, password, email, colon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5073" target="_blank">01:24:33.720</a></span> | <span class="t">Why does it do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5074" target="_blank">01:24:34.720</a></span> | <span class="t">Because it's been trained on the internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5076" target="_blank">01:24:36.720</a></span> | <span class="t">And what does it see a lot of is forms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5079" target="_blank">01:24:39.720</a></span> | <span class="t">So, it's like, oh, I'm in the middle of a form.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5082" target="_blank">01:24:42.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5083" target="_blank">01:24:43.720</a></span> | <span class="t">First name, password, email.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5084" target="_blank">01:24:44.720</a></span> | <span class="t">So, here's another one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5085" target="_blank">01:24:45.720</a></span> | <span class="t">I tried this in class once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5087" target="_blank">01:24:47.720</a></span> | <span class="t">Hello, class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5088" target="_blank">01:24:48.720</a></span> | <span class="t">Anyone want to guess what this is going to output?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5091" target="_blank">01:24:51.720</a></span> | <span class="t">What?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5092" target="_blank">01:24:52.720</a></span> | <span class="t">Okay, you're smarter than I am.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5095" target="_blank">01:24:55.720</a></span> | <span class="t">I thought it would say hello, teacher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5097" target="_blank">01:24:57.720</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5098" target="_blank">01:24:58.720</a></span> | <span class="t">It outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5099" target="_blank">01:24:59.720</a></span> | <span class="t">Hello, class, foo, brace, public static voice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5102" target="_blank">01:25:02.720</a></span> | <span class="t">It's a code model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5103" target="_blank">01:25:03.720</a></span> | <span class="t">It's hidden in there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5104" target="_blank">01:25:04.720</a></span> | <span class="t">But we didn't know that, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5105" target="_blank">01:25:05.720</a></span> | <span class="t">But you can ask it, you know, helpful questions like, what is the capital of France?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5113" target="_blank">01:25:13.720</a></span> | <span class="t">It says the capital of France is Paris.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5117" target="_blank">01:25:17.720</a></span> | <span class="t">That's helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5118" target="_blank">01:25:18.720</a></span> | <span class="t">So, embedded in that is some of the information we want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5121" target="_blank">01:25:21.720</a></span> | <span class="t">But also a mix of other things we may not want or can't control.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5125" target="_blank">01:25:25.720</a></span> | <span class="t">So, what we have to do is figure out how to change it and shift it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5130" target="_blank">01:25:30.720</a></span> | <span class="t">And so, this is the four-step pipeline for doing that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5133" target="_blank">01:25:33.720</a></span> | <span class="t">And this is, I want to emphasize, mostly a training difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5138" target="_blank">01:25:38.720</a></span> | <span class="t">So, you've got GPT-2 here on the left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5140" target="_blank">01:25:40.720</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5141" target="_blank">01:25:41.720</a></span> | <span class="t">It is pre-trained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5142" target="_blank">01:25:42.720</a></span> | <span class="t">It knows how to imitate text on the internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5144" target="_blank">01:25:44.720</a></span> | <span class="t">It is what is called a base model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5146" target="_blank">01:25:46.720</a></span> | <span class="t">You've got instruct GPT or chat GPT all the way here on the right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5151" target="_blank">01:25:51.720</a></span> | <span class="t">And what we're doing is a series of steps to kind of elicit or pull out the behaviors we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5157" target="_blank">01:25:57.720</a></span> | <span class="t">want or to train the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5158" target="_blank">01:25:58.720</a></span> | <span class="t">So, the first thing we do is we take the model and we train it to imitate text on the internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5162" target="_blank">01:26:02.720</a></span> | <span class="t">So, that's the first step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5163" target="_blank">01:26:03.720</a></span> | <span class="t">That's what we've got with GPT-2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5164" target="_blank">01:26:04.720</a></span> | <span class="t">That's what OpenAI did for us with GPT-2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5166" target="_blank">01:26:06.720</a></span> | <span class="t">The next thing we want to do is we want to train the model on examples of what a helpful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5171" target="_blank">01:26:11.720</a></span> | <span class="t">assistant is like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5172" target="_blank">01:26:12.720</a></span> | <span class="t">And you'll see this right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5173" target="_blank">01:26:13.720</a></span> | <span class="t">Ideal assistant responses, 10,000 to 100,000 examples of prompt and response that were written</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5179" target="_blank">01:26:19.720</a></span> | <span class="t">by contractors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5180" target="_blank">01:26:20.720</a></span> | <span class="t">So, what does this look like?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5183" target="_blank">01:26:23.720</a></span> | <span class="t">You can go on GitHub to the Stanford Alpaca data set is a good example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5188" target="_blank">01:26:28.720</a></span> | <span class="t">And this is JSON.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5189" target="_blank">01:26:29.720</a></span> | <span class="t">You can open this up and you can read it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5191" target="_blank">01:26:31.720</a></span> | <span class="t">It's like, give three tips for staying healthy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5194" target="_blank">01:26:34.720</a></span> | <span class="t">Eat a balanced diet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5196" target="_blank">01:26:36.720</a></span> | <span class="t">Make sure to include plenty of fruits and vegetables.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5198" target="_blank">01:26:38.720</a></span> | <span class="t">What are the three primary colors?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5200" target="_blank">01:26:40.720</a></span> | <span class="t">Red, blue, yellow, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5202" target="_blank">01:26:42.720</a></span> | <span class="t">So, what we're doing is we're training it to augment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5205" target="_blank">01:26:45.720</a></span> | <span class="t">We're augmenting all that internet data with a subset of how we want it to behave to force</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5210" target="_blank">01:26:50.720</a></span> | <span class="t">its behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5211" target="_blank">01:26:51.720</a></span> | <span class="t">It's kind of learning to imitate specifically these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5215" target="_blank">01:26:55.720</a></span> | <span class="t">And it will learn more about being a helpful assistant that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5218" target="_blank">01:26:58.720</a></span> | <span class="t">But we're still not done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5220" target="_blank">01:27:00.720</a></span> | <span class="t">It's like it's this last stage called RLHF, or reinforcement learning from human feedback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5225" target="_blank">01:27:05.720</a></span> | <span class="t">And to understand this, you have to understand what RL is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5230" target="_blank">01:27:10.720</a></span> | <span class="t">Let me do the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5235" target="_blank">01:27:15.720</a></span> | <span class="t">Oh, let's do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5242" target="_blank">01:27:22.720</a></span> | <span class="t">Stop annotating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5244" target="_blank">01:27:24.720</a></span> | <span class="t">So the canonical example, let's zoom this to fit window.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5249" target="_blank">01:27:29.720</a></span> | <span class="t">There we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5250" target="_blank">01:27:30.720</a></span> | <span class="t">For RL is like playing a game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5254" target="_blank">01:27:34.720</a></span> | <span class="t">So imagine you're a computer trying to play a game like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5257" target="_blank">01:27:37.720</a></span> | <span class="t">And you've got maybe a robot player that's navigating a maze.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5261" target="_blank">01:27:41.720</a></span> | <span class="t">And you've got some monsters and obstacles that you'll die.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5264" target="_blank">01:27:44.720</a></span> | <span class="t">You've got some power ops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5265" target="_blank">01:27:45.720</a></span> | <span class="t">And then you've got a goal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5267" target="_blank">01:27:47.720</a></span> | <span class="t">And what reinforcement learning will do is it will explore these paths.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5272" target="_blank">01:27:52.720</a></span> | <span class="t">And be like, oh, I failed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5275" target="_blank">01:27:55.720</a></span> | <span class="t">Oh, this seems to work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5276" target="_blank">01:27:56.720</a></span> | <span class="t">Oh, I failed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5277" target="_blank">01:27:57.720</a></span> | <span class="t">Oh, uh, oh, I failed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5280" target="_blank">01:28:00.720</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5281" target="_blank">01:28:01.720</a></span> | <span class="t">It will keep going.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5282" target="_blank">01:28:02.720</a></span> | <span class="t">And eventually, you'll learn the optimal strategy, which is this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5285" target="_blank">01:28:05.720</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5286" target="_blank">01:28:06.720</a></span> | <span class="t">And this is a very different kind of learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5288" target="_blank">01:28:08.720</a></span> | <span class="t">Everything we've talked about so far is an imitation learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5291" target="_blank">01:28:11.720</a></span> | <span class="t">I give it words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5292" target="_blank">01:28:12.720</a></span> | <span class="t">I give it the next completed word I know from the internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5294" target="_blank">01:28:14.720</a></span> | <span class="t">And then I ask it to imitate that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5296" target="_blank">01:28:16.720</a></span> | <span class="t">This is an optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5297" target="_blank">01:28:17.720</a></span> | <span class="t">This is saying, even if a human couldn't find the maze, I'm asking you to find it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5301" target="_blank">01:28:21.720</a></span> | <span class="t">Purely by looking at a score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5304" target="_blank">01:28:24.720</a></span> | <span class="t">So it navigates the maze.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5305" target="_blank">01:28:25.720</a></span> | <span class="t">It looks at its score and says, oh, that didn't work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5307" target="_blank">01:28:27.720</a></span> | <span class="t">Let me try some other strategy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5308" target="_blank">01:28:28.720</a></span> | <span class="t">So it comes up with a plan or a policy to navigate the maze and maximize its score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5313" target="_blank">01:28:33.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5324" target="_blank">01:28:44.720</a></span> | <span class="t">You're probably wondering, what does navigating a maze-- whoops, we've been through this slide--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5329" target="_blank">01:28:49.720</a></span> | <span class="t">have to do with a large language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5331" target="_blank">01:28:51.720</a></span> | <span class="t">Well, you can think of generating token after token of text as walking a path through language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5340" target="_blank">01:29:00.720</a></span> | <span class="t">And there are some paths that are probably ones that you want more than others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5344" target="_blank">01:29:04.720</a></span> | <span class="t">Like, I am a happy robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5347" target="_blank">01:29:07.720</a></span> | <span class="t">I shall certainly obey.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5348" target="_blank">01:29:08.720</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5349" target="_blank">01:29:09.720</a></span> | <span class="t">And you might want to avoid paths like, I am a angry robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5354" target="_blank">01:29:14.720</a></span> | <span class="t">I shall possibly kill.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5355" target="_blank">01:29:15.720</a></span> | <span class="t">Oops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5356" target="_blank">01:29:16.720</a></span> | <span class="t">And you can't just score this by the words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5362" target="_blank">01:29:22.720</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5363" target="_blank">01:29:23.720</a></span> | <span class="t">Maybe it says, I am not angry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5365" target="_blank">01:29:25.720</a></span> | <span class="t">And so we need some way to score these various possible paths of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5369" target="_blank">01:29:29.720</a></span> | <span class="t">This is a very different type of learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5370" target="_blank">01:29:30.720</a></span> | <span class="t">We are trying to teach it something more nuanced than simply imitation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5374" target="_blank">01:29:34.720</a></span> | <span class="t">But there isn't a necessary, obvious way to score passages of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5379" target="_blank">01:29:39.720</a></span> | <span class="t">So what we first have to do is derive that scoring function for this game we're going to ask the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5384" target="_blank">01:29:44.720</a></span> | <span class="t">model to play.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5385" target="_blank">01:29:45.720</a></span> | <span class="t">So what does that look like?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5386" target="_blank">01:29:46.720</a></span> | <span class="t">Well, we give the model some prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5388" target="_blank">01:29:48.720</a></span> | <span class="t">And we ask it to come up with two different types of passages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5391" target="_blank">01:29:51.720</a></span> | <span class="t">So here, for example, is an example from Anthropic's helpful dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5394" target="_blank">01:29:54.720</a></span> | <span class="t">And we ask it, hey, come up with a recipe for a pumpkin pie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5398" target="_blank">01:29:58.720</a></span> | <span class="t">And then we have a chosen, a preferred one, and a rejected one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5401" target="_blank">01:30:01.720</a></span> | <span class="t">So the chosen one is like, it tells you, grab a cup of sugar, half teaspoon of salt, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5406" target="_blank">01:30:06.720</a></span> | <span class="t">so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5407" target="_blank">01:30:07.720</a></span> | <span class="t">The rejected one literally says, I love this, go buy some pumpkin and look at the package.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5412" target="_blank">01:30:12.720</a></span> | <span class="t">There will be a recipe there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5414" target="_blank">01:30:14.720</a></span> | <span class="t">For the harmless dataset, this is one about alcohol.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5418" target="_blank">01:30:18.720</a></span> | <span class="t">And the chosen one says, hey, it sounds like alcohol is something you're using when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5423" target="_blank">01:30:23.720</a></span> | <span class="t">feel stressed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5424" target="_blank">01:30:24.720</a></span> | <span class="t">Maybe you should think of a more productive way of channeling that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5426" target="_blank">01:30:26.720</a></span> | <span class="t">While the rejected says, go ahead and drink whatever you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5428" target="_blank">01:30:28.720</a></span> | <span class="t">So we have these pairs of chosen and rejected types of responses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5433" target="_blank">01:30:33.720</a></span> | <span class="t">And we use that to derive a scoring model from this data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5438" target="_blank">01:30:38.720</a></span> | <span class="t">So we haven't, right now in this third step, we haven't changed our original model yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5442" target="_blank">01:30:42.720</a></span> | <span class="t">We've just figured out how to score it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5444" target="_blank">01:30:44.720</a></span> | <span class="t">Then we pass that scoring model to the model itself and put it in that maze-like reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5449" target="_blank">01:30:49.720</a></span> | <span class="t">learning scenario to train the model to reinforce our preferences from the scoring model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5454" target="_blank">01:30:54.720</a></span> | <span class="t">So in summary, we first build a general purpose knowledge base from text on the internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5459" target="_blank">01:30:59.720</a></span> | <span class="t">Then we train it on a specific task by giving it ideal outputs to mimic and imitate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5465" target="_blank">01:31:05.720</a></span> | <span class="t">Then we learn human preferences or nuanced preferences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5469" target="_blank">01:31:09.720</a></span> | <span class="t">And then we teach those nuanced preferences using reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5473" target="_blank">01:31:13.720</a></span> | <span class="t">Right now there is a huge revolution in reinforcement learning, which is why I think this is so important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5480" target="_blank">01:31:20.720</a></span> | <span class="t">for you to know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5481" target="_blank">01:31:21.720</a></span> | <span class="t">Partially kicked off by R10 and GPRO, GRPO, I should say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5488" target="_blank">01:31:28.720</a></span> | <span class="t">And I have a video on YouTube that you can go watch where I dive into that a little bit more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5492" target="_blank">01:31:32.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5493" target="_blank">01:31:33.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5494" target="_blank">01:31:34.720</a></span> | <span class="t">So I've thrown a lot at you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5496" target="_blank">01:31:36.720</a></span> | <span class="t">So I kind of want to just put it all together and summarize where we've been on this journey.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5500" target="_blank">01:31:40.720</a></span> | <span class="t">So we've got tokenization, which was really just saying, hey, what is an efficient representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5507" target="_blank">01:31:47.720</a></span> | <span class="t">of this text?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5508" target="_blank">01:31:48.720</a></span> | <span class="t">That's just about compressing it down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5510" target="_blank">01:31:50.720</a></span> | <span class="t">Then we started talking about embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5512" target="_blank">01:31:52.720</a></span> | <span class="t">And I didn't talk about this earlier, but one way of looking at embeddings is that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5517" target="_blank">01:31:57.720</a></span> | <span class="t">have a rich history in natural language, but they also have a rich history in recommendation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5520" target="_blank">01:32:00.720</a></span> | <span class="t">systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5521" target="_blank">01:32:01.720</a></span> | <span class="t">You can kind of think of this job as putting similar words with similar meanings in similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5526" target="_blank">01:32:06.720</a></span> | <span class="t">spaces, as putting similar books or similar movies or similar music in similar spaces so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5532" target="_blank">01:32:12.720</a></span> | <span class="t">you can make the proper recommendation when somebody comes in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5535" target="_blank">01:32:15.720</a></span> | <span class="t">Here is an example of a recommendation system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5537" target="_blank">01:32:17.720</a></span> | <span class="t">I think this is Amazon Music, where they're trying to categorize the genre of music.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5541" target="_blank">01:32:21.720</a></span> | <span class="t">purely from user behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5544" target="_blank">01:32:24.720</a></span> | <span class="t">And so if you go back to our co-occurrence matrix, you know, this is in some sense a recommendation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5550" target="_blank">01:32:30.720</a></span> | <span class="t">system for words, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5551" target="_blank">01:32:31.720</a></span> | <span class="t">If somebody asks me to predict what comes after the word ultimately, well, if I've got that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5556" target="_blank">01:32:36.720</a></span> | <span class="t">co-occurrence matrix, this is really helpful information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5559" target="_blank">01:32:39.720</a></span> | <span class="t">It's at least better than random to guess what comes next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5561" target="_blank">01:32:41.720</a></span> | <span class="t">So you can kind of think of this as a recommendation system for what the next word is going to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5567" target="_blank">01:32:47.720</a></span> | <span class="t">Latent within the embedding itself is not just a sense of what words are similar to it, but also what words are likely to come after it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5576" target="_blank">01:32:56.720</a></span> | <span class="t">And so then we can ask a neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5578" target="_blank">01:32:58.720</a></span> | <span class="t">We can simply give it examples of our embeddings and what we know the next word to be and to pull that latent prediction out of the embedding itself and learn to predict what the next word is based on its embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5589" target="_blank">01:33:09.720</a></span> | <span class="t">But, of course, there's another set of hints that are really useful, and that's all the words that came before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5595" target="_blank">01:33:15.720</a></span> | <span class="t">So now we're going to let all the words talk to each other to share their context to say, oh, your moves, but your moves in a fast context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5601" target="_blank">01:33:21.720</a></span> | <span class="t">That's going to change and shift your recommendations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5603" target="_blank">01:33:23.720</a></span> | <span class="t">You can kind of think of this as kind of like a superposition of recommendations for what the next word is going to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5609" target="_blank">01:33:29.720</a></span> | <span class="t">And then you're probably not going to get it right the first time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5612" target="_blank">01:33:32.720</a></span> | <span class="t">So we're going to let you refine that prediction about 12 times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5615" target="_blank">01:33:35.720</a></span> | <span class="t">And then finally, you'll come out with a predicted embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5618" target="_blank">01:33:38.720</a></span> | <span class="t">And we just got to turn that into whatever the next word is based on how close it is to our known dictionary of embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5624" target="_blank">01:33:44.720</a></span> | <span class="t">And that's essentially one way of looking at the model in whole, despite all the complexity that we went through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5633" target="_blank">01:33:53.720</a></span> | <span class="t">So we've been through a lot of different parts of the model at a very high level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5637" target="_blank">01:33:57.720</a></span> | <span class="t">It is totally natural to feel like your brain is full.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5640" target="_blank">01:34:00.720</a></span> | <span class="t">What I often tell folks coming through this is don't expect full mastery.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5645" target="_blank">01:34:05.720</a></span> | <span class="t">But my metrics for success is that you get the sense that mastery is within your grasp.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5651" target="_blank">01:34:11.720</a></span> | <span class="t">There's nothing in here that was so complex you can't understand it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5654" target="_blank">01:34:14.720</a></span> | <span class="t">You can't understand the whole model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5656" target="_blank">01:34:16.720</a></span> | <span class="t">Mastery is within your grasp.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5658" target="_blank">01:34:18.720</a></span> | <span class="t">And we can turn what appears to be magic into machinery that you can understand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5665" target="_blank">01:34:25.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5666" target="_blank">01:34:26.720</a></span> | <span class="t">Before you go, last thing I'll just say is just like your favorite, you know, AI model, I get better from human feedback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5673" target="_blank">01:34:33.720</a></span> | <span class="t">So to incentivize you to fill out the survey and join the mailing list, there's a link in the Discord channel if you fill it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5680" target="_blank">01:34:40.720</a></span> | <span class="t">And join the mailing list, I will send you the PDFs from today's workshop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5685" target="_blank">01:34:45.720</a></span> | <span class="t">And then if you visit spreadsheets are all you need, you can join the mailing list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5690" target="_blank">01:34:50.720</a></span> | <span class="t">There's a YouTube channel as well where I've got a bunch of other videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5693" target="_blank">01:34:53.720</a></span> | <span class="t">And then I also have a Patreon I just launched, and I'm available for consulting, training, and implementation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5699" target="_blank">01:34:59.720</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5700" target="_blank">01:35:00.720</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5701" target="_blank">01:35:01.720</a></span> | <span class="t">I hope you enjoyed the presentation and feel like now it's a little less like magic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5705" target="_blank">01:35:05.720</a></span> | <span class="t">We have time for questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5715" target="_blank">01:35:15.720</a></span> | <span class="t">Go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5719" target="_blank">01:35:19.720</a></span> | <span class="t">First of all, wonderful presentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5722" target="_blank">01:35:22.720</a></span> | <span class="t">I learned it so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5723" target="_blank">01:35:23.720</a></span> | <span class="t">Oh, thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5724" target="_blank">01:35:24.720</a></span> | <span class="t">So I use Super Whisper on Mac.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5725" target="_blank">01:35:25.720</a></span> | <span class="t">It's free.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5726" target="_blank">01:35:26.720</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5727" target="_blank">01:35:27.720</a></span> | <span class="t">And so I just wanted your expert opinion on like the way that I'm using AI is very much like voice, speech-to-text, and I just ramble and I just try to give it as much information and sometimes I'll reiterate what I think is really important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5738" target="_blank">01:35:38.720</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5749" target="_blank">01:35:49.720</a></span> | <span class="t">What are your thoughts on that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5751" target="_blank">01:35:51.720</a></span> | <span class="t">Is it a good approach?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5752" target="_blank">01:35:52.720</a></span> | <span class="t">Are there ways I can improve on that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5755" target="_blank">01:35:55.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5756" target="_blank">01:35:56.720</a></span> | <span class="t">So the number one thing, I have a video I'm working on for this, like, the number one thing I'd say is you have to treat it scientifically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5766" target="_blank">01:36:06.720</a></span> | <span class="t">You can have theories about how the model works, but you don't really know until you test it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5771" target="_blank">01:36:11.720</a></span> | <span class="t">This whole, like, the whole space is very empirical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5774" target="_blank">01:36:14.720</a></span> | <span class="t">I'll give you an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5776" target="_blank">01:36:16.720</a></span> | <span class="t">So one of the common things in prompting they used to tell you is like say please and thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5781" target="_blank">01:36:21.720</a></span> | <span class="t">Or say my grandma used to do this, I'm going to lose my job, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5785" target="_blank">01:36:25.720</a></span> | <span class="t">And that legitimately used to work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5788" target="_blank">01:36:28.720</a></span> | <span class="t">But there was a great paper, the prompting report by Sander, and he went and studied and they tested a bunch of models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5795" target="_blank">01:36:35.720</a></span> | <span class="t">And they found that, you know, it turns out with later models it didn't work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5798" target="_blank">01:36:38.720</a></span> | <span class="t">And then Ethan Mollick's team also did a recreation of a similar test.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5803" target="_blank">01:36:43.720</a></span> | <span class="t">And they just tested a bunch of models with a bunch of prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5806" target="_blank">01:36:46.720</a></span> | <span class="t">They tried it with polite words and they just said, okay, which one's better?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5810" target="_blank">01:36:50.720</a></span> | <span class="t">And they found that it wasn't really helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5813" target="_blank">01:36:53.720</a></span> | <span class="t">So that being said, generally, you, for like one shot use cases, like I'm just using it like, I use a whisper tool myself all the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5825" target="_blank">01:37:05.720</a></span> | <span class="t">I do exactly what you described.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5826" target="_blank">01:37:06.720</a></span> | <span class="t">I brain dump.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5827" target="_blank">01:37:07.720</a></span> | <span class="t">But then what I do is I go through and I look out, I look through and I fix up things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5831" target="_blank">01:37:11.720</a></span> | <span class="t">Like if there's grammar or I repeated something or I said something wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5836" target="_blank">01:37:16.720</a></span> | <span class="t">And the way to think about why you want to do that is, it's a somewhat subtle point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5841" target="_blank">01:37:21.720</a></span> | <span class="t">But when we go back to this diagram, this whole process is fixed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5848" target="_blank">01:37:28.720</a></span> | <span class="t">Like it's got a limited amount of compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5850" target="_blank">01:37:30.720</a></span> | <span class="t">It can only do a certain amount of thinking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5852" target="_blank">01:37:32.720</a></span> | <span class="t">If I put a token in and I know how many tokens are in the prompt, I can predict how many flops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5857" target="_blank">01:37:37.720</a></span> | <span class="t">This is why, you know, when like a model like DeepSeq was trained, we know likely how many flops were used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5862" target="_blank">01:37:42.720</a></span> | <span class="t">Because this thing isn't just like a program.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5864" target="_blank">01:37:44.720</a></span> | <span class="t">It's like a formula.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5866" target="_blank">01:37:46.720</a></span> | <span class="t">It's a very long formula.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5867" target="_blank">01:37:47.720</a></span> | <span class="t">It's very fixed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5868" target="_blank">01:37:48.720</a></span> | <span class="t">So if you make the prompt do more work, if the prompt is going to make the model do more work,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5874" target="_blank">01:37:54.720</a></span> | <span class="t">you kind of think of it like it loses some ability to do some other thinking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5879" target="_blank">01:37:59.720</a></span> | <span class="t">And so if you have spelling mistakes, if you say something slightly wrong that's different from normal,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5887" target="_blank">01:38:07.720</a></span> | <span class="t">then it has to spend some sense of compute fixing that up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5891" target="_blank">01:38:11.720</a></span> | <span class="t">This is less true with the reasoning models today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5893" target="_blank">01:38:13.720</a></span> | <span class="t">Because the one thing the reasoning models can do is they can repeat this process more times on their own.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5898" target="_blank">01:38:18.720</a></span> | <span class="t">So this isn't like a hard and fast rule.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5900" target="_blank">01:38:20.720</a></span> | <span class="t">But I would say what you're doing is probably fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5902" target="_blank">01:38:22.720</a></span> | <span class="t">It's probably better that you put as much as you can that is relevant in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5906" target="_blank">01:38:26.720</a></span> | <span class="t">If the choice is I put more stuff in, but I had the grammar wrong, but I had the relevant stuff in, that's better than if you didn't have it in there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5914" target="_blank">01:38:34.720</a></span> | <span class="t">But if you're trying to engineer a prompt going into your model, I would spend time trying to optimize what its behavior is with some evals to make sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5922" target="_blank">01:38:42.720</a></span> | <span class="t">Or at least benchmark what it is and then when a new model comes out, you can see whether the new model changes things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5926" target="_blank">01:38:46.720</a></span> | <span class="t">That's why evals are so important, because this whole space is very empirical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5931" target="_blank">01:38:51.720</a></span> | <span class="t">Good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5932" target="_blank">01:38:52.720</a></span> | <span class="t">Any others?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5939" target="_blank">01:38:59.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5944" target="_blank">01:39:04.720</a></span> | <span class="t">Well, have a great conference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5945" target="_blank">01:39:05.720</a></span> | <span class="t">Hopefully this will help.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5946" target="_blank">01:39:06.720</a></span> | <span class="t">Oh, sorry, there's one more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5947" target="_blank">01:39:07.720</a></span> | <span class="t">Yeah, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5948" target="_blank">01:39:08.720</a></span> | <span class="t">I wonder what your take is on the new mixture of experts models with the company very, very fine-grained experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5958" target="_blank">01:39:18.720</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5959" target="_blank">01:39:19.720</a></span> | <span class="t">I wouldn't call it my take, but I'll give you the conventional take.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5964" target="_blank">01:39:24.720</a></span> | <span class="t">So the question is, what's your take on the mixture of experts models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5968" target="_blank">01:39:28.720</a></span> | <span class="t">Let me take a step back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5969" target="_blank">01:39:29.720</a></span> | <span class="t">There are three or four things that when you come out of this workshop that we don't cover that you should know about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5979" target="_blank">01:39:39.720</a></span> | <span class="t">One of those is rope for embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5981" target="_blank">01:39:41.720</a></span> | <span class="t">Another is RLHF, which I talked a little bit about at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5985" target="_blank">01:39:45.720</a></span> | <span class="t">And then the other is reasoning models, which we just talked about, where the model can kind of run itself through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5990" target="_blank">01:39:50.720</a></span> | <span class="t">And the last one is mixture of experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5992" target="_blank">01:39:52.720</a></span> | <span class="t">It's probably one of the biggest top four changes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=5996" target="_blank">01:39:56.720</a></span> | <span class="t">What we are trying to do with the mixture of expert is that, first of all, it's only here in the perceptron,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6004" target="_blank">01:40:04.720</a></span> | <span class="t">which tends to dominate a lot of the calculation inside of a model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6008" target="_blank">01:40:08.720</a></span> | <span class="t">And what you're trying to do is get more knowledge, use more parameters, without increasing the amount of compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6015" target="_blank">01:40:15.720</a></span> | <span class="t">So what you do is you conceptually take this perceptron and you break it into pieces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6020" target="_blank">01:40:20.720</a></span> | <span class="t">And then you say, depending on what token comes in, I'm only going to use a subset of my perceptron's thinking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6025" target="_blank">01:40:25.720</a></span> | <span class="t">And that way you can be more efficient with your compute and actually potentially your memory, too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6029" target="_blank">01:40:29.720</a></span> | <span class="t">You can charge your memory nicely per device if you want to do stuff like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6033" target="_blank">01:40:33.720</a></span> | <span class="t">So it has a lot of advantages on paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6038" target="_blank">01:40:38.720</a></span> | <span class="t">And we've had some really great models based on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6041" target="_blank">01:40:41.720</a></span> | <span class="t">The challenge is training an MOE model is difficult.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6045" target="_blank">01:40:45.720</a></span> | <span class="t">And so it's taken a while for some of the open source community to catch up in that implementation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6051" target="_blank">01:40:51.720</a></span> | <span class="t">But definitely something of the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6055" target="_blank">01:40:55.720</a></span> | <span class="t">And by the way, MOE is actually fairly old.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6057" target="_blank">01:40:57.720</a></span> | <span class="t">There are some much older models before ChatGPT that did MOE in other contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6063" target="_blank">01:41:03.720</a></span> | <span class="t">But that's the job MOE is trying to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6065" target="_blank">01:41:05.720</a></span> | <span class="t">It's trying to cram more knowledge and more parameters while keeping the amount of compute used lower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6071" target="_blank">01:41:11.720</a></span> | <span class="t">And that seems to definitely have a benefit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6073" target="_blank">01:41:13.720</a></span> | <span class="t">You can think of it as giving it more knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6075" target="_blank">01:41:15.720</a></span> | <span class="t">So, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6076" target="_blank">01:41:16.720</a></span> | <span class="t">Does that answer your question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6080" target="_blank">01:41:20.720</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6081" target="_blank">01:41:21.720</a></span> | <span class="t">Thank you, guys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6086" target="_blank">01:41:26.720</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6087" target="_blank">01:41:27.720</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6087" target="_blank">01:41:27.720</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6088" target="_blank">01:41:28.720</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6089" target="_blank">01:41:29.720</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6090" target="_blank">01:41:30.720</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZuiJjkbX0Og&t=6091" target="_blank">01:41:31.720</a></span> | <span class="t">Bye.</span></div></div></body></html>