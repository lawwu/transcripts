<html><head><title>Sentence Similarity With Transformers and PyTorch (Python)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Sentence Similarity With Transformers and PyTorch (Python)</h2><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg"><img src="https://i.ytimg.com/vi/jVPd7lEvjtg/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=16">0:16</a> BERT Base Network<br><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=71">1:11</a> Sentence Vectors and Similarity<br><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=107">1:47</a> The Data and Model<br><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=181">3:1</a> Two Approaches<br><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=196">3:16</a> Tokenizing Sentences<br><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=551">9:11</a> Creating last_hidden_state Tensor<br><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=668">11:8</a> Creating Sentence Vectors<br><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1073">17:53</a> Cosine Similarity<br><br><div style="text-align: left;"><a href="./jVPd7lEvjtg.html">Whisper Transcript</a> | <a href="./transcript_jVPd7lEvjtg.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Today we're going to have a look at how we can use transformers like BERT to create embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=6" target="_blank">00:00:06.640</a></span> | <span class="t">for sentences and how we can then take those sentence vectors and use them to calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=13" target="_blank">00:00:13.600</a></span> | <span class="t">semantic similarity between different sentences. So at a high level what you can see on the screen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=21" target="_blank">00:00:21.040</a></span> | <span class="t">right now is a BERT base model. Inside BERT base we have multiple encoders and at the bottom we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=29" target="_blank">00:00:29.040</a></span> | <span class="t">can see we have our tokenized text, we have 512 tokens here and they get passed into our first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=37" target="_blank">00:00:37.680</a></span> | <span class="t">encoder to create these hidden state vectors which are of the size 768 in BERT. Now these get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=52" target="_blank">00:00:52.800</a></span> | <span class="t">processed through multiple encoders and between every one of these encoders, there's 12 in total,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=59" target="_blank">00:00:59.200</a></span> | <span class="t">there are going to be a vector of size 768 for every single token that we have so 512 tokens in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=69" target="_blank">00:01:09.840</a></span> | <span class="t">this case. Now what we're going to do is take the final tensor out here so this last hidden state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=76" target="_blank">00:01:16.800</a></span> | <span class="t">tensor and we're going to use mean pooling to compress it into a 768 by 1 vector and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=90" target="_blank">00:01:30.800</a></span> | <span class="t">is our sentence vector. Then once we've built our sentence vector we're going to use cosine similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=99" target="_blank">00:01:39.680</a></span> | <span class="t">to compare different sentences and see if we can get something that works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=107" target="_blank">00:01:47.120</a></span> | <span class="t">So switching across to Python, these are the sentences we're going to be comparing and there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=115" target="_blank">00:01:55.120</a></span> | <span class="t">two, so there's this one here which is three years later the coffin was still full of jello</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=120" target="_blank">00:02:00.480</a></span> | <span class="t">and that has the same meaning as this here. I just rewrote it but with completely different words so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=129" target="_blank">00:02:09.280</a></span> | <span class="t">I don't think there's really any words here that match so instead of years we have dozens of months</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=135" target="_blank">00:02:15.680</a></span> | <span class="t">jelly jello coffin person box right no normal human would even say that second well no normal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=144" target="_blank">00:02:24.400</a></span> | <span class="t">human would probably say either of those but we definitely wouldn't use person box for coffin</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=150" target="_blank">00:02:30.240</a></span> | <span class="t">and many dozens of months for years. So it's reasonably complicated but we'll see that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=160" target="_blank">00:02:40.720</a></span> | <span class="t">should work for similarity so we'll find that these two share the highest similarity score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=165" target="_blank">00:02:45.840</a></span> | <span class="t">after we've encoded them with BERT and calculate our cosine similarity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=170" target="_blank">00:02:50.640</a></span> | <span class="t">And down here is the model we'll be using so we're going to be using sentence transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=178" target="_blank">00:02:58.240</a></span> | <span class="t">and then the BERT base NLI mean tokens model. Now there's two approaches that we can take here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=184" target="_blank">00:03:04.160</a></span> | <span class="t">the easy approach using something called sentence transformers. I'm going to be covering that in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=188" target="_blank">00:03:08.800</a></span> | <span class="t">another video and this approach which is a little more involved where we're going to be using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=194" target="_blank">00:03:14.320</a></span> | <span class="t">transformers and PyTorch. So the first thing we need to do is actually create our last hidden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=203" target="_blank">00:03:23.440</a></span> | <span class="t">state tensor. So of course we need to import the libraries that we're going to be using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=209" target="_blank">00:03:29.840</a></span> | <span class="t">so transformers we're going to be using the auto tokenizer and the auto model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=218" target="_blank">00:03:38.320</a></span> | <span class="t">and then we need to import torch as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=223" target="_blank">00:03:43.200</a></span> | <span class="t">And then after we've imported these we need to first initialize our tokenizer model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=232" target="_blank">00:03:52.560</a></span> | <span class="t">which we just do auto tokenizer and then for both these we're going to use from pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=240" target="_blank">00:04:00.000</a></span> | <span class="t">and we're going to use the model name that we've already defined. So these are coming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=246" target="_blank">00:04:06.080</a></span> | <span class="t">from face library obviously and we can see the model here so it's this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=254" target="_blank">00:04:14.560</a></span> | <span class="t">and then our model is auto model from pre-trained again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=260" target="_blank">00:04:20.720</a></span> | <span class="t">from those and now what we want to do is tokenize all of our sentences. Now to do this we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=272" target="_blank">00:04:32.080</a></span> | <span class="t">to use a tokens dictionary and in here we're going to have input IDs and this will contain a list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=281" target="_blank">00:04:41.360</a></span> | <span class="t">and you'll see why in a moment and attention mask which will also contain a list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=288" target="_blank">00:04:48.320</a></span> | <span class="t">Now when we're going through each sentence we have to do this one by one for sentence in sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=301" target="_blank">00:05:01.280</a></span> | <span class="t">we are going to be using the tokenizers encode plus method. So tokenizer encode plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=310" target="_blank">00:05:10.960</a></span> | <span class="t">and then in here we need to pass our sentence we need to pass the maximum length of our sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=318" target="_blank">00:05:18.800</a></span> | <span class="t">so with BERT usually we would set this to 512 but because we're using this BERT based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=324" target="_blank">00:05:24.480</a></span> | <span class="t">NLI mean tokens model this should actually be set to 128. So we set max length to 128</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=334" target="_blank">00:05:34.240</a></span> | <span class="t">and anything longer than this we want to truncate so we set truncation equal to true and anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=342" target="_blank">00:05:42.720</a></span> | <span class="t">shorter than this which they all will be in our case we set padding equal to the max length to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=349" target="_blank">00:05:49.200</a></span> | <span class="t">pad it up to that max length and then here we want to say return tensors and we set this equal to pt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=357" target="_blank">00:05:57.840</a></span> | <span class="t">because we're using PyTorch. Now this will return a dictionary containing input IDs and attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=366" target="_blank">00:06:06.560</a></span> | <span class="t">mask for a single sentence so we'll take the new tokens assign it to that variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=377" target="_blank">00:06:17.200</a></span> | <span class="t">and then what we're going to do is access our tokens dictionary which inputs IDs first and append</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=385" target="_blank">00:06:25.520</a></span> | <span class="t">the input IDs for the single sentence from the new tokens variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=391" target="_blank">00:06:31.760</a></span> | <span class="t">so input IDs and then we do the same for our attention mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=404" target="_blank">00:06:44.320</a></span> | <span class="t">okay so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=406" target="_blank">00:06:46.400</a></span> | <span class="t">that gives us those there's another thing as well we these are wrapped as vectors so we also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=417" target="_blank">00:06:57.040</a></span> | <span class="t">want to just extract the first element there because it's they're like almost like lists</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=424" target="_blank">00:07:04.320</a></span> | <span class="t">within a list but in tensor format and we want to extract the list. Now that's good but obviously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=433" target="_blank">00:07:13.520</a></span> | <span class="t">we're using PyTorch here we want PyTorch tensors not lists so within these lists we do have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=440" target="_blank">00:07:20.640</a></span> | <span class="t">PyTorch tensors so in fact let me just show you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=444" target="_blank">00:07:24.400</a></span> | <span class="t">so if we have a look in here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=449" target="_blank">00:07:29.760</a></span> | <span class="t">we'll see that we have our PyTorch tensors but they're contained within a normal Python list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=460" target="_blank">00:07:40.800</a></span> | <span class="t">so we can even check that if we do type we see we get lists and inside there we have the torch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=468" target="_blank">00:07:48.560</a></span> | <span class="t">tensor which is what we want for all of them so to convert this list of PyTorch tensors into a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=476" target="_blank">00:07:56.000</a></span> | <span class="t">single PyTorch tensor what we do is we take this torch and we use the stack method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=488" target="_blank">00:08:08.800</a></span> | <span class="t">and what the stack method does is takes a list and within that list we expect PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=496" target="_blank">00:08:16.720</a></span> | <span class="t">tensors and it will stack all of those on top of each other essentially adding another dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=501" target="_blank">00:08:21.440</a></span> | <span class="t">and stacking them all on top of each other hence the hence the name</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=506" target="_blank">00:08:26.240</a></span> | <span class="t">so take that and we want to do for both input ids and attention mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=515" target="_blank">00:08:35.440</a></span> | <span class="t">and then let's have a look what we have so let's go attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=518" target="_blank">00:08:38.560</a></span> | <span class="t">or input ids and now we just have a single tensor okay so you type</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=526" target="_blank">00:08:46.320</a></span> | <span class="t">and now we just have a tensor now that's great</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=540" target="_blank">00:09:00.560</a></span> | <span class="t">check its size so we have six sentences that have all been encoded into the 128 tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=549" target="_blank">00:09:09.760</a></span> | <span class="t">ready to go into our model so to process these through our model we'll output the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=558" target="_blank">00:09:18.640</a></span> | <span class="t">outputs to this outputs variable and we take our model and we pass our tokens as keyword arguments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=569" target="_blank">00:09:29.360</a></span> | <span class="t">into the model input there so we process that and that will give us this output object and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=582" target="_blank">00:09:42.720</a></span> | <span class="t">inside this ipod object we have the last hidden state tensor here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=587" target="_blank">00:09:47.680</a></span> | <span class="t">and we can also see that if we print out keys you see that we have the last hidden state and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=594" target="_blank">00:09:54.640</a></span> | <span class="t">we also have this pooler output now we want to take our last hidden state tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=602" target="_blank">00:10:02.480</a></span> | <span class="t">and then perform the mean pooling operation to convert it into a sentence vector so to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=613" target="_blank">00:10:13.440</a></span> | <span class="t">get that last insight we will assign it to this embeddings variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=621" target="_blank">00:10:21.120</a></span> | <span class="t">and we extract it using hidden or last hidden state like that and let's just check what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=633" target="_blank">00:10:33.200</a></span> | <span class="t">have here so we'll just have a look at shape you see now we have the six sentences we have the 128</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=640" target="_blank">00:10:40.880</a></span> | <span class="t">tokens and then we have the 768 dimension size which is just the hidden state dimensions within</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=650" target="_blank">00:10:50.640</a></span> | <span class="t">bert so what we have at the moment is this last hidden state tensor and what we're going to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=659" target="_blank">00:10:59.920</a></span> | <span class="t">is now convert it into this using a mean pooling operation so the the first thing we need to do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=671" target="_blank">00:11:11.520</a></span> | <span class="t">multiply every value within this last hidden state tensor by zero where we shouldn't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=681" target="_blank">00:11:21.520</a></span> | <span class="t">a real token so if we look up here we padded all of these and obviously there's more padding tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=689" target="_blank">00:11:29.280</a></span> | <span class="t">in this sentence than there are in this sentence so we need to take each of those attention mass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=697" target="_blank">00:11:37.120</a></span> | <span class="t">tensors that we took here which just contain ones and zeros ones where there's real tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=702" target="_blank">00:11:42.160</a></span> | <span class="t">zeros where there are padding tokens and multiply that out to remove any activations where there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=709" target="_blank">00:11:49.360</a></span> | <span class="t">should just be padding tokens eg zeros now the only problem is that if we have a look at our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=717" target="_blank">00:11:57.520</a></span> | <span class="t">attention mask so tokens attention mass if we have a look at the size we get a six by 128</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=731" target="_blank">00:12:11.520</a></span> | <span class="t">so what we need to do is add this other dimension which is the 768 and then we can just multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=739" target="_blank">00:12:19.680</a></span> | <span class="t">those two tensors together and this will remove the embedding values where there shouldn't be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=745" target="_blank">00:12:25.840</a></span> | <span class="t">embedding values and to do that we'll we'll assign it to mass but we'll do it later actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=753" target="_blank">00:12:33.600</a></span> | <span class="t">so tension and what we want to do is use the unsqueeze method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=759" target="_blank">00:12:39.600</a></span> | <span class="t">and if we look at the shape so we can see what is actually happening here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=766" target="_blank">00:12:46.640</a></span> | <span class="t">see that we've added this other dimension and then what that allows us to do is expand that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=773" target="_blank">00:12:53.600</a></span> | <span class="t">dimension out to 768 which will then match to the correct shape that we need to multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=781" target="_blank">00:13:01.760</a></span> | <span class="t">those two together so we do expand and here what we want is we'll take embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=790" target="_blank">00:13:10.240</a></span> | <span class="t">and we want to expand it out to the embeddings shape that we have already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=799" target="_blank">00:13:19.120</a></span> | <span class="t">used up here so that will compare these two and see that we need to expand this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=806" target="_blank">00:13:26.720</a></span> | <span class="t">one dimension out to 768 and if we execute that we can see that it has worked so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=815" target="_blank">00:13:35.840</a></span> | <span class="t">the final thing that we need to do there is convert that into a float tensor then we assign</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=824" target="_blank">00:13:44.640</a></span> | <span class="t">that to the mask here so this uh float at the end that's just converting it from integer to float</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=831" target="_blank">00:13:51.040</a></span> | <span class="t">so now what we can do is apply this mask to our embeddings so we'll call this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=839" target="_blank">00:13:59.120</a></span> | <span class="t">mask embeddings and it is very simple we just do embeddings multiplied by mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=849" target="_blank">00:14:09.760</a></span> | <span class="t">and now if we just compare embeddings have a look what we have here so it's quite a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=856" target="_blank">00:14:16.080</a></span> | <span class="t">and now we have a look at mask embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=860" target="_blank">00:14:20.160</a></span> | <span class="t">and you see here that we have the same values here so looking at the top these are the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=871" target="_blank">00:14:31.600</a></span> | <span class="t">but then these values here have been mapped to zero because they are just padding tokens we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=880" target="_blank">00:14:40.160</a></span> | <span class="t">don't want to pay attention to those so that's the point of the masking operation there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=890" target="_blank">00:14:50.720</a></span> | <span class="t">so remove those and now what we want to do is take all of those embeddings because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=901" target="_blank">00:15:01.120</a></span> | <span class="t">if we have a look at the shape that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=902" target="_blank">00:15:02.960</a></span> | <span class="t">we still have this 128 tokens we want to convert this into one token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=912" target="_blank">00:15:12.000</a></span> | <span class="t">and there's two operations that we need to do here so we're doing a mean pooling operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=919" target="_blank">00:15:19.280</a></span> | <span class="t">so we need to calculate the sum within each of these so if we summed all these up together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=926" target="_blank">00:15:26.720</a></span> | <span class="t">that's what we are going to be doing and pushing them into a single value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=931" target="_blank">00:15:31.040</a></span> | <span class="t">and then we also need to count all of those values but only where we were supposed to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=939" target="_blank">00:15:39.760</a></span> | <span class="t">paying attention so where we converted them into zeros we don't want to count those values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=944" target="_blank">00:15:44.160</a></span> | <span class="t">and then we divide that sum by the count to get our mean so to get the summed we do torch dot sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=953" target="_blank">00:15:53.600</a></span> | <span class="t">and then it's just masked embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=955" target="_blank">00:15:55.280</a></span> | <span class="t">and this is in the dimension one which is this dimension here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=970" target="_blank">00:16:10.000</a></span> | <span class="t">let's have a look at the shape that we have here okay so now we can see that we've removed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=974" target="_blank">00:16:14.960</a></span> | <span class="t">this dimension and now what we want to do is create our counts and to do this we use a slightly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=982" target="_blank">00:16:22.320</a></span> | <span class="t">different approach we just do torch clamp and then inside here we do mask dot sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=993" target="_blank">00:16:33.760</a></span> | <span class="t">again in the dimension one and then we also have we also add a min argument here which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1004" target="_blank">00:16:44.080</a></span> | <span class="t">just stops us from creating any divide by zero error so we do one e and all this needs to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1015" target="_blank">00:16:55.600</a></span> | <span class="t">is a very small number i think by default it's one e to the minus eight but i usually just use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1021" target="_blank">00:17:01.760</a></span> | <span class="t">one e to the minus nine although in reality it shouldn't really make a difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1027" target="_blank">00:17:07.200</a></span> | <span class="t">and sorry just put counts there okay so that's our sum and our counts and now we get the mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1041" target="_blank">00:17:21.280</a></span> | <span class="t">pooled so we do mean pooled equals summed divided by the counts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1049" target="_blank">00:17:29.760</a></span> | <span class="t">and we'll just check the size of that again okay so that is our sentence vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1062" target="_blank">00:17:42.080</a></span> | <span class="t">so we have six of them here each one contains just 768 values and let's have a look at what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1071" target="_blank">00:17:51.120</a></span> | <span class="t">they look like we just get these values here now what we can do is compare each of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1078" target="_blank">00:17:58.640</a></span> | <span class="t">and see which ones get the highest cosine similarity value now we're going to be using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1088" target="_blank">00:18:08.560</a></span> | <span class="t">sklearn implementation which is metrics dot pairwise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1094" target="_blank">00:18:14.160</a></span> | <span class="t">we import cosine similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1099" target="_blank">00:18:19.280</a></span> | <span class="t">and then this would expect numpy arrays obviously we have pytorch tensors so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1108" target="_blank">00:18:28.160</a></span> | <span class="t">are going to get an error i'm going to i'm going to show you so you at least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1111" target="_blank">00:18:31.680</a></span> | <span class="t">see it you know how to fix it so we cosine similarity and in here we want to pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1122" target="_blank">00:18:42.800</a></span> | <span class="t">a single vector that we are going to be comparing so i'm going to compare the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1128" target="_blank">00:18:48.480</a></span> | <span class="t">first text sentence so if we just take these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1137" target="_blank">00:18:57.680</a></span> | <span class="t">put them down here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1138" target="_blank">00:18:58.640</a></span> | <span class="t">so i'm going to take the very first one of those which is mean pooled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1146" target="_blank">00:19:06.560</a></span> | <span class="t">zero and because we are extracting this out directly that means we get a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1155" target="_blank">00:19:15.120</a></span> | <span class="t">it's like a list format we want it to be in a vector format so it's a list within the list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1163" target="_blank">00:19:23.040</a></span> | <span class="t">and then we want to extract the remaining so five yeah five sentences so go one all the way to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1173" target="_blank">00:19:33.200</a></span> | <span class="t">end so that's those last five there now if we run this we're going to get this runtime error we go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1179" target="_blank">00:19:39.600</a></span> | <span class="t">down and we see common quantum pi on tensor that requires grad so this is just with pytorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1189" target="_blank">00:19:49.360</a></span> | <span class="t">we this tensor is currently within our pytorch model and we need to detach it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1196" target="_blank">00:19:56.480</a></span> | <span class="t">from pytorch in order to convert it into something that pytorch cannot read anymore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1201" target="_blank">00:20:01.840</a></span> | <span class="t">and it actually tells us exactly what we need to do so use tensor detach numpy instead so we take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1208" target="_blank">00:20:08.880</a></span> | <span class="t">detach and numpy and all we need to do is write mean pooled equals that rerun it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1223" target="_blank">00:20:23.760</a></span> | <span class="t">and we get our similarity scores so straight away we got 0.33 17 4455 this one is the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1237" target="_blank">00:20:37.280</a></span> | <span class="t">the highest similarity 0.72 by a fair bit as well so that is comparing this sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1246" target="_blank">00:20:46.640</a></span> | <span class="t">and sentence at index one of our last five which is this one so there we've calculated similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1258" target="_blank">00:20:58.800</a></span> | <span class="t">and it is clearly working so that's it for this video i hope it's been useful i think this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jVPd7lEvjtg&t=1265" target="_blank">00:21:05.760</a></span> | <span class="t">really cool. And I'll see you in the next one.</span></div></div></body></html>