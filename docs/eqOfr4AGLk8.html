<html><head><title>LangChain Data Loaders, Tokenizers, Chunking, and Datasets - Data Prep 101</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>LangChain Data Loaders, Tokenizers, Chunking, and Datasets - Data Prep 101</h2><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8"><img src="https://i.ytimg.com/vi/eqOfr4AGLk8/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=0">0:0</a> Data preparation for LLMs<br><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=45">0:45</a> Downloading the LangChain docs<br><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=209">3:29</a> Using LangChain document loaders<br><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=354">5:54</a> How much text can we fit in LLMs?<br><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=717">11:57</a> Using tiktoken tokenizer to find length of text<br><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=962">16:2</a> Initializing the recursive text splitter in Langchain<br><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1045">17:25</a> Why we use chunk overlap<br><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1223">20:23</a> Chunking with RecursiveCharacterTextSplitter<br><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1297">21:37</a> Creating the dataset<br><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1490">24:50</a> Saving and loading with JSONL file<br><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1720">28:40</a> Data prep is important<br><br><div style="text-align: left;"><a href="./eqOfr4AGLk8.html">Whisper Transcript</a> | <a href="./transcript_eqOfr4AGLk8.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">In this video we are going to take a look at what we need to do and what we need to consider</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=5" target="_blank">00:00:05.920</a></span> | <span class="t">when we are chunking text for large language models. The best way I can think of of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=14" target="_blank">00:00:14.000</a></span> | <span class="t">demonstrating this is to walk through an example. Now we're going to really go with the what I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=19" target="_blank">00:00:19.280</a></span> | <span class="t">believe is kind of like a rule of thumb that I tend to use when I'm chunking text in order to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=24" target="_blank">00:00:24.400</a></span> | <span class="t">put into a large language model and it doesn't necessarily apply to every use case. You know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=29" target="_blank">00:00:29.200</a></span> | <span class="t">every use case is slightly different but I think this is a pretty good approach at least when we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=34" target="_blank">00:00:34.400</a></span> | <span class="t">using retrieval augmentation and large language models which I think is where the chunking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=40" target="_blank">00:00:40.640</a></span> | <span class="t">question kind of comes up most often. So let's jump straight into it. In this example what we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=46" target="_blank">00:00:46.560</a></span> | <span class="t">going to be doing is taking the langchain docs here, literally every page on this website,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=53" target="_blank">00:00:53.760</a></span> | <span class="t">and we're going to be downloading those, taking each one of these pages and then we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=59" target="_blank">00:00:59.840</a></span> | <span class="t">be splitting them into more reasonably sized chunks. Now how are we going to do this? We're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=66" target="_blank">00:01:06.720</a></span> | <span class="t">going to take a look at this notebook here. Now if you'd like to follow along with the code you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=72" target="_blank">00:01:12.480</a></span> | <span class="t">also run this notebook. I will leave a link to it which will appear somewhere near the top of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=77" target="_blank">00:01:17.760</a></span> | <span class="t">video right now. Now to get started we're going to be using a few Python libraries. Langchain is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=83" target="_blank">00:01:23.760</a></span> | <span class="t">a pretty big one here so not only is it the documentation that we're downloading but it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=89" target="_blank">00:01:29.440</a></span> | <span class="t">also going to be how we download that documentation and it's also going to be how we split that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=95" target="_blank">00:01:35.600</a></span> | <span class="t">documentation into chunks. Another dependency here is the ticktoken tokenizer. We'll talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=102" target="_blank">00:01:42.160</a></span> | <span class="t">that later and we're just going to visualize and make things a little bit easier to follow with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=105" target="_blank">00:01:45.840</a></span> | <span class="t">these libraries here. In this example first thing we're going to do is download all of the docs from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=114" target="_blank">00:01:54.080</a></span> | <span class="t">langchain. Everything is contained within this is the top level page of the langchain docs. We're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=121" target="_blank">00:02:01.360</a></span> | <span class="t">going to save everything into this directory here and we are going to say we want to get all of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=130" target="_blank">00:02:10.080</a></span> | <span class="t">.html files. We run that and that will take a moment just to download everything. There's a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=138" target="_blank">00:02:18.480</a></span> | <span class="t">in there. My internet connection is also pretty slow so it will probably take me a moment but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=145" target="_blank">00:02:25.040</a></span> | <span class="t">let's go ahead and just have a look at where these are being downloaded. If we come over to the left</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=151" target="_blank">00:02:31.120</a></span> | <span class="t">here we can see there is the RT docs repository there and inside the RT docs we have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=159" target="_blank">00:02:39.040</a></span> | <span class="t">langchain-redux-en-latest which is just kind of like the path of our docs. In there you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=168" target="_blank">00:02:48.080</a></span> | <span class="t">everything's been downloaded. We have the index page which I think is the top level page. You can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=174" target="_blank">00:02:54.480</a></span> | <span class="t">see it's just HTML. We're not going to process this we're going to use langchain to clean this up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=182" target="_blank">00:03:02.080</a></span> | <span class="t">but if we come down a little bit I think maybe we can see something. This is the first page,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=191" target="_blank">00:03:11.120</a></span> | <span class="t">welcome to langchain, LLMs are emerging as a transformative technology, so on and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=197" target="_blank">00:03:17.440</a></span> | <span class="t">We have some other things, other pages. We're just going to process all of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=202" target="_blank">00:03:22.960</a></span> | <span class="t">Back to our code. It's done downloading now. We can come down to here and what we're going to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=210" target="_blank">00:03:30.560</a></span> | <span class="t">is use the langchain document loaders and we're going to use a Redux loader. Redux is a specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=217" target="_blank">00:03:37.440</a></span> | <span class="t">template that is used quite often for documentation for code libraries. Langchain includes a document</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=226" target="_blank">00:03:46.960</a></span> | <span class="t">loader that is specifically built for reading that type of documentation or those HTML pages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=233" target="_blank">00:03:53.120</a></span> | <span class="t">and processing them into a nicer format. It's really easy to use it. We just point it to our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=240" target="_blank">00:04:00.640</a></span> | <span class="t">directory that we just created. What are we doing here? We're loading those docs and here I'm just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=249" target="_blank">00:04:09.280</a></span> | <span class="t">printing out the length of those docs so that we can see. We have 390 HTML pages that have been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=256" target="_blank">00:04:16.080</a></span> | <span class="t">downloaded there for some reason. When I ran this about an hour ago, they actually had 389,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=265" target="_blank">00:04:25.280</a></span> | <span class="t">now they have 390 pages, so it's already out of date. Cool. Let's have a look at one of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=272" target="_blank">00:04:32.640</a></span> | <span class="t">pages. We have this document object. Inside that we have page content, which is all of our text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=280" target="_blank">00:04:40.960</a></span> | <span class="t">If we want to print that in a nicer format, we can see this. Looks pretty good. There is some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=291" target="_blank">00:04:51.040</a></span> | <span class="t">messy parts of this, but it's not really a problem. We could try and process that if we wanted to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=299" target="_blank">00:04:59.440</a></span> | <span class="t">but honestly, I don't really think it's worth it because a large language model can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=304" target="_blank">00:05:04.480</a></span> | <span class="t">handle this very easily. I personally wouldn't really bother with that. I'd just take it as it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=311" target="_blank">00:05:11.920</a></span> | <span class="t">is. Now, at the end of this object, we come right to the end if it lets me, we see that we have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=322" target="_blank">00:05:22.000</a></span> | <span class="t">metadata here. Inside the metadata we have the source, which is in this case the file path,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=330" target="_blank">00:05:30.480</a></span> | <span class="t">but fortunately the way that we've set this up is that we can just replace rtdocs with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=336" target="_blank">00:05:36.000</a></span> | <span class="t">HTTPS and that will give us a URL for this particular file. Let's come down here and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=343" target="_blank">00:05:43.040</a></span> | <span class="t">can see that's what I'm doing here. Replace rtdocs with HTTPS. Cool. Then we can click that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=351" target="_blank">00:05:51.200</a></span> | <span class="t">and we come over to here. Now, this is where we start talking about the chunking of what we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=358" target="_blank">00:05:58.960</a></span> | <span class="t">doing. When we are thinking about chunking, there are a few things to consider. The first thing to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=368" target="_blank">00:06:08.400</a></span> | <span class="t">consider is how much text or how many tokens can our large language model or whatever process is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=376" target="_blank">00:06:16.400</a></span> | <span class="t">what we're doing, how many tokens can it handle? What is optimal for our particular use case?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=382" target="_blank">00:06:22.480</a></span> | <span class="t">The use case that I'm envisioning here is retrieval augmentation for question answering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=390" target="_blank">00:06:30.240</a></span> | <span class="t">using a larger language model. What does that mean exactly? It's probably best if I draw it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=396" target="_blank">00:06:36.560</a></span> | <span class="t">We're going to have our large language model over here and we're going to ask it questions. We have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=402" target="_blank">00:06:42.480</a></span> | <span class="t">our question over here. It's supposed to be a queue. It's fine. We have our question. We're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=409" target="_blank">00:06:49.360</a></span> | <span class="t">going to say, "What is the LLM chain in LangChain?" If we pass that straight into our large language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=417" target="_blank">00:06:57.360</a></span> | <span class="t">model, at the moment using GPT 3.5 Turbo, even GPT 4, they can't answer that question because they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=424" target="_blank">00:07:04.960</a></span> | <span class="t">don't know what the LangChain library is. In this scenario, what we would do is we'd go to Vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=433" target="_blank">00:07:13.600</a></span> | <span class="t">Database. We don't really need to go into too much detail here. We go to Vector Database, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=440" target="_blank">00:07:20.000</a></span> | <span class="t">where we saw all of the documents that we're processing now, all those LangChain dots. They</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=445" target="_blank">00:07:25.760</a></span> | <span class="t">would end up within that space and they would be retrieved. We would pass in five or so of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=453" target="_blank">00:07:33.600</a></span> | <span class="t">chunks of text that are relevant to our particular query alongside our original query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=462" target="_blank">00:07:42.320</a></span> | <span class="t">What you'd end up with is rather than, let's say this is your prompt, you typically have your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=468" target="_blank">00:07:48.240</a></span> | <span class="t">query. Rather than just a query, you'd have your query and then you'd also have these five bits of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=477" target="_blank">00:07:57.200</a></span> | <span class="t">relevant information below the query. That would all go into the large language model. You would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=483" target="_blank">00:08:03.280</a></span> | <span class="t">essentially say to it, you'd probably have some instructions near the top and those instructions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=488" target="_blank">00:08:08.320</a></span> | <span class="t">would say, I want you to answer this question. You'd maybe give the questionnaire and give it a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=495" target="_blank">00:08:15.760</a></span> | <span class="t">bit later on using the context that we have provided. You would basically, in front of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=501" target="_blank">00:08:21.920</a></span> | <span class="t">contexts, you would write context. The large language model will answer the question based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=508" target="_blank">00:08:28.560</a></span> | <span class="t">on those contexts. That's the scenario we're envisioning here. In this scenario, if we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=517" target="_blank">00:08:37.280</a></span> | <span class="t">input five of these contexts into each one of our retrieval augmented queries, we need to think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=526" target="_blank">00:08:46.080</a></span> | <span class="t">what is the max token limit of our large language model and how much of that space can be reserved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=533" target="_blank">00:08:53.440</a></span> | <span class="t">for these contexts? In this scenario, let's say that we're using GPT 3.5 Turbo. The token limit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=543" target="_blank">00:09:03.520</a></span> | <span class="t">for GPT 3.5 Turbo is something like 4,096. This includes both. You have your large language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=557" target="_blank">00:09:17.200</a></span> | <span class="t">I'm going to put that here. Pretend this is your large language model. This 4,096 includes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=564" target="_blank">00:09:24.480</a></span> | <span class="t">input to the large language model, so all of your input tokens, and also all of your generated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=573" target="_blank">00:09:33.600</a></span> | <span class="t">output tokens. Basically, we can't just use that full 4,000 tokens on the input. We need to leave</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=582" target="_blank">00:09:42.800</a></span> | <span class="t">some space for the output. Also, within the input, we have other components. It's not just the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=589" target="_blank">00:09:49.280</a></span> | <span class="t">context, but we also have the query. That's supposed to say query. As well as that, we might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=600" target="_blank">00:10:00.560</a></span> | <span class="t">also have some instructions. I don't know why I'm writing so bad. As well as the instructions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=611" target="_blank">00:10:11.040</a></span> | <span class="t">we might also have a bit of track history if this is a track bot. Basically, the amount of contexts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=618" target="_blank">00:10:18.960</a></span> | <span class="t">that we can feed in is pretty limited. In this scenario, let's just assume that we can pass in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=626" target="_blank">00:10:26.000</a></span> | <span class="t">a context of around half of the 4,000 tokens. We'll say 2,000 is going to be our limit. If 2,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=634" target="_blank">00:10:34.240</a></span> | <span class="t">is our limit, that means we need to divide that by five because those 2,000 tokens need to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=643" target="_blank">00:10:43.840</a></span> | <span class="t">shared by our five contexts, which leaves us with about 400 of these tokens per context. That's our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=655" target="_blank">00:10:55.520</a></span> | <span class="t">maximum chunk size. Now, one question that we might have here is, could we reduce the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=661" target="_blank">00:11:01.440</a></span> | <span class="t">of tokens further? For sure, we can. I would say the minimum number of tokens that you need within</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=669" target="_blank">00:11:09.200</a></span> | <span class="t">a context is for you to read this context, does it make sense? If you have enough words in there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=678" target="_blank">00:11:18.000</a></span> | <span class="t">for that context to make sense to you as a human being, then that means that it is probably enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=685" target="_blank">00:11:25.840</a></span> | <span class="t">to feed as a chunk of text into a large language model, into a bedding model, and so on. If that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=694" target="_blank">00:11:34.240</a></span> | <span class="t">chunk of text has enough text in there to have some sort of meaning to itself, then the chunk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=701" target="_blank">00:11:41.040</a></span> | <span class="t">is probably big enough. As long as you satisfy that, that should be the criteria for your minimum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=708" target="_blank">00:11:48.000</a></span> | <span class="t">size of that chunk of text. Naturally, for the maximum size of chunk of text, we have the 400</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=714" target="_blank">00:11:54.720</a></span> | <span class="t">tokens that we just calculated now. With all of that in mind, we need to take a look at how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=721" target="_blank">00:12:01.200</a></span> | <span class="t">we would actually calculate the size of these chunks, because we're not basing this on character</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=728" target="_blank">00:12:08.720</a></span> | <span class="t">length, we're basing this on token length. In order to do that, we need to look at how to tokenize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=735" target="_blank">00:12:15.360</a></span> | <span class="t">text using the same tokenizer that our large language model uses, and then we can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=742" target="_blank">00:12:22.960</a></span> | <span class="t">count the number of tokens within each chunk. Getting started with that, we are going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=749" target="_blank">00:12:29.680</a></span> | <span class="t">using the TickToken tokenizer. Now, this is specific to OpenAI models. Obviously, if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=755" target="_blank">00:12:35.120</a></span> | <span class="t">using Cohere, HuggingFace, and so on, this is going to be a slightly different approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=759" target="_blank">00:12:39.280</a></span> | <span class="t">First, we want to get our encoding. There are multiple TickToken tokenizers that OpenAI uses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=768" target="_blank">00:12:48.000</a></span> | <span class="t">This is just one of those. Now, let's initialize that, and I will talk a little bit about where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=773" target="_blank">00:12:53.920</a></span> | <span class="t">we're getting these encoders from. You can actually find details for the tokenizer at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=780" target="_blank">00:13:00.000</a></span> | <span class="t">this link here. This link is in the GitHub repo, TickToken, TickTokenModel.py. I'm going to click</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=788" target="_blank">00:13:08.880</a></span> | <span class="t">through to that. This is in the OpenAI TickToken repository on GitHub. You can see we have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=795" target="_blank">00:13:15.360</a></span> | <span class="t">model to encoding dictionary here. Within this, you can see that we have a mapping from each of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=802" target="_blank">00:13:22.080</a></span> | <span class="t">the models to the particular tokenizer that it uses. We are going to use the GPT-3.5 Turbo model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=809" target="_blank">00:13:29.360</a></span> | <span class="t">which uses a CL-100K base. I would say I think most of the more recent models, like the models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=817" target="_blank">00:13:37.120</a></span> | <span class="t">that you'll be using at the time of recording this video, they all use this encoder. The embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=826" target="_blank">00:13:46.240</a></span> | <span class="t">model that is the most up-to-date uses CL-100K base. The trapped GPTs, GPT-3.5 Turbo uses CL-100K</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=835" target="_blank">00:13:55.840</a></span> | <span class="t">base. GPT-4 also uses it. The only one that is still kind of a relevant model is the Textive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=843" target="_blank">00:14:03.600</a></span> | <span class="t">Engine 0.0.3 model. That is the only relevant model that doesn't use that encoder. This one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=850" target="_blank">00:14:10.960</a></span> | <span class="t">uses a P50K base. In reality, you don't even need to go there to find out the encoding that you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=858" target="_blank">00:14:18.240</a></span> | <span class="t">to use. You can actually just see this. TickToken, encoding for model, and you can run this. You get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=865" target="_blank">00:14:25.440</a></span> | <span class="t">the CL-100K base. That's how we know. Now, anything else? I think that is pretty much it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=873" target="_blank">00:14:33.760</a></span> | <span class="t">Actually, here I'm creating this TickTokenLength function. That is going to take some text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=881" target="_blank">00:14:41.680</a></span> | <span class="t">It's going to use the tokenizer to calculate the length of that text in terms of TickToken tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=889" target="_blank">00:14:49.920</a></span> | <span class="t">That's important because we need to use that for our LineChainSplitter function in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=895" target="_blank">00:14:55.760</a></span> | <span class="t">We create that. Then what we can do is just first, before we jump into the whole chunking component,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=905" target="_blank">00:15:05.840</a></span> | <span class="t">I want to have a look at what the length of documents looks like at the moment. I'm going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=911" target="_blank">00:15:11.760</a></span> | <span class="t">to calculate the token counts, the TickTokenLength function. Come to here, we can see the minimum,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=918" target="_blank">00:15:18.720</a></span> | <span class="t">maximum, and average number of tokens. The smallest document contains just 45 tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=924" target="_blank">00:15:24.400</a></span> | <span class="t">This is probably a page that we don't really need. It probably doesn't contain anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=932" target="_blank">00:15:32.320</a></span> | <span class="t">useful in there. Maximum is almost 58,000 tokens, which is really big. I'm not sure what that is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=940" target="_blank">00:15:40.640</a></span> | <span class="t">but the average is a bit more normal, so 1.3 thousand there. We can visualize the distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=949" target="_blank">00:15:49.760</a></span> | <span class="t">of those pages and the amount of tokens they have. The vast majority of pages, they're more towards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=958" target="_blank">00:15:58.480</a></span> | <span class="t">the 1,000 token range, as we can see here. All right, cool. Now, let's continue and we'll start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=968" target="_blank">00:16:08.240</a></span> | <span class="t">and look at how we're going to chunk everything. Again, we're using LineChain here. We're using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=973" target="_blank">00:16:13.120</a></span> | <span class="t">a text splitter and we're using the recursive character text splitter. Now, this is, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=978" target="_blank">00:16:18.400</a></span> | <span class="t">probably one of the best chunkers or text splitters that LineChain offers at the moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=984" target="_blank">00:16:24.080</a></span> | <span class="t">It's very general purpose. They do also offer some text splitters that are more specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=990" target="_blank">00:16:30.160</a></span> | <span class="t">to Markdown, for example, but I like this one. You can use it for a ton of things. Let me just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=999" target="_blank">00:16:39.280</a></span> | <span class="t">explain it very quickly. Basically, what it's going to do is it's going to take your length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1005" target="_blank">00:16:45.120</a></span> | <span class="t">function, so the tick token length, and it's going to say, "I need to split your text so that each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1011" target="_blank">00:16:51.680</a></span> | <span class="t">chunk does not go over this chunk size here," so this 400. It's going to split based on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1019" target="_blank">00:16:59.040</a></span> | <span class="t">separators. The reason we have multiple separators is that it first starts by trying to find double</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1025" target="_blank">00:17:05.840</a></span> | <span class="t">new lines. This is a double new line separator. It's going to try and split on that first. If it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1030" target="_blank">00:17:10.720</a></span> | <span class="t">can't find a good split using the double new line characters, it will just try a single new line,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1038" target="_blank">00:17:18.960</a></span> | <span class="t">then it will try a space, and as a very last resort, it will just split on anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1044" target="_blank">00:17:24.800</a></span> | <span class="t">Cool. Then one final thing that we have here is this chunk overlap. This chunk overlap is saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1051" target="_blank">00:17:31.120</a></span> | <span class="t">for every chunk, we are going to overlap it with the next chunk by 20 tokens. Let me draw that out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1062" target="_blank">00:17:42.320</a></span> | <span class="t">so it makes more sense. Imagine we have a ton of texts. There's loads of texts here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1069" target="_blank">00:17:49.280</a></span> | <span class="t">Now, we are going to get a chunk of 400 characters. Let's say that chunk takes us from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1081" target="_blank">00:18:01.440</a></span> | <span class="t">here all the way to, say, here. We have 400 characters in this chunk. Then the next chunk,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1091" target="_blank">00:18:11.360</a></span> | <span class="t">if we don't have any chunk overlap, would be 400 characters from this. Let's say it's to here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1098" target="_blank">00:18:18.080</a></span> | <span class="t">This comes with a problem because we don't know what this information here and this information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1107" target="_blank">00:18:27.280</a></span> | <span class="t">here is about. They could be related. We might be missing out on some important information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1114" target="_blank">00:18:34.400</a></span> | <span class="t">by just splitting in the middle here. It's important to try and avoid that if possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1121" target="_blank">00:18:41.680</a></span> | <span class="t">The most naive way or naive approach for doing this is to include a chunk overlap.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1127" target="_blank">00:18:47.760</a></span> | <span class="t">What we would do is, let's say we take the 20 tokens behind this. We're going to go back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1138" target="_blank">00:18:58.720</a></span> | <span class="t">20 tokens, which maybe comes to here. That means that this space here is now going to be shared by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1149" target="_blank">00:19:09.280</a></span> | <span class="t">the first chunk and the next chunk, which will also bring back the next chunk to something like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1158" target="_blank">00:19:18.400</a></span> | <span class="t">here. Now, we have chunk one here, which goes from here up to here. Then we have chunk two, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1173" target="_blank">00:19:33.440</a></span> | <span class="t">from here to here. Following on from that, we would also add another chunk overlap for number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1183" target="_blank">00:19:43.120</a></span> | <span class="t">three. Number three would go from here to, let's say, here. Finally, for number four,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1189" target="_blank">00:19:49.040</a></span> | <span class="t">we go from here to here. The chunk overlap is just to make sure that we're not missing any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1195" target="_blank">00:19:55.600</a></span> | <span class="t">important connections between our chunks. It does mean that we're going to have a little bit more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1203" target="_blank">00:20:03.360</a></span> | <span class="t">data to store there, because we're including these chunks of 20 in multiple places.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1210" target="_blank">00:20:10.720</a></span> | <span class="t">But I think that's usually worth it in terms of the better performance that you can get by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1217" target="_blank">00:20:17.920</a></span> | <span class="t">not missing out that important information, that important connection between chunks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1222" target="_blank">00:20:22.640</a></span> | <span class="t">We initialize that. Then, to actually split the text, we use the text splitter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1230" target="_blank">00:20:30.080</a></span> | <span class="t">split text. We're going to take DOPS5, and we're going to take the page content, which is just the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1236" target="_blank">00:20:36.320</a></span> | <span class="t">plain text. Based on the parameters that we set here, chunk size of 400 and chunk overlap of 20</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1246" target="_blank">00:20:46.160</a></span> | <span class="t">using the tick token length token, we get two chunks. Let's have a look at the length of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1251" target="_blank">00:20:51.760</a></span> | <span class="t">two chunks. The first chunk that we get is 346 tokens. Next one, 247. Both within that max upper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1262" target="_blank">00:21:02.800</a></span> | <span class="t">end limit of 400. You see that it's not going to necessarily split on the 400 tokens specifically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1271" target="_blank">00:21:11.440</a></span> | <span class="t">because we have the specific separators that we would like to use. It's going to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1278" target="_blank">00:21:18.400</a></span> | <span class="t">preferably for this separator. We're not going right up to that limit with every single chunk,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1285" target="_blank">00:21:25.040</a></span> | <span class="t">which is fine. That's kind of ideal. We don't necessarily need to put in a ton of text there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1292" target="_blank">00:21:32.320</a></span> | <span class="t">That's it for a single document. What we're going to do now is we're going to repeat that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1300" target="_blank">00:21:40.400</a></span> | <span class="t">over the entire dataset. The final format that I want to create here is going to look like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1306" target="_blank">00:21:46.640</a></span> | <span class="t">We're going to have the ID, we're going to have our text, and we're going to have the source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1310" target="_blank">00:21:50.000</a></span> | <span class="t">where this text is actually coming from. One thing that you'll notice here is the ID. We're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1320" target="_blank">00:22:00.240</a></span> | <span class="t">create an ID and that ID will be unique to each page. We're going to have multiple chunks for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1328" target="_blank">00:22:08.160</a></span> | <span class="t">page. That means we're also going to add in this chunk identifier onto the end of the ID to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1334" target="_blank">00:22:14.320</a></span> | <span class="t">sure that every ID for every chunk is actually unique. Let me show you how we're going to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1341" target="_blank">00:22:21.760</a></span> | <span class="t">that. Essentially, we have the URL here. We're going to replace the RT docs that we have here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1350" target="_blank">00:22:30.480</a></span> | <span class="t">with the actual HTTPS protocol. I'm just going to print out so you can see what it is. Then we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1358" target="_blank">00:22:38.160</a></span> | <span class="t">going to take that URL, we're going to add it to this hashlib MD5. This is just a hashing function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1365" target="_blank">00:22:45.680</a></span> | <span class="t">that is going to take our URL and hash it into a unique identifier. This is useful because if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1375" target="_blank">00:22:55.040</a></span> | <span class="t">are updating this text at some point in the future or this dataset, we can use the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1382" target="_blank">00:23:02.400</a></span> | <span class="t">hashing function to create our unique IDs. That means that when we update this particular page,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1387" target="_blank">00:23:07.680</a></span> | <span class="t">it will just overwrite the previous versions of that item because we're using the same ID.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1395" target="_blank">00:23:15.760</a></span> | <span class="t">Of course, we can't use the same ID for every single chunk. We also need to add in this here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1402" target="_blank">00:23:22.640</a></span> | <span class="t">which is like the chunk identifier. It's just a count of the number of chunks. We can see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1410" target="_blank">00:23:30.400</a></span> | <span class="t">being created here. These are just two examples from the previous page that we just showed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1417" target="_blank">00:23:37.040</a></span> | <span class="t">So you can see we have the chunk identifier and indeed the chunks are different. This says</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1423" target="_blank">00:23:43.360</a></span> | <span class="t">language model cascades, ICE primer books, Socratic models. Okay, whatever. Let's take a look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1430" target="_blank">00:23:50.160</a></span> | <span class="t">at what is at the end of the first item. It should be something similar. There should be the overlap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1437" target="_blank">00:23:57.120</a></span> | <span class="t">that I mentioned. You can see language model cascades, ICE primer books, Socratic models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1446" target="_blank">00:24:06.000</a></span> | <span class="t">Same thing. Cool. So there is the overlap. Now what we need to do is repeat this same logic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1454" target="_blank">00:24:14.080</a></span> | <span class="t">that we've just created across our entire dataset. To do that, same thing that we just did. We're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1459" target="_blank">00:24:19.520</a></span> | <span class="t">going to take the URL, we're going to create our unique ID, we're going to take the chunks using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1463" target="_blank">00:24:23.760</a></span> | <span class="t">the text splitter, and then we're going to append these all to our documents list here. That's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1471" target="_blank">00:24:31.280</a></span> | <span class="t">going to be where we store everything. Okay. Now, so the length of the documents an hour ago was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1479" target="_blank">00:24:39.920</a></span> | <span class="t">a little bit less. Now it is 2,212 documents. Cool. We can now save them to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1493" target="_blank">00:24:53.040</a></span> | <span class="t">JSONlines file. To do that, we just do this. JSONlines is basically, it's what you can see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1501" target="_blank">00:25:01.200</a></span> | <span class="t">If we take a look at the documents, look at the first five, it's this, but it's just in a JSONlines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1508" target="_blank">00:25:08.640</a></span> | <span class="t">file. You can see it here. Same thing. Then once you've saved it and you create your JSONL file,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1517" target="_blank">00:25:17.840</a></span> | <span class="t">you just load it from file like this. With open train JSONL, wherever you saw it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1525" target="_blank">00:25:25.200</a></span> | <span class="t">and you just load it iteratively like that. You can take a look. Yeah. Okay, great. That's how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1533" target="_blank">00:25:33.760</a></span> | <span class="t">you would load it. Now, a couple of things here. The reason that we're using JSONL and the reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1539" target="_blank">00:25:39.840</a></span> | <span class="t">I'm calling this train.JSONL is because this makes it very compatible with HuggingFace datasets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1547" target="_blank">00:25:47.760</a></span> | <span class="t">Which is essentially a way of sharing your dataset with others, or just making it more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1554" target="_blank">00:25:54.000</a></span> | <span class="t">accessible for yourself if you set to being a private dataset. What I want to do is just show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1559" target="_blank">00:25:59.040</a></span> | <span class="t">you how we can actually go about doing that as well. The first thing that we need to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1564" target="_blank">00:26:04.160</a></span> | <span class="t">is go to HuggingFace.co. That will bring you to the first page of HuggingFace, which may look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1570" target="_blank">00:26:10.800</a></span> | <span class="t">different to you because you may not already have an account on HuggingFace. If you do need an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1577" target="_blank">00:26:17.680</a></span> | <span class="t">account or you need to sign in, there will be a little button over here that says sign up or log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1582" target="_blank">00:26:22.080</a></span> | <span class="t">in. You would follow that, create your account or log in. Then you will see something like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1587" target="_blank">00:26:27.920</a></span> | <span class="t">at which point you go over to your profile. We click new dataset. We give our dataset a name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1594" target="_blank">00:26:34.000</a></span> | <span class="t">I'm going to call it LangChainDots. You can obviously call this whatever you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1598" target="_blank">00:26:38.720</a></span> | <span class="t">You can set it to private if you want to keep this dataset private. For me, I'm going to just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1603" target="_blank">00:26:43.360</a></span> | <span class="t">leave it as public. You create your dataset. On here, this is like the page of your dataset,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1611" target="_blank">00:26:51.040</a></span> | <span class="t">like the homepage of your dataset. You go to files. You go to add file, upload files.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1617" target="_blank">00:26:57.200</a></span> | <span class="t">Then you just need to drag in the train.jsonl file to here. For me, that is here. I'm just going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1628" target="_blank">00:27:08.640</a></span> | <span class="t">go and drag that in. We go down, commit changes to main. We have now uploaded that. We can go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1637" target="_blank">00:27:17.200</a></span> | <span class="t">click on files here and we'll be able to see that we have the train.jsonl file in there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1642" target="_blank">00:27:22.000</a></span> | <span class="t">Now, to actually use that in our code, we would need to pip install datasets. This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1647" target="_blank">00:27:27.760</a></span> | <span class="t">the library for HuggingFace datasets. Then we would write this. We do from datasets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1656" target="_blank">00:27:36.320</a></span> | <span class="t">import load dataset. Then our data would be a load dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1665" target="_blank">00:27:45.040</a></span> | <span class="t">Here, we need the name of our dataset. Let's go back to the dataset page. We can find that at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1676" target="_blank">00:27:56.000</a></span> | <span class="t">the top here. It said it's James Callum LangChainDots. We can just copy it, add that into here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1682" target="_blank">00:28:02.400</a></span> | <span class="t">Our split is the training split. That's where the train.jsonl comes in. Then we can view the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1691" target="_blank">00:28:11.520</a></span> | <span class="t">details there. Once that has loaded, we will be able to see. We can just extract things. Data zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1700" target="_blank">00:28:20.960</a></span> | <span class="t">we can see that we have our text in there. It's super easy to work with. That's why I recommend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1709" target="_blank">00:28:29.040</a></span> | <span class="t">storing your data on HuggingFace datasets if you're wanting to share it. Even if you're wanting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1714" target="_blank">00:28:34.720</a></span> | <span class="t">to do the private approach, you can do that as well. You just need, I think it's like an API key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1719" target="_blank">00:28:39.680</a></span> | <span class="t">and that's pretty much it. That's it for this video. I just wanted to cover some of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1725" target="_blank">00:28:45.200</a></span> | <span class="t">approaches that we take when we are considering how to chunk our text and actually process it for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1733" target="_blank">00:28:53.760</a></span> | <span class="t">large language models and also see how we might store that data later on as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1740" target="_blank">00:29:00.400</a></span> | <span class="t">Both of these items, I think we miss a lot in the typical videos. We're really focusing on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1747" target="_blank">00:29:07.760</a></span> | <span class="t">the large language model processing or the retrieval augmentation or whatever else. This,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1755" target="_blank">00:29:15.600</a></span> | <span class="t">in reality, is probably one of the most important parts of the entire process. We miss it pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1761" target="_blank">00:29:21.120</a></span> | <span class="t">often. Anyway, that's it for this video. Thank you very much for watching. I hope this is all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1767" target="_blank">00:29:27.520</a></span> | <span class="t">being useful and interesting. I will see you again in the next one. Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eqOfr4AGLk8&t=1775" target="_blank">00:29:35.920</a></span> | <span class="t">[Music]</span></div></div></body></html>