<html><head><title>[Full Workshop] How to add secure code interpreting in your AI app: Vasek Mlejnsky</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>[Full Workshop] How to add secure code interpreting in your AI app: Vasek Mlejnsky</h2><a href="https://www.youtube.com/watch?v=k0VIgKAUkP4" target="_blank"><img src="https://i.ytimg.com/vi_webp/k0VIgKAUkP4/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>Vasek Reviewer: Peter van de Ven Hi, everyone. Thanks for coming. Thanks for coming to my workshop. My name is Vasek, and today I will show you, and you can code along, how to add AI code interpreting into your AI app or into your app. So what we will be building will be very similar to Claude's new AI Artifacts app that they-- or Anthropics AI Artifacts app that they just released about two days ago.</p><p>So if you are not familiar with it, the way it looks like is that you have a chat on the left, and on the right, you have this preview. And what this preview does is that Claude can write code. In Claude's case, it's supported just for HTML, JS, and CSS code, if I'm not mistaken.</p><p>And then it actually renders and run the code right next to the chat. We will build something very similar with a few caveats. And first is that it-- we will be able to run AI-generated Python code, not just HTML, CSS, JavaScript. I will also show you, in the end, how to run other-- other run times and other languages.</p><p>And we will be using E2B, which is a company that I co-funded, and for secure code interpreting, and you can code along while we are building it. So here's a quick demo of what we are going to build. On the left, we will be chatting with SONET 3.5, and every time it generates code, it's actually implemented through function calling.</p><p>We will run that code in a secure sandbox that can run pretty much anything that a Linux machine or Ubuntu machine can run. And you can spawn many, many of these sandboxes for secure code interpreting. The workshop is made for code along, but of course you can just watch.</p><p>But if you are interested in actually following me, you can go to our repository and clone the cookbook. And then check out your workshop start. And you also will need two API keys. One of them is E2B API key, which you can get on our documentation. So E2B, E2B.dev/docs, then you go to API key, you sign up, and we give you your API key.</p><p>And also you will need Antropics API key. In case you don't have that, or you don't want to get it, I created one for this workshop. And so if you go to -- and you are free to -- well, I kind of trust you that you will not misuse it.</p><p>So if you go to dup.sh/e2b-workshop, you can get the API key for free. I will delete it after the workshop. So I know everyone had a lunch, so you are probably kind of tired. I will try to keep it sort of short. So it most likely will not take two hours.</p><p>We will get to result much faster. So I will give you, like, a few quick minutes -- a few seconds, or tens of seconds to set up everything. But just to get a better idea, how many of you want to follow along? Okay. Cool. Sorry? Oh, shoot. Oh, that's probably because, like, we have images there that you can generate with -- oh, sorry.</p><p>I didn't realize that. Yeah. That's a good point. By the way, if anyone has any questions, just, like, feel free to ask during the workshop. If anything isn't clear, I will repeat a question and -- and try to give you an answer. Okay. Yes. Is the code interpreter running on our machines, or -- So the question is, is the code interpreter running on your machine, or somewhere else?</p><p>The code interpreter isn't running on your machine. It's using a tool called E2B, which I'm a co-founder of. We're basically building an open source runtime for AI agents that you can -- and very soon will be to self-host on AWS, GCP, Azure, and other big cloud providers. I can go a little bit more into detail.</p><p>What's the whole problem and challenge with code interpreting around security? I was thinking that maybe first we implement the app so you can kind of get an idea of how it works. And then during Q&A, or after that, I can get into the security and what are the challenges if you want to build this in-house.</p><p>Cool. So how -- how's the -- how's the repo cloning look like? Slow? Oh, shoot. Okay. Sorry? Oh, yeah. That's a good point. You know what? Let me -- I probably can create very quickly just a repo without other examples. And I will just put the example that we are interested in there, which should be much smaller.</p><p>Excuse me. Is there a link to your slides? Sorry. I don't have slides. Is there a link to your slides? Yeah. Okay. So if you go -- if you go to GitHub/etub-dev/etub-cookbook, you go to examples. Oh, okay. And there's this Anthropik. It's called Power Artifacts. And we will be building, like, an open source version of Anthropik's new Artifacts UI that has sort of code in it.</p><p>Okay. Thank you. Okay. Let me just quickly set up the new repository. Okay, this one should be much smaller. If you go to this URL, I will show it on a big screen. If you go to this URL, which is my personal GitHub, and go to /workshop-repo, you should be able to clone it much faster.</p><p>That link is in the Slack now. Thank you. Perfect. Okay. Can you -- you should be able to clone it now. I didn't have -- I have only readme there. Now all the files are there. And -- oh, I don't have branches there. I just realized. Actually, this should be good.</p><p>It should be good. Yeah. Yeah. Good. Perfect. So -- sorry for the confusion. Let me do it again properly. Okay. So if you want to clone -- if you want to code along, please go to this GitHub repository and clone it. This should be much faster. Is it faster?</p><p>Yes. Perfect. And do you already have it cloned? Yes. Perfect. Okay. Now we can start. When -- after you clone it, just install dependencies, and you will also need those API -- yeah, API keys. So if you create a file .env.local, which will need E2B API key and Anthropic API key.</p><p>So you can get E2B API key on this website. So if you go to E2B.dev, click get started, and go to API key. If you quickly sign up, you can get your API key. If you just want to follow along -- yeah, if you just want to look, you don't need that API key.</p><p>And you will also have -- you will also need Anthropic's API key. And in case you don't want to spend your money on that, I created -- I created an Anthropic API key just for this workshop. So if you go to this link, it should open a one password share -- share model or share -- share website.</p><p>And you can get your Anthropic API key there. And I will cancel it after the workshop. All good? Perfect. So once you have your API key set up, install dependencies, and then just run npm run dev. All right. And you should see what I'm seeing here. The app should start on the port 3000.</p><p>So what we are doing just now is we started an XJS app and -- which is like a starting point. And we will now follow -- you can now follow and code along. And I will show you how to add the code interpreting there. So if you open the -- the app, it -- oops.</p><p>It just shows an input, which is like -- this is a scaffold of the app where you can ask something. So let's say we want to generate random chart. And actually now it won't work because we need to implement everything. So I just hit an enter, but I'm -- I should be getting -- or you should see in your terminal 405.</p><p>So now that we have everything set up -- in case I'm too fast, just like scream at me. And I will slow down or just wait for you. Now we have everything set up. I want to just quickly show around the code, around the repository, and what we will be doing.</p><p>So there are two important parts of this project. First is what we are going to use is Vercel's AI SDK. And this in the JavaScript world is essentially easy and nice way how you can connect to different models and stream them to your -- to your frontend from your backend.</p><p>So it's already installed once -- when you -- when you ran npm install. In case you want to check out the docs after -- after the workshop or during, go to this link. And the second thing that we will be using is the code interpreter SDK. So that's -- that's something that we built at E2B.</p><p>And essentially what it does -- it's open source. And what it does is that you can create your own custom code interpreter. And that you can predefine the whole environment. That coding interpreter runs in a sandbox. Which is like a small VM. When you -- when you are using our cloud, it runs on our cloud.</p><p>Soon you will be able to self-deploy it. And specifically for this SDK, inside the sandbox is a Jupyter server running. To which you can just send Python, JavaScript, R code, and Java code. We have in beta. And you get back standard output, error output. But you also get back charts.</p><p>So the reason we are using a Jupyter server there is because that's what we notice has been one of the most frequent use case from our users and our customers. And generally, at least during the time of GPT-4 and for Turbo, the code that you usually got from the LLM was -- was for Python, like data science, Jupyter notebook.</p><p>And it's really great for visualization, which is like a big use case for what we are seeing with users. Just a -- just a quick note. This whole code interpreter SDK is built on like a general sandbox with a small VM. We start those VMs pretty quickly, around a few hundred milliseconds.</p><p>Every time you create an instance of code interpreter, we actually create that sandbox. So this is kind of like a wrapper around our sandbox SDK. It's also open source, which is -- which is here. And so if you want to build something pretty, pretty custom, you can do that.</p><p>So code interpreter is like a one specific use case of how you can use the sandbox. You can look at the sandbox as a general Ubuntu machine that's made with security in mind for running like AI-generated code. So if -- as you noticed, when we were running this app, when I wrote something, I just hit enter and nothing happened.</p><p>So first we need to -- we need to implement an endpoint to call the -- to call the VM -- sorry, to call the LLM. code interpreter. And for that, we are using Vercel's serverless function. And there's a predefined file called road.ts. And here is commented out a function that will handle our post requests that we need to implement.</p><p>And here we will be using the Vercel's AI SDK. So now we finally start coding. And now we will do just like a simple, simple implementation of messaging with Claude. First, we need to parse those messages coming from the request. And then we actually use the AI SDK versus AI SDK.</p><p>And from that, we will be using the stream text method. And so the model we are using is Anthropic, which is the sonnet one. And we want to parse the messages. Oh, yeah. There's an evade I was missing. And we also want to include a system prompt. And so I already predefined a prompt in a file.</p><p>It should be in a lib directory. There is a prompt file. And so the prompt is really simple. It's a simple prompt that tells the LLM sonnet that it's a skilled Python developer that can do a bunch of visualizations. So let's import that. Variable. Prompt. Okay. And. Add it here.</p><p>And now. That should be pretty much it for the basic implementation. Now we just need to return stream from our post endpoint. So result dot to AI stream response. All these are methods on the versus AI SDK. So if you are more curious, you can just jump into it there and enter your documentation.</p><p>But if we go back to our application. Sorry. Now we should see that if I type hello. First, our message is there and we get a response from cloud. Am I too fast or are you following? Too fast? Sorry. Where did you define messages? Yeah. So the message is, so this is just the back end part.</p><p>Oh yeah. And it's coming in the, in the request. The prompt is, sorry, I will show it here, in a, in a file dot prompt dot, in a file called prompt dot TS. And it basically just says that it's a skilled Python developer that's capable of running, like a visualization on data science code.</p><p>It will work for different use cases as well. Now I just wanted to show something where we at the end get a chart that you can render on the front end. Okay. One, one more thing that I wanted to mention is that in case you are not follow, you, you, you are like lost of what just happened.</p><p>In the original repository, in the examples repo, there are branches called workshop one, workshop two, workshop three, workshop five, four and five. And those are like the stages that we are implementing here. So in case you are wondering what is happening, it should be pretty much the same. And if you just want to get to the final point, you can check out the workshop dash final.</p><p>And you should get everything there. Sorry? That file should already be there. It's in lib directory. Lib dash prompt. So while we have the back end, main back end code on the left, I just also wanted to show you how it looks like on the front end really quickly.</p><p>It's a, it's a basic Next.js app, Next.js 13 app. So there is a layout file that just defines a basic layout of the whole app. And, but there's a one important or main important file, which is called page, which is our main page. And there, we are using one hook from the versus AI SDK, which is called use chat.</p><p>And this is how we are streaming the messages from the server and sending the messages to the server. So the use chat here, it's already implemented there, you don't need to write it. It's hitting the endpoint API slash API slash chat, which is what we implemented on the left.</p><p>And it takes care of all the streaming and sending the messages on the, on the back end. And we just then pass the messages to our chat component, which does those chat bubbles. Now, now the, the goal was just to get a communication with cloud, with Sonnet. And later, like we will implement Python.</p><p>Yeah, we will add a tool for running Python. So we'll do it through a function calling or tool usage. If anyone needs it, there's a channel now for the workshop. So the code can get thrown in there. Thank you. Okay. Can you, so how many of you have, have managed to get to this stage?</p><p>Just a few people. Okay. How many of you are still working? Okay. What's the name of the Slack channel? Workshop secure code in your AI. It's listed in general. So the name of the Slack channel is workshop secure code in your AI, right? Yeah. Yeah. Yeah. Sorry. So name of the branch.</p><p>So the repository we are working with is this repository. This is my personal repository. I just created it quickly now because the original cookbook repository wasn't working. If that's the, is that the question? I'll call the original one. Oh, the original one. Yeah. So, that is workshop-start. And, yeah, that's a, that's an original repo I wanted to use, but it's too big and it took too much time for everyone to download it.</p><p>But if you want to check out the final code for, for this, like, first step, you can go to, uh, original repository and, uh, workshop-1. And that should be, uh, the implementation, um, of this step pretty much. Okay. Um, can I get to the next step? Cool. So, uh, now we just are able to chat, uh, with, uh, Sonnet.</p><p>There really isn't any, uh, code execution going on. And the way we get to the code execution is, uh, in this case, through tool usage. So, you could do it in, in different ways. Uh, the easiest out of the box, uh, is, uh, create a tool called something like run python, in our case, for this specific use case.</p><p>And, uh, then inside the tool we will actually implement, uh, secure code execution inside the sandbox. Uh, you could have other tools like run javascript or run, uh, bash command, create a file inside the sandbox. Or you could do it in a completely different way. So, we have seen users, uh, just ditching tool usage because it was paying for the streaming.</p><p>And, uh, they didn't like how you manage the whole tools. And just, they just asked the model to, uh, define, uh, sorry, to, to generate everything in markdown. And they just parse it themselves. So, it's really depending on your use case and what you are looking for. But, like, for this demo tool is the, uh, easiest one.</p><p>So, uh, now we are getting to the, uh, part two. Uh, so if you want to eventually get the final code, you can go to workshop dash, uh, two. Uh, and first thing that we want to define is a new, uh, new, new tool on our, uh, uh, on our backend.</p><p>Uh, inside the versus AR SDK. So, uh, for that we have a helpful helper method called tool. And, uh, we want to create run Python. Uh, we will call our tool run Python. You can name it in whatever way you want. Uh, and there we will use the tool method from versus AR SDK.</p><p>And just describe, uh, what it should do to the LLM. So, what, in JavaScript world, uh, usually you would be using something like Zot for that. That describes the schema of the tool. Uh, in Python you might be using, you might know instructor or something like that. Uh, so first we will provide, like, a simple description, uh, that runs Python code.</p><p>Uh, Python code. And then, uh, we, uh, want to define the parameters for the tool. And we really have one main parameter, uh, parameter, which is code. That's the code we want to run. Uh, and for that we will be, now we'll be using, oh, sorry. Uh, I should have, uh, used Zot here.</p><p>Uh, so Zot object. Uh, we are basically describing the schema, uh, to the, to the LLM and forcing it to always generate, uh, a return this type of structure. So, uh, sorry. First we defined, uh, code, which should be string. And we can also pass description, uh, which is the, the code to run.</p><p>Uh, this is the main, uh, thing we want to pass. Uh, and just for, oh, I'm missing a column here. Just for the better UI that we can then build, uh, we are, we will also pass two more, uh, uh, parameters or ask LLM to pass or give us two more parameters.</p><p>First is title, which is like a, uh, short title of, uh, uh, uh, that describes the code. And we just wanted to use, uh, show it on the front end. And then a description. Which is pretty much similar, but, uh, just a little bit longer. Uh, of, uh, description of what the, what the code, uh, what the code does.</p><p>So just by doing this, uh, we are telling, or versus, versus AR SDK is doing it for us. Uh, it's, it's telling the LLM that it has a tool, uh, called run Python that it can call. But we are not calling it, uh, just yet. Is everything clear so far?</p><p>Or let me ask differently, is something not clear? Oh, thank you. So, uh, mm-hmm. Sorry? The Vercel, the AI SDK? The AI SDK? Yeah. So that's a, that's an SDK that we are using, uh, it's literally called just AI SDK. Uh, it's from Vercel. Uh, you can learn more about it if you go to, uh, sdk.vercel.ai/docs.</p><p>And it's basically, uh, like JavaScript, uh, very easy way to stream output from LLMs and to LLMs in JavaScript world. And especially if you are building like Next.js apps. So inside that tool that we are using, run Python, we need to implement one more function, which is called, uh, execute.</p><p>It will be async function. Uh, and this function is then automatically called when the LLM decides to call our run Python tool. And the parameters, uh, that we just defined here, will get passed to the, uh, to the execute, uh, uh, function. So what we will get here, mainly what we are, what we really care about here, uh, or really the only thing we care about is the code.</p><p>And granted like nothing is going to really happen, uh, but, uh, in, inside this body of this function, we will be calling the code interpreter SDK that I showed you previously. Uh, and the code interpreter sandbox where we execute the code and we get the results from that. Okay.</p><p>So, uh, there's a new, one more new concept that we are going to introduce and which is the concept of the sandbox. Uh, we have a predefined file called sandbox. It's in the lib directory. So if you go to lib, you go to sandbox. I will open it here on the right side.</p><p>Uh, you will see there's a import, uh, that we are using, which is code interpreter. Uh, and we are importing code interpreter from the code interpreter SDK, uh, which is this SDK I was talking about, uh, a few minutes ago. So what this SDK does, um, is that it will run AI generated code, in this case Python code, uh, in a secure environment, uh, and it will, uh, it will be actually running inside the sandbox, which is a small VM.</p><p>It will run on our cloud. Uh, soon it will be able to, uh, run it on your, uh, on your own, uh, cloud. Uh, we have a, we have two methods here that we need to implement. Uh, first is, um, that we need to acquire the sandbox or we need to create a sandbox.</p><p>And once we have a sandbox, uh, we then, uh, can run the Python code inside the sandbox, inside the code interpreter sandbox. And there we have predefined methods on the code interpreter sandbox for that. In the cloud, in the cloud, yeah. Um, so, uh, after, once, once we implement this, I can go into more details about the security and, uh, problems with actually running it on anything else but Linux machine.</p><p>Uh, but, uh, yeah, uh, if, if you have any questions, like, feel free to ask after that. Yeah, go ahead. Go ahead. Uh. Uh. It also refers to the function. Uh, uh, this one? Yeah. Oh, it's just a, it's a, it's a, it's a, oh, I see, I see the confusion.</p><p>Sorry. Uh, we are not referring to the, to the, you mean this function. Referring to this function? Yeah. Is that a key or a function? It's a key. Uh, so, on the line 27, where my cursor is, it's a key of the object. Yeah. And, uh, sorry, I didn't realize that.</p><p>Uh, we will be very soon calling this run Python function that we are importing from the file on the right. Yeah, yeah, yeah, yeah. Uh, that's a good question. I didn't realize that. Thanks. Um. Uh, yeah. So, so the question was, uh, it was confusion about the naming. So here we have, uh, a key called, uh, or field called run Python.</p><p>And there, we also are importing function called run Python on the line 15 here. Uh, and, uh, it's just a, it's not, the line 27 is not actually referring to, uh, function from the line 15. It's just a coincidence of the naming. The, the one on the line 15.</p><p>We are about to use it very, very soon. Uh, so we are just going to implement it. So the, uh, definition of the function, or the, the, um, the function is defined on the, inside the file sandbox that we have on the right. It's inside a lib directory. And this is where we will be using the code interpreter SDK.</p><p>Oof. So, uh, first we need to, first we need to actually get the sandbox. Uh, the code interpreter sandbox. And for that, uh, we can, uh, do it very, very quickly, uh, simply, uh, and that would be, that we create a new sandbox instance. Uh, so we import code interpreter here on line seven from the code interpreter SDK.</p><p>Um, and we can just, uh, call this. And we get, this actually starts, uh, the full, like, small VM on site, inside our cloud that you can control through the SDK. Um, it takes about currently 800, 900 milliseconds, uh, to start it. And you can do it many, many times, uh, uh, at once.</p><p>So, usually what we see is that every user inside your app, every user session would be a separate sandbox. And once we have the sandbox, uh, oh, sorry. Uh, oops. Uh, actually we need to, um, sorry, I, we need to call code interpreter dot create. Uh, I didn't realize.</p><p>And we need to evade that. Uh, that was Python syntax, what I just wrote. Um, uh, and this creates what I, what I, what I just said. And there's one problem with this, is that, now, every time, uh, if I would, uh, call this, uh, endpoint, post endpoint, and, on my back end, and, uh, I would just call run Python function and then create a new sandbox, there will be no concept of context.</p><p>So, it would create a new sandbox every time. And you would be just running the new code in a completely new sandbox every time. You don't have any reference to past code snippets that are generated by the, by the LLM. So, uh, for that, uh, you can, uh, actually have a sandbox running and reconnect to it a little bit later, uh, by, uh, calling reconnect.</p><p>So, what we will change a little bit in our create or connect function is that we first check if the sandbox exists for a given user ID, uh, which would be something like a user session, uh, that you would implement, uh, once you have authentication. And if it has, we will connect to the sandbox.</p><p>If it, uh, if it doesn't exist, uh, if it exists, we connect to the sandbox. If it doesn't exist, we create a new sandbox. Uh, so, we can first list all the sandboxes. Which is code interpreter dot list. Uh, you need to add the await there. And once we have all the sandboxes, uh, we can check if the sandbox has attached, uh, user ID onto its metadata.</p><p>So, what you can do when you are creating a new sandbox is that you can add metadata to the sandbox. Uh, and if the sandbox with our user ID inside its metadata exists, we want to connect to the sandbox. So we keep our old session and have the whole context there.</p><p>Uh, so first we want to find the sandbox if it exists. Uh, so we call find on the all sandboxes, which is a list, uh, of information of all, all our sandboxes that are running. And, uh, and then, uh, we call, uh, metadata. And on that metadata, uh, which is like optional, optional object, uh, or dictionary, uh, it has, it should have user ID key that should be equal to the parameter that we will be passing to this creator connect method.</p><p>And we want to evade this whole full code, this code. Oops. Um, okay. So, is everything clear? And once we have our sandbox info, uh, we check it actually exists. And, uh, if it doesn't exist, that's the moment we want to create a new sandbox. Uh, so we will be actually here, returning a new sandbox.</p><p>And as I said, you can add, um, metadata, uh, to the sandbox. So we will just use this metadata to make a note that, sorry, uh, to make a note that, uh, this sandbox belongs to this user. Uh, in our case, it will be just one user. But in your application, you would have many, many users.</p><p>So that's a case when the sandbox didn't exist and we need to create one. And this line will create a new sandbox in the cloud. And in case, uh, we have a sandbox with a user ID inside its metadata, we want to just connect to the sandbox. And for that, we have, uh, a function called, uh, reconnect on the code interpreter object.</p><p>And there you just, uh, pass the sandbox ID. And that we can get from the sandbox we found on the line 11. Uh, sandbox info dot sandbox ID. And let's just return it. So what this, uh, does is that at first, we, okay, I don't know, we check, uh, all our running sandboxes because we probably have separate sandbox for every user or users, every user session, most likely.</p><p>Um, and then once we have all the running sandboxes, we check the, uh, metadata of each sandbox and check if a user ID, if the sandbox has the same user ID for which we are calling, uh, this create or connect method. If it doesn't, if such sandbox doesn't exist, we will just create a new one.</p><p>We attach user ID to this sandbox metadata. Uh, um, and return this whole, uh, promise and object from the promise. Uh, if it actually exists, uh, this sandbox, sandbox for this user, we will just reconnect to the sandbox. Any question or is everything clear? Cool. So there's one--actually, now we finally can get to implementing the write run Python method.</p><p>So once we have our code--oh, sorry, this doesn't need to be here. Once we have our code for creating or connecting to our existing sandbox, we can just call it. So create or connect to the sandbox. Inside this run Python function, we pass our user ID. And once we have the sandbox, we can finally run the code.</p><p>So let's also add a console log to make sure that we know what kind of code we are running. And I mentioned that inside the sandbox or the code interpreter sandbox in this case, we have a Jupyter server running. And we give you programmatic access to the Jupyter server.</p><p>So you can call notebook on a sandbox instance, and call exec cell. And by default, this method will execute Python code. It can be--it can be AI generated, human written, predefined, whatever you want. And we just pass our code there. And we get results--result from it. And just need to evade that.</p><p>And return the result. Probably we want to log it as well. Yes. Yes. So it's really--it's really just like legit Jupyter notebook. We actually have a PR where you can connect to that notebook that's hosted inside the sandbox. It's probably at the moment the easiest way to how to implement like a consistent-- Is the--is the--is the--is the--is the--is the-- It's really just like legit Jupyter Notebook.</p><p>We actually have a PR where you can connect through that notebook that's hosted inside the sandbox. It's probably at the moment the easiest way how to implement like a consistent-- sorry, implement persistence when you have multiple different code snippets where one code snippet can reference a variable from a different code snippet or function.</p><p>And so during the single user session, you just want to keep usually what we have seen, a single context. But it really depends always on your specific use case. Anyone struggling with the run Python implementation? Yeah. So this is the exact cell capture standard out. But you also mentioned standard error from these.</p><p>Yeah. How do you capture the other-- The result is-- so the question is, if exact cell returns-- it seems like it just returns standard output, right? Yeah. So what it returns is our custom object or execution. And there, you have access to standard output, error output, any runtime errors that are nicely parsed with traceback that you can feed back into LLM to fix itself.</p><p>And also, any rich output, like PDFs, charts, PNG, JPEG files. And we'll be just sending this to the front end where we can use it and present it to the user. So this run Python function, it's the function that we are importing on the left in our endpoint when we are calling the LLM where we implemented the stream text.</p><p>And we can finally-- we can-- we can finally call the run Python function inside our execute method. So there's really nothing, like, too special about it. One thing that we need to do, though, is that we need to use that user ID. So we also need to get user IDs from somewhere.</p><p>We will be just sending this from the front end. So let's say in a real application, we probably have, like, an authenticated session with the user ID. In this case, we'll just send it from the front end. I will add it in a minute. And that user ID, we pass through that run Python method.</p><p>And once we have results, we want to return the results from the execute function. So when we call return here and we are returning an object, that's what we will get sent to the front end with LLM's answer. So that's, like, the result of the function call, our tool call.</p><p>OK. So just to make sure I don't forget anything-- yeah. So is that part clear, how we are importing the run Python function to our post-- the wrote file? And just we want to call that with the AI-generated code and then get the results to the front end. So now we are getting to more interesting part.</p><p>And once we implement this, we will have, like, actually running it inside our app. And we can try the app again. And we get to what you just asked. And we can get different types of output from the results. So first is we want standard output. And the result object from our exec cell, we are just returning here, contains logs.</p><p>So, oops, error output. And it also contains two more things-- any runtime error. So that's an error that you would get when AI probably generated wrong code or code that doesn't work, we can catch that and we will return that in an error field, which is nicely structured, that you can use to fix itself.</p><p>And the last part is that we-- we get something-- I have a little bit unfortunate naming here-- called Cell Results, essentially, so let's name it Cell Results. And those are, like, evaluated-- it's an evaluated notebook cell that's inside a sandbox. And those can be those charts, images, PDF files, HTML, JSON.</p><p>It can be just, like, text. So it's usually the last line of a Jupyter cell in a Jupyter cell-- in a Jupyter notebook. And also any time you call, like, a display data, like, you want to display a chart or anything like that, those-- that will be in a cell results, which is, like, an array of results.</p><p>And based on a type of a result, you can then parse it. And all these four objects, we just return it from the execute function, which will get us to-- which will get it to the front end. Any questions for this part? Mm-hmm. Go ahead. Can you speak to, like, Python version packages and any of that stuff?</p><p>Yeah. So the question is about Python version packages and everything, probably, inside a sandbox and the environment. For the Code Interpreter SDK-- so I mentioned that the Code Interpreter SDK is wrapping a sandbox. And that sandbox can be completely predefined by you. In our case, when we created a Code Interpreter SDK, we installed a bunch of packages that you would probably usually use in, like, a data science use case or AI data analysis use case, like NumPy, Pandas, Seaborn is installed there.</p><p>And so that's, like, something that gets you going out of the box. And there's also Python 3.10 installed in the environment. You can also create a completely custom sandbox if you want to. If you just give us a Docker file, and inside the Docker file, you can install whatever package you want, but you just-- you will be just using a little bit lower level API that we have.</p><p>It's not scary. It's just, like, you need to set up everything yourself. So usually you start with the Code Interpreter SDK, and then once you find out, hey, I don't need these packages or I need different, you can install whatever you want. Question there? I was curious on your choice of the Jupyter frame.</p><p>You mentioned that it was your plans, is there any other rational ? So, here we really went just with an optimized-- sorry-- the question was how we went about choosing the Jupyter frame. the Jupyter framework inside the sandbox, if there's any more to it than what I mentioned. So, the main decision was how we should set up the sandbox that out of the box, it's working for you when you plug it to an LLM and you don't get much, like, errors.</p><p>Because that's what users come to us, especially if they are a little bit less experienced with LLMs and they are wondering why it's not working. So, we were really optimizing on how most likely will AI generated Python code or code will look like. And the answer is, it's going to be Python code and it's going to be like a Jupyter-- Jupyter notebook code with preinstalled packages.</p><p>It's essentially like what you-- GPT-4 would expect. and that means what their code interpreter tool would have installed. Yeah? Are you able to expose the Jupyter environment as a notebook to use that? So, the question is, if you are able to expose the Jupyter notebook inside a sandbox. Yes, you can.</p><p>It's-- so-- oh, sorry. So, that's not, like, documented anywhere, but you actually can set up exactly this because we have a few customers that want to implement something like this. The answer is just text us on Discord or just send us an email and we will give you a guide.</p><p>We have, like, few experimental PRs like that, for example, we have a sandbox with full working graphical interface, where you can start apps, like, game-- so, it's-- everything is Linux, but you can pretty much start anything you want there. A lot of people have been using it for-- or have been using it for evals, when you want the LLM control, something more graphical.</p><p>Or if you want to build something that's more like a tandem human in the loop, you can do that as well. So, just to sum it up, we implemented the post request and execute-- sorry, the run Python tool. That's-- finally, now, we should be able to run the code on the front end.</p><p>So, one last thing we need to do is that we need to go to our page component, page.tsx. And there, we need to create this, like, a dummy user ID. This would be-- because we are now passing it on the front end, in your real app, you actually had-- you would actually have a user ID.</p><p>And in the use chat hook, that's coming from the AI SDK, we just passed the user ID to body. And let's add a console log here to the messages, so we can check if everything is working correctly. And if I go back to my app... Open my Chrome DevTools, and I see something like simple, like, print hello world.</p><p>I can see I'm getting the messages. There's a new UI that wasn't there before. And if I check the messages-- the reason there's so many messages is because, like, it's constantly streaming then, so it's updating the message-- messages object. And so one message-- first message is my prompt, print hello world.</p><p>And actually, can I zoom this in? Yeah. First message is print hello world, and the second message is from the Assistant, which has the answer, and there's also something interesting called tool invocations, which is an array of all the tool invocations that the Assistant or LLM decided to do.</p><p>And one of them-- the only one there is the run Python that we defined. It has two important fields, args and result. So args, that's what we described when we were defining that tool, that run Python tool. So this is the code. That's what we asked it. And the result, that's what we returned from that execute method inside that post function or execute function.</p><p>And there we have cell results, like an empty array, because really-- so cell results doesn't capture any standard output. It just evaluates the Python code, and there was nothing to evaluate, like, as a side effect of print, it's just nothing. No error output, but in the standard output, we have hello world.</p><p>And that's what we wanted. So now if we ask it to actually-- for actually something more complex, like calculate by using Monte Carlo method and visualize it. It will take a little bit time. Now it's writing the code, which unfortunately isn't streamed at the moment, because the tool streaming isn't implemented there in the AR SDK.</p><p>But if we check it out, we have a tool invocation here, which is our run Python. It has the whole code here. It's taking 1,000 or 10,000-- 10,000-- 100,000 iterations. So it will actually most likely take a little bit of time to finish this. And if we-- yeah, and we got two new messages, because we got a result from the-- from the run Python code.</p><p>And if we check it again, there's a tool invocation. We have result. And, yeah, we have standard output. But the interesting part is that in cell results, we now have a-- one of the results which has PNG and text fields. So PNG is a base 64 image from the cell.</p><p>And that's the visualization of the Monte Carlo simulation. And the text is just like a human-friendly description of the chart. And now what we will work now on in the next step is displaying this on the front end, on the right side, as in the artifact. No, it's-- it's here.</p><p>It's already finished. If you check out the cell results, you see the PNG. And that's what we asked it. The reason we don't see it anywhere here on-- inside the app is because we haven't implemented the front end yet. That's what we are going to do now. But we already have results from the code interpreter sandbox in our messages variable.</p><p>So the question is, where did I get the messages from? This is code on the front end. If you go to page.tsx. This line, the use chat line, is returning all the messages from the LLM, including tool calls. Thank you. Yep. Any questions? So I would say we are pretty near the-- the end now.</p><p>We need to just display it. And for that, we need to parse the tool invocation from the-- from the message. Latest message with-- we need to get the latest message with the tool invocation. So I will, like, for the sake of simplicity, uh, I will just do something a little bit naive here.</p><p>And that is, uh, I will just care about the latest message we get from the LLM. And from the latest message, I will just parse the tool invocations that was there. So first we get the latest message that has tool invocation. And, uh, this is a little bit annoying, but first we need to reverse the array in JavaScript, um, and then we search in this for message that had tool invocation field.</p><p>And that field wasn't empty, uh, so-- that array wasn't empty. So, uh, the length will be bigger than zero. So this can or might or might not exist, uh, this latest message, uh, with tool invocation. It will not exist, uh, in the first few seconds once we, uh, give an LLM a prompt to actually do something and generate code.</p><p>And now we just want to extract the tool invocation field, uh, from our, uh, from our, uh, message with tool invocation and it, again, it can or might or might not exist. So both, uh, latest message with tool invocation can be message or undefined and the tool invocation can be a tool invocation or undefined.</p><p>So we, so far we just added these, these two, uh, or like two, uh, function calls or two calls. And we are working with messages that were sent from the back end. And that's the part that we implemented, uh, up until now. The reason we are doing this is that we will be sending the tool invocation to the component, uh, that, uh, I will add in a sec.</p><p>And that component will be showing the chart that, uh, and any output from the running the code, uh, I, I trained it to code. Any questions? Any questions? Okay. Once we have a tool invocation, um, we want to, uh, display it. And for that I have, uh, a premade component called side view, uh, that exists in components and side view file.</p><p>And we, so, so we can just, uh, import it from there. And we want to render it right next to the chat, um, component. So, and this side view takes the tool invocation. That's why we did what we did, uh, with those messages here. So, I imported the side view here.</p><p>Uh, it's a component that's, uh, already exists inside a, inside a project. And I'm now rendering the side view, uh, next to the chat, uh, component that we already have there. And if you check out the app, uh, when you, when we ask, when we ask the Claude to print something, oops, yeah, it's working now.</p><p>It should, oops, it's not displaying. It should display the side view, but for some reason it's not, uh, showing now. And that might be, I forgot something to implement inside a side view. Um, do you need more time to add the side view, uh, to this file? Or I can go to the side view.</p><p>Okay. Oh, yeah. Uh, the side view has, uh, uh, one thing that I forgot. And that it also expects, uh, a field called data. And what that, uh, field does is that we want something that shows us that, um, the tool, that the LLM is using a tool and we are waiting for it.</p><p>And we want to know about it on the front end. So we want to send some additional data with the LLM's response from our server. Uh, from, from our endpoint. Um, and unless the, the, the, the idea is just simple. We want to show some kind of loader. And, uh, uh, I want to show you how you can, uh, know about what is happening with the tool execution more than just like waiting for a new message.</p><p>So for that, uh, if we go to back to route file. And there we have imported, uh, object called stream data. Uh, stream data is a helper object from the AI SDK. And that will help us to stream any, uh, arbitrary data back to the front end. Alongside our messages, uh, from the LLM.</p><p>So we will just, here, uh, below, uh, parsing the request. We will just create a, uh, data objects called stream data. Oops. And it's a stream. And we can append, uh, objects into that stream. So what we want to do is when we are calling execute. Uh, or when, when the function, uh, the tool gets called.</p><p>We want to first say, hey, the, uh, tool run Python is running. Oops. It's not update. It's append. And once we get what we need, we also want to update the stream with, uh, basically the same. Just the status will be, uh, complete. And that's what I arbitrary picked.</p><p>You can pick whatever you really want. And it's just like the, how the site view, uh, component is implemented, uh, uh, inside our front end. But the idea is that, like, now you can use the stream data to send, uh, arbitrary data from the backend to the front end and stream it, which is an important part.</p><p>What is just missing now that we need to include that stream data object to our response from our, uh, API endpoint. And for that, we need to just, uh, do two things. First, we need to close that stream data, uh, stream. Uh, because it's a stream, so it's currently open when we created it.</p><p>And we also need to change the, uh, uh, return here to include the data that we, uh, the stream, additional stream. So we kinda need to create two streams that we are, uh, in a, in a, combine them in a single stream and send it to the front end.</p><p>And, uh, for that, what we are going to do is, uh, first the result from the LLM call, uh, which is this result from the stream text, can be converted to the stream, to a stream. Uh, so result to AI stream. And there, uh, we can do one thing and that's, uh, we can pass it a callback called, uh, on final, which is once everything is done, we want to close our, uh, data stream.</p><p>Okay. Um, so this, uh, what this does is that it converts, uh, a result from the LLM that we are using with, uh, we get from stream text to a stream. And, uh, once LLM is finished with generating all the, uh, all the responses, uh, that's what this callback on file tells us.</p><p>We want to close our data stream that we created, uh, a few months ago. And the last missing part is just now we want to, uh, we want to return both the stream and the data. So, uh, for that, uh, there's a object called streaming text response from the AR SDK.</p><p>It's already imported. And if we call it, uh, you, you can pass it three, uh, should pass it three parameters. First is the stream, which is the response from the AI. Second is the initial data. We don't want any. And the last is any, uh, uh, arbitrary data that we want to send along the stream.</p><p>Okay. Any questions? What check time are you on? Uh, this should be stage, uh, uh, workshop dash, uh, uh, four. So actually on the page.tsx, what we need to do is we need to, uh, get the data object from the used chat hook. Uh, so that's what we are going to add now.</p><p>Uh, sorry? The question is if we have modified site view, uh, component. We haven't yet. Uh, we just, uh, are rendering site view component here. But it needs additional data about the information if the, uh, code is, uh, still running, uh, uh, the AI generated code. And that's what we are adding now.</p><p>And once we add that, it should, it should display. Uh, so if we, uh, one last part for this data stream, uh, is that we, uh, need to extract the data from the, that we are sending from the backend, and get it on the frontend. So use chat, again, from the AI SDK is pretty handy here, because, uh, uh, we can just add, it just returns another field called data, which is exactly what we are sending.</p><p>Uh, we can even print it. And, uh, all we need to do is, once we get the data variable, we just pass it as a prop to our site view. And that should be, uh, it. And now if you go to the app, again, and ask it anything that would result in generating code, uh, it's a little bit, uh, Sonnet is a little bit slow.</p><p>And now we have the side view rendered. And it's currently just showing code. Uh, but what we will add as the last part of this workshop is the preview. And there will be rendering, uh, any charts returned from the code interpreter and any standard output and, uh, error output.</p><p>Uh, the question is what's special about anthropics, uh, cloud, uh, model here. Uh, you could use any model you want. Uh, it's just the new Sonnet model is really good with code generation. And so it usually, uh, is capable of one shot, same, like, relatively, I would say, like, advanced examples.</p><p>Not like full projects, but it can give you interesting results. Yeah, question. You mentioned that all of your run times are Linux at the moment. And I wondered, uh, I think that there were some security reasons that that was the case. And I wonder if you could expand on that a little bit.</p><p>Yeah. The question is, uh, that all the sandbox run times are Linux. Uh, and what are the reasons for that, uh, and probably if, if those are security reasons. So yeah, that's correct. All those run times are, are Linux. And so there are two parts, uh, of the, of the question and the answer.</p><p>So, uh, the sandbox is a regular VM. It's not a, it's not a container. Um, and that VM is running on our server, on a host machine. And so both host machine and the sandbox operating system is Linux. Uh, the reason for that is that we are using something called Firecracker, which is an open source, uh, VM from AWS.</p><p>And they are using it for running, uh, AWS Lambda. So it's really battle tested with running, uh, untrusted code. And Firecracker is very specific in a way that it requires, uh, Linux with specific kernel for security reasons. And inside the Firecracker, inside the VM, you really can't run anything else at the moment than, than Linux.</p><p>Yeah, sir. Uh, go ahead. So you mentioned the dependency law of anything that's defaulting sandbox, right? So how hard would it be in this period to read the imports and customize the sandbox? Uh, it's basically just your custom Docker file. So the question is, um, how hard would it be to customize the sandbox, right?</p><p>Uh, with install, uh, custom dependencies. Uh, you would just give, uh, give us, uh, your Docker file. And inside the Docker file, you would have something like pip install, whatever you want. Or you can have even NPM, uh, if you want to build a little bit more communication on top, you can, we have users running, uh, sorry, uh, Fortran.</p><p>So, uh, you can really run anything you want, uh, depends on how much, uh, customizability you want. Uh, I can show it after, after the call, but if you go, uh, sorry, after the, after the, uh, workshop, if you go to our documentation, e2b.dev/docs, we have a guide there for customization, uh, uh, specifically for the code interpreter SDK.</p><p>Yeah. So the question is that, uh, if we are using just firecracker or firecracker and containers together, uh, I think like the confusion comes from the Docker file I mentioned. Yeah. So, uh, we are using just firecracker, uh, the containers and Docker file, uh, we are using that only for like a simple way to define your firecracker environment, essentially.</p><p>So, what we do on the background, uh, once you give us your Docker file to customize the sandbox, is, uh, we start it as a container, extract the file system, um, and then convert this to, uh, to filecracker VM. And, um, then when you start your sandbox, you are essentially starting your custom container, uh, programmatically.</p><p>Yeah. Question there. What's, like, the starting time in customizability? Uh, what's the static, uh, what's the timing? Yeah, like, to start the sandbox? Oh, yeah. Um, so how much time does it take to start the send, uh, customizability? It takes the same time as, uh, pre-made sandbox. Uh, so, at the moment, it should be around 900 milliseconds, uh, every time you call that, uh, sandbox.create that I wrote there, uh, in SDK.</p><p>And, uh, soon it should be around 400 milliseconds. Okay. Now we are getting to the last part, uh, of this workshop. And that's in case you want to just, uh, check out, uh, get check out to the last part. It's, uh, workflow dash five. And we will go into the site view.</p><p>And what we want to do, we want to implement this preview. Which, uh, that's, uh, so that's essentially the result of the code execution that we, uh, get from the, from the LLM. Um, let me close a few files first. So, uh, so, uh, we really care about the site view component.</p><p>Uh, uh, the only thing we care, uh, for now. And if we open the site view component, uh, it's like scaffold and predefined. Uh, the, uh, thing that's missing there is that we have, uh, artifact, uh, sorry. We have a artifact view, uh, component that's also already exists. That's currently commented out.</p><p>So, that we need to uncomment inside the site view. And inside, uh, let me turn that on left, uh, put it on left. And inside the artifact view, that's on the right now. Um, if you go to artifact view function, which is the component, we have a bunch of to-do's.</p><p>So, let's, uh, focus now on one main, uh, to-do and, uh, item. And that's render image. And so, let's just optimize now for the use case where we want to LLM to render a chart or any image. Uh, any image that we, uh, that we want. And we want to display that image here.</p><p>So, what artifact view gets through props, uh, propagation is, uh, the result of our code execution. That's the result here. Uh, that's what we send from the, from the backend. Uh, that's this file. Um, and we want to access the cell results. Because cell results, uh, has, uh, this object has all the PNG files, JPEG files, HTML, PDF, everything around that.</p><p>Um, so, uh, you can see that we are already doing the parsing here. Uh, when we are extracting the cell results, standard output, error output, and runtime error from a result object. And if there are any cell results, uh, we just expect there will be PNG. Uh, in a real app, you probably would need to check that, uh, a little bit more thoroughly.</p><p>And if we have a PNG file, we'll just, uh, render image. And that's basically it. So, uh, uh, I will just add a little bit more styling here. Um, we will use next.js image component. And because the image, uh, the image is PNG image is in base 64, uh, that's exactly what we are going to use here.</p><p>And the width and the size will be like 600, 400. It's really arbitrary for now. And, uh, we just want to put this image in a diff container. So it's centered. And make it a little bit nicer with padding. And render our logs there. The logs output is something that's already prepared here in this file.</p><p>So, uh, once we add this, um, when we, we should see an image, uh, inside our app, if we ask for something like that Monte Carlo, or just like, uh, generate or create a random chart, like a 3D chart. Uh, here it is. So this came straight from the sandbox, uh, from the, uh, Jupyter server running inside the sandbox.</p><p>Uh, the next step would be that we won't be able to cover in this, uh, in this workshop. Uh, but next step would be how to make these charts interactive. Uh, because now it's just an image generated from the, from the sandbox. One of the ways to do that, uh, I'm going to go back to code.</p><p>Um, uh, one of the ways to do that would be, uh, use the sandbox not for generating images, but to operate on top of charts, uh, sorry, on top of CSV files on your dataset. So what you can do is that you can upload files to the sandbox. You can connect a cloud storage to a sandbox that you already, or you already have a bunch of files.</p><p>And, uh, you can just let AI know at the LM know about how these files look like, like CSV files, what columns are there. And just ask it to questions about this dataset. And it was usual like Sonnet 3.5 is very capable of this already. It will start generating, um, code that's capable of extracting data from your CSV file, send the data, run that code inside the sandbox on top of your CSV file, send it to a front end and on the front end generate, uh, or display this data with something like, uh, char.js podly, uh, any library, library you want.</p><p>Yeah. The question? How table previews are those? Uh, uh, sorry, can you repeat that? Does core interpreter have table previews, you can display previews? Uh, if we can display table previews, uh, previews. Uh, so the, yeah, the question is if we can display table previews, yes. So anything you could display besides, like, interactive widgets, widgets, uh, inside, uh, Jupyter's, uh, notebook, uh, you will get in this result object or cell results object.</p><p>Uh, uh, uh, and it can be data frames, for example. Uh, uh, the question really is, uh, it depends, uh, sorry, the, the, the question is if the table preview is an image or, uh, something or just like a description of the table. Uh, the answer is, it depends.</p><p>It depends on what you tell the LLM to do. So if you ask the LLM to, um, use the right libraries, uh, that when are evaluated return, for example, HTML, you will get HTML on front end and you can just display it. Um, so, yeah, it depends on how the AI generated code looks like and what kind of libraries you are using and dependencies you are using inside the code interpreter.</p><p>Yeah, uh, question here. Do you want to say anything more about security architecture? I mean, you've given us, I guess it's really simple, but it's just executing in a sandbox, but do you want to say more about it? Yeah, so the question is, uh, if I can go a little bit deeper into the security of the sandbox.</p><p>Um, so, uh, firecracker really does very heavy lifting here. And, uh, the way it works, uh, and our infrastructure works is that you start a sandbox. We, uh, have a small VM, uh, prepared for you. Uh, and that VM is the firecracker VM. Uh, and the VM is designed in a way that when you try to get outside of the VM, it just restarts.</p><p>Uh, so we are actually giving you full root access. You can run any comment you want. And when you, unless there's like a network, um, hole somewhere where you could just connect to a third party service, uh, when you try to something like exit or system rest or anything like that, the, the, the machine will just restart and will just start the new sandbox for you.</p><p>Uh, on top of that, that, uh, firecracker is wrapped inside the jailer. So, which, uh, removes ability to run certain syscalls, which, uh, usually you shouldn't really care about. Everything just works, uh, as, as you would expect. And, uh, the way firecracker, uh, works for the, like, high level security is that it's using Linux's and, uh, kernel's KVM, uh, for virtualization.</p><p>So you are, there's a little bit more overhead instead of containers because it's a full VM. It's not just a process. And so every, like, user session is a separate VM. So, uh, that is isolated and, uh, can't, uh, really doesn't know about other VMs or sandboxes, uh, inside the network.</p><p>So it's out of the books multi-tenant environment. If you look, uh, at the sandbox as a single tenant, uh, it's a multi-tenant environment, uh, if you look at a sandbox as one of the tenants. Yeah. Question. Do you save the data, do you log that in your system? Uh, the question is if we save any data that you sent to the, uh, to the code interpreter sandbox or to the sandbox?</p><p>Uh, the answer is no. Uh, so at the moment, when you start the sandbox, um, and you kill the sandbox at some point, or it just closes by itself, uh, everything is destroyed. Uh, and what we log is only that you did certain operations. So you, for example, like, uh, created a file, but we don't save, like, content of the file.</p><p>We just want to, like, know about what is happening inside, uh, inside the VM. And, um, what you can do for the persistence, uh, is you can connect your cloud storage to the VM and save the data there. Yeah. Yes. Uh, can you, can you, sorry, repeat a little bit longer?</p><p>Yeah, so the question is, uh, that I mentioned docker files for customizing the sandboxes, the VMs. Uh, uh, yeah, so, um, uh, actually, uh, I can show it to you here. If you go to e2b.dev/docs, what you can do, uh, there is we have a way to customize the code interpreter.</p><p>And the way we do it, um, is through a docker file. And, uh, it's a regular docker file where you can use most of the docker file things, uh, and, and keywords. Uh, it just needs to be Ubuntu based. And uh, basically, uh, what the docker file is, uh, good for is that you can define the, like, a file system of the VM, the environment of the VM.</p><p>And, uh, that can be, like, you can install any packages, you can, uh, define any environment variables, uh, save any files or pre-save any files. So we have users save, like, a scaffolded Next.js app that just the LLM can send render, uh, uh, generated components. And they, then the LLM, uh, the Next.js app is just running inside the, inside the VM.</p><p>And so the docker file really is just like a means of letting us know how you want the environment to look like. Um, and then we just convert the docker file, actually that container to a, a VM. Does that answer the a question. Here's a question. Is the conversion from Dockerfile to firetack.com, is it a standard process or is it a standard process or is it a standard process or is it a standard process?</p><p>The question is, how does it look like that conversion from a container to a sandbox, right, like that file system? It's actually much simpler than it sounds. So what we really do is just, you know, like, you know how you have different types of file systems in your computer?</p><p>I don't think that many people touch it nowadays, but back in the days if you are booting your machine, you can pick how your file system would look like. That's what essentially we do. So we take, we run the container and on our infrastructure there's literally a shell command that copies the whole file system to an outside file outside from the container.</p><p>And we just convert that file to that correct format. And that format is supported as a root file system for the firecracker. So it really is just a bunch of files together that we extract from the container. And it's very similar to how the container gets created. So if you want, you could theoretically, you know, images have this bunch of tar files and layers.</p><p>You could extract those layers from the image. And you wouldn't probably even, most likely didn't need to run the container. And you could just take the image and convert it to the file system for the VM. Yeah, it's in the code base. So we are fully open source. We have a bunch of repositories here.</p><p>There's a repository called infra, which is a little bit less known. It's like one disclosure is it's not that now very friendly to get it running on your own. That's the next thing we are working on. And here, if you take a look into, especially scripts, you will find a script for conversion of the Docker file to the sandbox, the VM system.</p><p>There was one question first. Yeah, how do you end up registering your own custom modules inside the sandbox? Yeah, so the question is how do we end up registering custom modules inside the sandbox? Very similarly, like what I talked about it a few times, when you are creating a -- you can create a Docker file that describes how the sandbox will look like.</p><p>We convert the Docker file to a your custom sandbox. And then we start a sandbox -- you can start a sandbox using the SDKs. Because the Docker file creation happens with your Docker instance. We are not doing it on our cloud. You can use private packages or private images or whatever you really want.</p><p>Yes, question. So I was looking at this report earlier, and it says, I think at the bottom, that GCP is kind of the only platform that's supported currently. Do you have plans to support others in the future? The question is if we have plans for supporting other clouds than GCP.</p><p>Yes, strong yes. So AWS is actually where we are even migrating with our cloud version. We want to support AWS as the first thing. Then we will have a proper tutorial for GCP. And the next one is supporting any Linux machine. So custom on-prem clouds. And just as a follow-up.</p><p>So is your business model eventually -- so, you know, instead of firms trying to build this capability themselves today, there aren't any options for this. You want to sort of sell this as open source but support it as well? Is that where the revenue would come from? So the question is about business model.</p><p>Yes, so two parts. It's pretty similar or pretty much any open core business model. So most features are open source. If you can self-host it on your own and manage it on your own, or you can use our cloud. There will be a few features more targeted for enterprises.</p><p>Especially around, like, we have -- we got questions about how do you load, like, terabytes of data into a sandbox. That's not something that you encounter during your weekend hacking or something like that. And then we are also planning a lot of work on top of observability. Because, like, as the LLMs are getting better, you will want to know what is happening inside the sandbox.</p><p>Because it will not be just, like, running a simple Python script. It will be more like a workspace. Like a permanent workspace for your agent. And your AI app. And you kind of want to know what is happening inside the sandbox. Like, are any files getting created? Is any network requests happening?</p><p>You want to have a programmatic access to this information. And you want to be able to, like, stop the sandbox before it actually happens if you don't like it. So those are, like, some of the things that we are working on next. And might -- some of them might stay, like, private or behind the license.</p><p>This summer. This summer. Yes, question there. So the question is, if we have any GPU sandboxes? Correct? Not at the moment. So that's very intentional. Because we kind of don't want to go into, like, a whole GPU provider business. So -- and even, like, now, usually, if you have -- if you are serious about GPU work, you want to offload it to someone who does it really, really well.</p><p>And there's, like, a lot of players in the space. So, yeah. The short answer is no. The long answer, it makes sense for the future, but not something that we are focused on right now. Yeah. Question there. Yeah. Can you say a bit more about connecting these cloud data schools to the sandbox?</p><p>So this cloud what? You mentioned that you can connect cloud data schools. Oh, yeah. So the question is how you can connect something like S3 or good cloud storage. What kinds of schools can you connect? What kinds do you recommend? Yeah. So I will show you -- we have a guide for this in our documentation.</p><p>So currently, you can connect anything that looks like S3 or, like, has S3 API. Mainly, that's Google Cloud Storage. That's Amazon S3, obviously, or AWS. And Cloudflare's R2. There's a little bit more complicated setup. In the future version of the SDK, it will be just, like, a simple call that you say, hey, I want to mount this endpoint.</p><p>And you need to basically -- what we are doing is we are using something called Fuse Protocol for that, which allows you to connect your cloud storage to a file system. And then it sort of looks like, basically, that it's part of your file system. But actually, when -- every time you are reading or writing to a file inside a cloud storage, you are making a network request.</p><p>But the nice thing about that, especially if you are, like, more, like, enterprise customer and you have -- you care about your users' data is that the data doesn't really leave your storage. Yeah. Question? So, I understand you are using games for the file record. Is there a reason you are not planning on using containers instead?</p><p>So, the question is, if there is any reason, or what is the reason we are not using containers? I have a separate presentation on that, but -- and the short answer is security. So, we -- when customers come to us, they come to us 90% -- it's an in-house solution that's either serverless function, like a Lambda function, or they are managing a fleet of containers or something like that, with Kubernetes.</p><p>-- and you can sort of make containers secure. It just takes -- it just needs more work. And you then want things like -- like running Docker in Docker -- it's hard to run, for example, Docker, inside Docker. Like, it's doable, but it's just, like, more pain. And so, ergonomics and security is, like, part of the answer, where we eventually figure out, hey, like, if you make an ability to get you a VM really, really fast, you will just get full computer for your -- for LLM, which is, like, a nice win.</p><p>And there's nothing you need to do special. It's just, like, a Linux machine. The second -- the second part of this answer is something we don't have implemented yet, but we think will be super important, and that's snapshots. With Farquaker and a few other VM projects out there, you can very easily make a snapshot of the whole VM at its current state, and not only file system, but also memory.</p><p>And you can do it pretty fast. It takes, again, I think, like, 80 milliseconds or something like that. And we think it will be super important in the future, especially once LMs get more capable and cheaper. You will want to go into sort of this tree-search problem where you can let many agents, versions of your, like, AI app, explore the whole space.</p><p>And every time -- like, if you imagine, like, a graph of a tree, every node would be like a snapshot of the VM, and you can come back to it and load it again. So you can -- you bring sort of this -- -- a little bit of determinism into a non-deterministic system, because you can just save it at any point, and come back to it.</p><p>And you can also save it and prevent your agent from doing anything you don't like. And just one last note is that something like this is sort of possible with Docker containers. It's just, like, it -- the technology isn't really finished. It's, like, half working, half not working. It's called CRIU, C-R-I-U.</p><p>If anyone has any last question, now is the time. If not, thank you for following me and coding along. And if you want to ask any question, like, one-to-one to me, feel free to catch me in the halfway. Or send us a message on Discord, or just email me.</p><p>It's on E2B. It's vasek@etub.dev. Thank you. Thank you. you</p></div></div></body></html>