<html><head><title>Jeremy Howard: Very Fast Training of Neural Networks | AI Podcast Clips</title></head><body><a href="index.html">back to index</a><h2>Jeremy Howard: Very Fast Training of Neural Networks | AI Podcast Clips</h2><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs"><img src="https://i.ytimg.com/vi_webp/LqGTFqPEXWs/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=20">0:20</a> Super Convergence<br><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=45">0:45</a> Why is this important<br><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=96">1:36</a> Why the paper wasnt published<br><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=193">3:13</a> The future of learning rates<br><br><div style="text-align: left;"><a href="./LqGTFqPEXWs.html">Whisper Transcript</a> | <a href="./transcript_LqGTFqPEXWs.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=0">00:00:00.000</a></span> | <span class="t">There's some magic on learning rate that you played around with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=4">00:00:04.800</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=5">00:00:05.800</a></span> | <span class="t">It's quite interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=6">00:00:06.800</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=7">00:00:07.800</a></span> | <span class="t">So this is all work that came from a guy called Leslie Smith.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=8">00:00:08.800</a></span> | <span class="t">Leslie's a researcher who, like us, cares a lot about just the practicalities of training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=17">00:00:17.720</a></span> | <span class="t">neural networks quickly and accurately, which you would think is what everybody should care</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=21">00:00:21.800</a></span> | <span class="t">about, but almost nobody does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=25">00:00:25.280</a></span> | <span class="t">And he discovered something very interesting, which he calls superconvergence, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=30">00:00:30.160</a></span> | <span class="t">there are certain networks that with certain settings of high parameters could suddenly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=34">00:00:34.320</a></span> | <span class="t">be trained 10 times faster by using a 10 times higher learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=39">00:00:39.640</a></span> | <span class="t">Now no one published that paper because it's not an area of kind of active research in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=49">00:00:49.680</a></span> | <span class="t">the academic world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=50">00:00:50.680</a></span> | <span class="t">No academics recognize this is important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=53">00:00:53.020</a></span> | <span class="t">And also deep learning in academia is not considered a experimental science.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=60">00:01:00.140</a></span> | <span class="t">So unlike in physics where you could say like, I just saw a subatomic particle do something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=65">00:01:05.380</a></span> | <span class="t">which the theory doesn't explain, you could publish that without an explanation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=70">00:01:10.660</a></span> | <span class="t">And then in the next 60 years people can try to work out how to explain it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=74">00:01:14.320</a></span> | <span class="t">We don't allow this in the deep learning world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=76">00:01:16.400</a></span> | <span class="t">So it's literally impossible for Leslie to publish a paper that says, I've just seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=82">00:01:22.340</a></span> | <span class="t">something amazing happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=83">00:01:23.760</a></span> | <span class="t">This thing trained 10 times faster than it should have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=85">00:01:25.760</a></span> | <span class="t">I don't know why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=87">00:01:27.680</a></span> | <span class="t">And so the reviewers were like, well, you can't publish that because you don't know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=90">00:01:30.160</a></span> | <span class="t">why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=91">00:01:31.160</a></span> | <span class="t">So anyway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=92">00:01:32.160</a></span> | <span class="t">That's important to pause on because there's so many discoveries that would need to start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=95">00:01:35.660</a></span> | <span class="t">like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=96">00:01:36.660</a></span> | <span class="t">Every other scientific field I know of works that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=99">00:01:39.320</a></span> | <span class="t">I don't know why ours is uniquely disinterested in publishing unexplained experimental results,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=107">00:01:47.980</a></span> | <span class="t">but there it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=108">00:01:48.980</a></span> | <span class="t">So it wasn't published.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=111">00:01:51.480</a></span> | <span class="t">Having said that, I read a lot more unpublished papers than published papers because that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=117">00:01:57.280</a></span> | <span class="t">where you find the interesting insights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=120">00:02:00.280</a></span> | <span class="t">So I absolutely read this paper and I was just like, this is astonishingly mind-blowing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=128">00:02:08.080</a></span> | <span class="t">and weird and awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=130">00:02:10.040</a></span> | <span class="t">And like, why isn't everybody only talking about this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=132">00:02:12.720</a></span> | <span class="t">Because like, if you can train these things 10 times faster, they also generalize better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=136">00:02:16.800</a></span> | <span class="t">because you're doing less epochs, which means you look at the data less, you get better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=140">00:02:20.560</a></span> | <span class="t">accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=142">00:02:22.560</a></span> | <span class="t">So I've been kind of studying that ever since.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=145">00:02:25.040</a></span> | <span class="t">And eventually Leslie kind of figured out a lot of how to get this done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=150">00:02:30.440</a></span> | <span class="t">And we added minor tweaks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=152">00:02:32.480</a></span> | <span class="t">And a big part of the trick is starting at a very low learning rate, very gradually increasing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=157">00:02:37.760</a></span> | <span class="t">it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=158">00:02:38.760</a></span> | <span class="t">So as you're training your model, you would take very small steps at the start and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=162">00:02:42.400</a></span> | <span class="t">gradually make them bigger and bigger until eventually you're taking much bigger steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=166">00:02:46.400</a></span> | <span class="t">than anybody thought was possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=169">00:02:49.600</a></span> | <span class="t">There's a few other little tricks to make it work, but basically we can reliably get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=174">00:02:54.240</a></span> | <span class="t">superconvergence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=175">00:02:55.240</a></span> | <span class="t">And so for the dawn bench thing, we were using just much higher learning rates than people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=181">00:03:01.120</a></span> | <span class="t">expected to work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=182">00:03:02.120</a></span> | <span class="t">What do you think the future of, I mean, it makes so much sense for that to be a critical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=186">00:03:06.020</a></span> | <span class="t">hyperparameter learning rate that you vary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=188">00:03:08.320</a></span> | <span class="t">What do you think the future of learning rate magic looks like?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=193">00:03:13.040</a></span> | <span class="t">Well, there's been a lot of great work in the last 12 months in this area.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=197">00:03:17.600</a></span> | <span class="t">And people are increasingly realizing that optimize, like we just have no idea really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=201">00:03:21.640</a></span> | <span class="t">how optimizers work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=203">00:03:23.720</a></span> | <span class="t">And the combination of weight decay, which is how we regularize optimizers and the learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=208">00:03:28.320</a></span> | <span class="t">rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=209">00:03:29.440</a></span> | <span class="t">And then other things like the epsilon we use in the atom optimizer, they all work together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=215">00:03:35.180</a></span> | <span class="t">in weird ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=216">00:03:36.920</a></span> | <span class="t">And different parts of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=218">00:03:38.800</a></span> | <span class="t">This is another thing we've done a lot of work on is research into how different parts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=222">00:03:42.520</a></span> | <span class="t">of the model should be trained at different rates in different ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=226">00:03:46.840</a></span> | <span class="t">So we do something we call discriminative learning rates, which is really important,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=230">00:03:50.320</a></span> | <span class="t">particularly for transfer learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=233">00:03:53.460</a></span> | <span class="t">So really, I think in the last 12 months, a lot of people have realized that all this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=236">00:03:56.520</a></span> | <span class="t">stuff is important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=237">00:03:57.620</a></span> | <span class="t">There's been a lot of great work coming out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=240">00:04:00.320</a></span> | <span class="t">And we're starting to see algorithms appear, which have very, very few dials, if any, that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=247">00:04:07.160</a></span> | <span class="t">you have to touch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=248">00:04:08.160</a></span> | <span class="t">So like, I think what's going to happen is the idea of a learning rate will, it almost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=251">00:04:11.480</a></span> | <span class="t">already has disappeared in the latest research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=254">00:04:14.660</a></span> | <span class="t">And instead, it's just like, you know, we know enough about how to interpret the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=262">00:04:22.800</a></span> | <span class="t">and the change of gradients we see to know how to set every parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=266">00:04:26.360</a></span> | <span class="t">[END]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=268">00:04:28.360</a></span> | <span class="t">1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=269">00:04:29.360</a></span> | <span class="t">Page 2 of 9</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=269">00:04:29.360</a></span> | <span class="t">Page 3 of 9</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=270">00:04:30.360</a></span> | <span class="t">Page 4 of 9</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=271">00:04:31.360</a></span> | <span class="t">Page 5 of 9</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=272">00:04:32.360</a></span> | <span class="t">Page 6 of 9</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=273">00:04:33.360</a></span> | <span class="t">Page 7 of 9</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=274">00:04:34.360</a></span> | <span class="t">Page 8 of 9</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=275">00:04:35.360</a></span> | <span class="t">Page 9 of 9</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=276">00:04:36.360</a></span> | <span class="t">Page 10 of 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=277">00:04:37.360</a></span> | <span class="t">Page 11 of 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=278">00:04:38.360</a></span> | <span class="t">Page 12 of 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=279">00:04:39.360</a></span> | <span class="t">Page 13 of 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=280">00:04:40.360</a></span> | <span class="t">Page 14 of 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=281">00:04:41.360</a></span> | <span class="t">Page 15 of 15</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=282">00:04:42.360</a></span> | <span class="t">Page 16 of 15</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=283">00:04:43.360</a></span> | <span class="t">Page 17 of 15</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=284">00:04:44.360</a></span> | <span class="t">Page 18 of 15</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LqGTFqPEXWs&t=285">00:04:45.360</a></span> | <span class="t">Page 19 of 15</span></div></div></body></html>