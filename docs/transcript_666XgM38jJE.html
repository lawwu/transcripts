<html><head><title>Scaling LLMs further is an artform - Demis Hassabis (Google DeepMind CEO)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Scaling LLMs further is an artform - Demis Hassabis (Google DeepMind CEO)</h2><a href="https://www.youtube.com/watch?v=666XgM38jJE" target="_blank"><img src="https://i.ytimg.com/vi_webp/666XgM38jJE/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=666XgM38jJE&t=0 target="_blank"">0:0</a> Scaling LLMs<br><a href="https://www.youtube.com/watch?v=666XgM38jJE&t=96 target="_blank"">1:36</a> Predicting training loss<br><a href="https://www.youtube.com/watch?v=666XgM38jJE&t=171 target="_blank"">2:51</a> Biggest surprise<br><h3>Transcript</h3><div class='max-width'><p>Going back to Gemini, I'm curious what the bottlenecks were in the development. Like, why not make it immediately one order of magnitude bigger? Well, look, first of all, there are practical limits. How much compute can you actually fit in one data center? And actually, you know, you're bumping up against very interesting distributed computing kind of challenges, right?</p><p>Fortunately, we have some of the best people in the world on those challenges. And, you know, cross data center training, all these kinds of things. Very interesting challenges, hardware challenges. And we have our TPUs and so on that we're building and designing all the time, as well as using GPUs.</p><p>And so there's all of that. And then you also have to, the scaling laws, you know, they don't just work by magic. You sort of, you still need to scale up the hyperparameters and various innovations are going in all the time with each new scale. It's not just about repeating the same recipe.</p><p>At each new scale, you have to adjust the recipe. And that's a bit of an art form in a way. And you have to sort of almost get new data points. If you try and extend your predictions, extrapolate them, say several orders of magnitude out, sometimes they don't hold anymore, right?</p><p>Because new capabilities, they can be step functions in terms of new capabilities. And some things hold and other things don't. So often you do need those intermediate data points actually to correct some of your hyperparameter optimization and other things. So the scaling law continues to be true. So there's sort of various practical limitations onto that.</p><p>So, you know, kind of one order of magnitude is about probably the maximum that you want to carry on. You want to sort of do between each era. - Oh, that's so fascinating. You know, in the GPT-4 technical report, they say that they were able to predict the training loss, you know, tens of thousands of times less compute than GPT-4.</p><p>They could see the curve. But at the point you're making is that the actual capabilities that loss implies may not be so clear. - Yeah, the downstream capabilities sometimes don't follow from the... You can often predict the core metrics like training loss or something like that. But then it doesn't actually translate into MMLU or math or some other actual capability that you care about.</p><p>They're not necessarily linear all the time. I think we've got to push scaling as hard as we can. And that's what we're doing here. And, you know, it's an empirical question whether that will hit an asymptote or a brick wall. And there are, you know, different people argue about that.</p><p>But actually, I think we should just test it. I think no one knows. But in the meantime, we should also double down on innovation and invention. And this is something that Google Research and DeepMind and Google Brain have, you know, we've pioneered many, many things over the last decade.</p><p>That's something that's our bread and butter. And, you know, you can think of half our effort is to do with scaling and half our effort is to do with inventing the next architectures, the next algorithms that will be needed. Knowing that you've got this scaled, larger and larger model coming along the lines.</p><p>- What's been the biggest surprise to you if you go back to yourself in 2010, when you were starting DeepMind in terms of what AI progress has looked like? Did you anticipate back then that it would in some large sense amount to spend, you know, dumping billions of dollars into these models?</p><p>Or did you have a different sense of what it would look like? - We thought that, and actually, you know, if you, I know you've interviewed my colleague, Shane, and he always thought that in terms of like compute curves and then maybe comparing roughly to like the brain and how many neurons and synapses there are very loosely.</p><p>But we're actually, interestingly, in that kind of regime now, roughly in the right order of magnitude of, you know, number of synapses in the brain and the sort of compute that we have. But I think more fundamentally, you know, we always thought that we bet on generality and learning, right?</p><p>So those were always at the core of any technique we would use. That's why we triangulated on reinforcement learning and search and deep learning, right? As three types of algorithms that would scale and would be very general and not require a lot of handcrafted human priors, which we thought was the sort of failure mode really of the efforts to build AI in the '90s, right?</p><p>Places like MIT where there were very, you know, logic-based systems, expert systems, you know, masses of hand-coded, handcrafted human information going into that turned out to be wrong or too rigid. So we wanted to move away from that. I think we spotted that trend early and, you know, and obviously we use games as our proving ground and we did very well with that.</p><p>You know, things like AlphaGo, I think, was a big moment for inspiring many others to think, "Oh, actually these systems are ready to scale." And then of course, with the advent of transformers invented by our colleagues at Google, you know, research and brain, that was then, you know, the type of deep learning that allowed us to ingest masses of amounts of information.</p><p>And that, of course, has really turbocharged where we are today. So I think that's all part of the same lineage. You know, we couldn't have predicted every twist and turn there, but I think the general direction we were going in</p></div></div></body></html>