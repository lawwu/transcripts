<html><head><title>The AI 'Genie' is Out + Humanoid Robotics Step Closer</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>The AI 'Genie' is Out + Humanoid Robotics Step Closer</h2><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8"><img src="https://i.ytimg.com/vi/gGKsfXkSXv8/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./gGKsfXkSXv8.html">Whisper Transcript</a> | <a href="./transcript_gGKsfXkSXv8.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">We've heard of text-to-speech, text-to-video, and text-to-action, but have we slept on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=6" target="_blank">00:00:06.720</a></span> | <span class="t">text-to-interaction? Let's take a look at the new Genie concept from Google DeepMind and set it in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=14" target="_blank">00:00:14.640</a></span> | <span class="t">the context of new developments regarding Sora and Gemini. And we'll hear what Demis Sarbis,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=21" target="_blank">00:00:21.200</a></span> | <span class="t">CEO of Google DeepMind, has to say about Sam Altman's $7 trillion dollar chip ambitions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=27" target="_blank">00:00:27.280</a></span> | <span class="t">and touch on some recent notorious missteps. But I do want to make a confession up front to all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=34" target="_blank">00:00:34.080</a></span> | <span class="t">you guys. The entire industry will not be shocked by this video. Everything might not change,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=40" target="_blank">00:00:40.160</a></span> | <span class="t">and the world may well not be stunned by what I have to say. If you're willing to forgive that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=45" target="_blank">00:00:45.280</a></span> | <span class="t">it should still be an interesting time. So let's get started. The TL;DR of Genie,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=50" target="_blank">00:00:50.640</a></span> | <span class="t">released in the last few days, is this. You can now hand a relatively small AI model an image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=56" target="_blank">00:00:56.880</a></span> | <span class="t">and it could be any image. A photo you've just taken on your phone, a sketch that your child or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=62" target="_blank">00:01:02.800</a></span> | <span class="t">you just drew, or an image, of course, that you generated using, say, Midjourney or Dali 3. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=68" target="_blank">00:01:08.560</a></span> | <span class="t">Genie, that small model, will take this image and make it interactive. A bit like handing you a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=73" target="_blank">00:01:13.680</a></span> | <span class="t">PlayStation or Xbox controller. You could then make the main character jump, go left, go right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=79" target="_blank">00:01:19.200</a></span> | <span class="t">and the scene will change around it. Essentially, you've made an image playable. Or in other words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=84" target="_blank">00:01:24.880</a></span> | <span class="t">you've made imaginary worlds interactive. This is how Google put it. Genie is capable of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=90" target="_blank">00:01:30.640</a></span> | <span class="t">converting a variety of different prompts into interactive, playable environments. These can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=96" target="_blank">00:01:36.480</a></span> | <span class="t">be easily created, stepped into, and explored. Now, before we get into the meat of the paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=102" target="_blank">00:01:42.160</a></span> | <span class="t">I want to let your imagination run wild. Because your mind, of course, went to the same place as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=108" target="_blank">00:01:48.000</a></span> | <span class="t">me, which is Imagine This integrated into Sora. How about controlling the shark or dolphin in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=114" target="_blank">00:01:54.480</a></span> | <span class="t">this paper craft world? Remember that the promise of this paper is that as you move left, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=120" target="_blank">00:02:00.000</a></span> | <span class="t">up, down, or make jumping motions, the world is crafted around you. This would be open world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=125" target="_blank">00:02:05.920</a></span> | <span class="t">exploration in its truest sense. Or take this example, again generated by Sora. In the near</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=131" target="_blank">00:02:11.440</a></span> | <span class="t">future, we needn't have two separate models either. It could be the same model generating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=136" target="_blank">00:02:16.240</a></span> | <span class="t">the world and allowing you to interact within it. And yes, I do still find it incredible that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=141" target="_blank">00:02:21.040</a></span> | <span class="t">this video was generated by Sora. And the characters you create could take almost any form.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=146" target="_blank">00:02:26.720</a></span> | <span class="t">How about a tortoise made of glass? Or maybe you want to control a translucent jellyfish</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=152" target="_blank">00:02:32.000</a></span> | <span class="t">floating through a post-apocalyptic cityscape. Or how about this example? Yes, it's nice to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=157" target="_blank">00:02:37.360</a></span> | <span class="t">watch the video, but imagine controlling it so it would be prompted by an image, say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=161" target="_blank">00:02:41.600</a></span> | <span class="t">of your hometown. And I can't help but point out the speed with which many of us are now becoming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=166" target="_blank">00:02:46.960</a></span> | <span class="t">accustomed to new announcements and how we're adjusting to them. OpenAI's Sora model has been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=172" target="_blank">00:02:52.160</a></span> | <span class="t">out for just over a week, and here's a paper where we can imagine it being interactive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=177" target="_blank">00:02:57.120</a></span> | <span class="t">But that's the way things are going. Modalities are multiplying. Models are unifying across text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=182" target="_blank">00:03:02.880</a></span> | <span class="t">audio, video, action, and interaction. In a moment, I'll touch on how this might affect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=187" target="_blank">00:03:07.680</a></span> | <span class="t">robotics, but here's audio coming to videos generated by Sora. This is thanks to Eleven</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=193" target="_blank">00:03:13.600</a></span> | <span class="t">Labs, and you can just feel how sound elevates the experience of video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=198" target="_blank">00:03:18.480</a></span> | <span class="t">All of this 30-second clip is AI-generated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=201" target="_blank">00:03:21.760</a></span> | <span class="t">[VIDEO]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=227" target="_blank">00:03:47.920</a></span> | <span class="t">And there's one key detail that I don't want you to miss from the genie paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=231" target="_blank">00:03:51.600</a></span> | <span class="t">The final version of Genie at 11 billion parameters was trained in an unsupervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=237" target="_blank">00:03:57.040</a></span> | <span class="t">manner from unlabeled internet videos. To simplify, they didn't pair up an image with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=241" target="_blank">00:04:01.680</a></span> | <span class="t">some controller movements or text and tell the model what happened next. There was no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=246" target="_blank">00:04:06.000</a></span> | <span class="t">such human supervision. It was just hundreds of thousands of internet videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=250" target="_blank">00:04:10.560</a></span> | <span class="t">And if you don't find that interesting, well, how about this? The results they got from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=254" target="_blank">00:04:14.800</a></span> | <span class="t">genie architecture scale gracefully, they say, with additional computational resources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=260" target="_blank">00:04:20.400</a></span> | <span class="t">If you want Sora levels of fidelity, rather than the pixelated stuff we got, just scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=265" target="_blank">00:04:25.840</a></span> | <span class="t">up the compute. Then, as the paper says, we will have generative interactive environments,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=271" target="_blank">00:04:31.120</a></span> | <span class="t">which is a new paradigm whereby interactive environments can be generated from a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=275" target="_blank">00:04:35.840</a></span> | <span class="t">text or image prompt. At this point, though, before we get carried away,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=279" target="_blank">00:04:39.360</a></span> | <span class="t">I want to inject some realism. Genie was trained on 10 frames per second video clips</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=285" target="_blank">00:04:45.440</a></span> | <span class="t">at 160 by 90 resolution. For the website, they scaled up to 360p. But still,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=291" target="_blank">00:04:51.520</a></span> | <span class="t">we are not yet that close to Sora levels of immersion and interaction. And I don't just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=297" target="_blank">00:04:57.360</a></span> | <span class="t">mean that Sora and the genie interactions hallucinate badly, according to the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=302" target="_blank">00:05:02.800</a></span> | <span class="t">I'm referring to the fact that real time high fidelity generation is still a while away.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=308" target="_blank">00:05:08.160</a></span> | <span class="t">That's just not on my prediction list for this year. And that's despite me saying that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=312" target="_blank">00:05:12.640</a></span> | <span class="t">super realistic text video would happen this year in my January 1st video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=317" target="_blank">00:05:17.360</a></span> | <span class="t">What's my evidence that latency will slow everything down? Well, according to Bloomberg,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=322" target="_blank">00:05:22.080</a></span> | <span class="t">OpenAI won't say precisely how long Sora takes on each request. But apparently you can definitely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=328" target="_blank">00:05:28.080</a></span> | <span class="t">go grab a snack while you wait for these things to run. So real time, interactive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=333" target="_blank">00:05:33.760</a></span> | <span class="t">low resolution games by the end of this year, yes, and high resolution time limited interactive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=340" target="_blank">00:05:40.000</a></span> | <span class="t">generations by the end of this year. But I think we'll have to wait till next year for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=344" target="_blank">00:05:44.800</a></span> | <span class="t">combination of those two things. Still, I do think it's worth pausing to imagine a scenario</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=350" target="_blank">00:05:50.320</a></span> | <span class="t">that we might well get by the end of this year, be it inside Gemini 2 or GPT 5. Imagine either of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=356" target="_blank">00:05:56.480</a></span> | <span class="t">those models creating an intricate short story, say with this cute little robot character as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=362" target="_blank">00:06:02.000</a></span> | <span class="t">protagonist. And then alongside each chapter of that story, it generates a real time video that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=366" target="_blank">00:06:06.960</a></span> | <span class="t">you can play about with. You can almost picture it as the paper says it would emulate parallax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=371" target="_blank">00:06:11.760</a></span> | <span class="t">That's when the character and the foreground move around, but the background stays relatively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=377" target="_blank">00:06:17.120</a></span> | <span class="t">static. The model would have created not only a story, but a playable world. And just to reiterate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=382" target="_blank">00:06:22.720</a></span> | <span class="t">all we need is a single text prompt or a single image to create that new interactive environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=389" target="_blank">00:06:29.120</a></span> | <span class="t">We've already seen how it can make an AI image playable, but here is that concept applied to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=395" target="_blank">00:06:35.120</a></span> | <span class="t">a human design sketch and finally to some real world images. But just before we leave the paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=403" target="_blank">00:06:43.040</a></span> | <span class="t">I want to touch on just how well within their capabilities Google were when they made this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=408" target="_blank">00:06:48.560</a></span> | <span class="t">11 billion parameter model. And let's not even talk about Gemini 1.5 Ultra, which is coming soon,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=413" target="_blank">00:06:53.520</a></span> | <span class="t">or Gemini 2. What could they do with a bigger model size or more compute? Well, they could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=418" target="_blank">00:06:58.160</a></span> | <span class="t">train Genie 2 on an even larger proportion of internet videos to simulate realistic and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=424" target="_blank">00:07:04.640</a></span> | <span class="t">imagined environments. At this point, I'll even throw in another prediction. I think by the end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=428" target="_blank">00:07:08.800</a></span> | <span class="t">of this year, you could play a run through of a particular game from start to end, then feed in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=433" target="_blank">00:07:13.920</a></span> | <span class="t">that entire video to say Genie 2 or an open source equivalent. Then if you wait a few minutes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=439" target="_blank">00:07:19.280</a></span> | <span class="t">you'll essentially get an expansion pack, another level of the game generated by the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=444" target="_blank">00:07:24.640</a></span> | <span class="t">one which might have some hallucinations, but in which you can take all the same actions as before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=449" target="_blank">00:07:29.840</a></span> | <span class="t">Of course, the copyright issues with that will be multifarious, but there are some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=454" target="_blank">00:07:34.960</a></span> | <span class="t">other complications aside from copyright about all of these developments. And no,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=459" target="_blank">00:07:39.280</a></span> | <span class="t">I don't just mean an explosion of cheating in gaming. You can now buy monitors that alert you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=464" target="_blank">00:07:44.800</a></span> | <span class="t">of enemy movements and you're going to get AI powered peripherals that ensure you don't miss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=469" target="_blank">00:07:49.200</a></span> | <span class="t">your shots. But frankly, for me, take away the spirit of any game. But no, I'm more referring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=474" target="_blank">00:07:54.640</a></span> | <span class="t">to the growing unpredictability of the job market, not necessarily job losses,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=480" target="_blank">00:08:00.080</a></span> | <span class="t">but the inability to plan your career. Like this announcement from Tyler Perry isn't exactly about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=485" target="_blank">00:08:05.600</a></span> | <span class="t">job losses. He saw OpenAI's Sora last week and decided not to expand his studio. But those quote</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=492" target="_blank">00:08:12.080</a></span> | <span class="t">job losses wouldn't show up necessarily in the statistics because those jobs never necessarily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=497" target="_blank">00:08:17.360</a></span> | <span class="t">existed. It's just that they won't exist now. Let me know what you think, but I feel like that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=501" target="_blank">00:08:21.520</a></span> | <span class="t">might happen quite a lot. It's not that companies might start to fire everyone. They just might not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=506" target="_blank">00:08:26.000</a></span> | <span class="t">hire as many people as they originally would have done. And it almost goes without saying that that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=510" target="_blank">00:08:30.000</a></span> | <span class="t">doesn't just apply to gaming and entertainment. Samsung, and I raised an eyebrow at this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=515" target="_blank">00:08:35.200</a></span> | <span class="t">plans to have fully automated chip fabrication plants by 2030. And that article brought to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=522" target="_blank">00:08:42.160</a></span> | <span class="t">mind this one minute video that I think is appropriate to play here. Looking to upskill</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=527" target="_blank">00:08:47.840</a></span> | <span class="t">for the future. This new AI can perform all coding jobs in seconds, including blockchain development.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=534" target="_blank">00:08:54.640</a></span> | <span class="t">While this AI is already outperforming accounting firms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=539" target="_blank">00:08:59.360</a></span> | <span class="t">Meanwhile, the new graphic design AI aims to automate graphic design and could minimize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=546" target="_blank">00:09:06.320</a></span> | <span class="t">Relax, because in a future where AI does most of the work, there'll be one thing that humans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=553" target="_blank">00:09:13.120</a></span> | <span class="t">would finally get to do all day long. Nothing. Before we lose all of our jobs, though, a quick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=562" target="_blank">00:09:22.400</a></span> | <span class="t">plug for the new discord channel I've got set up for AI insiders. I've recruited thought leaders</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=567" target="_blank">00:09:27.760</a></span> | <span class="t">from 20 professions, from neurosurgeons to professors, cybersecurity experts, marketing CEOs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=574" target="_blank">00:09:34.640</a></span> | <span class="t">and AI engineers. And new people are joining as thought leaders every week, including a famous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=580" target="_blank">00:09:40.320</a></span> | <span class="t">game designer, hopefully next week. What we're trying to create is a friendly and professional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=585" target="_blank">00:09:45.280</a></span> | <span class="t">environment in which to swap tips and share best practices. Of course, I'd love to see you there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=590" target="_blank">00:09:50.640</a></span> | <span class="t">but if you join my Patreon, you don't, of course, just get access to the discord. There's also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=594" target="_blank">00:09:54.960</a></span> | <span class="t">podcasts and interviews tomorrow. Actually, I'm interviewing the CEO of perplexity and last but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=600" target="_blank">00:10:00.560</a></span> | <span class="t">not least exclusive AI explained style videos. This is one that I released four days ago that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=605" target="_blank">00:10:05.600</a></span> | <span class="t">draws upon seven or eight different papers. This is the same week, though, that Demis Hassabis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=610" target="_blank">00:10:10.480</a></span> | <span class="t">gently mocked that $7 trillion figure. He was asked in Wired about Sam Altman trying to raise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=615" target="_blank">00:10:15.920</a></span> | <span class="t">that much money for more AI chips to scale up the compute available. Demis Hassabis, the CEO</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=621" target="_blank">00:10:21.520</a></span> | <span class="t">of Google DeepMind said this. Was that a misquote? I heard someone say that maybe it was yen or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=627" target="_blank">00:10:27.680</a></span> | <span class="t">something. Of course, taking the Mickey because yen is worth a lot less than dollars. He went on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=632" target="_blank">00:10:32.800</a></span> | <span class="t">to point out that, of course, not everything rests on scale. He said you're not going to get new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=637" target="_blank">00:10:37.040</a></span> | <span class="t">capabilities like planning or tool use or agent like behavior just by scaling existing techniques.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=643" target="_blank">00:10:43.760</a></span> | <span class="t">I've got another video coming on agents, so that discussion will have to wait for another day. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=648" target="_blank">00:10:48.960</a></span> | <span class="t">what might be coming sooner than that video is a video on AI in robotics. Four days ago,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=654" target="_blank">00:10:54.800</a></span> | <span class="t">a researcher at Google DeepMind said this. There will be three to four massive news events coming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=659" target="_blank">00:10:59.840</a></span> | <span class="t">out in the next weeks that will rock the robotics plus AI space. Adjust your timelines. It will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=665" target="_blank">00:11:05.680</a></span> | <span class="t">a crazy 2024. Now, I would guess that Genie counts as one of those three to four announcements. He</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=671" target="_blank">00:11:11.680</a></span> | <span class="t">can't have been referring to Gemma, the open model from Google DeepMind, because that was released</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=676" target="_blank">00:11:16.480</a></span> | <span class="t">the day before. The most interesting part of the Gemma paper and release for me was the sheer scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=682" target="_blank">00:11:22.560</a></span> | <span class="t">of data that they used. For those of you who've been following the scene for a while and remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=687" target="_blank">00:11:27.280</a></span> | <span class="t">the chinchilla paper, that was back in 2022 when it was discovered that for a given compute budget,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=693" target="_blank">00:11:33.200</a></span> | <span class="t">the optimum number of tokens to train on, text tokens we're talking about, was roughly 20 times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=699" target="_blank">00:11:39.120</a></span> | <span class="t">the number of parameters. But for Gemma, which was seven or eight billion parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=704" target="_blank">00:11:44.240</a></span> | <span class="t">they trained on around six trillion tokens of text. That's a thousand tokens of text for every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=710" target="_blank">00:11:50.720</a></span> | <span class="t">parameter. Or in other words, when you've got the kind of compute that Google has,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=714" target="_blank">00:11:54.720</a></span> | <span class="t">you don't have to necessarily follow the compute optimal strategy. But I am prevaricating. What</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=720" target="_blank">00:12:00.000</a></span> | <span class="t">news do I think Ted is referring to? Well, here's my best guess. And no, it's not based on any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=725" target="_blank">00:12:05.120</a></span> | <span class="t">insider knowledge. I think Google is going to announce another embodied model like RT2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=730" target="_blank">00:12:10.720</a></span> | <span class="t">but powered by Gemini. Now, I've covered RT2 in previous videos back in October and indeed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=736" target="_blank">00:12:16.560</a></span> | <span class="t">interviewed the tech lead for RT2X for AI Insiders. But those models in a nutshell fuse robotics data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=743" target="_blank">00:12:23.600</a></span> | <span class="t">with transferred learning from text and web data. In other words, it got better at robotics through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=749" target="_blank">00:12:29.120</a></span> | <span class="t">having an LLM at its core, or you might say an MMM, a multimodal model. But in the case of RT2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=755" target="_blank">00:12:35.920</a></span> | <span class="t">that was Palm E at 12 billion parameters, or Parley X at 55 billion parameters. Imagine RT3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=762" target="_blank">00:12:42.960</a></span> | <span class="t">powered by Gemini at say one trillion parameters. It might understand the world around it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=768" target="_blank">00:12:48.720</a></span> | <span class="t">unprecedented degrees of depth and intelligence. And if it's powered by Gemini 1.5, it might be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=774" target="_blank">00:12:54.480</a></span> | <span class="t">able to remember that world for months and months and months. Indeed, I was so inspired by the RT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=779" target="_blank">00:12:59.520</a></span> | <span class="t">series that I made it the unofficial logo of AI Insiders. Evaluations for humanoid AI-powered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=785" target="_blank">00:13:05.680</a></span> | <span class="t">robotics startups are starting to get pretty wild too. The CEO of NVIDIA, Jensen Huang, said that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=791" target="_blank">00:13:11.440</a></span> | <span class="t">his equivalent for the Transformer paper in the near future is foundational robotics. He said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=796" target="_blank">00:13:16.880</a></span> | <span class="t">"If you could generate text, if you could generate images, can you also generate motion?" He said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=801" target="_blank">00:13:21.440</a></span> | <span class="t">"The answer is probably yes. And like we've seen, we can also generate interaction." He went on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=806" target="_blank">00:13:26.240</a></span> | <span class="t">"Humanoid robotics should be just around the corner." I think he was referring to NVIDIA's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=811" target="_blank">00:13:31.280</a></span> | <span class="t">GEAR, Generalist Embodied Agent Research. That's led by none other than Jim Phan. And he said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=817" target="_blank">00:13:37.360</a></span> | <span class="t">"2024 is the year of robotics and the year of gaming AI." Now this is Tesla's Optimus robot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=823" target="_blank">00:13:43.360</a></span> | <span class="t">but I do wonder if the Chachapiti or Sora moment for robotics will be when a humanoid robot walks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=830" target="_blank">00:13:50.160</a></span> | <span class="t">with the fluidity of a human. That will just seem so wild when it happens. Just imagine a humanoid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=836" target="_blank">00:13:56.880</a></span> | <span class="t">walking up to you with human-like swagger and shaking your hand, all while remembering a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=841" target="_blank">00:14:01.840</a></span> | <span class="t">conversation you had with it, say, a year ago. Now, I don't think I can end this video without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=846" target="_blank">00:14:06.640</a></span> | <span class="t">touching on some of the recent controversies that Google has faced. I just think they were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=851" target="_blank">00:14:11.360</a></span> | <span class="t">fairly clearly not given the kind of testing that they obviously required. And I'm not just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=856" target="_blank">00:14:16.080</a></span> | <span class="t">referring to how Gemini seems to be phobic of the word white. There's also evidence of false</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=861" target="_blank">00:14:21.360</a></span> | <span class="t">refusals to questions that have a pretty obvious answer. And my take on this is going to try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=866" target="_blank">00:14:26.560</a></span> | <span class="t">move beyond just the obvious take. I think these examples show that Google is genuinely rattled by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=872" target="_blank">00:14:32.240</a></span> | <span class="t">OpenAI, Microsoft, and players like Perplexity. And so they're cutting corners on the testing of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=877" target="_blank">00:14:37.840</a></span> | <span class="t">their models. After all, if you're six months behind OpenAI, what's one way to catch up? Just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=882" target="_blank">00:14:42.800</a></span> | <span class="t">cut out six months of human feedback for your models. I hope this isn't what Google did or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=888" target="_blank">00:14:48.160</a></span> | <span class="t">plans to do in the future, but it seems like that to me. So let me know what you think of all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=892" target="_blank">00:14:52.960</a></span> | <span class="t">this and whether we are indeed entering a new era of action and interaction. Thank you so much for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gGKsfXkSXv8&t=900" target="_blank">00:15:00.240</a></span> | <span class="t">watching all the way to the end and have a wonderful day.</span></div></div></body></html>