<html><head><title>Stanford XCS224U: NLU I Contextual Word Representations, Part 8: Seq2seq Architectures I Spring 2023</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U: NLU I Contextual Word Representations, Part 8: Seq2seq Architectures I Spring 2023</h2><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc"><img src="https://i.ytimg.com/vi/ymKWRZgHwPc/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./ymKWRZgHwPc.html">Whisper Transcript</a> | <a href="./transcript_ymKWRZgHwPc.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome back everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=6" target="_blank">00:00:06.000</a></span> | <span class="t">This is part eight in our series on contextual representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=9" target="_blank">00:00:09.440</a></span> | <span class="t">We're going to talk briefly about sequence to sequence architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=13" target="_blank">00:00:13.680</a></span> | <span class="t">To kick it off, I thought I would begin with tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=17" target="_blank">00:00:17.040</a></span> | <span class="t">These are going to be tasks that have natural sequence to sequence structure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=21" target="_blank">00:00:21.280</a></span> | <span class="t">and I'm trying to leave open for now whether we would actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=24" target="_blank">00:00:24.140</a></span> | <span class="t">model them with sequence to sequence architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=26" target="_blank">00:00:26.740</a></span> | <span class="t">That's a separate question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=28" target="_blank">00:00:28.460</a></span> | <span class="t">Seq2Seq tasks include machine translation, of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=31" target="_blank">00:00:31.800</a></span> | <span class="t">This is a classic one where a text in one language comes in,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=35" target="_blank">00:00:35.000</a></span> | <span class="t">and we would like to produce text in another language as the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=38" target="_blank">00:00:38.660</a></span> | <span class="t">Summarization, also a classic Seq2Seq problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=42" target="_blank">00:00:42.300</a></span> | <span class="t">A long text comes in and a presumably shorter one comes out summarizing the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=47" target="_blank">00:00:47.620</a></span> | <span class="t">Free-form question answering where we're trying to generate answers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=51" target="_blank">00:00:51.280</a></span> | <span class="t">This could also be a Seq2Seq problem where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=53" target="_blank">00:00:53.460</a></span> | <span class="t">a question maybe with some contextual information comes in,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=56" target="_blank">00:00:56.560</a></span> | <span class="t">and the task in decoding is to generate an answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=60" target="_blank">00:01:00.300</a></span> | <span class="t">Dialogue, of course, classic Seq2Seq problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=63" target="_blank">00:01:03.360</a></span> | <span class="t">utterances to utterances.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=65" target="_blank">00:01:05.400</a></span> | <span class="t">Semantic parsing could also be thought of as a Seq2Seq task here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=69" target="_blank">00:01:09.280</a></span> | <span class="t">natural language sentences come in,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=71" target="_blank">00:01:11.400</a></span> | <span class="t">and we try to map them to their logical forms capturing aspects of their meaning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=76" target="_blank">00:01:16.120</a></span> | <span class="t">Related task would be code generation here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=78" target="_blank">00:01:18.440</a></span> | <span class="t">a natural language sentence comes in,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=80" target="_blank">00:01:20.260</a></span> | <span class="t">and we try to produce a program that the sentence is describing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=84" target="_blank">00:01:24.200</a></span> | <span class="t">That is just a small sample of the many things that we could call Seq2Seq tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=90" target="_blank">00:01:30.200</a></span> | <span class="t">Even these are just special cases of the more general class of things that we might call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=95" target="_blank">00:01:35.160</a></span> | <span class="t">encoder-decoder problems which would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=97" target="_blank">00:01:37.760</a></span> | <span class="t">agnostic about whether the encoding and decoding involve sequences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=101" target="_blank">00:01:41.440</a></span> | <span class="t">they could involve images, video, speech, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=106" target="_blank">00:01:46.160</a></span> | <span class="t">I've been offering historical notes throughout this series of lectures,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=110" target="_blank">00:01:50.960</a></span> | <span class="t">and I think this is a nice point to emphasize that the RNN era really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=115" target="_blank">00:01:55.240</a></span> | <span class="t">primed us to think about Seq2Seq problems in the context of transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=120" target="_blank">00:02:00.020</a></span> | <span class="t">On the left here, I have a classic RNN formulation of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=123" target="_blank">00:02:03.720</a></span> | <span class="t">a Seq2Seq problem where we have the input sequence A, B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=126" target="_blank">00:02:06.560</a></span> | <span class="t">C, D, and then we begin decoding with this special symbol,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=130" target="_blank">00:02:10.420</a></span> | <span class="t">decode X, Y, Z, and then we produce our end token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=133" target="_blank">00:02:13.840</a></span> | <span class="t">and that is the job of decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=136" target="_blank">00:02:16.280</a></span> | <span class="t">The historical note here is that those tasks evolved from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=139" target="_blank">00:02:19.960</a></span> | <span class="t">standard RNNs into RNNs with lots of attention mechanisms on the top here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=145" target="_blank">00:02:25.400</a></span> | <span class="t">designed specifically to help the decoding steps remember what was in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=150" target="_blank">00:02:30.360</a></span> | <span class="t">the encoding part by offering all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=152" target="_blank">00:02:32.760</a></span> | <span class="t">these attention mechanisms back into that encoding phase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=156" target="_blank">00:02:36.640</a></span> | <span class="t">What we see again in the transformer paper is a full embrace of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=161" target="_blank">00:02:41.400</a></span> | <span class="t">attention as the primary mechanism and a dropping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=164" target="_blank">00:02:44.200</a></span> | <span class="t">away of all of these recurrent mechanisms here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=168" target="_blank">00:02:48.280</a></span> | <span class="t">In the context of transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=171" target="_blank">00:02:51.560</a></span> | <span class="t">we have a variety of ways that we could think about Seq2Seq problems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=176" target="_blank">00:02:56.520</a></span> | <span class="t">one of them being encoder-decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=179" target="_blank">00:02:59.160</a></span> | <span class="t">but other options present themselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=181" target="_blank">00:03:01.200</a></span> | <span class="t">This is a nice figure from the T5 paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=183" target="_blank">00:03:03.440</a></span> | <span class="t">which we'll talk about in a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=185" target="_blank">00:03:05.400</a></span> | <span class="t">On the left, you have encoder-decoder, as I said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=187" target="_blank">00:03:07.960</a></span> | <span class="t">where we fully encode the input in the encoder side with some set of parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=192" target="_blank">00:03:12.800</a></span> | <span class="t">and then possibly different parameters do decoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=195" target="_blank">00:03:15.980</a></span> | <span class="t">where in the decoding steps,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=197" target="_blank">00:03:17.480</a></span> | <span class="t">we attend fully back to all the steps from the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=201" target="_blank">00:03:21.320</a></span> | <span class="t">But we needn't have this encoder-decoder structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=205" target="_blank">00:03:25.380</a></span> | <span class="t">Another option, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=207" target="_blank">00:03:27.080</a></span> | <span class="t">would be to simply process these sequences with a standard language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=211" target="_blank">00:03:31.440</a></span> | <span class="t">In the middle here, you have a transformer-based language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=215" target="_blank">00:03:35.480</a></span> | <span class="t">and you can see that characteristic attention mask where we don't get to look into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=219" target="_blank">00:03:39.180</a></span> | <span class="t">the future but rather can only attend to the past,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=222" target="_blank">00:03:42.600</a></span> | <span class="t">even for the part that we're thinking of as the encoded part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=226" target="_blank">00:03:46.760</a></span> | <span class="t">An obvious variation of that would be to take our language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=230" target="_blank">00:03:50.700</a></span> | <span class="t">and when we do encoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=232" target="_blank">00:03:52.440</a></span> | <span class="t">do a full attention connection set across all the things that we're doing encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=237" target="_blank">00:03:57.800</a></span> | <span class="t">That's what you can see reflected here where when we're doing encoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=241" target="_blank">00:04:01.320</a></span> | <span class="t">just as in the encoder-decoder structure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=243" target="_blank">00:04:03.240</a></span> | <span class="t">we can have every element attend to every other element.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=246" target="_blank">00:04:06.980</a></span> | <span class="t">Then here, when we start to do decoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=249" target="_blank">00:04:09.400</a></span> | <span class="t">that's where the mask can only look into the past and not the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=254" target="_blank">00:04:14.320</a></span> | <span class="t">That's a nice framework for thinking about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=257" target="_blank">00:04:17.080</a></span> | <span class="t">The middle and right-hand options have become increasingly prominent as people have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=262" target="_blank">00:04:22.640</a></span> | <span class="t">explored ever larger variants of the GPT architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=266" target="_blank">00:04:26.200</a></span> | <span class="t">which is a standard language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=268" target="_blank">00:04:28.800</a></span> | <span class="t">But I'm going to focus now on two encoder-decoder releases that I think are very powerful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=274" target="_blank">00:04:34.600</a></span> | <span class="t">beginning with T5, which was the source of that nice previous framework there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=279" target="_blank">00:04:39.440</a></span> | <span class="t">T5 is an encoder-decoder variant that had extensive, multitask,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=284" target="_blank">00:04:44.920</a></span> | <span class="t">supervised, and unsupervised training across lots of different tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=289" target="_blank">00:04:49.880</a></span> | <span class="t">Then one very innovative thing that they did in the T5 paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=293" target="_blank">00:04:53.480</a></span> | <span class="t">which really gives us a glimpse of what was about to happen within context learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=297" target="_blank">00:04:57.960</a></span> | <span class="t">is that they offered task prefixes like translate English to German,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=303" target="_blank">00:05:03.000</a></span> | <span class="t">colon, and then you got the true input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=305" target="_blank">00:05:05.560</a></span> | <span class="t">That instruction on the left is telling the model what we want to do in decoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=311" target="_blank">00:05:11.000</a></span> | <span class="t">and it guides the model in this case to do translation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=313" target="_blank">00:05:13.840</a></span> | <span class="t">but the same part after the colon could be performing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=317" target="_blank">00:05:17.720</a></span> | <span class="t">a sentiment task given a different description of the task before the colon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=322" target="_blank">00:05:22.920</a></span> | <span class="t">Wonderfully insightful thing where we express all these tasks as natural language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=327" target="_blank">00:05:27.200</a></span> | <span class="t">which we simply encode,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=328" target="_blank">00:05:28.720</a></span> | <span class="t">and that guides the model's behavior,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=331" target="_blank">00:05:31.360</a></span> | <span class="t">essentially as though those task instructions were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=334" target="_blank">00:05:34.280</a></span> | <span class="t">themselves structured information as inputs to the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=338" target="_blank">00:05:38.320</a></span> | <span class="t">For T5, we have lots of model releases as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=341" target="_blank">00:05:41.920</a></span> | <span class="t">which has been tremendously empowering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=343" target="_blank">00:05:43.600</a></span> | <span class="t">This is a sample of the models that are available on Hugging Face,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=346" target="_blank">00:05:46.920</a></span> | <span class="t">and you can see that they range from very manageable 60 million parameter models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=351" target="_blank">00:05:51.320</a></span> | <span class="t">on up to really large 11 billion parameter releases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=355" target="_blank">00:05:55.760</a></span> | <span class="t">Relatedly, the Flan T5 models are variants of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=360" target="_blank">00:06:00.080</a></span> | <span class="t">the T5 architecture that were specifically instruction tuned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=364" target="_blank">00:06:04.240</a></span> | <span class="t">and that's a set of methods that we'll talk about in the next unit of the course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=368" target="_blank">00:06:08.600</a></span> | <span class="t">Those are also very powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=370" target="_blank">00:06:10.800</a></span> | <span class="t">That's T5. The other architecture that I thought I would highlight here is BART.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=376" target="_blank">00:06:16.360</a></span> | <span class="t">BART has some similarities and some real differences with T5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=380" target="_blank">00:06:20.400</a></span> | <span class="t">The essence of BART is that on the encoding side,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=383" target="_blank">00:06:23.720</a></span> | <span class="t">we're going to have a standard BERT-like architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=386" target="_blank">00:06:26.760</a></span> | <span class="t">and on the decoding side,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=388" target="_blank">00:06:28.680</a></span> | <span class="t">we're going to have a standard GPT-like architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=391" target="_blank">00:06:31.840</a></span> | <span class="t">That's fairly straightforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=394" target="_blank">00:06:34.000</a></span> | <span class="t">What's interesting about BART is the way pre-training happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=397" target="_blank">00:06:37.560</a></span> | <span class="t">This is essentially oriented around taking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=401" target="_blank">00:06:41.680</a></span> | <span class="t">corrupted sequences as input and figuring out how to uncorrupt them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=405" target="_blank">00:06:45.920</a></span> | <span class="t">What they did on the corrupting side is, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=409" target="_blank">00:06:49.320</a></span> | <span class="t">text infilling where whole parts of the input are masked out or removed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=414" target="_blank">00:06:54.080</a></span> | <span class="t">sentence shuffling where we reorganize parts of the input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=418" target="_blank">00:06:58.080</a></span> | <span class="t">token masking, token deletion, and document rotation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=423" target="_blank">00:07:03.360</a></span> | <span class="t">What they found is that the most effective pre-training regime was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=427" target="_blank">00:07:07.600</a></span> | <span class="t">a combination of that text infilling step and the sentence shuffling step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=432" target="_blank">00:07:12.000</a></span> | <span class="t">Remember, the idea here is that in pre-training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=435" target="_blank">00:07:15.240</a></span> | <span class="t">we're feeding in these corrupted sequences with these two techniques by and large,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=439" target="_blank">00:07:19.200</a></span> | <span class="t">and having the model learn to uncorrupt those sequences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=442" target="_blank">00:07:22.320</a></span> | <span class="t">The idea there, which is similar to the insight that we had from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=445" target="_blank">00:07:25.400</a></span> | <span class="t">Electra is that that kind of task can lead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=448" target="_blank">00:07:28.160</a></span> | <span class="t">the model to understand what good sequences look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=452" target="_blank">00:07:32.040</a></span> | <span class="t">That's the pre-training phase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=453" target="_blank">00:07:33.720</a></span> | <span class="t">If you download parameters from Hugging Face,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=456" target="_blank">00:07:36.120</a></span> | <span class="t">they're likely to be pre-trained in this uncorrupting fashion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=461" target="_blank">00:07:41.120</a></span> | <span class="t">For fine-tuning, the protocol is a little bit different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=464" target="_blank">00:07:44.640</a></span> | <span class="t">If we're doing classification tasks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=466" target="_blank">00:07:46.940</a></span> | <span class="t">we feed uncorrupted copies of the input into the encoder and the decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=471" target="_blank">00:07:51.680</a></span> | <span class="t">and then maybe we fine-tune the final decoder state as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=475" target="_blank">00:07:55.360</a></span> | <span class="t">we would with GPT against our classification task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=479" target="_blank">00:07:59.400</a></span> | <span class="t">For standard seek-to-seek problems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=481" target="_blank">00:08:01.760</a></span> | <span class="t">we simply feed in the input and the output to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=484" target="_blank">00:08:04.880</a></span> | <span class="t">the model and then fine-tune it on that basis with no corruption.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=488" target="_blank">00:08:08.560</a></span> | <span class="t">The corruption is by and large confined to the pre-training phase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=492" target="_blank">00:08:12.800</a></span> | <span class="t">The evidence that is offered in the paper is that that objective puts models in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=498" target="_blank">00:08:18.760</a></span> | <span class="t">a good pre-trained state where they're really good at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=501" target="_blank">00:08:21.100</a></span> | <span class="t">these fine-tuning tasks across a lot of different tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=504" target="_blank">00:08:24.760</a></span> | <span class="t">That's T5 and BART.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=506" target="_blank">00:08:26.400</a></span> | <span class="t">That's just two samples from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=508" target="_blank">00:08:28.120</a></span> | <span class="t">the wide range of different seek-to-seek architectures that are out there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=511" target="_blank">00:08:31.560</a></span> | <span class="t">But I think they're both very powerful as pre-trained artifacts that you can make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=515" target="_blank">00:08:35.480</a></span> | <span class="t">use of and also highlight some of the innovation that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=518" target="_blank">00:08:38.760</a></span> | <span class="t">happening with transformers in the seek-to-seek space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ymKWRZgHwPc&t=523" target="_blank">00:08:43.480</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>