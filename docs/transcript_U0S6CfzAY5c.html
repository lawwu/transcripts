<html><head><title>360Brew: LLM-based Personalized Ranking and Recommendation - Hamed and Maziar, LinkedIn AI</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>360Brew: LLM-based Personalized Ranking and Recommendation - Hamed and Maziar, LinkedIn AI</h2><a href="https://www.youtube.com/watch?v=U0S6CfzAY5c" target="_blank"><img src="https://i.ytimg.com/vi_webp/U0S6CfzAY5c/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>Hi, everyone. Very excited to be here. And I'm Hamid, this is Mazia, and today we're going to talk about our journey in leveraging large language models for personalization and ranking, and our paths to productionize such a large model for LinkedIn use cases. Recommendation ranking and personalization is deeply integrated in our daily life.</p><p>When you go to a feed to read an article and you're looking for a job and you're searching for something and you're buying something online, the backend powered by recommendation system tries to find the best content or best entity based on your interest and relevancy to you. However, this system usually suffers from some challenges, especially they are being trained on a specific task.</p><p>So they are disjoint optimized. They are usually not leveraging, for leveraging the most advanced architecture, they are being rolled out one by one, which is very time consuming and unproductive. So the question that we are asking is that what if you have only one model to solve all the tasks at the same time?</p><p>So the mission that we started was to build a large foundation model, based on large language models, that understand the holistic understanding of the user journey on LinkedIn platform. And can solve all the personalization tasks that LinkedIn has with just one model. And in addition to that, we wanted this model to have three other main characteristics.</p><p>One, we want this model to have zero shot capability, so that when you have a new problem or new surface, instead of basically collecting the data, building a new ranking model and putting it into production, which is a very time consuming journey, you can basically leverage this model out of the box to solve your task.</p><p>You just basically prompt the model and tell the model that this is the task that I want to solve. This is kind of recommendation. This is the entity. This is the user. And what do you think about the relevancy between these two entities? The second characteristic that we want to have this model to have is to leverage in-context learning as much as possible.</p><p>So that for a cold-star users problem, for example, we can leverage this model by just giving very few examples or just by explaining what the user might be interested in. And the model can solve that problem for the cold-star users. And the last one is following instruction. We want to basically give our users and members the ability to tell the model what they're interested in.</p><p>Imagine that next time you go to LinkedIn feed, or you can tell the model that these are my niche interests. And these are the topics that I'm interested to explore. And the model is basically the recommendation system will start finding the relevant information for you and recommend it to you.</p><p>Now, Mazi, I will talk about how we build this model, and then I will talk about how to serve this model. Okay. So let me talk a little bit about the brewing part, the building of the model. So in order to make use of the LLMs, which is what I think most of you guys are here for, is that we need to convert all the information we have about the users and the interactions and everything that they add into prompt.</p><p>And this is what we call the magic of promptification. So we take all the information we have about the user history and their profiles and a lot of interactions that they have had, and we turn it into a prompt, something like the one on the right-hand side here. So there's, as you can see, there's an instruction for the model to follow, for example, what we want the model to do in this case, so that we can actually generalize all the different instructions.</p><p>We give some information about the member profile, and we have some past, for example, interactions that they have had with the data that we have already shown to them. And then the question comes in, what do you think the user is going to do with this data or this new piece of information or this new item that we are showing to you?</p><p>So that's basically how we formalize the problem in order to feed it into an LLM. So obviously, I mean, if you take one of the LLMs out of the box and try to solve this problem with, it's going to work a little bit, but it's not going to be perfect.</p><p>So in order to do that, we have to train the model. So this is actually the pipeline that we have for developing the model and making it productionized. So as you can see, the left-hand side, we start with the open source model. Then we do some magic of upcycling to basically so that we can actually control the size of the model and the throughput versus the quality of the model.</p><p>And then we have like a few blocks of training, continuous pre-training, fine-tuning, and instruction fine-tuning, and also alignment. And at this point, we have this large model, which we call brew XL, which you can think of it as a large model with 150 million parameters that does really, really well.</p><p>And we have maximized the quality, but obviously, this model is not going to be able to serve online because, as you know, the recommendation systems are very, very cupid-hungry. So from here, we go all the way down to try to distill the model, so maximize the efficiency, and we're going to talk a little bit about that.</p><p>But basically, we go all the way down to, let's say, 3B model, which is actually something that can be productionized. But as you can see, there are so many different boxes here. And in order to make sure that the development cycle is actually smooth, we had to do a lot of automation.</p><p>So one of the key lessons from here is that you build a lot of automation into these pipelines in order to make the fact that making these models is actually very complicated into a much easier and more manageable situation. One big question that might actually come up here is that why do you actually need the XL model?</p><p>And in fact, we did a lot of experimentation to see if you can actually get away from not having the XL model. Unfortunately, that's not actually the case. You have to first go big and then go small. If you do try to train the model from scratch with a small model, it doesn't actually work that well.</p><p>So in this case, we did this and we showed that the distillation is actually something that is very important for the smaller models. But now, let me tell you a little bit about the levers that you can use in order to improve these models over time. This is actually something that's very important.</p><p>I mean, if you look at all the literature, there is a lot about scaling laws, how these models actually scale with data, with compute, and with this and that. So in this case, we have three different layers that I'm going to talk about. The first one is obviously the data scaling.</p><p>So what if we have actually more and more data? This is something that comes up a lot in the recommendation systems. We actually have a lot of data depending on how much you actually log about the user behavior. You might have a lot of data that goes back to six months, one year, or whatever.</p><p>And in this graph, as you see, as we increase the amount of data, the performance of the model actually improves. And we hope that we can actually improve the model even further with having more and more data fit into it. Another lever that you can actually pull in order to improve the quality of the model, especially the Excel model, is to increase the size of the model.</p><p>And in this experiment, we actually did this experiment over a mixture of architecture. You can see if you go from 7B to 8 by 22B, the performance of the model actually increases and improves. And finally, this is another thing that is kind of like I think one of the take-home message from here would be that the context length actually matters a lot for these kinds of applications with the recommendation systems.</p><p>And the context length actually defines how much history from the user you can actually give to the model. So in this experiment, we actually show that if you increase the context length by feeding more history from the user to the model, you can actually improve the performance of the model by feeding more and more data to the model.</p><p>As you can see, towards the end of this graph, the performance actually drops. We don't believe that this is because of the fact that the context is actually less informative. The problem is that the models, I mean, at least the model that we were using in this experiment, doesn't generalize that well to the longer context.</p><p>Actually, I mean, if you look at most of the literature, they tell that the performance of the model actually drops if you go beyond some context. Actually, I have to give it back to you. Okay. Okay. Let's talk a little bit about the results and see if we can actually deliver on some of the promises that we had.</p><p>So one of the things that we promised was that we can actually improve the performance of the model or performance of the behavior of the system on cold start users. In this case, we actually show the gap between our model and the production models on the users that have few interactions.</p><p>Like, for example, less than five interactions, less than 100 interactions, and so on. And as you can see, the gap between the 360 BOO model and the production model actually grows as the number of interactions decreases. So this actually shows you that having the word knowledge that the model inserts into these systems actually improves the quality of its predictions.</p><p>Finally, we were promising to give you some generalization to the new domains, meaning that the problems that model has never seen inside its training. So in this graph, as I showed, these are four different tasks, and these tasks are completely out of domain, no information about that surface the model has seen during the training.</p><p>But as you can see, it can actually be on par or even beat some of the production models. And just to say, these production models are specific for that specific task. So they have been trained on that task. So this is not actually a small feat. So it's actually something that's significant.</p><p>So as you can see, this gives the people who are developing these platforms to roll out features and roll out surfaces much more quickly because they can actually use these models to do recommendation for them. And now I give it back to Hamed to talk about serving. So let me walk you through that how can we production of such a large model in an environment that requires a very high QPS and low latency.</p><p>Many recommendation systems have tens of thousands of the QPS, and they also require less than a second, like a 500, 400 millisecond latency at best. So there are three levels that we can pull in order to make the model more efficient and improve the throughput and reduce the latency for these models.</p><p>A sparsification, going to the smaller model, and quantization. As Mazur explained before, smaller models definitely have a better throughput, but our recipe is that we need to go big and then go small. If you go with a smaller model initially, it doesn't have enough capacity, it doesn't have enough reasoning power to solve the complicated task that we have.</p><p>So we go with a larger model, and then we start this 250 billion parameter model, and then we start distilling it to the smaller model. And one of the recipes here is that we need to do the distillation step by step, and that means that we go with, for example, 8B model, then 3B model, and then 1B model.</p><p>So we slowly decrease the size of the model, and we distill over and over from the previous model. And that recipe shows to be much more effective rather than basically directly going from 150 billion parameter model to 1B parameter model. Same thing for pruning. So pruning is a mathematical optimization problem.</p><p>You want to either reduce the number of hits in the transformers. You can reduce the number of MLPs. overall these transformers models proven to be very, very redundant in terms of keeping the information. So we can start pruning and removing some of these layers or reduce basically the precision for each of the activations and parameters.</p><p>However, again, if you do the pruning very aggressively at the beginning, your performance would significantly suffer. So the recipe here is also do the gradual pruning. What we do is that we start pruning the model, very small pruning to the model. We distill to the smaller model, and we do it over and over again.</p><p>More pruning, more distillation, more pruning, more distillation. And as you can see from this plot, doing the gradual pruning can be as effective as basically no information loss. Whereas if you just basically do aggressive pruning at the beginning, you can have up to 1% reduction in the model quality.</p><p>Another level is quantization. We're going to lower precision. We are leveraging FB8 for activation model parameters. However, doing just FB8 in all the layers, here's the performance of the model, the quality of the model significantly. So now basically your tool would be to do mixed precision. And one of the important aspects when it comes to ranking and recommendations and overall prediction tasks is you want the model, the prediction or the probability output of the model to have a very good precision.</p><p>So the LM head at the end of the language model has to be in FP32. If you do it in FP16, BF16 or FP8, what happens is that the numbers collapse. And you don't have a very good calibration on top of that, and you cannot distinguish between different items recommended.</p><p>Last part is sparsification. We can sparsify basically the attentions. The most expensive part of the transformers is attention scores. And we can leverage sparsification. Not every item needs to attend to every item. And when you know your task, when you know this is a recommendation, these are the items that you want in the history, you can sparsify and not have every item attend to each other.</p><p>And same goes with when you are recommending the items. Instead of recommending one item, you can recommend 50 items, 500 items at the same time. But you want to make sure that these items are not attending to each other. So you sparsify the attention scores for the output and for the query.</p><p>If you put everything together, we can see that basically we can have a significant reduction in the latency. What we have done is that in four or five of our release, one release after the other, we were able to reduce the latency by 7x. And at the same time, increasing the throughput, which is basically the number of queries that we can handle by one GPU, by 30x.</p><p>So we are improving basically the amount of the work that the GPU is doing. At the same time, we are reducing the latency that each queries sync. These are some of basically technical reports and papers that we published during our journey to share to the community basically our lesson learned.</p><p>And that's the end of our talk. So we have some time also to answer some questions. Thank you. Please talk to the microphones if you want to answer the question. Yeah. Thank you. Great talk. One question. How did you measure that it doesn't lose generalization power? Obviously, you've done a lot of fine tuning.</p><p>And you mentioned it works for four or five tasks instead of task-specific models. How do you know it's going to work for the next five tasks? That's a good question. So we have a lot of -- I mean, the answer overall is having a very comprehensive benchmark instead. We have something around like 50 to 60 benchmarking.</p><p>Some of them are internal. Some of them are external. For example, we leverage if eva to make sure that the model still follows a very good instruction. And as Marzia mentioned, some of the tasks are not -- never being part of our training data. And that's how we are measuring basically the generalization to the new domain within LinkedIn use cases, for example.</p><p>some of the things that we can use in the same way. And that's how we do that. Thank you. Thanks for the job. I'm wondering what a small listing website can use out of the box. Have you heard of NLWeb, which was launched recently by Microsoft? If yes, what are your views on that as a recommendation system?</p><p>NLWeb. No, I haven't actually heard of it. Okay. Okay. Anything you -- for smaller ones listing -- let's say a real listed listing website has like thousands of real listed listings. What are the out-of-the-box recommendation models that people can start using? I mean, that's -- I wish that such a model would exist.</p><p>I don't really -- I mean, that's why I think we started this work. We were trying to see if we can actually make it a foundation model so that you can actually solve those kinds of problems. I think there's a lot of potential for this to be able to serve a lot of the use cases that are beyond the bigger companies.</p><p>But definitely, I don't know any of them. I think you should check out NLWeb once. Okay. I'll look at that. Thanks. Thank you for the great talk. On the slide where you mentioned you have a multi-item scoring. I'm curious, like, what does it effectively mean? Does it mean that you need to do multi-step decoding or it's just one step or just processing the logits for multiple items?</p><p>What does it -- It's a multi-step. We don't want to basically -- we didn't want to go to the, for example, complication of speculative decoding or basically the decoding aspect. We wanted to have everything at the pre-fill. Okay. So what we did was basically all the items are being sequenced.</p><p>All the recommended items or potential candidates are sequenced together. Mm-hmm. But we also wanted to avoid them to attend to each other. Mm-hmm. So we leveraged basically what we call it like a 4D attention mask. And we developed a special kernel actually in the SGLang and VLM to be able to do that.</p><p>And now when you have up to 500 items in your query segment, those items don't attend to each other. They only attend to the historical user and user profile information. Okay. Thank you. Hey. Great talk. So a user history means many things, right? So like there is all of the jobs that they've applied to are in the job postings.</p><p>There are so many entities and so on. The context of the model can get quite large. How did you manage that? Did you compress it or were there parts that you focused on? Yeah. So we actually experimented with a lot of things. We experimented with the RAC system so that basically when we have a query, we try to figure out what are the most closest items in your history to bring it up.</p><p>We also experimented with chronical orders and some sort of way to get on the chronical orders. It turns out that for majority of applications that we have, actually chronical order is good enough. And that kind of makes sense because recommendation systems are very biased to the freshness. So the more recent user activity helps.</p><p>One of the biggest challenge is actually the, this is more, now become more like a traditional LM problem. How do you balance the distribution of your positive and negative within the context? And I think that's become something that more like an ML engineering effort to figure out, okay, do I want more positive or negative?</p><p>Like how much information I need to put in the context? Yeah. I can add one more thing to this. Sure. There is also another complication when you go to the serving of these models. You don't want to break the KV caching or something that you're using in the serving.</p><p>So it's going to be a little bit more complicated, more cumbersome to do something that's smarter than just putting the chronological order. So that's something that needs to be designed. So it's not something that's very obvious. Absolutely. One more question. You guys did so many experiments, tried out so many things.</p><p>How's your entire system set up? Because I'm assuming that you say quantization, but you must have tried different forms of quantization, whatnot. How do you set up the system in such a way that you can try out multiple experiments and see what works best? Can you talk a bit about that?</p><p>Yeah, so I'll just touch a bit on that one. I think the one thing that we hold a very high bar for the one was automation. So our system is very automated to the extent that when you're running experimentation, actually the result of the experiment being pushed automatically into the Excel sheet.</p><p>And now when you have such an automated system, now basically the developers are very efficient in terms of like, I just want to figure out different quantization. So you just change the quantization parameters and everything else just by clicking a button happens end to end. So I think automation is the key if you want to basically really optimize for these models.</p><p>So did you build all of that automation in-house or did you? Yes. Most of them. Well, we leveraged, for example, lightning, VLL, MSG like. We leveraged basically a lot of open source tools, but we make sure that they are integrated very well with each other and optimize basically the entire flow.</p><p>Okay, thank you. Thank you. Thank you again, Hamad Mazah. Thank you. Thank you. Thank you. you you you</p></div></div></body></html>