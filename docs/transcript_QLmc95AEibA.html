<html><head><title>DeepSeek Math + The Mamba in the Llama: Distilling and Accelerating Hybrid Models!</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>DeepSeek Math + The Mamba in the Llama: Distilling and Accelerating Hybrid Models!</h2><a href="https://www.youtube.com/watch?v=QLmc95AEibA" target="_blank"><img src="https://i.ytimg.com/vi/QLmc95AEibA/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>Yeah, sir. Yeah, I need you to close the door because I can hear myself. Yeah. And the reason why the DeepSync Math paper is in particularly important, right, is that the previous time around when we went through the DeepSync R1 paper, we essentially kind of just skimmed this through, we skipped past this, but the DeepSync team credited the DeepSync Math paper as one of the key things behind like creating the whole reasoning data set and things like that.</p><p>There was a lot of emphasis on the math and, and math data set as being the entire path of how they achieve R1 performance. Well, R1, the paper itself covered more, more into the technical aspects of like how they got it to scale, how they got it to train that far.</p><p>This is more of like their previous math model that formed the foundation on how they built the data sets involved for the reasoning training. So, so this is a much older paper and what it did, what it covered was a 7B math model, which at that point in time when it came out, right, was literally one of the best math model out there for its size.</p><p>So, in fact, in fact, in fact, are outperforming even if at that point in time, even like some of the GPT-4 APIs, though, if memory serves me right, then GPT-4 updated itself to like be better than the math paper, the math model. So, but for a period of time, it was better.</p><p>And what was more exciting is actually for those who were at Kaggle, literally for a very decent period of time, this model just dominated all the math context in Kaggle. And this is literally how they did the pipeline to build the data set, which based on the responses of all the DCR1 papers and some through Twitter and some through the interviews, it is responsible for how they actually build up the data set subsequently.</p><p>So, so yeah, this is the performance there. So introduction, you know, skips, skips, skips. Okay, so here, right, the first, they cover the first step, right? And the challenges is actually to build the data set that is focusing on math itself. So there are existing data set like open web math and things like that, but it's, in for most part, insufficient.</p><p>So, so this is really more, this first part is more of like a data set paper, actually. And what they did, right, what they did, essentially, right, is that using, using known good math sources, yeah, known good math sources, right, they actually scan common crowd for relevant math articles, papers, and so on.</p><p>And this is what they use for the pre-training of the math-based model, including existing math data sets. So, so, and wait, I'm sure, I think I'm remembering. out of sequence. Where was it? Yeah, data collection decontamination. So here it is. So when, when, when they came, when they went through here, right, right, they, they started with 500,000 data points off for the, for the web corpus there, as such positive training examples of these, these are known good examples, and then subsequently, right, they, they, they did the embedding search, so vector dimension on that, and they basically used the, they used the existing models, right, to crawl through all the common crawl pages, to find the relevant math domain papers, to filter down the data set.</p><p>So, obviously, like, obviously, like, to crawl the entire common crawl is, with AI models is not practical, so what they did is that they, they, they, they require at least a, a match to one, the n-grams of three words, so they used the existing good data set, so the minimum number of word occurrence to three, oops, I can tap that, accordingly.</p><p>And then, and then, and then, and then, and then subsequently, right, subsequently, that resulted in 40 billion pages, and, and then subsequent, and then, and once they started using that, right, they subsequently used this to train the model. So, um, so, um, so, um, yeah, um, and then subsequently, like, when they try to, it covers the training settings itself.</p><p>Um, fairly standard, but what, what, what, what was interesting is that for them, right, they found that the downstream performance, subsequently, when they did train the model, pretty much outperform every other math data set prior to that. And then, uh, if from a token to performance perspective, um, because, because of how they thoroughly deduplicate the, try and, and try to, uh, curate a diverse topic.</p><p>Um, subsequently. Eugene, Eugene, can I stop you there? Yeah. One question. So was this the reason because they got a better data set where they saw the thing blow up? or was it more of that they improved the training part of it and that's why the model started to show up?</p><p>Um, so right now, previously, right, so you have to see, right, this is actually a fairly standard model. This is not a MOE 256 model. So this is just purely data set, uh, from a point of view of the data set. Even the, the parents that they shared previously, it's a fairly standard thing.</p><p>So hypothesis, I mean, this, this one, I'm actually need to go through the data set to double check it. It would be actually for the, one of the reasons why they did the search and scan through the common crawl and filtering, right. For those math concepts, right. Is that, um, if you take existing data, math data sets, which tends to be like, let's say using one of it is a filter set of ARXIV papers.</p><p>It tends to be math data that is a little bit overfitted to a certain framing or style or narration. Well, well, they wanted just a lot more data on math in general. So this is literally the, I guess, in a sense, a diversity and web scaling of the math data set.</p><p>Does that make sense? Yeah. Uh, I think I will need to double check this, but like, it will not surprise me if like, like random reddit discussion about math suddenly appeared in the data. Things like that. So that part I definitely need to fact check because I have, uh, uh, uh, uh, respectively.</p><p>But the idea is that it allows them to go beyond what's the existing sources. Um, this is no different from actually existing. Uh, techniques per se, but I think, I think it's more of like, no one really bothered doing it for math. So focus prior to this. So yeah, that's what I did.</p><p>But, uh, I think the next step, right. Beyond that, right. It's where, where, where, where, where is the important part where it's the, the seat for R1 is here. The, the, uh, problem solving step-by-step reasoning. So, uh, so they, they, they started doing the chain of thought prompts respectively.</p><p>And to, and because math is, uh, easy to validate code wise, like you can validate the generated result matches. They can rinse and repeat that respectively. So, so this, so this is, uh, so this is where the original seat for deep seat R1, which at that point in time, before we call it reasoning, everyone just calls it chain of thought.</p><p>Um, and, and essentially they, they, they prompted the model. They generated the data, they validated it, and then they repeat the process respectively. So this is how they bootstrap R1 essentially. So, um, and then subsequently they cover like, like training, introducing tool use. And then subsequently they prompt the model.</p><p>And then if they get a result, it, then it reinforces itself. You have to understand that this part, right. Well, they did have some humans to like validate and check stuff, right. The big thing here, right. Was that this is fully automated for most parts. So this is how they bootstrap.</p><p>They are reasoning data. Uh, without, I would say at that point in time, because this, um, reasoning models wasn't even a thing yet for open AI. Without even, uh, without needing them needing open AI to generate all this reasoning data for them. Because at the end of day, math is math.</p><p>If the model gets it right, provide formal proof. It succeeds. So they subsequently repeat this with other domains from, from what was understood. Uh, and then that lead to what the current reasoning models that are that we have. But even then they said a lot of it is just based on math and it's scale accordingly.</p><p>Uh, currently there's a coordinator open source effort throughout multiple groups, right. That essentially use this paper as reference. And the deep seek our model to try to create reasoning data sets for math and code. Because code, code is similar to math in the sense that it is easy to validate.</p><p>You can generate the code, you can generate the code, check whether the inputs and outputs are match, pass, rinse and repeat. And yep. And I think what is exciting is that if this is the case, and if, if this is really what is all that's needed to build reasoning data sets for R1.</p><p>Um, at least as claimed by deep seek. It means, right. They literally gave us the blueprint, right. Uh, and the, the rest of the community right now, as you can see on. You can see on cutting face, et cetera. I'm creating a data set. It means an R1 data set will be created.</p><p>If there is an additional secret source that's needed, which the deep seek team claim that isn't. Then it's something that we'll find out in time. So yeah, that's why I said this, this, this, this is kind of like a prerequisite paper to R1. And, um, the, and it helps.</p><p>It literally helps show, right. If you look at the date itself, right. This was when was it again? This was April, 2024. So, so, so they've been, uh, so for a lot of people who keep thinking R1 as a sudden thing, right. These are things that they've been building over time in the background to where it is today.</p><p>Um, I think what is interesting here is that because they, you, the, the way it's done, right. Is you have a hyper specialized model focused in one domain, beats out GPT-4, for example. And then you take that data training on a general, much bigger general model. And then that's where it, it starts learning how to apply in other domains.</p><p>Um, in this case, you can't prompt this math model for medical advice, for example. It never learned medical advice. Doesn't even know what medicine is properly. So yeah. Any questions on this part so far? I'm not sure whether you want to cover GPRO, PPO, but I'm not sure whether that's the most important thing about this paper, actually.</p><p>That's in my opinion. Eugene, I go with 7B. Sorry. 7B. Why, why did they choose 7B? I mean, did they play around and come to 7B? Or they thought that was good enough? Honestly, my gut feel was that because, uh, if you remember this, uh, at Kaggle, right, people already, they were already decent math model at 7 to 12B to 32B size.</p><p>So it's already been proven like a 7B model is good enough. And I think for them, this was just an exercise of just like finding the smallest, cheapest model that they can train that can get the job done at a good enough performance. Um, they might have done abolition or maybe they just picked it.</p><p>I honestly don't know. It wasn't covered in the paper why they picked 7B, but it's an educated guess, I guess. It's, I think it's a reasonable choice. The alternative I would have done would probably be the 12Bs because that was fairly popular as well. The 12 and 14Bs. The 12 and 14Bs.</p><p>The 12 and 14Bs. The 12 and 14Bs. The 12 and 14Bs. The 12 and 14Bs. The 12 and 14Bs. Eugene, I had a, I had a question. I was actually, I haven't read this paper yet. It's on my list. I was actually thinking that they were going to cover.</p><p>They were going to say they had, they use synthetic data generated by like leaner and other theorem proving language. Right? Like I'm really surprised that they didn't leverage mathlib and like all this stuff because there's been a ton of work on theorem proving languages recently. And it seems so obvious to me.</p><p>And I wonder if there's a reason why that's not being done. Do you know anything about this? I don't think we can double check it. But if I remember correctly, they did include other math data sets as well into the mix. So it doesn't mean what, what, what, what, what they did was to basically add more upside the existing data set.</p><p>So I don't think they excluded it. But I'm not sure whether they added those micro languages. So, so like, no, what I mean is, oh, I see. Okay. You could have, they could have used like mathlib itself. Yeah, that's, that makes sense. Okay. It's like a few math data sets that they used previously.</p><p>I apologize that I'm trying to figure out how to best use my phone in this. But like, if, if you told me, okay, RJ, go create a math model. I would, of course, kind of copy what they've done here. But I would also just like have, have, you know, a model generate proofs you like interesting proofs using using lean right.</p><p>Right. And then I would, and, and like sort of use that as synthetic data set to train the model, because I think that, you know, like that, that will show the model how to do like proper proofs. Right, so that you don't have to bake in the logic of the proof into the model, like you're just doing it through the data set, but that you are proving proper reasoning, or you're creating proper reasoning steps that are verifiably correct.</p><p>Yeah. So proof wiki is in, in there. So. Okay. Yeah. Interesting. Okay. It may not be the exact data set that you quoted, but. Yeah. Yeah. Yeah. Okay. Yeah. And maybe proof wiki is at leans on no, no pun intended leans on lean. Okay. Yeah. That makes sense. So, yeah.</p><p>Um, yep. Um, is there any other questions for this, uh, segment for, I think like subsequently the GPRO PPO. I don't see. When I read through it, I don't see something that is like, I'll classify as an innovation. There is just kind of like, we made the data set.</p><p>We taught it how to use tools. We, we, we, we improve on the data respectively. And then. So the, in this case, um, reference model, then got the reward. It, it found out that the proof is valid. Train it again, rinse and repeat. I, yeah, I don't see anything new in here unless I missed it.</p><p>I apologize because I only like skim through the paper in like half an hour prior to this. Yeah. If I made something. Okay. If I made something. Okay. There's a claim that GRPO is much more efficient than PPO. Um, the way I do PPO is have the same network kind of, uh, give you the value function and the policy.</p><p>So I don't know why they claim GRPO is more efficient. Oh no. Yeah. I saw that phrase. I know which one you're referring to. Uh, I would say. Um, if I remember the exact wording was, where were you, you didn't happen to know where the line is. Uh, you just, uh, passed it.</p><p>I mean, they say it's more, uh, effective and more efficient, but I think in more recent papers or in the R1 paper, they push on the efficiency, uh, even more. Um, I think, I think another thing to consider is that when they say efficiency, right? They, they also probably take into account the VRAM overhead and the compute overhead rather than, because here's the, and then when you're compute constraint, right?</p><p>Your, your consideration is not. Um, one thing that strikes me about deep C and, and, and this is very similar to how we operate the other KB group as well, is that we are not research purists per se. If you are research purists, right? You'll be like, Hey, if one token step, PPO versus GRPO, um, PPO is better.</p><p>Then GRPO is worse. Like per token step, like, like, like, like, like if you are research purists per token step, but for deep C and for us, you'll be like, if GRPO takes half the RAM and compute, VRAM compute. Even if it's not as efficient, given my compute budget, it is more efficient.</p><p>You, you, you, you get what I'm saying. So it's a, I don't think it's a contradiction there per se. Um, it's like, it's like, I, the, the struggle here is because they didn't show abolition. This is how I interpreted their claim. They found it more efficient on your hardware.</p><p>And then they proceed with this, um, rather than, rather than, um, this is GRPO is more efficient than PPO given the same amount of steps. Yeah. It didn't sound like. Eugene, I thought when we were covering the, uh, the, the R1 paper that there was some discussion of how they didn't need the value model.</p><p>And that was why it was more efficient. Yeah. If you, if you, if you look at the, the diagram of PPO on, on page 13, so there's that, that value model on the PPO side, that's not on the GRPO. I thought my understanding was that was the thing that they, that a lot ill GRPO allows you to use the average of the group instead of having a training, a value model.</p><p>And that, that was where the efficiency increase came from. Yeah. So, so like, once again, you read based on their language and trick and how they think things through it, significantly reduce, reducing training resource. So, yeah, yeah, it's like, less VRAM. You have to remember that they, they, they are, well, well, they are not GPU, that GPU poor, they are not exactly GPU rich like the big labs.</p><p>So, so these are. My point is in other, in other networks, or, uh, you can kind of use a common trunk for the policy and for the value model and just have two prediction heads. One that predicts the policy, the other one that predicts the value. So like that, you don't always need two models.</p><p>So I wonder if, uh, the usual setup is two models or one model that's multitask for, for, um, this RLHF models. So then the claim that you need just one model here, it's a bit, uh, exaggerated because usually, uh, you may use a multitask model for policy and value.</p><p>I mean, you can see in some older RL papers that, uh, yeah, they have two prediction heads instead of one. Hmm. That's a, that's a good point. And, and, and I think we're entering the speculation area because like, like, likewise also like some of the previous papers I seen, it's two models as well.</p><p>So yeah, maybe you're right. Maybe just, just using single model, uh, two heads or two, even like extra layers and head would have, have similar effect. And we have done that cost savings and, uh, training resource savings. Uh, as to why they never tried that, or maybe they tried it and it didn't work that way.</p><p>We have to ask them. We have to ask them. Yeah. Yeah. Like, so that I think there's one, one thing that is in very, uh, repeated fashion for deep sea. Right. Well, uh, one thing that is always missing, right, is the abolition on how they did things. So if I would have a criticism, which to be fair, it's not fair to them because everyone else kind of like releases even less details than them.</p><p>Uh, is that they'll be like, we did this. This is what worked. This is the path. Then, then when you, then when I have these questions, like, why didn't you try that? Then we don't know whether they did. There's a problem. Yeah. Probably the other thing with Oniki, when he said about the mathlib and lean, deep seek prover actually used it later on.</p><p>So that's, that's what it says. Okay. Um, this, it wasn't meant to be a deep paper. So, uh, is there anything else that we want to cover? Yeah. Can I just check my, my understanding? Oh, whoops. Yeah, sure. Um, so even if you had a single model with multiple heads, you'd still have to do the computation and the training on it.</p><p>Right. So it would save something there. Correct. V remedies. Yeah. It should also save, uh, compute. It would also save VRAM, but I think it should save some compute. Am I misunderstanding? I mean, usually when you have multi-task, the compute part is like two prediction heads that are different.</p><p>So very, like compared to the big trunk of, of running it. Okay. It should be. The head is a small number of parameters relative to the size of the, but, but it seems my understanding is wrong. Cause they, they claim. So I would do it with a multi-task and I saw other older papers, non LLM set up where they use two prediction heads.</p><p>So I was wondering why they haven't tried that here. Or if that's not the way to do it in LLMs or in RLHF. Okay. I'm willing to be willing to say it could very well just be the case that they never tried. So we have to ask them, like, I think this is generally, but conceptually, it will save VRAM as you're on.</p><p>The data goes in. Yeah. It just goes through. They don't need to backward pass this. So yeah, it, it, it, it isn't that much more compute as well. Okay. It does seem, I'll just say amazing that we've got a whole model in one case. And then in GRPR, they're like, let's just do a Monte Carlo average.</p><p>Uh, it just seems like, oh yeah. Why didn't, why didn't somebody else think of that? Um, but anyways, uh, we can, we can move on. Yeah. It seems they can run for the same prefix. They run more completions and then they get the average. My understanding is that in PPO, you just run one completion, for example, and then you score that completion.</p><p>But if you run 128, uh, things at the same time, uh, maybe you, you get a like more signal. So, so there's some trade-off in like, uh, preusing the prefix and generating more things versus, uh, just using one prefix at a time and scoring it. Um, and I, I don't have a mental model what the trade-offs are between the two.</p><p>That was going to be my, my question. Okay. Thank you. Yeah. But I think down here is just more of like, for each prefix, there's a multiplier effect. Well, in PPO, you have multiple individual prefix, but at the end of the PPO is still trained in batches. I mean, if not, I'll be too slow.</p><p>So, so it's just, yeah, I think it's just about, uh, it still needs to go through. But here they have a batch of, uh, a few prefixes and then for each prefix, they generate, um, a few outputs. Uh, so it's, it's kind of like, I don't know. I used to work in recommender systems and we would have point wise learning where we would learn for, from one label, um, like one click or one not click.</p><p>And then, uh, um, list wise learning where we look at the entire request and see which result was clicked and propagate based on a set of results. Uh, so it's kind of a same, like in group group RPO, you, you propagate signal from more, more things, some positive, some negative, and you also average them out.</p><p>Um, but yeah, it's kind of interesting to think about, but I haven't figured out exactly why it's one is much better than the other. All right. Okay. This does give me a little bit to think about. Yeah. Cause I, I, I did never really considered it too much because for, I think for, for me as well, like for us, right.</p><p>We always tend to lean in on if it's cheaper resources, all we care about actually. Yeah. So yeah. Okay. I'll move on to the next paper. Um, forgive for the buggy switches. Okay. Let's see. Yep. Yeah. Anyway, for those who's wondering, my laptop is like in an infinite log in loop.</p><p>We've, we've, um, yeah. Eugene. Uh, I, I'm sure you are not aware of that, but, uh, the speaking attendees picture that keeps popping in front of us, uh, and hiding. Oh, I can get it. Get rid of it now. Yeah. Thanks. Okay. So I shared how you can block it, uh, with the options.</p><p>Yeah. Oh yeah. Uh, it's just that I have no means to open that without, without sharing the screen. Yeah. Okay. Uh, even while you're sharing, you can go down to the options more and, uh, select the host option and, uh, hide the, uh, the profile pictures. But anyway. Yeah.</p><p>Are you able to see the mama and the llama picture, uh, right now? Just making sure. Yes. Okay. All right. So, um, so deep, deep, deep R1, um, like I said, there are multiple teams trying to replicate the data set based on that map paper and then subsequently on the R1 model itself.</p><p>Um, um, so that is like one road to reasoning data set with open AI. Um, I'm covering the, the other people I'm covering is mamba in the llama because, um, for those who don't know, uh, we did a similar process, uh, modified based from, uh, based on this paper.</p><p>Um, the authors of this paper are very well aware about what we did because, uh, we collaborated notes, uh, for upcoming paper as well. Um, and, and essentially we made, this is not the name on the paper, the RwKV in the llama or in this case, the RwKV in the quen.</p><p>Uh, so, uh, so using several of the concepts from this paper and we, we modified respectively, uh, how to modify an existing transformer model to a linear model. Um, we present, we, we launched our 32B model at, uh, at Europe and a little bit of a spoiler. We are launching our upcoming 72B hybrid version of using the, uh, altered version of the techniques that we did previously, uh, in the next few days.</p><p>So that's why I'm going through this paper. Um, okay, so for most parts, there's a lot of things in here, right? Literally because this paper is talking about retraining a model. We are retaining most of its benefit at a really low budget, uh, and changing the attention mechanism, right?</p><p>Um, you can actually replace, search and replace Mamba with RwKV in concept. You can search and replace it with XLSTM and Titan and et cetera, because conceptually the technique will work the same. So, uh, so the idea, the idea here is that, uh, the idea here is that you, uh, you take an existing, uh, transformer model.</p><p>And essentially you freeze the feed forward network layer because, uh, where were you, where were you? This is explanation. What's the different RNNs and transformers, but okay. Here it is. Okay. Okay. So at the end of the day, right? Uh, all, all transform models are attention layers, feed forward network layers.</p><p>Uh, then you have the embedding and then the, uh, the, uh, the output hits. And, and subsequently, right? Subsequently, right? Um, um, what you can do is take an existing pre-trained transformer. There's, let's say 15 trillion tokens trained. Freeze the feed forward network layer. So this is similar to what they did.</p><p>Hence the ice there, the freeze logo. Uh, and then subsequently you delete the attention and you, you, you delete the attention layer and then subsequently you, you add in the, the Mamba layer and then you train it respectively. Um, and what we realized, um, based on base, uh, when we, when we did the same thing for RWKB is that this is actually incredibly cheap and fast by transformer, by AI training standards.</p><p>To put into illustration, right? Our upcoming 72B model, right? Well, we started with two MI 300 nodes. Uh, we essentially made improvement to the VRAM technique stuff. Basically we optimize our training code to the point where we ran the whole training process. Again, we are trying to replicate the model on a single node.</p><p>So we were able, so we were able to successfully convert, uh, a transformer model. Be a 32E and 72B to a linear model with, with some loss in MMLU. But given the amount of tokens that we trained was like, it was like, it was only a few hundred millions, right?</p><p>There were, it was, there was no way for it to actually learn new knowledge given the very limited dataset. So one of the, one of the derivatives for this paper and, uh, and based on, uh, likewise in our replication was that. It's actually leaning into the conclusion right now that majority of your knowledge for the lack of better words, right?</p><p>Is actually only in this peak forward network layer. And the attention layer is just more of like. Being able to pay focus on things. You know, uh, to quote one of our teammates, right? The way they, the, the way the frame is like, if the FFN is the knowledge, right?</p><p>Then the attention layer is just, it's just basically the part of the brain that lets them, uh, pay focus. And when they reset the model, right? It's as if the person has ADHD and just can't focus. So, so us doing that limited training is basically fixing the nerves, uh, to, to essentially allow them to pay focus again.</p><p>And, and that's why we, uh, that's why it's something similar to here below. I believe they did cover MML, uh, the scoring. No, so speculative decoding. I'm going to skip the speculative decoding part because that's, that's not the point of this paper. Uh, is that they subsequently showed that in, in, uh, as they convert it and then, um, with different ratios of.</p><p>Linear layers to, to, uh, to transformer layers. It's able to actually retain quite a large percentage of the, uh, uh, transformer level performance. Um, so this is empty bench. Um, like for example, you look at the Lama empty bench. Um, the baseline is it. Uh, if, uh, and then subsequently as, as the supply as this pricing is, is, is around 7.7.</p><p>Um, for, and likewise, we have seen similar things. Um, I think the better example is below. So for example, 50% 7.32, 6.7, 4, 6.48, 5.64. Um, 5.64 is basically pure Mamba. Uh, that is so, well, there is a slight performance degradation, right? There, uh, there is a question of like, why do we bother them?</p><p>Because this makes the model slightly dumber. I think the key thing here, right. Is it's all about trade offs here. Um, one, um, one, right. Is that, well, it is slightly dumber. The computational cost is substantially cheaper. So, so for example, right. Even at, even, even at, uh, even at like 50%, right.</p><p>Uh, at 50%, uh, mixed transformers to, to, uh, to, uh, basically state space. State space, rwq, whatever it is, right. We are looking at easily cutting the VRAM requirement by close to half. Because essentially the RNN models, especially over longer context length, right. Their VRAM requirements are essentially rounding error for, for the layers.</p><p>Another thing that, which is not covered in this paper, which we actually managed to further dive deeper into, right. So what we realized is that in, in a train transformer layers, because we experimented by replacing different layers one at a time, and basically Frankenstein in various way, right. Long-term attention, right.</p><p>Is actually mostly done by the upper layers. Well, the lower layers, um, don't actually impact much on long-term attention. So once we realized that, right. It means that the lower layers, right. You can safely do the conversion, right. We are still maintaining stronger, uh, strong, uh, um, stronger longer-term attention, uh, scores.</p><p>And this is reflected, right, in this paper as well. Uh, likewise. When we go further down in, in here. So if you see, if you see respectively, right. Uh, this is the very classic needle in the haystack test, right. Um, uh, needle in the haystack test, right. Even, even with their, their, in one of their models, right.</p><p>Even though it was converted with 50% train, right. Um, it was able to, for most parts, pass the test. I know there's like some issues here. Um, what we realized, right. And this, I don't think it was covered in the paper, right. It was that, was that what we realized during our replication, right.</p><p>Is that these segments, right. Well, a lot of it looks bad, right. When you actually look at the exact needle in the haystack result, right. If the model was doing things like, for example, the test number was. Like 2001, the model was answering 2000. It was like getting like slightly off by one's off by one digit errors, uh, kind of thing.</p><p>Um, which, which was promising because, uh, what, what, what it meant was that it means there was a little bit misalignment during the conversion process, which is to be expected because we literally Frankenstein these things together. So one of the things that we did in addition to, to all of this, right.</p><p>Was that after we did the conversion, right. Is we did additional training to, for the lack of better words, we cue the layers together. And that actually improved the needle in the haystack performance respectively. Whether that will be replicated in Mamba, that one, um, we'll leave it to them after we share our details with them again.</p><p>So, so, so there's a little bit of back and forth between, between both groups right now, actually when, when it comes to like these kind of details respectively. So, yep. Uh, and that's about it. Yep. So it's exciting because if we all, if everything we hear work as planned, um, this technique conceptually will apply to any transformer model.</p><p>And even in the, even if let's say you want to retain most of the performance, you could literally just convert half. And we've no substantial loss. And then subsequently have that savings in compute. So it just means potentially just cheaper inference in the future for all models, including potentially the deep sink model, et cetera, et cetera.</p><p>Yeah. Yeah. Yeah. Any questions on that? So yeah, you didn't mention it might be in the paper though. So why so few, uh, tokens for training? Like that wouldn't, it wouldn't help to improve the performance on MMLU or whatever. if you just trained for longer? Um, I'm certain because it's just, especially in the case of like the larger models.</p><p>Uh, at least when we were explaining for the 2B, we just didn't have the budget. Oh, okay. Sorry. Okay. So you, you trained as many tokens as you could. Yeah. So, um, we did, we did, so we did train on additional tokens. It didn't get better. Very slightly. Uh, the, the, the long context range performance also got better.</p><p>And then we drew a trajectory and I was like, okay, this is going to take forever, which is like, to be fair. Right. That is how it's supposed to be. So it's. Yeah. Yeah. So it's just a more of a shortcut for like, if you want to view it this way, right?</p><p>This in a way it shortcuts. I'm doing very roughly. So if I say, you know, the, the training curve of 15 trillion tokens, this allows us to shortcut the first one quarter of the 15 trillion tokens to half depending on the percentage ratio. Okay. Yeah. Yeah. Yeah. So did you, I are, is, does your paper that you're about to release, does it have that training curve in it?</p><p>I'd be super interested in that. Uh, so yeah, that paper hasn't been written yet. And that's why I'm reading this paper. So don't ask me, can I cover the paper? I was like, okay. Yeah. Can I just cover what I'm reading? Because I'm reading it anyway. Yeah. Did that make sense?</p><p>Yeah. Yeah. Yeah. Awesome. Thank you. Yeah. Yeah. So how, Eugene, how does this work? You just replace, um, the attention layers with, with kind of linear layers and the linear layers are randomly initialized and you start from scratch and, uh, they learn pretty fast. Yeah. Yeah. The most important step is you need to freeze the feed forward network layer.</p><p>At least for the first stage of training. Um, so, so I, I think what we extended on is that we, we, we changed the process into different stages. Um, and we found that to be more reliable. Um, and we are quite certain about this because we, we haven't read repeated this process at least a hundred times on, on the seven beat model at this point.</p><p>So, so, so, so, so, so what we did is that we freeze the feed forward network layer similar to Mamba and the Lama. We train the attention layer, uh, attention layer. But the trick is that instead of training on the whole model input output, you just, you just, you just train on the output for the original attention layer.</p><p>So it's like, it's essentially like you just do that. One layer you forward, take the output, take that instead of training on the logit. So, so your layers are previous attention activations. So you're, you're trying to match the full attention with the linear attention. Yeah. We're trying to match the hidden state output.</p><p>Essentially. And it work. If anything, right. Between both teams, we were like, this works way too fast. And, and, and right now, right now, right. For us, right. The beauty for this technique, right. Is that because we don't need to invest a huge amount of money into the training, the FFN layer from scratch.</p><p>This actually allows us to iterate on the attention layer experiments on the cheap. I'll just use air code cheap here because we're still, to be fair, we are still using an MI 300 server. So, so it's not exactly that cheap, but it allows us to iterate without doing the full 2 trillion token strain in a very rapid fashion.</p><p>So, and, and yeah, we have modified the RWKV mechanism in our upcoming hybrid model because we found it to be better. Yeah. It just, it just allows a lot more experimentation. I think that's one thing that was exciting about, about this technique when it comes to experimenting with linear attention.</p><p>And yeah, I do expect newer space, uh, attention mechanism as well. In, in terms of data distribution, do you have to train on some specific data or you're like, you're using some random sample or cause I, I, I guess the attention path, it might matter to match the attention patterns of the pre-trained, uh, data set.</p><p>Yeah. Yeah. So that's the tricky thing because we, so both, I think both of us, we use DCRM, which right now is considered like quite a good data set. That's refined. fine. Um, but the issue for, for me was that at the end of the day, we have absolutely no idea.</p><p>What, what's the exact data composition from Lama and Quen? So there could be better. They are sets. It's just, I don't know. It makes sense. Thanks. Yep. By the way, the view, the, I'm still on screen. Yeah. Yeah. I only put it open because I was trying to respond to you.</p><p>Yeah. All right. Yeah. Is anyone else? Any more questions? I got 10 minutes. I guess. Yeah. Okay. So it looks like I will just keep in the bonus paper, which is, we'll not see too much time. Okay. The reason why, why I say this is an, uh, to me, this is a rather exciting paper in a sense that this is the first large scale text diffusion model, uh, at seven B in size, but it's also a very boring paper because it's just basically we scaled up image, the, the, the scaling expectation and it works.</p><p>So, so yeah, um, for those who don't know about text diffusion models. Um, the general idea, right. Is similar to like image diffusion where, where you have your prompt and response. Um, sorry, uh, your image, right. But then, but then like it, token by token, fill in, fill it in, but it doesn't necessarily need to fill it in, in sequence.</p><p>Uh, in fact, I think, I think the, I get hard, right. Has an example where, where they literally just showed. Um, I, I don't do that. I have it in here and I can't, I can't remember where, where the guitar is, but they showed, right. The text appear appearing at different, different times, not necessarily in linear order.</p><p>I think the big, the big thing is that for, for this, right. Cause I'm, I'm, I'm, I'm just always trying to shout out alternatives. Be it to RWKB to maybe this text diffusion architecture will end up killing RWKB anyone else. Um, is that, is that it worked and, and, and then subsequently now they are experimenting more into like scaling into different directions.</p><p>One thing that is interesting about this technique, right, is because since it goes in a rare repeated activity fashion, it did much better than all existing architecture, right. On the reversing test. So for those who don't know the reversing test is that I asked you to put in, put in a text and then I asked it to reverse the text.</p><p>Uh, a lot of models tend to do it badly in part because reversing tokens is not an easy task as well, because it depends on your tokenizer, but the diffusion models, right. The diffusion model, according to this paper, because they never released the weights. So we can't verify. Didn't have any issues with it.</p><p>And, and that was one of the things that they observed beyond that, uh, until they released the weights. Um, um, I am not, I don't really have much reasons to doubt what they put in the paper here because they were very transparent. Some things did bad. Some things did good.</p><p>They did. Uh, and, and, and yeah, it's a foreign paper beyond that other than the fact that it works. So. Yeah. The reason why I'm excited over text diffusion and this is my extra thing is that which they didn't do in this paper. Was was the fact that let me go back to zoom and stop screen share.</p><p>Is that? Is that my speculation on text diffusion, right? Is that? Is that if we do it right and we, we, we scale it correctly, right? Um, we, we can start testing multi-epop training. Um, why I think that's important is that if you remember the previous, all the, all existing transformers, linear models, whatever.</p><p>We have the same weakness that when we train on the same data set multiple times, our models are first. The multi-epop problem. If you are familiar with image models and diffusion models, that group trains over the data set thousands of times and the law still goes down. And there's something about it that we don't understand that allows that class of models to do it.</p><p>And for me, text diffusion models might be a way to unlock the understanding of how can we do multi-epop training? Because if anything, humans are fairly multi-epop trainable. Uh, if anything, we train by repetition sometimes as well. So, so this to me is a important research step in overall to figuring that out.</p><p>Um, yeah. Sorry. Okay. The question to Eugene, when you do the multi-epop training, can you always say that it improves or it could suffer also? Is there a way to say that you're doing better versus not better? Yeah. It's loss benchmarks. Um, it tends to end up creating a weird overfitting problem for transformer or even linear models.</p><p>In our internal test, which I think some people disagree with is that we found that for most transformer models, it tends to break after three or four epochs. For RWKV, it tends to break after like six, seven epochs. Not really a big win because it's like, um, it's like, yeah, it's still breaking.</p><p>Unlike diffusion model, thousands of epochs. So that's to me, to me, the real goal is to really like go to that like much larger direction. It also will make it easier for, uh, if we can get this right, right? It'll make it substantially much easier for like fine tuning task related for domain specific or even like company specific stuff.</p><p>Cause if you can retrain it a thousand times without any issue, right? When you're trying to refine the model, you can just add data, retrain on your company data again. And yeah, sure. It's not a problem. And that's exactly where I was coming from. Yeah. It's one of the things that for example, you train on your company data.</p><p>And then I say your company policy half change or only few paragraphs. I mean, and then you, you train it again. And the model goes crazy. It's like, it's a very annoying experience for, for, for fine tuning. Yeah. Oh yeah. Okay. How are these diffusion models different from BERT?</p><p>How are these text diffusion models different from BERT? I think the key thing for the text diffusion model is, one when the attention mechanic is applied, right? For the individual token. So, so you can like freeze the prompt. So it's like prompt then completion, right? So imagine it as like pixels in the diffusion model, like your traditional diffusion model, right?</p><p>These pixels or these tokens, right? It's allowed to pay attention in all directions. So that's my speculation. It's just probably why it's able to do the reversal test very well, because it's able to like, through multiple steps. Hey, the text is that character. Then I can reverse it like pairwise in, in a sorting direction.</p><p>Yeah. Uh, uh, uh, um, that part didn't really surprise me. And yeah. Um, so I think, I think that's the big benefit. Great benefit there. It's, um, other than that, it's essentially just like more towards transform on diffusion models with just a much wider attention range. If that makes sense.</p><p>Yup. But as a set threshold for masking, uh, and text diffusion is an activity process, I think. Yup. I think that's one way, one way to frame it as well. Correct. Um, yeah. It's sorry. I want to be in self drive remote. Oh, because I put my phone by the side.</p><p>Yeah. No, no. Okay. Sorry. Yeah. So I think like what to be clear about this diffusion model, right? Um, diffusion text models is not a very popular path in particular because you need to go through that multiple iterative steps. Like, so like if you want to generate, like, my 30, um, the whole, or completion, right?</p><p>You need to like, like, like expert diffusion to a hundred steps. So that doesn't sound great. Um, but if I say your completion is, uh, is more than a hundred tokens, right? It might actually be more efficient than a transformer model. The issue at hand is like, sometimes when you're doing answers like chat GP star, you don't actually know the amount of tokens.</p><p>Yeah. Okay. Yup. So yeah. Um, yeah, I think we are towards the end of the session, uh, for, for everyone else. Um, next week. Um, this week is a bit hectic because like a lot of the key members for the, this is a space discord. It's kind of like in New York, including me, uh, for, for the latest, uh, AI engineer summit.</p><p>Um, if any of you here, shout out, uh, we'll be probably glad to meet you around here at New York. Um, beyond that, beyond that, uh, beyond that, um, for next week, if anyone has any papers they want to volunteer to, please, please draw a message into, into the channel.</p><p>Um, I saw some people edit stuff into slido. Um, yeah, but you edit yourself as anonymous and saying, you might be able to, to probably cover the paper. Cover the paper. I have no idea who you are and just like, because you are anonymous. So yeah. If anyone wants to cover the papers that they edit this title, please do so as well.</p><p>Uh, yeah. Same time next week. Have a good day as well. Thanks Eugene. All right. Thanks Eugene. All right. Thanks Eugene. Thank you. All right. Thanks Eugene. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p></div></div></body></html>