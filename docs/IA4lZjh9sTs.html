<html><head><title>Pipecat Cloud: Enterprise Voice Agents Built On Open Source - Kwindla Hultman Kramer, Daily</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Pipecat Cloud: Enterprise Voice Agents Built On Open Source - Kwindla Hultman Kramer, Daily</h2><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs"><img src="https://i.ytimg.com/vi_webp/IA4lZjh9sTs/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./IA4lZjh9sTs.html">Whisper Transcript</a> | <a href="./transcript_IA4lZjh9sTs.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=14" target="_blank">00:00:14.820</a></span> | <span class="t">Hi, everybody.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=15" target="_blank">00:00:15.660</a></span> | <span class="t">My name is Quinn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=16" target="_blank">00:00:16.520</a></span> | <span class="t">I am a co-founder of a company called Daly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=18" target="_blank">00:00:18.360</a></span> | <span class="t">Daly's other founder is in the back there, Nina.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=21" target="_blank">00:00:21.040</a></span> | <span class="t">I'm stepping in for my colleague, Mark, who couldn't make it today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=23" target="_blank">00:00:23.640</a></span> | <span class="t">So we're going to do this fast and very informally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=26" target="_blank">00:00:26.320</a></span> | <span class="t">But I think that's a good way to do it at an engineering conference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=28" target="_blank">00:00:28.700</a></span> | <span class="t">I don't have as much code to show as the last awesome presentation, but I'll try to show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=32" target="_blank">00:00:32.420</a></span> | <span class="t">a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=33" target="_blank">00:00:33.420</a></span> | <span class="t">We're going to talk about building voice agents today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=36" target="_blank">00:00:36.800</a></span> | <span class="t">I work on an open-source, vendor-neutral project called PipeCat, and a lot of other people at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=41" target="_blank">00:00:41.300</a></span> | <span class="t">Daly do, too, because voice AI is growing fast and is super interesting and is a good fit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=46" target="_blank">00:00:46.000</a></span> | <span class="t">for what we do as a company.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=47" target="_blank">00:00:47.820</a></span> | <span class="t">We started in 2016.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=49" target="_blank">00:00:49.160</a></span> | <span class="t">We are global infrastructure for real-time audio, video, and now AI for developers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=55" target="_blank">00:00:55.360</a></span> | <span class="t">PipeCat sits somewhere higher up in the stack than our traditional infrastructure business.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=60" target="_blank">00:01:00.080</a></span> | <span class="t">So we'll talk a little bit about how you can build reliable, performant voice AI agents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=65" target="_blank">00:01:05.800</a></span> | <span class="t">completely using open-source software.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=67" target="_blank">00:01:07.800</a></span> | <span class="t">We also recently launched a layer just on top of our traditional infrastructure designed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=72" target="_blank">00:01:12.920</a></span> | <span class="t">for hosting voice AI agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=74" target="_blank">00:01:14.680</a></span> | <span class="t">We'll talk just a little bit about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=76" target="_blank">00:01:16.800</a></span> | <span class="t">So we've been doing this a long time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=78" target="_blank">00:01:18.520</a></span> | <span class="t">We care a lot about the developer experience for very fast, very responsive, real-time audio</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=82" target="_blank">00:01:22.520</a></span> | <span class="t">and video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=83" target="_blank">00:01:23.520</a></span> | <span class="t">We have a long list of engineering first we're proud of, but that's not why you're here today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=88" target="_blank">00:01:28.800</a></span> | <span class="t">Happy to talk about that later, though.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=91" target="_blank">00:01:31.780</a></span> | <span class="t">If we step back and orient a little bit, what are you doing when you build a voice agent?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=96" target="_blank">00:01:36.520</a></span> | <span class="t">I tend to sort of orient people with three things they have to think about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=100" target="_blank">00:01:40.500</a></span> | <span class="t">You've got to write the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=102" target="_blank">00:01:42.840</a></span> | <span class="t">You have to deploy that code somewhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=106" target="_blank">00:01:46.040</a></span> | <span class="t">And then you have to connect users over the network or over a telephony connection to that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=111" target="_blank">00:01:51.200</a></span> | <span class="t">agent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=112" target="_blank">00:01:52.200</a></span> | <span class="t">A few things here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=114" target="_blank">00:01:54.260</a></span> | <span class="t">User expectations are high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=115" target="_blank">00:01:55.760</a></span> | <span class="t">Voice AI is new, but it's growing fast, I think, because we're able to with sort of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=119" target="_blank">00:01:59.560</a></span> | <span class="t">best technologies that are just now becoming available to meet user expectations, but users</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=124" target="_blank">00:02:04.900</a></span> | <span class="t">expect the AI to understand what they're saying, to feel smart and conversational and human,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=130" target="_blank">00:02:10.740</a></span> | <span class="t">to be connected to knowledge bases, to have actual access to useful information for whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=135" target="_blank">00:02:15.320</a></span> | <span class="t">they are doing for that user.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=137" target="_blank">00:02:17.700</a></span> | <span class="t">To sound natural, there's definitely an uncanny valley problem that in generative AI we fell</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=142" target="_blank">00:02:22.860</a></span> | <span class="t">into for a very long time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=144" target="_blank">00:02:24.360</a></span> | <span class="t">Now we're on the other side of that for voice AI, which is really exciting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=148" target="_blank">00:02:28.840</a></span> | <span class="t">The agents have to respond fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=151" target="_blank">00:02:31.000</a></span> | <span class="t">Humans expect -- it varies by language and by culture and by individual, but roughly speaking,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=156" target="_blank">00:02:36.140</a></span> | <span class="t">humans expect a 500 millisecond response time in natural human conversation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=160" target="_blank">00:02:40.440</a></span> | <span class="t">If you don't do that in your voice AI interface, you are probably going to lose most of your normal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=165" target="_blank">00:02:45.520</a></span> | <span class="t">users.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=166" target="_blank">00:02:46.520</a></span> | <span class="t">So we tell people target 800 millisecond voice-to-voice response times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=169" target="_blank">00:02:49.920</a></span> | <span class="t">That's not easy to do with today's technology, but it is definitely possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=173" target="_blank">00:02:53.640</a></span> | <span class="t">And build UIs very thoughtfully to understand that humans expect fast responses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=178" target="_blank">00:02:58.120</a></span> | <span class="t">The other thing that's hard, a little bit like fast response times, is knowing when to respond.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=185" target="_blank">00:03:05.420</a></span> | <span class="t">Humans are good but not perfect at knowing when somebody we're talking to is done talking,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=190" target="_blank">00:03:10.680</a></span> | <span class="t">and when we should start talking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=192" target="_blank">00:03:12.280</a></span> | <span class="t">Voice AI agents are not as good at that yet, but they're getting better, so we'll talk a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=195" target="_blank">00:03:15.880</a></span> | <span class="t">tiny bit about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=197" target="_blank">00:03:17.420</a></span> | <span class="t">So why do developers use a framework like pipecat instead of writing all the code themselves?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=202" target="_blank">00:03:22.080</a></span> | <span class="t">Well, a little bit of it is all those hard things on the previous slide that you probably don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=205" target="_blank">00:03:25.980</a></span> | <span class="t">want to write the code for yourself if you're mostly thinking about your business logic and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=210" target="_blank">00:03:30.080</a></span> | <span class="t">your user experience and connecting to all of your systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=213" target="_blank">00:03:33.340</a></span> | <span class="t">We want to use battle-tested implementations of things like turn detection, interruption</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=219" target="_blank">00:03:39.140</a></span> | <span class="t">handling, context management, calling out to other tools, function calling in an asynchronous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=225" target="_blank">00:03:45.100</a></span> | <span class="t">environment, all that stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=227" target="_blank">00:03:47.720</a></span> | <span class="t">So developers tend to use frameworks these days for lots of agentic things they do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=232" target="_blank">00:03:52.440</a></span> | <span class="t">And voice AI, I think, is even more important to sit on top of really well-tested infrastructure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=238" target="_blank">00:03:58.860</a></span> | <span class="t">and code components than even in other domains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=242" target="_blank">00:04:02.820</a></span> | <span class="t">Pipecat appeals to developers because it's 100% open source and completely vendor-neutral.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=247" target="_blank">00:04:07.240</a></span> | <span class="t">You can use it with lots of different providers at every single layer of the stack that pipecat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=251" target="_blank">00:04:11.320</a></span> | <span class="t">enables.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=252" target="_blank">00:04:12.620</a></span> | <span class="t">For example, there's native telephony support in pipecat, so you can use pipecat with lots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=256" target="_blank">00:04:16.440</a></span> | <span class="t">of different telephony providers in a plug-and-play manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=259" target="_blank">00:04:19.400</a></span> | <span class="t">You can use Twilio, for example, which a lot of developers know if you're in a geography</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=264" target="_blank">00:04:24.580</a></span> | <span class="t">like India where Twilio doesn't have phone numbers, you can use Plevo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=267" target="_blank">00:04:27.520</a></span> | <span class="t">A bunch of other telephony providers are supported.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=271" target="_blank">00:04:31.200</a></span> | <span class="t">There's a native audio smart turn model that's completely open source in pipecat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=275" target="_blank">00:04:35.580</a></span> | <span class="t">So the community has gotten large enough that there's kind of cutting-edge ML research,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=279" target="_blank">00:04:39.280</a></span> | <span class="t">at least in the small model domain coming out of this open source community, which is really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=282" target="_blank">00:04:42.820</a></span> | <span class="t">fun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=283" target="_blank">00:04:43.820</a></span> | <span class="t">Pipecat Cloud, I think, is a really nice advantage for the pipecat ecosystem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=288" target="_blank">00:04:48.120</a></span> | <span class="t">It's the first open source voice AI cloud sort of built from the ground up to host code that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=293" target="_blank">00:04:53.300</a></span> | <span class="t">you write but that is designed for the problems of voice AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=297" target="_blank">00:04:57.440</a></span> | <span class="t">And pipecat supports a lot of models and services that count to something like 60 plus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=302" target="_blank">00:05:02.300</a></span> | <span class="t">All the things you would want to use in a voice AI agent are probably in pipecat main branch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=309" target="_blank">00:05:09.300</a></span> | <span class="t">So you probably don't have to write code to get started, though the appeal is that you can write lots and lots of code if you want to so there's no ceiling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=317" target="_blank">00:05:17.480</a></span> | <span class="t">I'll talk a little bit about what the architecture looks like and we probably won't have time to talk about client SDKs because most of you in this room are probably building for telephony use cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=326" target="_blank">00:05:26.480</a></span> | <span class="t">But there's a really rich and growing set of JavaScript, React, iOS, Android, client side components and SDKs that people in the pipecat community are using to build multimodal applications that run in the web and on native mobile platforms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=342" target="_blank">00:05:42.660</a></span> | <span class="t">So we talked about this, so I will actually just skip this slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=346" target="_blank">00:05:46.800</a></span> | <span class="t">I hope we'll have time for Q&A.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=347" target="_blank">00:05:47.840</a></span> | <span class="t">That's the most fun part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=350" target="_blank">00:05:50.200</a></span> | <span class="t">Here's the other piece that often helps orient people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=352" target="_blank">00:05:52.840</a></span> | <span class="t">This is what a pipecat agent looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=355" target="_blank">00:05:55.800</a></span> | <span class="t">So you're building a pipeline of programmable media handling elements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=361" target="_blank">00:06:01.240</a></span> | <span class="t">These are all written in Python, although lots of the performance sensitive ones bottom out in some kind of C code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=369" target="_blank">00:06:09.520</a></span> | <span class="t">It's pretty common in real time media handling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=372" target="_blank">00:06:12.800</a></span> | <span class="t">You probably don't have to worry about that level, though.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=374" target="_blank">00:06:14.900</a></span> | <span class="t">You're probably just thinking in Python.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=378" target="_blank">00:06:18.600</a></span> | <span class="t">Pipcat pipelines can be really simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=381" target="_blank">00:06:21.040</a></span> | <span class="t">They can have just a couple, maybe just three elements, something for the network, something that's doing some processing and something that's sending stuff back out the network, or they can be quite complicated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=389" target="_blank">00:06:29.580</a></span> | <span class="t">And we see enterprise voice agents often become quite complicated because they're doing complicated things and connecting out to a large variety of existing legacy systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=400" target="_blank">00:06:40.600</a></span> | <span class="t">So example of a little bit of that span, the left two screenshots here from the pipecat docs about how you work with the open AI audio centric models in pipecat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=411" target="_blank">00:06:51.040</a></span> | <span class="t">Open AI gives you a couple of different shapes of models and APIs that you can use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=415" target="_blank">00:06:55.580</a></span> | <span class="t">One is chaining together transcription, large language model operating in text mode, and voice output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=421" target="_blank">00:07:01.580</a></span> | <span class="t">The other is using their new and, in some ways, experimental speech-to-speech models, which are also really awesome and promising.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=430" target="_blank">00:07:10.120</a></span> | <span class="t">You can do either of those approaches in pipecat just by changing probably three or four lines of code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=436" target="_blank">00:07:16.120</a></span> | <span class="t">On the right is the Python core chunk of a few hundred lines of Python code and a flow diagram for a more complicated pipeline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=446" target="_blank">00:07:26.660</a></span> | <span class="t">This is one of my favorite starter kit examples for pipecat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=450" target="_blank">00:07:30.200</a></span> | <span class="t">It uses two instances of the Gemini multimodal live API in audio native mode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=457" target="_blank">00:07:37.200</a></span> | <span class="t">And one is the conversational flow, and the other is another participant in the conversation that plays a game with the user.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=465" target="_blank">00:07:45.620</a></span> | <span class="t">So there's sort of an LLM as a judge pattern here, but in the context of a game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=469" target="_blank">00:07:49.720</a></span> | <span class="t">And you're moving the audio frames around through both pipelines selectively, depending on the results of the real-time inference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=476" target="_blank">00:07:56.760</a></span> | <span class="t">which is a pattern we also see in enterprise use cases, but it's fun to clone this and run it and play the game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=483" target="_blank">00:08:03.260</a></span> | <span class="t">We listed some of the services here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=486" target="_blank">00:08:06.780</a></span> | <span class="t">We can talk a lot more if you want to in the Q&A about sort of what we see people actually using in production most often in terms of models and services in enterprise voice AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=497" target="_blank">00:08:17.580</a></span> | <span class="t">So that's a very quick rundown of the pipecat framework, which is how you write the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=504" target="_blank">00:08:24.220</a></span> | <span class="t">Now, how do you deploy it, and why am I talking to you about pipecat cloud today?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=508" target="_blank">00:08:28.520</a></span> | <span class="t">There are a bunch of hard things about voice AI that are unique to these use cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=513" target="_blank">00:08:33.220</a></span> | <span class="t">These are long-running sessions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=515" target="_blank">00:08:35.340</a></span> | <span class="t">They have to use network protocols that are designed for low latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=520" target="_blank">00:08:40.140</a></span> | <span class="t">things like auto-scaling are not available out of the box for these workloads the way they are for something like HTTP workloads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=527" target="_blank">00:08:47.780</a></span> | <span class="t">So I was actually quite resistant for a long time to building anything commercial around pipecat at daily, because we do the low-level infrastructure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=537" target="_blank">00:08:57.200</a></span> | <span class="t">We already have things that we do that serve the pipecat community.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=540" target="_blank">00:09:00.700</a></span> | <span class="t">But it got to the point where a very large percentage of the questions in the pipecat Discord were about how to deploy and scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=547" target="_blank">00:09:07.340</a></span> | <span class="t">And I initially sort of felt like that was a solved enough problem, because what we do in the infrastructure level helps you in one way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=555" target="_blank">00:09:15.980</a></span> | <span class="t">What a lot of our friends and customers do much higher up in the stack with platforms that sort of wrap all of the voice AI problem set in very easy-to-use dashboards and tools and GUIs are also really good solutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=569" target="_blank">00:09:29.980</a></span> | <span class="t">But what we came to realize is that there was sort of a middle of the stack that people were asking about a lot in the open-source community that boiled down to, "How do I do my Kubernetes?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=580" target="_blank">00:09:40.620</a></span> | <span class="t">So people would ask questions in the pipecat Discord about deployment and scaling, and we would say, "Oh, well, if you really want to run this stuff yourself on your own infrastructure, here are the five things you do in Kubernetes," and people would say, "Some version of Kuber-what?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=593" target="_blank">00:09:53.620</a></span> | <span class="t">And we don't have a good answer to that, so we thought we'd come up with a good answer to that, which is a very thin layer on top of our existing global media-oriented real-time infrastructure designed as what I think of -- this is not a very good marketing tagline -- but I think of this as a very thin wrapper around Docker and Kubernetes optimized for voice AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=616" target="_blank">00:10:16.660</a></span> | <span class="t">So what are the things we're trying to solve for?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=619" target="_blank">00:10:19.180</a></span> | <span class="t">So fast start times are very important. If somebody calls your voice agent and they hear ringing, they want to hear that voice agent pick up the phone and say, "Hello," pretty fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=630" target="_blank">00:10:30.580</a></span> | <span class="t">Almost no matter what you do in AI, you care about cold start times, but it's even more important when the user is initiating some action and expects you to hear audio back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=642" target="_blank">00:10:42.380</a></span> | <span class="t">Cold starts are hard. If you've built an AI infrastructure, you know that. We try to solve the cold start problem for voice AI. Happy to talk about cold starts in great detail, because it's something I've been thinking a lot about over the last few months.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=656" target="_blank">00:10:56.780</a></span> | <span class="t">Autoscaling is a little bit related to cold starts. You want your resources to expand as your traffic pattern expands. The alternative is you know exactly what your traffic pattern is, and you just deploy a bunch of resources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=670" target="_blank">00:11:10.180</a></span> | <span class="t">That doesn't work for most workloads. Most people have time dependent or completely unpredictable workloads, so you need to scale up and scale down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=678" target="_blank">00:11:18.580</a></span> | <span class="t">Real-time is different from non-real-time, and by non-real-time, I mean everything that's not conversational latency of a few hundred milliseconds or less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=688" target="_blank">00:11:28.540</a></span> | <span class="t">If you are making an HTTP request, you want it to be fast, but you don't really care if your P95 is fifteen hundred milliseconds or two thousand milliseconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=696" target="_blank">00:11:36.940</a></span> | <span class="t">In most cases, in a voice AI conversation, you care a lot if your P95 goes up above eight hundred, nine hundred, a thousand milliseconds for the entire voice-to-voice response chain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=709" target="_blank">00:11:49.340</a></span> | <span class="t">All the little inference calls you make as part of that have to be much faster than that by definition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=714" target="_blank">00:11:54.940</a></span> | <span class="t">So the whole networking stack from client to wherever your pipecat code is running and inside that Kubernetes cluster has to be optimized for real-time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=725" target="_blank">00:12:05.740</a></span> | <span class="t">You probably need global deployment. You probably have GDPR or data residency or other kinds of data privacy requirements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=735" target="_blank">00:12:15.540</a></span> | <span class="t">Or you just need global deployment because you want these servers close to users because that helps with latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=743" target="_blank">00:12:23.940</a></span> | <span class="t">And all these things have to be like delivered at reasonable cost, so we try to take these things off of your plate and help you build quickly and get to market with your voice agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=756" target="_blank">00:12:36.140</a></span> | <span class="t">A couple other things that are just worth flagging here. We've done a lot of work on turn detection, which is sort of one of the 2025 top three problems most people in voice AI are thinking about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=765" target="_blank">00:12:45.140</a></span> | <span class="t">How to make better. Check out the open source smart turn model that's part of the pipecat ecosystem if you're interested in that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=771" target="_blank">00:12:51.940</a></span> | <span class="t">The open source smart turn model is built into pipecat cloud and runs for free. Our friends at FAL host it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=778" target="_blank">00:12:58.540</a></span> | <span class="t">You've probably heard of FAL if you're doing Gen AI stuff. Very fast, very good, GPU optimized inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=785" target="_blank">00:13:05.140</a></span> | <span class="t">And ambient noise and background voices. So one problem with voice AI is that even though transcription models today are very resistant,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=794" target="_blank">00:13:14.740</a></span> | <span class="t">resilient to all kinds of noisy environments, the LLMs themselves are not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=798" target="_blank">00:13:18.540</a></span> | <span class="t">So if you are trying to do transcription and figure out when people are talking and figure out when to fire inference down the chain and ask your LLMs to do something,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=810" target="_blank">00:13:30.140</a></span> | <span class="t">having background noise that sounds a little bit like speech will trigger lots of interruptions that you don't mean to happen and will inject lots of spurious pseudospeech into your transcripts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=821" target="_blank">00:13:41.540</a></span> | <span class="t">And that's true even for speech-to-speech models today. They're not very resilient to background noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=827" target="_blank">00:13:47.340</a></span> | <span class="t">The best solution to background noise today is a commercial model from a really great small company called Crisp.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=835" target="_blank">00:13:55.340</a></span> | <span class="t">The Crisp model is only available with sort of big chunk of commercial licensing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=840" target="_blank">00:14:00.340</a></span> | <span class="t">You can use Crisp for free inside pipecat cloud if you run on pipecat cloud.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=844" target="_blank">00:14:04.340</a></span> | <span class="t">You can also use Crisp in your own pipecat pipelines with your own license if you run Crisp somewhere else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=850" target="_blank">00:14:10.140</a></span> | <span class="t">Finally, agents are non-deterministic. As we all know, there's a whole evals and PM track here and in every other track we talk about this problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=856" target="_blank">00:14:16.940</a></span> | <span class="t">We've got some nice low-level building blocks for logging and observability natively in pipecat and exposed through pipecat cloud and a bunch of partners we work with on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=865" target="_blank">00:14:25.940</a></span> | <span class="t">I'm happy to introduce you to the great teams we work with at various companies that are building observability stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=872" target="_blank">00:14:32.740</a></span> | <span class="t">That is my speed run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=874" target="_blank">00:14:34.740</a></span> | <span class="t">I came in 20 seconds under the 15 minutes, but because we are the last talk in this block, if people want to do Q&A, totally happy to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=883" target="_blank">00:14:43.740</a></span> | <span class="t">Thanks, Ryan. One, actually, I have two questions, a very quick question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=894" target="_blank">00:14:54.540</a></span> | <span class="t">One is, we're based on Sydney, Australia.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=897" target="_blank">00:14:57.540</a></span> | <span class="t">One of the problems we've gone into the 800-millisecond thing is the time to go and call OpenAI and come back and automate Australia.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=904" target="_blank">00:15:04.540</a></span> | <span class="t">So open-air processing is .</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=906" target="_blank">00:15:06.540</a></span> | <span class="t">Do you have any alternatives for that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=908" target="_blank">00:15:08.540</a></span> | <span class="t">Have you looked at other alternatives for people outside the states?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=911" target="_blank">00:15:11.540</a></span> | <span class="t">Yes. That's a great question. So the question -- I will repeat the question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=914" target="_blank">00:15:14.540</a></span> | <span class="t">The question is, if you're in a geography that is a long way from your inference servers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=918" target="_blank">00:15:18.940</a></span> | <span class="t">So in the case of this particular question, you're serving users in Australia.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=922" target="_blank">00:15:22.940</a></span> | <span class="t">You're using OpenAI. OpenAI only has inference servers in the U.S.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=927" target="_blank">00:15:27.140</a></span> | <span class="t">You don't want to make extra round trips to the U.S.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=929" target="_blank">00:15:29.940</a></span> | <span class="t">So there's a couple answers to that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=931" target="_blank">00:15:31.940</a></span> | <span class="t">One is, if you make one long haul to the U.S. for all the audio at the beginning of the chain and at the end of the chain,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=939" target="_blank">00:15:39.940</a></span> | <span class="t">that is much better than making three inference round trips for transcription, OpenAI, and voice generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=947" target="_blank">00:15:47.340</a></span> | <span class="t">So that's one tool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=948" target="_blank">00:15:48.340</a></span> | <span class="t">We often say to people, just deploy close to the inference servers rather than close to the users</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=953" target="_blank">00:15:53.340</a></span> | <span class="t">and optimize for having one long trip and then a bunch of very, very fast short trips.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=958" target="_blank">00:15:58.740</a></span> | <span class="t">That's good but not great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=960" target="_blank">00:16:00.740</a></span> | <span class="t">The other option is to run stuff using OpenWeights models locally in Australia, which you can definitely do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=968" target="_blank">00:16:08.740</a></span> | <span class="t">It's a longer conversation about what use cases you can use, say, the best OpenWeights models versus the GPT-40</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=976" target="_blank">00:16:16.140</a></span> | <span class="t">and Gemini 2 Flash level models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=979" target="_blank">00:16:19.540</a></span> | <span class="t">But there are definitely some voice AI workloads now that you can reliably run on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=983" target="_blank">00:16:23.540</a></span> | <span class="t">like the Gemma or the QIN3 or the Llama 4 models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=986" target="_blank">00:16:26.540</a></span> | <span class="t">Second question, maybe just related to that is, let's say, if we basically boost models in Australia itself,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=993" target="_blank">00:16:33.940</a></span> | <span class="t">what's the interconnectivity with the network from your cloud by GATT?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=997" target="_blank">00:16:37.940</a></span> | <span class="t">Do you go through the internet exchange locally out there?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1001" target="_blank">00:16:41.940</a></span> | <span class="t">Yes, so we have endpoints all over the world that are -- in our world, we call them points of presence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1008" target="_blank">00:16:48.340</a></span> | <span class="t">So we have sort of the edge server close to the user and we'll terminate the WebRTC or the telephony connection there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1014" target="_blank">00:16:54.340</a></span> | <span class="t">And then we'll route over our own private AWS or OCI backbones to wherever you need to route to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1019" target="_blank">00:16:59.940</a></span> | <span class="t">If you're hosting in Australia, you should be able to just hit our endpoints and then you're hosting in Australia.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1028" target="_blank">00:17:08.340</a></span> | <span class="t">So we also -- we have some regional availability of PipeCat Cloud now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1032" target="_blank">00:17:12.140</a></span> | <span class="t">We will launch a bunch more regional availability of PipeCat Cloud over the next quarter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1037" target="_blank">00:17:17.340</a></span> | <span class="t">So I hope we actually have PipeCat Cloud in Australia soon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1041" target="_blank">00:17:21.340</a></span> | <span class="t">Although you can also obviously self-host in Australia and still use either PipeCat itself or PipeCat Plus Daily in other ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1047" target="_blank">00:17:27.340</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1048" target="_blank">00:17:28.340</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1049" target="_blank">00:17:29.340</a></span> | <span class="t">Oh, sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1050" target="_blank">00:17:30.340</a></span> | <span class="t">Thanks for the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1052" target="_blank">00:17:32.340</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1053" target="_blank">00:17:33.340</a></span> | <span class="t">So there are others like Moshi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1055" target="_blank">00:17:35.340</a></span> | <span class="t">Yeah, yeah, yeah, I love Moshi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1057" target="_blank">00:17:37.340</a></span> | <span class="t">They basically claim that turn detection is no longer needed because they, in parallel, encode both the speaker and the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1065" target="_blank">00:17:45.340</a></span> | <span class="t">Do you have experience running those?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1066" target="_blank">00:17:46.340</a></span> | <span class="t">Do they actually scale?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1067" target="_blank">00:17:47.340</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1068" target="_blank">00:17:48.340</a></span> | <span class="t">The question is about a really cool open weights model called Moshi by a French lab called Kyotai.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1075" target="_blank">00:17:55.340</a></span> | <span class="t">Moshi is a sort of next-generation research model where the architecture is constant bidirectional streaming.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1081" target="_blank">00:18:01.340</a></span> | <span class="t">So you're always streaming tokens in and the model is always streaming tokens out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1085" target="_blank">00:18:05.340</a></span> | <span class="t">In a conversational voice situation, which Moshi was designed for, most of the tokens streaming out are silenced tokens of some kind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1092" target="_blank">00:18:12.340</a></span> | <span class="t">And when they're not silenced tokens, it's because the model decided it was going to do whatever the model is trained to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1098" target="_blank">00:18:18.340</a></span> | <span class="t">Which is really cool because that can mean not just that the model does natural turn taking, but also that the model can do things like back channeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1106" target="_blank">00:18:26.340</a></span> | <span class="t">So the model can do the things the humans do, that its data set has audio for, like, when you're talking, I can say, hmm, ah, yeah, mm-hmm, yeah, uh-huh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1116" target="_blank">00:18:36.340</a></span> | <span class="t">And it's not actually a new inference call.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1119" target="_blank">00:18:39.340</a></span> | <span class="t">It's just streaming.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1120" target="_blank">00:18:40.340</a></span> | <span class="t">That paper, the Kyotai Labs Moshi architecture paper was my very favorite ML research paper from last year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1127" target="_blank">00:18:47.340</a></span> | <span class="t">Now, that model itself is not usable in production for a bunch of reasons, including that it is too small a language model to be useful for basically any real-world use case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1144" target="_blank">00:19:04.340</a></span> | <span class="t">I have more to say about that, but I'm super, super excited about that architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1148" target="_blank">00:19:08.340</a></span> | <span class="t">But I don't think -- I mean, we're a couple years away from that architecture being actually usable and trained as a production model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1155" target="_blank">00:19:15.340</a></span> | <span class="t">There are speech-to-speech models from the large labs that are closer to being able to be used in production.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1162" target="_blank">00:19:22.340</a></span> | <span class="t">Now, they are not streaming architecture models, but they are native audio speech-to-speech models, which have a bunch of advantages, including really great multilingual support.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1171" target="_blank">00:19:31.340</a></span> | <span class="t">So, like, mixed-language stuff is great from those models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1174" target="_blank">00:19:34.340</a></span> | <span class="t">In theory, latency reductions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1177" target="_blank">00:19:37.340</a></span> | <span class="t">So, OpenAI has a real-time model called GPT-40 Audio Preview that sits behind their real-time API.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1184" target="_blank">00:19:44.340</a></span> | <span class="t">It's a good model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1186" target="_blank">00:19:46.340</a></span> | <span class="t">Gemini 2.0 Flash is usable in an audio-to-audio mode, and they're training to -- or they have preview releases of 2.5 Flash.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1194" target="_blank">00:19:54.340</a></span> | <span class="t">These models are now good enough that you can use them for use cases where you are more concerned about naturalness of the human conversation than you are about reliable instruction following and function calling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1206" target="_blank">00:20:06.340</a></span> | <span class="t">They are less reliable in audio mode than the SOTA models operating in text mode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1213" target="_blank">00:20:13.340</a></span> | <span class="t">So, what we generally see is that for a small subset of voice AI use cases today that are really about, like, conversational dynamics, narrative, storytelling, those models are starting to get adopted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1225" target="_blank">00:20:25.340</a></span> | <span class="t">For the majority of sort of enterprise voice AI use cases where you really need best possible instruction following and function calling, those models are not yet the right choice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1234" target="_blank">00:20:34.340</a></span> | <span class="t">But they are getting better every release, and all of us expect the world to move to speech-to-speech models being the default for, like, 95% of voice AI sometime in the next two years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1245" target="_blank">00:20:45.340</a></span> | <span class="t">The question is when, in your use case, will a particular model architecture sort of cross that threshold in your evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1251" target="_blank">00:20:51.340</a></span> | <span class="t">Sorry, what about Sesame?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1252" target="_blank">00:20:52.340</a></span> | <span class="t">Did you put Sesame in that same bucket as Gemini in OpenAI, or --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1256" target="_blank">00:20:56.340</a></span> | <span class="t">Sesame is closer to Moshi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1258" target="_blank">00:20:58.340</a></span> | <span class="t">In fact, Sesame -- so there's another open weights -- or partly open weights and really interesting model called Sesame.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1265" target="_blank">00:21:05.340</a></span> | <span class="t">It's a little like Moshi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1267" target="_blank">00:21:07.340</a></span> | <span class="t">It, in fact, uses the Moshi neural encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1270" target="_blank">00:21:10.340</a></span> | <span class="t">Yeah, it uses Mimi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1272" target="_blank">00:21:12.340</a></span> | <span class="t">Sesame -- so Sesame has not yet been fully released.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1277" target="_blank">00:21:17.340</a></span> | <span class="t">There isn't a full Sesame release.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1279" target="_blank">00:21:19.340</a></span> | <span class="t">Also, I think Sesame is smaller than probably you would need to use for most enterprise use cases today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1287" target="_blank">00:21:27.340</a></span> | <span class="t">Although the lab training Sesame, I think, has bigger versions coming.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1291" target="_blank">00:21:31.340</a></span> | <span class="t">There's also a speech-to-speech model called Ultravox, which is really good, which is trained on the Llama 3, 70B backbone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1297" target="_blank">00:21:37.340</a></span> | <span class="t">And that team supports that model and has a production voice AI API.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1301" target="_blank">00:21:41.340</a></span> | <span class="t">That model is worth trying if you are really interested in speech-to-speech models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1306" target="_blank">00:21:46.340</a></span> | <span class="t">If Llama 3, 70B can do what you want, I think Ultravox is a good choice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1310" target="_blank">00:21:50.340</a></span> | <span class="t">If Llama 3, 70B isn't quite there for your use case, probably not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1314" target="_blank">00:21:54.340</a></span> | <span class="t">But, you know, the next release of Ultravox.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1316" target="_blank">00:21:56.340</a></span> | <span class="t">So speech-to-speech is definitely the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1318" target="_blank">00:21:58.340</a></span> | <span class="t">I generally tell people experiment with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1321" target="_blank">00:22:01.340</a></span> | <span class="t">Don't necessarily start assuming you're going to use it for your enterprise use case, though, today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1326" target="_blank">00:22:06.340</a></span> | <span class="t">Given your vendor neutrality, can you speak to the strengths and weaknesses of using, like, the leading-edge multimodal input models like OpenAI and Gemini?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1338" target="_blank">00:22:18.340</a></span> | <span class="t">When should I choose OpenAI or when should I choose Gemini?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1343" target="_blank">00:22:23.340</a></span> | <span class="t">So my opinion is that GPT 4.0 in text mode and Gemini 2.0 Flash in text mode are roughly equivalent models for the use cases that I test every day.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1355" target="_blank">00:22:35.340</a></span> | <span class="t">So I would make the decision -- if you can, I would build a PipeCat pipeline and then just swap the two models and run your evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1366" target="_blank">00:22:46.340</a></span> | <span class="t">Because they're both really good models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1368" target="_blank">00:22:48.340</a></span> | <span class="t">One of the advantages of Gemini is that it's extremely aggressively priced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1372" target="_blank">00:22:52.340</a></span> | <span class="t">So, you know, a 30-minute conversation on Gemini is probably 10 times cheaper than a 30-minute conversation on GPT 4.0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1381" target="_blank">00:23:01.340</a></span> | <span class="t">You know, that may or may not stay true as they both change their prices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1385" target="_blank">00:23:05.340</a></span> | <span class="t">But that's definitely something we hear a lot from customers today is that they like the pricing of Gemini.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1390" target="_blank">00:23:10.340</a></span> | <span class="t">The other interesting thing about Gemini is that it operates in native audio input mode very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1396" target="_blank">00:23:16.340</a></span> | <span class="t">So you can use Gemini in native audio input mode and then text output mode in a pipeline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1402" target="_blank">00:23:22.340</a></span> | <span class="t">And that has advantages for some use cases in some languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1405" target="_blank">00:23:25.340</a></span> | <span class="t">And you can, again, test that on your evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1407" target="_blank">00:23:27.340</a></span> | <span class="t">And OpenAI also has native audio support in some of their newer models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1411" target="_blank">00:23:31.340</a></span> | <span class="t">But I think they're just a little bit behind the Gemini models in that regard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1416" target="_blank">00:23:36.340</a></span> | <span class="t">Time for one more or are we done?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1419" target="_blank">00:23:39.340</a></span> | <span class="t">One more and then we're done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1420" target="_blank">00:23:40.340</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1421" target="_blank">00:23:41.340</a></span> | <span class="t">What are the general advantages of speech-to-speech versus going speech-to-text, doing something and then going back to text-to-speech?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1426" target="_blank">00:23:46.340</a></span> | <span class="t">What are the general advantages of speech-to-speech instead of text, text, inference-to-speech and out?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1436" target="_blank">00:23:56.340</a></span> | <span class="t">So it's a super interesting question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1438" target="_blank">00:23:58.340</a></span> | <span class="t">And I have a practical answer and a philosophical answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1441" target="_blank">00:24:01.340</a></span> | <span class="t">I'll keep them both short.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1442" target="_blank">00:24:02.340</a></span> | <span class="t">The practical answer is that you lose information when you transcribe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1448" target="_blank">00:24:08.340</a></span> | <span class="t">And so if there's information that's useful in the transcription step, if there's information in the audio that you would lose that's useful for your use case, then a speech-to-speech model is great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1460" target="_blank">00:24:20.340</a></span> | <span class="t">So, for example, things like mixed language are very hard for small transcription models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1465" target="_blank">00:24:25.340</a></span> | <span class="t">You're almost always sort of losing a bunch more information in a mixed language transcription than you are in, like, an optimized model monolingual transcription.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1473" target="_blank">00:24:33.340</a></span> | <span class="t">So why not go to the big LLM that just has all this, like, language knowledge and can do a better job on the multilingual input?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1479" target="_blank">00:24:39.340</a></span> | <span class="t">The other advantage is potentially you have lower latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1483" target="_blank">00:24:43.340</a></span> | <span class="t">Like, if you've trained an end-to-end model for speech-to-speech and it's all one model and you're not, like, chaining together inference calls, you can probably get lower latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1489" target="_blank">00:24:49.340</a></span> | <span class="t">In practice, whether that's true today depends more on the sort of APIs and inference stack than it does on the model architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1496" target="_blank">00:24:56.340</a></span> | <span class="t">But I think we're all going towards assuming that we just want to do one inference call for, like, the bulk of things, and then we might use other little models on the side for, like, subsets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1505" target="_blank">00:25:05.340</a></span> | <span class="t">The philosophical answer, though, is that those advantages are probably outweighed by the challenges to today's LLM architecture is when you have big context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1515" target="_blank">00:25:15.340</a></span> | <span class="t">And big context -- and audio tokens take up a lot of context tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1520" target="_blank">00:25:20.340</a></span> | <span class="t">So when you're operating in audio mode, you're just sort of expanding the context massively relative to operating in text mode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1525" target="_blank">00:25:25.340</a></span> | <span class="t">And that tends to degrade the performance of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1529" target="_blank">00:25:29.340</a></span> | <span class="t">I think a little bit relatedly, nobody has as much audio data as they have text data for training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1534" target="_blank">00:25:34.340</a></span> | <span class="t">So even though a big model is doing a bunch of transfer learning when you give it a bunch of audio, and it is, in theory, sort of mapping all that audio to the same latent space as its text reasoning, in practice, it's definitely not doing that exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1547" target="_blank">00:25:47.340</a></span> | <span class="t">It's doing something like that, but not that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1550" target="_blank">00:25:50.340</a></span> | <span class="t">And so because we don't have as much audio data, you see a lot of issues with audio to audio models, like the model will sometimes just respond in a totally different language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1560" target="_blank">00:26:00.340</a></span> | <span class="t">And that's cool, but it's never what you want in the enterprise, you know, voice AI use case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1566" target="_blank">00:26:06.340</a></span> | <span class="t">And the best guess for why that's happening is it's in some right part of the latent space from some projection, but then from some other projection, it's totally in a different part of the latent space when you gave it audio instead of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1580" target="_blank">00:26:20.340</a></span> | <span class="t">Even though if you transcribed that text, it would be exactly the same as the audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1585" target="_blank">00:26:25.340</a></span> | <span class="t">So, you know, latent spaces are big, and to, like, actually find our way through them in post training, you really have to have a lot of data, and nobody has enough audio data yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1596" target="_blank">00:26:36.340</a></span> | <span class="t">But the big labs are going to fix that, because audio matters and multi-turn conversations matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=IA4lZjh9sTs&t=1600" target="_blank">00:26:40.340</a></span> | <span class="t">.</span></div></div></body></html>