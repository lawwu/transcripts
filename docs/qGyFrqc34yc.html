<html><head><title>Reinforcement Learning from Human Feedback explained with math derivations and the PyTorch code.</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Reinforcement Learning from Human Feedback explained with math derivations and the PyTorch code.</h2><a href="https://www.youtube.com/watch?v=qGyFrqc34yc"><img src="https://i.ytimg.com/vi/qGyFrqc34yc/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=232">3:52</a> Intro to Language Models<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=353">5:53</a> AI Alignment<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=408">6:48</a> Intro to RL<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=584">9:44</a> RL for Language Models<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=661">11:1</a> Reward model<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1239">20:39</a> Trajectories (RL)<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1773">29:33</a> Trajectories (Language Models)<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1889">31:29</a> Policy Gradient Optimization<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2496">41:36</a> REINFORCE algorithm<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2648">44:8</a> REINFORCE algorithm (Language Models)<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2715">45:15</a> Calculating the log probabilities<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2955">49:15</a> Calculating the rewards<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3042">50:42</a> Problems with Gradient Policy Optimization: variance<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3360">56:0</a> Rewards to go<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3559">59:19</a> Baseline<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3769">62:49</a> Value function estimation<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3870">64:30</a> Advantage function<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4254">70:54</a> Generalized Advantage Estimation<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4790">79:50</a> Advantage function (Language Models)<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4919">81:59</a> Problems with Gradient Policy Optimization: sampling<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5048">84:8</a> Importance Sampling<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5276">87:56</a> Off-Policy Learning<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5582">93:2</a> Proximal Policy Optimization (loss)<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6059">100:59</a> Reward hacking (KL divergence)<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6236">103:56</a> Code walkthrough<br><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8006">133:26</a> Conclusion<br><br><div style="text-align: left;"><a href="./qGyFrqc34yc.html">Whisper Transcript</a> | <a href="./transcript_qGyFrqc34yc.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys, welcome back to my channel. Today we are going to talk about reinforcement learning from human feedback and PPO</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4" target="_blank">00:00:04.800</a></span> | <span class="t">So reinforcement learning from human feedback is a technique that is used to align the behavior of a language model to what we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=11" target="_blank">00:00:11.440</a></span> | <span class="t">The language model to output for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=13" target="_blank">00:00:13.440</a></span> | <span class="t">We don't want the language model to use curse words or we don't want the language model to behave in an impolite way to the user</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=19" target="_blank">00:00:19.440</a></span> | <span class="t">So we need to do some kind of alignment and reinforcement learning from human feedback is one of the most famous technique</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=24" target="_blank">00:00:24.960</a></span> | <span class="t">Even if there are now new techniques like dpo, which I will talk about in another video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=30" target="_blank">00:00:30.400</a></span> | <span class="t">Now reinforcement learning from human feedback is also how they created chat gpt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=34" target="_blank">00:00:34.560</a></span> | <span class="t">So how they align chat gpt to the behavior they wanted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=38" target="_blank">00:00:38.320</a></span> | <span class="t">In the topics of today are first I will introduce a little bit the language models how they are used and how they work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=45" target="_blank">00:00:45.280</a></span> | <span class="t">Then we will talk about the topic of ai alignment why it's important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=49" target="_blank">00:00:49.780</a></span> | <span class="t">And later we will do a deep dive into reinforcement learning from human feedback in particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=54" target="_blank">00:00:54.960</a></span> | <span class="t">I will introduce first of all, what is reinforcement learning then I will describe all the setup of the reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=61" target="_blank">00:01:01.200</a></span> | <span class="t">So the reward model what are trajectories in particular, we will see the policy gradient optimization and we will derive the algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=68" target="_blank">00:01:08.260</a></span> | <span class="t">We will see also the problems with it. So how to reduce the variance the advantage estimation important sampling of policy learning, etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=75" target="_blank">00:01:15.280</a></span> | <span class="t">The goal for today's video is actually to derive the loss of the ppo</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=80" target="_blank">00:01:20.400</a></span> | <span class="t">So I don't want to just throw the formula at you. I want to actually derive step by step all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=85" target="_blank">00:01:25.680</a></span> | <span class="t">Algorithm of ppo and also show you all the history that led to it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=90" target="_blank">00:01:30.640</a></span> | <span class="t">So what were the problems that ppo was trying to solve mathematical from a mathematical point of view?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=95" target="_blank">00:01:35.600</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=97" target="_blank">00:01:37.280</a></span> | <span class="t">In the final part of the video, we will go through the code of an actual implementation of reinforcement learning from human feedback with ppo</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=103" target="_blank">00:01:43.920</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=105" target="_blank">00:01:45.760</a></span> | <span class="t">I will actually not code from uh by line by line</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=109" target="_blank">00:01:49.200</a></span> | <span class="t">I will actually explain the code line by line and in particular I will show the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=112" target="_blank">00:01:52.800</a></span> | <span class="t">Implementation as done by the HuggingFace team</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=115" target="_blank">00:01:55.680</a></span> | <span class="t">So I will not show you how to use the HuggingFace library to use reinforcement learning from human feedback</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=121" target="_blank">00:02:01.680</a></span> | <span class="t">But we will go inside the code of the HuggingFace library and see how it was implemented by the HuggingFace team</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=127" target="_blank">00:02:07.840</a></span> | <span class="t">This way we can combine the theory that we have learned with practice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=132" target="_blank">00:02:12.400</a></span> | <span class="t">Now the code written by the HuggingFace team is kind of obscure and complex to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=137" target="_blank">00:02:17.360</a></span> | <span class="t">So I deleted some parts and I also commented with my own comments some other parts that were not easy to understand this way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=144" target="_blank">00:02:24.080</a></span> | <span class="t">I hope to make it easier for everyone to follow the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=147" target="_blank">00:02:27.760</a></span> | <span class="t">Now there are some prerequisites before watching this video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=150" target="_blank">00:02:30.880</a></span> | <span class="t">First of all, I hope that you have some notions of probability and statistics. Not much. At least, you know, what is an expectation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=157" target="_blank">00:02:37.060</a></span> | <span class="t">um, we we need to know so of course some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=161" target="_blank">00:02:41.440</a></span> | <span class="t">Knowledge from deep learning for example gradient descent. What is the loss function?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=164" target="_blank">00:02:44.800</a></span> | <span class="t">And the fact that in gradient descent we calculate some kind of gradient etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=169" target="_blank">00:02:49.200</a></span> | <span class="t">We need to have some basic knowledge of reinforcement learning even if I will review most of it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=174" target="_blank">00:02:54.880</a></span> | <span class="t">So at least you know, what is an agent, the state, the environment and the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=178" target="_blank">00:02:58.640</a></span> | <span class="t">One important aspect of this video is that we will be using the transformer model a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=184" target="_blank">00:03:04.160</a></span> | <span class="t">So I recommend you watch my previous video on the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=186" target="_blank">00:03:06.960</a></span> | <span class="t">If you have not if you're not familiar with the concept of self-attention or the causal mask, which will be key to understanding this video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=194" target="_blank">00:03:14.000</a></span> | <span class="t">So the goal of this video is actually to combine theory with practice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=198" target="_blank">00:03:18.640</a></span> | <span class="t">So I will make sure that I will always kind of give an intuition to formulas that are complex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=204" target="_blank">00:03:24.240</a></span> | <span class="t">And don't worry if you don't understand everything at the beginning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=208" target="_blank">00:03:28.240</a></span> | <span class="t">Why? Because I will be giving a lot of theory at the beginning because later I will be showing the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=214" target="_blank">00:03:34.240</a></span> | <span class="t">I cannot show the code without giving the theoretical knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=217" target="_blank">00:03:37.600</a></span> | <span class="t">So don't be scared if you don't understand everything because when we will look at the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=221" target="_blank">00:03:41.760</a></span> | <span class="t">I will go back to the theory line by line so that we can combine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=225" target="_blank">00:03:45.680</a></span> | <span class="t">You know the practical and the theoretical aspect of this knowledge. So let's start our journey</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=231" target="_blank">00:03:51.200</a></span> | <span class="t">Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=233" target="_blank">00:03:53.360</a></span> | <span class="t">What is a language model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=234" target="_blank">00:03:54.560</a></span> | <span class="t">First of all a language model is a probabilistic model that assigns probabilities to sequence of words in particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=241" target="_blank">00:04:01.120</a></span> | <span class="t">A language model allows us to compute the probability of the next token given the input sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=246" target="_blank">00:04:06.960</a></span> | <span class="t">In particular, for example, if we have a prompt that says shanghai is a city in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=252" target="_blank">00:04:12.320</a></span> | <span class="t">What is the probability that the next word is china?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=255" target="_blank">00:04:15.680</a></span> | <span class="t">Or what is the probability that the next word is beijing or cat or pizza?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=259" target="_blank">00:04:19.360</a></span> | <span class="t">This is the kind of probability that the language model is modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=263" target="_blank">00:04:23.040</a></span> | <span class="t">Now in my tractation of language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=266" target="_blank">00:04:26.400</a></span> | <span class="t">I always make a simplification which is that each word is a token and each token is a word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=271" target="_blank">00:04:31.520</a></span> | <span class="t">This is not always the case because it depends on the tokenizer that we are using and actually in most cases. It's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=277" target="_blank">00:04:37.520</a></span> | <span class="t">like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=279" target="_blank">00:04:39.360</a></span> | <span class="t">But for simplicity, we will always consider for the rest of the video that each word is a token and each token is a word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=284" target="_blank">00:04:44.960</a></span> | <span class="t">Now you may be wondering how can we use the language models to generate text?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=290" target="_blank">00:04:50.480</a></span> | <span class="t">well, we do it iteratively which means that if we have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=293" target="_blank">00:04:53.600</a></span> | <span class="t">Prompt for example a question like where is shanghai then we ask the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=298" target="_blank">00:04:58.640</a></span> | <span class="t">What is the next token and for example greedily we select the token with the most probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=303" target="_blank">00:05:03.540</a></span> | <span class="t">So we select for example the word shanghai then we take this word shanghai. Let me use the laser</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=309" target="_blank">00:05:09.200</a></span> | <span class="t">We put it back into the input and we ask again the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=312" target="_blank">00:05:12.640</a></span> | <span class="t">What is the next token and the language model will tell us what are the probability of the next token and we select the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=318" target="_blank">00:05:18.000</a></span> | <span class="t">that is more probable suppose it's the word is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=320" target="_blank">00:05:20.240</a></span> | <span class="t">We take it and we put it back in the input and again we ask the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=324" target="_blank">00:05:24.480</a></span> | <span class="t">What is the next token suppose the next token is in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=327" target="_blank">00:05:27.360</a></span> | <span class="t">We take it we put it back in the input and we ask again the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=331" target="_blank">00:05:31.440</a></span> | <span class="t">What is the next token etc until we reach a number of tokens that we have generated?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=336" target="_blank">00:05:36.100</a></span> | <span class="t">Or we believe that the answer is complete</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=339" target="_blank">00:05:39.040</a></span> | <span class="t">So in this case we can stop for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=341" target="_blank">00:05:41.360</a></span> | <span class="t">Because we can see that the answer is shanghai is in china is the answer generated by the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=346" target="_blank">00:05:46.560</a></span> | <span class="t">So this is an iterative process of generating text with the language model and all language models actually work like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=352" target="_blank">00:05:52.800</a></span> | <span class="t">Now with what is the topic of ai alignment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=357" target="_blank">00:05:57.280</a></span> | <span class="t">A language model is usually pre-trained on a vast amount of data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=362" target="_blank">00:06:02.320</a></span> | <span class="t">which means that it has been pre-trained on billions of web pages or the entire of wikipedia or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=368" target="_blank">00:06:08.000</a></span> | <span class="t">thousands of books</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=370" target="_blank">00:06:10.160</a></span> | <span class="t">This gives the language model a lot of knowledge from which it can retrieve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=375" target="_blank">00:06:15.920</a></span> | <span class="t">And it can learn to complete a prompt in a reasonable way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=379" target="_blank">00:06:19.360</a></span> | <span class="t">However, this does not teach the language model to behave in a particular way. For example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=384" target="_blank">00:06:24.640</a></span> | <span class="t">Just by pre-training we do not teach the language model to not use offensive language or to not use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=390" target="_blank">00:06:30.560</a></span> | <span class="t">racist expressions or to not use curse words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=393" target="_blank">00:06:33.520</a></span> | <span class="t">To do this and to create for example a chat assistant that is friendly to the user</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=400" target="_blank">00:06:40.400</a></span> | <span class="t">We need to do some kind of alignment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=402" target="_blank">00:06:42.720</a></span> | <span class="t">So the topic of ai alignment is to align the model's behavior with some desired behavior</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=408" target="_blank">00:06:48.340</a></span> | <span class="t">Let's talk about reinforcement learning. So reinforcement learning is an area of artificial intelligence that is concerned with training an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=415" target="_blank">00:06:55.920</a></span> | <span class="t">Intelligent agent to take actions in an environment in order to maximize some reward that it receives from the environment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=424" target="_blank">00:07:04.080</a></span> | <span class="t">Let me give you a concrete example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=426" target="_blank">00:07:06.480</a></span> | <span class="t">So imagine we have a cat that lives in a very simple world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=430" target="_blank">00:07:10.080</a></span> | <span class="t">Suppose it's a room made up of many grids and this cat can move from one cell to another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=436" target="_blank">00:07:16.720</a></span> | <span class="t">now in this case our agent is the cat and this agent has a state and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=442" target="_blank">00:07:22.720</a></span> | <span class="t">Which describes for example the position of this agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=446" target="_blank">00:07:26.880</a></span> | <span class="t">In this case the state of the cat can be described by two variables</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=451" target="_blank">00:07:31.680</a></span> | <span class="t">One is the x coordinate and one is the y coordinate of the position of this cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=457" target="_blank">00:07:37.360</a></span> | <span class="t">Based on the state the cat can choose to do some actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=460" target="_blank">00:07:40.640</a></span> | <span class="t">Which could be for example to move down, move left, move right or move up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=465" target="_blank">00:07:45.680</a></span> | <span class="t">Based on the state the cat can take some actions and every time the cat takes some action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=472" target="_blank">00:07:52.000</a></span> | <span class="t">It will receive some reward from the environment. It will for sure move to a new position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=477" target="_blank">00:07:57.140</a></span> | <span class="t">And at the same time will receive some reward from the environment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=481" target="_blank">00:08:01.140</a></span> | <span class="t">And the reward is according to this reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=484" target="_blank">00:08:04.800</a></span> | <span class="t">So if the cat moves to an empty cell it will receive a reward of zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=488" target="_blank">00:08:08.800</a></span> | <span class="t">If it moves to the broom, for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=490" target="_blank">00:08:10.800</a></span> | <span class="t">It will receive a reward of -1 because my cat is scared of the broom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=494" target="_blank">00:08:14.800</a></span> | <span class="t">if somehow after a series of states and actions the cat arrives to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=499" target="_blank">00:08:19.040</a></span> | <span class="t">Bathtub it will receive a reward of -10 because my cat is super scared of water</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=505" target="_blank">00:08:25.760</a></span> | <span class="t">However, if the cat somehow manages to arrive to the meat it will receive a big reward of +100</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=513" target="_blank">00:08:33.120</a></span> | <span class="t">How should the cat move?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=514" target="_blank">00:08:34.800</a></span> | <span class="t">Well, there is a policy that tells what is the probability of the next action given the current state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=522" target="_blank">00:08:42.160</a></span> | <span class="t">So the policy describes for each position. So for each state of the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=526" target="_blank">00:08:46.880</a></span> | <span class="t">With what probability the cat should move up or down or left or right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=532" target="_blank">00:08:52.160</a></span> | <span class="t">And then the agent can choose to either choose a randomly an action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=536" target="_blank">00:08:56.320</a></span> | <span class="t">Or it can choose to select the action with the most probability for example, which is a greedy strategy etc etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=543" target="_blank">00:09:03.040</a></span> | <span class="t">now the goal of reinforcement learning is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=545" target="_blank">00:09:05.680</a></span> | <span class="t">Learn a probability so to optimize a policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=549" target="_blank">00:09:09.060</a></span> | <span class="t">Such that we maximize the expected return when the agent acts according to this policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=556" target="_blank">00:09:16.640</a></span> | <span class="t">which means that we should have a policy that with very high probability takes us to the meat because that's one way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=563" target="_blank">00:09:23.600</a></span> | <span class="t">Maximize the expected return in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=568" target="_blank">00:09:28.080</a></span> | <span class="t">Now you may be wondering okay the cat I can see it as a reinforcement learning agent and the reinforcement learning setup</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=574" target="_blank">00:09:34.560</a></span> | <span class="t">Makes sense for the cat and the meat and all these rewards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=577" target="_blank">00:09:37.840</a></span> | <span class="t">But what is the connection between reinforcement learning and language models? Let's try to clarify this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=584" target="_blank">00:09:44.080</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=585" target="_blank">00:09:45.680</a></span> | <span class="t">You can think of the language model as a policy itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=589" target="_blank">00:09:49.520</a></span> | <span class="t">So as we saw before the policy is something that given the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=593" target="_blank">00:09:53.440</a></span> | <span class="t">Tells you what is the probability of the action that you should take in that state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=598" target="_blank">00:09:58.480</a></span> | <span class="t">In the case of the language model. We know that the language model tells you given a prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=602" target="_blank">00:10:02.880</a></span> | <span class="t">What is the probability of the next token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=605" target="_blank">00:10:05.680</a></span> | <span class="t">So we can think of the prompt as the state and the next token as the action that the language model can choose to perform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=612" target="_blank">00:10:12.640</a></span> | <span class="t">Which will lead to a new state because every time we sample a next token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=617" target="_blank">00:10:17.040</a></span> | <span class="t">We put it back into the prompt then we can ask the language model again. What is the next next token etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=622" target="_blank">00:10:22.800</a></span> | <span class="t">So as you can see we can think of the language model as the reinforcement learning agent itself and also as the policy itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=629" target="_blank">00:10:29.040</a></span> | <span class="t">in which the state is the prompt and the action is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=632" target="_blank">00:10:32.880</a></span> | <span class="t">Next token that the language model will choose according to some strategy which could be the greedy one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=638" target="_blank">00:10:38.080</a></span> | <span class="t">Which could be the top p or the top k or etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=641" target="_blank">00:10:41.120</a></span> | <span class="t">The only thing that we are missing here is the reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=644" target="_blank">00:10:44.800</a></span> | <span class="t">How can we reward the language model for good responses and how can we kind of?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=650" target="_blank">00:10:50.860</a></span> | <span class="t">Penalize the language model for bad responses. This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=655" target="_blank">00:10:55.340</a></span> | <span class="t">Done through a reward model that we have to build. Let's see how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=659" target="_blank">00:10:59.980</a></span> | <span class="t">Okay, imagine we want to create a reward model for our language model, which will become our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=665" target="_blank">00:11:05.820</a></span> | <span class="t">Reinforcement learning agent. Now to reward the model for generating a particular answer for questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=673" target="_blank">00:11:13.580</a></span> | <span class="t">We could create a dataset like this of questions and answers generated by the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=679" target="_blank">00:11:19.660</a></span> | <span class="t">For example, imagine we ask the model where is shanghai the model language model could say. Okay. Shanghai is a city in china. We should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=686" target="_blank">00:11:26.140</a></span> | <span class="t">Give some reward to this answer. So how good this answer is?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=690" target="_blank">00:11:30.540</a></span> | <span class="t">Now in my case, I would give it a high reward because I believe that the answer is short and to the point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=697" target="_blank">00:11:37.340</a></span> | <span class="t">But some other people may think that this answer is too short. So they maybe want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=701" target="_blank">00:11:41.420</a></span> | <span class="t">They prefer an answer that is a little longer or in this case, for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=706" target="_blank">00:11:46.380</a></span> | <span class="t">What is two plus two suppose that our language model only says the word four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=709" target="_blank">00:11:49.980</a></span> | <span class="t">Now some in my case, I believe this answer is too short</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=713" target="_blank">00:11:53.820</a></span> | <span class="t">so it could be a little more elaborate, but some other people may think that this answer is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=717" target="_blank">00:11:57.980</a></span> | <span class="t">Is good enough now what kind of reward should we give to this answer or this answer as you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=725" target="_blank">00:12:05.020</a></span> | <span class="t">It's not easy to come up with a number that can be accepted by everyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=729" target="_blank">00:12:09.660</a></span> | <span class="t">So us humans are not very good at finding a common ground for agreement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=734" target="_blank">00:12:14.380</a></span> | <span class="t">But unfortunately, we are very good at comparing so we will exploit this fact to create our data set for training our reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=742" target="_blank">00:12:22.060</a></span> | <span class="t">So what if instead of generating one answer we could generate multiple answers using the same language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=748" target="_blank">00:12:28.940</a></span> | <span class="t">This can be done for example by using a high temperature and then we can ask a group of people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=754" target="_blank">00:12:34.940</a></span> | <span class="t">So expert labelers experts in this field to choose which answer they prefer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=761" target="_blank">00:12:41.900</a></span> | <span class="t">and having this data set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=764" target="_blank">00:12:44.460</a></span> | <span class="t">Preferences we can then create a model that will generate a numeric reward for each question and answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=772" target="_blank">00:12:52.860</a></span> | <span class="t">So first we create a data set of questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=777" target="_blank">00:12:57.340</a></span> | <span class="t">Then we ask the language model to generate multiple answers for the same question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=780" target="_blank">00:13:00.940</a></span> | <span class="t">For example by using a high temperature and then we ask people to choose which answer they prefer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=786" target="_blank">00:13:06.140</a></span> | <span class="t">Now our goal is to create a neural network, which will act as a reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=791" target="_blank">00:13:11.180</a></span> | <span class="t">so a model that given a question and an answer will generate a numeric value for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=796" target="_blank">00:13:16.780</a></span> | <span class="t">in such a way that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=799" target="_blank">00:13:19.740</a></span> | <span class="t">Answer that has been chosen should have a high reward and the answer that has not been chosen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=805" target="_blank">00:13:25.100</a></span> | <span class="t">Which is something that we don't like should have a low reward. Let's see how it is done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=809" target="_blank">00:13:29.820</a></span> | <span class="t">What we do in practice is that we take a pre-trained language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=814" target="_blank">00:13:34.620</a></span> | <span class="t">For example, we can take the pre-trained llama and we feed the language model the question and answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=821" target="_blank">00:13:41.180</a></span> | <span class="t">So the input tokens here you can see are the questions and the answer concatenated together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=826" target="_blank">00:13:46.700</a></span> | <span class="t">We give it to the language model as input the language model. It's a transformer model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=831" target="_blank">00:13:51.820</a></span> | <span class="t">So it will generate some output embeddings. These are called hidden states</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=836" target="_blank">00:13:56.140</a></span> | <span class="t">So as you know, the input are the tokens which are converted into embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=840" target="_blank">00:14:00.940</a></span> | <span class="t">Then the positional encoding then we feed it to the transformer layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=843" target="_blank">00:14:03.980</a></span> | <span class="t">The transformer layer will actually output some embeddings which are called hidden states</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=849" target="_blank">00:14:09.020</a></span> | <span class="t">And usually for text generation, we take the last hidden state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=854" target="_blank">00:14:14.060</a></span> | <span class="t">We send it to some linear layer which will project it into the vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=858" target="_blank">00:14:18.300</a></span> | <span class="t">Then we use the softmax and then we select the next token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=861" target="_blank">00:14:21.180</a></span> | <span class="t">But instead of selecting because here we are we do not want to generate a text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=865" target="_blank">00:14:25.820</a></span> | <span class="t">We just want to generate a numeric reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=868" target="_blank">00:14:28.540</a></span> | <span class="t">We can substitute the linear layer that is projecting the last hidden state into the vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=874" target="_blank">00:14:34.940</a></span> | <span class="t">But instead we replace it with another linear layer with only one output feature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=879" target="_blank">00:14:39.180</a></span> | <span class="t">So that it will take an input embedding as input and generate only one value as output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=885" target="_blank">00:14:45.020</a></span> | <span class="t">Which will be the reward assigned to the answer for the particular given question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=894" target="_blank">00:14:54.620</a></span> | <span class="t">Of course this is the architecture of the model. We also need to train it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=898" target="_blank">00:14:58.700</a></span> | <span class="t">So we also need to tell this model that it has to generate a high reward for answers that are chosen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=904" target="_blank">00:15:04.380</a></span> | <span class="t">And low reward for answers that are not chosen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=907" target="_blank">00:15:07.420</a></span> | <span class="t">Let's see what is the loss function that we will use to train this model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=911" target="_blank">00:15:11.100</a></span> | <span class="t">The loss function that we will be using is this one here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=914" target="_blank">00:15:14.780</a></span> | <span class="t">So you can see it's minus the log of the sigmoid of the reward assigned to the good answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=921" target="_blank">00:15:21.740</a></span> | <span class="t">Minus the reward assigned to the bad answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=924" target="_blank">00:15:24.700</a></span> | <span class="t">now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=926" target="_blank">00:15:26.860</a></span> | <span class="t">Let's analyze this loss function here. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=929" target="_blank">00:15:29.580</a></span> | <span class="t">Pen, okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=932" target="_blank">00:15:32.300</a></span> | <span class="t">So there are two possibilities either this difference here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=935" target="_blank">00:15:35.420</a></span> | <span class="t">So he is a negative or it is positive which means that either</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=940" target="_blank">00:15:40.140</a></span> | <span class="t">The response assigned to the so how do we train it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=944" target="_blank">00:15:44.140</a></span> | <span class="t">First of all basically because our data set is made up of questions and possible answers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=949" target="_blank">00:15:49.100</a></span> | <span class="t">I suppose there are only two possible answers. One is a good one. And one is the bad one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=953" target="_blank">00:15:53.100</a></span> | <span class="t">We take each question answers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=956" target="_blank">00:15:56.140</a></span> | <span class="t">We feed the question to the model along with the answer concatenated to it and we general model will generate some reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=963" target="_blank">00:16:03.500</a></span> | <span class="t">We do it for the good question and for the sorry for the good answer and also for the bad answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=969" target="_blank">00:16:09.740</a></span> | <span class="t">And it will generate two rewards suppose. This is the reward for the good one. So let's write good one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=977" target="_blank">00:16:17.180</a></span> | <span class="t">And this is the reward associated with the bad one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=979" target="_blank">00:16:19.900</a></span> | <span class="t">Now either the model assigned a high reward to the good one and a low reward to the bad one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=987" target="_blank">00:16:27.900</a></span> | <span class="t">So this difference will be positive and this is good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=991" target="_blank">00:16:31.020</a></span> | <span class="t">So in this case the loss will be like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=993" target="_blank">00:16:33.500</a></span> | <span class="t">So if the reward given to the good answer is higher than the reward given to the bad answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=999" target="_blank">00:16:39.260</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1001" target="_blank">00:16:41.100</a></span> | <span class="t">This difference will be positive. So let's see the sigmoid function. How does it behave when the input is positive?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1007" target="_blank">00:16:47.100</a></span> | <span class="t">So when the input is positive the sigmoid gives an output value that is between 0.5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1012" target="_blank">00:16:52.000</a></span> | <span class="t">And one so this stuff here will be between 0.5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1017" target="_blank">00:16:57.440</a></span> | <span class="t">And one when the log receives any because here you can think of as having a parenthesis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1025" target="_blank">00:17:05.820</a></span> | <span class="t">When the log sees an input that is between 0.5 and 1 will generate a number negative number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1031" target="_blank">00:17:11.820</a></span> | <span class="t">That is more or less between 0 and minus 1 more or less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1035" target="_blank">00:17:15.180</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1037" target="_blank">00:17:17.260</a></span> | <span class="t">With the minus sign here, it will become a positive number between 0 and 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1041" target="_blank">00:17:21.900</a></span> | <span class="t">So the loss in this case will be small because it will be a number between more or less between 0 and 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1049" target="_blank">00:17:29.980</a></span> | <span class="t">I maybe it's two or three but okay depends on the graph of the log. I don't remember. What is the exact value for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1057" target="_blank">00:17:37.660</a></span> | <span class="t">0.5 here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1060" target="_blank">00:17:40.220</a></span> | <span class="t">However, let's see if the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1062" target="_blank">00:17:42.540</a></span> | <span class="t">Gave a high score to the bad response and the low score to the good response. So let's start again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1069" target="_blank">00:17:49.260</a></span> | <span class="t">Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1072" target="_blank">00:17:52.380</a></span> | <span class="t">And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1074" target="_blank">00:17:54.380</a></span> | <span class="t">Okay, here is the bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1077" target="_blank">00:17:57.740</a></span> | <span class="t">Response and here is the good response</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1080" target="_blank">00:18:00.080</a></span> | <span class="t">Now what happens if this value here is smaller than this value here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1086" target="_blank">00:18:06.620</a></span> | <span class="t">So this difference will be negative when the sigmoid receives as input something that is negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1092" target="_blank">00:18:12.880</a></span> | <span class="t">It will return an output that is between 0 and 0.5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1097" target="_blank">00:18:17.440</a></span> | <span class="t">The log when it sees an input that is between 0 and 0.5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1103" target="_blank">00:18:23.660</a></span> | <span class="t">So more or less here it will return a number negative number that is between minus infinity and more or less one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1110" target="_blank">00:18:30.380</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1112" target="_blank">00:18:32.140</a></span> | <span class="t">It will return because there is a minus sign here. It will become a very big number in the negative range</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1117" target="_blank">00:18:37.420</a></span> | <span class="t">So the loss in this case will be big so big loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1121" target="_blank">00:18:41.260</a></span> | <span class="t">Here it was a small loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1125" target="_blank">00:18:45.340</a></span> | <span class="t">small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1128" target="_blank">00:18:48.140</a></span> | <span class="t">Loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1129" target="_blank">00:18:49.580</a></span> | <span class="t">Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1130" target="_blank">00:18:50.620</a></span> | <span class="t">Now as you can see when the reward model is real is giving a high reward to the good answer and a bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1137" target="_blank">00:18:57.660</a></span> | <span class="t">A low score to the bad answer. The loss is small. However, when the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1144" target="_blank">00:19:04.140</a></span> | <span class="t">model gives a high reward to the bad answer and a low score to the good answer the loss is very big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1151" target="_blank">00:19:11.980</a></span> | <span class="t">What does that what does it mean this for the model that it will force the model to always give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1159" target="_blank">00:19:19.260</a></span> | <span class="t">High rewards to the winning response and low reward to the losing response</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1164" target="_blank">00:19:24.140</a></span> | <span class="t">So it because that's the only way for the model to minimize the loss because the goal of the model always during training is to minimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1171" target="_blank">00:19:31.180</a></span> | <span class="t">The loss so the model will be forced to give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1173" target="_blank">00:19:33.420</a></span> | <span class="t">High reward to the chosen answer and the low reward to the not chosen answer or the bad answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1179" target="_blank">00:19:39.420</a></span> | <span class="t">In hugging face you we can this reward model is implemented in the reward trainer class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1188" target="_blank">00:19:48.380</a></span> | <span class="t">So if you want to train your own reward model, you need to use this reward trainer class and it will take as input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1195" target="_blank">00:19:55.020</a></span> | <span class="t">A auto model for sequence classification, which is exactly this architecture here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1200" target="_blank">00:20:00.300</a></span> | <span class="t">So it's a transformer model with instead of having the linear layer that projects into the vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1205" target="_blank">00:20:05.420</a></span> | <span class="t">It has a linear layer with only one output feature that gives the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1209" target="_blank">00:20:09.660</a></span> | <span class="t">And if you look at the code on how this is implemented in the hugging face library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1214" target="_blank">00:20:14.300</a></span> | <span class="t">You will see that they first generate the reward for the chosen answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1219" target="_blank">00:20:19.500</a></span> | <span class="t">So for the good answer, then they generate the reward for the bad answer. So for the rejected response here, it's called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1227" target="_blank">00:20:27.020</a></span> | <span class="t">And then they calculated the loss exactly using the formula that we saw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1231" target="_blank">00:20:31.020</a></span> | <span class="t">So the log sigmoid of the rewards given to the chosen one minus the rewards given to the rejected one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1238" target="_blank">00:20:38.460</a></span> | <span class="t">Let's talk about trajectories now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1241" target="_blank">00:20:41.900</a></span> | <span class="t">Now as I said previously in reinforcement learning the goal is to select a policy or to optimize a policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1248" target="_blank">00:20:48.060</a></span> | <span class="t">That maximizes the expected return of the agent when the agent acts according to this policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1254" target="_blank">00:20:54.700</a></span> | <span class="t">More formally we can write it as follows that we want to select a policy pi</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1259" target="_blank">00:20:59.740</a></span> | <span class="t">That gives us the maximum expected reward when the agent acts according to this policy pi</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1267" target="_blank">00:21:07.980</a></span> | <span class="t">Now, what is the expected return? The expected return of the policy is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1272" target="_blank">00:21:12.620</a></span> | <span class="t">Expected return over all possible trajectories that the agent can have when using this policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1278" target="_blank">00:21:18.860</a></span> | <span class="t">So it's the expected return over all possible trajectories as you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1284" target="_blank">00:21:24.220</a></span> | <span class="t">The expectation can also be written as an integral. So it is the probability of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1288" target="_blank">00:21:28.940</a></span> | <span class="t">Particular trajectory using this policy multiplied by the return over that particular trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1295" target="_blank">00:21:35.980</a></span> | <span class="t">Now, what is a trajectory first of all and later we will see what is the probability of a trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1300" target="_blank">00:21:40.860</a></span> | <span class="t">So the trajectory is a series of states and actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1304" target="_blank">00:21:44.620</a></span> | <span class="t">Which means that a trajectory you can think of in the case of the cat as a path that the cat can take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1311" target="_blank">00:21:51.980</a></span> | <span class="t">Suppose that each of the trajectory have a maximum length. So we don't want the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1316" target="_blank">00:21:56.460</a></span> | <span class="t">agent to perform more than 10 steps to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1321" target="_blank">00:22:01.340</a></span> | <span class="t">To arrive to its goal. Now the cat can go to the meat for example using this path here or it can choose this path here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1328" target="_blank">00:22:08.860</a></span> | <span class="t">Or it can use this path here or this one here or for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1332" target="_blank">00:22:12.700</a></span> | <span class="t">It can go forward and then go backward and then stop because it has already used the 10 steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1337" target="_blank">00:22:17.580</a></span> | <span class="t">Or it can go like this etc etc. So there are many many many paths. What we want is we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1344" target="_blank">00:22:24.300</a></span> | <span class="t">we want to find a policy that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1348" target="_blank">00:22:28.100</a></span> | <span class="t">Maximizes the expected return so the return that we get along each of these paths</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1353" target="_blank">00:22:33.880</a></span> | <span class="t">Now, we will also model the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1357" target="_blank">00:22:37.960</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1359" target="_blank">00:22:39.800</a></span> | <span class="t">The the next state of the cat has been stochastic. So first of all, let's introduce what is the these states and actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1366" target="_blank">00:22:46.840</a></span> | <span class="t">So let me give you an example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1368" target="_blank">00:22:48.920</a></span> | <span class="t">Suppose that our cat is starting from some state s0 which is the initial state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1374" target="_blank">00:22:54.680</a></span> | <span class="t">The policy tells us what is the next action that we should take given the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1379" target="_blank">00:22:59.080</a></span> | <span class="t">So the cat will ask the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1380" target="_blank">00:23:00.920</a></span> | <span class="t">What is the next action that it should take?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1383" target="_blank">00:23:03.080</a></span> | <span class="t">And because the policy is a stochastic this policy will tell us what is the probability of the next action. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1389" target="_blank">00:23:09.800</a></span> | <span class="t">Just like in the case of the language model we given a prompt we select what is the probability of the next token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1396" target="_blank">00:23:16.680</a></span> | <span class="t">So imagine that the policy tells us that the cat should move down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1401" target="_blank">00:23:21.240</a></span> | <span class="t">So action down for example with very high probability or it should move right with lower probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1408" target="_blank">00:23:28.760</a></span> | <span class="t">It should move left with even lower probability or it should move up with an even lower probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1414" target="_blank">00:23:34.380</a></span> | <span class="t">Suppose that we select to move down it will result in a new state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1418" target="_blank">00:23:38.920</a></span> | <span class="t">That may not be exactly this one. Why? Because we model the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1424" target="_blank">00:23:44.760</a></span> | <span class="t">Cat as being drunk, which means that the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1427" target="_blank">00:23:47.640</a></span> | <span class="t">Wants to move down but may not always move down and we will see later why this is helpful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1434" target="_blank">00:23:54.600</a></span> | <span class="t">But another case could be for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1436" target="_blank">00:23:56.680</a></span> | <span class="t">Imagine we have a robot and the robot wants to move down but the wheels of the robot are broken</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1443" target="_blank">00:24:03.080</a></span> | <span class="t">So the robot will not actually move down. It will remain in the same state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1446" target="_blank">00:24:06.760</a></span> | <span class="t">So we always model the next state not as being deterministically determined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1452" target="_blank">00:24:12.280</a></span> | <span class="t">But as being stochastic given the current state and the action that we choose to perform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1457" target="_blank">00:24:17.240</a></span> | <span class="t">So imagine that we choose to perform the action down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1460" target="_blank">00:24:20.600</a></span> | <span class="t">The cat may arrive to a new state s1 which will be according to some probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1466" target="_blank">00:24:26.380</a></span> | <span class="t">Then we can ask again the policy. What is the next action I should do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1469" target="_blank">00:24:29.800</a></span> | <span class="t">Policy might say okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1471" target="_blank">00:24:31.320</a></span> | <span class="t">You should move right with very high probability and you should move down with a lower probability or you should move left with an even lower</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1478" target="_blank">00:24:38.920</a></span> | <span class="t">Probability etc etc. So as you can see, we are creating a trajectory which is a series of states and actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1484" target="_blank">00:24:44.840</a></span> | <span class="t">Which define how our cat will move in a particular trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1490" target="_blank">00:24:50.700</a></span> | <span class="t">Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1494" target="_blank">00:24:54.120</a></span> | <span class="t">Let's see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1495" target="_blank">00:24:55.080</a></span> | <span class="t">Now, what is the probability of a trajectory? The probability of a trajectory as you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1499" target="_blank">00:24:59.960</a></span> | <span class="t">The fact that we chose a particular action depends only on the state we were in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1505" target="_blank">00:25:05.960</a></span> | <span class="t">And the fact that we arrived to this state here depended on the state we were in and the action that we have chosen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1512" target="_blank">00:25:12.040</a></span> | <span class="t">and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1513" target="_blank">00:25:13.880</a></span> | <span class="t">The fact that we have chosen this action here depended only on this state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1517" target="_blank">00:25:17.640</a></span> | <span class="t">We were in because the policy only takes as input the state and gives us what is the probability of the action that we should take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1523" target="_blank">00:25:23.720</a></span> | <span class="t">So we can because they are independent from each other these events</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1528" target="_blank">00:25:28.600</a></span> | <span class="t">We can multiply them together to get the probability of the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1532" target="_blank">00:25:32.840</a></span> | <span class="t">So the probability of the trajectory is the probability of starting from a particular starting point. So from this state zero here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1539" target="_blank">00:25:39.400</a></span> | <span class="t">Then for each step that we have take so for each action state of this particular trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1545" target="_blank">00:25:45.500</a></span> | <span class="t">We have the probability of choosing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1547" target="_blank">00:25:47.880</a></span> | <span class="t">This the action given the state and then to arriving a new state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1552" target="_blank">00:25:52.840</a></span> | <span class="t">Given that we were at this state at time step t and we chose action 80 at time step t</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1560" target="_blank">00:26:00.760</a></span> | <span class="t">And we multiply all these probabilities together because they are independent from each other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1565" target="_blank">00:26:05.160</a></span> | <span class="t">Another thing that we will consider is that is when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1569" target="_blank">00:26:09.400</a></span> | <span class="t">How do we calculate the reward of a trajectory?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1573" target="_blank">00:26:13.100</a></span> | <span class="t">A very simple way to calculate the reward of a trajectory is to just sum all the rewards that we get along this trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1579" target="_blank">00:26:19.800</a></span> | <span class="t">For example, imagine the cat to arrive to the meat follows this trajectory. You could say that the reward is zero here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1586" target="_blank">00:26:26.360</a></span> | <span class="t">So it's zero zero zero zero zero zero zero and then suddenly it becomes plus 100 when we reach the meat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1593" target="_blank">00:26:33.880</a></span> | <span class="t">If the cat for example follows this path here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1596" target="_blank">00:26:36.760</a></span> | <span class="t">We could say okay, it will receive minus one because the cat is scared of the broom then zero zero zero zero zero one hundred</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1603" target="_blank">00:26:43.880</a></span> | <span class="t">Actually, this is not how we will calculate the reward of a trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1608" target="_blank">00:26:48.280</a></span> | <span class="t">We will actually calculate the reward as a discounted which means that we prefer immediate rewards instead of future rewards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1615" target="_blank">00:26:55.560</a></span> | <span class="t">To give you an intuition in why this happens. First. Let me talk about money</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1619" target="_blank">00:26:59.560</a></span> | <span class="t">So if I give you ten thousand dollars today</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1622" target="_blank">00:27:02.120</a></span> | <span class="t">You prefer receiving it today instead of receiving it in one year</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1625" target="_blank">00:27:05.800</a></span> | <span class="t">Why because you could put the ten thousand dollars in the bank. It will generate some interest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1630" target="_blank">00:27:10.360</a></span> | <span class="t">So at the end of the year, you will have more than ten thousand dollars</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1633" target="_blank">00:27:13.560</a></span> | <span class="t">And in the case of reinforcement learning, this is helpful also for another case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1637" target="_blank">00:27:17.640</a></span> | <span class="t">For example, imagine the cat can only take 10 steps to arrive to the meat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1642" target="_blank">00:27:22.920</a></span> | <span class="t">Or 20 steps. So one way for the cat to arrive to the meat is to just go directly to the meat like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1648" target="_blank">00:27:28.840</a></span> | <span class="t">And this is one trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1650" target="_blank">00:27:30.840</a></span> | <span class="t">But another way for the cat is to go like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1653" target="_blank">00:27:33.320</a></span> | <span class="t">For example, go here then go here then go here then go here and then go here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1657" target="_blank">00:27:37.480</a></span> | <span class="t">So in this case, we prefer the cat to go directly to the meat instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1662" target="_blank">00:27:42.520</a></span> | <span class="t">Taking this longer route. Why? Because we modeled the next state as being stochastic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1668" target="_blank">00:27:48.520</a></span> | <span class="t">And if we take a longer route the probability of ending up in one of these obstacles is higher the longer the route is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1676" target="_blank">00:27:56.040</a></span> | <span class="t">So we prefer having shorter routes in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1681" target="_blank">00:28:01.080</a></span> | <span class="t">And this is also convenient from a mathematical point of view to have this discounted rewards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1686" target="_blank">00:28:06.780</a></span> | <span class="t">Because this series which is infinite in some cases, okay, we will not work with infinite series</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1694" target="_blank">00:28:14.200</a></span> | <span class="t">but it's helpful because this series can converge if this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1698" target="_blank">00:28:18.280</a></span> | <span class="t">Element of the series is becoming smaller and smaller and smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1704" target="_blank">00:28:24.520</a></span> | <span class="t">So let me give you a practical example of how to calculate the reward in a discounted case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1709" target="_blank">00:28:29.880</a></span> | <span class="t">so imagine the cat starts from here and it goes to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1714" target="_blank">00:28:34.440</a></span> | <span class="t">follows this path so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1717" target="_blank">00:28:37.640</a></span> | <span class="t">to calculate the reward of this trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1720" target="_blank">00:28:40.680</a></span> | <span class="t">We will do like this. So it is the reward at time step zero, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1724" target="_blank">00:28:44.920</a></span> | <span class="t">Arriving to the broom multiplied by gamma to the power of one. So it will be gamma multiplied by minus one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1732" target="_blank">00:28:52.440</a></span> | <span class="t">then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1734" target="_blank">00:28:54.360</a></span> | <span class="t">All these rewards are 0 0 0 so they will not be summed up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1737" target="_blank">00:28:57.880</a></span> | <span class="t">And finally we arrive here at where the reward is plus 100 at time step 1 2 3 4 5 6 7 8</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1746" target="_blank">00:29:06.920</a></span> | <span class="t">So it will be gamma to the power of 8 multiplied by 100</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1750" target="_blank">00:29:10.780</a></span> | <span class="t">So gamma is usually chosen. Not usually. It's always something that is between 0 and 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1757" target="_blank">00:29:17.240</a></span> | <span class="t">So it's a number smaller than 1. So it means that we are decaying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1761" target="_blank">00:29:21.100</a></span> | <span class="t">this reward by gamma to the power of 8 so it will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1765" target="_blank">00:29:25.640</a></span> | <span class="t">Smaller the longer we take to reach it. This is the intuition behind discounted rewards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1771" target="_blank">00:29:31.960</a></span> | <span class="t">Now you may be wondering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1776" target="_blank">00:29:36.120</a></span> | <span class="t">The trajectories make sense in the case of the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1778" target="_blank">00:29:38.440</a></span> | <span class="t">So I can see that the cat will follow some path to arrive to the meat and it can take many paths to arrive to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1784" target="_blank">00:29:44.040</a></span> | <span class="t">Meat to so so we know what is the trajectory in the case of the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1787" target="_blank">00:29:47.720</a></span> | <span class="t">But what are the trajectories in case of language model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1790" target="_blank">00:29:50.520</a></span> | <span class="t">Well, as I saw before, we want to we have a policy which is the language model itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1796" target="_blank">00:29:56.520</a></span> | <span class="t">So because the policy tells us given the state what is the next action and in the case of language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1801" target="_blank">00:30:01.800</a></span> | <span class="t">We can see that the language model itself is a policy and we want to optimize this policy such that it selects</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1809" target="_blank">00:30:09.320</a></span> | <span class="t">The next token in such a way as to maximize a cumulative reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1814" target="_blank">00:30:14.360</a></span> | <span class="t">According to the reward model that we have built before using the data set of preferences that I saw before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1820" target="_blank">00:30:20.360</a></span> | <span class="t">Also in the case of the language model the trajectory is a series of states and actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1825" target="_blank">00:30:25.480</a></span> | <span class="t">What are the states in the case of the language model? Are they prompts? What are the actions? Are the next tokens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1831" target="_blank">00:30:31.880</a></span> | <span class="t">So imagine we have a question like this too for the language model. So where is shanghai?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1836" target="_blank">00:30:36.280</a></span> | <span class="t">Of course, we will ask the language model. What is the next token which will this will become the initial prompt?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1841" target="_blank">00:30:41.960</a></span> | <span class="t">So the initial state of the language model we will ask the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1845" target="_blank">00:30:45.320</a></span> | <span class="t">What is the next token and that will become our action the token that we choose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1849" target="_blank">00:30:49.960</a></span> | <span class="t">But then we feed it back to the language model. So it will become the new state of the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1855" target="_blank">00:30:55.400</a></span> | <span class="t">And then we ask the language model again. What is the next token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1859" target="_blank">00:30:59.080</a></span> | <span class="t">It will be for example, the word is and this will become again the input of the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1864" target="_blank">00:31:04.280</a></span> | <span class="t">So the next state and then we ask the language model again. What is the next token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1868" target="_blank">00:31:08.840</a></span> | <span class="t">For example, we choose the token in and then the concatenation of all these tokens will become the new state of the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1875" target="_blank">00:31:15.880</a></span> | <span class="t">So we ask the language model again. What is the next token, etc, etc until we generate an answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1881" target="_blank">00:31:21.000</a></span> | <span class="t">So as you can see also in the case of the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1883" target="_blank">00:31:23.720</a></span> | <span class="t">We have trajectories which are the series of prompts and the tokens that we have chosen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1889" target="_blank">00:31:29.720</a></span> | <span class="t">Now imagine that we have a policy because we our goal is to optimize our language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1894" target="_blank">00:31:34.520</a></span> | <span class="t">Which is a policy such that we maximize a cumulative reward according to some reward model that we have built in the past</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1901" target="_blank">00:31:41.240</a></span> | <span class="t">now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1903" target="_blank">00:31:43.480</a></span> | <span class="t">Our more formally our goal is this so we want to maximize this function here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1908" target="_blank">00:31:48.600</a></span> | <span class="t">Which is the expected return over all possible trajectories that our language model can generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1914" target="_blank">00:31:54.120</a></span> | <span class="t">And we also saw that before the trajectory is a series of prompts and next tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1918" target="_blank">00:31:58.840</a></span> | <span class="t">Now when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1921" target="_blank">00:32:01.960</a></span> | <span class="t">Use stochastic gradient descent. So for example when we try to optimize the neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1925" target="_blank">00:32:05.880</a></span> | <span class="t">We use stochastic gradient descent, which means that we have some kind of loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1929" target="_blank">00:32:09.800</a></span> | <span class="t">We calculate the gradient of the loss function with respect to the parameters of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1935" target="_blank">00:32:15.000</a></span> | <span class="t">And we change the parameters of the model such that we move against the direction of this gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1941" target="_blank">00:32:21.800</a></span> | <span class="t">So we take little steps against the direction of the gradient to optimize the parameters of the model to minimize this loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1949" target="_blank">00:32:29.400</a></span> | <span class="t">In our case, we do not want to minimize a loss function. We want to maximize a function which is here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1955" target="_blank">00:32:35.720</a></span> | <span class="t">And this is can also be thought of as an objective function that we want to maximize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1960" target="_blank">00:32:40.860</a></span> | <span class="t">So instead of using a gradient descent, we will use a gradient ascent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1964" target="_blank">00:32:44.760</a></span> | <span class="t">The only difference between the two is that instead of having a minus sign here. We have a plus sign</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1970" target="_blank">00:32:50.760</a></span> | <span class="t">Now, this algorithm is called the policy gradient optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1975" target="_blank">00:32:55.740</a></span> | <span class="t">And the point is we need to calculate the gradient of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1979" target="_blank">00:32:59.720</a></span> | <span class="t">Function here of our objective function. So what is the gradient with respect to the parameters of our model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1986" target="_blank">00:33:06.760</a></span> | <span class="t">So our language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1988" target="_blank">00:33:08.680</a></span> | <span class="t">what is the gradient of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1990" target="_blank">00:33:10.680</a></span> | <span class="t">Expected return over all possible trajectories with respect to the parameters of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=1996" target="_blank">00:33:16.920</a></span> | <span class="t">We need to find an expression of this gradient so that we can calculate it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2001" target="_blank">00:33:21.560</a></span> | <span class="t">And use it to optimize the parameters of the model using gradient ascent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2005" target="_blank">00:33:25.800</a></span> | <span class="t">Using also a learning rate alpha you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2009" target="_blank">00:33:29.000</a></span> | <span class="t">Now, let's see how to derive the expression of the gradient of this objective function that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2015" target="_blank">00:33:35.640</a></span> | <span class="t">Now the gradient of the objective function is the gradient of this expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2021" target="_blank">00:33:41.880</a></span> | <span class="t">so it's the expectation over all possible trajectory of multiplied by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2026" target="_blank">00:33:46.280</a></span> | <span class="t">The return over the particular trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2030" target="_blank">00:33:50.700</a></span> | <span class="t">As we know the expectation is also an integral</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2033" target="_blank">00:33:53.720</a></span> | <span class="t">So it can be written as the gradient of the integral of the probability of following a particular trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2039" target="_blank">00:33:59.900</a></span> | <span class="t">Multiplied by the return over this trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2043" target="_blank">00:34:03.020</a></span> | <span class="t">as you know from high school the gradient of a sum is equal to the sum of the gradients or the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2051" target="_blank">00:34:11.720</a></span> | <span class="t">You may recall it as the derivative. So the derivative of a sum of a function is equal to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2057" target="_blank">00:34:17.480</a></span> | <span class="t">sum of the derivatives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2060" target="_blank">00:34:20.040</a></span> | <span class="t">So we can bring this gradient sign inside and it can it can be written like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2065" target="_blank">00:34:25.400</a></span> | <span class="t">Now we will use a trick called the log derivative trick to expand this expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2071" target="_blank">00:34:31.080</a></span> | <span class="t">So p of tau given theta into this expression here. Let me show you how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2077" target="_blank">00:34:37.400</a></span> | <span class="t">Let's use the pen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2081" target="_blank">00:34:41.480</a></span> | <span class="t">Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2083" target="_blank">00:34:43.000</a></span> | <span class="t">You may recall also from calculus that the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2086" target="_blank">00:34:46.300</a></span> | <span class="t">with respect to theta of the log function of the log of a function in this case of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2093" target="_blank">00:34:53.720</a></span> | <span class="t">p of tau given theta</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2097" target="_blank">00:34:57.640</a></span> | <span class="t">Is equal to so the gradient of the derivative of the log function is one over the function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2106" target="_blank">00:35:06.200</a></span> | <span class="t">p of tau given theta multiplied by the gradient with respect to theta</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2112" target="_blank">00:35:12.840</a></span> | <span class="t">of the function that is inside the log so p of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2116" target="_blank">00:35:16.040</a></span> | <span class="t">tau given theta</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2119" target="_blank">00:35:19.240</a></span> | <span class="t">We can take this term to the left side multiply it here and this expression here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2125" target="_blank">00:35:25.800</a></span> | <span class="t">Will become equal to the this expression multiplied by this expression and this is exactly what you see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2132" target="_blank">00:35:32.760</a></span> | <span class="t">So we can replace this expression that we see in the equation above. So this expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2137" target="_blank">00:35:37.900</a></span> | <span class="t">with this expression we can see here in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2141" target="_blank">00:35:41.320</a></span> | <span class="t">equation below</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2143" target="_blank">00:35:43.720</a></span> | <span class="t">now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2145" target="_blank">00:35:45.720</a></span> | <span class="t">We can this integral we can write it back as an expectation over all possible trajectories of this quantity here now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2153" target="_blank">00:35:53.480</a></span> | <span class="t">Because the probability is only this term here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2156" target="_blank">00:35:56.520</a></span> | <span class="t">So we can write it back as a expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2161" target="_blank">00:36:01.000</a></span> | <span class="t">Now we need to expand this term here. So what is the gradient of the log?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2165" target="_blank">00:36:05.560</a></span> | <span class="t">So this this expression here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2168" target="_blank">00:36:08.440</a></span> | <span class="t">So what is the gradient of the log of probability of a particular trajectory given the parameters of the model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2175" target="_blank">00:36:15.320</a></span> | <span class="t">Let's expand it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2177" target="_blank">00:36:17.000</a></span> | <span class="t">we saw before that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2179" target="_blank">00:36:19.000</a></span> | <span class="t">probability of a trajectory is just the product of all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2182" target="_blank">00:36:22.280</a></span> | <span class="t">Probabilities of the state actions that are in this trajectory. So the probability of starting from a particular state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2189" target="_blank">00:36:29.540</a></span> | <span class="t">Multiplied by the probability of taking a particular action given the state we are in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2194" target="_blank">00:36:34.740</a></span> | <span class="t">multiplied by the probability of ending up in a new state given that we started from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2199" target="_blank">00:36:39.780</a></span> | <span class="t">The state at time step t and we took action at time step t</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2204" target="_blank">00:36:44.340</a></span> | <span class="t">And we do it for all the state actions that we have in this trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2208" target="_blank">00:36:48.840</a></span> | <span class="t">If we apply a log to this expression here, the product here will become a sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2217" target="_blank">00:36:57.060</a></span> | <span class="t">And let's do it actually. Okay, so we circle the log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2220" target="_blank">00:37:00.980</a></span> | <span class="t">of p of tau</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2225" target="_blank">00:37:05.060</a></span> | <span class="t">given pi</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2228" target="_blank">00:37:08.500</a></span> | <span class="t">Pi theta actually because we model our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2231" target="_blank">00:37:11.140</a></span> | <span class="t">Policy pi as parameterized by parameter theta here. I forgot the theta but doesn't matter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2237" target="_blank">00:37:17.540</a></span> | <span class="t">It's equal to the log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2240" target="_blank">00:37:20.500</a></span> | <span class="t">Of all this expression. So it's the log of a series of products so it can be written as the log of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2247" target="_blank">00:37:27.220</a></span> | <span class="t">p</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2249" target="_blank">00:37:29.220</a></span> | <span class="t">0 of s 0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2251" target="_blank">00:37:31.220</a></span> | <span class="t">plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2254" target="_blank">00:37:34.020</a></span> | <span class="t">the summation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2256" target="_blank">00:37:36.020</a></span> | <span class="t">The log of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2259" target="_blank">00:37:39.460</a></span> | <span class="t">p</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2264" target="_blank">00:37:44.340</a></span> | <span class="t">of s</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2266" target="_blank">00:37:46.340</a></span> | <span class="t">t plus 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2268" target="_blank">00:37:48.180</a></span> | <span class="t">Given that we are in st</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2270" target="_blank">00:37:50.180</a></span> | <span class="t">plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2272" target="_blank">00:37:52.260</a></span> | <span class="t">at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2273" target="_blank">00:37:53.540</a></span> | <span class="t">not plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2274" target="_blank">00:37:54.900</a></span> | <span class="t">and at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2276" target="_blank">00:37:56.900</a></span> | <span class="t">And we are in we took action at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2279" target="_blank">00:37:59.060</a></span> | <span class="t">plus the log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2282" target="_blank">00:38:02.100</a></span> | <span class="t">Of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2285" target="_blank">00:38:05.220</a></span> | <span class="t">This the action that we took according to our policy at given that we were in st</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2291" target="_blank">00:38:11.860</a></span> | <span class="t">Okay. Now we are also taking the gradient of this expression and as you can see here there is no term that depends on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2300" target="_blank">00:38:20.020</a></span> | <span class="t">Theta so it can be deleted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2303" target="_blank">00:38:23.220</a></span> | <span class="t">Also in this case, we do not have any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2305" target="_blank">00:38:25.460</a></span> | <span class="t">Expression that in this expression here. We do not have anything that depends on theta. So this can be deleted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2312" target="_blank">00:38:32.420</a></span> | <span class="t">Because the derivative of something that does not have the variable being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2318" target="_blank">00:38:38.500</a></span> | <span class="t">Derived is a constant so it can be deleted because it will be zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2322" target="_blank">00:38:42.980</a></span> | <span class="t">So the only term surviving in the summation is only these terms here because it's the only one that contains the theta</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2330" target="_blank">00:38:50.260</a></span> | <span class="t">As you can see here. So in the final expression is this one here. So this summation now, let me delete</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2336" target="_blank">00:38:56.660</a></span> | <span class="t">So we have derived</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2340" target="_blank">00:39:00.820</a></span> | <span class="t">An expression that allow us to calculate the gradient of the objective function because why we need the gradient of the objective function?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2347" target="_blank">00:39:07.940</a></span> | <span class="t">Because we want to run gradient ascent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2349" target="_blank">00:39:09.940</a></span> | <span class="t">Now one thing that we can see here. We still have this expectation over all possible trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2355" target="_blank">00:39:15.880</a></span> | <span class="t">now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2358" target="_blank">00:39:18.260</a></span> | <span class="t">To calculate over all possible trajectories in the case of the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2361" target="_blank">00:39:21.940</a></span> | <span class="t">It means that we need to calculate this gradient over all the possible paths that the cat can take of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2367" target="_blank">00:39:27.460</a></span> | <span class="t">Length, for example, 10 steps. So if we want to model trajectories of only length 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2373" target="_blank">00:39:33.460</a></span> | <span class="t">It means that we need to calculate all the possible paths that the cat can take of length 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2378" target="_blank">00:39:38.340</a></span> | <span class="t">And it could be a huge number in the case of language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2381" target="_blank">00:39:41.540</a></span> | <span class="t">It's even bigger because usually imagine we want to generate trajectories of size 100. It means that what are the possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2388" target="_blank">00:39:48.100</a></span> | <span class="t">All the possible texts that we can generate of size 100 tokens using our language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2394" target="_blank">00:39:54.660</a></span> | <span class="t">And for each of them we need to calculate the reward and the log action probabilities, which I will show later how to calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2400" target="_blank">00:40:00.820</a></span> | <span class="t">Now as you can see the problem is this expectation is over a lot of terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2406" target="_blank">00:40:06.420</a></span> | <span class="t">So it's intractable computationally to calculate them to calculate this expression because we would generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2412" target="_blank">00:40:12.020</a></span> | <span class="t">Need to generate a lot a lot a lot of text for the language model. So one way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2417" target="_blank">00:40:17.540</a></span> | <span class="t">To calculate this expectation is to approximate it with the sample mean so we can always approximate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2426" target="_blank">00:40:26.280</a></span> | <span class="t">An expectation with the sample mean so instead of calculating it over all the possible trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2433" target="_blank">00:40:33.300</a></span> | <span class="t">We can calculate it over some trajectories. So in the case of the cat it means that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2437" target="_blank">00:40:37.380</a></span> | <span class="t">Take the cat and we ask it to move using the policy for some number of steps and each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2444" target="_blank">00:40:44.020</a></span> | <span class="t">And we will generate one trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2446" target="_blank">00:40:46.900</a></span> | <span class="t">We do it many times and it will generate some trajectories in the case of the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2451" target="_blank">00:40:51.620</a></span> | <span class="t">We have some prompt we ask the language model to generate some text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2455" target="_blank">00:40:55.140</a></span> | <span class="t">Then we do it many times using different temperatures and different sampling strategies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2459" target="_blank">00:40:59.860</a></span> | <span class="t">For example by sampling randomly instead of using the greedy strategy. We can use the top p so it will generate many texts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2466" target="_blank">00:41:06.100</a></span> | <span class="t">Each text will represent a trajectory. We do not have to do it over all the possible text that the language model can generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2473" target="_blank">00:41:13.140</a></span> | <span class="t">But only some so it means that we will generate some trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2476" target="_blank">00:41:16.740</a></span> | <span class="t">So we can calculate this expression here only on some trajectory that our language model will generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2482" target="_blank">00:41:22.120</a></span> | <span class="t">And this will give us an approximation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2484" target="_blank">00:41:24.520</a></span> | <span class="t">of this gradient here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2487" target="_blank">00:41:27.620</a></span> | <span class="t">Once we have this gradient here, we can evaluate it over the trajectories that we have sampled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2493" target="_blank">00:41:33.160</a></span> | <span class="t">And then run gradient ascent on it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2495" target="_blank">00:41:35.700</a></span> | <span class="t">So practically it works like this in the case of the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2500" target="_blank">00:41:40.020</a></span> | <span class="t">We have some kind of neural network that defines the policy which is taking the state of the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2506" target="_blank">00:41:46.260</a></span> | <span class="t">Which is the position of the cat tells us what is the probability of the next action that the cat should take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2513" target="_blank">00:41:53.220</a></span> | <span class="t">We can use this policy, which is not optimized to generate some trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2518" target="_blank">00:41:58.180</a></span> | <span class="t">So for example, we start from here. We ask the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2521" target="_blank">00:42:01.220</a></span> | <span class="t">Where should I go and we for example, we use the greedy strategy and we move down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2525" target="_blank">00:42:05.300</a></span> | <span class="t">then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2527" target="_blank">00:42:07.060</a></span> | <span class="t">Or we use the top p for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2528" target="_blank">00:42:08.820</a></span> | <span class="t">Also in this case, we can use top p to sample randomly the action given the probabilities generated by the network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2536" target="_blank">00:42:16.340</a></span> | <span class="t">So imagine the cat goes down and then we ask again the policy. Where should I go?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2541" target="_blank">00:42:21.620</a></span> | <span class="t">Policy may say okay move right move down move right move right etc. So we will generate one trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2546" target="_blank">00:42:26.760</a></span> | <span class="t">We do it many times by sampling always randomly according to the probabilities generated by the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2552" target="_blank">00:42:32.760</a></span> | <span class="t">For each state actions, we will generate many trajectories in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2557" target="_blank">00:42:37.700</a></span> | <span class="t">Then we can evaluate because we also know the rewards that we accumulate over each state actions. We calculate the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2565" target="_blank">00:42:45.060</a></span> | <span class="t">We also know the log probabilities of the each action because for each state we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2570" target="_blank">00:42:50.420</a></span> | <span class="t">The log what is what was the probability of taking that action and we choose it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2575" target="_blank">00:42:55.540</a></span> | <span class="t">And we need to calculate also the gradient of this log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2581" target="_blank">00:43:01.140</a></span> | <span class="t">This is done by automatically by pytorch when you run lost dot backwards. So pytorch actually will calculate the gradient for you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2588" target="_blank">00:43:08.020</a></span> | <span class="t">We do it for all the other possible trajectories. This will give us the approximated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2595" target="_blank">00:43:15.200</a></span> | <span class="t">Gradient of over the trajectories that we have collected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2598" target="_blank">00:43:18.340</a></span> | <span class="t">We run gradient ascent and we optimize the parameters of the model using a step towards the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2605" target="_blank">00:43:25.540</a></span> | <span class="t">Now then we need to go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2608" target="_blank">00:43:28.480</a></span> | <span class="t">We do we need to do it again. So we need to collect more trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2612" target="_blank">00:43:32.180</a></span> | <span class="t">We evaluate them. We evaluate the gradient of the log probabilities. We run a gradient ascent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2618" target="_blank">00:43:38.880</a></span> | <span class="t">So we take one little step towards the direction of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2622" target="_blank">00:43:42.740</a></span> | <span class="t">And then we do it again. We go again collect some trajectories. We evaluate this expression here to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2629" target="_blank">00:43:49.760</a></span> | <span class="t">Calculate the gradient of the policy with respect to the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2634" target="_blank">00:43:54.180</a></span> | <span class="t">And we run again gradient ascent so a little step towards the direction of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2640" target="_blank">00:44:00.880</a></span> | <span class="t">This is known as the reinforcement learning algorithm in literature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2645" target="_blank">00:44:05.840</a></span> | <span class="t">And we can use it also to optimize our language model. So in the case of the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2651" target="_blank">00:44:11.040</a></span> | <span class="t">We we have to also generate some trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2653" target="_blank">00:44:13.780</a></span> | <span class="t">So one way to generate the trajectories would be to for example use the database of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2659" target="_blank">00:44:19.040</a></span> | <span class="t">Questions and answers that we have built before for the reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2662" target="_blank">00:44:22.800</a></span> | <span class="t">Which means that we have some questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2665" target="_blank">00:44:25.680</a></span> | <span class="t">So we ask the language model to generate some answer for each question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2671" target="_blank">00:44:31.280</a></span> | <span class="t">Using for example the top piece strategy. So it will generate according to the temperature many different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2677" target="_blank">00:44:37.200</a></span> | <span class="t">answers for the same given question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2679" target="_blank">00:44:39.840</a></span> | <span class="t">This will be a series of trajectories because the language model generation process is an iterative process made up of states</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2688" target="_blank">00:44:48.080</a></span> | <span class="t">So prompts and actions and which are the next tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2691" target="_blank">00:44:51.520</a></span> | <span class="t">And this will result in a list of trajectories for which we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2696" target="_blank">00:44:56.320</a></span> | <span class="t">The log probabilities because the language model generates a list of probabilities over the next token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2701" target="_blank">00:45:01.840</a></span> | <span class="t">And we can also calculate the gradient of this state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2705" target="_blank">00:45:05.440</a></span> | <span class="t">Log probabilities using PyTorch because when we run loss.backward it will calculate the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2711" target="_blank">00:45:11.620</a></span> | <span class="t">But how do we do it in practice? Let's see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2715" target="_blank">00:45:15.360</a></span> | <span class="t">Now we want to calculate this term here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2718" target="_blank">00:45:18.960</a></span> | <span class="t">So the log probabilities of the action given the state for language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2724" target="_blank">00:45:24.400</a></span> | <span class="t">Which means what is the probability of the next token given a particular prompt?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2728" target="_blank">00:45:28.480</a></span> | <span class="t">Imagine that our language model has generated the following response</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2732" target="_blank">00:45:32.560</a></span> | <span class="t">So we asked the language model where is Shanghai and the language model said Shanghai is in China</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2738" target="_blank">00:45:38.160</a></span> | <span class="t">Our language model is a transformer model. So it is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2742" target="_blank">00:45:42.240</a></span> | <span class="t">transformer layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2745" target="_blank">00:45:45.280</a></span> | <span class="t">And it will generate a given an input sequence of embeddings. It will generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2749" target="_blank">00:45:49.300</a></span> | <span class="t">An output sequence of embeddings which are called hidden states one for each input token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2755" target="_blank">00:45:55.440</a></span> | <span class="t">As you know the language model when we use it for text generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2760" target="_blank">00:46:00.080</a></span> | <span class="t">It has a linear layer that allow us to calculate the logits for each position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2765" target="_blank">00:46:05.440</a></span> | <span class="t">So usually we calculate the logits only of the last token because we want to understand what is the next token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2771" target="_blank">00:46:11.520</a></span> | <span class="t">But actually we can calculate the logits for each position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2774" target="_blank">00:46:14.560</a></span> | <span class="t">So for example, we can also calculate the logits for this position and the logits for this position will indicate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2779" target="_blank">00:46:19.600</a></span> | <span class="t">What is the most likely next token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2782" target="_blank">00:46:22.240</a></span> | <span class="t">Given this input. So where is Shanghai?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2786" target="_blank">00:46:26.000</a></span> | <span class="t">question mark Shanghai is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2788" target="_blank">00:46:28.540</a></span> | <span class="t">So this is because of the causal mask that we apply during the self-attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2794" target="_blank">00:46:34.240</a></span> | <span class="t">So each hidden state actually encapsulates information about the current token. So in this case of the token is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2801" target="_blank">00:46:41.840</a></span> | <span class="t">And also all the previous tokens. This is a property of the transformer model that is used during training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2808" target="_blank">00:46:48.640</a></span> | <span class="t">So during training as you know, we do not calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2811" target="_blank">00:46:51.860</a></span> | <span class="t">The output of the language model step by step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2814" target="_blank">00:46:54.560</a></span> | <span class="t">We just give it the input sentence the output sentence, which is the shifted version of the input sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2820" target="_blank">00:47:00.080</a></span> | <span class="t">we calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2822" target="_blank">00:47:02.880</a></span> | <span class="t">For we do the forward pass and then we calculate the log using only one forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2827" target="_blank">00:47:07.600</a></span> | <span class="t">We can use the same mechanism to calculate the log probabilities for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2831" target="_blank">00:47:11.760</a></span> | <span class="t">States and actions in this trajectory, which as I showed you is a series of prompts and next tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2839" target="_blank">00:47:19.040</a></span> | <span class="t">Now we can calculate the logits for this position for this position for this position and for this position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2844" target="_blank">00:47:24.800</a></span> | <span class="t">then we usually we apply the softmax to understand what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2849" target="_blank">00:47:29.280</a></span> | <span class="t">Probability of the next token, but in this case, we want the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2854" target="_blank">00:47:34.080</a></span> | <span class="t">So we can apply the log softmax for each position. This will give us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2858" target="_blank">00:47:38.160</a></span> | <span class="t">What is the log probability of the next token given only the previous tokens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2862" target="_blank">00:47:42.720</a></span> | <span class="t">Compared to the current one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2864" target="_blank">00:47:44.640</a></span> | <span class="t">So for this position it will give us the log probability of the next token given that the input is only where is shanghai?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2870" target="_blank">00:47:50.880</a></span> | <span class="t">question mark shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2873" target="_blank">00:47:53.360</a></span> | <span class="t">Of course, we do not want all the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2876" target="_blank">00:47:56.240</a></span> | <span class="t">We only want the log probability of the token that actually has been chosen in this trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2881" target="_blank">00:48:01.440</a></span> | <span class="t">What is the actual token that has been chosen for this particular?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2885" target="_blank">00:48:05.220</a></span> | <span class="t">Position. Well, we know it. It's the word is so we only selected the log probability corresponding to the word is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2893" target="_blank">00:48:13.360</a></span> | <span class="t">This will return us the log probability for the entire trajectory because now we have the log probability of selecting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2900" target="_blank">00:48:20.020</a></span> | <span class="t">The word shanghai given the state where is shanghai?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2904" target="_blank">00:48:24.580</a></span> | <span class="t">We have the log probability of selecting the word is given the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2909" target="_blank">00:48:29.120</a></span> | <span class="t">Where is shanghai question mark shanghai?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2911" target="_blank">00:48:31.360</a></span> | <span class="t">We have the log probability of selecting the word in given the input where is shanghai question mark shanghai is etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2918" target="_blank">00:48:38.080</a></span> | <span class="t">So now we have the log probabilities of each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2921" target="_blank">00:48:41.440</a></span> | <span class="t">Of each position of each state action in this trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2927" target="_blank">00:48:47.140</a></span> | <span class="t">When we have this stuff here, we can always ask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2930" target="_blank">00:48:50.720</a></span> | <span class="t">PyTorch to run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2934" target="_blank">00:48:54.160</a></span> | <span class="t">The backward step to calculate the gradients and then we multiply each gradient by the reward that we receive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2941" target="_blank">00:49:01.440</a></span> | <span class="t">From our reward model we can then calculate this expression and then we can run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2946" target="_blank">00:49:06.400</a></span> | <span class="t">Gradient ascent to optimize our policy based on this approximated gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2952" target="_blank">00:49:12.100</a></span> | <span class="t">Let's see how to calculate the reward now for the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2956" target="_blank">00:49:16.020</a></span> | <span class="t">So calculating the reward is a similar process as you saw before we have a reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2961" target="_blank">00:49:21.440</a></span> | <span class="t">That is a transformer model with a linear layer on top that it has only one output feature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2966" target="_blank">00:49:26.960</a></span> | <span class="t">So imagine our sentence is the same. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2969" target="_blank">00:49:29.280</a></span> | <span class="t">Where is shanghai shanghai is in china. This is the trajectory that has been generated by our language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2975" target="_blank">00:49:35.040</a></span> | <span class="t">Now we give it to the reward model. The reward model will generate some hidden states because it's a transformer model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2981" target="_blank">00:49:41.840</a></span> | <span class="t">And we apply the linear layer to all the positions that are corresponding to the action that are in this trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2989" target="_blank">00:49:49.840</a></span> | <span class="t">So first action is the selection of this word. The second action is this one the third and the fourth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2994" target="_blank">00:49:54.400</a></span> | <span class="t">So we can generate the reward for each time step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=2997" target="_blank">00:49:57.600</a></span> | <span class="t">We can just sum these rewards to generate the total reward of the trajectory or we can sum the discounted reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3004" target="_blank">00:50:04.960</a></span> | <span class="t">Which means that we will calculate something like this. For example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3008" target="_blank">00:50:08.080</a></span> | <span class="t">We will calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3010" target="_blank">00:50:10.400</a></span> | <span class="t">Let's write it. So it will be the reward at time step zero plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3015" target="_blank">00:50:15.120</a></span> | <span class="t">gamma multiplied by the reward at time step one plus gamma multiplied by the reward at time gamma to the power of two multiplied at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3021" target="_blank">00:50:21.760</a></span> | <span class="t">By the reward at time step two plus gamma to the power of three multiplied by the reward at time step three, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3027" target="_blank">00:50:27.840</a></span> | <span class="t">Etc. So now we also know how to calculate the reward for each trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3031" target="_blank">00:50:31.620</a></span> | <span class="t">So now we know how to evaluate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3034" target="_blank">00:50:34.480</a></span> | <span class="t">This expression you can see here. So now we know also how to run gradient ascent to optimize our language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3042" target="_blank">00:50:42.560</a></span> | <span class="t">The algorithm that I have described before is called the gradient policy optimization and it works fine for very small problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3049" target="_blank">00:50:49.600</a></span> | <span class="t">But it exhibits problems. It is not perfect for bigger problems. So for example language modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3055" target="_blank">00:50:55.060</a></span> | <span class="t">And the problem is very simple. The problem is that we are approximating. So let's write here something so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3062" target="_blank">00:51:02.400</a></span> | <span class="t">We as you saw before our objective function, which is j of theta, which is an expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3071" target="_blank">00:51:11.520</a></span> | <span class="t">Over all possible trajectories that are sampled according to our policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3076" target="_blank">00:51:16.580</a></span> | <span class="t">And expectation each one with its reward along the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3082" target="_blank">00:51:22.740</a></span> | <span class="t">So we are approximating the expectation with a sample mean so we do not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3088" target="_blank">00:51:28.320</a></span> | <span class="t">Calculate this expression over all possible trajectories. We calculate it only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3093" target="_blank">00:51:33.040</a></span> | <span class="t">on some trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3095" target="_blank">00:51:35.440</a></span> | <span class="t">now this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3097" target="_blank">00:51:37.120</a></span> | <span class="t">Fair, it means that the result that we will get will be an approximation that on average will converge to the true expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3104" target="_blank">00:51:44.500</a></span> | <span class="t">So it means that on the long term it will converge to the true expectation, but it exhibits high variance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3110" target="_blank">00:51:50.660</a></span> | <span class="t">So to give you an intuition into what this means, let's talk about something more simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3115" target="_blank">00:51:55.760</a></span> | <span class="t">For example, imagine I ask you to calculate the average</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3119" target="_blank">00:51:59.040</a></span> | <span class="t">age of the American population. Now the American population is made up of 330 million people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3126" target="_blank">00:52:06.240</a></span> | <span class="t">To calculate the average age means that you need to go to every person ask what is their birthday calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3132" target="_blank">00:52:12.080</a></span> | <span class="t">Age and then sum all these ages that you collect divide by the number of people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3137" target="_blank">00:52:17.600</a></span> | <span class="t">And this will give you the true average age of the American population</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3141" target="_blank">00:52:21.120</a></span> | <span class="t">But of course as you can see, this is not easy to compute because you would need to interview 330 million people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3146" target="_blank">00:52:26.880</a></span> | <span class="t">Another idea would be say okay. I don't go to every American person</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3152" target="_blank">00:52:32.640</a></span> | <span class="t">I only go to some Americans and I calculate their average age which could give me a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3158" target="_blank">00:52:38.720</a></span> | <span class="t">indication of what is the average age of the American population</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3162" target="_blank">00:52:42.020</a></span> | <span class="t">But the result of this approximation depends on how many people you interview because if you only interview one person</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3169" target="_blank">00:52:49.440</a></span> | <span class="t">It may not be representative of the whole population. Even if you interview 10 people, it may not be representative of the whole population</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3176" target="_blank">00:52:56.640</a></span> | <span class="t">So the more people you interview the better and this is actually a result that is statistically proven by the central limit theorem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3183" target="_blank">00:53:03.840</a></span> | <span class="t">So let's talk about the variance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3187" target="_blank">00:53:07.280</a></span> | <span class="t">Of this estimator. So we want to calculate the average age of the American population</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3192" target="_blank">00:53:12.340</a></span> | <span class="t">Suppose that the average age of the American population is 40 years or 45 years or whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3200" target="_blank">00:53:20.560</a></span> | <span class="t">If we approximate it using a sample mean which means that we do not ask every American but some Americans what is their average age</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3208" target="_blank">00:53:28.240</a></span> | <span class="t">We need to sample randomly some people and ask what their age. Suppose that we only interview one person because we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3214" target="_blank">00:53:34.640</a></span> | <span class="t">We do not have time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3217" target="_blank">00:53:37.040</a></span> | <span class="t">Suppose that we are unlucky and this person happens to be a kindergarten student and this person will probably say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3223" target="_blank">00:53:43.040</a></span> | <span class="t">The age is a six. So we will get a result that is very far from the true mean of the population</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3230" target="_blank">00:53:50.500</a></span> | <span class="t">On the other hand, we may ask again some random people and these people happen to be for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3235" target="_blank">00:53:55.700</a></span> | <span class="t">All people from retirement homes. So we will get some number that is very high which is for example 80 years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3241" target="_blank">00:54:01.540</a></span> | <span class="t">Which is also not representative of the true population</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3244" target="_blank">00:54:04.200</a></span> | <span class="t">So the smaller the sample the more unlucky we are in getting these values that are very far from the true mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3251" target="_blank">00:54:11.700</a></span> | <span class="t">So one way is to increase the sample size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3254" target="_blank">00:54:14.900</a></span> | <span class="t">So if we ask 1000 people what is their average age very probably we'll get something that is closer to this 40 years old</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3261" target="_blank">00:54:21.940</a></span> | <span class="t">because we cannot be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3264" target="_blank">00:54:24.020</a></span> | <span class="t">so unlucky to get six or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3266" target="_blank">00:54:26.020</a></span> | <span class="t">That all of them happen to be in the kindergarten or in the retirement age</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3270" target="_blank">00:54:30.180</a></span> | <span class="t">in the retirement home</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3272" target="_blank">00:54:32.500</a></span> | <span class="t">This happens also when we approximate an estimation with a sample mean here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3278" target="_blank">00:54:38.580</a></span> | <span class="t">The quality of this approximation depends on how many trajectories we choose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3284" target="_blank">00:54:44.820</a></span> | <span class="t">and as you saw before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3287" target="_blank">00:54:47.060</a></span> | <span class="t">Choosing too many trajectories from language models is not easy because it means that you need to run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3291" target="_blank">00:54:51.860</a></span> | <span class="t">Inference on the language model many times to calculate these trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3295" target="_blank">00:54:55.880</a></span> | <span class="t">Now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3299" target="_blank">00:54:59.300</a></span> | <span class="t">So the problem is we cannot easily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3302" target="_blank">00:55:02.100</a></span> | <span class="t">Increase the number of trajectories, but we need to find a way to reduce this value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3306" target="_blank">00:55:06.660</a></span> | <span class="t">so we do not because this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3308" target="_blank">00:55:08.660</a></span> | <span class="t">This tells us what is the direction of the gradient that we will use to run a gradient ascent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3314" target="_blank">00:55:14.420</a></span> | <span class="t">We want to find the true direction of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3317" target="_blank">00:55:17.380</a></span> | <span class="t">so imagine the true direction of the gradient is this one if we have high variance it means that sometimes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3322" target="_blank">00:55:22.740</a></span> | <span class="t">This approximation may tell us that the gradient is actually pointing in this direction or it's pointing in this direction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3328" target="_blank">00:55:28.180</a></span> | <span class="t">Or it's pointing in this direction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3330" target="_blank">00:55:30.020</a></span> | <span class="t">But if we increase the reduce the variance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3332" target="_blank">00:55:32.740</a></span> | <span class="t">It will probably tell us something that is more closer to the true direction of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3336" target="_blank">00:55:36.580</a></span> | <span class="t">So we will move our weights in a way that is moving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3340" target="_blank">00:55:40.020</a></span> | <span class="t">To maximize the objective function because we are moving according to the true direction of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3345" target="_blank">00:55:45.700</a></span> | <span class="t">So this is why we want to reduce the variance of this estimator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3349" target="_blank">00:55:49.080</a></span> | <span class="t">Now, let's see what are the techniques that we can use to reduce the variance of this estimator without increasing the sample size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3361" target="_blank">00:56:01.460</a></span> | <span class="t">The first thing that we should notice is that okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3364" target="_blank">00:56:04.580</a></span> | <span class="t">First of all, we had this expectation that we approximate using the sample mean you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3370" target="_blank">00:56:10.740</a></span> | <span class="t">Now each of these log probabilities. So this log probabilities here are multiplied by the reward over the entire trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3378" target="_blank">00:56:18.680</a></span> | <span class="t">Now the first thing that we should notice is that each action cannot alter the reward that it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3385" target="_blank">00:56:25.700</a></span> | <span class="t">That we received in previous steps. So imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3390" target="_blank">00:56:30.020</a></span> | <span class="t">We have a series of states and actions. So for example, we started from state zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3394" target="_blank">00:56:34.100</a></span> | <span class="t">And then we take action one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3396" target="_blank">00:56:36.740</a></span> | <span class="t">Which led us to action zero and then this led us to state one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3401" target="_blank">00:56:41.860</a></span> | <span class="t">In which we we took action one which led us to state two in which we took action two, etc, etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3410" target="_blank">00:56:50.180</a></span> | <span class="t">For each state action we receive a reward because when we take an action it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3415" target="_blank">00:56:55.780</a></span> | <span class="t">For example in the cat it will move to a new cell or remain in the same cell and it will receive some reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3420" target="_blank">00:57:00.420</a></span> | <span class="t">And also for this one we will have some reward. So reward one and for this one we will have reward two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3426" target="_blank">00:57:06.500</a></span> | <span class="t">Now when we take this action here, for example action number two, it cannot alter the reward that we already received in the past</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3434" target="_blank">00:57:14.340</a></span> | <span class="t">So when we multiply by this term reward of tau</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3438" target="_blank">00:57:18.100</a></span> | <span class="t">We do not consider all the rewards that came before the action that we are considering in this summation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3444" target="_blank">00:57:24.580</a></span> | <span class="t">So instead of calculating the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3446" target="_blank">00:57:26.580</a></span> | <span class="t">For the trajectory starting from zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3449" target="_blank">00:57:29.540</a></span> | <span class="t">We can calculate the reward starting from the time step of the action that we are considering for the log probabilities of the action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3457" target="_blank">00:57:37.060</a></span> | <span class="t">This term here is known as the rewards to go which means what is the total reward if I start from this state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3465" target="_blank">00:57:45.700</a></span> | <span class="t">And take this action and then act according to the policy for the rest of the trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3471" target="_blank">00:57:51.240</a></span> | <span class="t">Why do we want to do this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3474" target="_blank">00:57:54.100</a></span> | <span class="t">Because as you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3476" target="_blank">00:57:56.100</a></span> | <span class="t">This expression here is an approximation of the true expectation here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3483" target="_blank">00:58:03.780</a></span> | <span class="t">The less terms we have the better because we will have less noise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3489" target="_blank">00:58:09.140</a></span> | <span class="t">Why? Because first of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3492" target="_blank">00:58:12.260</a></span> | <span class="t">As we know each action cannot alter the rewards that we received in the past</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3500" target="_blank">00:58:20.260</a></span> | <span class="t">Which means that on average all these past terms will cancel out with each other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3505" target="_blank">00:58:25.460</a></span> | <span class="t">But so we if we do not consider them we avoid adding some noise in this approximation that will send our gradient in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3513" target="_blank">00:58:33.380</a></span> | <span class="t">Directions that are further from the true gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3516" target="_blank">00:58:36.520</a></span> | <span class="t">So if we can remove some terms from this expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3520" target="_blank">00:58:40.340</a></span> | <span class="t">It is better because we have less chance of introducing noise that sends our gradient in two directions that are far from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3526" target="_blank">00:58:46.980</a></span> | <span class="t">The one that is the true gradient that would be given by this expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3530" target="_blank">00:58:50.840</a></span> | <span class="t">So the first thing we do is we instead of calculating the reward over all the trajectory. We only calculate the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3538" target="_blank">00:58:58.660</a></span> | <span class="t">For each state action of the reward starting from that state action onwards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3544" target="_blank">00:59:04.980</a></span> | <span class="t">Until we reach the end of the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3547" target="_blank">00:59:07.780</a></span> | <span class="t">So this T big T here you can see here capital T</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3551" target="_blank">00:59:11.380</a></span> | <span class="t">Indicates from the time of the current state action that we are considering here until the end of the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3557" target="_blank">00:59:17.480</a></span> | <span class="t">Now this is one way to reduce the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3561" target="_blank">00:59:21.060</a></span> | <span class="t">variance of the estimator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3564" target="_blank">00:59:24.740</a></span> | <span class="t">Another way is to introduce a baseline. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3568" target="_blank">00:59:28.100</a></span> | <span class="t">You can introduce it has been proven in the research of reinforcement learning that introducing a constant here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3577" target="_blank">00:59:37.140</a></span> | <span class="t">Reduces the variance and it doesn't have to be a constant but it can also be something that depends on the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3583" target="_blank">00:59:43.860</a></span> | <span class="t">So it could be also a function of the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3586" target="_blank">00:59:46.260</a></span> | <span class="t">For which we are calculating the reward of the trajectory. So for each log probability we multiply by a term here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3594" target="_blank">00:59:54.020</a></span> | <span class="t">That indicates the rewards to go so the reward from this state action until the end of the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3600" target="_blank">01:00:00.200</a></span> | <span class="t">minus a baseline that does not have to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3603" target="_blank">01:00:03.220</a></span> | <span class="t">Constant, but it can also be a function of the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3607" target="_blank">01:00:07.300</a></span> | <span class="t">And the function that we will choose is called the value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3611" target="_blank">01:00:11.860</a></span> | <span class="t">So this baseline we will there are many baselines, but with the one we will choose is the value function the value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3618" target="_blank">01:00:18.180</a></span> | <span class="t">tells us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3620" target="_blank">01:00:20.740</a></span> | <span class="t">Of S according to some policy pi tells us what is the expected reward if you start from S</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3627" target="_blank">01:00:27.540</a></span> | <span class="t">And then act according to the policy for the rest of the trajectory. This is the value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3634" target="_blank">01:00:34.900</a></span> | <span class="t">Let me show you some examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3636" target="_blank">01:00:36.900</a></span> | <span class="t">So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3638" target="_blank">01:00:38.900</a></span> | <span class="t">The value function of this particular cell</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3641" target="_blank">01:00:41.140</a></span> | <span class="t">Of this cell here. We expect it to be high why because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3645" target="_blank">01:00:45.860</a></span> | <span class="t">It's very probable that the cat will take the action move down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3649" target="_blank">01:00:49.780</a></span> | <span class="t">And go directly to the meat in the case of language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3653" target="_blank">01:00:53.780</a></span> | <span class="t">this is a prompt because it's a series of tokens that we will feed to the language model to generate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3661" target="_blank">01:01:01.040</a></span> | <span class="t">Probabilities of the next token and it's very good to be in this state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3665" target="_blank">01:01:05.360</a></span> | <span class="t">Why because it's very probable that the next token will be generated in such a way that it will actually answer the question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3672" target="_blank">01:01:12.000</a></span> | <span class="t">Of where is shanghai?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3674" target="_blank">01:01:14.400</a></span> | <span class="t">So if the model has already generated these two tokens, for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3677" target="_blank">01:01:17.360</a></span> | <span class="t">Shanghai is it's very probable that the next token will be the word in and the next next token will be the word china</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3683" target="_blank">01:01:23.840</a></span> | <span class="t">Which answers our question which will result in a good response by the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3689" target="_blank">01:01:29.680</a></span> | <span class="t">Which in turn will give us a good reward according to our reward model on the other hand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3695" target="_blank">01:01:35.600</a></span> | <span class="t">If we are here, for example with the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3698" target="_blank">01:01:38.080</a></span> | <span class="t">This is a state that can lead us to move to the bathtub</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3702" target="_blank">01:01:42.580</a></span> | <span class="t">So we expect the value of this state to be lower than that of this state because it's less probable that from here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3709" target="_blank">01:01:49.680</a></span> | <span class="t">We end up on the bathtub. Maybe we get closer to the bathtub, but we do not end up directly on the bathtub</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3715" target="_blank">01:01:55.040</a></span> | <span class="t">But from here we can end up there so it will reduce the value of this state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3719" target="_blank">01:01:59.520</a></span> | <span class="t">So what is a bad value for a language model? For example in the case for this prompt here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3725" target="_blank">01:02:05.520</a></span> | <span class="t">So we started with a prompt and the language model somehow generated these two words chocolate muffins for the question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3731" target="_blank">01:02:11.600</a></span> | <span class="t">Where is shanghai?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3733" target="_blank">01:02:13.040</a></span> | <span class="t">Now if we ask the language model to generate the next tokens for given this prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3737" target="_blank">01:02:17.280</a></span> | <span class="t">It will probably move far from the actual response of where is shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3742" target="_blank">01:02:22.240</a></span> | <span class="t">It will not tell us that shanghai is in china</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3744" target="_blank">01:02:24.880</a></span> | <span class="t">So the value that we can get starting from this state is not so high because we will probably end up generating a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3751" target="_blank">01:02:31.680</a></span> | <span class="t">Bad response which will give us a low reward according to our reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3758" target="_blank">01:02:38.080</a></span> | <span class="t">So this is the meaning of a value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3760" target="_blank">01:02:40.880</a></span> | <span class="t">The value function tells us if I start from this state and then act according to the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3766" target="_blank">01:02:46.160</a></span> | <span class="t">What is the expected return I can get?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3768" target="_blank">01:02:48.960</a></span> | <span class="t">Now, how do we estimate this value function?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3774" target="_blank">01:02:54.240</a></span> | <span class="t">Well, just like we did for the reward model we can generate a neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3780" target="_blank">01:03:00.480</a></span> | <span class="t">To which we add a linear layer on top that can estimate this value function and usually what is done in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3789" target="_blank">01:03:09.280</a></span> | <span class="t">Practically is we use the same language model that we are trying to optimize we add another linear layer on top</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3795" target="_blank">01:03:15.440</a></span> | <span class="t">So apart from the one that projects into the vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3798" target="_blank">01:03:18.480</a></span> | <span class="t">We add another one that can also estimate the value so that the parameters of the transformer layer are shared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3805" target="_blank">01:03:25.700</a></span> | <span class="t">For the language modeling and the estimation of the value. The only two differences are the linear layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3811" target="_blank">01:03:31.200</a></span> | <span class="t">One is used for projecting the tokens into the vocabulary and one is used to estimate the value of the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3817" target="_blank">01:03:37.600</a></span> | <span class="t">Which is the prompt basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3819" target="_blank">01:03:39.760</a></span> | <span class="t">So suppose our language model has generated this response for our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3825" target="_blank">01:03:45.200</a></span> | <span class="t">Prompt, so where is Shanghai and the language model has said Shanghai is in China</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3829" target="_blank">01:03:49.120</a></span> | <span class="t">We send it to the policy model. So the language model that we're trying to optimize this is called the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3837" target="_blank">01:03:57.460</a></span> | <span class="t">It will generate some hidden states one corresponding to each token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3842" target="_blank">01:04:02.800</a></span> | <span class="t">and then instead of using the linear layer of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3846" target="_blank">01:04:06.400</a></span> | <span class="t">Vocabulary, so that will project each hidden state into the vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3850" target="_blank">01:04:10.640</a></span> | <span class="t">We use another linear layer that with only one output feature that will be used to estimate the value of each state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3857" target="_blank">01:04:17.280</a></span> | <span class="t">So we can estimate the value of this state of this state of this state and also of the entire sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3864" target="_blank">01:04:24.640</a></span> | <span class="t">By using the values generated by this linear layer for each hidden state that we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3869" target="_blank">01:04:29.680</a></span> | <span class="t">Okay, now we have seen that before to reduce the variance first of all, we transformed the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3878" target="_blank">01:04:38.560</a></span> | <span class="t">The reward of the entire trajectory in rewards to go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3882" target="_blank">01:04:42.240</a></span> | <span class="t">So something that starts not from t zero, but t equal to the action state that we are considering here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3889" target="_blank">01:04:49.040</a></span> | <span class="t">And we also saw that we can introduce a baseline that depends on the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3894" target="_blank">01:04:54.240</a></span> | <span class="t">And this will not change the approximation. So this approximator is still unbiased</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3900" target="_blank">01:05:00.980</a></span> | <span class="t">Which means that it will on average converge to the true gradient, but will have lower variance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3907" target="_blank">01:05:07.360</a></span> | <span class="t">Which means that in the case of for example, we are calculating the average age of the american population</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3912" target="_blank">01:05:12.100</a></span> | <span class="t">which means that we are reducing the chance of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3916" target="_blank">01:05:16.000</a></span> | <span class="t">Getting very low very low values for the age or very high values for the age</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3921" target="_blank">01:05:21.360</a></span> | <span class="t">But we will get something that is more closer to the true average age of the american population</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3925" target="_blank">01:05:25.780</a></span> | <span class="t">Now this function here this rewards to go is in reinforcement literature. It's also called the Q function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3932" target="_blank">01:05:32.880</a></span> | <span class="t">So the Q function tells us if I start from this state and take this action. What is the future?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3938" target="_blank">01:05:38.240</a></span> | <span class="t">Expected reward if I act according to the policy for the rest of the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3943" target="_blank">01:05:43.060</a></span> | <span class="t">So the Q function tells us the expected reward if I start from this state and take this action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3950" target="_blank">01:05:50.560</a></span> | <span class="t">So we get some immediate reward and then act according to the policy for the rest of the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3956" target="_blank">01:05:56.020</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3958" target="_blank">01:05:58.560</a></span> | <span class="t">So we can simplify the expression that we have seen before as Q of state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3963" target="_blank">01:06:03.600</a></span> | <span class="t">And action at time step t here. I forgot the t minus the value of the state at time step t</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3970" target="_blank">01:06:10.160</a></span> | <span class="t">The difference between the two is known as advantage function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3974" target="_blank">01:06:14.320</a></span> | <span class="t">Now, I know that I am introducing a lot of terms and terminology bear with me because it will make sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3980" target="_blank">01:06:20.960</a></span> | <span class="t">later now just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3983" target="_blank">01:06:23.680</a></span> | <span class="t">Don't you don't have to remember all the terms. I will repeat multiple times these concepts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3988" target="_blank">01:06:28.580</a></span> | <span class="t">So what we were trying to do we are trying to reduce the variance of this estimator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3992" target="_blank">01:06:32.680</a></span> | <span class="t">and we saw that we can instead of calculating the reward for all the trajectories only for the rewards for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=3999" target="_blank">01:06:39.300</a></span> | <span class="t">Starting from the time step in which we are considering the action values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4003" target="_blank">01:06:43.940</a></span> | <span class="t">Then we saw that we can introduce this baseline called the value function that will reduce further the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4010" target="_blank">01:06:50.180</a></span> | <span class="t">variance of this estimator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4014" target="_blank">01:06:54.100</a></span> | <span class="t">The difference between these two is called advantage function in the literature of reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4019" target="_blank">01:06:59.620</a></span> | <span class="t">And the advantage function if you look at the expression here tells us. Okay. First of all, let's analyze these two terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4027" target="_blank">01:07:07.060</a></span> | <span class="t">pen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4029" target="_blank">01:07:09.140</a></span> | <span class="t">Now the Q function tells us what is the expected return if I start from state s at time step t</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4035" target="_blank">01:07:15.940</a></span> | <span class="t">Take action a so here. I forgot the t's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4039" target="_blank">01:07:19.300</a></span> | <span class="t">t</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4041" target="_blank">01:07:21.060</a></span> | <span class="t">Action t and t and also here t and t. Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4046" target="_blank">01:07:26.340</a></span> | <span class="t">so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4048" target="_blank">01:07:28.500</a></span> | <span class="t">Q function tells us if I start from state t take action a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4052" target="_blank">01:07:32.740</a></span> | <span class="t">And then act according to the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4055" target="_blank">01:07:35.620</a></span> | <span class="t">What is the expected return the value function on the other hand tells us if I start from state s</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4061" target="_blank">01:07:41.700</a></span> | <span class="t">And I act according to the policy. What is the expected return?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4066" target="_blank">01:07:46.180</a></span> | <span class="t">now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4068" target="_blank">01:07:48.260</a></span> | <span class="t">in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4069" target="_blank">01:07:49.620</a></span> | <span class="t">For example, let's use the pen in this case here in this state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4073" target="_blank">01:07:53.540</a></span> | <span class="t">If I choose the action go down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4076" target="_blank">01:07:56.660</a></span> | <span class="t">It is better than going left because by going down I will move towards the mid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4082" target="_blank">01:08:02.660</a></span> | <span class="t">So it is better to use the action go down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4085" target="_blank">01:08:05.140</a></span> | <span class="t">The advantage term that is the difference between these two terms tells us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4090" target="_blank">01:08:10.100</a></span> | <span class="t">How better is this particular action compared to the average action that we can take in the state s</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4098" target="_blank">01:08:18.260</a></span> | <span class="t">Which means that the advantage function for the state for the action go down in this state here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4104" target="_blank">01:08:24.180</a></span> | <span class="t">So in this state here will be higher than the advantage function of another action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4109" target="_blank">01:08:29.940</a></span> | <span class="t">so the advantage function tells us how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4112" target="_blank">01:08:32.820</a></span> | <span class="t">Better than the average is this action that we are considering compared to the other actions that we have in this state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4119" target="_blank">01:08:39.300</a></span> | <span class="t">And if we want to give an interpretation to this whole expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4124" target="_blank">01:08:44.180</a></span> | <span class="t">It tells our model that for each log probability. So for each action in a particular state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4130" target="_blank">01:08:50.340</a></span> | <span class="t">We want to multiply it by its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4132" target="_blank">01:08:52.420</a></span> | <span class="t">advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4135" target="_blank">01:08:55.040</a></span> | <span class="t">Because this is the gradient it will indicate a direction in which we need to optimize our parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4141" target="_blank">01:09:01.320</a></span> | <span class="t">By using gradient ascent basically what we are doing is we are forcing our policy to push up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4149" target="_blank">01:09:09.300</a></span> | <span class="t">So to increase the likelihood or the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4152" target="_blank">01:09:12.840</a></span> | <span class="t">Of the actions that have high advantage, which means that they result in a better than average</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4159" target="_blank">01:09:19.880</a></span> | <span class="t">Returns and push down the log probabilities of those actions in each state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4166" target="_blank">01:09:26.260</a></span> | <span class="t">That result in lower than average returns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4170" target="_blank">01:09:30.100</a></span> | <span class="t">Which means that for example, let's talk about language modeling if someone asks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4175" target="_blank">01:09:35.220</a></span> | <span class="t">Where is Shanghai? So where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4178" target="_blank">01:09:38.660</a></span> | <span class="t">is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4179" target="_blank">01:09:39.680</a></span> | <span class="t">Shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4181" target="_blank">01:09:41.680</a></span> | <span class="t">What is better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4184" target="_blank">01:09:44.260</a></span> | <span class="t">And the question mark what's a good action to take? What's the good next token to select?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4191" target="_blank">01:09:51.460</a></span> | <span class="t">Well, we know that the starting with the chocolate is going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4196" target="_blank">01:09:56.260</a></span> | <span class="t">less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4197" target="_blank">01:09:57.940</a></span> | <span class="t">Going to produce a reward that is worse than average because very probably it will lead to a bad answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4204" target="_blank">01:10:04.900</a></span> | <span class="t">however starting the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4206" target="_blank">01:10:06.900</a></span> | <span class="t">the answer with the word Shanghai will probably result in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4211" target="_blank">01:10:11.460</a></span> | <span class="t">In the correct answer because the next token will be in Shanghai is in China</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4216" target="_blank">01:10:16.660</a></span> | <span class="t">So it will actually result in a good answer which will be rewarded well by our reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4222" target="_blank">01:10:22.420</a></span> | <span class="t">So our model will be more likely to select the word Shanghai when it will see this prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4228" target="_blank">01:10:28.740</a></span> | <span class="t">So this is how to interpret this advantage term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4232" target="_blank">01:10:32.400</a></span> | <span class="t">Basically, what we are trying to do is we are trying to push up the log probabilities of those actions for a given state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4238" target="_blank">01:10:38.640</a></span> | <span class="t">That result in better than average reward according to our reward model and push down the probabilities of those actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4246" target="_blank">01:10:46.160</a></span> | <span class="t">Given the state that result in low than average reward for according to our reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4253" target="_blank">01:10:53.280</a></span> | <span class="t">Let's see how to estimate this advantage term now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4256" target="_blank">01:10:56.720</a></span> | <span class="t">So first of all, let me write again the expression of the advantage term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4261" target="_blank">01:11:01.040</a></span> | <span class="t">So let's use the pen. So as we saw before the advantage term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4264" target="_blank">01:11:04.560</a></span> | <span class="t">at time step t so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4267" target="_blank">01:11:07.600</a></span> | <span class="t">Starting from state s and taking action t is equal to the Q function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4274" target="_blank">01:11:14.080</a></span> | <span class="t">at time step t</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4277" target="_blank">01:11:17.440</a></span> | <span class="t">Action a at time step t minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4280" target="_blank">01:11:20.480</a></span> | <span class="t">Minus the value at time step t</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4288" target="_blank">01:11:28.240</a></span> | <span class="t">What is the Q function the Q function tells us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4290" target="_blank">01:11:30.720</a></span> | <span class="t">If we start from state s and take action a and then act according to the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4297" target="_blank">01:11:37.920</a></span> | <span class="t">What is the expected return if we start from state a state s and take action a and then we act according to the policy?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4304" target="_blank">01:11:44.640</a></span> | <span class="t">For the rest of the trajectory while the value function tells us what is the expected return?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4309" target="_blank">01:11:49.840</a></span> | <span class="t">If we start from state s and then act according to the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4314" target="_blank">01:11:54.880</a></span> | <span class="t">Which means that imagine we have a trajectory a trajectory is what it's a list of state ended actions. So we have a state 0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4322" target="_blank">01:12:02.000</a></span> | <span class="t">action 0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4324" target="_blank">01:12:04.800</a></span> | <span class="t">And this will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4326" target="_blank">01:12:06.160</a></span> | <span class="t">Have some reward associated maybe reward 0 this will lead us to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4330" target="_blank">01:12:10.000</a></span> | <span class="t">State 1 in which we will take maybe action 1 this will have some reward associated with it, which is reward 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4337" target="_blank">01:12:17.120</a></span> | <span class="t">This will take us to another state for example state 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4341" target="_blank">01:12:21.360</a></span> | <span class="t">Action in which we will take action 2 and this will have some reward associated with it, which is reward 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4347" target="_blank">01:12:27.040</a></span> | <span class="t">and then state 3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4349" target="_blank">01:12:29.760</a></span> | <span class="t">In which we will take action 3 it will have some reward associated which is reward 3 etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4355" target="_blank">01:12:35.440</a></span> | <span class="t">Etc, etc, etc for the rest of the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4357" target="_blank">01:12:37.700</a></span> | <span class="t">Let's try to understand how can we estimate this advantage term?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4361" target="_blank">01:12:41.920</a></span> | <span class="t">We saw also before that for the estimating the value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4365" target="_blank">01:12:45.680</a></span> | <span class="t">We can build a neural network, which is a linear head on top of our policy network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4370" target="_blank">01:12:50.640</a></span> | <span class="t">Which is the language model that we are trying to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4372" target="_blank">01:12:52.980</a></span> | <span class="t">So instead of using the linear network that linear layer that projects</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4377" target="_blank">01:12:57.700</a></span> | <span class="t">the hidden state into the vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4380" target="_blank">01:13:00.080</a></span> | <span class="t">We can use another special linear layer with only one output feature that can estimate the value function of that particular state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4386" target="_blank">01:13:06.320</a></span> | <span class="t">later, we will see also how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4388" target="_blank">01:13:08.880</a></span> | <span class="t">Which loss function we need to use to train this value head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4394" target="_blank">01:13:14.400</a></span> | <span class="t">So now let's concentrate on estimating this advantage term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4397" target="_blank">01:13:17.200</a></span> | <span class="t">Now imagine we have a trajectory this advantage term can be estimated like follows. So as we know the advantage term tells us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4404" target="_blank">01:13:24.560</a></span> | <span class="t">The Q function, so this is the Q function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4408" target="_blank">01:13:28.720</a></span> | <span class="t">at given state S and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4412" target="_blank">01:13:32.400</a></span> | <span class="t">action A at time step T</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4415" target="_blank">01:13:35.440</a></span> | <span class="t">Can be calculated as follows. So if we start from state S, we will receive some reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4422" target="_blank">01:13:42.560</a></span> | <span class="t">and then we can calculate because for each trajectory we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4426" target="_blank">01:13:46.560</a></span> | <span class="t">for each trajectory we can calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4429" target="_blank">01:13:49.440</a></span> | <span class="t">The Q function so if we start from state S at time step T and take action T in this state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4437" target="_blank">01:13:57.440</a></span> | <span class="t">And then act according to the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4440" target="_blank">01:14:00.160</a></span> | <span class="t">We can either sum all of these terms that we have for the trajectory or we can just say okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4445" target="_blank">01:14:05.600</a></span> | <span class="t">If I start from state 0 and take action 0 I will have some immediate reward, which is this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4451" target="_blank">01:14:11.600</a></span> | <span class="t">Plus I approximate the rest of the rewards with the value function because I will end up in some state S1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4457" target="_blank">01:14:17.200</a></span> | <span class="t">And I just approximate all this rest of the summation as the V of S1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4461" target="_blank">01:14:21.840</a></span> | <span class="t">or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4464" target="_blank">01:14:24.320</a></span> | <span class="t">We can let me delete some stuff now because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4467" target="_blank">01:14:27.040</a></span> | <span class="t">Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4471" target="_blank">01:14:31.120</a></span> | <span class="t">Now or we can say okay the advantage term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4474" target="_blank">01:14:34.640</a></span> | <span class="t">Which means that if I start from state S at time step T and take action T can also be approximated as follows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4481" target="_blank">01:14:41.200</a></span> | <span class="t">So I have some immediate reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4483" target="_blank">01:14:43.200</a></span> | <span class="t">Plus the reward that I get in the next state plus the rest of the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4488" target="_blank">01:14:48.880</a></span> | <span class="t">I approximate it with the value function at time step T</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4492" target="_blank">01:14:52.160</a></span> | <span class="t">plus 2 so S2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4494" target="_blank">01:14:54.720</a></span> | <span class="t">And this is exactly what we are doing here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4497" target="_blank">01:14:57.360</a></span> | <span class="t">And we are also discounting it with the gamma parameter that we saw here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4501" target="_blank">01:15:01.680</a></span> | <span class="t">So we want to discount future rewards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4504" target="_blank">01:15:04.180</a></span> | <span class="t">And this minus V is just because of the formula of the advantage term has this minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4510" target="_blank">01:15:10.480</a></span> | <span class="t">value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4512" target="_blank">01:15:12.080</a></span> | <span class="t">We can also do it with three terms or four terms or five terms or whatever we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4517" target="_blank">01:15:17.040</a></span> | <span class="t">And then we can cut the rest just with the value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4519" target="_blank">01:15:19.840</a></span> | <span class="t">Now, why do we want to do this? Let me delete some stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4523" target="_blank">01:15:23.680</a></span> | <span class="t">Okay, if we stop too early</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4527" target="_blank">01:15:27.040</a></span> | <span class="t">So we calculate or for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4528" target="_blank">01:15:28.960</a></span> | <span class="t">Just the first approximation because we are approximating most of the trajectory with the value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4534" target="_blank">01:15:34.640</a></span> | <span class="t">It will exhibit high bias, which means that the value of the estimation of this advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4540" target="_blank">01:15:40.180</a></span> | <span class="t">Will not be very correct because we are approximating most of the trajectory with the value function, which is itself an approximation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4548" target="_blank">01:15:48.260</a></span> | <span class="t">Or to improve this approximation we can introduce more rewards from the actual trajectory that we got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4555" target="_blank">01:15:55.520</a></span> | <span class="t">And only approximate a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4559" target="_blank">01:15:59.040</a></span> | <span class="t">Of the trajectory with the value function or we can approximate all of the trajectory with the rewards that we get and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4565" target="_blank">01:16:05.760</a></span> | <span class="t">Use no approximation with the value head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4570" target="_blank">01:16:10.400</a></span> | <span class="t">But if we use more terms, it will result in a higher variance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4575" target="_blank">01:16:15.780</a></span> | <span class="t">If we use less terms, it will result in a high bias because we are approximating more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4582" target="_blank">01:16:22.480</a></span> | <span class="t">So in order to solve this bias variance problem, we can use a generalized advantage estimation, which basically takes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4589" target="_blank">01:16:29.920</a></span> | <span class="t">Weighted sum of all these terms. So of this one, this one, this one each multiplied by a decay parameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4597" target="_blank">01:16:37.280</a></span> | <span class="t">lambda</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4599" target="_blank">01:16:39.680</a></span> | <span class="t">We can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4601" target="_blank">01:16:41.200</a></span> | <span class="t">So basically this results in a recursive formula in which we can calculate the advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4605" target="_blank">01:16:45.860</a></span> | <span class="t">At each time step t given the future advantage at time step t plus one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4612" target="_blank">01:16:52.240</a></span> | <span class="t">Let's try to use this formula. For example, imagine we have a trajectory which is a series of states and actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4617" target="_blank">01:16:57.360</a></span> | <span class="t">So we have a state zero with action zero which will result in a reward zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4622" target="_blank">01:17:02.560</a></span> | <span class="t">Then we have this will result in another state s1 in which we take action one and it will have some reward one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4629" target="_blank">01:17:09.680</a></span> | <span class="t">This will result in a new state s2 in which we take action two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4633" target="_blank">01:17:13.360</a></span> | <span class="t">Which will lead us to state three in which we take action three, etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4637" target="_blank">01:17:17.600</a></span> | <span class="t">This one will have reward three and this one reward two and this one will have reward three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4643" target="_blank">01:17:23.600</a></span> | <span class="t">Let's try to calculate the advantage. For example, the advantage at time step three because it's the last term in our trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4651" target="_blank">01:17:31.780</a></span> | <span class="t">Is equal to delta at time step t</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4655" target="_blank">01:17:35.600</a></span> | <span class="t">plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4657" target="_blank">01:17:37.840</a></span> | <span class="t">Gamma multiplied by lambda at time step four, but we do not have any time step four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4662" target="_blank">01:17:42.400</a></span> | <span class="t">So we this term does not exist. So delta three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4666" target="_blank">01:17:46.400</a></span> | <span class="t">Is equal to the return that we have at time step t plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4670" target="_blank">01:17:50.640</a></span> | <span class="t">Gamma multiplied by the value function at time step four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4675" target="_blank">01:17:55.600</a></span> | <span class="t">But we do not have this term because there is no state four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4679" target="_blank">01:17:59.120</a></span> | <span class="t">Minus the value of the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4682" target="_blank">01:18:02.240</a></span> | <span class="t">S3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4685" target="_blank">01:18:05.200</a></span> | <span class="t">this tells us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4687" target="_blank">01:18:07.200</a></span> | <span class="t">The advantage estimation at time step three then we can use it to calculate the advantage estimation at time step two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4696" target="_blank">01:18:16.480</a></span> | <span class="t">which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4697" target="_blank">01:18:17.760</a></span> | <span class="t">A2 is equal to delta two plus lambda</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4703" target="_blank">01:18:23.200</a></span> | <span class="t">Gamma lambda</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4706" target="_blank">01:18:26.800</a></span> | <span class="t">Oops</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4708" target="_blank">01:18:28.560</a></span> | <span class="t">A3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4710" target="_blank">01:18:30.320</a></span> | <span class="t">But what is delta two? Delta two is equal to the reward that we have at time step two plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4716" target="_blank">01:18:36.800</a></span> | <span class="t">Gamma</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4719" target="_blank">01:18:39.120</a></span> | <span class="t">multiplied by the value of the state three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4723" target="_blank">01:18:43.360</a></span> | <span class="t">Minus the value of the state two, etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4726" target="_blank">01:18:46.400</a></span> | <span class="t">So we can recursively calculate the advantage estimation of each term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4730" target="_blank">01:18:50.320</a></span> | <span class="t">Why do we need to calculate the advantage estimation because the advantage is in the formula of our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4734" target="_blank">01:18:54.960</a></span> | <span class="t">gradient that we need to calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4738" target="_blank">01:18:58.400</a></span> | <span class="t">That we need to run a gradient ascent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4741" target="_blank">01:19:01.360</a></span> | <span class="t">I know that I have introduced a lot of concepts. I have introduced the value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4746" target="_blank">01:19:06.320</a></span> | <span class="t">I have introduced the Q function and the advantage function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4749" target="_blank">01:19:09.600</a></span> | <span class="t">I also know that it may not be very clear to you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4752" target="_blank">01:19:12.560</a></span> | <span class="t">Why we are calculating all this stuff because we have not seen the code and how it will be used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4757" target="_blank">01:19:17.600</a></span> | <span class="t">So please bear with me now. I know that there is a lot of stuff that you need to remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4761" target="_blank">01:19:21.760</a></span> | <span class="t">But when we will see the code, I will go back to all these slides for now. I just made this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4767" target="_blank">01:19:27.600</a></span> | <span class="t">I just made all these formulas because later when we go back it they will make more sense to you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4774" target="_blank">01:19:34.320</a></span> | <span class="t">And also if you want to in the future review this video, you don't have to kind of watch the code to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4780" target="_blank">01:19:40.000</a></span> | <span class="t">the formulas because once you understand the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4783" target="_blank">01:19:43.040</a></span> | <span class="t">This video once you can just review the parts that you're interested and they will be more clarified to you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4789" target="_blank">01:19:49.440</a></span> | <span class="t">Okay, now let's see what is the advantage term for language model so just like the example I made before I said, okay we have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4797" target="_blank">01:19:57.840</a></span> | <span class="t">expression for our gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4800" target="_blank">01:20:00.960</a></span> | <span class="t">In which we are multiplying each log probability by the advantage function also here. I forgot the t</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4806" target="_blank">01:20:06.880</a></span> | <span class="t">And here I forgot the t later. I will fix the slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4810" target="_blank">01:20:10.560</a></span> | <span class="t">Now as I saw before as we saw before if we have our question is where is shanghai and our language model selects the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4818" target="_blank">01:20:18.720</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4820" target="_blank">01:20:20.080</a></span> | <span class="t">word shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4821" target="_blank">01:20:21.600</a></span> | <span class="t">Very probably this will be a new state that will be fed to the language model for generating the next next next next tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4829" target="_blank">01:20:29.920</a></span> | <span class="t">This the first choice of shanghai will lead to a good answer because very probably the next tokens will be selected in such a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4837" target="_blank">01:20:37.120</a></span> | <span class="t">that it will result in the for example, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4839" target="_blank">01:20:39.200</a></span> | <span class="t">Phrase shanghai is in china, which is a good response because it matches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4844" target="_blank">01:20:44.800</a></span> | <span class="t">What is what are the chosen answer in our data set of the reward model. So our reward model will give a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4852" target="_blank">01:20:52.560</a></span> | <span class="t">reward to this kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4856" target="_blank">01:20:56.000</a></span> | <span class="t">Answer so we can say that this is a good state to be in because it will lead to future states that will be rewarded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4862" target="_blank">01:21:02.640</a></span> | <span class="t">Well by the reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4864" target="_blank">01:21:04.240</a></span> | <span class="t">However, if our language model happens to choose the word chocolate as the next token after this question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4870" target="_blank">01:21:10.720</a></span> | <span class="t">This new state will lead to new tokens being selected that are not very close to the answer that we are trying to find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4880" target="_blank">01:21:20.000</a></span> | <span class="t">This will result in a bad response. So it will result in a low reward from our reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4886" target="_blank">01:21:26.240</a></span> | <span class="t">So in the case of language models, we are trying to push up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4889" target="_blank">01:21:29.920</a></span> | <span class="t">The log probabilities of the word shanghai when it sees the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4895" target="_blank">01:21:35.840</a></span> | <span class="t">Where is shanghai and push down the log probability of the word chocolate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4902" target="_blank">01:21:42.560</a></span> | <span class="t">When the state is where is shanghai because the advantage for choosing shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4907" target="_blank">01:21:47.540</a></span> | <span class="t">Is higher than the advantage for choosing the word chocolate given this prompt. This is the how do we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4914" target="_blank">01:21:54.240</a></span> | <span class="t">Interpret the advantage estimation for language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4919" target="_blank">01:21:59.040</a></span> | <span class="t">Another problem that we have a policy gradient optimization is because of the sampling that we are doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4925" target="_blank">01:22:05.120</a></span> | <span class="t">So as you know in the policy gradient optimization, the algorithm is like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4929" target="_blank">01:22:09.280</a></span> | <span class="t">So we have a language model. We sample some trajectories from this language model. We calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4934" target="_blank">01:22:14.560</a></span> | <span class="t">Rewards associated with these trajectories. We calculate the advantages associated with these trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4941" target="_blank">01:22:21.220</a></span> | <span class="t">We calculate the log probabilities associated with these trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4945" target="_blank">01:22:25.140</a></span> | <span class="t">Then we can use all this information to calculate this big expression here, which is the direction of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4951" target="_blank">01:22:31.840</a></span> | <span class="t">so which is the gradient of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4955" target="_blank">01:22:35.820</a></span> | <span class="t">Expected reward with respect to the parameters of the model and then we can run gradient ascent to optimize the parameters of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4963" target="_blank">01:22:43.820</a></span> | <span class="t">according to the direction of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4965" target="_blank">01:22:45.900</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4967" target="_blank">01:22:47.820</a></span> | <span class="t">This is a process that is also used in gradient descent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4971" target="_blank">01:22:51.020</a></span> | <span class="t">So using gradient descent we have a loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4973" target="_blank">01:22:53.500</a></span> | <span class="t">We calculate the gradient of the loss function with respect to the parameter of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4977" target="_blank">01:22:57.420</a></span> | <span class="t">And then we optimize the parameters of the model according to the direction of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4982" target="_blank">01:23:02.300</a></span> | <span class="t">We do this process many many many times. Why? Because we do little steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4986" target="_blank">01:23:06.460</a></span> | <span class="t">With respect to the direction of the gradient according to a learning rate alpha</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4992" target="_blank">01:23:12.620</a></span> | <span class="t">Now the problem is that we are sampling trajectories from the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=4999" target="_blank">01:23:19.100</a></span> | <span class="t">For each step that you are making in this gradient ascent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5002" target="_blank">01:23:22.700</a></span> | <span class="t">So for each step of this optimization process, we need to sample many trajectories. We need to calculate many advantages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5009" target="_blank">01:23:29.500</a></span> | <span class="t">We need to calculate many rewards. We need to calculate many log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5013" target="_blank">01:23:33.040</a></span> | <span class="t">So this can be very very inefficient because we will when doing gradient ascent, we are taking only small steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5020" target="_blank">01:23:40.380</a></span> | <span class="t">so for each of those small steps, we need to do a lot of calculation which makes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5024" target="_blank">01:23:44.460</a></span> | <span class="t">Which is makes the computation nearly impossible because we cannot run all these forward steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5031" target="_blank">01:23:51.020</a></span> | <span class="t">on many different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5033" target="_blank">01:23:53.020</a></span> | <span class="t">Models to calculate the values the advantages and the rewards etc. We need to find a better way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5038" target="_blank">01:23:58.780</a></span> | <span class="t">so as you remember this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5040" target="_blank">01:24:00.780</a></span> | <span class="t">This formula for the gradient that we have found is an approximation of an expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5046" target="_blank">01:24:06.240</a></span> | <span class="t">And in probability we have this thing called important sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5051" target="_blank">01:24:11.580</a></span> | <span class="t">So when evaluating the expectation with respect to one distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5056" target="_blank">01:24:16.160</a></span> | <span class="t">we can calculate the expectation with respect to another distribution different from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5062" target="_blank">01:24:22.940</a></span> | <span class="t">The previous one as long as we modify the we multiply the function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5067" target="_blank">01:24:27.760</a></span> | <span class="t">Inside the expectation by an additional term here. So let's try to understand. What does it mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5073" target="_blank">01:24:33.980</a></span> | <span class="t">Imagine we are trying to calculate this expectation and I want to remind you that in the case of the language model optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5080" target="_blank">01:24:40.400</a></span> | <span class="t">Or the gradient policy optimization. We are calculating the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5084" target="_blank">01:24:44.480</a></span> | <span class="t">of e over all the possible trajectory according to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5089" target="_blank">01:24:49.340</a></span> | <span class="t">policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5091" target="_blank">01:24:51.960</a></span> | <span class="t">Parameterized by theta of what of the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5096" target="_blank">01:24:56.380</a></span> | <span class="t">of each trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5099" target="_blank">01:24:59.100</a></span> | <span class="t">Here so in this case the x is we can consider x to be the trajectory sampled from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5106" target="_blank">01:25:06.780</a></span> | <span class="t">policy theta</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5109" target="_blank">01:25:09.500</a></span> | <span class="t">Pi theta and this could be the reward of each theta. Now, as you know, the expectation can be written as a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5117" target="_blank">01:25:17.560</a></span> | <span class="t">integral of the probability of each item in the expectation multiplied by the function f of x which is the inside here in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5125" target="_blank">01:25:25.160</a></span> | <span class="t">parentheses of the expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5127" target="_blank">01:25:27.240</a></span> | <span class="t">We can multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5129" target="_blank">01:25:29.480</a></span> | <span class="t">by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5130" target="_blank">01:25:30.680</a></span> | <span class="t">By the this constant here, which is basically the number one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5133" target="_blank">01:25:33.800</a></span> | <span class="t">So we can always multiply by the number one in a multiplication without changing the result of this multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5139" target="_blank">01:25:39.020</a></span> | <span class="t">So we are multiplying up and down in this fraction by the same quantity, which is the number one so we can do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5146" target="_blank">01:25:46.200</a></span> | <span class="t">then we can rearrange the terms such that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5148" target="_blank">01:25:48.760</a></span> | <span class="t">We divide the p</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5151" target="_blank">01:25:51.800</a></span> | <span class="t">Basically the p of x by this q of x where q of x this term here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5156" target="_blank">01:25:56.680</a></span> | <span class="t">Is the distribution it's another distribution is the probability density function of another distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5161" target="_blank">01:26:01.820</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5163" target="_blank">01:26:03.960</a></span> | <span class="t">Then we can return back this integral to the expectation form</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5168" target="_blank">01:26:08.360</a></span> | <span class="t">So now we we can write the expectation as a sample from the distribution q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5174" target="_blank">01:26:14.520</a></span> | <span class="t">And calculate the the with respect to a function that is the f of x multiplied by this additional term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5181" target="_blank">01:26:21.640</a></span> | <span class="t">So this means that in order to calculate the initial expectation here instead of sampling from the distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5188" target="_blank">01:26:28.460</a></span> | <span class="t">For which we want to calculate the expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5190" target="_blank">01:26:30.680</a></span> | <span class="t">We can sample from another distribution as long as each item is multiplied by this additional factor here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5198" target="_blank">01:26:38.760</a></span> | <span class="t">And we can do the same for our expression of the gradient policy optimization in which we were sampling from some policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5205" target="_blank">01:26:45.480</a></span> | <span class="t">Here, which is the policy that we are trying to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5209" target="_blank">01:26:49.420</a></span> | <span class="t">But we can modify it by using important sampling to sample from another policy, which could be a different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5217" target="_blank">01:26:57.000</a></span> | <span class="t">Neural network, but we will see that actually it's the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5220" target="_blank">01:27:00.680</a></span> | <span class="t">But okay, suppose that it's a different neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5224" target="_blank">01:27:04.920</a></span> | <span class="t">Because sampling trajectory means that we generate some text given some questions. So it's actually we are sampling from our neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5232" target="_blank">01:27:12.200</a></span> | <span class="t">And each of the items so each of this advantage term instead of being multiplied only by the probability according to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5239" target="_blank">01:27:19.800</a></span> | <span class="t">To the network that we're trying to optimize. We also divide it by this q of x. So this the log probabilities of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5247" target="_blank">01:27:27.800</a></span> | <span class="t">Distribution from which we are sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5252" target="_blank">01:27:32.200</a></span> | <span class="t">We will call the distribution from which we are sampling pi offline and the distribution that we are trying to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5259" target="_blank">01:27:39.660</a></span> | <span class="t">pi online</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5261" target="_blank">01:27:41.720</a></span> | <span class="t">Let me give you an example a graphical example on how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5265" target="_blank">01:27:45.480</a></span> | <span class="t">So for now, just remember that with important sampling we can calculate this expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5271" target="_blank">01:27:51.500</a></span> | <span class="t">By sampling from another network while optimizing another one a different one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5277" target="_blank">01:27:57.240</a></span> | <span class="t">It works like this. This is called off policy learning in reinforcement learning literature. So imagine we have a language model and we will call it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5284" target="_blank">01:28:04.760</a></span> | <span class="t">Parameterized by some parameters called theta offline and we will call it the offline policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5291" target="_blank">01:28:11.880</a></span> | <span class="t">We will sample some trajectories. What does it mean? We give some questions according to our reward model data set, for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5298" target="_blank">01:28:18.520</a></span> | <span class="t">So we ask it where is shanghai and we ask the language model to generate many answers giving using a high temperature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5305" target="_blank">01:28:25.560</a></span> | <span class="t">For example, then we calculate the rewards for these trajectories that are generated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5310" target="_blank">01:28:30.440</a></span> | <span class="t">We calculate the advantages for all the state action pairs. We calculate the log probabilities for this state action pairs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5317" target="_blank">01:28:37.000</a></span> | <span class="t">and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5319" target="_blank">01:28:39.240</a></span> | <span class="t">We optimize another another model called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5322" target="_blank">01:28:42.520</a></span> | <span class="t">online policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5324" target="_blank">01:28:44.600</a></span> | <span class="t">So we take all these trajectories that we have sampled from the offline policy and we save it in some database or in some memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5332" target="_blank">01:28:52.360</a></span> | <span class="t">And we keep it there. Then we take some mini batch of trajectories from this database or from this memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5339" target="_blank">01:28:59.000</a></span> | <span class="t">And then we run we calculate this expression here because we can calculate it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5344" target="_blank">01:29:04.440</a></span> | <span class="t">So we can calculate the log probabilities according to the online model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5349" target="_blank">01:29:09.000</a></span> | <span class="t">So for this the trajectories that we have sampled from this memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5353" target="_blank">01:29:13.320</a></span> | <span class="t">We can also calculate again the advantage term according to the online policy, which is another neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5360" target="_blank">01:29:20.360</a></span> | <span class="t">We can also calculate and later i will show in the code how it's done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5363" target="_blank">01:29:23.400</a></span> | <span class="t">We can also calculate the advantage term according to the online policy. We can also calculate the rewards according to the online policy, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5370" target="_blank">01:29:30.440</a></span> | <span class="t">And then we run gradient ascent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5373" target="_blank">01:29:33.960</a></span> | <span class="t">Based on this expression only optimizing this online policy here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5379" target="_blank">01:29:39.240</a></span> | <span class="t">And we do it for a few epochs, which means for a few mini batches that we sample from this big memory of trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5387" target="_blank">01:29:47.800</a></span> | <span class="t">And after a while, we just set the online policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5391" target="_blank">01:29:51.400</a></span> | <span class="t">The parameters of the offline policy equal to the parameters of the online policy and restart the loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5396" target="_blank">01:29:56.760</a></span> | <span class="t">So we start again by sampling some trajectories, which we keep them in the memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5401" target="_blank">01:30:01.240</a></span> | <span class="t">then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5402" target="_blank">01:30:02.760</a></span> | <span class="t">For a few epochs, we sample some trajectories from here. We calculate the log probabilities with respect to the online policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5409" target="_blank">01:30:09.260</a></span> | <span class="t">We calculate this expression here, which is needed to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5414" target="_blank">01:30:14.600</a></span> | <span class="t">With the gradient ascent and then after a while we set the offline policy equal to the online policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5419" target="_blank">01:30:19.960</a></span> | <span class="t">now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5421" target="_blank">01:30:21.880</a></span> | <span class="t">They look like two different network neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5424" target="_blank">01:30:24.520</a></span> | <span class="t">But actually it's the same neural network in which we first sample from the neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5429" target="_blank">01:30:29.160</a></span> | <span class="t">We keep the memory of the trajectories that we sample and then we optimize this neural network by taking these trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5436" target="_blank">01:30:36.620</a></span> | <span class="t">After a while, we do this process again. I know that this is not easy to visualize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5443" target="_blank">01:30:43.400</a></span> | <span class="t">So later we will see this in the code, but the important thing is that now we have found a way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5448" target="_blank">01:30:48.920</a></span> | <span class="t">Run gradient ascent multiple times without having to sample each time from the policy that we are optimizing from the network that we are trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5457" target="_blank">01:30:57.480</a></span> | <span class="t">To optimize. We can sample once, keep these trajectories in memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5462" target="_blank">01:31:02.040</a></span> | <span class="t">Optimize the network for some steps and then after we have optimized for some steps, we can sample new trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5469" target="_blank">01:31:09.100</a></span> | <span class="t">We do not have to do it for every step of gradient ascent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5472" target="_blank">01:31:12.840</a></span> | <span class="t">So this makes the computation of this policy gradient algorithm tractable because otherwise it was too slow to run it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5481" target="_blank">01:31:21.080</a></span> | <span class="t">And this is how we do it in the code, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5485" target="_blank">01:31:25.720</a></span> | <span class="t">I also created some pseudocode in how to do this offline policy. So imagine we have a model that we want to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5493" target="_blank">01:31:33.800</a></span> | <span class="t">Okay, let's use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5499" target="_blank">01:31:39.000</a></span> | <span class="t">This one here, okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5501" target="_blank">01:31:41.000</a></span> | <span class="t">For now, just ignore the frozen model. We're not using it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5503" target="_blank">01:31:43.720</a></span> | <span class="t">So we have a neural network that we want to train with gradient ascent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5508" target="_blank">01:31:48.200</a></span> | <span class="t">So we have a policy that we want to optimize with gradient ascent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5511" target="_blank">01:31:51.580</a></span> | <span class="t">We sample some trajectories from this policy and we keep them in memory. For each trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5517" target="_blank">01:31:57.560</a></span> | <span class="t">We calculate the log probabilities, the rewards, the advantages, the KL divergence, etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5524" target="_blank">01:32:04.760</a></span> | <span class="t">Later, we will see why we need the KL divergence for now. Just ignore it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5529" target="_blank">01:32:09.640</a></span> | <span class="t">This part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5531" target="_blank">01:32:11.320</a></span> | <span class="t">Then we sample some mini-batch from these trajectories that we have seen. We run the PPO algorithm that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5537" target="_blank">01:32:17.800</a></span> | <span class="t">calculated the loss, basically the expression that we saw before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5541" target="_blank">01:32:21.720</a></span> | <span class="t">We calculate the gradient using loss.backward and we run optimizer step, but we do not need to sample again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5548" target="_blank">01:32:28.840</a></span> | <span class="t">We just take another sample from the trajectories that we have already saved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5552" target="_blank">01:32:32.840</a></span> | <span class="t">We do again another step of gradient ascent and then etc, etc until we reach a specified number of steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5559" target="_blank">01:32:39.240</a></span> | <span class="t">And then after we have optimized the model for some number of steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5563" target="_blank">01:32:43.240</a></span> | <span class="t">We can sample new trajectories and then run again this loop of optimization for many steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5568" target="_blank">01:32:48.760</a></span> | <span class="t">So not for every step of gradient ascent, we have to sample new trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5573" target="_blank">01:32:53.100</a></span> | <span class="t">We sample once, we do many steps of gradient ascent and then we sample again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5577" target="_blank">01:32:57.320</a></span> | <span class="t">We do many steps of gradient ascent and then we sample again. This makes the training much faster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5582" target="_blank">01:33:02.520</a></span> | <span class="t">Okay, I promise this is the last group of formulas that we are going to see. So this is finally the PPO loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5587" target="_blank">01:33:07.800</a></span> | <span class="t">Let's try to understand it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5591" target="_blank">01:33:11.400</a></span> | <span class="t">So based on what we have seen before, the first thing that we should see is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5595" target="_blank">01:33:15.480</a></span> | <span class="t">This term here is exactly the one that we saw before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5598" target="_blank">01:33:18.920</a></span> | <span class="t">So we have the log probabilities according to the policy that we are trying to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5603" target="_blank">01:33:23.420</a></span> | <span class="t">Divided by the log probability of the policy that we sample from, so the offline policy. Yeah, I don't know why it's so ugly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5612" target="_blank">01:33:32.280</a></span> | <span class="t">So we have the log probability of the, this is called the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5615" target="_blank">01:33:35.480</a></span> | <span class="t">Online policy, so the policy that we are trying to optimize. So let's call it online</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5620" target="_blank">01:33:40.600</a></span> | <span class="t">This is the log probabilities according to the policy that we sample from, so we sample some trajectories from this policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5628" target="_blank">01:33:48.360</a></span> | <span class="t">This is the offline policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5630" target="_blank">01:33:50.680</a></span> | <span class="t">And then we have this advantage term which is multiplied by each of the action state pairs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5638" target="_blank">01:33:58.440</a></span> | <span class="t">We are calculating the minimum value of this expression and this other expression here. So this clipped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5644" target="_blank">01:34:04.520</a></span> | <span class="t">log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5647" target="_blank">01:34:07.400</a></span> | <span class="t">Why? Well, first of all, what is the clip function? The clip function says that if this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5652" target="_blank">01:34:12.200</a></span> | <span class="t">expression we can see here is bigger than 1 plus epsilon, then it will be clipped to 1 plus epsilon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5660" target="_blank">01:34:20.680</a></span> | <span class="t">If this expression is smaller than 1 minus epsilon, then it will be clipped to 1 minus epsilon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5667" target="_blank">01:34:27.900</a></span> | <span class="t">Why do we want this? Well, it means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5670" target="_blank">01:34:30.700</a></span> | <span class="t">First of all, let's try to interpret this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5673" target="_blank">01:34:33.740</a></span> | <span class="t">This term here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5677" target="_blank">01:34:37.660</a></span> | <span class="t">The difference, the ratio of the two log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5681" target="_blank">01:34:41.360</a></span> | <span class="t">so we have some log, we have some policy that we sample from and then we have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5686" target="_blank">01:34:46.860</a></span> | <span class="t">policy that we are optimizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5689" target="_blank">01:34:49.740</a></span> | <span class="t">This means that if the log probability in the policy that we are optimizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5695" target="_blank">01:34:55.020</a></span> | <span class="t">Is much higher for a specific action compared to the one that we sampled from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5700" target="_blank">01:35:00.300</a></span> | <span class="t">Which means that we are trying to increase the likelihood of selecting that action in the future</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5705" target="_blank">01:35:05.580</a></span> | <span class="t">We don't want this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5708" target="_blank">01:35:08.700</a></span> | <span class="t">This increase to be too far. So we want to clip it to maximum at this value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5715" target="_blank">01:35:15.340</a></span> | <span class="t">On the other hand, if we are trying to decrease the likelihood of an action compared to what it was before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5723" target="_blank">01:35:23.740</a></span> | <span class="t">We don't want it to decrease by too much, but at most by this quantity here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5728" target="_blank">01:35:28.780</a></span> | <span class="t">This means that in our optimization step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5731" target="_blank">01:35:31.580</a></span> | <span class="t">We are moving the action probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5734" target="_blank">01:35:34.700</a></span> | <span class="t">So the probabilities of selecting a particular token given a particular prompt we are changing them continuously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5741" target="_blank">01:35:41.040</a></span> | <span class="t">But we don't want them to change too much. We want to make little steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5746" target="_blank">01:35:46.220</a></span> | <span class="t">Why? Because we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5748" target="_blank">01:35:48.860</a></span> | <span class="t">If we move them too much, maybe the model will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5752" target="_blank">01:35:52.940</a></span> | <span class="t">Run into maybe the model will kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5757" target="_blank">01:35:57.100</a></span> | <span class="t">Not explore enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5760" target="_blank">01:36:00.220</a></span> | <span class="t">Of the other options so the model may actually optimize for that particular action too much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5765" target="_blank">01:36:05.900</a></span> | <span class="t">So it may always avoid that action or it will always use that action in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5771" target="_blank">01:36:11.020</a></span> | <span class="t">We want to do it little by little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5773" target="_blank">01:36:13.100</a></span> | <span class="t">So we want to the model to make little steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5776" target="_blank">01:36:16.060</a></span> | <span class="t">In increasing a particular action or a little step in decreasing the log probability of that particular action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5782" target="_blank">01:36:22.620</a></span> | <span class="t">Why are we talking about actions? Because we are talking about language models and so we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5786" target="_blank">01:36:26.620</a></span> | <span class="t">Increase or decrease the probability of selecting a particular token given a prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5792" target="_blank">01:36:32.380</a></span> | <span class="t">But we don't want this probability to change too much. This is why we have the minimum here. So we want to make the most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5800" target="_blank">01:36:40.380</a></span> | <span class="t">Pessimistic update we can we don't want to be too optimistic. We don't want the model to make the most optimistic steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5807" target="_blank">01:36:47.900</a></span> | <span class="t">So if the model is very sure that it can always select this token, we don't want the model to be sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5813" target="_blank">01:36:53.340</a></span> | <span class="t">We want the model to make a little step towards what the model thinks is better choice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5817" target="_blank">01:36:57.820</a></span> | <span class="t">The other head that we introduced before was the head for calculating the value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5824" target="_blank">01:37:04.940</a></span> | <span class="t">So as you remember, we also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5827" target="_blank">01:37:07.020</a></span> | <span class="t">Introduced this value function and we say that this value function which is a function of the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5832" target="_blank">01:37:12.380</a></span> | <span class="t">indicates what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5835" target="_blank">01:37:15.160</a></span> | <span class="t">Expected reward that we can receive from start by starting from that particular state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5840" target="_blank">01:37:20.120</a></span> | <span class="t">And the example that I gave you was for example, imagine we are our question is where is Shanghai?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5846" target="_blank">01:37:26.520</a></span> | <span class="t">So where is Shanghai?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5849" target="_blank">01:37:29.080</a></span> | <span class="t">If the model has selected for example the word Shanghai as the next token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5856" target="_blank">01:37:36.600</a></span> | <span class="t">We expect the value of this state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5860" target="_blank">01:37:40.360</a></span> | <span class="t">So because this will become a new input for the language model to be high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5863" target="_blank">01:37:43.720</a></span> | <span class="t">Why? because it will probably result in a good answer that will be rewarded well by our model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5869" target="_blank">01:37:49.720</a></span> | <span class="t">But of course, we also need to train our neural network to approximate this value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5875" target="_blank">01:37:55.240</a></span> | <span class="t">Well, so what we do is we use this other term for the PPO loss is for training the value function estimator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5882" target="_blank">01:38:02.940</a></span> | <span class="t">and basically it means the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5886" target="_blank">01:38:06.040</a></span> | <span class="t">The value function estimator based on a particular state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5890" target="_blank">01:38:10.280</a></span> | <span class="t">So this is the output of the model and we compare it with what is the value actual value of this state based on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5896" target="_blank">01:38:16.760</a></span> | <span class="t">trajectories that we have sampled because we have trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5900" target="_blank">01:38:20.220</a></span> | <span class="t">Each trajectory is made up of state actions. Each state action has some reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5905" target="_blank">01:38:25.320</a></span> | <span class="t">So we actually can calculate the value of this state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5910" target="_blank">01:38:30.200</a></span> | <span class="t">According to the trajectory that we have sampled. So we want to optimize the value function estimator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5915" target="_blank">01:38:35.580</a></span> | <span class="t">According to the trajectories that we have actually sampled from our policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5919" target="_blank">01:38:39.160</a></span> | <span class="t">The last term in the policy the PPO loss. So we have first of all the policy optimization term, which is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5925" target="_blank">01:38:45.880</a></span> | <span class="t">This is the one I described here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5928" target="_blank">01:38:48.200</a></span> | <span class="t">Then we have the the loss because for the value function estimator and then we have another term here the entropy loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5936" target="_blank">01:38:56.360</a></span> | <span class="t">This is to introduce some kind of this is to force our model to explore more options</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5942" target="_blank">01:39:02.360</a></span> | <span class="t">so imagine our model if we don't have this term here the model may just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5947" target="_blank">01:39:07.000</a></span> | <span class="t">optimize the actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5950" target="_blank">01:39:10.260</a></span> | <span class="t">in such a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5952" target="_blank">01:39:12.760</a></span> | <span class="t">In such a way to select the actions that resulted in a very good advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5956" target="_blank">01:39:16.780</a></span> | <span class="t">To select them more often and the actions that resulted in lower than average advantage to select them less often</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5965" target="_blank">01:39:25.480</a></span> | <span class="t">So this will kind of make the model very rigid in selecting tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5970" target="_blank">01:39:30.200</a></span> | <span class="t">The model will always choose the tokens that resulted in good advantage and never select the tokens that resulted in bad advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5977" target="_blank">01:39:37.660</a></span> | <span class="t">But this will make also the model not explore other options</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5982" target="_blank">01:39:42.200</a></span> | <span class="t">Which means that for example, imagine we sample some trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5985" target="_blank">01:39:45.180</a></span> | <span class="t">And for the question, where is shanghai the model always selects the word shanghai because it results in a good answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5991" target="_blank">01:39:51.480</a></span> | <span class="t">But we want the model to be also kind of explore other options. Maybe there is another word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5996" target="_blank">01:39:56.440</a></span> | <span class="t">For example, where is shanghai?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=5999" target="_blank">01:39:59.000</a></span> | <span class="t">Maybe the next word is can be the word it because it will result in it is in china</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6004" target="_blank">01:40:04.520</a></span> | <span class="t">So we also want the model to give the model the possibility to explore or more of these options</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6012" target="_blank">01:40:12.760</a></span> | <span class="t">And this is why we introduce this entropy term because we want the model for each state actions to also explore other options</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6019" target="_blank">01:40:19.640</a></span> | <span class="t">So we want to force the model to explore other options. So because we are maximizing this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6025" target="_blank">01:40:25.480</a></span> | <span class="t">This objective here. So we are maximizing the this objective function here. We also want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6031" target="_blank">01:40:31.640</a></span> | <span class="t">Minimize this loss here. We will see later how to do it and we want to maximize the entropy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6037" target="_blank">01:40:37.260</a></span> | <span class="t">So that the model can also explore more options why we use the entropy because the entropy tells us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6045" target="_blank">01:40:45.320</a></span> | <span class="t">How much kind of disorder there is uncertainty there is in the prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6050" target="_blank">01:40:50.280</a></span> | <span class="t">So we want the model to be more uncertain why because it will help the model to explore more next tokens for a given prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6057" target="_blank">01:40:57.640</a></span> | <span class="t">The last thing that we need to consider is that if we kind of optimize the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6064" target="_blank">01:41:04.040</a></span> | <span class="t">Using the ppo loss that we have described before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6066" target="_blank">01:41:06.760</a></span> | <span class="t">the model may learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6069" target="_blank">01:41:09.480</a></span> | <span class="t">Some tokens or some sequence of tokens that always result in a good reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6074" target="_blank">01:41:14.360</a></span> | <span class="t">And the model may always choose these tokens to always get good rewards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6079" target="_blank">01:41:19.080</a></span> | <span class="t">But these tokens may not make sense for us humans. So for example, imagine our model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6084" target="_blank">01:41:24.840</a></span> | <span class="t">our data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6087" target="_blank">01:41:27.480</a></span> | <span class="t">Forces our data set for the reward model forces the model to be polite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6091" target="_blank">01:41:31.640</a></span> | <span class="t">The model may just use the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6094" target="_blank">01:41:34.520</a></span> | <span class="t">Thank you. Thank you. Thank you continuously because we know that it is very polite and it results in a good reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6100" target="_blank">01:41:40.360</a></span> | <span class="t">but this is not a good answer for a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6103" target="_blank">01:41:43.320</a></span> | <span class="t">Question because if I ask you where is shanghai and you are just keep if the model just keeps telling me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6108" target="_blank">01:41:48.600</a></span> | <span class="t">Thank you. Thank you. Thank you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6109" target="_blank">01:41:49.400</a></span> | <span class="t">Then for sure the reward model will give a good reward to this answer because it's a polite answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6114" target="_blank">01:41:54.040</a></span> | <span class="t">But it does not make sense to humans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6116" target="_blank">01:41:56.200</a></span> | <span class="t">So we want the model to actually generate output that makes sense that are very similar to the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6121" target="_blank">01:42:01.640</a></span> | <span class="t">It has seen during the training. That's why we want to constrain the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6126" target="_blank">01:42:06.360</a></span> | <span class="t">Not only to get good rewards, but at the same time to generate answers that are very similar to the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6132" target="_blank">01:42:12.760</a></span> | <span class="t">It would generate by just looking at the untrained model. So at the unaligned model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6139" target="_blank">01:42:19.400</a></span> | <span class="t">This is why we make another copy of the model that we want to optimize and we freeze its weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6145" target="_blank">01:42:25.080</a></span> | <span class="t">So this is the frozen model. We generate the rewards for each step in the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6151" target="_blank">01:42:31.960</a></span> | <span class="t">But we penalize by how much the log probabilities at each step change from the frozen model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6158" target="_blank">01:42:38.360</a></span> | <span class="t">So for each hidden state we can generate the reward by using the linear layer that we saw before with only one output feature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6165" target="_blank">01:42:45.240</a></span> | <span class="t">But at the same time for each hidden state, we will also calculate the log probabilities using the other linear layer for generating the logits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6172" target="_blank">01:42:52.440</a></span> | <span class="t">So we'll send it also this one to the linear layer to generate the logits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6176" target="_blank">01:42:56.520</a></span> | <span class="t">This will calculate the logits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6181" target="_blank">01:43:01.160</a></span> | <span class="t">And then the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6183" target="_blank">01:43:03.160</a></span> | <span class="t">So the prob</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6186" target="_blank">01:43:06.040</a></span> | <span class="t">We do the same for the frozen model and then we penalize the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6191" target="_blank">01:43:11.560</a></span> | <span class="t">So this reward here for this time step, we say the reward is equal to the reward at the time step zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6198" target="_blank">01:43:18.120</a></span> | <span class="t">minus the KL divergence between the log probabilities of the frozen model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6203" target="_blank">01:43:23.960</a></span> | <span class="t">so the log probabilities of the frozen model and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6208" target="_blank">01:43:28.760</a></span> | <span class="t">The log probabilities of the policy that we are optimizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6212" target="_blank">01:43:32.300</a></span> | <span class="t">We want to penalize the model for generating answers that are too different from the frozen model. So we want the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6219" target="_blank">01:43:39.720</a></span> | <span class="t">Reward to be maximized but at the same time we don't want the model to cheat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6224" target="_blank">01:43:44.120</a></span> | <span class="t">In just getting reward by generating any kind of output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6228" target="_blank">01:43:48.040</a></span> | <span class="t">But we want the model to actually get rewards for good answer that are very similar to the one that it would generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6233" target="_blank">01:43:53.640</a></span> | <span class="t">If it was not optimized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6236" target="_blank">01:43:56.440</a></span> | <span class="t">Okay, I know that you are tired of looking at all this explanation and all this theory. So let's jump into the code now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6242" target="_blank">01:44:02.360</a></span> | <span class="t">Okay. So the the goal that we are the code that we are going to see is a code that I took from the HuggingFace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6250" target="_blank">01:44:10.040</a></span> | <span class="t">Website which basically allow us to train a reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6254" target="_blank">01:44:14.680</a></span> | <span class="t">Setup in which we want to train a language model to generate positive reviews</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6259" target="_blank">01:44:19.480</a></span> | <span class="t">So if we have a language model that is generating text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6262" target="_blank">01:44:22.520</a></span> | <span class="t">But we want to force the language model to generate positive reviews of a particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6267" target="_blank">01:44:27.320</a></span> | <span class="t">For example a restaurant or a movie or something like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6271" target="_blank">01:44:31.640</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6273" target="_blank">01:44:33.800</a></span> | <span class="t">We want the language model to be still similar to to generate something that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6278" target="_blank">01:44:38.440</a></span> | <span class="t">Comprehensible to humans, but at the same time we want to like we force the language model to be positive to generate positive stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6284" target="_blank">01:44:44.840</a></span> | <span class="t">So say stuff like for example, I really like this movie or I really like this restaurant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6290" target="_blank">01:44:50.200</a></span> | <span class="t">We will be using the imdb data set. So as you can see from the website of HuggingFace the imdb data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6296" target="_blank">01:44:56.120</a></span> | <span class="t">It's a data set made up of text of reviews and for each review</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6300" target="_blank">01:45:00.120</a></span> | <span class="t">It indicates what is if the review is positive or negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6304" target="_blank">01:45:04.140</a></span> | <span class="t">And we will use this imdb data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6308" target="_blank">01:45:08.440</a></span> | <span class="t">To understand what is the score that we want to give to a review</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6313" target="_blank">01:45:13.160</a></span> | <span class="t">So if the review will be positive according to this data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6317" target="_blank">01:45:17.080</a></span> | <span class="t">It will be given a high reward and if the text generated will be similar to a negative review</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6322" target="_blank">01:45:22.360</a></span> | <span class="t">Then it will be given a low reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6324" target="_blank">01:45:24.440</a></span> | <span class="t">so the first thing that we do is we create the model that we want to optimize which is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6329" target="_blank">01:45:29.400</a></span> | <span class="t">which is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6332" target="_blank">01:45:32.520</a></span> | <span class="t">language model here, so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6334" target="_blank">01:45:34.520</a></span> | <span class="t">I think it's gpt2 already fine-tuned on the imdb data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6338" target="_blank">01:45:38.840</a></span> | <span class="t">And then we create a reference model why because we need a frozen model of with frozen weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6345" target="_blank">01:45:45.640</a></span> | <span class="t">that we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6347" target="_blank">01:45:47.400</a></span> | <span class="t">keep the weights frozen to compare how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6349" target="_blank">01:45:49.720</a></span> | <span class="t">different is the response of the model that we are trying to optimize from the frozen model because we don't want the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6355" target="_blank">01:45:55.640</a></span> | <span class="t">Output to be much different. We just want it to be a little positive, but we don't want the model to just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6361" target="_blank">01:46:01.240</a></span> | <span class="t">Output garbage just to get high reward. We want to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6364" target="_blank">01:46:04.840</a></span> | <span class="t">Get actual text that makes sense. This is why we keep also a frozen model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6372" target="_blank">01:46:12.520</a></span> | <span class="t">And then we load this PPO trainer. The PPO trainer in HuggingFace is the class that is used to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6379" target="_blank">01:46:19.560</a></span> | <span class="t">To run reinforcement learning from human feedback using the PPO algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6384" target="_blank">01:46:24.860</a></span> | <span class="t">So, let's see. First of all, what is the reward model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6388" target="_blank">01:46:28.200</a></span> | <span class="t">The reward model is basically just a sentiment analysis or using this model here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6392" target="_blank">01:46:32.920</a></span> | <span class="t">It will give us for each text that we feed to this reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6398" target="_blank">01:46:38.840</a></span> | <span class="t">A number that indicates how positive it is according to this imdb data set you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6404" target="_blank">01:46:44.280</a></span> | <span class="t">so it will tell us if the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6406" target="_blank">01:46:46.760</a></span> | <span class="t">The text that we are receiving is a positive review or a negative review</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6410" target="_blank">01:46:50.600</a></span> | <span class="t">For example, if we give this text here, it will probably tell us that it's a bad review</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6415" target="_blank">01:46:55.000</a></span> | <span class="t">So low reward and if this we give it give this text here for this movie was really good. It will give us a positive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6424" target="_blank">01:47:04.200</a></span> | <span class="t">Reward and we will use this number here as the reward. So the score corresponding to the positive class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6430" target="_blank">01:47:10.120</a></span> | <span class="t">Okay, the first step in PPO is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6434" target="_blank">01:47:14.520</a></span> | <span class="t">Is to generate the trajectories. So we have some model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6439" target="_blank">01:47:19.480</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6441" target="_blank">01:47:21.400</a></span> | <span class="t">Policy that is the offline policy and we need to sample some trajectories from it. What do I mean by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6448" target="_blank">01:47:28.680</a></span> | <span class="t">Sampling some trajectories means that we give it some text and it will generate some responses some output text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6455" target="_blank">01:47:35.800</a></span> | <span class="t">And what we will be using as a kind of questions or prompt for generating the text we will be using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6462" target="_blank">01:47:42.680</a></span> | <span class="t">Just some initial sampled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6465" target="_blank">01:47:45.240</a></span> | <span class="t">Text from this imdb data set you can see here. So for example, uh, this data set is composed of many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6472" target="_blank">01:47:52.600</a></span> | <span class="t">Reviews some are positive. Some are negative. We just randomly take the initial part of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6478" target="_blank">01:47:58.600</a></span> | <span class="t">Review and we use it as a prompt to generate the rest of the review</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6482" target="_blank">01:48:02.760</a></span> | <span class="t">And then we ask the reward model to judge this review that was generated if it's positive or negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6487" target="_blank">01:48:07.800</a></span> | <span class="t">It's positive then it will achieve a high reward if it's negative, it will achieve low reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6492" target="_blank">01:48:12.920</a></span> | <span class="t">So we take some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6496" target="_blank">01:48:16.680</a></span> | <span class="t">Okay, we generate some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6499" target="_blank">01:48:19.080</a></span> | <span class="t">Lengths, so random select how many tokens we need to take from each review. We select it randomly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6505" target="_blank">01:48:25.480</a></span> | <span class="t">We get these prompts from our data set and we ask the ppo model to generate some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6511" target="_blank">01:48:31.560</a></span> | <span class="t">Answers for these questions for these prompts. So generate the rest of the text up to a maximum length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6519" target="_blank">01:48:39.080</a></span> | <span class="t">That is also sampled randomly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6522" target="_blank">01:48:42.440</a></span> | <span class="t">These are our trajectories for now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6525" target="_blank">01:48:45.720</a></span> | <span class="t">These are just the combination of prompt and the generated text. We did not calculate the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6532" target="_blank">01:48:52.680</a></span> | <span class="t">We did not calculate the advantages. We did not calculate the rewards etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6537" target="_blank">01:48:57.320</a></span> | <span class="t">Okay. So now for now, we only have the query and the response generated by our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6542" target="_blank">01:49:02.920</a></span> | <span class="t">offline policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6545" target="_blank">01:49:05.560</a></span> | <span class="t">What is the offline policy is the model that we are trying to train. So this variable here model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6550" target="_blank">01:49:10.360</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6552" target="_blank">01:49:12.440</a></span> | <span class="t">Now that we have some responses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6553" target="_blank">01:49:13.960</a></span> | <span class="t">We can ask our reward model to judge these responses and we use basically just do a sentiment classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6561" target="_blank">01:49:21.160</a></span> | <span class="t">in which we give the response that was given by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6564" target="_blank">01:49:24.520</a></span> | <span class="t">Policy and we ask the sentiment pipe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6568" target="_blank">01:49:28.440</a></span> | <span class="t">So the sentiment analysis pipes which will act as our reward model to judge this text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6573" target="_blank">01:49:33.640</a></span> | <span class="t">So how positive is this review that was generated and we will take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6578" target="_blank">01:49:38.600</a></span> | <span class="t">Score associated with the positive class that will be generated as you can see here. So as a reward we take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6584" target="_blank">01:49:44.840</a></span> | <span class="t">We assign the reward to the full response. So for each response, we will get one number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6591" target="_blank">01:49:51.480</a></span> | <span class="t">And this number is actually the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6593" target="_blank">01:49:53.480</a></span> | <span class="t">Logits, so the score corresponding to the positive class according to this sentiment analysis pipeline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6600" target="_blank">01:50:00.540</a></span> | <span class="t">Now that we have some trajectories, which are some questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6604" target="_blank">01:50:04.360</a></span> | <span class="t">So some prompts along with the text that was generated along with the reward for each of this text that was generated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6611" target="_blank">01:50:11.900</a></span> | <span class="t">We can run the PPO training setup. So let's now go inside the code of the library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6618" target="_blank">01:50:18.600</a></span> | <span class="t">So the first thing we do is we call this function here step in which we give out the prompt that we gave to the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6624" target="_blank">01:50:24.600</a></span> | <span class="t">Model the responses that were generated and the rewards associated with each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6628" target="_blank">01:50:28.520</a></span> | <span class="t">response</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6630" target="_blank">01:50:30.360</a></span> | <span class="t">And then we run this step function here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6633" target="_blank">01:50:33.000</a></span> | <span class="t">Now the step function here. Okay. First it checks if the tensors that you pass it are correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6640" target="_blank">01:50:40.040</a></span> | <span class="t">So the data types and the shapes of the tensors, etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6644" target="_blank">01:50:44.360</a></span> | <span class="t">Then it converts the scores into a tensor because the scores are at least one score for each response</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6650" target="_blank">01:50:50.360</a></span> | <span class="t">So it converts it into a tensor. I commented the code that I don't find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6654" target="_blank">01:50:54.920</a></span> | <span class="t">Useful for my explanation. So there are many functions in a hugging phase, but we will not be using all of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6660" target="_blank">01:51:00.920</a></span> | <span class="t">I will just concentrate on explaining the vanilla PPO like it was described in my slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6665" target="_blank">01:51:05.800</a></span> | <span class="t">Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6668" target="_blank">01:51:08.200</a></span> | <span class="t">The first thing that we need to do is to calculate all the log probabilities of the actions that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6673" target="_blank">01:51:13.560</a></span> | <span class="t">that we need to calculate the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6675" target="_blank">01:51:15.960</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6677" target="_blank">01:51:17.560</a></span> | <span class="t">we do it here in this function here, so given the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6680" target="_blank">01:51:20.280</a></span> | <span class="t">answers the text generated by our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6684" target="_blank">01:51:24.040</a></span> | <span class="t">model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6685" target="_blank">01:51:25.720</a></span> | <span class="t">And the queries that were used so here they are called queries and responses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6689" target="_blank">01:51:29.480</a></span> | <span class="t">But they are actually the prompts and they generated the text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6692" target="_blank">01:51:32.520</a></span> | <span class="t">The hugging phase they calculate the log probabilities for each step. How do they calculate it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6697" target="_blank">01:51:37.720</a></span> | <span class="t">Well, they calculate the call this function batched forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6701" target="_blank">01:51:41.080</a></span> | <span class="t">in which they pass the model from which the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6704" target="_blank">01:51:44.200</a></span> | <span class="t">Answers were generated. So the text was generated the prompt that were used to generate this text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6711" target="_blank">01:51:51.080</a></span> | <span class="t">And they divide each of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6714" target="_blank">01:51:54.680</a></span> | <span class="t">Questions and responses into mini batches and then they run it through the model the model as we saw in the slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6722" target="_blank">01:52:02.440</a></span> | <span class="t">So let's go back here, I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6725" target="_blank">01:52:05.480</a></span> | <span class="t">So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6727" target="_blank">01:52:07.480</a></span> | <span class="t">Here we know that we can calculate the log probabilities corresponding to each position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6733" target="_blank">01:52:13.320</a></span> | <span class="t">based on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6735" target="_blank">01:52:15.720</a></span> | <span class="t">Text and the question that was asked so we can create a concatenation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6739" target="_blank">01:52:19.820</a></span> | <span class="t">Of the question and the text that was generated. We pass it to the model. The model will generate some logits one for each position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6747" target="_blank">01:52:27.180</a></span> | <span class="t">Of the token. We only take the log probability of the next token because we already know which next token was generated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6754" target="_blank">01:52:34.200</a></span> | <span class="t">So we know that for this particular prompt made up of these four tokens. The next token is shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6759" target="_blank">01:52:39.400</a></span> | <span class="t">So we only take the log probability corresponding to the word shanghai and this is what is done in this line here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6765" target="_blank">01:52:45.240</a></span> | <span class="t">so we ask the language model to generate the logits corresponding to all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6769" target="_blank">01:52:49.400</a></span> | <span class="t">Positions then we calculate the log probabilities from this logits. How?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6775" target="_blank">01:52:55.880</a></span> | <span class="t">Here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6778" target="_blank">01:52:58.520</a></span> | <span class="t">We calculated the log softmax so exactly like in my slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6781" target="_blank">01:53:01.960</a></span> | <span class="t">So we calculate the log softmax here as you can see. So for each logits we calculate the log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6787" target="_blank">01:53:07.320</a></span> | <span class="t">log softmax which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6790" target="_blank">01:53:10.520</a></span> | <span class="t">Log probabilities for each position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6793" target="_blank">01:53:13.720</a></span> | <span class="t">But we are only interested in the position corresponding to the next token and this is done here with the gather function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6799" target="_blank">01:53:19.880</a></span> | <span class="t">You can see here. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6801" target="_blank">01:53:21.480</a></span> | <span class="t">From all the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6802" target="_blank">01:53:22.920</a></span> | <span class="t">It only selects the one corresponding to the next token because we already know which token was generated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6808" target="_blank">01:53:28.120</a></span> | <span class="t">So now we have the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6810" target="_blank">01:53:30.120</a></span> | <span class="t">and we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6812" target="_blank">01:53:32.680</a></span> | <span class="t">We can save them because we don't have the log we don't want the log probabilities for all the tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6818" target="_blank">01:53:38.280</a></span> | <span class="t">We also need to keep track of where the log probabilities start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6821" target="_blank">01:53:41.960</a></span> | <span class="t">So the one that we want to consider and where they end why because as you can see from my slide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6827" target="_blank">01:53:47.480</a></span> | <span class="t">Our trajectory here. The question was where is shanghai the model generated four tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6833" target="_blank">01:53:53.320</a></span> | <span class="t">Where is shanghai is in china? So we are all interested in this trajectory. We only have four steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6838" target="_blank">01:53:58.760</a></span> | <span class="t">So we are all interested in the log probabilities of four tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6842" target="_blank">01:54:02.520</a></span> | <span class="t">And this is exactly what we do here. So we consider</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6845" target="_blank">01:54:05.720</a></span> | <span class="t">which is the starting point from which we consider the log probabilities and which is the ending token for which we consider the log probabilities because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6854" target="_blank">01:54:14.280</a></span> | <span class="t">this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6856" target="_blank">01:54:16.600</a></span> | <span class="t">The model will generate the log probabilities for all the positions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6859" target="_blank">01:54:19.560</a></span> | <span class="t">But we only want some of them and here is what we do. So we create a mask in which we say that the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6865" target="_blank">01:54:25.160</a></span> | <span class="t">Only consider we will be considering only these four probabilities or four five probabilities according to which token were actually generated by the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6875" target="_blank">01:54:35.880</a></span> | <span class="t">So now we have the log probabilities of each action. So let's go back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6881" target="_blank">01:54:41.720</a></span> | <span class="t">To the step function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6889" target="_blank">01:54:49.320</a></span> | <span class="t">Okay, so we calculated the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6891" target="_blank">01:54:51.420</a></span> | <span class="t">according to our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6894" target="_blank">01:54:54.180</a></span> | <span class="t">offline policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6895" target="_blank">01:54:55.880</a></span> | <span class="t">Why do we do it here inside the step method and not outside?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6899" target="_blank">01:54:59.480</a></span> | <span class="t">Well, because the hugging face is a library that is user friendly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6902" target="_blank">01:55:02.840</a></span> | <span class="t">So they don't want to give to the user the burden of calculating the log probabilities of each action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6909" target="_blank">01:55:09.240</a></span> | <span class="t">They do it inside the library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6911" target="_blank">01:55:11.160</a></span> | <span class="t">So they only ask the user to generate the responses for each prompt and then they take care of calculating the rest of the information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6919" target="_blank">01:55:19.420</a></span> | <span class="t">Now we also need to calculate the log probability with respect to the reference model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6923" target="_blank">01:55:23.900</a></span> | <span class="t">So the frozen model why because we also need to calculate the KL divergence that will be used to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6929" target="_blank">01:55:29.660</a></span> | <span class="t">penalize the reward for each position because we want to penalize the model for generating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6935" target="_blank">01:55:35.680</a></span> | <span class="t">Log probabilities that are much different from the frozen model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6940" target="_blank">01:55:40.540</a></span> | <span class="t">Otherwise the model will just do what is known as a reward hacking which is just generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6946" target="_blank">01:55:46.220</a></span> | <span class="t">Random tokens that actually give a good reward, but they do not make sense for the user</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6951" target="_blank">01:55:51.660</a></span> | <span class="t">So we also need to generate using the same method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6954" target="_blank">01:55:54.700</a></span> | <span class="t">So this batched forward pass using the frozen model to generate the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6960" target="_blank">01:56:00.160</a></span> | <span class="t">Which will be used to calculate the KL divergence to penalize the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6964" target="_blank">01:56:04.300</a></span> | <span class="t">The next step we do is we actually compute these rewards. So how do we compute the rewards?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6970" target="_blank">01:56:10.140</a></span> | <span class="t">Well using the log probabilities of the model that we are trying to optimize and the frozen model because we need to calculate the KL divergence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6977" target="_blank">01:56:17.280</a></span> | <span class="t">We have this mask which indicate which log probabilities we need to take into consideration because we have the log probabilities of all the response</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6985" target="_blank">01:56:25.100</a></span> | <span class="t">But only some of them are interesting for us because they belong to the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6989" target="_blank">01:56:29.440</a></span> | <span class="t">And let's see how to compute the rewards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6994" target="_blank">01:56:34.620</a></span> | <span class="t">So the rewards are computed as follows. So we calculate the KL penalty, which is the difference in log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=6999" target="_blank">01:56:39.980</a></span> | <span class="t">So if you go to here, you can see that the KL divergence is just a difference in log probabilities as you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7006" target="_blank">01:56:46.940</a></span> | <span class="t">and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7009" target="_blank">01:56:49.400</a></span> | <span class="t">penalize as you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7011" target="_blank">01:56:51.260</a></span> | <span class="t">The reward is basically just the KL divergence penalization, which is the KL divergence multiplied by some factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7017" target="_blank">01:56:57.820</a></span> | <span class="t">Which is the penalty factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7019" target="_blank">01:56:59.900</a></span> | <span class="t">and then we sum the score so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7024" target="_blank">01:57:04.380</a></span> | <span class="t">We saw before that the score is what is just the score associated to each response</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7030" target="_blank">01:57:10.140</a></span> | <span class="t">By our reward model. Our reward model is just a sentiment classification pipeline that will generate one reward one single number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7037" target="_blank">01:57:17.980</a></span> | <span class="t">for each response so indicating how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7040" target="_blank">01:57:20.940</a></span> | <span class="t">Positive is the response that was generated or how negative it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7045" target="_blank">01:57:25.420</a></span> | <span class="t">Because we only have one generated response</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7050" target="_blank">01:57:30.620</a></span> | <span class="t">We and this response this reward is associated with the last token. So let me show you in the slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7057" target="_blank">01:57:37.340</a></span> | <span class="t">Here we were computing the reward for each step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7061" target="_blank">01:57:41.660</a></span> | <span class="t">But actually the sentiment classification model will compute the reward only for the last token for the full answer for the full generated text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7070" target="_blank">01:57:50.380</a></span> | <span class="t">So we basically we create but we need of course to calculate the reward of the trajectory. We need the reward for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7077" target="_blank">01:57:57.900</a></span> | <span class="t">state actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7080" target="_blank">01:58:00.140</a></span> | <span class="t">so we compute the KL penalty for each position because we know the log probabilities of the frozen model and of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7087" target="_blank">01:58:07.660</a></span> | <span class="t">Model that we are trying to optimize. So we have the KL penalty for each position, but we have the reward only for the last one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7093" target="_blank">01:58:13.660</a></span> | <span class="t">So this is exactly what we are doing here. We calculate the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7097" target="_blank">01:58:17.280</a></span> | <span class="t">For the KL penalty for each position, but the score is only added to the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7103" target="_blank">01:58:23.340</a></span> | <span class="t">token, so here in this position here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7107" target="_blank">01:58:27.580</a></span> | <span class="t">And then when we compute the advantage because we compute the advantage starting from the last to the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7112" target="_blank">01:58:32.780</a></span> | <span class="t">we will kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7115" target="_blank">01:58:35.340</a></span> | <span class="t">Take this reward and put it in the previous steps and we will see this later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7119" target="_blank">01:58:39.180</a></span> | <span class="t">so now we have found a way to calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7121" target="_blank">01:58:41.820</a></span> | <span class="t">The rewards associated with each position in which each position is given some score by the sentiment classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7128" target="_blank">01:58:48.720</a></span> | <span class="t">But this is only given to the last token while the KL penalty is given to each position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7136" target="_blank">01:58:56.540</a></span> | <span class="t">So, let's go back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7138" target="_blank">01:58:58.540</a></span> | <span class="t">Okay, so we have computed the rewards now we can compute the advantages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7144" target="_blank">01:59:04.720</a></span> | <span class="t">Let's see how we compute the advantages to compute the advantages. We need the values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7148" target="_blank">01:59:08.860</a></span> | <span class="t">What are the values? Well, the value is the estimation of the value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7153" target="_blank">01:59:13.020</a></span> | <span class="t">value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7155" target="_blank">01:59:15.740</a></span> | <span class="t">As we saw before the value is computed by using the same model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7160" target="_blank">01:59:20.140</a></span> | <span class="t">So the policy network with an additional head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7163" target="_blank">01:59:23.500</a></span> | <span class="t">Which is a linear layer that gives us the value estimation for that particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7168" target="_blank">01:59:28.160</a></span> | <span class="t">State so let me show you in the slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7171" target="_blank">01:59:31.260</a></span> | <span class="t">Here we saw before that of the policy network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7176" target="_blank">01:59:36.700</a></span> | <span class="t">So the model that we are trying to optimize also has an additional linear layer that gives us a value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7181" target="_blank">01:59:41.900</a></span> | <span class="t">estimation for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7184" target="_blank">01:59:44.220</a></span> | <span class="t">step of the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7186" target="_blank">01:59:46.540</a></span> | <span class="t">And this is actually already when we calculated the log probabilities the this function also returns the value head the value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7193" target="_blank">01:59:53.340</a></span> | <span class="t">estimation for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7195" target="_blank">01:59:55.900</a></span> | <span class="t">Step of the trajectory then we can use the values estimated plus the rewards that we calculated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7201" target="_blank">02:00:01.520</a></span> | <span class="t">Plus the mask because we need to know which value we have and which value we don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7206" target="_blank">02:00:06.220</a></span> | <span class="t">to compute the advantage using the same formula that we saw before so we start from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7212" target="_blank">02:00:12.620</a></span> | <span class="t">The formula of the which is a this one here. So let's go back to the formula</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7217" target="_blank">02:00:17.360</a></span> | <span class="t">Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7223" target="_blank">02:00:23.660</a></span> | <span class="t">here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7224" target="_blank">02:00:24.700</a></span> | <span class="t">We calculate the delta t</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7226" target="_blank">02:00:26.700</a></span> | <span class="t">to compute the advantage estimation at time step t so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7230" target="_blank">02:00:30.940</a></span> | <span class="t">Here we are computing the first delta t which is the reward at time step t plus gamma as you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7239" target="_blank">02:00:39.180</a></span> | <span class="t">Multiplied by the value at time step t plus one and this is here. So it's zero if we do not have any future</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7245" target="_blank">02:00:45.420</a></span> | <span class="t">Values, otherwise, it's the value at time step t plus one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7249" target="_blank">02:00:49.020</a></span> | <span class="t">Minus the value at time step t exactly according to this formula here. You can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7254" target="_blank">02:00:54.860</a></span> | <span class="t">and then we use this delta value to compute the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7258" target="_blank">02:00:58.060</a></span> | <span class="t">Ge estimation, which is the delta plus gamma multiplied by lambda multiplied by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7265" target="_blank">02:01:05.260</a></span> | <span class="t">Ge at the next time step which is exactly what we do here. So delta at time step t plus gamma multiplied by lambda multiplied by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7273" target="_blank">02:01:13.420</a></span> | <span class="t">Advantage estimation at time step t plus one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7276" target="_blank">02:01:16.460</a></span> | <span class="t">And we do it from the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7279" target="_blank">02:01:19.340</a></span> | <span class="t">From the last item in the trajectory to the first item in the trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7284" target="_blank">02:01:24.240</a></span> | <span class="t">That's why we do this for loop in reverse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7287" target="_blank">02:01:27.820</a></span> | <span class="t">And then we reverse it back because we computed the advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7292" target="_blank">02:01:32.060</a></span> | <span class="t">Reversed and then we reverse the computed advantages to have them from zero to time step t to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7298" target="_blank">02:01:38.460</a></span> | <span class="t">Capital t instead of capital t to zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7300" target="_blank">02:01:40.940</a></span> | <span class="t">Then we compute the q values that will be used to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7305" target="_blank">02:01:45.900</a></span> | <span class="t">Optimize the value function. So as you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7309" target="_blank">02:01:49.740</a></span> | <span class="t">To optimize the value head. So the value estimation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7313" target="_blank">02:01:53.280</a></span> | <span class="t">we need to have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7316" target="_blank">02:01:56.060</a></span> | <span class="t">estimation of the value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7319" target="_blank">02:01:59.260</a></span> | <span class="t">But according to the trajectory that we have sampled, but what is the estimation of the value function according to the trajectory?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7325" target="_blank">02:02:05.840</a></span> | <span class="t">It is actually the q function because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7329" target="_blank">02:02:09.180</a></span> | <span class="t">For the value function tells us. Okay. Let me use some kind of let me write here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7335" target="_blank">02:02:15.580</a></span> | <span class="t">Otherwise, it's not easy to understand. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7337" target="_blank">02:02:17.980</a></span> | <span class="t">The value function here is tells us what is the value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7343" target="_blank">02:02:23.260</a></span> | <span class="t">Of a particular state. So what is the expected return that we can get?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7349" target="_blank">02:02:29.500</a></span> | <span class="t">By starting from a particular state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7351" target="_blank">02:02:31.500</a></span> | <span class="t">and we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7354" target="_blank">02:02:34.300</a></span> | <span class="t">We can approximate it also actually with the q function from the sample trajectories. Why?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7360" target="_blank">02:02:40.060</a></span> | <span class="t">Because the value function is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7364" target="_blank">02:02:44.140</a></span> | <span class="t">At time step t is the expected return</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7368" target="_blank">02:02:48.480</a></span> | <span class="t">Over all possible actions that we can take starting from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7376" target="_blank">02:02:56.540</a></span> | <span class="t">State s</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7377" target="_blank">02:02:57.980</a></span> | <span class="t">And taking action a so the value function here can be actually calculated from the q function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7383" target="_blank">02:03:03.900</a></span> | <span class="t">But it's an estimated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7386" target="_blank">02:03:06.540</a></span> | <span class="t">An expectation over all the possible actions that we can take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7389" target="_blank">02:03:09.500</a></span> | <span class="t">which means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7391" target="_blank">02:03:11.900</a></span> | <span class="t">The q function tells us what is the expected return if we start from state s and take action a the value function tells us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7398" target="_blank">02:03:18.460</a></span> | <span class="t">What is the expected return that we can get if we only start from?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7401" target="_blank">02:03:21.260</a></span> | <span class="t">State s and react according to the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7405" target="_blank">02:03:25.980</a></span> | <span class="t">Which is also the which basically can also be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7408" target="_blank">02:03:28.380</a></span> | <span class="t">Calculated as the expected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7411" target="_blank">02:03:31.400</a></span> | <span class="t">return over the q function, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7414" target="_blank">02:03:34.860</a></span> | <span class="t">Expected expectation over all the possible actions that we can take which kind of can be thought of as what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7422" target="_blank">02:03:42.140</a></span> | <span class="t">average return that we can get by starting from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7426" target="_blank">02:03:46.220</a></span> | <span class="t">State s and taking some actions over all the possible actions that we can take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7431" target="_blank">02:03:51.580</a></span> | <span class="t">But we do not have all the possible actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7435" target="_blank">02:03:55.260</a></span> | <span class="t">So we can approximate this expectation with a sample mean according to the one that we have in our trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7442" target="_blank">02:04:02.000</a></span> | <span class="t">So we have some actions state actions in our trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7445" target="_blank">02:04:05.280</a></span> | <span class="t">So we can actually approximate it this using the q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7449" target="_blank">02:04:09.180</a></span> | <span class="t">S a that we have in our trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7451" target="_blank">02:04:11.920</a></span> | <span class="t">And how do we compute this q?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7454" target="_blank">02:04:14.780</a></span> | <span class="t">S a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7456" target="_blank">02:04:16.940</a></span> | <span class="t">As you remember the formula for the advantage is advantage of s a at particular time step is equal to the q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7464" target="_blank">02:04:24.700</a></span> | <span class="t">Of s a minus v of s so we can get q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7469" target="_blank">02:04:29.740</a></span> | <span class="t">S a is equal to advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7473" target="_blank">02:04:33.520</a></span> | <span class="t">S a plus the value s</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7476" target="_blank">02:04:36.700</a></span> | <span class="t">And this is exactly what we are doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7479" target="_blank">02:04:39.660</a></span> | <span class="t">Here, so we are saying to get the q function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7483" target="_blank">02:04:43.420</a></span> | <span class="t">We are calculating the advantages plus values and this term here will be used to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7487" target="_blank">02:04:47.660</a></span> | <span class="t">Calculate the loss for the value head. We will see later. So remember these returns we are doing here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7494" target="_blank">02:04:54.460</a></span> | <span class="t">Okay. So now we have computed the advantages and the values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7499" target="_blank">02:04:59.100</a></span> | <span class="t">Now we still are in the first phase. So we have sampled some trajectories from our model that we're trying to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7507" target="_blank">02:05:07.200</a></span> | <span class="t">We computed the rewards using the rewards. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7511" target="_blank">02:05:11.100</a></span> | <span class="t">We also computed the log probabilities for each time step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7514" target="_blank">02:05:14.620</a></span> | <span class="t">We also computed the advantages for each time step and we also computed the q values for each time step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7520" target="_blank">02:05:20.380</a></span> | <span class="t">Which are used for the value head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7523" target="_blank">02:05:23.260</a></span> | <span class="t">Now let's go to the second phase of the ppo algorithm, which is the phase two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7527" target="_blank">02:05:27.340</a></span> | <span class="t">Which means that we take some mini-batch from these trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7531" target="_blank">02:05:31.040</a></span> | <span class="t">We optimize the model based on the estimated gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7535" target="_blank">02:05:35.680</a></span> | <span class="t">We do it with many steps and then again, we sample new trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7539" target="_blank">02:05:39.440</a></span> | <span class="t">We sample some mini-batches. We optimize the model according to the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7545" target="_blank">02:05:45.100</a></span> | <span class="t">We do it many times and then again, we sample new trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7548" target="_blank">02:05:48.000</a></span> | <span class="t">So let's go back to our step function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7551" target="_blank">02:05:51.340</a></span> | <span class="t">So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7553" target="_blank">02:05:53.340</a></span> | <span class="t">We are here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7556" target="_blank">02:05:56.860</a></span> | <span class="t">Okay, so we computed the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7559" target="_blank">02:05:59.020</a></span> | <span class="t">the advantages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7561" target="_blank">02:06:01.420</a></span> | <span class="t">Now we can use the sampled trajectories to optimize the model. So what do we do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7567" target="_blank">02:06:07.020</a></span> | <span class="t">We sample some mini-batches. This is the mini-batch that we are sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7570" target="_blank">02:06:10.540</a></span> | <span class="t">So we sample a mini-batch as you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7574" target="_blank">02:06:14.060</a></span> | <span class="t">And then what we need to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7576" target="_blank">02:06:16.700</a></span> | <span class="t">First of all, we go as we saw in the formula of the ppl also we need to have the log probabilities according to the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7582" target="_blank">02:06:22.460</a></span> | <span class="t">that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7583" target="_blank">02:06:23.900</a></span> | <span class="t">We sampled from which is this pi old and also according to the model that we are trying to optimize using this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7590" target="_blank">02:06:30.540</a></span> | <span class="t">Sampled mini-batches, which is exactly what we did here with offline policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7594" target="_blank">02:06:34.380</a></span> | <span class="t">So we sample from some policy and we need to have the trajectories from this policy and also the log probabilities from this policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7601" target="_blank">02:06:41.420</a></span> | <span class="t">Which is the offline policy and then we use this sample trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7604" target="_blank">02:06:44.780</a></span> | <span class="t">So we take a mini-batch and then we run a gradient ascent on an online policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7609" target="_blank">02:06:49.740</a></span> | <span class="t">But we also need to have the log probabilities according to this online policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7613" target="_blank">02:06:53.260</a></span> | <span class="t">The one we are that we are trying to optimize and this is exactly what we do here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7617" target="_blank">02:06:57.580</a></span> | <span class="t">So we run again this method that we ran before so the batch the forward pass to calculate the log probabilities the logits and the value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7625" target="_blank">02:07:05.420</a></span> | <span class="t">Head prediction according to the mini-batch that you are considering and then we train the model according to this mini-batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7632" target="_blank">02:07:12.780</a></span> | <span class="t">Let's see how it's done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7634" target="_blank">02:07:14.860</a></span> | <span class="t">The first thing that we need to do is to calculate the loss of ppu according to the formula that we saw on the slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7641" target="_blank">02:07:21.900</a></span> | <span class="t">So let's go in the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7643" target="_blank">02:07:23.900</a></span> | <span class="t">In the loss we have to calculate three losses. The first is the loss for the value head, which is this loss here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7650" target="_blank">02:07:30.700</a></span> | <span class="t">So they are actually doing it the hugging face is actually calculating also the clipped loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7656" target="_blank">02:07:36.060</a></span> | <span class="t">But let's not consider the clipped loss for now. It's just some new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7659" target="_blank">02:07:39.660</a></span> | <span class="t">It's just an optimization, but it doesn't have to be in the vanilla ppu. We don't have to do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7666" target="_blank">02:07:46.300</a></span> | <span class="t">so we are taking the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7668" target="_blank">02:07:48.700</a></span> | <span class="t">Values that were predicted by the model and the returns that we calculated as the sum of the advantages plus the values that we saw before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7675" target="_blank">02:07:55.660</a></span> | <span class="t">So this is how we this is the loss for the value head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7680" target="_blank">02:08:00.620</a></span> | <span class="t">According to this formula here. So as you can see, so this is basically the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7685" target="_blank">02:08:05.660</a></span> | <span class="t">Estimated q functions according to our trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7690" target="_blank">02:08:10.880</a></span> | <span class="t">And this is the loss of the value head then we have the loss of the ppu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7696" target="_blank">02:08:16.720</a></span> | <span class="t">Which is just the advantage term multiplied by the ratio of the log probabilities. What is the ratio of the probabilities? It's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7704" target="_blank">02:08:24.380</a></span> | <span class="t">here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7707" target="_blank">02:08:27.260</a></span> | <span class="t">The log probabilities, okay, let's go to the formula first. Okay, as you can see here we have the ratio of the two probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7714" target="_blank">02:08:34.080</a></span> | <span class="t">But we have the log probabilities. So what we can do is we can calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7718" target="_blank">02:08:38.880</a></span> | <span class="t">Let's use the here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7722" target="_blank">02:08:42.620</a></span> | <span class="t">Okay, we have the log probabilities so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7726" target="_blank">02:08:46.220</a></span> | <span class="t">The log of a minus the log of b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7735" target="_blank">02:08:55.740</a></span> | <span class="t">And then we are doing the exponential of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7737" target="_blank">02:08:57.740</a></span> | <span class="t">This is equivalent to doing the exponential of the log of a divided by b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7747" target="_blank">02:09:07.660</a></span> | <span class="t">Which is equal to doing a divided by b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7751" target="_blank">02:09:11.340</a></span> | <span class="t">Of the two probabilities. So because we do not have the log with the probabilities, but we have the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7756" target="_blank">02:09:16.880</a></span> | <span class="t">We are calculating like this. So we first do the log probabilities of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7761" target="_blank">02:09:21.180</a></span> | <span class="t">Online model minus the log probabilities of the offline model and then we apply the exponential which will result in a divided by b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7768" target="_blank">02:09:28.540</a></span> | <span class="t">Which is exactly what we want here. So let's check</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7771" target="_blank">02:09:31.260</a></span> | <span class="t">In the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7775" target="_blank">02:09:35.180</a></span> | <span class="t">We are calculating the difference in the log probabilities and applying the exponential which will result in this ratio here being calculated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7782" target="_blank">02:09:42.320</a></span> | <span class="t">Then we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7785" target="_blank">02:09:45.020</a></span> | <span class="t">This ratio is multiplied by the advantage term as you can see here. So we need to multiply it by this term advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7791" target="_blank">02:09:51.680</a></span> | <span class="t">And then we need to also calculate the other part of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7795" target="_blank">02:09:55.360</a></span> | <span class="t">Expression, which is this clipped advantage as you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7799" target="_blank">02:09:59.600</a></span> | <span class="t">So again the ratio but clipped between the value one minus epsilon and one plus epsilon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7805" target="_blank">02:10:05.920</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7808" target="_blank">02:10:08.000</a></span> | <span class="t">We are doing it here. So the advantage multiplied by the ratio clipped between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7811" target="_blank">02:10:11.840</a></span> | <span class="t">One minus epsilon and one plus epsilon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7815" target="_blank">02:10:15.220</a></span> | <span class="t">Why do we have this minus sign here because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7820" target="_blank">02:10:20.720</a></span> | <span class="t">The goal in ppo is we want to maximize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7823" target="_blank">02:10:23.780</a></span> | <span class="t">This term here, but we are doing you is using pytorch and the optimizer of pytorch, pytorch always run gradient descent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7833" target="_blank">02:10:33.140</a></span> | <span class="t">Which means that it's the opposite of gradient ascent. So if we want to um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7838" target="_blank">02:10:38.800</a></span> | <span class="t">So it's basically we are instead of maximizing this we can minimize the negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7844" target="_blank">02:10:44.500</a></span> | <span class="t">Loss that we can see here and this is exactly why we have this minus sign</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7850" target="_blank">02:10:50.160</a></span> | <span class="t">So because pytorch always minimizes we can multiply this by minus so it's like we are maximizing this term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7857" target="_blank">02:10:57.280</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7859" target="_blank">02:10:59.520</a></span> | <span class="t">The entropy is calculated here as you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7862" target="_blank">02:11:02.240</a></span> | <span class="t">Then they have also others</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7865" target="_blank">02:11:05.360</a></span> | <span class="t">Other terms that we do not use because they they do some optimizations, but they are not present in the vanilla loss of the ppo</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7872" target="_blank">02:11:12.880</a></span> | <span class="t">So the loss of the ppo is calculated as the the loss of the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7876" target="_blank">02:11:16.960</a></span> | <span class="t">Plus the value head multiplied by its coefficient that you can see here. So loss policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7882" target="_blank">02:11:22.240</a></span> | <span class="t">They calculate also the entropy, but they do not use it. I don't know why to be honest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7886" target="_blank">02:11:26.880</a></span> | <span class="t">So they calculated the entropy here. So they calculated the entropy using the logits as you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7891" target="_blank">02:11:31.840</a></span> | <span class="t">And they do it not using the formula that I show in the slides, which is the actual formula of the entropy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7897" target="_blank">02:11:37.600</a></span> | <span class="t">But they're using an optimized version called the log sum exp and I am putting here some information for those who want to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7905" target="_blank">02:11:45.840</a></span> | <span class="t">The derivation of how it's done. But basically wikipedia says that the convex conjugate of the log sum eps is the negative entropy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7912" target="_blank">02:11:52.500</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7916" target="_blank">02:11:56.000</a></span> | <span class="t">Yeah, so we have also this entropy term here and we return our loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7921" target="_blank">02:12:01.600</a></span> | <span class="t">So let's go back to the optimization step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7924" target="_blank">02:12:04.400</a></span> | <span class="t">So we go here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7927" target="_blank">02:12:07.520</a></span> | <span class="t">So now we are optimizing over a mini batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7930" target="_blank">02:12:10.960</a></span> | <span class="t">Which means that the first thing that we do is we calculate the loss and then we run a back propagation on this loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7936" target="_blank">02:12:16.640</a></span> | <span class="t">and then we optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7938" target="_blank">02:12:18.720</a></span> | <span class="t">and we do it for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7940" target="_blank">02:12:20.560</a></span> | <span class="t">Oops, we do it for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7942" target="_blank">02:12:22.560</a></span> | <span class="t">Many mini batches, so let me go back again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7946" target="_blank">02:12:26.720</a></span> | <span class="t">So we train on a one mini batch then we do it again for many mini batches you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7955" target="_blank">02:12:35.360</a></span> | <span class="t">And after a while we return</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7957" target="_blank">02:12:37.600</a></span> | <span class="t">We return here and we do again the procedure again. So we again generate the new trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7964" target="_blank">02:12:44.100</a></span> | <span class="t">Then the hugging face library will calculate and we calculate of course also the rewards the hugging face library will calculate the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7973" target="_blank">02:12:53.460</a></span> | <span class="t">According to these trajectories the advantage estimation according to these trajectories the value estimation according to the trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7981" target="_blank">02:13:01.140</a></span> | <span class="t">Then we'll iteratively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7984" target="_blank">02:13:04.080</a></span> | <span class="t">Sample from these trajectories some mini batches and then we run a gradient ascent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7989" target="_blank">02:13:09.600</a></span> | <span class="t">According to the ppo loss on these mini batches many times and then again, we restart the loop and this is how we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=7996" target="_blank">02:13:16.560</a></span> | <span class="t">Run the ppo algorithm for reinforcement learning from human feedback</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8002" target="_blank">02:13:22.180</a></span> | <span class="t">Let's go back to the slides and thank you guys for watching this video, I know it has been very very demanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8011" target="_blank">02:13:31.760</a></span> | <span class="t">It has been one of my most difficult video also for me to describe all these parts without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8017" target="_blank">02:13:37.360</a></span> | <span class="t">Getting lost myself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8020" target="_blank">02:13:40.240</a></span> | <span class="t">I know that I gave a lot of knowledge because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8022" target="_blank">02:13:42.560</a></span> | <span class="t">Actually ppo and the reinforcement learning are quite big topic. So there are entire university courses on this stuff. So it's not easy to give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8030" target="_blank">02:13:50.160</a></span> | <span class="t">A complete understanding in just a few hours. This is also one of the reason I decided not to code it from scratch because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8040" target="_blank">02:14:00.000</a></span> | <span class="t">It would make the video like 10 hours of video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8042" target="_blank">02:14:02.160</a></span> | <span class="t">but at least I hope that now you have a deep understanding into how each step of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8048" target="_blank">02:14:08.320</a></span> | <span class="t">Reinforcement learning from human feedback is done. I will share with you the code commented by me with all the parts that are unnecessary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8055" target="_blank">02:14:15.540</a></span> | <span class="t">Removed or anyway, I will comment telling explicitly which parts are not necessary for the ppo algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8062" target="_blank">02:14:22.160</a></span> | <span class="t">It took me more than one month of research to prepare this video and I had to record it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8068" target="_blank">02:14:28.560</a></span> | <span class="t">multiple times because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8070" target="_blank">02:14:30.560</a></span> | <span class="t">I made some some mistakes and then I realized that I forgot something in the slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8074" target="_blank">02:14:34.880</a></span> | <span class="t">Then I had to fix them etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8077" target="_blank">02:14:37.520</a></span> | <span class="t">So the best way to help me guys is to share this video with others if you found it useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8082" target="_blank">02:14:42.320</a></span> | <span class="t">I know that it's very difficult</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8083" target="_blank">02:14:43.600</a></span> | <span class="t">So I suggest watching it multiple times because the first time you watch this video you will have some understanding but not very deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8091" target="_blank">02:14:51.360</a></span> | <span class="t">The second time you will realize that you will have a better understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8095" target="_blank">02:14:55.520</a></span> | <span class="t">And maybe you will need to review some concepts from reinforcement learning or from the transformer to better understand it fully</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8102" target="_blank">02:15:02.000</a></span> | <span class="t">So I recommend watching it multiple times and please leave in the comments if some part was not clear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGyFrqc34yc&t=8108" target="_blank">02:15:08.240</a></span> | <span class="t">I will always try to help you and yeah, have a nice day</span></div></div></body></html>