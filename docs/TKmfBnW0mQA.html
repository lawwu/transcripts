<html><head><title>Fixing bugs in Gemma, Llama, & Phi 3: Daniel Han</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Fixing bugs in Gemma, Llama, & Phi 3: Daniel Han</h2><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA"><img src="https://i.ytimg.com/vi_webp/TKmfBnW0mQA/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=32">0:32</a> Overview<br><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=135">2:15</a> Double BS Tokens<br><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=180">3:0</a> Untrained Tokens<br><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=518">8:38</a> Free Fine Tuning<br><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=640">10:40</a> Collab Notebook<br><br><div style="text-align: left;"><a href="./TKmfBnW0mQA.html">Whisper Transcript</a> | <a href="./transcript_TKmfBnW0mQA.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello, everyone. So today I'm going to talk about how to fix bugs in open source models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=18" target="_blank">00:00:18.800</a></span> | <span class="t">Thanks for coming again. We had a talk yesterday, the three-hour workshop, and thanks for coming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=24" target="_blank">00:00:24.880</a></span> | <span class="t">again. So we have slides. It's at tinyurl.com/unsloth2. For the workshop slides, which we did yesterday,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=33" target="_blank">00:00:33.280</a></span> | <span class="t">you can also access that now. Tinyurl.com/unsloth as well. So you might know me from, like, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=41" target="_blank">00:00:41.060</a></span> | <span class="t">Gemma bug fixes that we did. So Gemma was an open source model by Google, and there was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=44" target="_blank">00:00:44.920</a></span> | <span class="t">a few bugs in there, and we fixed a few of them. And we just did some tweets about this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=51" target="_blank">00:00:51.060</a></span> | <span class="t">and, you know, like, there's many bugs in there, like, you know, the activation function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=54" target="_blank">00:00:54.460</a></span> | <span class="t">had to be approximate JLU, not exact JLU, and there are some other issues that we talked about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=60" target="_blank">00:01:00.620</a></span> | <span class="t">for Gemma. We also have, like, some -- a few stickers, which you can, you know, get them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=65" target="_blank">00:01:05.440</a></span> | <span class="t">from, like, when we're outside. But this is -- yeah, we won't be handing them during the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=70" target="_blank">00:01:10.160</a></span> | <span class="t">talk. But, yeah, they're very cool and cute. And also there's, like, tokenization problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=75" target="_blank">00:01:15.520</a></span> | <span class="t">as well in language models, which we also help to fix. Today I'm just going to be talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=80" target="_blank">00:01:20.860</a></span> | <span class="t">about LLAMA 3 bugs. So yesterday I talked about Gemma and V3, and today we're just sharing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=85" target="_blank">00:01:25.860</a></span> | <span class="t">all the stuff that we found with LLAMA 3. For Gemma, you can access, like, all the bug fixes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=91" target="_blank">00:01:31.160</a></span> | <span class="t">that we did in our blog post, and we have a collab notebook for all of the Gemma bug fixes as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=96" target="_blank">00:01:36.520</a></span> | <span class="t">well. For V3, for example, we talked about this yesterday, and I just pasted some slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=104" target="_blank">00:01:44.200</a></span> | <span class="t">again if you want to, like, review this in your own time. For example, like, the sliding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=108" target="_blank">00:01:48.320</a></span> | <span class="t">window should be 2048, not 2047. You can also, like, you should unfuse all of the QKV matrices,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=116" target="_blank">00:01:56.820</a></span> | <span class="t">otherwise lower fine tuning will not work that well. But we'll be talking mainly about LLAMA 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=123" target="_blank">00:02:03.360</a></span> | <span class="t">So there's actually eight bugs in LLAMA 3. Some of them are not announced yet. We will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=127" target="_blank">00:02:07.640</a></span> | <span class="t">be announcing these later. So this is, like, a pre-release. And we'll be going through each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=132" target="_blank">00:02:12.920</a></span> | <span class="t">of them separately. The first one is you must not use double BOS tokens. So this is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=139" target="_blank">00:02:19.880</a></span> | <span class="t">a very common theme in fine tuning LLAMA 3. Some people don't actually know that you're adding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=144" target="_blank">00:02:24.240</a></span> | <span class="t">two beginning of sentence tokens to the fine tune. And this will actually ruin your fine tune</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=148" target="_blank">00:02:28.980</a></span> | <span class="t">by making the accuracy of your inference time lower. So please, like, check before you fine tune if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=154" target="_blank">00:02:34.260</a></span> | <span class="t">using double BOS tokens. In Unsloft, we do this-- we check this automatically and we'll remove the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=159" target="_blank">00:02:39.740</a></span> | <span class="t">extra BOS token automatically for you. So this will actually cause your model to lose accuracy because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=165" target="_blank">00:02:45.260</a></span> | <span class="t">if you trained on two BOS tokens and you do inference on one, then your model template will be incorrect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=170" target="_blank">00:02:50.700</a></span> | <span class="t">So please check this. It's not just a LLAMA 3 problem. Other models like Mistral</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=175" target="_blank">00:02:55.980</a></span> | <span class="t">and Gemma also have problems like this. So just be careful of this issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=181" target="_blank">00:03:01.100</a></span> | <span class="t">So a very easy way to check if you have double BOS tokens, if you use the apply chat template</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=186" target="_blank">00:03:06.380</a></span> | <span class="t">from Hugging Face, if you do the first one, your chat template must have a BOS token. Otherwise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=192" target="_blank">00:03:12.380</a></span> | <span class="t">it won't add it. LLAMA 3 does require a BOS token. If you do the second one, you're actually having</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=198" target="_blank">00:03:18.620</a></span> | <span class="t">two BOS tokens if you do this. So please do not add a BOS token to the chat template.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=204" target="_blank">00:03:24.380</a></span> | <span class="t">The second issue we found was you must not use the LLAMA 3 base model if you're using the LLAMA 3 template.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=211" target="_blank">00:03:31.660</a></span> | <span class="t">There are some untrained tokens in LLAMA 3 base. The Instruct version actually has these tokens trained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=218" target="_blank">00:03:38.460</a></span> | <span class="t">So please be careful when you want to use the LLAMA 3 base model when you want to do your fine tuning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=223" target="_blank">00:03:43.340</a></span> | <span class="t">because some of these tokens will cause NANDs for your gradients. These tokens include the reserve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=227" target="_blank">00:03:47.580</a></span> | <span class="t">special tokens from zero to 250, the end of turn token, the start header, and the end of header.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=235" target="_blank">00:03:55.020</a></span> | <span class="t">And the graph I showed shows you the mean of the embeddings versus the other tokens. And some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=241" target="_blank">00:04:01.100</a></span> | <span class="t">them actually are zero. So the LLAMA 3 team made some of these tokens go to zero purposefully because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=246" target="_blank">00:04:06.940</a></span> | <span class="t">these tokens are not actually used for the model. So just please don't use some of these tokens when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=252" target="_blank">00:04:12.540</a></span> | <span class="t">you do fine tuning as well. If you want to fix them, set them to the mean of the entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=257" target="_blank">00:04:17.420</a></span> | <span class="t">tokens. And in Onsloft, we do this automatically as well for you. So we showed some code where you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=263" target="_blank">00:04:23.500</a></span> | <span class="t">can take the mean of the trained tokens and set them for the untrained tokens. Just be careful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=269" target="_blank">00:04:29.740</a></span> | <span class="t">don't do this like incorrectly as well. If you want to take the average of all the tokens, don't just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=274" target="_blank">00:04:34.700</a></span> | <span class="t">take the average. You must remove the untrained tokens from the average. If you do not do that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=279" target="_blank">00:04:39.340</a></span> | <span class="t">you might actually have an incorrect average, right? If there's like 10,000 tokens which are untrained,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=283" target="_blank">00:04:43.980</a></span> | <span class="t">if you divide it by 10,000 plus the number of trained tokens, your average will be incorrect. So you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=290" target="_blank">00:04:50.220</a></span> | <span class="t">have to do this more complicated method of masking out the untrained tokens and then take the average.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=295" target="_blank">00:04:55.260</a></span> | <span class="t">Also, reminder, because of this issue, the LLAMA 3 chat template will not work for the base model. I have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=302" target="_blank">00:05:02.940</a></span> | <span class="t">known that many fine tuning people have used LLAMA 3 Instruct chat template for the base model and your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=308" target="_blank">00:05:08.940</a></span> | <span class="t">fine tune will actually be incorrect. You will get NANs in your gradients and your whole fine tune will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=313" target="_blank">00:05:13.740</a></span> | <span class="t">broken. So please do not use the LLAMA 3 Instruct chat template for the LLAMA 3 base model. Only use this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=319" target="_blank">00:05:19.820</a></span> | <span class="t">for the Instruct model itself. Another way to fix this is to actually train the LM head and the embed tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=325" target="_blank">00:05:25.340</a></span> | <span class="t">which will actually learn and remove the NANs in your models. Another interesting fact, and not just a LLAMA 3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=333" target="_blank">00:05:33.180</a></span> | <span class="t">problem, but for other models, is the pad token and the EOS token must not be the same. If you do this the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=340" target="_blank">00:05:40.300</a></span> | <span class="t">same, your model will have infinite generations. The reason is because the pad token gets masked out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=346" target="_blank">00:05:46.460</a></span> | <span class="t">during the error, during the cross entropy loss, and if you use the same pad token as the EOS token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=352" target="_blank">00:05:52.540</a></span> | <span class="t">then your EOS token, the end of sentence token, will be masked out. So just be very, very careful when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=357" target="_blank">00:05:57.500</a></span> | <span class="t">you do fine tuning to check what is the pad token ID and the EOS token ID. For example, if you look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=363" target="_blank">00:06:03.020</a></span> | <span class="t">V3, they're the same. So technically, V3, when you do fine tuning, it will be infinite generations. So just be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=370" target="_blank">00:06:10.460</a></span> | <span class="t">careful and look, you know, before you do the fine tune, check what is the EOS token and what is the pad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=375" target="_blank">00:06:15.340</a></span> | <span class="t">token? It must be different. For unsloft, we also do this automatically. We fix this for you. And we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=383" target="_blank">00:06:23.340</a></span> | <span class="t">essentially check if there is any unreserved tokens, and we just select one which is untrained. If there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=390" target="_blank">00:06:30.140</a></span> | <span class="t">is no untrained tokens, then we will add an extra pad token ourselves. Be careful, do not add a pad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=395" target="_blank">00:06:35.900</a></span> | <span class="t">token which has the same, like, vocabulary as your current vocabulary. So what we do is we actually check</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=401" target="_blank">00:06:41.820</a></span> | <span class="t">the tokens inside the vocabulary and add, like, extra hashes to see, you know, to make a new pad token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=407" target="_blank">00:06:47.580</a></span> | <span class="t">Another issue we found for fine tuning people is, like, when you finish your fine tune,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=414" target="_blank">00:06:54.380</a></span> | <span class="t">you don't actually know how to export it to Olama. And that is because the chat template for Olama must</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=419" target="_blank">00:06:59.340</a></span> | <span class="t">be exactly the same as your fine tune. And this was actually very complicated to do before. And now we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=424" target="_blank">00:07:04.940</a></span> | <span class="t">can actually automatically generate the model file for you during the fine tune. So we have, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=429" target="_blank">00:07:09.740</a></span> | <span class="t">two collab notebooks for you to use for Olama. One of them is the alpaca dataset. And one of them is a -- you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=435" target="_blank">00:07:15.660</a></span> | <span class="t">can upload a CSV file to make Olama work after you finish fine tuning. Now, there are some community</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=443" target="_blank">00:07:23.500</a></span> | <span class="t">contributions for Olama 3 bugs. There is, like, three of them. The first one is someone noticed that you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=450" target="_blank">00:07:30.300</a></span> | <span class="t">only use CPU conversion and not GPU conversion when you convert to GGOF or Lama CPP. So be -- you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=456" target="_blank">00:07:36.780</a></span> | <span class="t">be careful when you convert to Lama CPP that you must use the CPU version. I think the main reason is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=462" target="_blank">00:07:42.300</a></span> | <span class="t">because the precision is different in a GPU than a CPU. The CPU, when you do float 16, it's different from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=468" target="_blank">00:07:48.860</a></span> | <span class="t">when the GPU does float 16 conversion. So just be careful on that as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=474" target="_blank">00:07:54.060</a></span> | <span class="t">Another issue is, remember, we talked about the WBOS tokens. Through our community contribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=479" target="_blank">00:07:59.020</a></span> | <span class="t">Lama CPP now has a warning for you to tell you that you're using WBOS tokens. So please, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=486" target="_blank">00:08:06.300</a></span> | <span class="t">take heed of the warning and do not add WBOS tokens to your chat template. And when you do inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=491" target="_blank">00:08:11.420</a></span> | <span class="t">Another point someone found was adding a system prompt could make fine tuning much better. And so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=499" target="_blank">00:08:19.580</a></span> | <span class="t">like, sometimes when you do inference on Lama 3 instruct, if you add an actual system prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=504" target="_blank">00:08:24.540</a></span> | <span class="t">this could make your whole fine tuning better. I think for some people, when they add the system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=509" target="_blank">00:08:29.260</a></span> | <span class="t">prompt, you actually miss the system prompt. Like, you don't actually add one. So maybe try your fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=513" target="_blank">00:08:33.340</a></span> | <span class="t">tune with the system prompt. And you never know, this could work. So we have, like, a GitHub package,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=520" target="_blank">00:08:40.460</a></span> | <span class="t">which is open source. And you can click the button "Start Free Fine Tune" to start your first free</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=525" target="_blank">00:08:45.340</a></span> | <span class="t">fine tune using Unsloth. We already pushed all the Lama 3 bug fixes to our GitHub repo. And so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=530" target="_blank">00:08:50.380</a></span> | <span class="t">the Start Free Fine Tune button will redirect you to a fixed collab notebook for all of these issues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=535" target="_blank">00:08:55.340</a></span> | <span class="t">Feel free to star us as well. We also, like, have a Discord channel. So if you have any questions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=540" target="_blank">00:09:00.860</a></span> | <span class="t">you can ask, you know, any questions that you like about our, you know, how to do fine tuning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=546" target="_blank">00:09:06.140</a></span> | <span class="t">talk about AI, and talk about our bugs as well. We also have, like, a blog post. So blog posts about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=552" target="_blank">00:09:12.700</a></span> | <span class="t">all our fixes, about Gemma, Lama 3, Fee 3, and more. For example, we talked about continued pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=559" target="_blank">00:09:19.740</a></span> | <span class="t">You can do continued pre-training using Unsloth now. You can train on the LM head and the embed tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=565" target="_blank">00:09:25.340</a></span> | <span class="t">And we show that instead of just training like that, you need to reduce the learning rate of the LM head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=570" target="_blank">00:09:30.220</a></span> | <span class="t">and the embed tokens by 10. Or, you know, maybe 5 to 10. And this will make your training much better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=575" target="_blank">00:09:35.500</a></span> | <span class="t">We also support four times longer contacts using Unsloth. And this also does not increase the time of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=582" target="_blank">00:09:42.860</a></span> | <span class="t">completion. So we make it 1 to 2 percent slower. But you get four times longer contacts using Unsloth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=589" target="_blank">00:09:49.740</a></span> | <span class="t">And this was because, like, we used something called offloading gradient checkpointing, where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=594" target="_blank">00:09:54.540</a></span> | <span class="t">we offload the gradients to system RAM. There are some other systems which offload the gradients to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=599" target="_blank">00:09:59.580</a></span> | <span class="t">the disk. Please do not do that. If you offload to disk, then your time of completion of your fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=604" target="_blank">00:10:04.300</a></span> | <span class="t">tune will be extremely slow. So try to offload to system RAM first, and then offload to disk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=609" target="_blank">00:10:09.420</a></span> | <span class="t">Although if you don't -- if you offload incorrectly, you might actually make this slower as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=614" target="_blank">00:10:14.460</a></span> | <span class="t">So your offloading must be non-blocking calls. And do not do blocking calls to the system RAM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=619" target="_blank">00:10:19.260</a></span> | <span class="t">Yeah. So I will show you -- okay. Let's see if I can open up -- let me go to --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=631" target="_blank">00:10:31.500</a></span> | <span class="t">Okay. I'm going to open up a Colab Notebook for the Olama one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=648" target="_blank">00:10:48.700</a></span> | <span class="t">Okay. So for the Olama Colab Notebook, you can simply just install Unsloth over here. This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=655" target="_blank">00:10:55.500</a></span> | <span class="t">already for free for everyone to use. And essentially, you don't forget when you do the Colab Notebook,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=660" target="_blank">00:11:00.460</a></span> | <span class="t">you have to select a max sequence length. This determines how long your model wants to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=665" target="_blank">00:11:05.820</a></span> | <span class="t">long contacts fine tuning. You can set this to any number that you like. But remember,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=669" target="_blank">00:11:09.980</a></span> | <span class="t">your data set must match the max sequence length. So for example, if you have -- if you want to set the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=674" target="_blank">00:11:14.380</a></span> | <span class="t">max sequence length to be like 10 million or one million, but your data set is only like one million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=678" target="_blank">00:11:18.940</a></span> | <span class="t">tokens or like less, try to like not set that max sequence length to be that large. Otherwise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=683" target="_blank">00:11:23.660</a></span> | <span class="t">your model cannot do fine tuning a long sequence. Load in 4-bit does 4-bit training. So this actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=689" target="_blank">00:11:29.740</a></span> | <span class="t">reduces memory usage by four times. If you do -- if you do it to false, your memory usage will explode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=695" target="_blank">00:11:35.500</a></span> | <span class="t">So please do not try false. Especially on a free Colab Tesla T4. If you do false, your memory usage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=702" target="_blank">00:11:42.700</a></span> | <span class="t">might skyrocket to 16 GB. So do not do that. You only should do this if you use more stronger GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=709" target="_blank">00:11:49.260</a></span> | <span class="t">We support -- like Unsloth supports fine tuning or models including like Lama, Mistral, Gemma, V3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=715" target="_blank">00:11:55.980</a></span> | <span class="t">and more. So this area, like the model name over here, you can actually try to select any model name</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=721" target="_blank">00:12:01.980</a></span> | <span class="t">that you like. I don't think that people know that Unsloth can support other models other than the ones</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=726" target="_blank">00:12:06.460</a></span> | <span class="t">we listed. So please try to put any model, like a hugging face model name in there. And it should work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=731" target="_blank">00:12:11.900</a></span> | <span class="t">So for the get peft model, this is where you add the peft LoRa adapters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=738" target="_blank">00:12:18.620</a></span> | <span class="t">The R is the rank. So we set it to be 16. But you can select any number that you like for fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=745" target="_blank">00:12:25.500</a></span> | <span class="t">So we suggest you normally to use powers of two. But you can use any number, like one, two, three,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=749" target="_blank">00:12:29.580</a></span> | <span class="t">like any number that you like. The larger the rank you select, you can make the model learn more about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=754" target="_blank">00:12:34.780</a></span> | <span class="t">your data set. So -- but if you add too large of a rank, you might actually overfit your data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=759" target="_blank">00:12:39.180</a></span> | <span class="t">And also your memory usage might skyrocket again. So we normally suggest people to select 16, 32, 64,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=764" target="_blank">00:12:44.220</a></span> | <span class="t">or 128. Try not to select too large ranks. The maximum rank you should select is the size of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=770" target="_blank">00:12:50.300</a></span> | <span class="t">dimension of the model itself. So if it's 4096, set this to be 4096.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=775" target="_blank">00:12:55.020</a></span> | <span class="t">For the target modules, be careful. You must do fine tuning on all linear layers, right? So Q,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=783" target="_blank">00:13:03.100</a></span> | <span class="t">K, V, O, down, up, and gate. Some people have done fine tuning without doing some of these layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=789" target="_blank">00:13:09.260</a></span> | <span class="t">Please do not do that. Because this will cause your fine tune to be not optimal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=792" target="_blank">00:13:12.700</a></span> | <span class="t">And the LoRa alpha, there is actually a trick for this. Normally speaking, select the alpha to be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=798" target="_blank">00:13:18.540</a></span> | <span class="t">same as the same as the rank or larger. We found that if you do 16 times 2, so the rank times 2, this can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=804" target="_blank">00:13:24.700</a></span> | <span class="t">make your fine tuning much better. You can also use use RS LoRa to be true to set the rank automatically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=810" target="_blank">00:13:30.780</a></span> | <span class="t">for you -- to set the alpha automatically for you. For the gradient checkpointing, unsloft is the method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=815" target="_blank">00:13:35.580</a></span> | <span class="t">which we showed that you can do long context fine tuning. You can also set this to be true, but your memory usage will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=821" target="_blank">00:13:41.420</a></span> | <span class="t">increase again. We also show you how to do data preparation in an Olama Colab notebook. So this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=828" target="_blank">00:13:48.060</a></span> | <span class="t">one is we upload a Titanic CSV. So the Titanic data set, the goal was, can you predict if someone died or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=833" target="_blank">00:13:53.660</a></span> | <span class="t">survived if you're on the Titanic? And you get details about the person. For example, their age, their, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=840" target="_blank">00:14:00.860</a></span> | <span class="t">fair, where did they embark from, and so on. With our new Colab notebooks, you have to be very careful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=847" target="_blank">00:14:07.260</a></span> | <span class="t">when you do Olama chat templates. Because when you do fine tuning, you can only have two columns,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=852" target="_blank">00:14:12.460</a></span> | <span class="t">the instruction and the output. But what happens if your CSV has more than one -- like, more than two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=856" target="_blank">00:14:16.940</a></span> | <span class="t">columns, the instruction and output? What we can do is you can merge the columns into one column. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=861" target="_blank">00:14:21.980</a></span> | <span class="t">with unsloft now, you can actually do that. You can merge the columns into one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=869" target="_blank">00:14:29.260</a></span> | <span class="t">And also, we show you that you can do customizable chat templates now. So previously, if you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=873" target="_blank">00:14:33.740</a></span> | <span class="t">do an Alpaca-style fine tune, you have to use instruction, input, and response for the Alpaca-style</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=878" target="_blank">00:14:38.220</a></span> | <span class="t">fine tuning. But remember, the problem is, if you want to output to Olama or GGWeb, you can only have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=884" target="_blank">00:14:44.140</a></span> | <span class="t">two columns, the instruction and output. Right? If you do ChatTBT, you have to type something and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=888" target="_blank">00:14:48.940</a></span> | <span class="t">the output comes along. You can't have, like, three inputs, right? So what we do is you can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=895" target="_blank">00:14:55.260</a></span> | <span class="t">customize your chat template. And you must include the input and the output. And you must do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=900" target="_blank">00:15:00.380</a></span> | <span class="t">repetition twice. Some people have asked me, like, why do you have to do two repetitions of this chat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=905" target="_blank">00:15:05.420</a></span> | <span class="t">template? It's because there is dangling new lines. And we found this -- we found a solution to this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=910" target="_blank">00:15:10.940</a></span> | <span class="t">you have to specify two iterations of your chat template. We also show examples of how to do the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=917" target="_blank">00:15:17.500</a></span> | <span class="t">Lama 3 chat template using our methodology. So you can see there is two iterations of the chat template.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=923" target="_blank">00:15:23.020</a></span> | <span class="t">Reminder, if you don't use the two iterations, you actually -- it will error out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=927" target="_blank">00:15:27.420</a></span> | <span class="t">And this is the training methodologies. We normally suggest people to use a batch size of two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=933" target="_blank">00:15:33.500</a></span> | <span class="t">gradient accumulation of four. Remember, the memory usage is only relevant to the batch size. So try not to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=939" target="_blank">00:15:39.340</a></span> | <span class="t">set the batch size to be very large, otherwise your memory usage will explode. Instead, set your gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=943" target="_blank">00:15:43.740</a></span> | <span class="t">accumulation steps to be larger. So the formula for the effective batch size is batch size times the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=949" target="_blank">00:15:49.340</a></span> | <span class="t">gradient accumulation. So in this case, it's two times four, which is eight. Set your learning rate to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=953" target="_blank">00:15:53.820</a></span> | <span class="t">be 2e minus four or smaller, maybe 2e minus five. And after that, you can also do inference on the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=963" target="_blank">00:16:03.180</a></span> | <span class="t">So now you have to use the apply chat template. Remember, be careful of double BOS tokens. But we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=967" target="_blank">00:16:07.660</a></span> | <span class="t">in Unslaw fixed this. And finally, you have to save this to Ollama. And, you know, you have to install</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=975" target="_blank">00:16:15.260</a></span> | <span class="t">Ollama first. Saving now -- we now support saving multiple ggware files. So you don't actually have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=980" target="_blank">00:16:20.940</a></span> | <span class="t">to save it to one ggware file. You can save it to multiple. And we actually allow you to do this now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=984" target="_blank">00:16:24.780</a></span> | <span class="t">Before, if you want to save to multiple ggware files, you have to wait 10 minutes extra. You can now do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=990" target="_blank">00:16:30.140</a></span> | <span class="t">this automatically by, you know, specifying more than one format.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=993" target="_blank">00:16:33.340</a></span> | <span class="t">We also can show you the model file which we created. So you can actually copy-paste the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=999" target="_blank">00:16:39.180</a></span> | <span class="t">file and put this to custom -- like a custom Ollama as well. So the model file was the complicated part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=1005" target="_blank">00:16:45.100</a></span> | <span class="t">when we had to automatically generate this. So we have, like, internal code to generate the model file</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=1009" target="_blank">00:16:49.100</a></span> | <span class="t">automatically. And finally, when you want to do inference, you can do Ollama to do inference. And, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=1016" target="_blank">00:16:56.380</a></span> | <span class="t">know, it works in general. So try that out. The Ollama chat template notebook is in the slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=1022" target="_blank">00:17:02.940</a></span> | <span class="t">So tinyurl.com/unsloth2. And remember, the workshop slides, which we did yesterday,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=1027" target="_blank">00:17:07.820</a></span> | <span class="t">is tinyurl.com/unsloth. And don't forget to join our Discord channel. If you have any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=1034" target="_blank">00:17:14.380</a></span> | <span class="t">questions, I'm outside. You can ask questions and stuff like that. And, yes, like, thanks for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=1038" target="_blank">00:17:18.940</a></span> | <span class="t">thanks for coming. I much appreciate it. Thanks a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=TKmfBnW0mQA&t=1048" target="_blank">00:17:28.940</a></span> | <span class="t">I'll see you next time.</span></div></div></body></html>