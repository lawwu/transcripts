<html><head><title>How to build world-class AI products — Sarah Sachs (AI lead @ Notion) &  Carlos Esteban (Braintrust)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>How to build world-class AI products — Sarah Sachs (AI lead @ Notion) &  Carlos Esteban (Braintrust)</h2><a href="https://www.youtube.com/watch?v=6YdPI9YbjbI" target="_blank"><img src="https://i.ytimg.com/vi_webp/6YdPI9YbjbI/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>Carlos Esteban: Wow, look at this turnout. It's kind of crazy how many people are excited to listen to Sarah and hear what Notion AI has been building. So just going to do some quick introductions before handing it over to her. My name is Carlos Esteban. I'm a Solution Engineer here at BrainTrust.</p><p>So Doug and I are going to share a bit about BrainTrust after Sarah presents. Just wanted to say hi. You'll see me talk a bit more today. I've been at BrainTrust for six weeks. Previously, I was at an infant company. Yeah, the six weeks may be funny, but also I'm the veteran on the team.</p><p>So Doug here. It's his third week. So we're up here in front of you and going to teach you about BrainTrust. We have some devals. Yeah, if you want to go ahead. Yeah, sure. Solutions Engineer also alongside Carlos. Like you mentioned, been here a full three weeks at this point.</p><p>Actually, not even full. It's like my, I don't know, 11th or 9th day. But yeah, this is incredibly exciting to be here. Obviously, a lot of interest in Sarah's talk. And hopefully, we can teach you a little bit about BrainTrust in the process. So yeah, I just want to present Sarah.</p><p>She's the lead of Notion AI. They've been customers pretty much since the beginning of BrainTrust, definitely pioneered the platform. And yeah, super exciting to hear what she has to say about their journey. Awesome. Thanks. I've been working for 10 months. So I might be the most experienced BrainTrust user on the panel.</p><p>Just kidding. No. I mean, I think I want to save time for questions, too, before we dig into the workshop. I think at a higher level, what I tell the team and what I tell people who are also building is like all of the rigor and excellence that comes from building great AI products comes from observability and good evals.</p><p>And that's how you scale an engineering team. That's how you build good product. And ultimately, we spend maybe 10% of our time prompting and 90% of our time looking at evals and iterating on our evals and looking at our usage and BrainTrust. And that, I believe, is the right balance of work in order to know that you're not just shipping something that worked well in a demo with your VP or that you got working on the Caltrain to work and finally did what you wanted that one time, but actually worked consistently and for the users you were curious about.</p><p>So I'll speak a little high level, save some time for questions, and then I hope everyone sees value in the workshop as well because it's really a tool that is accessible to all different levels, like I have tech leads on our team that themselves are true experts in BrainTrust, someone like me that might not be necessarily executing experiments, but I'm in the platform every day, and then a whole host of data labelers and specialists that work out of the platform as well, which we'll talk about.</p><p>So, who here has used Notion? Okay. Love it. Who here has used Notion AI? Okay. Some sales opportunities. So, what is Notion AI? Oh, that's me. I've been at Notion for about 10 months, as I said. Notion, this idea, for those of you that didn't raise your hand, is a connected workspace.</p><p>What are we connecting? We're everything from workplace management, asynchronous work, documents, but we also connect to third-party tools like Slack and Jira and Google Drive. We have 100 million users, over 100 million users now. And something that's really important to note about Notion AI is we offer a free trial of all of our AI products, almost all of them.</p><p>And what does that mean? It means that the scale that we build for has to support that scale. So, if you looked at the balance of who raised their hands, right, maybe not every user is an active Notion AI user, but the scale in which we support Notion AI, we want to offer it.</p><p>So, a new feature, for instance, like concurrently might have far more users than paid enterprise plan users or people in our business plan. And I like to think we're known for our exceptional customer experience. We're certainly a design-driven company, which is to say that we care deeply about our product, and there's a lot of polish that's associated with the brand.</p><p>And polish is not often associated with Gen AI experiences. So, how do you add that level of polish and care into what you're building while still building at the speed and rate of acceleration that exists in the industry, right? For instance, we're very proud that we partner with a lot of foundation model providers.</p><p>We also fine-tune our own models. Anytime a new model is released, within usually less than a day, we're able to give that to all of our users in production. In order to move at that pacing, but still have a polished product, you need to have evals, and you need to have integration with a product like BrainTrust.</p><p>So, here's our latest launch. This was two weeks ago, a suite of projects that we just launched with AI. No audio, but I'll give you a little voiceover. So, AI meeting notes is kind of our first. So, we now have text-to-speech, or speech-to-text, excuse me, along with transcription AI generated summaries, and fairly soon you'll be able to see those also interact with your task database, your whole workspace, workspace awareness, when talking about action items.</p><p>We have an enterprise search product that, when you're searching, can search across everything. I use this constantly, particularly in stand-ups, when they're talking about Slack threads that are like 50 messages deep that I stopped following at 10 p.m. I can use Notion AI, and for much deeper searches, we have a deep research tool.</p><p>That deep research executes parallel searches, serves a lot of our fine-tuned agentic capabilities, and is kind of our first transition out of workflows into agents, where rather than having like a set list of tasks or flows that your AI program is running through, now we're giving it the reasoning capability to decide on different tools and spend longer on the work that it's doing.</p><p>So that's Notion AI for work. That's the latest suite that we built, and I'll talk a little bit about what we've learned as we've built this latest suite, and as well, older generation Notion AI products. Let's see. No, we already watched that. Okay. So, believe it or not, Notion AI actually came out before ChatGPT.</p><p>That's kind of its own story that none of you are here to hear about, but we had early access to the generative models and always believed that content generation was core to how Notion worked. So our first product launched right around the same time. It was called the AI Writer.</p><p>It allowed you to generate just in line, you know, write a sentence about XYZ, right? From there, we started building Autofill. Autofill is our AI kind of agent that lives in every database property. Notion allows you to have databases. Now, all of a sudden, the AI is not just acting on the page, but is acting across the database and is triggered frequently and is doing things like translating things so that every column is in a different language.</p><p>This is where we started seeing a lot of usage that was more unpredicted from our users, and it's around the same time that we also started building a core AI team. Then we built kind of a natural rag solution. This was kind of the, I should say, like, the first time that we had, like, a full data platform AI full collaboration, and this is where we started offering, for instance, Q&A to, like, free users.</p><p>So we have to have embeddings for everyone. We have to think about multilingual workspaces, things like that. And then now we get to the era where we started working with Braintrust. So you saw you could search across all apps. We launched the idea to have attachments. A lot of people use Notion as file storage, or they'll upload things to Notion.</p><p>We can search over those attachments. And finally, the things that we just talked about. So one thing I want to note is that we didn't start with what I just showed you, and I think that would have been quite naive. Obviously, with the technology we have today, that would have been quite easy.</p><p>But we also knew that we didn't have the technology to build those things yet, and we worked really with where the models were most capable. What makes it hard to evaluate? So number one, these are exceptionally large data sets. Notion's really lucky because we use Notion constantly when we work on Notion.</p><p>And so we can generate a lot of training data or evaluation data just from our own dogfooding. I think that's actually one of the unique advantages that allows us to be one of the more fast-paced enterprise AI solutions. Similarly, our human evaluators were just exceptionally overwhelmed. Like, how do they look at – I think we were – like, they were working in Google Sheets.</p><p>I think we hired them even before we onboarded Notion. We have data specialists, and they were looking at this, like, dump in Google Sheets and trying to figure out how to parse the prompt with, like, a Google Sheet formula, right, and figure out what did the user actually say, how do I play with a few shots.</p><p>It's very involved. And I think anyone that utilizes human labelers well – and it's also been proven in research – is that, particularly when it comes to fine-tuning but also iteration, quality is much more important than quantity in terms of the insights and things that you extract. And so we definitely needed a scalable, efficient solution to support how we were looking at our data and also keeping track of user feedback, all of the thumbs-downs that we were getting in the development usage.</p><p>So this is kind of what our iteration cycle looks like. So let's say that we want to decide on an improvement. So, for instance, let's say that we wanted to launch a JIRA connector in our universal search product, right? What does it mean to query on JIRA? How do I have my AI even figure out what's a task, what's a sprint, how to query from the JIRA workspace?</p><p>We then curate targeted data sets from that workspace. You'll hear me keep mentioning these data specialists for much smaller enterprises. That might be your PM. I would highly encourage it to also be your engineer. It should be the people that are closest to the data. We're at a scale now where we have a specialty that's kind of like an LLM trainer that's a mix of a PM and a data analyst and a data annotator.</p><p>Those are our data specialists. But they are creating, just from logs and using a product and prototype, handcrafted data sets. Those can be 10 things, right? But I would actually, let's pause, make it 10 things first and make sure that it's formatted the way that you want. We have definitely done the bad thing of creating lots and lots and lots of dummy data, and it's not structured the way we want, and that's a real pain.</p><p>So first is like making sure everything is structured and your data flywheel is set up successfully to get insights that you might want. Then we tie them to scoring functions. Scoring functions should come after you've looked at the data for a long time. There are a lot of out-of-the-box scoring functions.</p><p>We don't use them frequently. We tend to use things that are specific to the product. I'll talk a bit about the LLM as a judge process that we use. That's probably what we rely on the most. But there are also a lot of things that are heuristic-based. So, for instance, if I have, if I want to make sure that my reasoning model triggers querying from Jira appropriately, everything in that data set, that tool call should have in its query must be Jira, right?</p><p>And so we can also make deterministic functions. Similarly, we have a data set of places where there are multiple languages happening, and we know what the output language should be. So, for instance, Toyota's a large customer of ours. Toyota might be working in Japanese and English. Someone might be asking questions in Japanese, but the output might be like, write me a paragraph about XYZ.</p><p>They're asking in Japanese, and, like, the lucky job of the product team is to figure out what language they actually want the response to be, right? It's a hard problem to have. We'll have a data set of those as a curated brain trust data set so we can, before we ship anything, run that eval and make sure that we don't break the multilingual language context switching experience, right?</p><p>So for certain experiences, we actually run the eval as kind of like an ad hoc CI experience. I think there are some companies that actually integrate in CI. We don't do that currently, but you could. That's more just to do with how long it takes to run the evals and how CI is built at Notion.</p><p>Everyone submitting code would run evals, and that would be a lot. We inspect the results, and then we keep working. The other thing I'll say is it's not just engineers that are on brain trust. Because of that, we often will have, like, our PMs in brain trust working very closely.</p><p>So, for instance, for that research mode product, we found that people were trying to draft reports much more than they were just trying to do research. And we found that by looking through all of that thumbs-down data that was propagated into brain trust from internal dev usage. That's something that we could escalate to our PM, but we also want the product thinkers and the designers to be thinking about that as, like, a first-class citizen.</p><p>You can think of this as your version of UXR, right, is to actually see what the model is doing well and how people end up wanting the model to do different things for them. And then in terms of that feedback by development, I want to talk a little bit about the LLM-as-a-judge system.</p><p>There's kind of two components that I've seen pretty prominently in industry. One is LLM-as-a-judge in which you have one prompt, which judges everything in your data set. So, for instance, is this information concise? Is this information faithful? Those are very common types of LLM-as-a-judge prompts. There's another version of it, which is a little bit more laborious from a creation standpoint, but I find to be far more insightful, which is for every single element in your data set or trace or whatever it is that you're evaluating, you have a particular prompt.</p><p>So, for instance, I might write a prompt that says, this answer should be in Japanese. The bullets should be formatted this way, and it should answer XYZ and point to page A, right? This is because, obviously, Levenstein distance isn't going to do a very good job of capturing the expected value to what we want, but we actually know exactly what we want the output to look like, and we don't want to have too conservative of a prompt that will always break.</p><p>For instance, just having a golden piece of data saying this is what the output should look like. We actually just say, like, what are the rules for what we want? This works really well with search evals, actually, because our index is always changing. When we want to reevaluate how search retrieval goes, we say the first result should be the most recent element in the list about the Q1 offsite, right?</p><p>And maybe since we redid that, maybe since we created that element in our data set, there's a new document about the Q1 offsite that wasn't what was in our golden set. This allows your golden set to be much more up-to-date and current. Yeah, and so I highly suggest that, and that's actually what our data specialists run, and what's really nice about it, as I mentioned earlier, our ability to switch to new models.</p><p>This can output a score that's far more reliable, and from that score, we can quickly understand if a new model has any serious regressions, and this allows us to, given our modular infrastructure of different prompts for different tasks and different models for different prompts, we can really quickly say, "Okay, Nano came out.</p><p>Nano was a fairly effective model that's very fast, maybe lower reasoning, and very cheap. What are kind of the high-frequency use cases where we don't need advanced reasoning, but we do want it to be able to run quickly? Let's gather those, and with one button, run all of the evals on them, see which prompts it does or doesn't do well on, and then quickly just change to those prompts, you know?" And that cycle can become very fast, and that allows us to stay at the top of the frontier in a way that I think has really been advantageous for us as a customer and, more importantly, for our users.</p><p>And the outcomes have been crazy. I don't think that we could exist without Braintrust or a similar type of software today. It is critical to our iteration flow, and it actually is our IP. So everything that we build, the IP comes from how we evaluate it and how we build it, and that comes from where we build on top of Braintrust.</p><p>Obviously, we have prompts that are like a series of strings, and obviously we have code that navigates through them, but how we decide if those work well is just as critical, and how we decide on model selection, and all of that lives inside of Braintrust. Similarly, our AI product quality has skyrocketed because now we have observability.</p><p>60% of Notion Enterprise users aren't speaking English. I will tell you, 100% of Notion AI engineers are speaking English. So how do we build something that works for a majority non-English speakers? We have to use rigorous evaluation metrics that understand that multi-lingual experience. I think that was the last slide I prepared.</p><p>I want to save some time to answer any questions that you guys might have. Yeah, I think there's a mic if you want to, or you can just scream it, and I'll repeat it. That might be faster. You had a question? So for the LLM judge, so what you're saying is actually you guys have multiple judges, and each responsible for a tiny thing to check?</p><p>Yeah, the question was for LLMs as a judge, she wanted to clarify that we have multiple judges, each responsible for a smaller thing. The answer is yes. Let's think about it in terms of scope of what they're evaluating and magnitude of number of samples that they're evaluating. Usually we have a variety.</p><p>Sometimes we have like a one-to-one mapping where for a single thumbs-down output, we just have a prompt that is shared with no one else. And that's what we engineer. We have some layers on top of brain trust that make that easier for our data specialists to write that work with the brain trust SDK.</p><p>But we have that, and then we also have LLMs as a judge that will operate on top of everything that's in that data set. You'll learn more in the workshop, the distinctions with like data sets themselves and logs, but data sets are hand-curated, and it can be just one aspect of your trace.</p><p>So in an AI interaction, you can have like five LLM calls that happen between the user and like the Notion AI response. We can extract just one of those and put it in a data set. Yeah. Yeah. Have you tried doing any like automated prompt optimization, or is it generally like you'll have to go manually through a person, go look at the results, and then update them?</p><p>The question was about automated prompt optimization. The answer is yes, we have played with it. I'm not sure that a majority of our problems are as solved by that as they could be in other workplace contexts. I've been to like dinners and events where I've heard massive success from it.</p><p>Maybe we haven't cracked it yet. A lot of people have, but we've played with it. One quick question. Yeah. What's your process of aligning the thumbs up, thumbs down, with the scoring function? Yeah, the question was about aligning thumbs up, thumbs down with scoring functions. They don't. I mean, so a majority of things in our data set were things that are either thumbs up or thumbs down.</p><p>Sometimes it's just legally how it works, because we have a process with some alpha users where that's the only way they give us permission to look at their data for evaluation purposes. We don't really rely on thumbs up for anything, except for maybe internal Notino's thumbs upping things is like good golden data for fine tuning, but we don't really rely.</p><p>There's like no consistency in what makes someone thumbs up something, so we don't look at that that closely. And for thumbs down data, it's more just that this is a functionality that we know we didn't do our best work on, but that thumbs down could have been given in September of 2023.</p><p>And so we don't perform how we did in September 2023, so it doesn't necessarily align with the LLM as a judge, because that's judging a particular experiment, not what the production user experience was at that time. And that's what makes this really powerful, because we don't just need to look at what our output was in September 2023, right?</p><p>Our data can be far more robust and last much longer, because really what we're getting from the thumbs down is the natural language request from the user. Everything else we can re-modify. We know the state of their workspace and what their request was. Everything else, you know, we, as our server code changes, changes the output of the LLM and the engine.</p><p>Any other questions? Yeah? For the LLM as a judge, is the scoring more like binary or is it on a scale? Yeah, the question was if LLM as a judge scoring as binary or a scale. I'll tell you like the honest answer in practice. I believe that we do it as a scale, and I don't think that scale is very well calibrated.</p><p>And I believe that we just grab everything that's less than a certain score and look at them as all equal. That's just like how we do it. I'm sure that there are technical ways that I've read about people that run it multiple times and calibrate their LLMs as a judge.</p><p>For us, we still have humans look through those outputs. We actually will take the failures and pass them into another LLM and ask them to summarize what the failures are so that we can write a quick report for the engineer working on it to see because we have thousands of samples.</p><p>So then we actually look at those deltas and pass it to another LLM to tell us what the biggest themes and difference were. Yeah, that's how we do it. I actually think there's been a lot of academic research on that calibration, but it has not been necessary for us from an investment perspective, so we haven't worked on it much.</p><p>Yeah, the question was about pairwise combinations versus just saying yes or no. It depends on the experiment that we're doing. Oftentimes, if we're experimenting between two methodologies, yes, or if the control is particularly important, like this is what's in prod, and I don't want to break prod, then we will use that kind of A-B setup.</p><p>If it's the development cycle that I talked about earlier, where we're still in dev or we're still just with alpha customers, and we don't know what the golden experience is, and we're far more comfortable breaking things, we're much less likely to use that type of setup. But Braintrust actually has a great UI for letting you do both.</p><p>Yeah. I'm curious if you have a process around figuring out what criteria to judge for you as a feature. Is there any best practices, or is it thinking? The question was about criteria to judge as a feature. I mean, I'm sure there are best practices. I think for us, the reminder, so like, you know, there's thousands of these, and then they live in this ether, and it's maintained by the people that wrote them, and then they take on a lot of power, right?</p><p>And very few people actually investigate the losses, and that's really risky for building a robust enterprise application. So things that we've learned, lessons that we've learned, because that, and I see some people nodding, you can imagine how that's a little bad sometimes. For instance, we used to use them just to catch specific regressions.</p><p>It's like, we have a special markdown format that works with Notion. We just used them for formatting at first, and we'd be like, has to, you know, be consistent with our markdown and news bullet points, and has to talk about this topic. All of a sudden, we weren't catching that it would, like, switch languages, because that's true in that prompt, right?</p><p>And so then, when you start relying on it to catch everything, or to be like a yes for something, then you have a problem where you're over-reliant on it, and you're not catching regressions. So there's kind of two approaches that I would suggest that make that successful. One, actually, goes back to your very intuitive question, which is having it for a particular task.</p><p>So we have a particular task of markdown formatting. We have a particular task of language following. We don't assume that they can do everything. Or you have a very small set, and commit yourself to looking at the losses. The problem is you don't commit to the losses, and you know that they're lossy.</p><p>I think that's, like, a trap that we certainly fell into when we were first developing this concept about nine months ago, before they were called LLMs as a judge. We just were LLM graders, right? I would say that's, like, kind of the two biggest pitfalls. Yeah. Do we have time for more, or it's up to you?</p><p>Yeah, we can do a couple more. Okay. Yeah. Yes. How do you isolate RAG versus generation? How do we isolate RAG versus generation is a very good question. Are you asking about, well, do you mean, like, in terms of how we evaluate it, how we evaluate changes? Yeah. So this has more to do with, like, what you're experimenting against.</p><p>So, for instance, there are times where the index itself changes when you're doing an evaluation, and things like recent docs, particularly for retrieval, freshness of the index or the index at time of evaluation are exceptionally important. And can definitely change the downstream results, which can be unobservable and confusing to the rater or to anyone looking at the evaluation.</p><p>So when is our next offsite? You know, maybe I did that before our Q3 offsite, and it's Q1 right now, right? So, like, the answer would change. One approach is to create a technology that freezes that index that's actually really expensive, because you don't know what elements of that index you want to freeze.</p><p>You don't know what the query is. It's a lot of storage. What we have found is that we freeze the retrieval and take the retrieved results and operate on, like, did this retrieval actually retrieve the right thing? We operate on that as its own evaluation that has its own evaluation framework that has much more to do with, like, our vector databases and Elasticsearch setups.</p><p>And then we have a retrieval that says, like, presume that this is what was retrieved, execute everything else, but, like, what was actually returned is frozen. And then the point in which the index, the point in which, like, the actual answer wasn't in that index and the eval can't be done, we just get rid of that sample.</p><p>So, we either assume that retrieval worked or just do the re-retrieval. We don't really merge them very much for that exact reason. Yeah. Just one follow-up. Yeah. So, do you isolate the retrieval at the time of creating the building dataset? Yeah. Do we isolate retrieval at the time of creating?</p><p>It kind of depends on what we're building and what we have, like, access to. The answer is, the majority of the time, we try really hard not to find what the index looks like at that period of time. That's very technically difficult. It brings up privacy questions. For instance, the permissions of that object could have changed.</p><p>We try not to do that. Yeah. But we also get enough feedback where it doesn't become as frequent of a problem, again, because we're lucky in that we use Notion so much that we get a lot of natural, particularly for retrieval and natural use cases. Let's do maybe one more, two more?</p><p>Yeah. One more. Let's do someone from the back. Yes. How many prompts do we have? I don't think this is private. Let me think about if it would be. We have over 100, like hundreds. How do we manage dependencies? I mean, some of them, well, we have our evals for them, and that helps a lot.</p><p>A lot of them are different variations of things. For instance, like there's some model-specific variations on our prompts. We have a large team. It hasn't really come up. I think the only time that managing a large number of prompts has been a problem is when a particular model provider has an outage, and we need to switch traffic over to other model providers.</p><p>It's not like all 4.0 mini traffic can just go to Sonnet 3.7 for a variety of reasons. Obviously, that works for a lot of it. It's like, okay, I'm willing to pay more for a small amount of time until OpenAI comes back up. But for some of them, like our database autofill, it's 12 times more expensive.</p><p>We'd have like millions and millions of dollars of debt for that decision in that period of time, and we can't make that. And so I think it comes more to like ownership over prompts and making sure that a fallback mechanism is up to date so that whoever's on call, they can just say like, you know, 4.0 is down.</p><p>And we can automatically, for the owner of that product or that prompt, they've already determined what to switch to instead in like a code-driven or config-based mechanism. I would say that's been the most laborious aspect is like getting everyone together and creating that system. I wish we did that from day one.</p><p>We did it more recently. And that was kind of like a tribal of elders. Everyone that's ever written a prompt, please come and fill out this config. But otherwise, it hasn't been a problem yet. Yeah. Great. Thank you. Thank you, everyone. Thank you, Sarah Sachs. Yeah, Notion AI is my lifeline.</p><p>It's great to search on there. It gives me all the answers I need. She did not endorse this. Okay, great. So we are the BrainTrust speakers. The two left here on the podium. And we'll be going over some lectures, then jumping into the platform. You'll do an activity, then we'll come back, cover a new topic, and so on.</p><p>So we're going to start with just covering why eval, what is an eval, what is an eval, excuse me. And then we'll talk about running the same thing via the code via the SDK, then we'll move into production, so day two, how are you logging, how are you looking at the data points that you're collecting in your production application.</p><p>And then finally covering human in the loop, so how do you incorporate user feedback or actual human annotators to improve the quality of your prompts, improve the quality of your data set. And if we have time at the end, we'll do some remote evals, stuff which extends what the UI is capable of in the playground specifically.</p><p>So I think we have a poll in the Slack that's currently pinned, if you're part of the Slack channel, feel free to add your response there. So jumping into evals and getting started. These are some of the curated tweets that we've seen notable people talk about online, so just something to think about why it may be so useful for them.</p><p>Well, it helps you answer questions. So, again, you know, is my change regressing the performance in production, am I using the best model for my use case, the cheapest model for my use case, does it have brand consistency in its responses, am I learning from the data that I'm capturing, from the logs, and am I able to debug and troubleshoot some of the responses that are underperforming?</p><p>This is a trend across the whole industry. Everybody's dealing with hallucinations and performance degradation, and so we're here to try to set up a system using statistical analysis to catch these mistakes proactively and also with online evals reactively on that online traffic. This can help your business by allowing you to move faster, helping you reduce costs, and scale teams.</p><p>It's really helpful to have non-technical people collaborate with technical people to build the best AI apps possible by using the playground for non-technical people and then having that SDK compatibility and everything centrally managed in one place. Moving into the core concepts of Brain Trust, so you can think of it in these three slivers, starting off with prompt engineering, right, you want to keep track of the versions of the prompts that you're iterating on, you want to do this rapidly, so having a playground or a place where you can quickly go through these changes and identify improvements or regressions is really crucial.</p><p>Then you want to have evals that are automated ideally, right, and this can be kicked off via the SDK, you'll have a score that is from zero to one as a percentage that will give you a signal of, is this going up, is this going down, what needs attention, what doesn't.</p><p>And then finally, observability, what's happening in production, are users thumbs upping or thumbs down, are they providing ideal output that I can then use in my data set to keep improving the evals or keep improving my AI feature. So jumping into, you know, what, what even is an eval, well the definition that we come up with is that it's a structured test that checks how well your AI system is performing, it helps you measure quality, reliability and correctness across scenarios, again, the criteria is really up to you, as we heard Sarah say, the LLM as a judge can be across whatever dimension you want to test.</p><p>There are three components that go into an eval, so you have your task which is the thing that you want to test, this is the code or prompt that you're evaluating, it can be a simple, you know, one-time call to an LLM or a full agentic workflow, the complexity is really up to you, we'll see some simpler versions in the UI and then in the SDK, it's really, you know, the complexity is up to you, you can, you can go crazy there.</p><p>The next piece is the data set, so this is the set of real world examples, the test cases that you're going to be throwing at the prompt and seeing how it's performing based on the score output, right, so the score is the logic behind the eval, this is outputting from 0 to 100 a score, you can use LLM as a judge or full heuristic functions, so it's great to use a combination of the two and try to meet in the middle.</p><p>There are two mental models to think through, offline evals, which are evals in development, as you're iterating on the prompt, figuring out what works best or what model to go with, you're writing these structured tests and running them through these predefined data sets, right, you're not using live traffic in this scenario, but on the online eval side, so this is real time tracing, you're monitoring that live production application, and you're also getting scores from those real outputs, and that will allow you to diagnose problems, monitor performance, and capture that user feedback so you can keep improving and close that feedback loop.</p><p>So we'll show how you can use both of these types of evals offline and online. This matrix here is really helpful to understand what should I even be improving, you know, and you have to look at two things, right, the score that's being output by your evals, and using your own eyes, looking at the output of the LLM, do I think this is a good or bad output?</p><p>If both match, you look at the prompt, it's good, high score, great, that's where you want to be. But if not, you have to decide, okay, do I want to improve my evals, the actual scores, or do I want to improve my AI app? And this is not very trivial, there does require some thinking of what requires my focus, improving the eval, improving the actual app.</p><p>So now zooming into each of these components. So the task, this is the thing that you want to test. So starting off with a prompt, right, this is a screenshot from the UI. It's the input that you're going to give to the LLM, right? You can set up a system prompt in Braintrust, and then pass user prompts as mustache templating, and that's something that we'll see in the activity shortly.</p><p>So, you know, this is great for getting started. You may have some multi-turn use cases where you're having a whole conversation and you want to evaluate that with a tool like Braintrust. So that's where extra messages comes in, and you can provide the system prompt, followed by the user message, the assistant response, the user response, the assistant response, maybe a tool call thrown in there, right?</p><p>All of that prepackaged and given at once to your eval system that will then output a score. Tools are also used with Braintrust, and they should be for, you know, RAG applications or for agentic applications. So this can live in Braintrust. You can push your tools to the Braintrust library, and they'll become accessible to anybody playing in the playground or running experiments.</p><p>And the last piece here in the task are agents in the UI. So now you can have prompts chained together where the output of the first prompt becomes the input of the next. So this is going to keep increasing in scope within Braintrust. We'll have branching and more capabilities.</p><p>This is currently a beta feature, but really exciting to be able to do end-to-end evals on prompts being chained, as many as you'd like. So now moving into datasets. So there are three fields, right, three columns in a dataset, one of which is required, the input field. So this is what's going to be passed into the prompt.</p><p>This is the user input into your AI feature application, right? The other two columns are optional. You can have your expected column, which would be that anticipated output or that ideal response. This is difficult to create initially, and we see a lot of customers leave this blank at first and then start to fill it in with human annotation or user feedback.</p><p>If you're doing a rag bot, it's common to have assertions. So what do you want to be included in the output? Providing that in the expected column will help make sure that it's using that as the ground truth. And then metadata is for any additional information that you want to track at that specific dataset row, so with the specific user prompt.</p><p>So some tips here. We recommend for you to just get started. You don't need to create 200 rows in a dataset to run your first eval. You know, five, ten rows is great. Just start small and get some feedback. And, you know, the next thing is don't stop iterating.</p><p>Keep adding rows or tweaking rows, using logs as that source of truth, right? How are your users interacting with the feature that you're developing? Of course, at first, you may not be capturing logs, but there's still that process, whether it's via synthetic data or with internal testing, that you can help yourself improve the dataset.</p><p>So, human reviews is another way of establishing ground truth. Very much needed in certain industries. If you're dealing in the medical space, you need doctors to look at the output. Or you need lawyers or people with highly specialized skills. Cool. So now moving into the score types. So this is the last piece.</p><p>So we covered tasks, we covered datasets, and now scores. So there's two types, right? We covered the LLM as a judge a little bit. So this is subjective, non-deterministic, more of a qualitative assessment that will be done on the output. And then on the other side, there's a full code base heuristic score.</p><p>This is very exact, deterministic, objective. So you want to try to use a combination of the two. And like I said, meet in the middle, right? There are some organizations that will choose to go one way or the other, but we found that most will typically incorporate both as much as they can.</p><p>I know Sarah mentioned that LLM as a judge has been crucial for her. So some tips here. So we recommend for you to use a higher model in the LLM as a judge to evaluate the smaller ones. Just something that we've noticed could be useful. You want to make sure that the judge is scoped to a specific criteria.</p><p>We want it to be focused and not be a broad decision that it has to make across five or six different criteria. You want to make sure that you're evaluating the score. This is probably the most crucial piece. But you want to put the score prompt, the LLM as a judge prompt, through the same rigor that your other prompts are receiving.</p><p>Right? So you want to make sure that you're testing it on human annotated feedback. So what would a human think looking at the same outputs? Is it matching the LLM as a judge? If not, probably needs some improvement. This is a little tidbit about the Braintrust UI. So we have two views, playgrounds and experiments.</p><p>They look very similar. This wasn't always the case historically, but they've now grown to be very similar because of customer requests. The playground, a way to think of it is a quick iteration. It's more ephemeral, right? So what you do in the playground won't necessarily stick around and be part of the historical view.</p><p>But you can always save a snapshot of it to the experiments view and sort of bring it over. And that will then kick off the more traditional experiment, which is the same thing that happens when you run an experiment via the SDK, right? So you run an eval via the SDK or from your CI pipeline.</p><p>That will end up in the experiments view. And the benefit of that is that you have everything in one place that you can review and compare across weeks and months. So you can compare your performance with the new model that dropped today with a prompt from three weeks ago in the experiments view and see the scores change over time.</p><p>Cool. So that was probably a lot. But now you understand the components and ingredients behind running an eval. So now we're going to jump into the activity. There should be a document pinned in the Slack channel. If not, we can also pull up the QR code quickly. But if you can pull that up and try to follow along.</p><p>I know the Wi-Fi has been a little spotty. If you can't follow along, no worries at all. Doug here will be leading through those steps. All right. Let's have a little fun here actually getting our hands dirty with the Braintrust platform. As Carlos mentioned, we have -- sorry, wrong button -- the document guide out here that will walk you through the setup of the workshop.</p><p>Again, I'll walk through it. It'll give you sort of enumerate the different requirements that you'll have to have at least for how it's set up today. Just as like a call out here, we're using OpenAI here under the hood. That certainly doesn't mean that Braintrust only works with OpenAI.</p><p>You can use any sort of AI provider, custom AI provider. You can sort of bring your own here as well. But just wanted to call out we're going to use OpenAI here to do this. Another thing to call out -- somebody ran into this in our last workshop -- is the particular version of Node that you're using.</p><p>Just don't be on version 23. 2220 are generally pretty good ones to use. And then let's get going. So you can start here at the top by installing Node, Git, and TypeScript. I've already done this here on my machine, but you can get a sense for what this looks like.</p><p>Obviously, we need a Braintrust organization and a project here to actually go and play with that playground, create some evals, run some experiments. So if you go to www.braintrust.dev, this is where you can create your organization and your project. We're calling our project "Unreleased AI" today. You can certainly call it something different.</p><p>You would just see multiple projects be created inside of your account, and I'll show you why in just a second. But this is where all of this activity is going to be happening underneath. You can almost think of it as a project as like a particular feature, right? So you have feature A, B, C, and they may each have their own unique projects.</p><p>I speak very high-level about the use case here, unreleased AI. What we're trying to do is build sort of an application that looks at a GitHub URL and looks at the commits that have happened since the most recent release. And the idea is to inform us as developers what's coming in maybe a subsequent release.</p><p>Once we've created our account, I'll come out to the Braintrust platform here. So I'm going to kind of follow along with all y'all. I'll create a project, right, unreleased AI. Then you're going to need to configure your AI provider, right? And so this will be the OpenAI API key that you'll configure.</p><p>Again, here are the different AI providers that you can configure within the platform. You also can use default cloud providers, and then, like I mentioned, bring in your own custom providers to the platform as well. Configure that, and then we'll come back to the guide. This is where the repo exists, right?</p><p>It's a public repo in our Braintrust org. You just clone this locally. Let's zoom in a little bit. Obviously, I have it here locally. The next thing that we'll do is we're going to copy this .env.local.example file to a .env.local. You're going to replace the Braintrust API key and OpenAI API key with your specific keys.</p><p>Optionally, you can include this GitHub token. This will allow you to not hit sort of like rate limits on the GitHub side. Not super important for this. We're probably not going to hit that here with just running a couple examples. But that's essentially what you do, right? We're just going to run that cp command, and you have that there.</p><p>Fill in your API keys. Okay. Let's do this now. So the first thing that I'll do is run pnpm install. All right, this will install all of the dependencies for this application. The other thing to call out here is it's going to run this command after installation. So this is going to actually create some of those resources that Carlos was just talking about.</p><p>It's going to create a couple different prompts. It's going to create some scores and actually create a data set within that project that we just created. So this is, I think, just to highlight here, one of the really unique things about Braintrust is being able to connect the things that we're doing within our code to what's happening within the platform itself.</p><p>A couple different ways to actually utilize or use the Braintrust platform, whether you are kind of maybe strictly code-based using our SDKs or you are using a lot more of the platform itself, you can do a lot of these things and share code and so on. But just to kind of scroll through here, this is how I'm creating my prompt.</p><p>If you scroll down a little bit further, here's another prompt. But when I run this, this is going to actually go and push all of these resources into that project that we just created. OK, cool. So we have our prompts. We have our data sets. We even have some scores here down below.</p><p>So let's just kind of investigate some of these different things that we created. So here's my prompt. As Carlos mentioned, we have that sort of mustache syntax here that allows us to inject certain things based on whatever our data set looks like. Maybe to back up, thinking a little bit about what Sarah said in her talk, thinking more about that data structure that you want.</p><p>And then this is how we will then create that underlying data set. But this is really going to map to the data set that we've created within that repo. Right? So again, what we're trying to do is we're trying to get that list of commits, summarize them, and give sort of that summary to that developer of what's coming in that next release.</p><p>The other thing to highlight here are our scores. And luckily, I was able to follow most of Sarah's best practices, being very targeted in what we're doing with these individual scores. To highlight a couple, I'll just open up my completeness score. So this is really just being able to understand, did the LLM do a good enough job?</p><p>When it looked at the commits, is the summary complete? Did it summarize all of the breaking changes? Did it summarize all of the net new features? All right, so this is where we can actually start to create that criteria that meets the expectations that we have of what is excellent, what is good, and so on.</p><p>And then this then maps to individual scores between zero and one. The other one here I'll highlight here is this formatting score. All right, this is actually just more of that heuristic, just being able to understand, is it following a particular format that we've laid out? And this is a little bit more binary, just a zero or one, but just to give you a sense of the different scores that we can add to the project.</p><p>Last one I'll highlight is just the dataset. If you look through here, and maybe I'll zoom in a little bit further, you should start to see some of these things actually map what we have within that prompt. And so this is how we're going to be able to pull those things in and actually run our evals.</p><p>All right, come back here, make sure I haven't skipped any steps. So now that we have a sense for what we just put within the platform, let's actually start playing around with it. So we're going to go into our eval playground. So this is where we can actually create evals on the fly.</p><p>I'll create an initial playground here. Feel free to name it something better than that. And here's what we can do. So let's load in our two prompts that we've created. So we have our change log one and our change log two. Thinking back to what Carlos was talking about, like the different sort of components of an eval, right?</p><p>We need our task, right? In this case, it's our prompts. We need a dataset. And then we need an ability to score it. And so we can actually include all of these different scores here. And now we have all of the different components to run an eval here within the platform.</p><p>So I'll click Run. And then you'll actually start to see all of these different rows within this dataset actually run in parallel. And then we're going to generate those scores against the output that the LLM created for those individual prompts. And we should start to see some of these scores come back pretty soon.</p><p>Lots of different ways to actually go in and, you know, you can highlight each one of these different things. You're able to actually, once this starts to come back, look at the rationale that the LLM gave to providing the score that it gave. But this allows you to, like, you know, you can even look at a diff.</p><p>Again, once some of this stuff comes back. Or we can maybe get more of a summary type layout. And I can look at, like, okay, so my accuracy score for my base prompt up here or my base task, how does it compare relative to my comparison task? So very, very quickly, I got some scores here that give me a sense for how well these particular prompts fare against the scores that I've defined against them.</p><p>Maybe another really quick thing to highlight here is up here at the top, right? We were talking a little bit about, like, maybe understanding over time how are these things faring as I start to change these prompts. That's where experiments come into play. So I can create an experiment, again, very quickly.</p><p>It's going to use what I've configured within the playground. This will actually create, again, back up a little bit here to experiments. But now I can start to track over time these scores. I can change the model of these. There's a lot of different ways to start to view this data.</p><p>Maybe I want to understand what does cost look like for these particular tasks over time. I'm able to look at that. If I change the underlying model, I'm able to group it by that model and see how those models affect that score. But this becomes a very trivial thing now, right?</p><p>Going back to that playground, maybe I want to test here with GPT-4-1. How does this impact my scores? And then how does it impact my scores relative to the cost maybe that I'm incurring to do that? So run that experiment. Again, now I can track these things over time.</p><p>Maybe really quickly as sort of a bonus to this, going a little bit off-grid here, just because I've actually heard this numerous times at this conference. And I just had a question over here. How can we automatically optimize prompts? This is something that we are thinking about internally. And hopefully within a couple weeks or so, we can release this feature to our customers.</p><p>But this loop here allows you to either optimize prompts or generate net new dataset rows. And then the kind of unique thing here is that it has all of the context of this playground. What are the evaluation results the last time it ran? How can we change this prompt to ensure we beat those results?</p><p>And so you can see it's -- I'm guessing most of you all are familiar with something like cursor. So sort of a similar type of interface here. But it will actually, again, use those results, start to change the prompt. Pretty soon it will give me like a diff here.</p><p>Here is maybe a better prompt that we can accept. And then it will, again, it will run those scores for that new task. And it will give us a sense for whether or not we are better or worse because of that. But now this becomes a little bit easier to do that iteration when we have sort of AI layered here in the mix.</p><p>And another question that we got in the previous workshop is are we using brain trust to actually evaluate this? And we certainly are. Yeah, this is really cool. This can also improve the data set, right? You could ask it to enhance the data set, add another row, maybe even highlight some of the data set rows and be like can you improve the prompt based on these specific scores?</p><p>Yep, absolutely. Yeah. Pretty cool. Any questions that you guys may have? Yeah, we can open it up. Yeah. How do we add, let's say if we have a home grown model, how do we add it in the list? Yeah, that's a good question. There's some custom AI models being used in brain trust.</p><p>In the settings here, in the AI providers, you can add your own custom model. So if it speaks open AI, you should be able to add it in here or choose whichever one you want. If you have it in inference. Yeah? What if your score does not have like upper bound that you can convert into percentages?</p><p>So how do you calculate score then? So what kind of score would you be thinking about here? So for example, I'm building finance ticker, right? Offering like buy some kind of stocks. And I use score as how much money I made. So that could range, you know, from $10 to like $10,000, right?</p><p>How can I convert this like percentage score? Is that value relative to something that's expected though? Like you're talking about like an unbounded range. That's not necessarily what I think these scores are designed to do. Right, but I can engineer my prompt and try to maximize some kind of score that I can get, right?</p><p>But that does not mean that I have a range in which I try to operate. I'm just trying to optimize a function, some generic that I'm, you know, define my score. Yeah, it's definitely something we've heard before. I think that's part of the struggle of writing scores is trying to normalize them and bring them into that range of zero to one.</p><p>So up to you of what you decide the floor and ceiling are. Yeah. Do you have anything for evaluating multi-turn conversations? And as part of that, would you be able to review the agent's features? I think you kind of went through that earlier, right? Yeah, in the playground we can just-- so here at the bottom you can add more messages.</p><p>So the idea is that you could provide a whole back and forth in context and then evaluate that multi-turn conversation at once. You could do it as well in the SDK. And then for the agents here, the idea is that you can chain multiple prompts together. So here we could just grab the two.</p><p>But the idea is that the output of the initial prompt will become the input of the next and so on. And then you can evaluate them as a unit, right? And as opposed to the multi-turn, right, you're providing all the context at once. Whereas here it's, you know, making multiple LLM calls.</p><p>And just piggybacking on this question, for a multi-turn scenario, do we score each turn versus the entire conversation? Also, how about if the conversation has, let's say, 100 messages in between the agent and the user? What about the memory? Do we contain the context for the message one versus the message 90th message?</p><p>How does this all play out? Do we score the entire conversation versus each turn? With the multi-turn extra messages approach, you're providing everything at once to the LLM. So it's one LLM call. So yeah, you would need to comply with the context window of the model that you're working with.</p><p>The agent feature, though, each prompt is its own LLM call. I think maybe just speaking also more generally, if I come back over here. I just wanted to highlight something because I think it's relevant to the question. Do you evaluate sort of like the individual turns? To me, like, it's always like somewhat dependent.</p><p>But if I look at these logs here, this is sort of like this application where there's multiple steps that have to be taken for the user to get the output that it needs. So you can see here that the first step is actually rephrasing a question, right? So there's actually this chat history that is needed and then the most recent input.</p><p>And we need to be able to rephrase that question into something that the user actually means. And if the rephrase here, the task that we have for this, falls down, it means the rest of the application is going to fall down. So I think one of the things that becomes really powerful here is the ability to actually score these individual calls as they're happening.</p><p>So I have a rephrased question. From there, I'm going to determine the intent. But if I scroll down a little bit further, I can actually create scores for those individual spans. So I can understand the, like, the application has all of these different steps. I want to make sure that, like, if something does not work, if the output doesn't match what I would expect, I need to be able to go back through there and figure out where it fell down.</p><p>I don't know if that helps at all, but-- Almost like a session trace there, right? You're going back to the session and seeing what happened and whether that fell through the tracks. Yeah, yeah. And then you're able to configure scores for these individual spans that are created. Does BrainQ support evaluating speech-to-speech models for real-time or Gemini multimodal evaluations of voice, et cetera?</p><p>Yeah, that's a good question. We have a cookbook about evaluating a voice agent that you should check out. It would be through the SDK. Yeah. Yeah. Any questions? Cool. Are you guys able to follow along in the activity? Like, the internet's working, everything's okay? Sort of, kinda. Sort of.</p><p>Yeah. Do you guys offer brain trust for on-prem installation? We have a hybrid deployment. So we would manage the control plane. You would manage the data plane. Everything meets in the browser. Similar to the Databricks model. Similar to the Databricks model. So yes. Our response is, you would have the data fully in your VPC.</p><p>Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Do you guys offer brain trust for on-prem installation? We have a hybrid deployment. So we would manage the control plane. You would manage the data plane. Everything meets in the browser. Similar to the Databricks model. So, yes. My response is, you would have the data fully in your VPC.</p><p>Yes. Question over here. There's a lot of similarities with Langsmith. Yeah. That's a good question. So the question is, how do we compare it to Langsmith since we're both in the eval space? And, you know, I think you're right in that we do very similar things. And maybe if you're looking from a distance, it looks like it's the same.</p><p>But up close, what we hear is that our UI/UX is more intuitive, cleaner, easier to work with. And crucially, the performance and scale of brain trust is unmatched due to our underlying infrastructure. BrainStore was actually developed in-house. And it was only possible because of the technical team, especially Anker and the founding engineers that have deep database knowledge from working at single store MemSQL and having built many databases before.</p><p>So they were able to increase the full text search capabilities and, yeah. Cool. So, finished with activity one. Again, this will be available for you guys to do at your own pace at home whenever you want. There's also a free tier available for brain trust. So, you have still a bunch of credits to use.</p><p>So, moving into the second lecture. So, now doing the same thing that we did via the SDK. So, we got to see a little bit of the code, right? We cloned the repo. We pushed the prompts, the scores into the brain trust so we could use them in the playground.</p><p>But here, we'll actually do an experiment via code. So, just wanted to quickly go over high level how this works. So, you know, the top row we went through. You didn't really get to see the assets defined in code. But we talked through the brain trust push and then how they ended up in the brain trust library.</p><p>So, the second row is what we'll go over now where you're defining the evals in the code. You're running a brain trust eval command. This can occur via the CLI or in a CI pipeline. That's very common, right? So, when you're trying to open a PR and merge it into main, you would trigger this eval process.</p><p>And then all the experiments would appear in brain trust. And, you know, Sarah was mentioning that they have their own process that looks at those experiment scores. Summarizes them with an LLM and then writes a report. So, you can get creative with this pipeline. So, pushing the brain trust, very similar, right?</p><p>The key here is defining the assets as code. So, you would just follow the pattern to the right here. Projects.prompts.create or it would be project.scores.create. And then you would just define what would appear in brain trust. And this could be used in production. You know, a lot of people will either push their prompts into brain trust or pull them.</p><p>Really, the prompt management style is up to you. But it is really helpful to have that source controlled prompt versioning in place. Depends if you want to go push or pull. So, running an eval via SDK is very similar to what we saw in the UI. You need the three things again, right?</p><p>So, you need a task, a data set, and one or more scores. So, we'll show you what that looks like in the actual code. And then we'll run the command and we'll see what that looks like in the UI. And then we'll come back and do some of the logging day two, moving into production stuff.</p><p>Cool. Back to the guide here. If you're looking on activity two. We'll start again with just kind of understanding what we have within the code base. And then we'll actually run some evals. You'll again, like Carlos mentioned, you'll see some similarities with what we did within the platform. But now we're just kind of strictly within the code.</p><p>Again, just trying to like hammer home here. There's like multiple ways to actually interact with the Braintrust platform. Trying to meet you all where you want to be. But if I come over to the code base. Again, this is where the resources.ts. This is where all of this will be defined.</p><p>All of our prompts, scores, and such. I have this eval folder. I'm going to open up my changelog.eval.ts. One thing to kind of highlight here. If you name your files with that .eval.ts, and then you pass just a folder. We can pick up all of those evals and run those within the platform.</p><p>But let me open this up a little bit. So this is really all we're doing. Right? So we have this eval, which was just coming from the Braintrust SDK. Here's our data set that we've created. The task that, you know, that lives over here from Braintrust. And this is really like, again, all we're doing from the repo side to run evals within the platform.</p><p>So I'll clear this and then run pnpm eval. This is, again, just another command that is unique to this repo. All we're doing under the hood is running this Braintrust eval and then giving it the name of this file. But you can actually see the experiments running right now within the platform.</p><p>Here is the link that you can go out to. And this should look very similar to what you just saw when we ran the experiments within the UI specifically. But now, again, this becomes part of the history of our experiments for this particular feature and can track this over time.</p><p>But now we are kind of strictly within the code while we are running this. Maybe just another thing to kind of highlight, I think I did last time, but I didn't really pull anything in here, is different ways in which we can start to compare this. So maybe we're really worried about sort of like the duration or we are worried about sort of the number of completion tokens.</p><p>There are different ways in which we can start to view the data here. Again, I didn't change -- actually, I did change the model, right? This is another way in which we can start to understand the scores for the particular model. But again, like, this all just happened via code.</p><p>Some simple commands that we just wrote on our command line and pushed that into the Braintrust platform. Cool. Any questions about running evals with the SDK and that eval class that we showed and the Braintrust eval command? Yeah? I just wanted to inform you, like, there is image, like, like, vision models and stuff.</p><p>Does that work with the UI? Or is that only SDK? Yeah, that should work with the UI images, yeah. Can I do non-LLM, like, models? Like, if I had, like, a -- like, a grounding model as part of my pipeline? Yeah, that's a good question. I'm not sure if it will let you add it as a custom model.</p><p>Yeah, I can't -- I would say go try it and see. And if there's any issues, we have a Discord channel and, you know, we can add it to see if more people want that feature. Yeah. Thank you. Yes? Python-based SDK? Yeah, we have Python and TypeScript-based SDK. And then we have some additional ones in other languages, like Java or Kotlin, that -- Ruby, that are created with stainless.</p><p>So they're just wrappers of the API. We're creating a Go SDK as well. Great. Any other questions about Activity 2? Ready to move into production and we can talk about logging? All right, let's do it. So why should you even set up logging? Well, you know, there's a few reasons here.</p><p>The main reason is to measure the quality of your live traffic. So this will allow you to set up alerts, if you'd like, of when it starts to dip, right? If you're putting out less than your best work and users are noticing, maybe they're providing feedback or, you know, maybe they -- you know, the scores that you have on those specific logs are just dipping, you can notify the right teams.</p><p>You can debug, troubleshoot a lot faster by having that visibility into all of the functions that are running, all of the LLM calls going back and forth. And crucially, you can close the feedback loop. So you can capture the feedback. And, you know, we hear a lot of customers that are doing that, but they're not implementing it into their iteration process.</p><p>They're not changing the prompts or adding it to data sets and closing that loop. So this can really help you do that. So what does logging into Braintrust look like? We have a very easy path, which is wrapping your LLM client. So if you're using OpenAI, you can just use the wrap OpenAI function around it.</p><p>If you're using the Vercel AI SDK, we have a very similar thing. So this is essentially like one line of code, and then now everything is getting tracked, all your metrics, right, the amount of tokens, your latency, all the cost, all of that is getting populated in Braintrust. You can also trace arbitrary functions.</p><p>We have the same sort of wrapping approach where you use a trace decorator or a wrap trace, and it will track everything occurring within that function. If you want to be more specific or more granular or have more flexibility, you can also do that. We integrate with Otel, and we have a span.log support, which can allow you to add any additional information metadata that you want to track and add it to that interaction.</p><p>And initializing the logger, right, that's important because it tracks which project you want this to be synced to, and it authenticates you into Braintrust. So that's part of the process, and in the code repo that you clone, you'll see that if you go to the generate/root.ts, you'll be able to see all of these steps in the code and understand what's needed to get it into Braintrust.</p><p>So now that you have the logging and real user traffic entering Braintrust, you want to have that assurance of quality, or you want to have that visibility into quality of responses. And a great way of doing that is through online scoring. So this is going to use the scores from your playground or from your experiments, right, from your offline evals, and bringing them into that production environment, into that live traffic, and giving you scores that you can then use to filter down the logs, grab the, you know, underperforming use cases, and add them to a dataset, keep iterating, keep improving.</p><p>You can also do some A/B testing, right, you tag certain logs in different ways, and you can compare the online scores for each one of those. So we'll show you in the activity how you can set up online scoring rules, and which spans you can enforce them to be on, and the sample size.</p><p>So you could choose to online score 100% of the logs, or 1%, it's up to you. We do recommend for you to start with a low sampling rate, and then work your way up as you trust the metrics more and more. So this is a bit of the process, and then Doug here will show you in the platform how you can go about creating the rule.</p><p>So once you have online scoring set up, it's really helpful to have one-click lenses of what matters to you or to your team. So you can think, you know, if you've used Notion, you've probably realized that they have views, right? You can customize the filter, save it to a view.</p><p>Same idea in Braintrust. You can filter, sort, do whatever you want, save it to the view. It'll retain those settings, and then your team can just use that. So it's a great way of moving quickly, defining the filters that matter to you. Great, so now back to the platform, back to the repo, and go over logging and online scoring.</p><p>Awesome. Let's configure some online scoring. All right. Before we do that, obviously we need to, like, get some logs into our Braintrust project. To do that, we'll come back to our application, and we're going to spin this up. So I'm going to run pnpm dev, and this is going to run this localhost 3000.</p><p>So again, the use case that we're building for is that change log generator. The idea here is you provided that GitHub URL. We're going to give you, you know, the LLM will give you the summary of those commits, and then maybe categorize them into different categories. So I'll click that.</p><p>This should go through the process. Actually, while it does that, let me come back here just to kind of highlight where this is all happening. So just kind of connecting what Carlos was talking about, like how we can start to instrument our application. There's a few different things here, right?</p><p>We have our Braintrust SDK for TypeScript, obviously, and we're going to wrap the AI SDK model. So this is really going to give us a lot of goodness out of the box, being able to understand, like, the metrics, right? What are the completion tokens? What is the cost? All of that stuff happens simply by just wrapping the LLM client with that.</p><p>That's, again, like really the easy way to do it. Then there's being able to, like, you know, instrument your application with a little bit more granularity. And so maybe you want to use this trace function from the logger to actually be able to specify what that input is, what that output should be, and then add some metadata.</p><p>The reason you might want to do something like this is now this maps to the data structure, right? That becomes really powerful to set up those scores against, right? So we start with something that's very basic where we get something very easy out of the box, and then we can go down to the actual, like, you know, the individual tool calls, if you will, and then actually create the inputs and outputs for that so that we can then create scores on top of those.</p><p>Back to the application here, right? Here's the, you know, essentially the response from the large language model. Coming back into Braintrust, we should now see a log. And then this is essentially what just happened, right? Because of what we configured with that WRAP-AI SDK, as well as the different logs that we had set up, right?</p><p>So there's this top level, think of this as a trace, and all of these sort of under the hood or underneath it are the individual spans that we actually want to understand. This allows us the visibility into the multiple steps that the application will take, and allows us, like I showed you in that previous example, to create scores for these individual spans as well.</p><p>But you can see here, right, the top level generate request. Here is my input, right? What are my commits for this repo? Here's the URL, and then here is the output. But then being able to drill down into these individual things, right? So what are my commits? What are my, what is my latest, excuse me, release?</p><p>And then again, being able to drill down into these becomes really beneficial as we start to need to understand how our application is performing. So from there, right, just going to come back here, we now want to maybe set up some online scoring. So we walked through both within the platform running experiments, walked through the SDK running experiments as well.</p><p>This is pre-production, right? This is, now when we have our logs running in production, we want to understand how our application is performing, relevant, or excuse me, relative to the scores that we've already configured. So I'm going to come back into Braintrust, click on the settings, and then the, where am I?</p><p>Excuse me, just configuration over here on the left, and then online scoring. So this is where we can now create the rule. Let's say, this is, come on. This will allow you to select the different scores that you've configured within your project. Now, I could select all of these, or maybe I have, like in that example that I showed to you previously, I really just want to apply this to a particular span.</p><p>All right, and this allows you to get very granular. Right now, we'll just configure all of these for the overall trace, and we'll do it for 100% of the samples. Okay, so now when we come back to our application, and when we generate this, we'll have the same sort of workflow, right?</p><p>We're going to generate this, the logs will look the exact same as they did previously, but now we are going to score this. We're going to understand the things that are happening within production. How does this output fare to the scores that we've configured for this particular task? This allows us to understand, we could even create automation rules here, so maybe a score drops below 50%, whatever it is.</p><p>We can now create sort of an automation that allows us to understand when something like that happens. So you could see here, we have our scores ran. Again, you can drill down into these things, and you can see, you know, what is the rationale for that particular score in that instance.</p><p>Maybe the other thing here, and then because we had some good outputs, but this is where, like, we would want to think about, how do we give users the ability to traverse through these logs in a really scoped down way? This is where what Carlos was talking about with views.</p><p>And so maybe it's being able to understand, if I open this up, and I create a filter on my completeness score, and I'm just going to modify this slightly, I'll say less than 50%, this now can become a view that I save to this project. And now it becomes really easy for somebody to come through and look through these logs, and they can start to understand the inputs and the outputs, and if it makes sense, we can add these spans to the data set.</p><p>That's another thing that I think we think about, like, the sort of feedback loop that we need here. We were kind of like in pre-prod, right? We were developing evals in the UI or in our code base. We pushed it into production. How do we create that flywheel effect from those two different places?</p><p>This is part of it, right? Being able to enable sort of like the humans looking at these things, and then being able to add those really unique or relevant spans to the existing data sets, or even creating net new data sets from them. Cool. That's activity three. Questions? Yes.</p><p>Can you aggregate those scores? If you go to the settings, aggregate scores here, so you can do that. So like, yeah, you could create a true north, right? And just based on the aggregate score, decide if it's improving or regressing. So the SDK is meant to be pushed to production then to get all those data amounts?</p><p>If you want to do online scoring, you'll need to push scores into Braintrust, right? So that they can then be referenced, because online scoring, if you're using the SAS model, right, we're running that compute. We're, you know, we're grading the logs. So yeah, I think one you would want to push to Braintrust is for online scoring.</p><p>If you have teammates that are using the playground, and they want access to the prompts or the tools that you've made in your SDK, that's when it makes sense. But you don't need to, right? Like you could still do everything with code and get value from Braintrust. Maybe just to highlight really quick, we also have just sort of scores out of the box that you can configure.</p><p>So if I go to the playground, like if you don't want to push up certain things. Now, there's not as much flexibility in this as there are with your custom scores. But there is an auto evals package that is developed internally by us that you can use to add to these tasks as well.</p><p>And then you can use an online scoring also. Any other questions about logging or online scoring? Okay, cool. Now we're going to move into the human section, human in the loop. My favorite part. I'm needed. All right, let's get, let's get into it. Let's get into it. So yeah, it's honestly really helpful for certain companies to implement human in the loop, right?</p><p>Not all AI mistakes are created equal. Depending on the industry that you're in, like healthcare, finance, or legal tech, a single failure can have, you know, huge, serious consequences. So that's where you may want to have and invest more resources and putting humans in front of those AI responses and making sure that they're up to par.</p><p>This can be really helpful for catching hallucinations, especially in specific fields that require specific knowledge, establishing ground truth. So if you're going to start filling out that expected column in the data set of ideal outputs or, you know, ideal responses, it may be helpful to have a human help create that.</p><p>And, you know, you also want to capture user feedback and make sure that that's being incorporated in your feedback level. And being used in, in the prompted duration. So this is great for quality, reliability, providing that ground truth and aligning with what human, humans need in specific areas where the, the LLM just doesn't have as much visibility.</p><p>There are two types of human in the loop that we're going to cover. Human review. So this is an annotator and a subject matter expert that's going to come into brain trust. They're going to manually label a specific data set or a specific log, right? They'll score it or they'll audit outputs.</p><p>And this is really useful for ground truth to the data set. They can audit certain edge cases and they can also help eval the LLM as a judge. So based on what that human annotator determined was a good response, you could train the LLM as a judge to also label it the same.</p><p>And then for user feedback, that's, you know, closing the feedback loop. You're incorporating what your real users are thinking. If they keep thumbs downing a certain edge case, you can incorporate that feedback and add it into a specific data set, you know? And that is then used to help improve that problem that you're dealing with.</p><p>So that's it. You know, now we're going to jump back in and show you how you can enable user feedback and set up human review. And at the end, we'll do some questions. We also have a bonus lecture and activity about remote evals that if you stick around, we'll probably have time to get to.</p><p>Cool. Let's jump into activity four. Obviously, we're going to look at user feedback and human review. There's sort of like two different routes here with human in the loop. Maybe to give a little background here or just to show you where some of this is happening. If you look at API feedback and then route.ts, the one thing I'll highlight is this log feedback.</p><p>So this is really all like what we need on the brain trust side. Again, using the brain trust SDK. We know of the span that we want to log this to from the current span that we are in. This is the user feedback score. Right? So this is like essentially what we're calling it.</p><p>User underscore feedback. And then the score. In our case, it's really just going to be a thumbs up or thumbs down. So a zero or a one. We also optionally have the ability to add a comment. So if the user, like your application has the ability for the user to define free text, you can also put that in here as well.</p><p>And then also, of course, using metadata. Metadata becomes really powerful from the filtering aspect. So like we can use this within our custom views to filter on these different metadata keys. coming back to the app. And you may have seen this at the bottom when we ran this last time.</p><p>But we have at the bottom the thumbs up and the thumbs down. This will allow the application here to log this particular user's feedback. So based on what they think, we can obviously do a thumbs up. We can submit with comment. And now if we go back into the Braintrust application, we have that net new log.</p><p>We have over here on the right, we have our comment from the user. And then we should see a net new column here within our logs for that user feedback. But again, as Carlos pointed out, we can now use this in a lot of different ways to help augment the workflows that we've already created.</p><p>This can help create custom views on top of this. This can now feed into a workflow for our human annotators to understand what that user feedback is and whether or not we should incorporate it into other datasets. Again, being able to click here, add a span to a dataset, and so on.</p><p>That's the sort of user feedback route. The other one is the human review. So back in our configuration pane over here, we have the ability to create different human review scores. So again, this will be a little bit more specific to your use case and what you want your humans to review or the scores that they should create.</p><p>It could be an option, it could be a slider, or it could be sort of free form text. So we could say something like, what's a better score, and then give it sort of an option. You know, thumbs up and thumbs down. So sort of like the human now can sort of look at what the output was and give their own sort of thumbs up, thumb down of this.</p><p>And now if you come back into the logs UI, users that are doing human review can have a different interface into those logs where maybe something like this is a little bit too much for what they're trying to do. Let's give them a little bit more pared down look at the inputs and outputs of this particular span.</p><p>And then they have the different scores that we've configured over here. And so maybe this is a thumbs up. They'll go through and they can do that review. Again, this now can add to that sort of flywheel effect that we need to create to build this Gen AI app.</p><p>Of course, yeah, I think I mentioned this a few different times, but being able to now add some of these spans to the data set, we can do that here as well. So now we should have, you know, if you look at the sample data, we had seven rows.</p><p>Through this process, we've added two net new rows to this data set. This now can help our offline evals and really create a rigorous sort of workflow to building a robust AI app. Cool. Any questions? Yes. So those human evaluators, they have to have their own account. Yes. But how will they see those evals that they need to evaluate?</p><p>Yeah. So you can add them to the specific project that you would want them to annotate. You could assign them. So we have the ability to add comments under specific, under data sets, under experiments, or a specific trace. You would tag them in something that you would want them to see.</p><p>We have sometimes, you know, you'll create an experiment and you'll assign it to a specific annotator. And they'll go in and fill out everything in that experiment or in a data set. Yeah. Sorry. The first one is developed by the client, right? Because it's shown in that web page where you have thumbs up and thumbs down.</p><p>Yeah. So the actual, like, the text box that you want them to fill out or, you know, the categories that you want them to thumbs up or thumbs down, you would define initially. And then you would let them go in and, and they would say, okay, based on completeness, I would give this a thumbs down.</p><p>Or, you know, what, what is the ideal output? Let me use this text box and, and type it all out. So, so yeah, you would essentially create the form that they would then go and fill out. Okay. Any other questions? Yeah. This isn't so much of the e-mails, but I saw in your documentation that you support, like, writing tools and uploading those as well and kind of, like, injecting those in your, like, the evaluation prompt, right?</p><p>Is there a way to, like, version the tools as well? Because, like, tools will also, like, be changing over time. And so, like, how can we keep track of that as well in the system? Yeah. That's a good question. Yeah. I believe it supports versioning. I guess this project doesn't have any tools.</p><p>No. I'm going to give you a soft, like, yes, it should be versioned. Same with the prompts and, and scores. Like, they all, at the very bottom, if you create a tool and add a new version of it, at the very bottom, well, we can show you with the prompts.</p><p>Potentially, if, yeah, so at the very bottom, we see this one version that was initially created. But if we were to go and, and make a change, save version, now we'd have that new version. So I, I believe the same thing would happen with the tools. So, like, you would, like, add a tool to this and that would just be, like, a new version of it, you know?</p><p>Yeah, yeah, exactly. Yeah. If you were to add a tool or change the prompt at all, it would be a new version of the prompt. You'd save it. It would now appear at the bottom. And your question was specifically, like, in tools, right? Like, if you change a tool, would it be versioned?</p><p>Yeah, I guess because you have, like, different-- oh, thanks. I guess you have, like, different prompt versions and then you have different tool versions. And then you're going to, like, evaluate those, like, mix and match those together, right? To, like, see which performs the best. Yeah. So there would be a scenario where the tool doesn't change, just the version that you're using in the prompt changes.</p><p>And how would you then version the prompt? Yeah. Right. Yeah. That's an interesting scenario. I think we versioned it correctly today. But if there's any issues, you know, the SE team is here to help. So feel free to reach out. Great. Thank you. Yeah. Any other questions? Human in the loop?</p><p>Okay, cool. All right, so now we're into the bonus session, I believe. Oh, not this one, I guess. Let's see. No bonus for you guys. No. This one has it. Okay, we can just do it this way. All right, so the new bonus session. Look at this. Extending playgrounds with remote evals.</p><p>So as you've seen, right, the playground can handle one LLM call being made. It can handle multiple LLM calls chained together, right? The agent feature. But if you wanted to have intermediate code steps in between the prompts, not supported, right? You'd have to go via the SDK. Or if you wanted it to talk to these external systems that are part of your VPC, that also wouldn't work.</p><p>Essentially, if your task gets too complex, it doesn't fit into the mold of the playground at the moment. So that's where remote evals comes in. It allows you to expose that local environment that you've set up in your laptop that has all the bells and whistles, has everything that you want set up.</p><p>You can now make that available in the playground. So if you have custom internal tooling, intermediate code steps, you know, a dynamic R&D environment that's constantly changing and you don't want to keep pushing tools or pushing scores, everything, to brain trust, then you can just set up a remote eval and it will become available to anybody in the playground.</p><p>So this is a great way of bridging the gap between very complex technical teams building crazy tasks in the SDK and non-technical people, maybe PMs, SMEs that still want to be hands-on, still want to iterate on the prompt, change certain parameters and see how that impacts the scores. So the remote evals can really help there.</p><p>So how this works, right, brain trust will send an eval request to your remote server. This can be local host or you can set it up somewhere else. Your server will then run the task and the score logic all locally. So all the intermediate steps, crazy business that you have going on, it'll all work because it's occurring on that server.</p><p>Same with the scores. You could have crazy complex scores. That would all work as well. And then it will return the outputs, the scores, metadata to brain trust and it'll populate the playground. To start this, it's very simple. You do the same brain trust eval and then point to a specific eval.ts file.</p><p>The difference is that you use the --dev flag. So this will start that remote eval server. It will default to local host, but you can change it. And it will allow it to bind to all the -- it will allow it to list it to all the network interfaces so you could access it via -- yeah, access remote servers out in the world.</p><p>Cool. So now we can get into the activity. We have something to -- you know, if you follow along with the activity document, you could set this up yourself. But if not, we can just show you quickly. All right. Let's first look at the code. It's going to look very similar to what I showed the first time.</p><p>Slightly different prompt, just trying to layer in a little bit more complexity here within this task. We now have all these different parameters that we want the user to be able to define and don't yet necessarily have the ability to do that within -- or in a great way within -- within brain trust.</p><p>Again, this can -- this can be very, very complex. Whatever sort of like, you know, the intermediate steps that you want to create are certainly allowed here. I'm going to take down this server and I'm going to run this command, pnpm remote eval, which is just under the hood.</p><p>Is running that brain trust eval and it's giving the location of that file that we're running the remote eval on. Back in the brain trust playground, this now is exposed as something that we can -- we can load in here. So let me remove this. I'll remove this here as well.</p><p>And now if you look down here, we have this -- this other option for remote eval. And this should link out to my change log generator eval. Here are the different -- excuse me -- parameters that I'm able to now configure that I've sort of exposed in that prompt.</p><p>But again, now think of like the more complex type of workflows that you may have within your code base. And you don't have the ability necessarily to push those into brain trust. We can still leverage this platform with that existing code base via remote evals. And this sort of like interaction works very similar to what I showed earlier with the evals locally via the SDK as well as the playground.</p><p>We should -- oh, well, probably have to -- oh, we did configure that. My scores. I'll switch this to the grid layout so we can start to see what the output is. But this is how we can really meet a maybe more technical team with a more complex task.</p><p>And they're not necessarily wanting to push all of these things into the brain trust platform, but still leverage it in some way. This is how we can sort of bridge that gap. And grounding this in the notion use case, they use a lot of services, right? They have a lot of tools that will retrieve different user information.</p><p>And they run all their evals via the SDK. And if they notice that there's an issue, right, it's underperforming in a certain area, a certain edge case isn't great. Maybe they isolate a certain row that they want to keep rerunning. Currently, they have to go to the SDK and run everything over and over again.</p><p>So if they were to use remote evals, they could bring that custom task into the playground, and they could isolate just one row and run it on one specific thing that they're working on. They'd save money, and they'd move faster. So that's something that we're going to discuss shortly with them and a lot of our other customers.</p><p>This is a brand new feature, and it's something that we've been hearing a lot of excitement around. So I hope you see the vision of where this could be effective. And, yeah, feel free to, you know, if you have any questions about remote evals or about anything else in the presentation, we've pretty much reached the end.</p><p>And thank you for sticking around. I know it's been a long day, to say the least. For the remote, do you need your IP... I'm right here. I'm right here. I'm right here. For the remote... For the remote... Yeah, it's good. For the remote, do you need your IP address publicly available so Braintrust can hit it, or...?</p><p>Yes, or you can use Logo Host. So, I mean, I guess, yeah. I can't imagine, like, if it's private, then Braintrust wouldn't be able to connect to it. Okay. All right. Any other questions? Great. Well, thanks, everyone. Thanks, all. Hope you have a good night, and we'll see you tomorrow.</p><p>Day two. Day two. Day two. Day two. Day two. Day two. Day two. Day two.</p></div></div></body></html>