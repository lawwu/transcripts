<html><head><title>Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 7 - Translation, Seq2Seq, Attention</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 7 - Translation, Seq2Seq, Attention</h2><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY"><img src="https://i.ytimg.com/vi/wzfWHP6SXxY/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=80">1:20</a> Assignment Three<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=147">2:27</a> Pre-History of Machine Translation<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=568">9:28</a> Learn the Translation Model<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=668">11:8</a> Alignment Variable<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1172">19:32</a> Statistical Machine Translation<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1321">22:1</a> Sequence To Sequence Models<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1647">27:27</a> Conditional Language Models<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1778">29:38</a> How To Train a Neural Machine Translation System and Then How To Use<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1947">32:27</a> Multi-Layer Rnns<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1982">33:2</a> Stacked Rnn<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2409">40:9</a> Greedy Decoding<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2612">43:32</a> Beam Searches<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2913">48:33</a> Stopping Criterion<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3111">51:51</a> Neural Translation<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3296">54:56</a> Evaluate Machine Translation<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3858">64:18</a> Problems of Agreement and Choice<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4123">68:43</a> Bible Translations<br><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4307">71:47</a> Writing System<br><br><div style="text-align: left;"><a href="./wzfWHP6SXxY.html">Whisper Transcript</a> | <a href="./transcript_wzfWHP6SXxY.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello everyone and welcome back into week four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=9" target="_blank">00:00:09.600</a></span> | <span class="t">So for week four, it's going to come in two halves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=14" target="_blank">00:00:14.760</a></span> | <span class="t">So today I'm going to talk about machine translation related topics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=19" target="_blank">00:00:19.540</a></span> | <span class="t">And then in the second half of the week, we take a little bit of a break from learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=24" target="_blank">00:00:24.980</a></span> | <span class="t">more and more neural networks topics and talk about final projects, but also some practical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=31" target="_blank">00:00:31.280</a></span> | <span class="t">tips for building neural network systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=35" target="_blank">00:00:35.080</a></span> | <span class="t">So for today's lecture, this is an important content for lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=40" target="_blank">00:00:40.540</a></span> | <span class="t">So first of all, I'm going to introduce a new task, machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=46" target="_blank">00:00:46.160</a></span> | <span class="t">And it turns out that task is a major use case of a new architectural technique to teach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=54" target="_blank">00:00:54.200</a></span> | <span class="t">you about deep learning, which is sequence to sequence models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=57" target="_blank">00:00:57.900</a></span> | <span class="t">And so we'll spend a lot of time on those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=60" target="_blank">00:01:00.680</a></span> | <span class="t">And then there's a crucial way that's been developed to improve sequence to sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=66" target="_blank">00:01:06.100</a></span> | <span class="t">models, which is the idea of attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=69" target="_blank">00:01:09.340</a></span> | <span class="t">And so that's what I'll talk about in the final part of the class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=74" target="_blank">00:01:14.780</a></span> | <span class="t">Just checking everyone's keeping up with what's happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=78" target="_blank">00:01:18.440</a></span> | <span class="t">So first of all, assignment three is due today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=84" target="_blank">00:01:24.220</a></span> | <span class="t">So hopefully you've all gotten your neural dependency parsers parsing text well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=89" target="_blank">00:01:29.720</a></span> | <span class="t">At the same time, assignment four is out today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=92" target="_blank">00:01:32.760</a></span> | <span class="t">And really, today's lecture is the primary content for what you'll be using for building</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=98" target="_blank">00:01:38.360</a></span> | <span class="t">your assignment four systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=101" target="_blank">00:01:41.360</a></span> | <span class="t">Switching it up for a little, for assignment four, we give you a mighty two extra days.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=107" target="_blank">00:01:47.440</a></span> | <span class="t">So you get nine days for it, and it's due on Thursday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=111" target="_blank">00:01:51.920</a></span> | <span class="t">On the other hand, do please be aware that assignment four is bigger and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=117" target="_blank">00:01:57.960</a></span> | <span class="t">harder than the previous assignments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=121" target="_blank">00:02:01.620</a></span> | <span class="t">So do make sure you get started on it early.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=124" target="_blank">00:02:04.800</a></span> | <span class="t">And then as I mentioned Thursday, I'll turn to final projects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=127" target="_blank">00:02:07.880</a></span> | <span class="t">Okay, so let's get straight into this with machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=136" target="_blank">00:02:16.680</a></span> | <span class="t">So very quickly, I wanted to tell you a little bit about where we were and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=143" target="_blank">00:02:23.040</a></span> | <span class="t">what we did before we get to neural machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=147" target="_blank">00:02:27.500</a></span> | <span class="t">And so let's do the prehistory of machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=151" target="_blank">00:02:31.680</a></span> | <span class="t">So machine translation is the task of translating a sentence X from one language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=157" target="_blank">00:02:37.840</a></span> | <span class="t">which is called the source language, to another language, the target language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=162" target="_blank">00:02:42.760</a></span> | <span class="t">forming a sentence Y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=164" target="_blank">00:02:44.720</a></span> | <span class="t">So we start off with a source language sentence X,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=169" target="_blank">00:02:49.920</a></span> | <span class="t">and then we translate it and we get out the translation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=175" target="_blank">00:02:55.400</a></span> | <span class="t">man is born free, but everywhere he is in chains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=179" target="_blank">00:02:59.440</a></span> | <span class="t">Okay, so there's our machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=181" target="_blank">00:03:01.920</a></span> | <span class="t">Okay, so in the early 1950s, there started to be work on machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=189" target="_blank">00:03:09.040</a></span> | <span class="t">And so it's actually a thing about computer science.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=191" target="_blank">00:03:11.640</a></span> | <span class="t">If you find things that have machine in the name, most of them are old things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=196" target="_blank">00:03:16.080</a></span> | <span class="t">So, and this really kind of came about in the US context,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=202" target="_blank">00:03:22.000</a></span> | <span class="t">in the context of the Cold War.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=204" target="_blank">00:03:24.960</a></span> | <span class="t">So there was this desire to keep tabs on what the Russians were doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=209" target="_blank">00:03:29.560</a></span> | <span class="t">And people had the idea that because some of the earliest computers had been so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=214" target="_blank">00:03:34.360</a></span> | <span class="t">successful at doing code breaking during the Second World War,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=219" target="_blank">00:03:39.280</a></span> | <span class="t">that maybe we could set early computers to work during the Cold War to do translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=227" target="_blank">00:03:47.320</a></span> | <span class="t">And hopefully this will play and you'll be able to hear it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=229" target="_blank">00:03:49.920</a></span> | <span class="t">Here's a little video clip showing some of the earliest work in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=234" target="_blank">00:03:54.720</a></span> | <span class="t">machine translation from 1954.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=237" target="_blank">00:03:57.760</a></span> | <span class="t">[MUSIC]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=242" target="_blank">00:04:02.360</a></span> | <span class="t">>> They hadn't reckoned with ambiguity when they set out to use computers to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=246" target="_blank">00:04:06.080</a></span> | <span class="t">translate languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=247" target="_blank">00:04:07.920</a></span> | <span class="t">>> A $500,000 super calculator, most versatile electronic brain known,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=253" target="_blank">00:04:13.160</a></span> | <span class="t">translates Russian into English.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=255" target="_blank">00:04:15.480</a></span> | <span class="t">Instead of mathematical wizardry, a sentence in Russian is to be set in-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=259" target="_blank">00:04:19.040</a></span> | <span class="t">>> One of the first non-numerical applications of computers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=262" target="_blank">00:04:22.480</a></span> | <span class="t">it was hyped as the solution to the Cold War obsession of keeping tabs on what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=266" target="_blank">00:04:26.320</a></span> | <span class="t">the Russians were doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=268" target="_blank">00:04:28.240</a></span> | <span class="t">Claims were made that the computer would replace most human translators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=272" target="_blank">00:04:32.280</a></span> | <span class="t">>> At present, of course, you're just in the experimental stage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=274" target="_blank">00:04:34.840</a></span> | <span class="t">When you go in for full scale production, what will the capacity be?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=278" target="_blank">00:04:38.120</a></span> | <span class="t">>> We should be able to do about, with a modern commercial computer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=282" target="_blank">00:04:42.800</a></span> | <span class="t">about one to two million words an hour.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=285" target="_blank">00:04:45.120</a></span> | <span class="t">And this will be quite an adequate speed to cope with the whole output of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=288" target="_blank">00:04:48.640</a></span> | <span class="t">the Soviet Union in just a few hours computer time a week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=292" target="_blank">00:04:52.400</a></span> | <span class="t">>> When do you hope to be able to achieve this speed?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=294" target="_blank">00:04:54.520</a></span> | <span class="t">>> If our experiments go well, then perhaps within five years or so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=298" target="_blank">00:04:58.800</a></span> | <span class="t">>> And finally, Mr. McDaniel, does this mean the end of human translators?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=303" target="_blank">00:05:03.320</a></span> | <span class="t">>> I would say yes for translators of scientific and technical material.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=307" target="_blank">00:05:07.240</a></span> | <span class="t">But as regards poetry and novels, no,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=309" target="_blank">00:05:09.480</a></span> | <span class="t">I don't think we'll ever replace the translators of that type of material.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=313" target="_blank">00:05:13.040</a></span> | <span class="t">>> Mr. McDaniel, thank you very much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=314" target="_blank">00:05:14.440</a></span> | <span class="t">>> But despite the hype, it ran into deep trouble.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=317" target="_blank">00:05:17.960</a></span> | <span class="t">>> [INAUDIBLE] >> Yeah, I'll stop there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=320" target="_blank">00:05:20.760</a></span> | <span class="t">Yeah, so the experiments did not go well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=325" target="_blank">00:05:25.040</a></span> | <span class="t">And so in retrospect, it's not very surprising that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=331" target="_blank">00:05:31.440</a></span> | <span class="t">the early work did not work out very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=336" target="_blank">00:05:36.120</a></span> | <span class="t">I mean, this was in the sort of really beginning of the computer age in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=339" target="_blank">00:05:39.880</a></span> | <span class="t">the 1950s.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=341" target="_blank">00:05:41.080</a></span> | <span class="t">But it was also the beginning of people starting to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=345" target="_blank">00:05:45.720</a></span> | <span class="t">the science of human languages, the field of linguistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=348" target="_blank">00:05:48.680</a></span> | <span class="t">So really, people had not much understanding of either side of what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=353" target="_blank">00:05:53.200</a></span> | <span class="t">was happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=354" target="_blank">00:05:54.560</a></span> | <span class="t">So what you had was people were trying to write systems on really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=359" target="_blank">00:05:59.000</a></span> | <span class="t">incredibly primitive computers, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=362" target="_blank">00:06:02.080</a></span> | <span class="t">It's probably the case that now if you have a USB-C power brick,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=367" target="_blank">00:06:07.600</a></span> | <span class="t">that it has more computational capacity inside it than the computers that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=371" target="_blank">00:06:11.960</a></span> | <span class="t">they were using to translate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=374" target="_blank">00:06:14.480</a></span> | <span class="t">And so effectively, what you were getting were very simple rule-based systems and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=379" target="_blank">00:06:19.800</a></span> | <span class="t">word lookup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=381" target="_blank">00:06:21.040</a></span> | <span class="t">So there was sort of like dictionary look up a word and get its translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=385" target="_blank">00:06:25.160</a></span> | <span class="t">But that just didn't work well because human languages are much more complex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=389" target="_blank">00:06:29.680</a></span> | <span class="t">than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=390" target="_blank">00:06:30.240</a></span> | <span class="t">Often, words have many meanings in different senses,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=393" target="_blank">00:06:33.960</a></span> | <span class="t">as we've sort of discussed about a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=396" target="_blank">00:06:36.840</a></span> | <span class="t">Often there are idioms, you need to understand the grammar to rewrite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=400" target="_blank">00:06:40.240</a></span> | <span class="t">the sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=401" target="_blank">00:06:41.320</a></span> | <span class="t">So for all sorts of reasons, it didn't work well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=405" target="_blank">00:06:45.280</a></span> | <span class="t">And this idea was largely canned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=408" target="_blank">00:06:48.000</a></span> | <span class="t">In particular, there was a famous US government report in the mid-1960s,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=412" target="_blank">00:06:52.360</a></span> | <span class="t">the ALPAC report, which basically concluded this wasn't working.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=416" target="_blank">00:06:56.680</a></span> | <span class="t">Oops, okay, work then did revive in AI at doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=422" target="_blank">00:07:02.320</a></span> | <span class="t">rule-based methods of machine translation in the 90s.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=430" target="_blank">00:07:10.480</a></span> | <span class="t">But when things really became alive was once you got into the mid-90s and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=436" target="_blank">00:07:16.000</a></span> | <span class="t">when they were in the period of statistical NLP that we've seen in other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=441" target="_blank">00:07:21.240</a></span> | <span class="t">places in the course, and then the idea began.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=446" target="_blank">00:07:26.520</a></span> | <span class="t">Can we start with just data about translation, i.e.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=452" target="_blank">00:07:32.080</a></span> | <span class="t">sentences and their translations, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=455" target="_blank">00:07:35.000</a></span> | <span class="t">learn a probabilistic model that can predict the translations of fresh sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=460" target="_blank">00:07:40.560</a></span> | <span class="t">So suppose we're translating French into English.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=464" target="_blank">00:07:44.440</a></span> | <span class="t">So what we would want to do is build a probabilistic model that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=468" target="_blank">00:07:48.360</a></span> | <span class="t">given a French sentence, we can say what's the probability of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=472" target="_blank">00:07:52.560</a></span> | <span class="t">English translations, and then we'll choose the most likely translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=476" target="_blank">00:07:56.680</a></span> | <span class="t">We can then found, it was found to felicitous to break this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=483" target="_blank">00:08:03.960</a></span> | <span class="t">down into two components by just reversing this with Bayes rule.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=489" target="_blank">00:08:09.240</a></span> | <span class="t">So if instead we had probability over English sentences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=495" target="_blank">00:08:15.960</a></span> | <span class="t">given a P of Y, and then a probability of a French sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=502" target="_blank">00:08:22.040</a></span> | <span class="t">given an English sentence, that people are able to make more progress.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=506" target="_blank">00:08:26.240</a></span> | <span class="t">And it's not immediately obvious as to why this should be,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=509" target="_blank">00:08:29.600</a></span> | <span class="t">because this is just sort of a trivial rewrite with Bayes rule.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=513" target="_blank">00:08:33.360</a></span> | <span class="t">But it allowed the problem to be separated into two parts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=516" target="_blank">00:08:36.800</a></span> | <span class="t">which proved to be more tractable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=519" target="_blank">00:08:39.440</a></span> | <span class="t">So on the left hand side, you effectively had a translation model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=524" target="_blank">00:08:44.480</a></span> | <span class="t">where you could just give a probability of words or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=528" target="_blank">00:08:48.480</a></span> | <span class="t">phrases being translated between the two languages,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=533" target="_blank">00:08:53.160</a></span> | <span class="t">without having to bother about the structural word order of the languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=537" target="_blank">00:08:57.760</a></span> | <span class="t">And then on the right hand,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=539" target="_blank">00:08:59.360</a></span> | <span class="t">you saw precisely what we spent a long time with last week,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=543" target="_blank">00:09:03.800</a></span> | <span class="t">which is this is just a probabilistic language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=547" target="_blank">00:09:07.280</a></span> | <span class="t">So if we have a very good model of what good fluent English sentences sound like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=553" target="_blank">00:09:13.880</a></span> | <span class="t">which we can build just from monolingual data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=556" target="_blank">00:09:16.600</a></span> | <span class="t">we can then get it to make sure we're producing sentences that sound good,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=561" target="_blank">00:09:21.920</a></span> | <span class="t">while the translation model hopefully puts the right words into them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=565" target="_blank">00:09:25.800</a></span> | <span class="t">So how do we learn the translation model since we haven't covered that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=573" target="_blank">00:09:33.040</a></span> | <span class="t">So the starting point was to get a large amount of parallel data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=577" target="_blank">00:09:37.680</a></span> | <span class="t">which is human translated sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=580" target="_blank">00:09:40.560</a></span> | <span class="t">At this point, it's mandatory that I show a picture of the Rosetta Stone,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=585" target="_blank">00:09:45.960</a></span> | <span class="t">which is the famous original piece of parallel data that allowed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=591" target="_blank">00:09:51.440</a></span> | <span class="t">the decoding of Egyptian hieroglyphs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=594" target="_blank">00:09:54.920</a></span> | <span class="t">because it had the same piece of text in different languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=599" target="_blank">00:09:59.200</a></span> | <span class="t">In the modern world, there are fortunately for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=602" target="_blank">00:10:02.400</a></span> | <span class="t">people who build natural language processing systems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=605" target="_blank">00:10:05.760</a></span> | <span class="t">quite a few places where parallel data is produced in large quantities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=611" target="_blank">00:10:11.400</a></span> | <span class="t">So the European Union produces a huge amount of parallel text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=616" target="_blank">00:10:16.600</a></span> | <span class="t">across European languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=619" target="_blank">00:10:19.400</a></span> | <span class="t">The French, sorry, not the French,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=622" target="_blank">00:10:22.040</a></span> | <span class="t">the Canadian Parliament conveniently produces parallel text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=627" target="_blank">00:10:27.760</a></span> | <span class="t">between French and English and even a limited amount in a new substitute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=633" target="_blank">00:10:33.000</a></span> | <span class="t">Canadian, Eskimo, and then the Hong Kong Parliament produces English and Chinese.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=641" target="_blank">00:10:41.720</a></span> | <span class="t">So there's a fair availability from different sources, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=645" target="_blank">00:10:45.600</a></span> | <span class="t">we can use that to build models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=647" target="_blank">00:10:47.720</a></span> | <span class="t">So how do we do it though?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=650" target="_blank">00:10:50.840</a></span> | <span class="t">All we have is these sentences, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=653" target="_blank">00:10:53.240</a></span> | <span class="t">it's not quite obvious how to build a probabilistic model out of those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=657" target="_blank">00:10:57.760</a></span> | <span class="t">Well, as before, what we wanna do is break this problem down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=662" target="_blank">00:11:02.680</a></span> | <span class="t">So in this case, what we're gonna do is introduce an extra variable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=668" target="_blank">00:11:08.280</a></span> | <span class="t">which is an alignment variable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=670" target="_blank">00:11:10.360</a></span> | <span class="t">So A is the alignment variable, which is going to give a word level or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=676" target="_blank">00:11:16.000</a></span> | <span class="t">sometimes phrase level correspondence between parts of the source sentence and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=681" target="_blank">00:11:21.840</a></span> | <span class="t">the target sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=683" target="_blank">00:11:23.640</a></span> | <span class="t">So this is an example of an alignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=687" target="_blank">00:11:27.160</a></span> | <span class="t">And so if we could induce this alignment between the two sentences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=693" target="_blank">00:11:33.280</a></span> | <span class="t">then we can have probabilities of pieces of how likely a word or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=698" target="_blank">00:11:38.960</a></span> | <span class="t">a short phrase is translated in a particular way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=703" target="_blank">00:11:43.520</a></span> | <span class="t">And in general, alignment is working out the correspondence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=709" target="_blank">00:11:49.400</a></span> | <span class="t">between words that is capturing the grammatical differences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=715" target="_blank">00:11:55.240</a></span> | <span class="t">between languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=717" target="_blank">00:11:57.400</a></span> | <span class="t">So words will occur in different orders in different languages,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=722" target="_blank">00:12:02.080</a></span> | <span class="t">depending on whether it's a language that puts the subject before the verb,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=727" target="_blank">00:12:07.640</a></span> | <span class="t">or the subject after the verb, or the verb before both the subject and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=732" target="_blank">00:12:12.920</a></span> | <span class="t">the object.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=734" target="_blank">00:12:14.000</a></span> | <span class="t">And the alignments will also capture something about differences about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=738" target="_blank">00:12:18.240</a></span> | <span class="t">the ways that languages do things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=740" target="_blank">00:12:20.440</a></span> | <span class="t">So what we find is that we get every possibility of how words can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=745" target="_blank">00:12:25.280</a></span> | <span class="t">align between languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=747" target="_blank">00:12:27.600</a></span> | <span class="t">So you can have words that don't get translated at all in the other language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=754" target="_blank">00:12:34.640</a></span> | <span class="t">So in French, you put a definite article, the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=758" target="_blank">00:12:38.560</a></span> | <span class="t">before country names like Le Japon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=761" target="_blank">00:12:41.680</a></span> | <span class="t">So when that gets translated into English, you just get Japan.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=765" target="_blank">00:12:45.080</a></span> | <span class="t">So there's no translation of the the, so it just goes away.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=769" target="_blank">00:12:49.320</a></span> | <span class="t">On the other hand, you can get many to one translations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=774" target="_blank">00:12:54.720</a></span> | <span class="t">where one French word gets translated as several English words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=781" target="_blank">00:13:01.120</a></span> | <span class="t">So for the last French word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=783" target="_blank">00:13:03.480</a></span> | <span class="t">it's being translated as Aboriginal people as multiple words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=788" target="_blank">00:13:08.640</a></span> | <span class="t">You can get the reverse, where you can have several French words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=793" target="_blank">00:13:13.880</a></span> | <span class="t">that get translated as one English word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=797" target="_blank">00:13:17.320</a></span> | <span class="t">So mise en application is getting translators implemented.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=802" target="_blank">00:13:22.720</a></span> | <span class="t">And you can get even more complicated one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=806" target="_blank">00:13:26.840</a></span> | <span class="t">So here we sort of have four English words being translated as two French words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=812" target="_blank">00:13:32.640</a></span> | <span class="t">but they don't really break down and translate each other well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=817" target="_blank">00:13:37.600</a></span> | <span class="t">I mean, these things don't only happen across languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=821" target="_blank">00:13:41.000</a></span> | <span class="t">They also happen within the language when you have different ways of saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=824" target="_blank">00:13:44.760</a></span> | <span class="t">the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=825" target="_blank">00:13:45.840</a></span> | <span class="t">So another way you might have expressed the poor don't have any money,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=831" target="_blank">00:13:51.200</a></span> | <span class="t">is to say the poor are moneyless.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=833" target="_blank">00:13:53.760</a></span> | <span class="t">And that's much more similar to how the French is being rendered here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=839" target="_blank">00:13:59.200</a></span> | <span class="t">And so even English to English, you have the same kind of alignment problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=845" target="_blank">00:14:05.160</a></span> | <span class="t">So probabilistic or statistical machine translation is more commonly known.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=851" target="_blank">00:14:11.240</a></span> | <span class="t">What we wanted to do is learn these alignments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=855" target="_blank">00:14:15.200</a></span> | <span class="t">And there's a bunch of sources of information you could use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=858" target="_blank">00:14:18.840</a></span> | <span class="t">If you start with parallel sentences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=861" target="_blank">00:14:21.840</a></span> | <span class="t">you can see how often words and phrases co-occur in parallel sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=867" target="_blank">00:14:27.280</a></span> | <span class="t">You can look at their positions in the sentence and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=871" target="_blank">00:14:31.720</a></span> | <span class="t">figure out what are good alignments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=875" target="_blank">00:14:35.200</a></span> | <span class="t">But alignments are categorical thing, they're not probabilistic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=880" target="_blank">00:14:40.680</a></span> | <span class="t">And so they are latent variables.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=883" target="_blank">00:14:43.080</a></span> | <span class="t">And so you need to use special learning algorithms like the expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=887" target="_blank">00:14:47.240</a></span> | <span class="t">maximization algorithm for learning about latent variables.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=891" target="_blank">00:14:51.040</a></span> | <span class="t">In the olden days of CS224n, before we started doing it all with deep learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=896" target="_blank">00:14:56.760</a></span> | <span class="t">we spent tons of CS224n dealing with latent variable algorithms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=902" target="_blank">00:15:02.120</a></span> | <span class="t">But these days we don't cover that at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=904" target="_blank">00:15:04.600</a></span> | <span class="t">And you're gonna have to go off and see CS228 if you wanna know more about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=909" target="_blank">00:15:09.680</a></span> | <span class="t">And we're not really expecting you to understand the details here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=913" target="_blank">00:15:13.520</a></span> | <span class="t">But I did then wanna say a bit more about how decoding was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=918" target="_blank">00:15:18.720</a></span> | <span class="t">done in a statistical machine translation system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=925" target="_blank">00:15:25.320</a></span> | <span class="t">So what we wanted to do is to say we had a translation model and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=929" target="_blank">00:15:29.800</a></span> | <span class="t">a language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=931" target="_blank">00:15:31.440</a></span> | <span class="t">And we want to pick out the most likely why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=935" target="_blank">00:15:35.800</a></span> | <span class="t">there's the translation of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=938" target="_blank">00:15:38.400</a></span> | <span class="t">And what kind of process could we use to do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=942" target="_blank">00:15:42.520</a></span> | <span class="t">Well, the naive thing is to say, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=947" target="_blank">00:15:47.040</a></span> | <span class="t">let's just enumerate every possible why and calculate its probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=952" target="_blank">00:15:52.600</a></span> | <span class="t">But we can't possibly do that because there's a number of translation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=957" target="_blank">00:15:57.760</a></span> | <span class="t">sentences in the target language that's exponential in the length of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=963" target="_blank">00:16:03.040</a></span> | <span class="t">So that's way too expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=965" target="_blank">00:16:05.240</a></span> | <span class="t">So we need to have some way to break it down more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=969" target="_blank">00:16:09.520</a></span> | <span class="t">And while we had a simple way for language models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=973" target="_blank">00:16:13.640</a></span> | <span class="t">we just generated words one at a time and laid out the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=979" target="_blank">00:16:19.040</a></span> | <span class="t">And so that seems a reasonable thing to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=981" target="_blank">00:16:21.840</a></span> | <span class="t">But here we need to deal with the fact that things occur in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=986" target="_blank">00:16:26.560</a></span> | <span class="t">different orders in source languages and in translations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=994" target="_blank">00:16:34.040</a></span> | <span class="t">And so we do wanna break it into pieces with an independence assumption like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=998" target="_blank">00:16:38.080</a></span> | <span class="t">the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=999" target="_blank">00:16:39.360</a></span> | <span class="t">But then we want a way of breaking things apart and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1002" target="_blank">00:16:42.760</a></span> | <span class="t">exploring it in what's called a decoding process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1006" target="_blank">00:16:46.600</a></span> | <span class="t">So this is the way it was done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1008" target="_blank">00:16:48.560</a></span> | <span class="t">So we'd start with a source sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1011" target="_blank">00:16:51.120</a></span> | <span class="t">So this is a German sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1014" target="_blank">00:16:54.160</a></span> | <span class="t">And as is standard in German, you're getting this second position verb.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1022" target="_blank">00:17:02.600</a></span> | <span class="t">So that's probably not in the right position for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1025" target="_blank">00:17:05.960</a></span> | <span class="t">where the English translation is gonna be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1028" target="_blank">00:17:08.200</a></span> | <span class="t">So we might need to rearrange the words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1031" target="_blank">00:17:11.560</a></span> | <span class="t">So what we have is based on the translation model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1036" target="_blank">00:17:16.160</a></span> | <span class="t">we have words or phrases that are reasonably likely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1042" target="_blank">00:17:22.240</a></span> | <span class="t">translations of each German word or sometimes a German phrase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1048" target="_blank">00:17:28.040</a></span> | <span class="t">So these are effectively the Lego pieces out of which we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1053" target="_blank">00:17:33.000</a></span> | <span class="t">want to create the translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1055" target="_blank">00:17:35.960</a></span> | <span class="t">And so then inside that, making use of this data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1061" target="_blank">00:17:41.680</a></span> | <span class="t">we're going to generate the translation piece by piece,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1065" target="_blank">00:17:45.800</a></span> | <span class="t">kind of like we did with our neural language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1069" target="_blank">00:17:49.320</a></span> | <span class="t">So we're gonna start with an empty translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1072" target="_blank">00:17:52.960</a></span> | <span class="t">And then we're gonna say, well, we want to use one of these Lego pieces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1078" target="_blank">00:17:58.640</a></span> | <span class="t">And so we could explore different possible ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1082" target="_blank">00:18:02.000</a></span> | <span class="t">So there's a search process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1083" target="_blank">00:18:03.600</a></span> | <span class="t">But one of the possible pieces is we could translate with he.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1088" target="_blank">00:18:08.120</a></span> | <span class="t">Or we could start the sentence with our translating the second word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1092" target="_blank">00:18:12.960</a></span> | <span class="t">So we could explore various likely possibilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1097" target="_blank">00:18:17.680</a></span> | <span class="t">And if we're guided by our language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1100" target="_blank">00:18:20.120</a></span> | <span class="t">it's probably much more likely to start the sentence with he than it is to start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1104" target="_blank">00:18:24.880</a></span> | <span class="t">the sentence with our though ours not impossible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1107" target="_blank">00:18:27.920</a></span> | <span class="t">Okay, and then the other thing we're doing with these little blotches of black up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1111" target="_blank">00:18:31.360</a></span> | <span class="t">the top, we're sort of recording which German words we've translated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1116" target="_blank">00:18:36.200</a></span> | <span class="t">And so we explore forward in a translation process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1121" target="_blank">00:18:41.360</a></span> | <span class="t">And we could decide that we could translate next the second word goes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1128" target="_blank">00:18:48.920</a></span> | <span class="t">or we could translate the negation here and translate that as does not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1134" target="_blank">00:18:54.440</a></span> | <span class="t">And we explore various continuations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1137" target="_blank">00:18:57.880</a></span> | <span class="t">And in the process, I'll go through in more detail later when we do the neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1141" target="_blank">00:19:01.440</a></span> | <span class="t">equivalent, we sort of do this search where we explore likely translations and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1147" target="_blank">00:19:07.720</a></span> | <span class="t">prune.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1148" target="_blank">00:19:08.680</a></span> | <span class="t">And eventually we've translated the whole of the input sentence and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1152" target="_blank">00:19:12.600</a></span> | <span class="t">have worked out a fairly likely translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1155" target="_blank">00:19:15.240</a></span> | <span class="t">He does not go home, and that's what we'll use as the translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1158" target="_blank">00:19:18.840</a></span> | <span class="t">Okay, so in the period from about 1997 to around 2013,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1167" target="_blank">00:19:27.920</a></span> | <span class="t">statistical machine translation was a huge research field.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1173" target="_blank">00:19:33.640</a></span> | <span class="t">The best systems were extremely complex.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1181" target="_blank">00:19:41.920</a></span> | <span class="t">They had hundreds of details that I certainly haven't mentioned here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1186" target="_blank">00:19:46.120</a></span> | <span class="t">The systems had lots of separately designed and built components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1190" target="_blank">00:19:50.360</a></span> | <span class="t">So I mentioned language model and the translation model, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1194" target="_blank">00:19:54.440</a></span> | <span class="t">they have lots of other components for reordering models and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1198" target="_blank">00:19:58.320</a></span> | <span class="t">inflection models and other things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1200" target="_blank">00:20:00.480</a></span> | <span class="t">There was lots of feature engineering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1203" target="_blank">00:20:03.480</a></span> | <span class="t">Typically, the models also made use of lots of extra resources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1209" target="_blank">00:20:09.080</a></span> | <span class="t">And there were lots of human effort to maintain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1213" target="_blank">00:20:13.400</a></span> | <span class="t">But nevertheless, they were already fairly successful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1216" target="_blank">00:20:16.520</a></span> | <span class="t">So Google Translate launched in the mid 2000s, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1220" target="_blank">00:20:20.640</a></span> | <span class="t">people thought, wow, this is amazing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1223" target="_blank">00:20:23.840</a></span> | <span class="t">You could start to get sort of semi decent automatic translations for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1229" target="_blank">00:20:29.880</a></span> | <span class="t">different web pages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1232" target="_blank">00:20:32.200</a></span> | <span class="t">But that was chugging along well enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1236" target="_blank">00:20:36.480</a></span> | <span class="t">And then we got to 2014.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1239" target="_blank">00:20:39.680</a></span> | <span class="t">And really with enormous suddenness, people then worked out ways</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1245" target="_blank">00:20:45.320</a></span> | <span class="t">of doing machine translation using a large neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1251" target="_blank">00:20:51.440</a></span> | <span class="t">And these large neural networks proved to be just extremely successful and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1257" target="_blank">00:20:57.160</a></span> | <span class="t">largely blew away everything that preceded it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1260" target="_blank">00:21:00.560</a></span> | <span class="t">So for the next big part of the lecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1263" target="_blank">00:21:03.040</a></span> | <span class="t">what I'd like to do is tell you something about neural machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1268" target="_blank">00:21:08.400</a></span> | <span class="t">Neural machine translation, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1272" target="_blank">00:21:12.240</a></span> | <span class="t">it means you're using a neural network to do machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1276" target="_blank">00:21:16.240</a></span> | <span class="t">But in practice, it's meant slightly more than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1279" target="_blank">00:21:19.880</a></span> | <span class="t">It has meant that we're going to build one very large neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1285" target="_blank">00:21:25.400</a></span> | <span class="t">which completely does translation end to end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1290" target="_blank">00:21:30.080</a></span> | <span class="t">So we're going to have a large neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1292" target="_blank">00:21:32.440</a></span> | <span class="t">We're going to feed in the source sentence into the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1295" target="_blank">00:21:35.600</a></span> | <span class="t">And what's going to come out as the output of the neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1299" target="_blank">00:21:39.880</a></span> | <span class="t">is the translation of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1302" target="_blank">00:21:42.680</a></span> | <span class="t">We're going to train that model end to end on parallel sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1307" target="_blank">00:21:47.280</a></span> | <span class="t">And it's the entire system rather than being lots of separate components</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1312" target="_blank">00:21:52.840</a></span> | <span class="t">as in an old fashioned machine translation system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1316" target="_blank">00:21:56.360</a></span> | <span class="t">And we'll see that in a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1318" target="_blank">00:21:58.640</a></span> | <span class="t">So these neural network architectures are called sequence to sequence models or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1323" target="_blank">00:22:03.880</a></span> | <span class="t">commonly abbreviated seek to seek.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1326" target="_blank">00:22:06.120</a></span> | <span class="t">And they involve two neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1331" target="_blank">00:22:11.560</a></span> | <span class="t">Here it says two RNNs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1332" target="_blank">00:22:12.960</a></span> | <span class="t">The version I'm presenting now has two RNNs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1336" target="_blank">00:22:16.120</a></span> | <span class="t">But more generally, they involve two neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1339" target="_blank">00:22:19.000</a></span> | <span class="t">There's one neural network that is going to encode the source sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1344" target="_blank">00:22:24.520</a></span> | <span class="t">So if we have a source sentence here, we are going to encode that sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1350" target="_blank">00:22:30.280</a></span> | <span class="t">And well, we know about a way that we can do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1353" target="_blank">00:22:33.280</a></span> | <span class="t">So using the kind of LSTMs that we saw last class, we can start at the beginning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1359" target="_blank">00:22:39.920</a></span> | <span class="t">and go through a sentence and update the hidden state each time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1365" target="_blank">00:22:45.200</a></span> | <span class="t">And that will give us a representation of the content of the source sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1371" target="_blank">00:22:51.000</a></span> | <span class="t">So that's the first sequence model, which encodes the source sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1378" target="_blank">00:22:58.000</a></span> | <span class="t">And we'll use the idea that the final hidden state of the encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1384" target="_blank">00:23:04.920</a></span> | <span class="t">RNN is going to in a sense represent the source sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1391" target="_blank">00:23:11.320</a></span> | <span class="t">And we're going to feed it in directly as the initial hidden state for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1395" target="_blank">00:23:15.440</a></span> | <span class="t">the decoder RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1397" target="_blank">00:23:17.400</a></span> | <span class="t">So then on the other side of the picture, we have our decoder RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1401" target="_blank">00:23:21.840</a></span> | <span class="t">And it's a language model that's going to generate a target sentence conditioned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1407" target="_blank">00:23:27.240</a></span> | <span class="t">on the final hidden state of the encoder RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1414" target="_blank">00:23:34.160</a></span> | <span class="t">So we're going to start with the input of start symbol.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1417" target="_blank">00:23:37.800</a></span> | <span class="t">We're going to feed in the hidden state from the encoder RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1422" target="_blank">00:23:42.120</a></span> | <span class="t">And now this second green RNN has completely separate parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1427" target="_blank">00:23:47.080</a></span> | <span class="t">I might just emphasize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1428" target="_blank">00:23:48.760</a></span> | <span class="t">But we do the same kind of LSTM computations and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1432" target="_blank">00:23:52.680</a></span> | <span class="t">generate a first word of the sentence he.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1436" target="_blank">00:23:56.400</a></span> | <span class="t">And so then doing LSTM generation just like last class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1441" target="_blank">00:24:01.920</a></span> | <span class="t">we copy that down as the next input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1444" target="_blank">00:24:04.800</a></span> | <span class="t">We run the next step of the LSTM, generate another word here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1449" target="_blank">00:24:09.160</a></span> | <span class="t">copy it down and chug along.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1451" target="_blank">00:24:11.680</a></span> | <span class="t">And we've translated the sentence, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1456" target="_blank">00:24:16.960</a></span> | <span class="t">So this is showing the test time behavior when we're generating the next sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1465" target="_blank">00:24:25.280</a></span> | <span class="t">For the training time behavior, when we have parallel sentences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1470" target="_blank">00:24:30.600</a></span> | <span class="t">we're still using the same kind of sequence to sequence model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1474" target="_blank">00:24:34.880</a></span> | <span class="t">But we're doing it with the decoder part just like training a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1480" target="_blank">00:24:40.960</a></span> | <span class="t">where we're wanting to do teacher forcing and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1484" target="_blank">00:24:44.000</a></span> | <span class="t">predict each word that's actually found in the source language sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1488" target="_blank">00:24:48.400</a></span> | <span class="t">Sequence to sequence models have been an incredibly powerful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1496" target="_blank">00:24:56.080</a></span> | <span class="t">widely used workhorse in neural networks for NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1502" target="_blank">00:25:02.160</a></span> | <span class="t">So although historically, machine translation was the first big use of them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1509" target="_blank">00:25:09.680</a></span> | <span class="t">and is sort of the canonical use, they're used everywhere else as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1515" target="_blank">00:25:15.360</a></span> | <span class="t">So you can do many other NLP tasks for them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1519" target="_blank">00:25:19.000</a></span> | <span class="t">So you can do summarization, you can think of text summarization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1523" target="_blank">00:25:23.240</a></span> | <span class="t">as translating a long text into a short text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1527" target="_blank">00:25:27.520</a></span> | <span class="t">But you can use them for other things that are in no way a translation whatsoever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1533" target="_blank">00:25:33.080</a></span> | <span class="t">So they're commonly used for neural dialogue systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1538" target="_blank">00:25:38.040</a></span> | <span class="t">So the encoder will encode the previous two utterances, say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1544" target="_blank">00:25:44.080</a></span> | <span class="t">and then you will use the decoder to generate the next utterance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1550" target="_blank">00:25:50.480</a></span> | <span class="t">Some other uses are even freakier, but have proven to be quite successful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1557" target="_blank">00:25:57.000</a></span> | <span class="t">So if you have any way of representing the parse of a sentence as a string,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1565" target="_blank">00:26:05.960</a></span> | <span class="t">and if you sort of think a little, it's fairly obvious how you can turn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1571" target="_blank">00:26:11.120</a></span> | <span class="t">the parse of a sentence into a string by just making use of extra syntax,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1576" target="_blank">00:26:16.400</a></span> | <span class="t">like parentheses, or putting in explicit words that are saying left arc,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1582" target="_blank">00:26:22.960</a></span> | <span class="t">right arc, shifts like the transition systems that you used for assignment three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1590" target="_blank">00:26:30.280</a></span> | <span class="t">Well, then we could say, let's use the encoder, feed the input sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1596" target="_blank">00:26:36.400</a></span> | <span class="t">to the encoder, and let it output the transition sequence of our dependency parser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1602" target="_blank">00:26:42.680</a></span> | <span class="t">And somewhat surprisingly, that actually works well as another way to build</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1607" target="_blank">00:26:47.880</a></span> | <span class="t">a dependency parser or other kinds of parser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1610" target="_blank">00:26:50.800</a></span> | <span class="t">These models have also been applied not just to natural languages, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1616" target="_blank">00:26:56.200</a></span> | <span class="t">to other kinds of languages, including music and also programming language code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1623" target="_blank">00:27:03.480</a></span> | <span class="t">So you can train a seek-to-seek system where it reads in pseudocode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1630" target="_blank">00:27:10.920</a></span> | <span class="t">in natural language, and it generates out Python code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1635" target="_blank">00:27:15.040</a></span> | <span class="t">And if you have a good enough one, it can do the assignment for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1641" target="_blank">00:27:21.640</a></span> | <span class="t">So the essential new idea here with our sequence-to-sequence models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1646" target="_blank">00:27:26.800</a></span> | <span class="t">is we have an example of conditional language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1650" target="_blank">00:27:30.680</a></span> | <span class="t">So previously, the main thing we were doing was just sort of start at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1655" target="_blank">00:27:35.480</a></span> | <span class="t">the beginning of the sentence and generate a sentence based on nothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1661" target="_blank">00:27:41.840</a></span> | <span class="t">But here we have something that is going to determine, or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1666" target="_blank">00:27:46.600</a></span> | <span class="t">partially determine, that is going to condition what we should produce.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1671" target="_blank">00:27:51.120</a></span> | <span class="t">So we have a source sentence, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1673" target="_blank">00:27:53.160</a></span> | <span class="t">that's going to strongly determine what is a good translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1677" target="_blank">00:27:57.960</a></span> | <span class="t">And so to achieve that, what we're going to do is have some way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1684" target="_blank">00:28:04.120</a></span> | <span class="t">of transferring information about the source sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1689" target="_blank">00:28:09.560</a></span> | <span class="t">from the encoder to trigger what the decoder should do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1695" target="_blank">00:28:15.240</a></span> | <span class="t">And the two standard ways of doing that are you either feed in a hidden state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1700" target="_blank">00:28:20.520</a></span> | <span class="t">as the initial hidden state to the decoder, or sometimes you will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1704" target="_blank">00:28:24.960</a></span> | <span class="t">feed something in as the initial input to the decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1709" target="_blank">00:28:29.360</a></span> | <span class="t">And so in neural machine translation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1714" target="_blank">00:28:34.240</a></span> | <span class="t">we're directly calculating this conditional model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1718" target="_blank">00:28:38.240</a></span> | <span class="t">probability of target language sentence given source language sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1723" target="_blank">00:28:43.640</a></span> | <span class="t">And so at each step, as we break down the word by word generation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1728" target="_blank">00:28:48.920</a></span> | <span class="t">that we're conditioning not only on previous words of the target language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1734" target="_blank">00:28:54.760</a></span> | <span class="t">but also each time on our source language sentence x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1739" target="_blank">00:28:59.640</a></span> | <span class="t">Because of this, we actually know a ton more about what our sentence that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1744" target="_blank">00:29:04.520</a></span> | <span class="t">generate should be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1746" target="_blank">00:29:06.080</a></span> | <span class="t">So if you look at the perplexities of these kind of conditional language models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1752" target="_blank">00:29:12.920</a></span> | <span class="t">you will find them like the numbers I showed last time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1756" target="_blank">00:29:16.320</a></span> | <span class="t">They usually have almost frequently low perplexities that you will have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1761" target="_blank">00:29:21.440</a></span> | <span class="t">models with perplexities that are something like four or even less,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1765" target="_blank">00:29:25.960</a></span> | <span class="t">sometimes 2.5, because you get a lot of information about what words you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1771" target="_blank">00:29:31.120</a></span> | <span class="t">should be generating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1773" target="_blank">00:29:33.560</a></span> | <span class="t">OK, so then we have the same questions as we had for language models in general,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1779" target="_blank">00:29:39.560</a></span> | <span class="t">how to train a neural machine translation system, and then how to use it at runtime.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1785" target="_blank">00:29:45.840</a></span> | <span class="t">So let's go through both of those in a bit more detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1791" target="_blank">00:29:51.000</a></span> | <span class="t">So the first step is we get a large parallel corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1796" target="_blank">00:29:56.000</a></span> | <span class="t">So we run off to the European Union, for example, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1800" target="_blank">00:30:00.760</a></span> | <span class="t">we grab a lot of parallel English French data from the European Parliament proceedings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1807" target="_blank">00:30:07.880</a></span> | <span class="t">So then once we have our parallel sentences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1811" target="_blank">00:30:11.680</a></span> | <span class="t">what we're gonna do is take batches of source sentences and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1817" target="_blank">00:30:17.600</a></span> | <span class="t">target sentences, we'll encode the source sentence with our encoder LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1826" target="_blank">00:30:26.440</a></span> | <span class="t">We'll feed its final hidden state into a target LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1834" target="_blank">00:30:34.960</a></span> | <span class="t">And this one, we are now then going to train word by word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1840" target="_blank">00:30:40.560</a></span> | <span class="t">by comparing what it predicts is the most likely word to be produced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1846" target="_blank">00:30:46.040</a></span> | <span class="t">versus what the actual first word and then the actual second word is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1851" target="_blank">00:30:51.840</a></span> | <span class="t">And to the extent that we get it wrong, we're going to suffer some loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1857" target="_blank">00:30:57.440</a></span> | <span class="t">So this is gonna be the negative log probability of generating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1862" target="_blank">00:31:02.320</a></span> | <span class="t">the correct next word he and so on along the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1866" target="_blank">00:31:06.680</a></span> | <span class="t">And so in the same way that we saw last time for language models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1871" target="_blank">00:31:11.000</a></span> | <span class="t">we can work out our overall loss for the sentence doing this teacher forcing style,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1877" target="_blank">00:31:17.640</a></span> | <span class="t">generate one word at a time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1879" target="_blank">00:31:19.680</a></span> | <span class="t">calculate a loss relative to the word that you should have produced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1884" target="_blank">00:31:24.120</a></span> | <span class="t">And so that loss then gives us information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1889" target="_blank">00:31:29.960</a></span> | <span class="t">that we can back propagate through the entire network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1894" target="_blank">00:31:34.080</a></span> | <span class="t">And the crucial thing about the sequence to sequence models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1899" target="_blank">00:31:39.440</a></span> | <span class="t">that has made them extremely successful in practice is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1903" target="_blank">00:31:43.840</a></span> | <span class="t">the entire thing is optimized as a single system end to end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1909" target="_blank">00:31:49.160</a></span> | <span class="t">So starting with our final loss, we back propagate it right through the system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1916" target="_blank">00:31:56.480</a></span> | <span class="t">So we not only update all the parameters of the decoder model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1921" target="_blank">00:32:01.960</a></span> | <span class="t">but we also update all of the parameters of the encoder model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1927" target="_blank">00:32:07.160</a></span> | <span class="t">which in turn will influence what conditioning gets passed over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1932" target="_blank">00:32:12.160</a></span> | <span class="t">from the encoder to the decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1934" target="_blank">00:32:14.440</a></span> | <span class="t">So this moment is a good moment for me to return to the three slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1942" target="_blank">00:32:22.240</a></span> | <span class="t">that I skipped running out of time at the end of last time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1946" target="_blank">00:32:26.760</a></span> | <span class="t">which is to mention multi-layer RNNs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1951" target="_blank">00:32:31.840</a></span> | <span class="t">So the RNNs that we've looked at so far are already deep on one dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1958" target="_blank">00:32:38.400</a></span> | <span class="t">then unroll horizontally over many time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1963" target="_blank">00:32:43.200</a></span> | <span class="t">But they've been shallow in that there's just been a single layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1967" target="_blank">00:32:47.440</a></span> | <span class="t">of recurrent structure above our sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1971" target="_blank">00:32:51.120</a></span> | <span class="t">We can also make them deep in the other dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1973" target="_blank">00:32:53.960</a></span> | <span class="t">by applying multiple RNNs on top of each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1978" target="_blank">00:32:58.280</a></span> | <span class="t">And this gives us a multi-layer RNN, often also called a stacked RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1985" target="_blank">00:33:05.000</a></span> | <span class="t">And having a multi-layer RNN allows us the network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1991" target="_blank">00:33:11.040</a></span> | <span class="t">to compute more complex representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=1994" target="_blank">00:33:14.280</a></span> | <span class="t">So simply put, the lower RNNs tend to compute lower level features,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2000" target="_blank">00:33:20.440</a></span> | <span class="t">and the higher RNNs should compute higher level features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2004" target="_blank">00:33:24.920</a></span> | <span class="t">And just like in other neural networks, whether it's feedforward networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2010" target="_blank">00:33:30.280</a></span> | <span class="t">or the kind of networks you see in vision systems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2013" target="_blank">00:33:33.520</a></span> | <span class="t">you get much greater power and success</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2017" target="_blank">00:33:37.480</a></span> | <span class="t">by having a stack on multi, multiple layers of recurrent neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2023" target="_blank">00:33:43.560</a></span> | <span class="t">Right? That you might think that, oh, there are two things I could do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2027" target="_blank">00:33:47.640</a></span> | <span class="t">I could have a single LSTM with a hidden state of dimension 2000,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2032" target="_blank">00:33:52.400</a></span> | <span class="t">or I could have four layers of LSTMs with a hidden state of 500 each.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2039" target="_blank">00:33:59.920</a></span> | <span class="t">And it shouldn't make any difference because I've got the same number of parameters roughly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2044" target="_blank">00:34:04.400</a></span> | <span class="t">But that's not true. In practice, it does make a big difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2048" target="_blank">00:34:08.520</a></span> | <span class="t">And multi-layer or stacked RNNs are more powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2053" target="_blank">00:34:13.120</a></span> | <span class="t">So...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2055" target="_blank">00:34:15.120</a></span> | <span class="t">- Could I ask you, there's a good student question here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2058" target="_blank">00:34:18.840</a></span> | <span class="t">about what lower level versus higher level features mean in this context?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2062" target="_blank">00:34:22.480</a></span> | <span class="t">- Sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2064" target="_blank">00:34:24.480</a></span> | <span class="t">Yeah, so, I mean, in some sense, these are kind of somewhat flimsy ways...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2072" target="_blank">00:34:32.880</a></span> | <span class="t">You know, terms, this meaning isn't precise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2078" target="_blank">00:34:38.480</a></span> | <span class="t">But typically what that's meaning is that lower level features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2084" target="_blank">00:34:44.160</a></span> | <span class="t">are knowing sort of more basic things about words and phrases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2089" target="_blank">00:34:49.920</a></span> | <span class="t">So that commonly might be things like, what part of speech is this word?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2095" target="_blank">00:34:55.600</a></span> | <span class="t">Or are these words the name of a person or the name of a company?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2101" target="_blank">00:35:01.760</a></span> | <span class="t">Whereas higher level features refer to things that are at a higher semantic level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2108" target="_blank">00:35:08.560</a></span> | <span class="t">So knowing more about the overall structure of a sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2112" target="_blank">00:35:12.760</a></span> | <span class="t">knowing something about what it means,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2115" target="_blank">00:35:15.400</a></span> | <span class="t">whether a phrase has positive or negative connotations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2119" target="_blank">00:35:19.720</a></span> | <span class="t">what its semantics are when you put together several words into an idiomatic phrase,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2126" target="_blank">00:35:26.680</a></span> | <span class="t">are roughly the higher level kinds of things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2129" target="_blank">00:35:29.680</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2137" target="_blank">00:35:37.320</a></span> | <span class="t">Jump ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2141" target="_blank">00:35:41.320</a></span> | <span class="t">Okay, so when we build one of these end-to-end neural machine translation systems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2151" target="_blank">00:35:51.320</a></span> | <span class="t">if we want them to work well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2155" target="_blank">00:35:55.320</a></span> | <span class="t">single layer LSTM encoder decoder neural machine translation systems just don't work well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2163" target="_blank">00:36:03.320</a></span> | <span class="t">But you can build something that is no more complex than the model that I've just explained now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2170" target="_blank">00:36:10.320</a></span> | <span class="t">that does work pretty well by making it a multi-layer stacked LSTM neural machine translation system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2180" target="_blank">00:36:20.320</a></span> | <span class="t">So therefore, the picture looks like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2183" target="_blank">00:36:23.320</a></span> | <span class="t">So we've got this multi-layer LSTM that's going through the source sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2189" target="_blank">00:36:29.320</a></span> | <span class="t">And so now at each point in time, we calculate a new hidden representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2195" target="_blank">00:36:35.320</a></span> | <span class="t">that rather than stopping there, we sort of feed it as the input into another layer of LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2202" target="_blank">00:36:42.320</a></span> | <span class="t">And we calculate in the standard way, it's new hidden representation and the output of it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2208" target="_blank">00:36:48.320</a></span> | <span class="t">we feed into a third layer of LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2210" target="_blank">00:36:50.320</a></span> | <span class="t">And so we run that right along.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2213" target="_blank">00:36:53.320</a></span> | <span class="t">And so our representation of the source sentence from our encoder is then this stack of three hidden layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2224" target="_blank">00:37:04.320</a></span> | <span class="t">And then that we use to then feed in as the initial as the initial hidden layer into then sort of generating translations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2240" target="_blank">00:37:20.320</a></span> | <span class="t">or for training the model of comparing to losses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2243" target="_blank">00:37:23.320</a></span> | <span class="t">So this is kind of what the picture of a LSTM encoder decoder neural machine translation system really looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2253" target="_blank">00:37:33.320</a></span> | <span class="t">So in particular, you know, to give you some idea of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2260" target="_blank">00:37:40.320</a></span> | <span class="t">So a 2017 paper by Denny Brits and others that what they found was that for the encoder RNN,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2269" target="_blank">00:37:49.320</a></span> | <span class="t">it worked best if it had two to four layers and four layers was best for the decoder RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2278" target="_blank">00:37:58.320</a></span> | <span class="t">And the details here, like for a lot of neural nets, depends so much on what you're doing and how much data you have and things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2287" target="_blank">00:38:07.320</a></span> | <span class="t">But, you know, as rules of thumb to have in your head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2290" target="_blank">00:38:10.320</a></span> | <span class="t">it's almost invariably the case that having a two layer LSTM works a lot better than having a one layer LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2300" target="_blank">00:38:20.320</a></span> | <span class="t">After that, things become much less clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2303" target="_blank">00:38:23.320</a></span> | <span class="t">You know, it's not so infrequent that if you try three layers, it's a fraction better than two, but not really.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2310" target="_blank">00:38:30.320</a></span> | <span class="t">And if you try four layers, it's actually getting worse again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2313" target="_blank">00:38:33.320</a></span> | <span class="t">You know, it depends on how much data, etc. you have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2317" target="_blank">00:38:37.320</a></span> | <span class="t">At any rate, it's normally very hard with the kind of model architecture that I just showed back here to get better results with more than four layers of LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2330" target="_blank">00:38:50.320</a></span> | <span class="t">Normally, to do deeper LSTM models and get even better results,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2337" target="_blank">00:38:57.320</a></span> | <span class="t">you have to be adding extra skip connections of the kind that I talked about at the very end of the last class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2347" target="_blank">00:39:07.320</a></span> | <span class="t">Next week, John is going to talk about transformer based networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2352" target="_blank">00:39:12.320</a></span> | <span class="t">In contrast, for fairly fundamental reasons, they're typically much deeper, but we'll leave discussing them until we get on further.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2371" target="_blank">00:39:31.320</a></span> | <span class="t">So that was how we trained the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2374" target="_blank">00:39:34.320</a></span> | <span class="t">So let's just go a bit more through what the possibilities are for decoding and explore a more complex form of decoding than we've looked at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2385" target="_blank">00:39:45.320</a></span> | <span class="t">The simplest way to decode is the one that we presented so far, so that we have our LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2393" target="_blank">00:39:53.320</a></span> | <span class="t">We start, generate a hidden state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2396" target="_blank">00:39:56.320</a></span> | <span class="t">It has a probability distribution over words and you choose the most probable one, the argmax, and you say he and you copy it down and you repeat over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2407" target="_blank">00:40:07.320</a></span> | <span class="t">So doing this is referred to as greedy decoding, taking the most probable word on each step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2414" target="_blank">00:40:14.320</a></span> | <span class="t">And it's sort of the obvious thing to do and doesn't seem like it could be a bad thing to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2421" target="_blank">00:40:21.320</a></span> | <span class="t">But it turns out that it actually can be a fairly problematic thing to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2427" target="_blank">00:40:27.320</a></span> | <span class="t">And the idea of that is that, you know, with greedy decoding, you're sort of taking locally what seems the best choice and then you're stuck with it and you have no way to undo decisions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2441" target="_blank">00:40:41.320</a></span> | <span class="t">So if these examples have been using this sentence about he hit me with a pie going from translating from French to English.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2450" target="_blank">00:40:50.320</a></span> | <span class="t">So, you know, if you start off and you say, OK, ill, the first word in the translation should be he.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2458" target="_blank">00:40:58.320</a></span> | <span class="t">That looks good. But then you and then you say, well, hit, I'll generate hit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2465" target="_blank">00:41:05.320</a></span> | <span class="t">Then somehow the model thinks that the most likely next word is hit after hit is ah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2471" target="_blank">00:41:11.320</a></span> | <span class="t">And there are lots of reasons it could think so, because after hit, most commonly there's a direct object noun and, you know, he hit a car, he hit a roadblock.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2484" target="_blank">00:41:24.320</a></span> | <span class="t">Right. So that's pretty sounds pretty likely. But, you know, once you've generated it, there's no way to go backwards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2492" target="_blank">00:41:32.320</a></span> | <span class="t">And so you just have to keep on going from there and you may not be able to generate the translation you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2500" target="_blank">00:41:40.320</a></span> | <span class="t">At best, you can generate he hit a pie. Oops, something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2507" target="_blank">00:41:47.320</a></span> | <span class="t">So we'd like to be able to explore a bit more in generating our translations. And, well, you know, what could we do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2518" target="_blank">00:41:58.320</a></span> | <span class="t">Well, you know, I sort of mentioned this before looking at the statistical MT models overall, what we'd like to do is find translations that maximize the probability of Y given X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2534" target="_blank">00:42:14.320</a></span> | <span class="t">And at least if we know what the length of that translation is, we can do that as a product of generating a word at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2543" target="_blank">00:42:23.320</a></span> | <span class="t">And so to have a full model, we also have to have a probability distribution over how long the translation length would be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2551" target="_blank">00:42:31.320</a></span> | <span class="t">So we could say this is the model and let's, you know, generate and score all possible sequences Y using this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2562" target="_blank">00:42:42.320</a></span> | <span class="t">And that's where that then requires generating an exponential number of translations and is far, far too expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2572" target="_blank">00:42:52.320</a></span> | <span class="t">So beyond greedy decoding, the most important method that is used and you'll see lots of places is something called beam search decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2583" target="_blank">00:43:03.320</a></span> | <span class="t">And so this isn't what you all, well, any kind of machine translation is one place where it's commonly used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2591" target="_blank">00:43:11.320</a></span> | <span class="t">But this isn't a method that's specific to machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2595" target="_blank">00:43:15.320</a></span> | <span class="t">You find lots of other places, including all other kinds of sequence to sequence models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2601" target="_blank">00:43:21.320</a></span> | <span class="t">It's not the only other decoding method. Once when we got on to the language generation class, we'll see a couple more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2608" target="_blank">00:43:28.320</a></span> | <span class="t">But this is sort of the next one that you should know about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2612" target="_blank">00:43:32.320</a></span> | <span class="t">So beam search's idea is that you're going to keep some hypotheses to make it more likely that you'll find good generation while keeping the search tractable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2627" target="_blank">00:43:47.320</a></span> | <span class="t">So what we do is check, choose a beam size. And for neural MT, the beam size is normally fairly small, something like five to ten.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2636" target="_blank">00:43:56.320</a></span> | <span class="t">And at each step of the decoder, we're going to keep track of the K most probable partial translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2643" target="_blank">00:44:03.320</a></span> | <span class="t">So initial sub sequences of what we're generating, which we call hypotheses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2650" target="_blank">00:44:10.320</a></span> | <span class="t">So a hypothesis, which is then sort of the prefix of a translation, has a score, which is this log probability up to what's been generated so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2661" target="_blank">00:44:21.320</a></span> | <span class="t">So we can generate that in the typical way using our conditional language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2666" target="_blank">00:44:26.320</a></span> | <span class="t">So as written, all of the scores are negative. And so the least negative one, the highest probability one, is the best one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2676" target="_blank">00:44:36.320</a></span> | <span class="t">So what we want to do is search for high probability hypotheses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2683" target="_blank">00:44:43.320</a></span> | <span class="t">So this is a heuristic method. It's not guaranteed to find the highest probability decoding, but at least it gives you more of a shot than simply doing greedy decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2695" target="_blank">00:44:55.320</a></span> | <span class="t">So let's go through an example to see how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2701" target="_blank">00:45:01.320</a></span> | <span class="t">So in this case, so I can fit it on a slide, the size of our beam is just two, though normally it would actually be a bit bigger than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2711" target="_blank">00:45:11.320</a></span> | <span class="t">And the blue numbers are the scores of the prefixes. So these are these log probabilities of a prefix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2720" target="_blank">00:45:20.320</a></span> | <span class="t">So we start off with our start symbol and we're going to say, OK, what are the two most likely words to generate first, according to our language model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2732" target="_blank">00:45:32.320</a></span> | <span class="t">And so maybe the first two most likely words are he and I, and there are their log probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2740" target="_blank">00:45:40.320</a></span> | <span class="t">Then what we do next is for each of these K hypotheses, we find what are likely words to follow them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2750" target="_blank">00:45:50.320</a></span> | <span class="t">In particular, we find what are the K most likely words to follow each of those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2756" target="_blank">00:45:56.320</a></span> | <span class="t">So we might generate he hit, he struck, I was, I got.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2762" target="_blank">00:46:02.320</a></span> | <span class="t">OK, so at this point, it sort of looks like we're heading down what will turn into an exponential sized tree structure again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2772" target="_blank">00:46:12.320</a></span> | <span class="t">But what we do now is we work out the scores of each of these partial hypotheses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2780" target="_blank">00:46:20.320</a></span> | <span class="t">So we have four partial hypotheses. He hit, he struck, I was, I got.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2785" target="_blank">00:46:25.320</a></span> | <span class="t">And we can do that by taking the previous score that we had for the partial hypothesis and adding on the log probability of generating the next word, he hit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2799" target="_blank">00:46:39.320</a></span> | <span class="t">So this gives us scores for each hypothesis. And then we can say which of those two partial hypotheses, because our beam size K equals two, have the highest score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2811" target="_blank">00:46:51.320</a></span> | <span class="t">And so they are I was and he hit. So we keep those two and ignore the rest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2819" target="_blank">00:46:59.320</a></span> | <span class="t">And so then for those two, we're going to generate K hypotheses for the most likely following word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2828" target="_blank">00:47:08.320</a></span> | <span class="t">He hit, he hit me. I was hit. I was struck.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2833" target="_blank">00:47:13.320</a></span> | <span class="t">And again, now we want to find the K most likely hypotheses out of this full set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2841" target="_blank">00:47:21.320</a></span> | <span class="t">And so that's going to be he struck me and I was, oh no, he struck me and he hit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2849" target="_blank">00:47:29.320</a></span> | <span class="t">So we keep just those ones. And then for each of those, we generate the K most likely next words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2859" target="_blank">00:47:39.320</a></span> | <span class="t">Tart pie with on. And then again, we filter back down to size K by saying, OK, the two most likely things here are pie or with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2872" target="_blank">00:47:52.320</a></span> | <span class="t">So we continue working on those, generate things, find the two most likely, generate things, find the two most likely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2884" target="_blank">00:48:04.320</a></span> | <span class="t">And at this point, we would generate end of string and say, OK, we've got a complete hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2893" target="_blank">00:48:13.320</a></span> | <span class="t">He struck me with a pie and we could then trace back through the tree to obtain the full hypothesis for this sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2907" target="_blank">00:48:27.320</a></span> | <span class="t">So that's most of the algorithm. There's one more detail, which is the stopping criterion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2914" target="_blank">00:48:34.320</a></span> | <span class="t">So in greedy decoding, we usually decode until the model produces an end token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2922" target="_blank">00:48:42.320</a></span> | <span class="t">And when it produces the end token, we say we are done. In beam search decoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2929" target="_blank">00:48:49.320</a></span> | <span class="t">different hypotheses may produce end tokens on different time steps. And so we don't want to stop as soon as one path through the search tree has generated end,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2944" target="_blank">00:49:04.320</a></span> | <span class="t">because it could turn out there's a different path through the search tree, which will still prove to be better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2950" target="_blank">00:49:10.320</a></span> | <span class="t">So what we do is sort of put us put it aside as a complete hypothesis and continue exploring other hypotheses via our beam search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2961" target="_blank">00:49:21.320</a></span> | <span class="t">And so usually we will then either stop when we hit a cut off length or when we've completed n complete hypotheses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2976" target="_blank">00:49:36.320</a></span> | <span class="t">And then we'll look through the hypotheses that we've completed and say, which is the best one of those?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2983" target="_blank">00:49:43.320</a></span> | <span class="t">And that's the one we'll use. OK, so at that point, we have our list of completed hypotheses and we want to select the top one with the highest score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=2997" target="_blank">00:49:57.320</a></span> | <span class="t">Well, that's exactly what we've been computing. Each one has a probability that we've worked out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3006" target="_blank">00:50:06.320</a></span> | <span class="t">But it turns out that we might not want to use that just so naively because there turns out to be a kind of a systematic problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3015" target="_blank">00:50:15.320</a></span> | <span class="t">which is, you know, not as a theorem, but in general, longer hypotheses have lower scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3022" target="_blank">00:50:22.320</a></span> | <span class="t">So if you think about this as probabilities of successively generating each word that basically at each step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3030" target="_blank">00:50:30.320</a></span> | <span class="t">you're multiplying by another chance of generating the next word probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3035" target="_blank">00:50:35.320</a></span> | <span class="t">And commonly, those might be, you know, 10 to the minus three, 10 to the minus two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3040" target="_blank">00:50:40.320</a></span> | <span class="t">So just from the length of the sentence, your probabilities are getting much lower the longer that they go on in a way that appears to be unfair.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3050" target="_blank">00:50:50.320</a></span> | <span class="t">Since although in some sense, extremely long sentences aren't as likely as short ones, they're not less likely by that much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3058" target="_blank">00:50:58.320</a></span> | <span class="t">A lot of the time we produce long sentences. So, for example, you know, a newspaper, the median length of sentences is over 20.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3069" target="_blank">00:51:09.320</a></span> | <span class="t">So you wouldn't want to be having a decoding model when translating news articles that sort of says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3076" target="_blank">00:51:16.320</a></span> | <span class="t">"Oh, just generate two word sentences. They're just way higher probability according to my language model."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3082" target="_blank">00:51:22.320</a></span> | <span class="t">So the commonest way of dealing with that is that we normalise by length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3088" target="_blank">00:51:28.320</a></span> | <span class="t">So if we're working in log probabilities, that means taking, dividing through by the length of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3095" target="_blank">00:51:35.320</a></span> | <span class="t">And then you have a per word log probability score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3100" target="_blank">00:51:40.320</a></span> | <span class="t">And, you know, you can argue that this isn't quite right in some theoretical sense, but in practice, it works pretty well and it's very commonly used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3111" target="_blank">00:51:51.320</a></span> | <span class="t">Neural translation has proven to be much, much better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3117" target="_blank">00:51:57.320</a></span> | <span class="t">I'll show you a couple of statistics about that in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3121" target="_blank">00:52:01.320</a></span> | <span class="t">It has many advantages. It gives better performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3126" target="_blank">00:52:06.320</a></span> | <span class="t">The translations are better. In particular, they're more fluent because neural language models produce much more fluent sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3136" target="_blank">00:52:16.320</a></span> | <span class="t">But also they much better use context because neural language models, including conditional neural language models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3146" target="_blank">00:52:26.320</a></span> | <span class="t">give us a very good way of conditioning on a lot of context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3150" target="_blank">00:52:30.320</a></span> | <span class="t">In particular, we can just run a long encoder and condition on the previous sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3157" target="_blank">00:52:37.320</a></span> | <span class="t">or we can translate words well in context by making use of neural context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3164" target="_blank">00:52:44.320</a></span> | <span class="t">Neural models better understand phrase similarities and phrases that mean approximately the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3173" target="_blank">00:52:53.320</a></span> | <span class="t">And then the technique of optimizing all parameters of the model end to end in a single large neural network is just proved to be a really powerful idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3186" target="_blank">00:53:06.320</a></span> | <span class="t">So previously, a lot of the time people were building separate components and tuning them individually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3194" target="_blank">00:53:14.320</a></span> | <span class="t">which just meant that they weren't actually optimal when put into a much bigger system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3200" target="_blank">00:53:20.320</a></span> | <span class="t">So really a hugely powerful guiding idea in neural network land is if you can sort of build one huge network and just optimize the entire thing end to end,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3211" target="_blank">00:53:31.320</a></span> | <span class="t">that will give you much better performance and component wise systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3216" target="_blank">00:53:36.320</a></span> | <span class="t">We'll come back to the costs of that later in the course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3222" target="_blank">00:53:42.320</a></span> | <span class="t">The models are also actually great in other ways. They actually require much less human effort to build.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3228" target="_blank">00:53:48.320</a></span> | <span class="t">There's no feature engineering. There's in general no language specific components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3234" target="_blank">00:53:54.320</a></span> | <span class="t">You're using the same method for all language pairs. Of course, it's rare for things to be perfect in every way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3243" target="_blank">00:54:03.320</a></span> | <span class="t">So neural machine translation systems also have some disadvantages compared to the older statistical machine translation systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3252" target="_blank">00:54:12.320</a></span> | <span class="t">They're less interpretable. It's harder to see why they're doing what they're doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3258" target="_blank">00:54:18.320</a></span> | <span class="t">Before you could actually look at phrase tables and they were useful. So they're hard to debug.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3263" target="_blank">00:54:23.320</a></span> | <span class="t">They also tend to be sort of difficult to control. So compared to anything like writing rules,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3272" target="_blank">00:54:32.320</a></span> | <span class="t">you can't really give much specification as if you like to say, oh, I'd like my translations to be more casual or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3281" target="_blank">00:54:41.320</a></span> | <span class="t">It's hard to know what they'll generate. So there are various safety concerns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3288" target="_blank">00:54:48.320</a></span> | <span class="t">I'll show a few examples of that in just a minute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3292" target="_blank">00:54:52.320</a></span> | <span class="t">But first, before doing that quickly, how do we evaluate machine translation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3299" target="_blank">00:54:59.320</a></span> | <span class="t">The best way to evaluate machine translation is to show a human being who's fluent in the source and target languages,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3308" target="_blank">00:55:08.320</a></span> | <span class="t">the sentences, and get them to give judgment on how good a translation it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3315" target="_blank">00:55:15.320</a></span> | <span class="t">But that's expensive to do and might not even be possible if you don't have the right human beings around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3323" target="_blank">00:55:23.320</a></span> | <span class="t">So a lot of work was put into finding automatic methods of scoring translations that were good enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3330" target="_blank">00:55:30.320</a></span> | <span class="t">And the most famous method of doing that is what's called BLUE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3335" target="_blank">00:55:35.320</a></span> | <span class="t">And the way you do BLUE is you have a human translation or several human translations of the source sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3345" target="_blank">00:55:45.320</a></span> | <span class="t">and you're comparing a machine generated translation to those pre-given human written translations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3353" target="_blank">00:55:53.320</a></span> | <span class="t">And you score them for similarity by calculating N-gram precisions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3359" target="_blank">00:55:59.320</a></span> | <span class="t">i.e. words that overlap between the computer and human written translation, bigrams, trigrams, and four-grams,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3368" target="_blank">00:56:08.320</a></span> | <span class="t">and then working out a geometric average between overlaps of N-grams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3375" target="_blank">00:56:15.320</a></span> | <span class="t">Plus, there's a penalty for too short system translations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3379" target="_blank">00:56:19.320</a></span> | <span class="t">So BLUE has proven to be a really useful measure, but it's an imperfect measure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3385" target="_blank">00:56:25.320</a></span> | <span class="t">But commonly, there are many valid ways to translate a sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3389" target="_blank">00:56:29.320</a></span> | <span class="t">And so there's some luck as to whether the human written translations you have happen to correspond to what might be a good translation from the system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3403" target="_blank">00:56:43.320</a></span> | <span class="t">There's more to say about the details of BLUE and how it's implemented,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3408" target="_blank">00:56:48.320</a></span> | <span class="t">but you're going to see all of that doing assignment four, because you will be building your machine translation systems and evaluating with them with the BLUE algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3421" target="_blank">00:57:01.320</a></span> | <span class="t">And there are full details about BLUE in the assignment handout.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3426" target="_blank">00:57:06.320</a></span> | <span class="t">But at the end of the day, BLUE gives a score between zero and 100, where your score is 100 if you're exactly producing one of the human written translations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3438" target="_blank">00:57:18.320</a></span> | <span class="t">and zero if there's not even a single unigram that overlaps between the two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3445" target="_blank">00:57:25.320</a></span> | <span class="t">With that rather brief intro, I wanted to show you sort of what happened in machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3455" target="_blank">00:57:35.320</a></span> | <span class="t">So machine translation with statistical models, phrase based statistical machine translation that I showed at the beginning of the class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3463" target="_blank">00:57:43.320</a></span> | <span class="t">had been going on since the mid 2000s decade, and it had produced sort of semi good results of the kind that are in Google Translate in those days.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3474" target="_blank">00:57:54.320</a></span> | <span class="t">But by the time you'd entered the 2010s, basically progress in statistical machine translation had stalled and you are getting barely any increase over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3490" target="_blank">00:58:10.320</a></span> | <span class="t">And most of the increase in time you were getting over time was simply because you're training your models on more data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3499" target="_blank">00:58:19.320</a></span> | <span class="t">In those years, around the early 2010s, the big hope that most people had.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3509" target="_blank">00:58:29.320</a></span> | <span class="t">Someone asked, what is the y-axis here? This y-axis is this BLUE score that I told you about on the previous slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3516" target="_blank">00:58:36.320</a></span> | <span class="t">In the early 2010s, the big hope that most people in the machine translation field had was,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3523" target="_blank">00:58:43.320</a></span> | <span class="t">well, if we built a more complex kind of machine translation model that knows about the syntactic structure of languages,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3531" target="_blank">00:58:51.320</a></span> | <span class="t">that makes use of tools like dependency parsers, we'll be able to build much better translations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3537" target="_blank">00:58:57.320</a></span> | <span class="t">And so those are the purple systems here, which I haven't described at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3543" target="_blank">00:59:03.320</a></span> | <span class="t">But it's sort of as the years went by, it was pretty obvious that that barely seemed to help.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3552" target="_blank">00:59:12.320</a></span> | <span class="t">And so then in the mid 2010s, so in 2014, was the first modern attempt to build a neural network for machine translation as an encoder decoder model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3567" target="_blank">00:59:27.320</a></span> | <span class="t">And by the time it was sort of evaluated in Bake-offs in 2015, it wasn't as good as what had been built up over the preceding decade.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3576" target="_blank">00:59:36.320</a></span> | <span class="t">But it was already getting pretty good. But what was found was that these neural models just really opened up a whole new pathway to start building much,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3586" target="_blank">00:59:46.320</a></span> | <span class="t">much better machine translation systems. And since then, things have just sort of taken off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3593" target="_blank">00:59:53.320</a></span> | <span class="t">And year by year, neural machine translation systems are getting much better and far better than anything we had preceding that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3602" target="_blank">01:00:02.320</a></span> | <span class="t">So for, you know, at least the early part of the application of deep learning and natural language processing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3612" target="_blank">01:00:12.320</a></span> | <span class="t">neural machine translation was the huge, big success story.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3617" target="_blank">01:00:17.320</a></span> | <span class="t">In the last few years, when we've had models like GPT-2 and GPT-3 and other huge neural models like BERT, improving web search,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3630" target="_blank">01:00:30.320</a></span> | <span class="t">you know, it's a bit more complex, but this was the first area where there was a neural network which was hugely better than what it preceded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3640" target="_blank">01:00:40.320</a></span> | <span class="t">and was actually solving a practical problem that lots of people in the world need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3647" target="_blank">01:00:47.320</a></span> | <span class="t">And it was stunning with the speed at which success was achieved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3654" target="_blank">01:00:54.320</a></span> | <span class="t">So 2014 were the first, what I call here, fringe research attempts to build a neural machine translation system,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3665" target="_blank">01:01:05.320</a></span> | <span class="t">meaning that three or four people who were working on neural network models thought,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3672" target="_blank">01:01:12.320</a></span> | <span class="t">oh, why don't we see if we can use one of these to translate, learn to translate sentences where they weren't really people with a background in machine translation at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3681" target="_blank">01:01:21.320</a></span> | <span class="t">But success was achieved so quickly that within two years time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3688" target="_blank">01:01:28.320</a></span> | <span class="t">Google had switched to using neural machine translation for most languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3696" target="_blank">01:01:36.320</a></span> | <span class="t">And by a couple of years later after that, essentially anybody who does machine translation is now deploying live neural machine translation systems and getting much, much better results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3710" target="_blank">01:01:50.320</a></span> | <span class="t">So that was sort of just an amazing technological transition that for the preceding decade,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3717" target="_blank">01:01:57.320</a></span> | <span class="t">the big statistical machine translation systems like the previous generation of Google Translate had literally been built up by hundreds of engineers over years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3728" target="_blank">01:02:08.320</a></span> | <span class="t">But comparatively small group of deep learning people in a few months with a small amount of code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3738" target="_blank">01:02:18.320</a></span> | <span class="t">And hopefully you'll even get a sense of this doing assignment four,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3742" target="_blank">01:02:22.320</a></span> | <span class="t">were able to build neural machine translation systems that proved to work much better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3750" target="_blank">01:02:30.320</a></span> | <span class="t">Does that mean that machine translation is solved? No.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3754" target="_blank">01:02:34.320</a></span> | <span class="t">There are still lots of difficulties which people continue to work on very actively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3759" target="_blank">01:02:39.320</a></span> | <span class="t">And you can see more about it in the Skynet Today article that's linked at the bottom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3764" target="_blank">01:02:44.320</a></span> | <span class="t">But, you know, there are lots of problems with out of vocabulary words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3768" target="_blank">01:02:48.320</a></span> | <span class="t">There are domain mismatches between the training and test data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3772" target="_blank">01:02:52.320</a></span> | <span class="t">So it might be trained mainly on newswire data, but you want to translate people's Facebook messages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3780" target="_blank">01:03:00.320</a></span> | <span class="t">There are still problems of maintaining context over longer text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3784" target="_blank">01:03:04.320</a></span> | <span class="t">We'd like to translate languages for which we don't have much data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3789" target="_blank">01:03:09.320</a></span> | <span class="t">And so these methods work by far the best when we have huge amounts of parallel data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3797" target="_blank">01:03:17.320</a></span> | <span class="t">Even our best multilayer LSTMs aren't that great at capturing sentence meaning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3804" target="_blank">01:03:24.320</a></span> | <span class="t">There are particular problems such as interpreting what pronouns refer to or in languages like Chinese or Japanese where there's often no pronoun present,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3816" target="_blank">01:03:36.320</a></span> | <span class="t">but there is an implied reference to some person working out how to translate that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3821" target="_blank">01:03:41.320</a></span> | <span class="t">For languages that have lots of inflectional forms of nouns, verbs and adjectives, these systems often get them wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3829" target="_blank">01:03:49.320</a></span> | <span class="t">So there's still tons of stuff to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3832" target="_blank">01:03:52.320</a></span> | <span class="t">So here's just sort of quick, funny examples of the kind of things that go wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3838" target="_blank">01:03:58.320</a></span> | <span class="t">Right. So if you ask to translate paper jam, Google Translate is deciding that this is a kind of jam, just like there's some raspberry jam and strawberry jam.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3854" target="_blank">01:04:14.320</a></span> | <span class="t">And so this becomes a jam of paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3858" target="_blank">01:04:18.320</a></span> | <span class="t">There are problems of agreement and choice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3863" target="_blank">01:04:23.320</a></span> | <span class="t">So if you have many languages don't distinguish gender.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3868" target="_blank">01:04:28.320</a></span> | <span class="t">And so the sentences are neutral between things are masculine or feminine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3875" target="_blank">01:04:35.320</a></span> | <span class="t">So Malay or Turkish are two well-known languages of that sort.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3879" target="_blank">01:04:39.320</a></span> | <span class="t">But what happens when that gets translated into English by Google Translate is that the English language model just kicks in and applies stereotypical biases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3892" target="_blank">01:04:52.320</a></span> | <span class="t">And so these gender neutral sentences get translated into she works as a nurse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3898" target="_blank">01:04:58.320</a></span> | <span class="t">He works as a programmer. So if you want to help solve this problem, all of you can help by using singular they in all contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3908" target="_blank">01:05:08.320</a></span> | <span class="t">When you're putting material online and that could then change the distribution of what's generated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3914" target="_blank">01:05:14.320</a></span> | <span class="t">But people also work on modeling improvements to try and avoid this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3920" target="_blank">01:05:20.320</a></span> | <span class="t">Here's one more example that's kind of funny.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3925" target="_blank">01:05:25.320</a></span> | <span class="t">People noticed a couple of years ago that if you choose one of the rarer languages that Google will translate, such as Somali, that you just write in some rubbish like ag, ag, ag, ag, frequently it had produced out of nowhere, prophetic and biblical texts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3951" target="_blank">01:05:51.320</a></span> | <span class="t">As the name of the Lord was written in the Hebrew language, it was written in the language of the Hebrew nation, which makes no sense at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3961" target="_blank">01:06:01.320</a></span> | <span class="t">We're about to see a bit more about why this happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3966" target="_blank">01:06:06.320</a></span> | <span class="t">But, but that was sort of a bit worrying this as far as I can see this problem is now fixed in 2021 I couldn't actually get Google Translate to generate examples like this anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3981" target="_blank">01:06:21.320</a></span> | <span class="t">But, you know, so there are lots of ways to keep on doing research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3987" target="_blank">01:06:27.320</a></span> | <span class="t">And empty is certainly is, you know, flagship tasks for NLP and deep learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=3993" target="_blank">01:06:33.320</a></span> | <span class="t">And it was a place where many of the innovations of deep learning NLP were pioneered and people continue to work hard on it people found many, many improvements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4007" target="_blank">01:06:47.320</a></span> | <span class="t">And actually for the last bit of the class in the minute, I'm going to present one huge improvement, which is so important that it's really come to dominate the whole of the recent field of neural, neural networks for NLP, and that's the idea of attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4026" target="_blank">01:07:06.320</a></span> | <span class="t">But before I get on to attention. I want to spend three minutes on our assignment for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4034" target="_blank">01:07:14.320</a></span> | <span class="t">So for assignment for this year. We've got a new version of the assignment, which we hope will be interesting, but it's also a real challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4045" target="_blank">01:07:25.320</a></span> | <span class="t">So for assignment for this year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4048" target="_blank">01:07:28.320</a></span> | <span class="t">We've decided to do Cherokee English machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4052" target="_blank">01:07:32.320</a></span> | <span class="t">So Cherokee is an endangered Native American language it has about 2000 fluent speakers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4059" target="_blank">01:07:39.320</a></span> | <span class="t">It's an extremely low resource language so it's just there isn't much written Cherokee data available period, and particularly, there's not a lot of parallel sentences between Cherokee and English.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4074" target="_blank">01:07:54.320</a></span> | <span class="t">So here's the answer to the Google's freaky prophetic translations for languages for which there isn't much parallel data available, commonly, the biggest place where you can get parallel data is from Bible translations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4094" target="_blank">01:08:14.320</a></span> | <span class="t">So, you can have your own personal choice wherever it is over the map as to where you stand with respect to religion, but the fact of the matter is, if you work on indigenous languages what you very, very quickly find is that a lot of the work that's done on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4115" target="_blank">01:08:35.320</a></span> | <span class="t">data on indigenous languages and a lot of the material that is available in written form for many indigenous languages is Bible translations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4127" target="_blank">01:08:47.320</a></span> | <span class="t">Yeah. Okay, so this is what Cherokee looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4133" target="_blank">01:08:53.320</a></span> | <span class="t">And so you can see that the writing system has a mixture of things that look like English letters, and then all sorts of letters that don't. And so here's the initial bit of a story long ago was seven boys who used to spend all their time down by the townhouse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4152" target="_blank">01:09:12.320</a></span> | <span class="t">and this is a piece of parallel data that we can learn from. So, the Cherokee writing system has 85 letters, and the reason why it has so many letters, is that each of these letters actually represents a syllable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4169" target="_blank">01:09:29.320</a></span> | <span class="t">So many languages of the world have strict consonant vowel syllable structure so you have words like rata pair or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4180" target="_blank">01:09:40.320</a></span> | <span class="t">But what Cherokee.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4182" target="_blank">01:09:42.320</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4183" target="_blank">01:09:43.320</a></span> | <span class="t">And another language like that's Hawaiian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4187" target="_blank">01:09:47.320</a></span> | <span class="t">Each of the letters represents a combination of a consonant and a vowel, and that's the set of those, you then get 17 by five gives you 85 letters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4201" target="_blank">01:10:01.320</a></span> | <span class="t">Yeah, so being able to do this assignment big thanks to people from University of North Carolina Chapel Hill, who've provided the resources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4212" target="_blank">01:10:12.320</a></span> | <span class="t">We're using for this assignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4215" target="_blank">01:10:15.320</a></span> | <span class="t">You can do quite a lot of languages on Google Translate Cherokee is not a language that Google offers on Google Translate so we can see how far we can get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4226" target="_blank">01:10:26.320</a></span> | <span class="t">But we have to be modest in our expectations because it's hard to build a very good MT system with only a fairly limited amount of data, so we'll see how far we can get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4238" target="_blank">01:10:38.320</a></span> | <span class="t">And then the flip side which is for you students doing the assignment, the advantage of having not too much data is that your models will train relatively quickly, so it'll actually have less troubles than we did last year with people's models taking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4254" target="_blank">01:10:54.320</a></span> | <span class="t">hours to train.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4255" target="_blank">01:10:55.320</a></span> | <span class="t">As the assignment deadline closed in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4259" target="_blank">01:10:59.320</a></span> | <span class="t">There's a couple more words about Cherokee so we have some idea what we're talking about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4265" target="_blank">01:11:05.320</a></span> | <span class="t">Cherokee originally lived in Western North Carolina, and Tennessee, Eastern Tennessee.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4273" target="_blank">01:11:13.320</a></span> | <span class="t">They then sort of got shunted southwest from that. And then in particular, for those of you who went to American high schools and paid attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4286" target="_blank">01:11:26.320</a></span> | <span class="t">You might remember discussion of the Trail of Tears, when a lot of the Native Americans from the southeast of the US, got forcibly shoved a long way further west.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4299" target="_blank">01:11:39.320</a></span> | <span class="t">And so, most Cherokee now live in Oklahoma, though there are some that are in North Carolina. The writing system that I showed on this previous slide. It was invented by a Cherokee man Sequoia, that's a drawing of him there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4319" target="_blank">01:11:59.320</a></span> | <span class="t">And that was actually kind of incredible thing so he started off a literate and worked out how to write or produce a writing system that would be good for Cherokee and given that it has this consonant vowel structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4339" target="_blank">01:12:19.320</a></span> | <span class="t">He chose a syllabary, which turned out to be a good choice. So here's, here's a neat historical fact. So in the 1830s and 1840s, the percentage of Cherokee that were literate in Cherokee written like this was actually higher than the percentage of white people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4362" target="_blank">01:12:42.320</a></span> | <span class="t">in the southeastern United States at that point in time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4367" target="_blank">01:12:47.320</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4369" target="_blank">01:12:49.320</a></span> | <span class="t">Before time disappears.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4372" target="_blank">01:12:52.320</a></span> | <span class="t">Oops, time has almost disappeared. I'll just start to say, and then I'll have to do a bit more of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4379" target="_blank">01:12:59.320</a></span> | <span class="t">I'll have to do a bit more of this next time that we okay. Right. So the final idea that's really important for sequence to sequence models is the idea of attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4390" target="_blank">01:13:10.320</a></span> | <span class="t">So, we had this model of doing sequence to sequence models such as the neural machine translation. And the problem with this architecture is that we have this one hidden state, which has to encode all the information about the source sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4413" target="_blank">01:13:33.320</a></span> | <span class="t">So it acts as a kind of an information bottleneck. And that's all the information that the generation is conditioned on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4422" target="_blank">01:13:42.320</a></span> | <span class="t">Well, I didn't already mention one idea last time of how to get more information, where I said, look, maybe you could kind of average all of the vectors of the source to get a sentence representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4435" target="_blank">01:13:55.320</a></span> | <span class="t">You know, that method turns out to be better for things like sentiment analysis and not so good for machine translation, where the order of words is very important to preserve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4447" target="_blank">01:14:07.320</a></span> | <span class="t">So it's, it seems like we would do better if somehow we could get more information from the source sentence while we're generating the translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4460" target="_blank">01:14:20.320</a></span> | <span class="t">And in some sense, this just corresponds to what a human translator does, right? If you're a human translator, you read the sentence that you're meant to translate, and you maybe start translating a few words, but then you look back at the source sentence to see what else was in it and translate some more words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4478" target="_blank">01:14:38.320</a></span> | <span class="t">So very quickly after the first neural machine translation systems, people came up with the idea of maybe we could build a better neural MT model that did that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4490" target="_blank">01:14:50.320</a></span> | <span class="t">And that's the idea of attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4494" target="_blank">01:14:54.320</a></span> | <span class="t">So the core idea is on each step of the decoder, we're going to use a direct link between the encoder and the decoder that will allow us to focus on a particular word or words in the source sequence and use it to help us generate what words come next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4519" target="_blank">01:15:19.320</a></span> | <span class="t">And I'll just go through now, showing you the pictures of what attention does. And then at the start of next time, we'll go through the equations in more detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4532" target="_blank">01:15:32.320</a></span> | <span class="t">We generate, we use our encoder just as before and generate our representations, feed in our conditioning as before and say we're starting our translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4545" target="_blank">01:15:45.320</a></span> | <span class="t">But at this point, we take this hidden representation and say, I'm going to use this hidden representation to look back at the source to get information directly from it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4558" target="_blank">01:15:58.320</a></span> | <span class="t">So what I will do is I will compare the hidden state of the decoder with the hidden state of the encoder at each position and generate an attention score, which is a kind of similarity score like a dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4578" target="_blank">01:16:18.320</a></span> | <span class="t">And then based on those attention scores, I'm going to calculate a probability distribution as to by using a softmax as usual to say which of these encoder states is most like my decoder state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4599" target="_blank">01:16:39.320</a></span> | <span class="t">And so we'll be training the model here to be saying, well, probably you should translate the first word of the sentence first. So that's where the attention should be placed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4609" target="_blank">01:16:49.320</a></span> | <span class="t">So then based on this attention distribution, which is a probability distribution coming out of the softmax, we're going to generate a new attention output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4624" target="_blank">01:17:04.320</a></span> | <span class="t">And so this attention output is going to be an average of the hidden states of the encoder model, but it's going to be a weighted average based on our attention distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4638" target="_blank">01:17:18.320</a></span> | <span class="t">And so we're then going to take that attention output, combine it with the hidden state of the decoder RNN, and together the two of them are then going to be used to predict via a softmax what word to generate first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4659" target="_blank">01:17:39.320</a></span> | <span class="t">And we hope to generate he.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4662" target="_blank">01:17:42.320</a></span> | <span class="t">And then at that point, we sort of chug along and keep doing these same kind of computations at each position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4673" target="_blank">01:17:53.320</a></span> | <span class="t">There's a little side note here that says, sometimes we take the attention output from the previous step and also feed into the decoder along with the usual decoder input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4684" target="_blank">01:18:04.320</a></span> | <span class="t">So we're taking this attention output and actually feeding it back in to the hidden state calculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4691" target="_blank">01:18:11.320</a></span> | <span class="t">And that can sometimes improve performance. And we actually have that trick in the assignment four system, and you can try it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4700" target="_blank">01:18:20.320</a></span> | <span class="t">Okay, so we generate along and generate our whole sentence in this manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4707" target="_blank">01:18:27.320</a></span> | <span class="t">And that's proven to be a very effective way of getting more information from the source sentence more flexibly to allow us to generate a good translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4720" target="_blank">01:18:40.320</a></span> | <span class="t">I'll stop here for now. And at the start of next time, I'll finish this off by going through the actual equations for how attention works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4728" target="_blank">01:18:48.320</a></span> | <span class="t">Thanks for watching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wzfWHP6SXxY&t=4730" target="_blank">01:18:50.320</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>