<html><head><title>RT-X and the Dawn of Large Multimodal Models: Google Breakthrough and 160-page Report Highlights</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>RT-X and the Dawn of Large Multimodal Models: Google Breakthrough and 160-page Report Highlights</h2><a href="https://www.youtube.com/watch?v=GZdytTKeGYM"><img src="https://i.ytimg.com/vi/GZdytTKeGYM/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./GZdytTKeGYM.html">Whisper Transcript</a> | <a href="./transcript_GZdytTKeGYM.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Just as I was reaching the finishing pages of this epic 168 page report on GPT Vision,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=7" target="_blank">00:00:07.020</a></span> | <span class="t">which showcased unexpected abilities, novel use cases and predictions for the future,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=11" target="_blank">00:00:11.880</a></span> | <span class="t">Google dropped their colossal RTX Endeavor. And the data Google used with over 500 skills</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=19" target="_blank">00:00:19.180</a></span> | <span class="t">and 150,000 tasks is open source. I've picked out over 75 highlights from both papers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=25" target="_blank">00:00:25.980</a></span> | <span class="t">which I read in full, and I'll also bring in an exclusive report from the information to give us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=31" target="_blank">00:00:31.520</a></span> | <span class="t">an idea of what is next. By the end of the video, I want you to have a great sense for what GPT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=37" target="_blank">00:00:37.940</a></span> | <span class="t">with Vision can do today and a more acute awareness of what is coming in Vision, Video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=43" target="_blank">00:00:43.320</a></span> | <span class="t">and Robotics tomorrow. But let's start with this Google DeepMind report released just a few hours</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=49" target="_blank">00:00:49.460</a></span> | <span class="t">ago. Essentially, what it showed is that you could create a general purpose robot from data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=55" target="_blank">00:00:55.720</a></span> | <span class="t">from the data that you have in your computer. And that's what I'm going to do today. So let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=55" target="_blank">00:00:55.960</a></span> | <span class="t">start with the first report from diverse robotic tasks. These were data sets from different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=59" target="_blank">00:00:59.960</a></span> | <span class="t">universities on different continents. And Google wanted to see if this diverse data could improve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=66" target="_blank">00:01:06.100</a></span> | <span class="t">their famous RT2 model. I talked a lot more about RT2 in the video you can see on the screen,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=71" target="_blank">00:01:11.560</a></span> | <span class="t">but essentially it was trained on web data as well as robotics data. That meant that it could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=76" target="_blank">00:01:16.620</a></span> | <span class="t">understand questions like pick up the extinct animal. But the RTX series is another step up,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=82" target="_blank">00:01:22.020</a></span> | <span class="t">even though it comes just two months later. The report highlighted that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=85" target="_blank">00:01:25.940</a></span> | <span class="t">conventionally, robotic learning methods train a separate model for every application,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=90" target="_blank">00:01:30.340</a></span> | <span class="t">every robot and even every environment. Their big finding was that training a single model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=95" target="_blank">00:01:35.620</a></span> | <span class="t">on that diverse data enabled that robot to outperform even the specialist robots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=101" target="_blank">00:01:41.600</a></span> | <span class="t">The improved version of RT1 became RT1X and RT2 became RT2X. But here you can see RT1X</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=110" target="_blank">00:01:50.440</a></span> | <span class="t">out-competing specialist models, weirdly aside from this wiping robot at Berkeley.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=115" target="_blank">00:01:55.920</a></span> | <span class="t">In a range of domains. You've got kitchen manipulation, cable routing, door opening,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=120" target="_blank">00:02:00.800</a></span> | <span class="t">and many more tasks that I'll get to in a second. The paper even demonstrated applicability to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=125" target="_blank">00:02:05.960</a></span> | <span class="t">robot arms and even quadrupeds. Think four-legged dog-like robots. And here you can see how it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=132" target="_blank">00:02:12.260</a></span> | <span class="t">improved RT2, which was already a big improvement on RT1. And as Google says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=138" target="_blank">00:02:18.080</a></span> | <span class="t">our results suggest that co-training with data from other platforms imbues RT2X with additional skills</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=145" target="_blank">00:02:25.900</a></span> | <span class="t">that were not present in the original dataset, enabling it to perform novel tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=150" target="_blank">00:02:30.540</a></span> | <span class="t">Apparently it couldn't do things like this before. Move the apple between the can and the orange,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=156" target="_blank">00:02:36.580</a></span> | <span class="t">or move the apple near but not on top of the cloth, or move the apple on top of the pot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=163" target="_blank">00:02:43.440</a></span> | <span class="t">This gives you a taste of the kind of skills they incorporated. Picking, moving, pushing, placing, sliding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=169" target="_blank">00:02:49.480</a></span> | <span class="t">putting, navigating, separating, pointing, and on and on. I like that at the end we have assembling and turning on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=175" target="_blank">00:02:55.880</a></span> | <span class="t">The paper draws the analogy with large language models that are trained on massive web-scale text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=181" target="_blank">00:03:01.880</a></span> | <span class="t">data. And those models tend to outperform systems that are only trained on narrow task-specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=187" target="_blank">00:03:07.500</a></span> | <span class="t">datasets. Well, same thing now with robotics. That's why I call this the GPT-2 moment for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=193" target="_blank">00:03:13.740</a></span> | <span class="t">robotics. And the paper says even if such robotic datasets in their current size and coverage are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=199" target="_blank">00:03:19.240</a></span> | <span class="t">insufficient to attain the impressive generalization results that have been demonstrated by LLMs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=205" target="_blank">00:03:25.860</a></span> | <span class="t">in the future the union of such data can potentially provide this kind of coverage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=212" target="_blank">00:03:32.080</a></span> | <span class="t">One way to put it would be to think of how general and multi-skilled GPT-4 is in language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=217" target="_blank">00:03:37.840</a></span> | <span class="t">From coding to poetry and mathematics and more. Now imagine that, but with robotic skills.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=223" target="_blank">00:03:43.540</a></span> | <span class="t">Now one question you might have is how would data on folding clothes help with pushing an apple?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=229" target="_blank">00:03:49.620</a></span> | <span class="t">Well they say that unlike most prior works, we directly train our policy on all of this X in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=235" target="_blank">00:03:55.840</a></span> | <span class="t">body-mode data without any mechanisms to reduce the embodiment gap. They didn't try any big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=241" target="_blank">00:04:01.020</a></span> | <span class="t">translation between domains or breaking down the problem into sub-aspects. They did however put the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=246" target="_blank">00:04:06.720</a></span> | <span class="t">input image data into a common resolution and unified the action vectors across these seven</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=252" target="_blank">00:04:12.920</a></span> | <span class="t">dimensions. Now that 55 billion parameter number and the fact that it comes from the Pali model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=258" target="_blank">00:04:18.280</a></span> | <span class="t">that's undergirding RT2X is the perfect segue to GPT vision. After all, OpenAI directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=265" target="_blank">00:04:25.820</a></span> | <span class="t">compared their GPT-4 vision model to Pali 17 billion which was a precursor model to Pali X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=273" target="_blank">00:04:33.580</a></span> | <span class="t">And you might notice that at least for visual question answering, the Pali model, that's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=278" target="_blank">00:04:38.780</a></span> | <span class="t">precursor 17 billion parameter model, outperformed GPT-4 vision. So everything you're about to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=285" target="_blank">00:04:45.100</a></span> | <span class="t">from the 168 page GPT-4 vision report actually represents the lower bound of current frontier capability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=293" target="_blank">00:04:53.100</a></span> | <span class="t">To recap, that's GPT-4 vision being beefed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=295" target="_blank">00:04:55.800</a></span> | <span class="t">by Pali 17 billion parameters, which is beaten by Pali X 55 billion parameters, which has now been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=303" target="_blank">00:05:03.300</a></span> | <span class="t">incorporated into RT2 the robot. And all of that is not even to bring in OpenAI Gobi model or Google</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=310" target="_blank">00:05:10.620</a></span> | <span class="t">Gemini, which I'll talk about later. So the main focus of the video, the dawn of large multimodal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=317" target="_blank">00:05:17.160</a></span> | <span class="t">models. A huge 160 plus page report from Microsoft. It was released just a few days ago and to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=325" target="_blank">00:05:25.780</a></span> | <span class="t">honest, it's a bit of a surprise to me that they've actually released an entire video on its own.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=327" target="_blank">00:05:27.780</a></span> | <span class="t">I'm going to give you all of the highlights here in this video. So please do leave a like or subscribe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=332" target="_blank">00:05:32.420</a></span> | <span class="t">if you find it helpful. For all of the fascinating demos you're about to see, Microsoft says that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=338" target="_blank">00:05:38.100</a></span> | <span class="t">carefully controlled both the images and text to prevent them from being seen during GPT-4V training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=344" target="_blank">00:05:44.900</a></span> | <span class="t">The images were either not accessible online or had a timestamp beyond April 2023. Their headline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=351" target="_blank">00:05:51.380</a></span> | <span class="t">finding was that GPT vision shows impressive human level capabilities across the entire world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=355" target="_blank">00:05:55.760</a></span> | <span class="t">And that's because they're not just a tool for testing and testing for the best results, but they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=357" target="_blank">00:05:57.760</a></span> | <span class="t">are a tool for testing and testing for the best results. And they're also a tool for testing and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=359" target="_blank">00:05:59.760</a></span> | <span class="t">testing for the best results. So they're a tool for testing and testing for the best results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=361" target="_blank">00:06:01.760</a></span> | <span class="t">So they're a tool for testing and testing for the best results. And they're a tool for testing and testing for the best results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=363" target="_blank">00:06:03.760</a></span> | <span class="t">So they're a tool for testing and testing for the best results. And they're a tool for testing and testing for the best results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=365" target="_blank">00:06:05.760</a></span> | <span class="t">So they're a tool for testing and testing for the best results. And they're a tool for testing and testing for the best results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=367" target="_blank">00:06:07.760</a></span> | <span class="t">So they're a tool for testing and testing for the best results. And they're a tool for testing and testing for the best results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=369" target="_blank">00:06:09.760</a></span> | <span class="t">So they're a tool for testing and testing for the best results. And they're a tool for testing and testing for the best results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=371" target="_blank">00:06:11.760</a></span> | <span class="t">and by the end they're proposing agent structures and testing for self-consistency it gets pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=376" target="_blank">00:06:16.800</a></span> | <span class="t">wild but it's time for the first demo they showed gpt vision this table with the drinks that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=382" target="_blank">00:06:22.080</a></span> | <span class="t">had ordered and they took a photo of the menu they asked how much should i pay for the beer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=386" target="_blank">00:06:26.980</a></span> | <span class="t">on the table according to the price on the menu and gpt vision got it we're starting a little slow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=391" target="_blank">00:06:31.940</a></span> | <span class="t">here but imagine you're drunk on the beach and you don't even know what you've ordered this could be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=396" target="_blank">00:06:36.140</a></span> | <span class="t">useful next was gpt vision putting the information from a driver's license into json format now first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=402" target="_blank">00:06:42.800</a></span> | <span class="t">time round it wasn't perfect listing his hair color as non-applicable when the license says</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=408" target="_blank">00:06:48.060</a></span> | <span class="t">brown but there are ways to improve performance as we'll see later in fact the first way of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=412" target="_blank">00:06:52.800</a></span> | <span class="t">improving performance is right now that's using chain of thought chain of thought is basically a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=417" target="_blank">00:06:57.840</a></span> | <span class="t">way of getting the model to put out its intermediate reasoning often by using a phrase like let's think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=423" target="_blank">00:07:03.620</a></span> | <span class="t">step by step as you can see here the first time around the model is using chain of thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=426" target="_blank">00:07:06.120</a></span> | <span class="t">and this time it couldn't identify that there's 11 apples and actually even when they used let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=431" target="_blank">00:07:11.220</a></span> | <span class="t">think step by step it still got it wrong what about let's count the apples row by row nope</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=436" target="_blank">00:07:16.140</a></span> | <span class="t">it got the right final answer but got the rows mixed up they tried some other prompts but finally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=441" target="_blank">00:07:21.480</a></span> | <span class="t">settled on this one you are an expert in counting things in the image and this time it got it right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=446" target="_blank">00:07:26.700</a></span> | <span class="t">now all those new methods you've been seeing like llms as prompt optimizers or prompt reader it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=452" target="_blank">00:07:32.100</a></span> | <span class="t">looks like they are going to be equally applicable to gpt vision in fact after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=456" target="_blank">00:07:36.100</a></span> | <span class="t">that example they say that throughout the paper we're going to employ that technique of calling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=460" target="_blank">00:07:40.240</a></span> | <span class="t">it an expert across various scenarios for better performance but here is one of the big revelations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=466" target="_blank">00:07:46.040</a></span> | <span class="t">from the paper for me the paper says that this particular ability isn't seen in existing models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=471" target="_blank">00:07:51.520</a></span> | <span class="t">and that is being able to follow pointers that might be circles squares or even arrows drawn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=477" target="_blank">00:07:57.960</a></span> | <span class="t">on a diagram and the amazing thing is this seems to work better than giving gpt vision coordinates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=483" target="_blank">00:08:03.920</a></span> | <span class="t">the researchers from microsoft drew these out of the paper and they said that they were able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=486" target="_blank">00:08:06.080</a></span> | <span class="t">use the same technique to draw the same number of arrows onto the photo putting the label object one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=489" target="_blank">00:08:09.260</a></span> | <span class="t">and object two and gpt vision analyzes those objects perfectly but there was something that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=494" target="_blank">00:08:14.100</a></span> | <span class="t">i noticed which is that technically the green arrow is pointing to the pavement and the red</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=500" target="_blank">00:08:20.220</a></span> | <span class="t">arrow is pointing to the table i should say that the arrows end at those places and instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=506" target="_blank">00:08:26.020</a></span> | <span class="t">interpreting the literal end of the arrows it sussed what the human meant it meant the nearest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=512" target="_blank">00:08:32.060</a></span> | <span class="t">big object the glass bottle and the beer i know that's something small but i thought it was a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=516" target="_blank">00:08:36.060</a></span> | <span class="t">find that really impressive. The next big finding was that in context few shot learning is still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=521" target="_blank">00:08:41.440</a></span> | <span class="t">really crucial even for vision models. In context means as part of the prompt not in the pre-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=527" target="_blank">00:08:47.320</a></span> | <span class="t">and few shot just means giving a few examples before you ask your key question. And the proof</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=532" target="_blank">00:08:52.580</a></span> | <span class="t">of concept examples that you're about to see they say vividly demonstrate the rising significance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=537" target="_blank">00:08:57.580</a></span> | <span class="t">of in context few shot learning for achieving improved performance with large multimodal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=543" target="_blank">00:09:03.600</a></span> | <span class="t">models. I'm going to speed through this example just so you get the gist. Essentially they asked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=548" target="_blank">00:09:08.080</a></span> | <span class="t">it to read the speed on this speed meter. It gets it wrong and even when they ask it to think step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=553" target="_blank">00:09:13.400</a></span> | <span class="t">by step it still gets it wrong. Then they gave it instructions and it still got it wrong. They</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=558" target="_blank">00:09:18.240</a></span> | <span class="t">tried loads of things but it just couldn't seem to do it. Even when they gave it one example one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=563" target="_blank">00:09:23.480</a></span> | <span class="t">shot it still got it wrong. You can see in the prompt they gave a correct worked example and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=569" target="_blank">00:09:29.000</a></span> | <span class="t">then asked the question again but no it said just passing 70 miles an hour.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=573" target="_blank">00:09:33.440</a></span> | <span class="t">But finally two shot with two worked examples it then gets it right. Next they showed that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=579" target="_blank">00:09:39.640</a></span> | <span class="t">model could recognize celebrities and there was one particularly interesting example of Jensen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=584" target="_blank">00:09:44.940</a></span> | <span class="t">Huang. He's the CEO of Nvidia which produces the GPUs that went into training GPT-4 vision. Anyway</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=591" target="_blank">00:09:51.600</a></span> | <span class="t">it could apparently recognize its own ingredients saying he's likely holding a GPU. Next they had it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=597" target="_blank">00:09:57.720</a></span> | <span class="t">recognizing landmarks even if they were at weird angles or at night. It could recognize dishes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=603" target="_blank">00:10:03.180</a></span> | <span class="t">even if they were at weird angles or at night. It could recognize dishes even if they were at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=603" target="_blank">00:10:03.420</a></span> | <span class="t">even if they had toppings and condiments. It also did really pretty well in medical image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=608" target="_blank">00:10:08.900</a></span> | <span class="t">understanding identifying what was wrong with this particular foot. You can see it also working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=613" target="_blank">00:10:13.660</a></span> | <span class="t">with a CT scan. Of course before we get too excited our old friend hallucination is still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=619" target="_blank">00:10:19.420</a></span> | <span class="t">there. It described a bridge overpass that I frankly can't see at all. It skipped over North</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=624" target="_blank">00:10:24.920</a></span> | <span class="t">Carolina entirely when asked about the states shown on this map. And it also gets seemingly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=630" target="_blank">00:10:30.360</a></span> | <span class="t">random numbers wrong. Take this table where it noted that the state of North Carolina was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=633" target="_blank">00:10:33.400</a></span> | <span class="t">down almost every number correctly down to three decimal places but then for some reason the 15</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=639" target="_blank">00:10:39.640</a></span> | <span class="t">million 971 880 became 15 971 421 actually i've just noticed while filming that that's the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=650" target="_blank">00:10:50.200</a></span> | <span class="t">ending as the profit in the next column so maybe there was a reason but it's still pretty random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=655" target="_blank">00:10:55.400</a></span> | <span class="t">point is you still can't fully rely on the outputs and it seems to me that in figure 36 there was a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=661" target="_blank">00:11:01.000</a></span> | <span class="t">mistake that even the researchers didn't notice if i'm right that shows how pernicious these mistakes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=666" target="_blank">00:11:06.120</a></span> | <span class="t">can be the researchers say that the model not only understands the program in the given flowchart but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=671" target="_blank">00:11:11.720</a></span> | <span class="t">also translates the details to a python code if you go to figure 36 you see this flowchart and it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=678" target="_blank">00:11:18.040</a></span> | <span class="t">was asked can you translate the flowchart to a python code you can see the flowchart and you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=683" target="_blank">00:11:23.640</a></span> | <span class="t">see the code now obviously it's impressive that it can even attempt to do this but that code is dodgy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=689" target="_blank">00:11:29.000</a></span> | <span class="t">taking a string as input essentially the code is not the code that you can use to do this but it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=690" target="_blank">00:11:30.600</a></span> | <span class="t">essentially a bunch of letters instead of a floating point number now the goal was to print</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=695" target="_blank">00:11:35.640</a></span> | <span class="t">the larger number and that's what gpt vision says in the explanation that it will do but that input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=700" target="_blank">00:11:40.920</a></span> | <span class="t">problem that i mentioned means that it returned three when comparing three with twenty the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=706" target="_blank">00:11:46.680</a></span> | <span class="t">also called this answer correct when averaging these two numbers which comes out to 76.555</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=712" target="_blank">00:11:52.840</a></span> | <span class="t">that rounds to 76 dollars and 56 cents not 76 dollars and 55 cents now you might say all of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=720" target="_blank">00:12:00.200</a></span> | <span class="t">is pedantic but the errors keep coming the paper says that in the bottom row of figure 37 gpt vision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=726" target="_blank">00:12:06.600</a></span> | <span class="t">shows a clear understanding of both x and y axis and explains the key insight presented in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=732" target="_blank">00:12:12.600</a></span> | <span class="t">chart go to figure 37 and you get this chart the key insight to me from this chart is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=738" target="_blank">00:12:18.840</a></span> | <span class="t">publishing bad okay or pretty good papers makes almost no difference it's only when they get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=744" target="_blank">00:12:24.280</a></span> | <span class="t">very creative original and good that it makes lots of impact on your career now gpt vision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=749" target="_blank">00:12:29.800</a></span> | <span class="t">does say that publishing a bad paper has little impact on your career and a creative paper has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=755" target="_blank">00:12:35.800</a></span> | <span class="t">significant impact correct but then it says the impact of the paper on a person's career increases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=761" target="_blank">00:12:41.000</a></span> | <span class="t">as the quality of the paper improves now while that's technically correct it misses the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=766" target="_blank">00:12:46.120</a></span> | <span class="t">insight basically a flat line and then a sudden upward turn anyway loads of errors but let's focus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=772" target="_blank">00:12:52.360</a></span> | <span class="t">on the potential use cases because we must remember that gpt vision is still capable of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=778" target="_blank">00:12:58.760</a></span> | <span class="t">like this a gpt vision is still capable of things like this a gpt vision is still capable of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=779" target="_blank">00:12:59.400</a></span> | <span class="t">a guy on twitter or x daniel lit said this i've been told gpt4 with code interpreter is good at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=785" target="_blank">00:13:05.800</a></span> | <span class="t">math he was taking the mickey because the output is this can you compute the seventh root of three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=791" target="_blank">00:13:11.640</a></span> | <span class="t">to the power of seven now the seventh root is the opposite of raising a number to the power of seven</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=796" target="_blank">00:13:16.440</a></span> | <span class="t">so the answer should be three but it said the seventh root of three to the seven is approximately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=801" target="_blank">00:13:21.240</a></span> | <span class="t">4.2 but then someone else put that image into gpt vision and said why is this tweet funny and gpt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=809" target="_blank">00:13:29.000</a></span> | <span class="t">vision was able to pick up on the humor the humor in this tweet arises from the mathematical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=815" target="_blank">00:13:35.080</a></span> | <span class="t">inconsistency the question posed to gpt4 with code interpreter asked for the seventh root of three to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=820" target="_blank">00:13:40.600</a></span> | <span class="t">the seven mathematically the seventh root of three to the seven is simply three it corrected its own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=825" target="_blank">00:13:45.560</a></span> | <span class="t">error the incongruity between the question and the answer creates a comedic effect so it was not only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=831" target="_blank">00:13:51.480</a></span> | <span class="t">able to correct its error it was able to see the humor in someone pointing that out but then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=836" target="_blank">00:13:56.520</a></span> | <span class="t">someone went even more meta pasting this entire thing into gpt vision saying why is this analysis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=843" target="_blank">00:14:03.560</a></span> | <span class="t">funny and then gpt vision is able to summarize the entire situation describing gpt4 itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=850" target="_blank">00:14:10.520</a></span> | <span class="t">as analyzing its own mistake the irony lies in gpt4 critiquing its own incorrect answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=856" target="_blank">00:14:16.680</a></span> | <span class="t">i would have given it bonus points if it said and here i am talking about it but let's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=860" target="_blank">00:14:20.920</a></span> | <span class="t">ask for too much this is already impressive speaking of gpt vision being a bit more critical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=866" target="_blank">00:14:26.200</a></span> | <span class="t">it was asked this question how many families are earning more than 13 000 and owns more than two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=872" target="_blank">00:14:32.440</a></span> | <span class="t">cars the question is very ambiguous it gave no time period earning more than 13 000 a month a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=878" target="_blank">00:14:38.600</a></span> | <span class="t">year and it talked about owning the cars when the table was just about vehicles per family of course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=884" target="_blank">00:14:44.200</a></span> | <span class="t">a family might have a vehicle without owning it i'd have loved it if gpt vision picked up on this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=889" target="_blank">00:14:49.720</a></span> | <span class="t">ambiguity in the question and then it was asked how many families are earning more than 13 000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=890" target="_blank">00:14:50.520</a></span> | <span class="t">the question and asked clarifying questions instead it outputted a reasonable answer based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=895" target="_blank">00:14:55.960</a></span> | <span class="t">on a few assumptions and the paper marked it as correct they did show it analyzing a full academic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=902" target="_blank">00:15:02.120</a></span> | <span class="t">paper and making only a few mistakes though and to me that shows some pretty crazy potential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=908" target="_blank">00:15:08.120</a></span> | <span class="t">especially for the next model down the line imagine a model being able to read all ai related</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=913" target="_blank">00:15:13.480</a></span> | <span class="t">papers in any language and synthesize some of the findings that's when things might get a little out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=920" target="_blank">00:15:20.120</a></span> | <span class="t">control i do think the paper gets a little bit over eager in places though for example here it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=925" target="_blank">00:15:25.320</a></span> | <span class="t">fed gpt vision a series of frames depicting a player taking a penalty as you can see in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=931" target="_blank">00:15:31.320</a></span> | <span class="t">last frame the ball is in the net gpt vision correctly said the ball was not blocked by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=937" target="_blank">00:15:37.640</a></span> | <span class="t">goalkeeper the conclusion of the paper is that this demonstrates cause and effect reasoning by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=942" target="_blank">00:15:42.760</a></span> | <span class="t">determining whether the ball was blocked based on the goalkeeper ball interaction but to me it could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=947" target="_blank">00:15:47.640</a></span> | <span class="t">be simple memorization based on the goalkeeper interaction but to me it could be simple memorization based on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=949" target="_blank">00:15:49.720</a></span> | <span class="t">web scale of data it was trained on for example it might have seen many many images of a ball in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=955" target="_blank">00:15:55.160</a></span> | <span class="t">back of a net and it understands that those images correspond to a penalty not being saved you can let</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=960" target="_blank">00:16:00.760</a></span> | <span class="t">me know in the comments if you think this demonstrates a considerable level of sophistication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=966" target="_blank">00:16:06.120</a></span> | <span class="t">in the model's reasoning abilities i was really impressed by this though they sent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=970" target="_blank">00:16:10.360</a></span> | <span class="t">gpt vision a series of photos and highlighted one of the guys in the photo and it was able to deduce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=976" target="_blank">00:16:16.840</a></span> | <span class="t">that he is playfully pretending to punch the ball in the net and that's why i think this is a really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=979" target="_blank">00:16:19.320</a></span> | <span class="t">cool image of a ball in the net and it's really cool to see that he is playing a real punch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=983" target="_blank">00:16:23.640</a></span> | <span class="t">now i am sure that many models and even quite a few humans might think that these images depict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=989" target="_blank">00:16:29.400</a></span> | <span class="t">a real punch but if you look at it carefully it does seem like he's playing so that was really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=994" target="_blank">00:16:34.200</a></span> | <span class="t">impressive to me it could also identify south park characters just from ascii art that's despite it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=999" target="_blank">00:16:39.800</a></span> | <span class="t">not being able to generate good ascii art currently itself or maybe it can but the reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1005" target="_blank">00:16:45.320</a></span> | <span class="t">learning has drained it of that ability anyway it is able to read emotions of people from their faces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1008" target="_blank">00:16:48.920</a></span> | <span class="t">so if you one day approach a gpt vision model looking like this it's going to know what you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1014" target="_blank">00:16:54.600</a></span> | <span class="t">thinking i don't know if this quite counts as emotional intelligence or empathy though those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1019" target="_blank">00:16:59.880</a></span> | <span class="t">were some of the words used by the paper i did find it interesting though that they said that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1024" target="_blank">00:17:04.840</a></span> | <span class="t">understanding anger or and fear will be essential in use cases such as home robots i'm not sure if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1031" target="_blank">00:17:11.720</a></span> | <span class="t">they're anticipating many people being angry in awe or in fear of their home robot which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1037" target="_blank">00:17:17.160</a></span> | <span class="t">presumably they bought maybe it feels like they're just trying to get the robot to be angry and angry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1038" target="_blank">00:17:18.520</a></span> | <span class="t">maybe it finds faces easier to read than helmets because it says there are eight people in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1043" target="_blank">00:17:23.560</a></span> | <span class="t">image wearing helmets and as i speculated previously it is able to iterate on those prompts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1049" target="_blank">00:17:29.240</a></span> | <span class="t">it noticed how this output didn't match the original request have it look like a graphic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1054" target="_blank">00:17:34.680</a></span> | <span class="t">novel and then it suggested improvements to the prompt as i've said before imagine this combined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1059" target="_blank">00:17:39.800</a></span> | <span class="t">with dali 3 with constant iterations it might take a bit longer but only the output that gets 10 out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1065" target="_blank">00:17:45.640</a></span> | <span class="t">of 10 from gpt vision would be handed to you so if you're interested in learning more about gpt vision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1068" target="_blank">00:17:48.120</a></span> | <span class="t">you can check out the gpt vision website and the gpt vision website is a great place to start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1070" target="_blank">00:17:50.280</a></span> | <span class="t">some of you may know that steve wozniak proposed a somewhat peculiar test for agi could a machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1076" target="_blank">00:17:56.440</a></span> | <span class="t">enter the average american home and figure out how to make coffee as wikipedia says this has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1081" target="_blank">00:18:01.400</a></span> | <span class="t">not yet been completed but it might not be far away after all gpt vision was able to figure out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1088" target="_blank">00:18:08.040</a></span> | <span class="t">the buttons on a coffee machine and then it could work its way through a house via images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1094" target="_blank">00:18:14.280</a></span> | <span class="t">to enact a plan for example it wanted to go to the fridge and it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1097" target="_blank">00:18:17.720</a></span> | <span class="t">proposed a series of actions turn right and move forward toward the hallway then next when it was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1103" target="_blank">00:18:23.080</a></span> | <span class="t">in a different position it said i would now turn right and move toward the kitchen it goes on that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1108" target="_blank">00:18:28.680</a></span> | <span class="t">it would head toward the fridge and finally in this example it would now open the fridge door</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1113" target="_blank">00:18:33.720</a></span> | <span class="t">and retrieve the requested item now you might say oh that's all well and good having the plan</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1119" target="_blank">00:18:39.000</a></span> | <span class="t">and being able to use vision to propose a plan but that's still not the same as being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1123" target="_blank">00:18:43.480</a></span> | <span class="t">dexterous enough to actually pour the coffee let alone get out the cups from the fridge and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1127" target="_blank">00:18:47.320</a></span> | <span class="t">handle them but of course we started the video with the rtx series we're getting close to that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1133" target="_blank">00:18:53.240</a></span> | <span class="t">level of manipulation i could honestly see that task being achieved in the next three years or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1139" target="_blank">00:18:59.240</a></span> | <span class="t">perhaps even sooner if a team went straight out to achieve it next they showed gpt vision being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1145" target="_blank">00:19:05.000</a></span> | <span class="t">able to handle a computer screen it knew at least the general direction of where to click and what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1150" target="_blank">00:19:10.840</a></span> | <span class="t">to do next you can see it here via the researchers navigating google search and it's a very simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1156" target="_blank">00:19:16.920</a></span> | <span class="t">search it does still have problems with exact coordinates though so its clicks might be a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1161" target="_blank">00:19:21.960</a></span> | <span class="t">inaccurate also of course it still hallucinates here it is trying to read the news and it decides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1167" target="_blank">00:19:27.880</a></span> | <span class="t">to close the tab by clicking the x in the top right corner of course that would not just close</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1173" target="_blank">00:19:33.880</a></span> | <span class="t">the current tab it can also handle phone screens and even phone notifications though it does make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1179" target="_blank">00:19:39.400</a></span> | <span class="t">one key mistake the sender yyk hahaha has sent the message i see you are in seattle let's meet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1186" target="_blank">00:19:46.520</a></span> | <span class="t">up and gpt vision proposes this let's move my finger to the maps app icon and that will allow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1192" target="_blank">00:19:52.440</a></span> | <span class="t">me to search for a location in seattle and plan a meetup with the user clearly though here the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1197" target="_blank">00:19:57.800</a></span> | <span class="t">correct answer was to simply delete the message ain't nobody got time for that kind of thing it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1203" target="_blank">00:20:03.160</a></span> | <span class="t">can also watch videos if they're broken down frame by frame it correctly identified here that this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1208" target="_blank">00:20:08.920</a></span> | <span class="t">a recipe tutorial for strawberry stuffed french toast however with gemini being trained on youtube</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1215" target="_blank">00:20:15.320</a></span> | <span class="t">according to the info i've just shared with you in the description below if you're interested in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1216" target="_blank">00:20:16.120</a></span> | <span class="t">the information and openai already planning to follow up gpt vision with a model called gobi that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1222" target="_blank">00:20:22.840</a></span> | <span class="t">model by the way would be designed as multimodal from the start at that point when it can properly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1228" target="_blank">00:20:28.440</a></span> | <span class="t">ingest video data that's when image and video capabilities might really take off i can imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1234" target="_blank">00:20:34.360</a></span> | <span class="t">today with what we have already teachers having a self-monitored camera facing their whiteboard as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1239" target="_blank">00:20:39.800</a></span> | <span class="t">they write out their questions and answers and explanations gpt vision could be monitoring for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1245" target="_blank">00:20:45.160</a></span> | <span class="t">errors this could apply to any of the gpt vision models that we have already seen in the past</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1245" target="_blank">00:20:45.720</a></span> | <span class="t">this could apply particularly to primary education where teachers have to sometimes cover topics that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1250" target="_blank">00:20:50.600</a></span> | <span class="t">they're not fully familiar with with one click gpt vision could check for any mistakes and give you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1256" target="_blank">00:20:56.200</a></span> | <span class="t">feedback anyway i've just thought of that you let me know in the comments other use cases not covered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1261" target="_blank">00:21:01.160</a></span> | <span class="t">so far this is certainly a wild time and thank you so much for watching all the way to the end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1267" target="_blank">00:21:07.480</a></span> | <span class="t">if you've learned anything like i say please do leave a like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=GZdytTKeGYM&t=1270" target="_blank">00:21:10.600</a></span> | <span class="t">do check out my patreon if you're feeling extra generous and have a wonderful day</span></div></div></body></html>