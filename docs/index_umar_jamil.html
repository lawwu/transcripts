<html><head><title>Umar Jamil Transcripts</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><h1>Umar Jamil Transcripts</h1><table style="width:100%; border-collapse: collapse;"><a href="index.html">back to index</a><tr><th>Date</th><th>Title</th><th>Duration</th><th>Whisper Transcript</th><th>Transcript Only</th></tr><tr style="background-color: #f2f2f2;"><td>2023-04-16</td><td>Wav2Lip (generate talking avatar videos) - Paper reading and explanation</td><td>6 min</td><td><a href="./n9ILOE2kyB0.html">Whisper Transcript</a></td><td><a href="./transcript_n9ILOE2kyB0.html">Transcript Only</a></td></tr><tr style=""><td>2023-04-24</td><td>CLIP - Paper explanation (training and inference)</td><td>14 min</td><td><a href="./L3BTG8ETY_Y.html">Whisper Transcript</a></td><td><a href="./transcript_L3BTG8ETY_Y.html">Transcript Only</a></td></tr><tr style="background-color: #f2f2f2;"><td>2023-05-25</td><td>Coding a Transformer from scratch on PyTorch, with full explanation, training and inference.</td><td>179 min</td><td><a href="./ISNdQcPhsts.html">Whisper Transcript</a></td><td><a href="./transcript_ISNdQcPhsts.html">Transcript Only</a></td></tr><tr style=""><td>2023-05-28</td><td>Attention is all you need (Transformer) - Model explanation (including math), Inference and Training</td><td>58 min</td><td><a href="./bCz4OMemCcA.html">Whisper Transcript</a></td><td><a href="./transcript_bCz4OMemCcA.html">Transcript Only</a></td></tr><tr style="background-color: #f2f2f2;"><td>2023-06-07</td><td>Variational Autoencoder - Model, ELBO, loss function and maths explained easily!</td><td>27 min</td><td><a href="./iwEzwTTalbg.html">Whisper Transcript</a></td><td><a href="./transcript_iwEzwTTalbg.html">Transcript Only</a></td></tr><tr style=""><td>2023-07-04</td><td>How diffusion models work - explanation and code!</td><td>21 min</td><td><a href="./I1sPXkm2NH4.html">Whisper Transcript</a></td><td><a href="./transcript_I1sPXkm2NH4.html">Transcript Only</a></td></tr><tr style="background-color: #f2f2f2;"><td>2023-07-15</td><td>LongNet: Scaling Transformers to 1,000,000,000 tokens: Python Code + Explanation</td><td>29 min</td><td><a href="./nC2nU9j9DVQ.html">Whisper Transcript</a></td><td><a href="./transcript_nC2nU9j9DVQ.html">Transcript Only</a></td></tr><tr style=""><td>2023-07-25</td><td>LoRA: Low-Rank Adaptation of Large Language Models - Explained visually + PyTorch code from scratch</td><td>26 min</td><td><a href="./PXWYUTMt-AU.html">Whisper Transcript</a></td><td><a href="./transcript_PXWYUTMt-AU.html">Transcript Only</a></td></tr><tr style="background-color: #f2f2f2;"><td>2023-08-14</td><td>Segment Anything - Model explanation with code</td><td>42 min</td><td><a href="./eYhvJR4zFUM.html">Whisper Transcript</a></td><td><a href="./transcript_eYhvJR4zFUM.html">Transcript Only</a></td></tr><tr style=""><td>2023-08-24</td><td>LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU</td><td>70 min</td><td><a href="./Mn_9W1nCFLo.html">Whisper Transcript</a></td><td><a href="./transcript_Mn_9W1nCFLo.html">Transcript Only</a></td></tr><tr style="background-color: #f2f2f2;"><td>2023-09-03</td><td>Coding LLaMA 2 from scratch in PyTorch - KV Cache, Grouped Query Attention, Rotary PE, RMSNorm</td><td>184 min</td><td><a href="./oM4VmoabDAI.html">Whisper Transcript</a></td><td><a href="./transcript_oM4VmoabDAI.html">Transcript Only</a></td></tr><tr style=""><td>2023-09-27</td><td>Coding Stable Diffusion from scratch in PyTorch</td><td>303 min</td><td><a href="./ZBKpAp_6TGI.html">Whisper Transcript</a></td><td><a href="./transcript_ZBKpAp_6TGI.html">Transcript Only</a></td></tr><tr style="background-color: #f2f2f2;"><td>2023-10-26</td><td>BERT explained: Training, Inference,  BERT vs GPT/LLamA, Fine tuning, [CLS] token</td><td>54 min</td><td><a href="./90mGPxR2GgY.html">Whisper Transcript</a></td><td><a href="./transcript_90mGPxR2GgY.html">Transcript Only</a></td></tr><tr style=""><td>2023-11-27</td><td>Retrieval Augmented Generation (RAG) Explained: Embedding, Sentence BERT, Vector Database (HNSW)</td><td>49 min</td><td><a href="./rhZgXNdhWDY.html">Whisper Transcript</a></td><td><a href="./transcript_rhZgXNdhWDY.html">Transcript Only</a></td></tr><tr style="background-color: #f2f2f2;"><td>2023-12-11</td><td>Quantization explained with PyTorch - Post-Training Quantization, Quantization-Aware Training</td><td>50 min</td><td><a href="./0VdNflU08yA.html">Whisper Transcript</a></td><td><a href="./transcript_0VdNflU08yA.html">Transcript Only</a></td></tr><tr style=""><td>2023-12-19</td><td>Distributed Training with PyTorch: complete tutorial with cloud infrastructure and code</td><td>72 min</td><td><a href="./toUSzwR0EV8.html">Whisper Transcript</a></td><td><a href="./transcript_toUSzwR0EV8.html">Transcript Only</a></td></tr><tr style="background-color: #f2f2f2;"><td>2023-12-27</td><td>Mistral / Mixtral Explained: Sliding Window Attention, Sparse Mixture of Experts, Rolling Buffer</td><td>86 min</td><td><a href="./UiX8K-xBUpE.html">Whisper Transcript</a></td><td><a href="./transcript_UiX8K-xBUpE.html">Transcript Only</a></td></tr><tr style=""><td>2024-01-07</td><td>Mamba and S4 Explained: Architecture, Parallel Scan, Kernel Fusion, Recurrent, Convolution, Math</td><td>74 min</td><td><a href="./8Q_tqwpTpVU.html">Whisper Transcript</a></td><td><a href="./transcript_8Q_tqwpTpVU.html">Transcript Only</a></td></tr><tr style="background-color: #f2f2f2;"><td>2024-02-27</td><td>Reinforcement Learning from Human Feedback explained with math derivations and the PyTorch code.</td><td>135 min</td><td><a href="./qGyFrqc34yc.html">Whisper Transcript</a></td><td><a href="./transcript_qGyFrqc34yc.html">Transcript Only</a></td></tr><tr style=""><td>2024-04-14</td><td>Direct Preference Optimization (DPO) explained: Bradley-Terry model, log probabilities, math</td><td>48 min</td><td><a href="./hvGa5Mba4c8.html">Whisper Transcript</a></td><td><a href="./transcript_hvGa5Mba4c8.html">Transcript Only</a></td></tr><tr style="background-color: #f2f2f2;"><td>2024-05-11</td><td>Kolmogorov-Arnold Networks: MLP vs KAN, Math, B-Splines, Universal Approximation Theorem</td><td>75 min</td><td><a href="./-PFIkkwWdnM.html">Whisper Transcript</a></td><td><a href="./transcript_-PFIkkwWdnM.html">Transcript Only</a></td></tr></table></body></html>