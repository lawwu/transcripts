<html><head><title>MCP 201 | Code w/ Claude</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>MCP 201 | Code w/ Claude</h2><a href="https://www.youtube.com/watch?v=HNzH5Us1Rvg" target="_blank"><img src="https://i.ytimg.com/vi/HNzH5Us1Rvg/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>. Well, hello. My name is David. I'm a member of technical staff at Anthropic and one of the co-creators of MCP. Today I'm going to tell you a little bit more about the protocol and the things you can do just to give you an understanding of what there's more to the protocol than what most people use it for at the moment, which would be tools.</p><p>So really the goal today is to showcase you what the protocol is capable of and how you can use it in ways to build richer interactions with MCP clients that goes beyond the tool calling that most people are used to. And I will first go through all the different like what we call primitives, like ways for the servers to expose information to a client before we go into some of the bit more lesser known aspects of the protocol.</p><p>And then I want to talk a little bit about like how to build a really rich interaction before we take a little stab of what's coming next for MCP and how we bring MCP to the web. But to just get you started, I want to talk about one of the MCP primitives that servers can expose to MCP clients, but very few people know.</p><p>And those are called prompts. And what prompts are really are predefined templates for AI interactions. And that's to say it's a way for an MCP server to expose a set of texts, you know, like a prompt in a way that allows users to directly add this to the context window and see how they would use, for example, the MCP server you're building.</p><p>And they're really the two main use cases here is for you as an MCP server author to provide an example for that you can showcase to the user so that they know how to use the MCP server in the best way. Because realistically, you are the one who has built it.</p><p>You are the one who knows how to use it in the best possible way. And probably at the time you would release it are the one who has used it the most time. But since MCP prompts are also dynamic in a way, they're just code under the hood that are executed in MCP server, they allow you to do even richer things than that.</p><p>What you can do, and I want to showcase this in this scenario, is an MCP prompt that a user invokes in this Z editor here that will fetch directly GitHub comments into my context window. And so what you see me here doing is just basically put into the context window the comments from my pull request that I've written so that I can go and interact with it and have then the model go and help me apply the changes that's been requested to me or whatever I want to do.</p><p>And so this is really a way for exposing things that the user should directly interact and the user should directly want to put into the context window before it interacts with the yellow lamp. So it's different from that from tools where the model decides when to do it. This is what the user decides.</p><p>I want to add this to the context window. And if you look carefully, there's one additional thing that very, very few people know that you can do, and that is prompt completion. So if you have looked carefully, there was a way where it showcased quickly a pop-up of me selecting the pull requests that are available to myself.</p><p>And that is a way that you can -- that is a thing that you can provide as an MCP server author to build richer parameterized templates, for example. And this is exceptionally easy to do in the code. Like if you were in TypeScript, building a prompt that provides users with like such a template and have parameters for it and like auto-completion is nothing more than a few lines of code that cloud code together with cloud four can most of the time write basically for you.</p><p>And it's just that simple. It's a function for the completion and it's a function for generating the prompt. And so this is already like one of these primitives you can use to build an interaction for users with an MCP server, but it's just a little bit more richer than a tool call.</p><p>And a second one of these is something that we call resources. It's another primitive that an MCP server can expose to an MCP client. And while prompts are really focused on text snippets that a user can add into the context window, resources are about exposing raw data or content from a server.</p><p>And why would you want to do this? There are two ways why you want to do this. One thing is most of the clients today would allow you to add this raw content directly to the context window. So in that way, they're not that different from prompts. But it also allows application to do additional things to that raw data.</p><p>And that could be, for example, building embeddings around this data that server exposes, and then do retrieval augmented generation by adding to the context window the most appropriate things. And so this is an area that at the moment I feel is a bit underexplored. And I just want to quickly showcase you how resources work.</p><p>In this case, this is, again, one of these ways where an MCP client exposes a resource as a file-like object. And in this scenario here, we are exposing the database schema for a Postgres database as resources. And then you can add them in Cloud Desktop just like files. And that way you can tell Cloud, this is the tables I care about, and now please go ahead and visualize them.</p><p>And so in this scenario, what you're going to see is Cloud is going to go and write a beautiful diagram that visualizes the database schema for me. And I've exposed the schema via resources. There's a lot of unexplored space still here, again, if you go beyond just adding a file again and think about retrieval augmentation or any other thing the application might want to do.</p><p>And so those are two primitives. One is prompts, again, the things that a user interacts with. The second one is resources that the application interacts with. Then, of course, there should be a third one that you all are very familiar with, that I don't want to get into too much depth, because if you have built an MCP server, you probably have built it for exposing a tool.</p><p>And so tools are really these actions, of course, that can be invoked. That's like one of the, I think, most magical moment I feel when you build an MCP server is when the model for the first time invokes something that you care about, that you have built for and has this little impact on, you know, it might be like carrying a database for you or whatever it might be.</p><p>But this is, again, the thing that the model decides when to call to an action. And so these are three very basic primitives that the protocol exposes. And if you think carefully about these three primitives that I just showcased to you, there's a little bit of overlap about, like, how do you use, like, when do you use what, really?</p><p>And so there's something that we don't talk enough about, and it's somewhere buried in the specification language of the model context protocol, is what I call the interaction model. And I think showcasing it hopefully makes clear when you use what. Because the interaction model is built in such a way that you can expose the same data in three different ways, depending on when you want to have it show up.</p><p>And prompts, again, are these user-driven things. It's the thing the user invokes, adds to the context window. And the most common scenario where how you see these pop up is a slash command, an add command, something like that. Resources, on the other hand, are all application-driven. The application that uses the LLM, be it Cloud Desktop, be it VS Code, something like that, fully decides what it wants to do with that.</p><p>And then, lastly, tools are driven by the model. And between, you know, an AI application using a model and a user, we have all three parts that we eventually cover using these three basic primitives. And that allows you already to go to a little bit of a richer application and experience than what most people can currently do with tools.</p><p>Because you just have a way to interact with the user a bit more nuanced than if you just wait for this model to call the tool. But we can even go beyond that. Because while these basic primitives get us a little bit further than what we see most MCP servers do at the moment, there are even richer interactions that we want to enable.</p><p>And to make this a bit more understandable, here's really an example I want to give you that showcases this problem. So how can you build an MCP server, for example, that summarizes a discussion from an issue tracker? So on one side I can build an MCP server that exposes this kind of data, very simple.</p><p>And that's quite clear. But how do I do the summarization step? Because for the summarization step, I obviously need a model. And so one way to go and build this is you can build an MCP server that is this issue tracker server. And you have a choice here. You can bring your own SDK and call the model, have the model summarizes.</p><p>But then there's a little problem to that. And the problem is that the client has a model selected, be it like Claude or whatever else. But the server, the MCP server that you've built, it doesn't know what model the client has configured. And so you bring your own SDK off of a model provider, and be it the anthropic SDK, you still need them, like an API key that this user needs to provide.</p><p>And it gets very quickly, very awkward. And so MCP has a little hidden feature or a little primitive called sampling that allows a server to request a completion from the client. What does this mean? It means that the server can use a model independently from, like, don't having to provide an SDK itself, but asks the client which model you have configured.</p><p>And the client is the one providing the completion to the server. And what does this do? It does two things. First of all, it allows the client to get full control over the security, privacy, and the cost. So instead of having to provide an additional API key, you might tap into the subscription that your client might already have.</p><p>But it allows also a second part, which is that if you take multiple, if you chain MCP servers in an interesting way, it makes this whole pattern very recursive. And what do I mean by that? It's a bit abstract. You can take an MCP server that exposes a tool.</p><p>But during the tool execution, you might want to use more MCP servers downstream. And somewhere downstream in this, like, system, there might be then your Azure Tracker server that wants to go and have a completion. But using sampling, you can bubble up the request such that the client always stays in full control over the cost of the subscription, whatever you want to use.</p><p>It stays in full control of the privacy over the cost of this interaction and basically manages every interaction that an MCP server wants to do with a model. And that allows for very powerful chaining and it allows for more complex patterns that go already into ways of how you can build little MCP agents.</p><p>But that's sampling. Sampling at the moment is sadly, I think, one of the more exciting features, but also one of the features that's the least supported in clients. For our first-party products, we will bring sampling somewhere this year. And so then you can hopefully start building more exciting MCP servers.</p><p>And then there's the last primitive that I want to touch on that's also a bit more interesting. It's one of these things that, in retrospective, as one of the person who has built the protocol, I've probably named terribly, to be fair. I'm not very good at naming. You will see this throughout the talk probably.</p><p>But there's a thing called roots. And roots is also an interesting aspect. Because let's imagine I want to build today an MCP server that deals with my Git commands. I don't want to deal with Git. I don't want to do source control commands. I don't remember any of that.</p><p>I want to have an MCP server deal with this. So now I'm going to hook up an MCP server into my favorite IDE. But how does the IDE know, how does the MCP server know what are the open projects in the IDE? Because obviously I want to run the Git commands in the workspaces I have opened, right?</p><p>And so roots is a way for the server to inquire from the client, such as VS Code, for example, what are the projects you have opened, so that I can operate within only those directories that the server has opened. And I know where I want to execute my Git commands.</p><p>And this again is a feature that is not that widely used, but for example VS Code currently does support this. And so these are, you know, just all the big primitives that MCP offers. So we have five primitives, three on the server side, two on the client side. But how do you put it all together to actually build a rich interaction?</p><p>Because that's what we want. We want to build something for users that's a bit richer than just tool calling. And so let's take a look at how you will build a hypothetical MCP server that interacts with your favorite chat application, be that Discord, be it Slack. You could use prompts to give examples to users, such as, like, summarize this discussion.</p><p>And you can use completions with recent threats, users, or whatever you want them to expose. You can have additional prompts, like, what's new? What happened since yesterday? And so that's one way the user can just kickstart right away into using the server you've provided and get the ideas that you, how you intended it to be used.</p><p>And then you can use resources to directly list the channels, to expose recent threats that happen in the, you know, chat application, such that the MCP client can index it, deal with it in ways that it wants. And then, of course, last but not least, we still have our tools.</p><p>We have search, we have read channels, we have reading of threats, and we will use sampling to summarize a threat, for example, and really expose this. And that's a way to really build a much, much, much richer experience with MCP to use the full power that the protocol has to offer.</p><p>But this is just the beginning, because most of these experiences, if we build MCP servers so far, have been experiences that stayed local. Out of the 10,000 MCP servers the community has built over the last six to seven months, the vast majority are local experiences. But I think we can take the next step, and I think this is MCP's really big next thing, is bringing MCP servers away from the local experience to the web.</p><p>And so what does this mean? And so what does this mean? It means that instead of having an MCP server that is, you know, a Docker container or some form of local executable, it is nothing else but a website that your client can connect to and expose this MCP and you talk to.</p><p>But for that, we need two critical components: we need authorization and we need scaling. And in the most recent specification of MCP, we made a ton of changes towards this from the lessons we have learned and the feedback we honestly got from the community as well as key partners.</p><p>And we work closely, for example, with, like, people in the industry that worked on OAuth and other aspects to get this right. And so with authorization in MCP, what you can do is you can basically provide the private context of a user that might be behind an online account or something directly to the LLM application.</p><p>And it really enables MCP authors to bind the capability of the MCP servers to a user or an online account or something like that. And in order to do that, the way this currently has to work in MCP is that you do this by providing OAuth as the authorization layer.</p><p>And the MCP specification basically says you need to do OAuth 2.1, and that's a bit daunting because very few people know what OAuth 2.1 is. But OAuth 2.1 is usually just OAuth 2.0 with all the basic things you would do anyway, all the security considerations that people that have done OAuth telling you anyway to do.</p><p>So it's just OAuth 2.0, a little bit cleaned up, and you're probably already doing it if you're doing OAuth. And if you do implement this OAuth flow, you get two interesting patterns out of that. And the first one is the scenario of an MCP server in the web. And a good example of this is if you're, for example, a payment provider, and you have, you know, a website, payment.com, and I, as a user, have an online account there.</p><p>Now I, as the payment provider, can expose mcp.payment.com that the user can put into an MCP client, and the MCP client will do the OAuth flow. I log in as my account, and I know this is payment.com. I know this is the person that is my online account with the provider that I trust.</p><p>I don't trust some random Docker container running locally built by a third-party developer anymore. I trust the person I already trust with the data anyway and their developers. And on their development side, they can just, like, update this server as they want, and they don't have to wait for me to download a new, like, Docker image.</p><p>And so this is, I think, will be a really, really big step for enabling MCP servers to be exposed on the web and MCP clients to interact basically with all the online interactions that you already have. And here is just a small little example of this. In this scenario, we use Cloud AI integrations, which we launched earlier this month, to connect to a remote server and use this OAuth flow to log in our user to then have tools available that are aware of my data, that I care about it, that it is for me.</p><p>But it enables another aspect. It enables enterprises to smoothly integrate MCP into their ecosystem, how they usually build applications. And what does this mean? And what does this mean? It means that internally they can deploy an MCP server to their intranet or whatever, like, they use, and use an identity provider like Azure ID or Okta or whatever that central identity provider that you usually use for single sign-on.</p><p>And you can have that still exist and it will be the one that gives you the tokens that you require to interact with the MCP server. And that is a lot to say that what it ends up with is a very smooth integration. You're, as a development team internally, you're going to build an MCP server that you control, that you could control the deployment.</p><p>And the user just logs in in the morning with their normal SSO like they always would do. And any time they use an MCP server from them on out, they will just be logged in and have access to the data that, you know, that is their data that the company has for them.</p><p>And so this, I think, enables a new way that I've already seen some of the big enterprises do to build really vast systems of MCP servers that allow part of the company to build a server while the other part deals about the integrations. It really nicely separates integration builder and platform builders.</p><p>And then the second part that we require is scaling. And we just added a new thing called streamable HTTP, which is just to say, it's a lot of words to say, basically, we want MCP servers to scale similar to normal APIs. And it's as simple as that. You have, as a server author, you can choose to either return results directly, as you would be in a REST API, except that it's not quite just REST.</p><p>Or if you need to, you can open a stream and get richer interactions. So in the most simple way, you just want to provide a tool call result. You get a request, return application JSON, and off you go. End of the story. You close the connection. And the next connection come in and, you know, gets served by yet another Lambda function.</p><p>But if you need richer interactions, such as notification or features we talked about, like sampling, a request comes in, you start a stream, the client accepts the stream, and now you're being able to send additional things to the client before you're returning finally the result. And those authorization and scaling together is really the foundation to make MCP go from this local experience now to be truly a standard for how LLM applications will interact with the web.</p><p>And just to finish it all up, I just want to show you quickly about like what's coming for MCP in the next few months of some of the most important highlights. And the most important part is that we are starting to think more and more about agents. And there's a lot to do there.</p><p>There are asynchronous tasks that you, of course, want to run, things that are longer running, that are not just like a minute long, but maybe a few hours long. Tasks that an agent takes and that eventually I want to have a result for them. So we think a lot about that and we're going to work to build primitives for that into MCP in the near future.</p><p>The second part of that is elicitation, so really MCP server authors being able to ask for input from the user. And that is something that's going to land just about today or on Monday in the protocol. And then we're doing two additional things. We first and foremost are going to build an official registry to make sure there's a central place where you can find and publish to MCP servers so that we can really have one common place where we're going to look for these servers, but also allow agents to dynamically download servers and install them and use them.</p><p>And then, of course, we're thinking more about multi-modality. And that can be, for example, streaming of results. But that can have other aspects that I just don't want to go into details yet. And that's just the specification part. On the ecosystem part, we're going and having a lot of more things to go that we're doing at the moment.</p><p>We're adding a Ruby SDK that is donated by Shopify in the next few weeks. And the Google folks, the Google Go team, is currently building an official Go SDK for MCP. And so I just hope that I was able to give you a bit of a more in-depth view of what you could do with MCP if you used the full power of the protocol.</p><p>And with that, I think I'm a bit low on time, so I can't ask questions. We can't ask questions. We can't do Q&A. But just grab me afterwards, and I can be happy to answer around the hallway any questions you might have. So thank you so much. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. you you you you you</p></div></div></body></html>