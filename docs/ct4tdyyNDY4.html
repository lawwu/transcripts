<html><head><title>Stanford CS25: V2 I Robotics and Imitation Learning</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V2 I Robotics and Imitation Learning</h2><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4"><img src="https://i.ytimg.com/vi/ct4tdyyNDY4/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./ct4tdyyNDY4.html">Whisper Transcript</a> | <a href="./transcript_ct4tdyyNDY4.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">I'm really happy to be here. I guess to shortly introduce myself. My name is Ted Xiao. I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=13" target="_blank">00:00:13.580</a></span> | <span class="t">a senior research engineer at the Google Brain team. I've been on working on robotics now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=18" target="_blank">00:00:18.460</a></span> | <span class="t">for the past five years. I've touched upon a few topics including multitask learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=23" target="_blank">00:00:23.980</a></span> | <span class="t">reinforcement learning, and then lately just broadly thinking about how we can scale robots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=28" target="_blank">00:00:28.300</a></span> | <span class="t">to make sure that we can actually work in the wild in the real world. I guess today</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=33" target="_blank">00:00:33.060</a></span> | <span class="t">I'll be talking about quite a few different topics, but as a first preface, I guess the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=38" target="_blank">00:00:38.940</a></span> | <span class="t">first thing to know is that our team is pretty massive now. All of these projects are huge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=43" target="_blank">00:00:43.020</a></span> | <span class="t">collaborations with some products have more than 40 people working on these for many years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=48" target="_blank">00:00:48.240</a></span> | <span class="t">So these are large efforts and I'm just very fortunate to call myself to be on teams of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=52" target="_blank">00:00:52.500</a></span> | <span class="t">very smart people. Secondly, some of my takes are spicier or more controversial than others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=59" target="_blank">00:00:59.300</a></span> | <span class="t">So all of those opinions are definitely only my own and don't reflect those of Google or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=64" target="_blank">00:01:04.060</a></span> | <span class="t">anyone else on the team. So with that out of the way, yeah, welcome to my TEDx talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=76" target="_blank">00:01:16.040</a></span> | <span class="t">So I think maybe some of you have seen a lot of the cool robot learning videos out in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=81" target="_blank">00:01:21.820</a></span> | <span class="t">wild these days, but I am more excited than ever and it's not just hype I think. I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=87" target="_blank">00:01:27.180</a></span> | <span class="t">there's been a fundamental shift in how researcher and robotics view learning over the past two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=92" target="_blank">00:01:32.340</a></span> | <span class="t">years and I think the shift has a lot to do with all of the trends happening more broadly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=98" target="_blank">00:01:38.160</a></span> | <span class="t">in foundation modeling, in large-scale internet models, across different fields like language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=104" target="_blank">00:01:44.220</a></span> | <span class="t">audio, and so on. But I think my goal today is to convey to you why I am particularly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=110" target="_blank">00:01:50.280</a></span> | <span class="t">excited about this time today right now and why there's been a very fundamental one 80-degree</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=116" target="_blank">00:01:56.340</a></span> | <span class="t">paradigm shift I think across the robot learning field. And if you walk away from this talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=121" target="_blank">00:02:01.740</a></span> | <span class="t">with just one thing and that's you're slightly a bit more excited about robotics than you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=125" target="_blank">00:02:05.860</a></span> | <span class="t">were before or believe that the time is now for these robots to really start scaling exponentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=131" target="_blank">00:02:11.380</a></span> | <span class="t">and doing something really cool, I think then my talk will have succeeded. The talk will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=142" target="_blank">00:02:22.320</a></span> | <span class="t">have a few parts. We're going to start at a very high level and just talk about why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=146" target="_blank">00:02:26.920</a></span> | <span class="t">a foundation model for robotics at all, what that might look like, and the ingredients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=152" target="_blank">00:02:32.380</a></span> | <span class="t">and recipe for how we might get there. Then we'll dive into a few different works pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=158" target="_blank">00:02:38.000</a></span> | <span class="t">deeply that my team has been very proud of over the past year or two. And finally, we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=162" target="_blank">00:02:42.920</a></span> | <span class="t">go back to the high level and then zoom out and think about what's next for robot learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=169" target="_blank">00:02:49.780</a></span> | <span class="t">So why a foundation model for robotics? One second, let me try to hide this thing. No,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=179" target="_blank">00:02:59.980</a></span> | <span class="t">that's fine. I'll keep that bar there for now. But the top bar says why a foundation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=183" target="_blank">00:03:03.520</a></span> | <span class="t">model for robotics. Being coined here at Stanford, and I'll use the phrases internet scale model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=190" target="_blank">00:03:10.000</a></span> | <span class="t">foundation model, and large language model pretty interchangeably throughout. And I hope</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=193" target="_blank">00:03:13.600</a></span> | <span class="t">it's pretty clear. But generally, when I'm talking about these big monolithic beasts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=198" target="_blank">00:03:18.080</a></span> | <span class="t">that are training on tons of data, they have two very important properties that I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=202" target="_blank">00:03:22.640</a></span> | <span class="t">are quite nice. One is emergence. When very simple things kind of work at a small scale,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=210" target="_blank">00:03:30.220</a></span> | <span class="t">they get a ton better when you just scale things up more data, more compute larger models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=216" target="_blank">00:03:36.080</a></span> | <span class="t">And what we see here is that when these models even become good enough, the domain space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=221" target="_blank">00:03:41.700</a></span> | <span class="t">of what they're good at and able to do starts to go combinatorial even larger. And here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=226" target="_blank">00:03:46.560</a></span> | <span class="t">for these two points, I would like to suggest two blog posts I highly recommend. One is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=231" target="_blank">00:03:51.980</a></span> | <span class="t">from Jacob Steinhardt called more is different for AI. And this kind of links the phenomenon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=236" target="_blank">00:03:56.220</a></span> | <span class="t">that we see in other fields, like physics or biology, for example, individual water</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=241" target="_blank">00:04:01.380</a></span> | <span class="t">molecules will behave very differently and have very different, let's say electrostatic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=245" target="_blank">00:04:05.960</a></span> | <span class="t">forces, then they start to clump up clump up and start behaving as a liquid altogether.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=250" target="_blank">00:04:10.440</a></span> | <span class="t">We see this in herds of animal and flocking patterns, we see this in humans and economies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=254" target="_blank">00:04:14.760</a></span> | <span class="t">we see this all across different fields. And now even an AI, we see models that are doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=259" target="_blank">00:04:19.060</a></span> | <span class="t">stuff that will not be even possible where they add a smaller scale. But when they reach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=263" target="_blank">00:04:23.500</a></span> | <span class="t">some critical scale in size, they start to work really, really well. This is documented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=268" target="_blank">00:04:28.760</a></span> | <span class="t">by Jason in his blog post emergence and LLMs, which you see this plot on the bottom left,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=275" target="_blank">00:04:35.580</a></span> | <span class="t">successfully across a bunch of different tasks, whether it's modular arithmetic or purging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=279" target="_blank">00:04:39.640</a></span> | <span class="t">question answering, the success rate is basically flat until these models get big enough, good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=284" target="_blank">00:04:44.500</a></span> | <span class="t">enough. And then the success rates just kind of skyrocket. And that's why I think these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=290" target="_blank">00:04:50.320</a></span> | <span class="t">are particularly exciting. So yeah, question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=294" target="_blank">00:04:54.480</a></span> | <span class="t">I'm curious to know, do robotic foundation models display scale in real life?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=300" target="_blank">00:05:00.980</a></span> | <span class="t">Great question. And I'm really glad you asked. We have, I'm pretty excited to present some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=306" target="_blank">00:05:06.120</a></span> | <span class="t">directions we have along that I hope will answer your question in maybe about 10 minutes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=309" target="_blank">00:05:09.320</a></span> | <span class="t">or so. Yeah. But I think that's a question on all of our minds, including myself. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=315" target="_blank">00:05:15.920</a></span> | <span class="t">I think before we even get to the feasibility or the existence of any robotic foundation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=320" target="_blank">00:05:20.100</a></span> | <span class="t">models, like is this even needed? And I think the argument that I don't think is obvious</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=325" target="_blank">00:05:25.600</a></span> | <span class="t">is that I think emerging capabilities and relying on these might be actually indispensable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=330" target="_blank">00:05:30.360</a></span> | <span class="t">for robotics to actually work. A lot of the research over the past decades of robotics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=334" target="_blank">00:05:34.360</a></span> | <span class="t">has been in one bin, one room, one table, one robot, one building even, but these are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=340" target="_blank">00:05:40.280</a></span> | <span class="t">so vastly different from the orders of magnitude, more complex while real world situations that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=346" target="_blank">00:05:46.240</a></span> | <span class="t">humans operate in every single day. And I think to make that gigantic leap, we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=350" target="_blank">00:05:50.780</a></span> | <span class="t">to have to rely on this emerging capability scaling curve where things kind of work. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=355" target="_blank">00:05:55.600</a></span> | <span class="t">have very canned demos. Maybe you have, you know, a humanoid robot program to backflip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=360" target="_blank">00:06:00.440</a></span> | <span class="t">after hundreds of trials, but going from that to like the chaotic real world, I think we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=365" target="_blank">00:06:05.000</a></span> | <span class="t">going to have to rely on this emergence phenomenon for that. And I think maybe even intellectually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=372" target="_blank">00:06:12.360</a></span> | <span class="t">or academically, it's also interesting to think about why or why not a foundation model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=378" target="_blank">00:06:18.160</a></span> | <span class="t">for robotics might even work. It's worked in so many other domains. There's existence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=382" target="_blank">00:06:22.720</a></span> | <span class="t">proofs in audio, music, coding, language, another domain every single day, it seems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=386" target="_blank">00:06:26.560</a></span> | <span class="t">with 3D models and beyond. But if there is something very special about robotics, whether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=392" target="_blank">00:06:32.800</a></span> | <span class="t">it's embodiment or causality or physical grounding, and that is the barrier to making this very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=398" target="_blank">00:06:38.520</a></span> | <span class="t">simple recipe that's working all these other domains. If there is something special about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=402" target="_blank">00:06:42.720</a></span> | <span class="t">robotics that causes this recipe to fail, I think that's quite interesting to study</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=406" target="_blank">00:06:46.620</a></span> | <span class="t">why that is. I'm personally an optimist. I don't think there is some magical secret sauce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=411" target="_blank">00:06:51.600</a></span> | <span class="t">that's going to keep robotics from being tackled with the same formulas and recipes that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=416" target="_blank">00:06:56.000</a></span> | <span class="t">worked elsewhere. But, you know, I think this is a question I'd like to find out the answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=419" target="_blank">00:06:59.800</a></span> | <span class="t">to. And so maybe then instead of just motivating this philosophically, okay, we need foundation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=427" target="_blank">00:07:07.400</a></span> | <span class="t">models, foundation models are great. Let's try to build one for robotics. How do we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=431" target="_blank">00:07:11.520</a></span> | <span class="t">do that? Well, I think we can leverage a few ingredients by standing on the shoulder of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=437" target="_blank">00:07:17.600</a></span> | <span class="t">giants and looking at other domains. The first one is looking at different design principles</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=442" target="_blank">00:07:22.360</a></span> | <span class="t">of ML scaling from other domains. Let's look first at high capacity architectures, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=448" target="_blank">00:07:28.520</a></span> | <span class="t">topic of this class today. Ideas such as self-attention, as all the different ideas encompassed in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=455" target="_blank">00:07:35.000</a></span> | <span class="t">the transformer, as Andrej Karpathy famously said, it's like a magical universal differentiable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=459" target="_blank">00:07:39.880</a></span> | <span class="t">computer that's very general, very robust, and very remarkably scalable on many different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=464" target="_blank">00:07:44.720</a></span> | <span class="t">dimensions. Let's use those. We should also leverage the more guiding principles that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=470" target="_blank">00:07:50.280</a></span> | <span class="t">have been seen, the scaling laws, the trends, this year's Cinchilla, you know, we not only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=475" target="_blank">00:07:55.160</a></span> | <span class="t">have to scale the model size, we also have to scale compute, and we also have to scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=479" target="_blank">00:07:59.480</a></span> | <span class="t">the number of unique tokens in the corpus of the vast data sets that we train on. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=484" target="_blank">00:08:04.040</a></span> | <span class="t">if we do all three together, this has been shown to reliably have a pretty good chance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=489" target="_blank">00:08:09.120</a></span> | <span class="t">of succeeding, no matter what domain you're looking at. And so, and finally, what that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=494" target="_blank">00:08:14.560</a></span> | <span class="t">kind of means, and I think this is actually going to come up later, is that data set size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=498" target="_blank">00:08:18.720</a></span> | <span class="t">seems to matter these days a lot more than quality. Even if you have some sentences on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=502" target="_blank">00:08:22.880</a></span> | <span class="t">Wikipedia that are misspelled, or some, you know, falsehoods, or some things that aren't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=507" target="_blank">00:08:27.360</a></span> | <span class="t">so desirable, if in aggregate, your data set is diverse enough, and interesting enough,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=512" target="_blank">00:08:32.480</a></span> | <span class="t">these things will hopefully wash out in the mix. Ingredient number two, the proliferation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=519" target="_blank">00:08:39.120</a></span> | <span class="t">of the internet scale models themselves, not just the principles. What's exciting, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=524" target="_blank">00:08:44.880</a></span> | <span class="t">I'm sure it's, you know, definitely been very shocking for both experts and lay people alike,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=530" target="_blank">00:08:50.080</a></span> | <span class="t">is that a lot of these generative models across many different modalities have been experiencing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=534" target="_blank">00:08:54.320</a></span> | <span class="t">emerging capabilities and have been surpassing all of our wildest expectations time and time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=539" target="_blank">00:08:59.400</a></span> | <span class="t">and again. But even when we think that we're exhausted, all this stuff is too much, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=543" target="_blank">00:09:03.720</a></span> | <span class="t">not going to work, something will come out and completely blow me out of the water. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=546" target="_blank">00:09:06.880</a></span> | <span class="t">I think this trend will definitely keep continuing. And I think, in addition to that, they not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=551" target="_blank">00:09:11.440</a></span> | <span class="t">only will continue coming on and accelerate more rapidly, they're going to happen with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=555" target="_blank">00:09:15.200</a></span> | <span class="t">it, whether or not like we do anything, you know, in the grand scale, speaking, me as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=560" target="_blank">00:09:20.200</a></span> | <span class="t">a robotics researcher, or, you know, you and whatever subfield you're on, there are parts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=565" target="_blank">00:09:25.180</a></span> | <span class="t">of machine learning that likely you'll probably not ever touch in at least the near future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=569" target="_blank">00:09:29.840</a></span> | <span class="t">And those parts will be seeing tremendous breakthroughs and scaling and new capabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=573" target="_blank">00:09:33.280</a></span> | <span class="t">coming online every single week. And you can look at this not only in the impressiveness</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=579" target="_blank">00:09:39.760</a></span> | <span class="t">of the models, but also the acceleration of progress, the timescales in which new models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=584" target="_blank">00:09:44.560</a></span> | <span class="t">are being released, why we're large collaborations are being worked on by many groups, and then,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=589" target="_blank">00:09:49.440</a></span> | <span class="t">you know, being available to access for all to use and build upon. And the final ingredient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=596" target="_blank">00:09:56.460</a></span> | <span class="t">in this trend is more of a robotic specific one, but it is a vast shift from online robotic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=602" target="_blank">00:10:02.860</a></span> | <span class="t">learning, where robots collect experience online, make actions and learn through trial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=608" target="_blank">00:10:08.380</a></span> | <span class="t">and error to an offline setting where we decouple the data generation process from the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=614" target="_blank">00:10:14.260</a></span> | <span class="t">consumption process. As we've seen, and all these other foundation modeling domains, these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=619" target="_blank">00:10:19.740</a></span> | <span class="t">big internet scale data sets are so diverse, and they're static, we just scrape them once</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=624" target="_blank">00:10:24.320</a></span> | <span class="t">or scrape them multiple times continuously. But we aggregate a continuous pile that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=629" target="_blank">00:10:29.040</a></span> | <span class="t">just growing. Here, we see either the pile data set from a Luther or Lyon 5b for image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=634" target="_blank">00:10:34.860</a></span> | <span class="t">paired image text. And these are pretty big, and they're orders of magnitude more than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=639" target="_blank">00:10:39.200</a></span> | <span class="t">what we've seen before. And they are definitely a key ingredient to why other domains have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=643" target="_blank">00:10:43.560</a></span> | <span class="t">been doing so well at training these big foundation models. And this coming back to robotics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=650" target="_blank">00:10:50.800</a></span> | <span class="t">then I'd like to take a brief detour into how the shift came to be because it's very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=655" target="_blank">00:10:55.840</a></span> | <span class="t">easy to say in a sentence, yeah, robotics is offline more than online. And this is coming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=659" target="_blank">00:10:59.640</a></span> | <span class="t">as kind of a no brainer to many folks who are coming from other domains, like this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=664" target="_blank">00:11:04.040</a></span> | <span class="t">the way things are done. But in robotics, this has been a very big shift. And I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=669" target="_blank">00:11:09.040</a></span> | <span class="t">robotics has also been synonymous with RL, reinforcement learning for a lot of people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=673" target="_blank">00:11:13.720</a></span> | <span class="t">And I think increasingly, this is becoming less true. And so I'd like to take you down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=677" target="_blank">00:11:17.840</a></span> | <span class="t">a brief trip down the history of my team, their side of the talks as brief history of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=683" target="_blank">00:11:23.400</a></span> | <span class="t">robotics at Google. And yeah, of course, thanks. And I think this is not just for dramatic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=689" target="_blank">00:11:29.480</a></span> | <span class="t">exposition, it's really to try to guide you through how drastically our team's thinking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=694" target="_blank">00:11:34.960</a></span> | <span class="t">has kind of evolved over the years, and how that's going to inform the design decisions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=700" target="_blank">00:11:40.400</a></span> | <span class="t">and the kind of risks and research directions that we take in the specific projects that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=705" target="_blank">00:11:45.240</a></span> | <span class="t">I'm going to show coming up. Thank you. So in 2016, some of you may have seen this, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=710" target="_blank">00:11:50.560</a></span> | <span class="t">had what we call the arm farm, seven KUKA robots in a room collecting picking data 24/7.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=716" target="_blank">00:11:56.480</a></span> | <span class="t">And this was doing on policy RL in the real world, we were the first team to kind of say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=720" target="_blank">00:12:00.960</a></span> | <span class="t">hey, can we can we even do this with the goal of saying, can we do end to end robot learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=725" target="_blank">00:12:05.880</a></span> | <span class="t">with results in the real world, this was kind of risky at the time, it was not a common</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=730" target="_blank">00:12:10.000</a></span> | <span class="t">take. And from that we developed several interesting research directions that we started exploring,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=734" target="_blank">00:12:14.720</a></span> | <span class="t">we looked into stuff like QT opt, which is a Q learning method, working on continuous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=740" target="_blank">00:12:20.960</a></span> | <span class="t">control actions. While taking a vision inputs, we worked on cycle GAN to transform simulation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=747" target="_blank">00:12:27.820</a></span> | <span class="t">based images into real real looking images for sensor real, we looked at concurrent role</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=752" target="_blank">00:12:32.560</a></span> | <span class="t">of how we get robots moving faster and more efficiently in the real world. I'm sorry,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=756" target="_blank">00:12:36.200</a></span> | <span class="t">do you have a question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=759" target="_blank">00:12:39.040</a></span> | <span class="t">Yeah, great question. And that one, I think was basically, the arms would pick stuff up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=769" target="_blank">00:12:49.000</a></span> | <span class="t">from the bin, if they messed up, and it fell out, well, we come back the next morning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=772" target="_blank">00:12:52.680</a></span> | <span class="t">and there'd be objects scattered all throughout the room. So there was no reset. But if they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=777" target="_blank">00:12:57.400</a></span> | <span class="t">missed a little bit, the objects would fall back into the bin and hopefully be in a position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=781" target="_blank">00:13:01.000</a></span> | <span class="t">where they could pick them up again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=783" target="_blank">00:13:03.160</a></span> | <span class="t">Oh, yeah, of course. Thanks. I'll do that in the future. On this specific question was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=789" target="_blank">00:13:09.680</a></span> | <span class="t">for this 24 seven arm farm, how did we do resets? And the answer is, well, we didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=795" target="_blank">00:13:15.480</a></span> | <span class="t">we designed the bin so that they were kind of banked. So that object slightly missed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=799" target="_blank">00:13:19.080</a></span> | <span class="t">they would fall back in the bin, rearm themselves, maybe add more diversity with the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=802" target="_blank">00:13:22.880</a></span> | <span class="t">data. But this was doing off policy online RL with q learning. And we mixed it with some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=808" target="_blank">00:13:28.400</a></span> | <span class="t">data deployed again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=811" target="_blank">00:13:31.800</a></span> | <span class="t">Next, we kind of went through this consolidation phase around 2020. When we're like, alright,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=817" target="_blank">00:13:37.880</a></span> | <span class="t">this is pretty cool. And you know, but we want to get out of the bin, how do we do more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=821" target="_blank">00:13:41.760</a></span> | <span class="t">complex tasks and a more practical setting that could be closer to something that humans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=826" target="_blank">00:13:46.440</a></span> | <span class="t">would want to use that's more general every day. There, we kind of settled on this office</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=830" target="_blank">00:13:50.740</a></span> | <span class="t">micro kitchen environment, if you've heard of the famous Google micro kitchens. And I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=835" target="_blank">00:13:55.240</a></span> | <span class="t">think this was the setting we decided to operate in. And there, we started collecting data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=840" target="_blank">00:14:00.600</a></span> | <span class="t">we scaled our real operations. And there, we kind of scaled approaches to some different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=844" target="_blank">00:14:04.360</a></span> | <span class="t">things. And I think in the bottom right here is like the more mechanized reset version,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=848" target="_blank">00:14:08.800</a></span> | <span class="t">I would say of the arm farm. Here, we had a bin that folded in half. And this was doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=853" target="_blank">00:14:13.640</a></span> | <span class="t">multitask RL in the real world. And the bin would flip in half dumping objects from one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=857" target="_blank">00:14:17.480</a></span> | <span class="t">side to the other. So you could do more interesting tasks, whereas the arm farm was pick anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=861" target="_blank">00:14:21.200</a></span> | <span class="t">up. Now we could say, hey, pick up the carrot and place the tomato on to the plate. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=866" target="_blank">00:14:26.440</a></span> | <span class="t">then the bin would flip and you'd reset. Some other works so far at multitask imitation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=871" target="_blank">00:14:31.060</a></span> | <span class="t">learning, this is BC zero. And then we also look at stuff like combining reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=874" target="_blank">00:14:34.900</a></span> | <span class="t">learning with imitation learning bootstrapping.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=879" target="_blank">00:14:39.360</a></span> | <span class="t">But in 2020, once again, we realized we were working on a ton of different directions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=885" target="_blank">00:14:45.160</a></span> | <span class="t">and we wanted to consolidate. And I think the two main things that were really bothering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=889" target="_blank">00:14:49.080</a></span> | <span class="t">us at the point at the time, where we were hitting two main walls across all these methods,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=893" target="_blank">00:14:53.980</a></span> | <span class="t">some of them were plateauing at this 50 to 70% of, you know, rough range in the real</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=898" target="_blank">00:14:58.880</a></span> | <span class="t">world. And other methods were requiring very specific data distributions, they had to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=904" target="_blank">00:15:04.240</a></span> | <span class="t">on policy, or they could only use demonstrations, or they blah, blah, blah, like, there were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=907" target="_blank">00:15:07.920</a></span> | <span class="t">so many different nuances and like gotchas to all these different methods, and all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=911" target="_blank">00:15:11.880</a></span> | <span class="t">different drawbacks. And so the question we posed was, we're open to any method, any strategy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=918" target="_blank">00:15:18.180</a></span> | <span class="t">that will enable us to solve tasks in a very performant matter more than 90% in the real</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=922" target="_blank">00:15:22.720</a></span> | <span class="t">world. And also that can scale with some kind of data that we can collect, you know, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=928" target="_blank">00:15:28.560</a></span> | <span class="t">maybe this is a bit more lax than let's say, an academic setting where you're much more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=933" target="_blank">00:15:33.120</a></span> | <span class="t">resource constrained. But at the end of the day, you know, even our team does not have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=936" target="_blank">00:15:36.600</a></span> | <span class="t">infinite money, we still have a certain number of robots, a certain number of operators,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=940" target="_blank">00:15:40.560</a></span> | <span class="t">and we're constrained by the laws of physics. So we need some way to acquire more data that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=944" target="_blank">00:15:44.000</a></span> | <span class="t">we can then learn from. And so we're all scratching our heads thinking about this for a few months</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=947" target="_blank">00:15:47.720</a></span> | <span class="t">in spring 2022. We decided on going with multitask imitation learning. So this was a vast departure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=954" target="_blank">00:15:54.600</a></span> | <span class="t">from the 24/7 arm farm. This was a vast evolution of how we approach the problem. We found that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=960" target="_blank">00:16:00.560</a></span> | <span class="t">you know, with enough, you know, gentle care and love, multitask imitation learning was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=964" target="_blank">00:16:04.520</a></span> | <span class="t">able to hit these 90% numbers, and it was able to get better with more demonstrations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=969" target="_blank">00:16:09.320</a></span> | <span class="t">These aren't the cheapest thing, but it was able to scale with additional demonstrations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=973" target="_blank">00:16:13.760</a></span> | <span class="t">which was the sign of life that we were looking for. So that brings us to less than a year</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=978" target="_blank">00:16:18.360</a></span> | <span class="t">ago, our team was deciding this is the path forward, at least in the near term future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=983" target="_blank">00:16:23.120</a></span> | <span class="t">But maybe, you know, we could just think about how the approach we were taking here might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=990" target="_blank">00:16:30.040</a></span> | <span class="t">also spread out in the future. And we might be able to bring back these other threads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=994" target="_blank">00:16:34.280</a></span> | <span class="t">For example, if now that we're decoupling this data collection of demonstrations or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=999" target="_blank">00:16:39.160</a></span> | <span class="t">etc. from how you learn from them with a multitask imitation learning policy, maybe we can in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1004" target="_blank">00:16:44.080</a></span> | <span class="t">the future then do something like offline RL. But I think at a high level now, I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1008" target="_blank">00:16:48.840</a></span> | <span class="t">just you know, in a few short minutes, just compressed six years of very bitter lessons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1014" target="_blank">00:16:54.040</a></span> | <span class="t">that our team has been learning. And I think from where we are today, and looking back,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1017" target="_blank">00:16:57.960</a></span> | <span class="t">even just two years ago, if you told me that the strategies we're deploying today could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1021" target="_blank">00:17:01.680</a></span> | <span class="t">just scale the way they are, I probably would not have believed you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1025" target="_blank">00:17:05.920</a></span> | <span class="t">Great question. So I think task conditioning is definitely still was an open question at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1040" target="_blank">00:17:20.320</a></span> | <span class="t">the time. But I think with this work, BC zero, we found that language was able, at least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1047" target="_blank">00:17:27.120</a></span> | <span class="t">in a templated language, kind of representation was good enough where we could direct I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1052" target="_blank">00:17:32.120</a></span> | <span class="t">BC zeros over 80 tasks. So they were they were very templated, like pick grapes, or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1057" target="_blank">00:17:37.320</a></span> | <span class="t">like move grapes onto play or drag this across black drag cloth across table. And I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1064" target="_blank">00:17:44.240</a></span> | <span class="t">this representation was still enough where you're learning a good number of skills that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1067" target="_blank">00:17:47.840</a></span> | <span class="t">you're passing in essentially a one hot ID into your policy network, and it will learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1071" target="_blank">00:17:51.400</a></span> | <span class="t">to imitate that. And for each one of those 80 tasks, we'd collect hundreds or 1000s of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1075" target="_blank">00:17:55.320</a></span> | <span class="t">demonstrations. And I will touch upon the specifics of that a bit later, too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1083" target="_blank">00:18:03.720</a></span> | <span class="t">So yeah, today, and or at least in 2022, let's do offline methods, let's decouple data generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1090" target="_blank">00:18:10.320</a></span> | <span class="t">from data consumption. And let's take these three lessons now that we touched upon. Let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1095" target="_blank">00:18:15.320</a></span> | <span class="t">take the design principles of ML scaling, and then figure out what lessons can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1098" target="_blank">00:18:18.960</a></span> | <span class="t">be applied when you look into the future for recipe for robot learning and foundation models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1106" target="_blank">00:18:26.080</a></span> | <span class="t">The first lesson I think is very important is these high capacity architectures like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1109" target="_blank">00:18:29.680</a></span> | <span class="t">attention. The second I'll touch on later is data interoperability, tokenization, tokenization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1114" target="_blank">00:18:34.960</a></span> | <span class="t">discretization. And the second ingredient is the proliferation of these models themselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1120" target="_blank">00:18:40.440</a></span> | <span class="t">Can we leverage them because they will get better over time. And I think here, I would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1124" target="_blank">00:18:44.680</a></span> | <span class="t">like to plug my colleague Carol Hausman's bitter lesson 2.0, which is the bitter lesson.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1129" target="_blank">00:18:49.560</a></span> | <span class="t">The first one from Richard Sutton was, you should leverage methods that scale with more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1133" target="_blank">00:18:53.560</a></span> | <span class="t">compute. And maybe in today's day and age, the lesson is that we should leverage methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1138" target="_blank">00:18:58.800</a></span> | <span class="t">that are able to utilize improvements in foundation models, because they're going to get better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1143" target="_blank">00:19:03.400</a></span> | <span class="t">Yeah. So both in the lesson 1.0 and 2.0, one thing that's always been clear to me is suppose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1150" target="_blank">00:19:10.400</a></span> | <span class="t">I have a set of methods. And I want to choose the methods that are going to scale with more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1153" target="_blank">00:19:13.400</a></span> | <span class="t">compute or in this case, scale with better foundation models. The question is, how do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1157" target="_blank">00:19:17.360</a></span> | <span class="t">I actually decide which of those methods meet those criteria?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1162" target="_blank">00:19:22.080</a></span> | <span class="t">Yeah, great question. I think, and maybe it's, I think that's a very, I don't have a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1167" target="_blank">00:19:27.720</a></span> | <span class="t">answer for that. Oh, sorry. Yeah, yeah. The question was in bitter lesson 1.0 and bitter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1172" target="_blank">00:19:32.560</a></span> | <span class="t">lesson 2.0, the question is, well, that's great. That's the lesson, but how do we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1176" target="_blank">00:19:36.480</a></span> | <span class="t">decide which methods meet this criteria? And I think, you know, my answer is it's not always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1182" target="_blank">00:19:42.000</a></span> | <span class="t">obvious and it's actually quite tricky sometimes, but maybe, you know, sometimes, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1186" target="_blank">00:19:46.680</a></span> | <span class="t">what you can be very confident that, oh yeah, this will definitely scale with more data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1190" target="_blank">00:19:50.320</a></span> | <span class="t">and compute. And some that are same, but basically the more hard-coded you are, the more assumptions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1194" target="_blank">00:19:54.400</a></span> | <span class="t">more heuristics you bake in, the more you in our, in our day and age, the more you rely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1198" target="_blank">00:19:58.400</a></span> | <span class="t">on a specific implementation of a specific foundation model of a specific class of algorithm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1204" target="_blank">00:20:04.560</a></span> | <span class="t">maybe that will be less robust than a method that just assumes some very abstract input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1209" target="_blank">00:20:09.000</a></span> | <span class="t">and output and assumes that how you get from that input and output can improve over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1213" target="_blank">00:20:13.280</a></span> | <span class="t">And maybe the algorithm itself even changes altogether. So I think that would be my take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1217" target="_blank">00:20:17.760</a></span> | <span class="t">on the bitter lesson 2.0, but this is definitely still, I think the jury is still out on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1225" target="_blank">00:20:25.440</a></span> | <span class="t">And my, my, my basic, my, my, one of the things I like to propose is that language is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1230" target="_blank">00:20:30.280</a></span> | <span class="t">way that we can leverage bitter lesson 2.0. If you have language as the universal representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1235" target="_blank">00:20:35.600</a></span> | <span class="t">through which all of these foundations communicate to each other, whether it's, you know, captioning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1239" target="_blank">00:20:39.640</a></span> | <span class="t">or generation or whatnot, I think that's one way that we could leverage a bitter lesson</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1244" target="_blank">00:20:44.160</a></span> | <span class="t">2.0. And finally, the third ingredient offline robot learning, decoupling data generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1251" target="_blank">00:20:51.680</a></span> | <span class="t">from data consumption, putting these all together, my recipe for how one take at a modern attempt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1258" target="_blank">00:20:58.740</a></span> | <span class="t">that embodied intelligence would look like would be to combine these large offline datasets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1263" target="_blank">00:21:03.640</a></span> | <span class="t">with high capacity architectures by using language as the universal glue. And in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1268" target="_blank">00:21:08.560</a></span> | <span class="t">works I'm going to present shortly, all of our different projects, I think in some way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1273" target="_blank">00:21:13.040</a></span> | <span class="t">or another are inspired by this philosophy. And now, now that we've kind of, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1282" target="_blank">00:21:22.320</a></span> | <span class="t">understood the motivations and potentially one possible approach, of course, largely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1289" target="_blank">00:21:29.600</a></span> | <span class="t">the first offline is a high capacity architectures using language as a universal glue. I'm curious</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1294" target="_blank">00:21:34.320</a></span> | <span class="t">to know which, if any of these are currently bottlenecks, not the right word, which means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1299" target="_blank">00:21:39.560</a></span> | <span class="t">they're limited. Got it. Because it seems to me like we already have large offline datasets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1303" target="_blank">00:21:43.320</a></span> | <span class="t">we have high capacity architectures, and you know, those architectures are relatively just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1306" target="_blank">00:21:46.560</a></span> | <span class="t">a piece of language, but it seems like we already have all the components necessary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1309" target="_blank">00:21:49.720</a></span> | <span class="t">So why is this then not a solved problem? The question was these, it seems like we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1315" target="_blank">00:21:55.880</a></span> | <span class="t">a lot of these ingredients. And so why hasn't robotics been solved yet? So I would argue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1321" target="_blank">00:22:01.240</a></span> | <span class="t">that actually this take here, and maybe I'm, this is to the wrong audience at the moment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1325" target="_blank">00:22:05.880</a></span> | <span class="t">but I think this is non, very non-obvious across the robotics field. Many people do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1329" target="_blank">00:22:09.720</a></span> | <span class="t">not agree with all of these, much less two of these, or even any of these points. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1335" target="_blank">00:22:15.160</a></span> | <span class="t">so I think, and also the existence of the scale of how mature each of these components</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1340" target="_blank">00:22:20.520</a></span> | <span class="t">are within robotics is at very different stages. And I would say like, and we can talk a bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1344" target="_blank">00:22:24.760</a></span> | <span class="t">later about like, for example, like data scale, or the architectures that have kind of diffused</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1349" target="_blank">00:22:29.560</a></span> | <span class="t">through osmosis from other ML domains into robotics. But I think we're still at very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1354" target="_blank">00:22:34.440</a></span> | <span class="t">different stages on how, how much people have actually bought into these lessons and invested</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1359" target="_blank">00:22:39.000</a></span> | <span class="t">in them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1360" target="_blank">00:22:40.000</a></span> | <span class="t">Yeah, I can probably, I also don't want to get into too much trouble here, but I'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1377" target="_blank">00:22:57.840</a></span> | <span class="t">probably get myself in a bit of hot water in a few slides. So I'll, I'll extend upon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1381" target="_blank">00:23:01.560</a></span> | <span class="t">it a bit then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1382" target="_blank">00:23:02.560</a></span> | <span class="t">I'm just curious to know what their opinion is and why you think they're wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1387" target="_blank">00:23:07.280</a></span> | <span class="t">Yeah. And I would say that like me personally, and, you know, not speaking for my team, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1392" target="_blank">00:23:12.960</a></span> | <span class="t">a lot of people on my team are probably at the very extreme end of learning, scaling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1398" target="_blank">00:23:18.480</a></span> | <span class="t">data-driven, you know, foundation model based, let's go big. And I think a lot of people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1404" target="_blank">00:23:24.160</a></span> | <span class="t">don't believe that. And yeah, happy to discuss why later, maybe after the Zoom as well. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1410" target="_blank">00:23:30.160</a></span> | <span class="t">so yeah. Well, okay then let's, let's go ahead and dive in and see how this recipe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1415" target="_blank">00:23:35.920</a></span> | <span class="t">might actually percolate into specific domains. And the first one is RT1. This is a recent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1423" target="_blank">00:23:43.360</a></span> | <span class="t">work from our group that works on how we can scale imitation learning. And let's look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1427" target="_blank">00:23:47.960</a></span> | <span class="t">how we can actually apply these first principles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1430" target="_blank">00:23:50.880</a></span> | <span class="t">So the first one is to consider what we actually, let's, let's put ourselves into the spring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1435" target="_blank">00:23:55.920</a></span> | <span class="t">2020 mindset. We we've been collecting demonstrations for a while. This is a ton of demos, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1440" target="_blank">00:24:00.840</a></span> | <span class="t">a hundred thousand over that was collected over like a year and a half on many, many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1445" target="_blank">00:24:05.200</a></span> | <span class="t">robots on many, many tasks that exists. It was expensive. And over time, this will actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1451" target="_blank">00:24:11.200</a></span> | <span class="t">you know, not trickle up at insane amounts. Like we won't just get a hundred thousand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1454" target="_blank">00:24:14.840</a></span> | <span class="t">new high quality demos every day. This will grow over time, but it's not going to, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1459" target="_blank">00:24:19.000</a></span> | <span class="t">know, grow for free. And autonomous ways of doing this is very hard. As you saw earlier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1463" target="_blank">00:24:23.320</a></span> | <span class="t">with MPOP with the bin reset mechanism, or DeepMind has a work on RGB stacking, where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1467" target="_blank">00:24:27.560</a></span> | <span class="t">they try to do autonomous resets. And you know what, the way that we're doing it right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1471" target="_blank">00:24:31.160</a></span> | <span class="t">now, or at least for this paper was human teleoperation pioneered by BC zero, and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1476" target="_blank">00:24:36.960</a></span> | <span class="t">was very expensive as well. So there's going to be a limited throughput. And finally BC</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1481" target="_blank">00:24:41.480</a></span> | <span class="t">zero used a ResNet based backbone, and it was pretty good, but I found that it was very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1485" target="_blank">00:24:45.120</a></span> | <span class="t">sensitive to training distributions. For example, when they remove data from some teleoperators</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1489" target="_blank">00:24:49.680</a></span> | <span class="t">to make the data more homogenous performance got better, and that's not really a property</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1493" target="_blank">00:24:53.680</a></span> | <span class="t">we like, right? We want more data, even if it's not exactly the same. So the lesson here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1500" target="_blank">00:25:00.120</a></span> | <span class="t">models need to be robust and they need to generalize. Cool. So we have models and to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1504" target="_blank">00:25:04.440</a></span> | <span class="t">be robust and generalized. What else do we have? Well, off the shelf models are pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1507" target="_blank">00:25:07.840</a></span> | <span class="t">slow. If we take in these huge, you know, vision transformers from other domains, they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1512" target="_blank">00:25:12.120</a></span> | <span class="t">not going to run on the real robot. We need to be able to run at a pretty high frequency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1516" target="_blank">00:25:16.000</a></span> | <span class="t">They need to be reactive. Inference time needs to be slow because all our models are vision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1520" target="_blank">00:25:20.520</a></span> | <span class="t">based. And finally, we want our data to be able to understand language. As I mentioned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1526" target="_blank">00:25:26.720</a></span> | <span class="t">robust language is the universal glue. Our data set already has some language. We want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1530" target="_blank">00:25:30.400</a></span> | <span class="t">eventual models to be very multimodal. This is a first principle that we need to dig in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1535" target="_blank">00:25:35.840</a></span> | <span class="t">What does this mean? We can't just take something existing. We probably need to design or at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1539" target="_blank">00:25:39.520</a></span> | <span class="t">least modify something from the ground up. And let's take the best practices that we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1543" target="_blank">00:25:43.780</a></span> | <span class="t">seen work in other fields. And so we worked for a bit and we came up with this architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1551" target="_blank">00:25:51.160</a></span> | <span class="t">for RT1. Again, once again, this was a large team with a bunch of different contributions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1556" target="_blank">00:25:56.040</a></span> | <span class="t">and I'll just go through a few of them here. At a high level, RT1 is robotics transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1562" target="_blank">00:26:02.560</a></span> | <span class="t">It operates at three hertz. It takes in visual input from the robot RGB camera, as well as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1568" target="_blank">00:26:08.600</a></span> | <span class="t">a natural language instruction. There, the image is patchified and fed into a film efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1574" target="_blank">00:26:14.680</a></span> | <span class="t">net tokenizer. It's then passed into token learner, which I'll talk about soon. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1580" target="_blank">00:26:20.000</a></span> | <span class="t">also the language instructions are tokenized and then they are put into the same transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1585" target="_blank">00:26:25.440</a></span> | <span class="t">And then finally, we output discretized actions as tokens and send that to the real world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1591" target="_blank">00:26:31.040</a></span> | <span class="t">in three hertz in closed loop. This transformer is a decoder one. We use a sparse categorical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1598" target="_blank">00:26:38.740</a></span> | <span class="t">entropy objective for action prediction by applying a causal mask. We use the pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1604" target="_blank">00:26:44.400</a></span> | <span class="t">efficient net backbone, and we also use token learner for faster inference. Diving a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1610" target="_blank">00:26:50.320</a></span> | <span class="t">bit deeper. Oh, sorry. Yeah. A question. Great question. So the image token, when it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1621" target="_blank">00:27:01.920</a></span> | <span class="t">goes in from, so each image is the, you know, the high fidelity RGB image from the camera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1627" target="_blank">00:27:07.320</a></span> | <span class="t">We split that up into 81 separate patches. And so each patch is, you know, it's spatially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1632" target="_blank">00:27:12.240</a></span> | <span class="t">just like the square there. But the cool thing is that what token learner does here, this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1638" target="_blank">00:27:18.080</a></span> | <span class="t">thing here is it's a previous work from our group that takes in a bunch of possible you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1644" target="_blank">00:27:24.640</a></span> | <span class="t">know, image patches and dynamically selects which of those image patch tokens are more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1650" target="_blank">00:27:30.280</a></span> | <span class="t">relevant for the tax at hand, given the existing context. So from those 81 image patch tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1656" target="_blank">00:27:36.240</a></span> | <span class="t">we sub sample eight of them to use for inference. And this happens at every time step. And that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1662" target="_blank">00:27:42.520</a></span> | <span class="t">process has learned which of the eight patches are relevant at any given moment. And otherwise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1668" target="_blank">00:27:48.840</a></span> | <span class="t">we're sending in way too many tokens and the context length would explode and we wouldn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1672" target="_blank">00:27:52.600</a></span> | <span class="t">be able to do inference on robots. We are also passing in a sequence sequence length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1676" target="_blank">00:27:56.780</a></span> | <span class="t">of six images. History is quite important when you're doing temporally coherent tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1681" target="_blank">00:28:01.920</a></span> | <span class="t">in the real world where things like physics and you know, exactly this, this nuanced detail</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1686" target="_blank">00:28:06.320</a></span> | <span class="t">of what the objects are doing in relation to each other into your robot. Those details</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1690" target="_blank">00:28:10.240</a></span> | <span class="t">really matter. And in total, the the model size is 35 million parameters, which is quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1697" target="_blank">00:28:17.600</a></span> | <span class="t">a bit smaller than a lot of these other, you know, huge internet scale models. And finally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1703" target="_blank">00:28:23.720</a></span> | <span class="t">one main difference here is action discretization. Before a lot of the products we're doing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1708" target="_blank">00:28:28.680</a></span> | <span class="t">we're doing continuous control. And if you think about it, right, our robot does have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1712" target="_blank">00:28:32.640</a></span> | <span class="t">we do end effector pose control on position control. And there, the real world is a continuous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1717" target="_blank">00:28:37.260</a></span> | <span class="t">state space. But, um, and to do that, we, we had to come up with many algorithmic novelties,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1723" target="_blank">00:28:43.080</a></span> | <span class="t">for example, a, a, a CEM actor that did basically sampling of these continuous action spaces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1728" target="_blank">00:28:48.440</a></span> | <span class="t">to propose the highest ones that would get rated by the Q function. And we do this twice,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1732" target="_blank">00:28:52.280</a></span> | <span class="t">blah, blah, blah. And, but that's like so sensitive, but we needed to get, do that to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1735" target="_blank">00:28:55.840</a></span> | <span class="t">get things to work. But now we just decided, let's just, you know, bin our actions. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1740" target="_blank">00:29:00.560</a></span> | <span class="t">only 256 discrete actions. And let's just predict those as tokens. Um, any question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1746" target="_blank">00:29:06.360</a></span> | <span class="t">Yeah, what I was going to ask is, so you're mentioning that you have this design required</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1751" target="_blank">00:29:11.620</a></span> | <span class="t">or engineering requirement about speed and latency reaction. And then you say that that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1756" target="_blank">00:29:16.060</a></span> | <span class="t">necessitates having a relatively small model, which makes sense. But one message of scaling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1760" target="_blank">00:29:20.540</a></span> | <span class="t">when we're talking about foundation models is that we don't want to be bottlenecked by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1763" target="_blank">00:29:23.380</a></span> | <span class="t">either data compute or parameters. So I guess what I'm curious to know is how do you balance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1767" target="_blank">00:29:27.900</a></span> | <span class="t">these off in the sense that you want to have lots of parameters to have a really powerful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1771" target="_blank">00:29:31.580</a></span> | <span class="t">model, while on the other hand, you want to have very fast input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1774" target="_blank">00:29:34.660</a></span> | <span class="t">Yeah, great question. And to repeat it, the question is, um, we kind of set a pretty hard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1779" target="_blank">00:29:39.560</a></span> | <span class="t">constraint with a hundred millisecond inference time yet. A lot of the lessons in foundation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1783" target="_blank">00:29:43.840</a></span> | <span class="t">modeling is that you shouldn't be constraining yourself against any dimension, whether it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1787" target="_blank">00:29:47.260</a></span> | <span class="t">data set, size, compute, or model capacity. And I think my initial answer to that is that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1792" target="_blank">00:29:52.440</a></span> | <span class="t">a very great point and something I think that's going to be coming up as a severe bottleneck</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1797" target="_blank">00:29:57.000</a></span> | <span class="t">in the future. But for, for our initial case, I think this is more of an exploration of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1801" target="_blank">00:30:01.080</a></span> | <span class="t">whether these principles and even scaling well beyond what we were looking at now to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1805" target="_blank">00:30:05.440</a></span> | <span class="t">work already on this 35 million is gigantic compared to a lot of prior work using, for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1810" target="_blank">00:30:10.800</a></span> | <span class="t">example, a ResNet-34 or whatnot. So this is already much bigger than, you know, a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1815" target="_blank">00:30:15.240</a></span> | <span class="t">of other options. And maybe for now, at least it's the easiest, it's the largest scale we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1821" target="_blank">00:30:21.040</a></span> | <span class="t">could go to roughly in the short term without having to think of more tricks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1825" target="_blank">00:30:25.200</a></span> | <span class="t">Yeah, we can talk about it a bit later, maybe. I think I'd also love to hear your thoughts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1833" target="_blank">00:30:33.040</a></span> | <span class="t">too, because it's very non-obvious how we can get past some of these bottlenecks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1860" target="_blank">00:31:00.160</a></span> | <span class="t">Yeah, great question. We ran some ablations on model size. I might have that in a few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1866" target="_blank">00:31:06.040</a></span> | <span class="t">slides, but maybe we can return to that then. And if not, I can, yeah, but great question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1875" target="_blank">00:31:15.040</a></span> | <span class="t">So yeah, that's the architecture and I'll discuss some of the ablations and the trends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1878" target="_blank">00:31:18.720</a></span> | <span class="t">later on, but maybe, you know, this is a robotics lecture, I should show you some pretty visuals,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1883" target="_blank">00:31:23.920</a></span> | <span class="t">right? So let's look at some evaluations we did. We compared against some baselines. One</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1888" target="_blank">00:31:28.880</a></span> | <span class="t">is Gato, which you might be familiar with. And then the other one is BC0, the ResNet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1894" target="_blank">00:31:34.560</a></span> | <span class="t">based one. And we find that we evaluate unseen tasks versus unseen tasks. And we also add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1900" target="_blank">00:31:40.320</a></span> | <span class="t">in various distractor objects. Our normal data collect looks like this top left picture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1905" target="_blank">00:31:45.160</a></span> | <span class="t">three cans on a gray desk, that's basically it. But then we push it further by bringing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1909" target="_blank">00:31:49.960</a></span> | <span class="t">in a lot more objects so that the table is so cluttered that even as a human, sometimes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1914" target="_blank">00:31:54.040</a></span> | <span class="t">it's hard to find the object that you're actually looking for. We add in table class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1918" target="_blank">00:31:58.800</a></span> | <span class="t">we make the textures very different. We bring it to new micro kitchens with new surfaces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1923" target="_blank">00:32:03.120</a></span> | <span class="t">all together. And we find that RT1 is more robust than these other different methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1928" target="_blank">00:32:08.200</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1929" target="_blank">00:32:09.200</a></span> | <span class="t">[inaudible]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1930" target="_blank">00:32:10.200</a></span> | <span class="t">Good question. The question was, was the Gato model trained on our data or was it just already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1945" target="_blank">00:32:25.200</a></span> | <span class="t">included in Gato? The answer is this data was not included in Gato. And so we retrained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1949" target="_blank">00:32:29.560</a></span> | <span class="t">the Gato model only on our data. Yeah. And yeah, so here's just a different visualization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1955" target="_blank">00:32:35.680</a></span> | <span class="t">of the robot going out in our micro kitchen and doing different interesting things. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1959" target="_blank">00:32:39.760</a></span> | <span class="t">can see here that it's trained on one setting, but then it goes into brand new kitchen, brand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1964" target="_blank">00:32:44.400</a></span> | <span class="t">new countertops, new objects, and it's able to do all of them pretty robustly. We also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1969" target="_blank">00:32:49.520</a></span> | <span class="t">put it into a long horizon setting using the SACAN framework that we'll talk about next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1976" target="_blank">00:32:56.360</a></span> | <span class="t">But in these settings, a lot of them are mixing all of these generalization capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1980" target="_blank">00:33:00.920</a></span> | <span class="t">And on the plot on the left here, we're using what we call generalization levels inspired</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1984" target="_blank">00:33:04.600</a></span> | <span class="t">by the VIMA paper that would basically increasingly change more factors of variation simultaneously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1990" target="_blank">00:33:10.400</a></span> | <span class="t">And here we found RT1 is the most robust.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=1994" target="_blank">00:33:14.080</a></span> | <span class="t">Yeah, good question. We'll go into a bit more detail later, but I think at a high level,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2005" target="_blank">00:33:25.360</a></span> | <span class="t">teleoperators get a structure template at a command of like verb, noun, verb or something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2010" target="_blank">00:33:30.400</a></span> | <span class="t">like pick Coke can or move Apple near sponge. And we have around 700 tasks set up this way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2018" target="_blank">00:33:38.400</a></span> | <span class="t">and they go ahead and collect that data, test done. And then later we have, we make sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2023" target="_blank">00:33:43.240</a></span> | <span class="t">that successes are actually successes and we discard stuff that's like unsafe, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2028" target="_blank">00:33:48.000</a></span> | <span class="t">Oh yeah, I got it. For this paper, we, we, we utilize 130,000 demonstrations for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2036" target="_blank">00:33:56.720</a></span> | <span class="t">Yeah, great question. I think a lot of prior work has also been done on this, but it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2053" target="_blank">00:34:13.120</a></span> | <span class="t">also noted that when you have, for example, the question was, did you find that the, the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2058" target="_blank">00:34:18.920</a></span> | <span class="t">the trajectories in your dataset were very multimodal. And I think what you mean by that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2062" target="_blank">00:34:22.720</a></span> | <span class="t">is that to go from point A to point B, I can go left or I can go right, or I can go straight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2069" target="_blank">00:34:29.040</a></span> | <span class="t">And I think this kind of diversity in basically for a single image state, but yet my data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2074" target="_blank">00:34:34.880</a></span> | <span class="t">has three possible labels that can have very bad effects sometimes. For us, I think because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2080" target="_blank">00:34:40.120</a></span> | <span class="t">we are using teleoperator demonstrations, the data was more homogenous than perhaps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2084" target="_blank">00:34:44.800</a></span> | <span class="t">like in the wild, for example, there's a type of data function called play data where operators</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2089" target="_blank">00:34:49.160</a></span> | <span class="t">just do whatever they want and we label in hindsight. And I think our data is more homogenous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2093" target="_blank">00:34:53.120</a></span> | <span class="t">than that, but we did not find a lot of the issues that we've seen in prior projects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2097" target="_blank">00:34:57.760</a></span> | <span class="t">One potential answer is maybe it's, it's the, it's the architecture itself, but we can talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2103" target="_blank">00:35:03.160</a></span> | <span class="t">about that later too. Yeah. Question. Great question. We actually do have a termination</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2116" target="_blank">00:35:16.640</a></span> | <span class="t">action. So the, the policy itself, so the question was how do you determine when a episode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2121" target="_blank">00:35:21.440</a></span> | <span class="t">is complete and the policy is able to predict terminate because at the end of each teleoperation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2126" target="_blank">00:35:26.800</a></span> | <span class="t">session, the operator can click a button and it's marked as episodes done. Yeah, I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2139" target="_blank">00:35:39.720</a></span> | <span class="t">for these evaluations, we were quite strict, but definitely I think in some cases, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2144" target="_blank">00:35:44.760</a></span> | <span class="t">know, maybe, maybe if we're just doing an experiment for ourselves, we'll have a dense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2148" target="_blank">00:35:48.800</a></span> | <span class="t">reward scale of like grasp the object and move closer, grasp the object and almost got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2153" target="_blank">00:35:53.560</a></span> | <span class="t">there, but mess up at the end. And we'll have like a, a grading curve basically. But for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2157" target="_blank">00:35:57.320</a></span> | <span class="t">all of these, all of these stats I'm showing here, it was zero or one, one fully complete</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2162" target="_blank">00:36:02.320</a></span> | <span class="t">zero was not fully complete. Yeah. Cool. And I think what was exciting side and maybe talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2170" target="_blank">00:36:10.600</a></span> | <span class="t">about the multimodality aspect is then we pushed the limit even further. We were Trent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2174" target="_blank">00:36:14.360</a></span> | <span class="t">we decided to train on very diverse data distributions. You're you're back by then. Yeah. Okay. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2181" target="_blank">00:36:21.680</a></span> | <span class="t">right now you saw 130 to a thousand demonstrations train on this everyday robot proprietary mobile</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2188" target="_blank">00:36:28.760</a></span> | <span class="t">manipulator, but we were also looking to train on very different data distributions with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2193" target="_blank">00:36:33.080</a></span> | <span class="t">very different, you know, action distributions, very different trajectories, even very different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2197" target="_blank">00:36:37.000</a></span> | <span class="t">visuals objects tasks. And to do that, we included two other data sources. One was simulation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2202" target="_blank">00:36:42.520</a></span> | <span class="t">data, which was kind of our robot blood and sin, but it looked quite different. And also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2207" target="_blank">00:36:47.220</a></span> | <span class="t">this data was collected with reinforcement learning and not with teleoperate demonstrations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2212" target="_blank">00:36:52.080</a></span> | <span class="t">in the past with all of the aisle plus RL work that I mentioned, we found that combined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2216" target="_blank">00:36:56.920</a></span> | <span class="t">these, these two types of data was going to be very difficult because RL data has very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2221" target="_blank">00:37:01.920</a></span> | <span class="t">short action. It's very quick. That's very optimized for the specific reward function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2226" target="_blank">00:37:06.800</a></span> | <span class="t">versus human collective tele-operation data is a lot more, you know, human life, so to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2231" target="_blank">00:37:11.960</a></span> | <span class="t">speak. And finally, we revived a data set from many years ago at 2018. If you remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2236" target="_blank">00:37:16.640</a></span> | <span class="t">the Kuka project, that arm farm has not been operational in that state for many years now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2241" target="_blank">00:37:21.000</a></span> | <span class="t">but we had that data still. And so we were hoping to see if a different robot with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2245" target="_blank">00:37:25.920</a></span> | <span class="t">different action space on different objects with different visuals in a different building</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2250" target="_blank">00:37:30.040</a></span> | <span class="t">could still be combined with data from this micro kitchen, a robot data set that we train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2255" target="_blank">00:37:35.680</a></span> | <span class="t">on originally. And what was very surprising to me is that Archie one was able to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2260" target="_blank">00:37:40.280</a></span> | <span class="t">from all of these very diverse data distributions. I had never seen a result like this or any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2264" target="_blank">00:37:44.880</a></span> | <span class="t">other architecture, for example, a Resnet or even another learning method like reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2270" target="_blank">00:37:50.180</a></span> | <span class="t">learning could successfully learn on such different data distributions. So robustly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2275" target="_blank">00:37:55.840</a></span> | <span class="t">And we evaluated, for example, on combining concepts. So we would have the original everyday</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2281" target="_blank">00:38:01.000</a></span> | <span class="t">robot robot pick up objects that were only seen in the Kuka project, or we would put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2286" target="_blank">00:38:06.200</a></span> | <span class="t">objects only seen in simulation and see if our policy could understand that. So it did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2290" target="_blank">00:38:10.080</a></span> | <span class="t">seem like it could generalize between objects and seen in other data sets and concepts that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2294" target="_blank">00:38:14.080</a></span> | <span class="t">had seen in other data sets into the setting it was in now in the real micro kitchen. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2298" target="_blank">00:38:18.960</a></span> | <span class="t">that was a very fun result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2300" target="_blank">00:38:20.760</a></span> | <span class="t">I have a question. How did you find the action spaces of the everyday robot with the Kuka?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2307" target="_blank">00:38:27.160</a></span> | <span class="t">Great question. Yeah, we just tokenized it and make sure that the tokenization scheme</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2311" target="_blank">00:38:31.560</a></span> | <span class="t">was kind of interoperable. And I think that was the I can dive into that in a bit later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2316" target="_blank">00:38:36.840</a></span> | <span class="t">too. Yeah. And note that does not mean we can send the exact actions for one robot to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2323" target="_blank">00:38:43.160</a></span> | <span class="t">another and have it execute. It was more just like in the data set, I think even by human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2327" target="_blank">00:38:47.320</a></span> | <span class="t">inspect, you can tell that these are coming from two different robots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2331" target="_blank">00:38:51.680</a></span> | <span class="t">So yeah, let's look at some ablations for the scaling laws that we're all here for now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2336" target="_blank">00:38:56.040</a></span> | <span class="t">We found that, you know, reducing data site size reduces performance. But more interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2340" target="_blank">00:39:00.240</a></span> | <span class="t">maybe is task diversity was quite important. Here we have two different trends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2346" target="_blank">00:39:06.600</a></span> | <span class="t">The green line is what happens when you reduce the total amount of episodes per task. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2352" target="_blank">00:39:12.120</a></span> | <span class="t">then gray on here, the purple curve is for what happens when you reduce the total number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2357" target="_blank">00:39:17.260</a></span> | <span class="t">of tasks. And we found that having more tasks is relatively more important than having more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2363" target="_blank">00:39:23.080</a></span> | <span class="t">data for each task. And I think this was a lesson that I think is probably going to suggest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2369" target="_blank">00:39:29.920</a></span> | <span class="t">ways that, you know, we should scale robotics even further is not to just collect more data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2374" target="_blank">00:39:34.080</a></span> | <span class="t">of the same task in the same settings, but to go out into the wild and get more diverse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2378" target="_blank">00:39:38.440</a></span> | <span class="t">behavior. How do you define diversity for data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2384" target="_blank">00:39:44.200</a></span> | <span class="t">Great question. Question is, how do you define data diversity? In this case, it's just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2389" target="_blank">00:39:49.040</a></span> | <span class="t">number of unique structured templated commands that teleoperators receive. So those 700 templated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2394" target="_blank">00:39:54.760</a></span> | <span class="t">commands, when we start reducing them and only train on 500 or only train on 300 of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2399" target="_blank">00:39:59.880</a></span> | <span class="t">them, performance drops much quicker than if we had taken the same proportional cuts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2405" target="_blank">00:40:05.200</a></span> | <span class="t">to the total amount. Yeah, so I guess I'm very familiar with, like, it seems almost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2415" target="_blank">00:40:15.760</a></span> | <span class="t">a linear relationship for diversity and structure. Yeah, I don't think we, the question was,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2425" target="_blank">00:40:25.280</a></span> | <span class="t">there seems to be almost a linear correlation between data size and success rate. And I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2429" target="_blank">00:40:29.240</a></span> | <span class="t">think, you know, we could apply some fancy, like, you know, scaling law, you know, trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2433" target="_blank">00:40:33.240</a></span> | <span class="t">to curve fitting, but we didn't look too much into that because, you know, this is a trend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2437" target="_blank">00:40:37.800</a></span> | <span class="t">that we kind of expected. We just weren't sure about the magnitude of how much it would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2441" target="_blank">00:40:41.640</a></span> | <span class="t">affect us. And I think I don't have any really good insights on this besides that we see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2449" target="_blank">00:40:49.000</a></span> | <span class="t">this phenomenon empirically. Yeah. Yeah, and great question. So the question is, oh, maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2468" target="_blank">00:41:08.880</a></span> | <span class="t">this will just go on indefinitely. Or is there something magical about, you know, January</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2472" target="_blank">00:41:12.760</a></span> | <span class="t">and I think this is maybe also a, this is one we start to conflate the algorithmic exploration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2479" target="_blank">00:41:19.920</a></span> | <span class="t">with like the practical considerations of scaling real world operations, which was when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2483" target="_blank">00:41:23.960</a></span> | <span class="t">we got enough data, our policies were hitting, you know, saturating on these hitting close</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2487" target="_blank">00:41:27.320</a></span> | <span class="t">to a hundred percent. We were like, all right, let's connect, collect another data set. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2491" target="_blank">00:41:31.360</a></span> | <span class="t">we basically collect until it's at a hundred and then we switch to something else. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2495" target="_blank">00:41:35.240</a></span> | <span class="t">at this point, what was interesting is that when we kind of bet really big on this RT1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2499" target="_blank">00:41:39.480</a></span> | <span class="t">feature, we'd already been collecting demos for a while. So it was possible that we had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2503" target="_blank">00:41:43.600</a></span> | <span class="t">collected more than we needed. And in some cases, I actually, you could cut tasks without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2507" target="_blank">00:41:47.680</a></span> | <span class="t">losing too much performance, which was quite interesting. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2510" target="_blank">00:41:50.200</a></span> | <span class="t">yeah, great question. And the question is whether or not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2540" target="_blank">00:42:20.040</a></span> | <span class="t">all tasks are created equal in terms of like their capacity and entropy for different behaviors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2544" target="_blank">00:42:24.120</a></span> | <span class="t">you could learn from them. And yeah, that's definitely true. Some tasks are much easier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2548" target="_blank">00:42:28.000</a></span> | <span class="t">We have a task that's just pick up this object. It's going to have much less interesting stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2551" target="_blank">00:42:31.840</a></span> | <span class="t">you can squeeze out of it then, you know, moving something into a drawer and then closing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2555" target="_blank">00:42:35.620</a></span> | <span class="t">the drawer. But yeah, great question. Great. Now ablations. We also trained without the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2563" target="_blank">00:42:43.760</a></span> | <span class="t">big model size. We did it without pre-training, without you know, with continuous instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2567" target="_blank">00:42:47.920</a></span> | <span class="t">of discrete actions, with autoregressive actions, without history, without the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2573" target="_blank">00:42:53.720</a></span> | <span class="t">And I think all of these design choices did seem to be required for robust performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2579" target="_blank">00:42:59.440</a></span> | <span class="t">Oh, yeah, of course. Yeah, I think all I mean, like, and again, you know, for paper writing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2597" target="_blank">00:43:17.240</a></span> | <span class="t">it's kind of like the best thing that we can empirically find. That's that's the method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2601" target="_blank">00:43:21.560</a></span> | <span class="t">And then we'll figure out why each of these are important. And so, yeah, I think what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2605" target="_blank">00:43:25.080</a></span> | <span class="t">one surprising thing here, perhaps, was that autoregressive actions hurt, you might think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2609" target="_blank">00:43:29.240</a></span> | <span class="t">that passing in more information is always better than passing in fewer, fewer information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2613" target="_blank">00:43:33.760</a></span> | <span class="t">But in this case, maybe conditioning on your previous actions was kind of doing kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2618" target="_blank">00:43:38.480</a></span> | <span class="t">like in context learning, it was doing online systems identification to figure out what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2624" target="_blank">00:43:44.120</a></span> | <span class="t">teleoperator this data came from, and like how you can overfit to that specific set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2628" target="_blank">00:43:48.680</a></span> | <span class="t">action history. And so removing that was actually better. One interesting tidbit there. Cool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2636" target="_blank">00:43:56.880</a></span> | <span class="t">then. And maybe in the interest of time, I'll try to get through the other ones a bit more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2642" target="_blank">00:44:02.760</a></span> | <span class="t">quicker. And then we can maybe just do a few, I'll just do the questions at the end, if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2647" target="_blank">00:44:07.120</a></span> | <span class="t">that's possible, just so we have time to get through everything. The next work here, moving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2651" target="_blank">00:44:11.700</a></span> | <span class="t">a bit away from skill learning, then and actually on to the planning level, I think the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2655" target="_blank">00:44:15.720</a></span> | <span class="t">project took a lot of the design principles of other fields, and this offline robot learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2660" target="_blank">00:44:20.400</a></span> | <span class="t">paradigm and put it into the skill learning. Can we actually bring that now to other parts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2664" target="_blank">00:44:24.680</a></span> | <span class="t">of the robotic system? And the first work here is SACAN. If you remember here, back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2668" target="_blank">00:44:28.640</a></span> | <span class="t">in this timeline, in 2022, we started thinking about, oh, yeah, how do we scale this multitask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2674" target="_blank">00:44:34.440</a></span> | <span class="t">imitation learning, but at the same time, large language models and, you know, other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2678" target="_blank">00:44:38.880</a></span> | <span class="t">types of foundation models are really picking up steam, whether it was Imogen or Dolly 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2684" target="_blank">00:44:44.440</a></span> | <span class="t">And we definitely wanted to figure out how we could use those as well. We had come up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2687" target="_blank">00:44:47.880</a></span> | <span class="t">with this RTU 2.1 design that we're betting big on. But from here, we started to explore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2693" target="_blank">00:44:53.080</a></span> | <span class="t">how all of the beta lesson 2.0, we could start utilizing foundation models within the context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2698" target="_blank">00:44:58.280</a></span> | <span class="t">of our full stack system. The problem of doing this naively is that language models are not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2704" target="_blank">00:45:04.880</a></span> | <span class="t">completely a very natural fit for robotics. For example, if you're a robot in a kitchen,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2709" target="_blank">00:45:09.900</a></span> | <span class="t">you ask a language model, I spilled my drink, what can you do? Language model will give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2712" target="_blank">00:45:12.960</a></span> | <span class="t">you stuff that's not very relevant. It's going to ask you to vacuum it, it's going to ask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2716" target="_blank">00:45:16.560</a></span> | <span class="t">you to call a cleaner, or it's going to apologize. And these are not things that the robot can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2720" target="_blank">00:45:20.640</a></span> | <span class="t">do in your kitchen with your spilled drink to help you. And so there are two parts of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2725" target="_blank">00:45:25.920</a></span> | <span class="t">this then. The one issue is that our robots are limited. They are very constrained with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2731" target="_blank">00:45:31.480</a></span> | <span class="t">what they can do. They cannot do everything, but they can do certain things. And then the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2735" target="_blank">00:45:35.600</a></span> | <span class="t">second problem is that the language models are also constrained. They don't know what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2740" target="_blank">00:45:40.560</a></span> | <span class="t">the robot sees. They don't understand that they are in a robot body in a micro kitchen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2744" target="_blank">00:45:44.760</a></span> | <span class="t">needing to do real stuff in the physical world. And so we need to get the robots to speak</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2750" target="_blank">00:45:50.240</a></span> | <span class="t">language model language, and then the language model to speak robot language. To do this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2755" target="_blank">00:45:55.440</a></span> | <span class="t">we present SACAN in the same setting. Please put an apple on the table. We score the predictions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2762" target="_blank">00:46:02.600</a></span> | <span class="t">of the language model on a constrained set of tasks that we know the robot has been trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2767" target="_blank">00:46:07.240</a></span> | <span class="t">to do. And then we also take the affordance function from the robot. An affordance function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2771" target="_blank">00:46:11.120</a></span> | <span class="t">is a estimation of, given some kind of state, what the robot is able to do, how confident</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2777" target="_blank">00:46:17.480</a></span> | <span class="t">it is that it can successfully accomplish that task in the given state. In our case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2781" target="_blank">00:46:21.480</a></span> | <span class="t">we use something like a value function from reinforcement learning, which kind of encompasses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2784" target="_blank">00:46:24.800</a></span> | <span class="t">this quality. Given these two values, these two scores, we have the confidence from a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2788" target="_blank">00:46:28.760</a></span> | <span class="t">language model, and then the confidence from the robot. We can combine these, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2793" target="_blank">00:46:33.080</a></span> | <span class="t">hopefully the combined prediction is both something that's going to be very semantically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2797" target="_blank">00:46:37.000</a></span> | <span class="t">relevant for the high level instruction. Finding an apple is the first step, and please put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2801" target="_blank">00:46:41.400</a></span> | <span class="t">an apple on the table. But it's also something that the robot can do. There's no robot in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2804" target="_blank">00:46:44.960</a></span> | <span class="t">the frame, but it knows that it's been trained to find an apple, so it can navigate around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2808" target="_blank">00:46:48.800</a></span> | <span class="t">to find it. And so hopefully we can do this then in closed loop, and then keep on going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2813" target="_blank">00:46:53.100</a></span> | <span class="t">and predicting a high level plan from the language model that's grounded with the affordance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2817" target="_blank">00:46:57.160</a></span> | <span class="t">function of what the robot understands. There's a video here of the SIGCHIAN doing different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2824" target="_blank">00:47:04.120</a></span> | <span class="t">stuff, but happy to share it later offline. It's very cool, trust me. It's the greatest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2829" target="_blank">00:47:09.420</a></span> | <span class="t">thing since sliced bread. And yeah, some numbers then. We tested this out on very long horizon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2840" target="_blank">00:47:20.800</a></span> | <span class="t">instructions encompassing more than 10 separate navigation and manipulation skills in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2846" target="_blank">00:47:26.080</a></span> | <span class="t">micro kitchen that you see on the bottom right. We evaluated hundreds of different evaluations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2852" target="_blank">00:47:32.460</a></span> | <span class="t">on this, and we tested out a lot of different concepts, including things like rephrasing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2857" target="_blank">00:47:37.300</a></span> | <span class="t">by using single primitives, by drawing instructions that just came from colleagues and friends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2863" target="_blank">00:47:43.980</a></span> | <span class="t">And then we found that while there were failures in both the language model planning side,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2869" target="_blank">00:47:49.420</a></span> | <span class="t">where it would predict the wrong path for the current situation, as well as on the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2873" target="_blank">00:47:53.000</a></span> | <span class="t">execution side, even when it gets a good plan, the robot will mess up sometimes. Overall,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2877" target="_blank">00:47:57.440</a></span> | <span class="t">it was still doing quite well. And now let's kind of take this back to the lesson. I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2885" target="_blank">00:48:05.520</a></span> | <span class="t">this is a very great example of how we can leverage internet scale foundation models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2891" target="_blank">00:48:11.320</a></span> | <span class="t">as they get better. When we started the project, we started with a language model called Flan</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2895" target="_blank">00:48:15.280</a></span> | <span class="t">from Google. Throughout our implementation, Palm came online, Pathways language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2901" target="_blank">00:48:21.040</a></span> | <span class="t">And when that happened, we were able to just hot swap it in, and performance just kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2905" target="_blank">00:48:25.860</a></span> | <span class="t">of got better for free without us having to do anything. By just assuming that language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2910" target="_blank">00:48:30.100</a></span> | <span class="t">was the API, the plan just has to be any string. It can come from any source. It can come from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2914" target="_blank">00:48:34.440</a></span> | <span class="t">a human. It can come from a language model. When we improve that language model, the system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2918" target="_blank">00:48:38.340</a></span> | <span class="t">gets better overall. And here you see with the scaling sizes as the model LLM increased</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2923" target="_blank">00:48:43.540</a></span> | <span class="t">in size, our planning performance got even better. And some cool tricks here to get it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2932" target="_blank">00:48:52.460</a></span> | <span class="t">working. Well, how do we actually produce this plan? Well, just by prompting, as is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2937" target="_blank">00:48:57.020</a></span> | <span class="t">the rich these days, with chain of thought and with better prompting of just giving examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2941" target="_blank">00:49:01.540</a></span> | <span class="t">of here are some great robot plans. Now give me a new plan starting with this high-level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2946" target="_blank">00:49:06.860</a></span> | <span class="t">instruction. We saw that the robot could do all things from understanding different languages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2951" target="_blank">00:49:11.660</a></span> | <span class="t">to asking them to do very complex reasoning, like, hey, give me something caffeinated,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2956" target="_blank">00:49:16.780</a></span> | <span class="t">or I don't do caffeine anymore. Get me something better. Or I could bring me a healthy snack</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2961" target="_blank">00:49:21.620</a></span> | <span class="t">versus bring me an unhealthy snack. Seikan was able to reason through all of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2969" target="_blank">00:49:29.380</a></span> | <span class="t">I think that was our kind of the first contact of robotics with language models on our team.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2974" target="_blank">00:49:34.520</a></span> | <span class="t">And it was the first exploration into how these two worlds could overlap. There was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2978" target="_blank">00:49:38.820</a></span> | <span class="t">definitely still improvements, though. And in our monologue, we tried to improve those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2981" target="_blank">00:49:41.940</a></span> | <span class="t">further by bringing in vision language models. The idea here is that we had very high plan</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2989" target="_blank">00:49:49.180</a></span> | <span class="t">rate success with Seikan. But unfortunately, it wasn't really able to recover from failures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2994" target="_blank">00:49:54.980</a></span> | <span class="t">What I mean by that is that the language model would not really get updates of what was going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=2999" target="_blank">00:49:59.040</a></span> | <span class="t">on in the world, so that if this was the plan it proposed, go to the table, pick up a Coke,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3003" target="_blank">00:50:03.140</a></span> | <span class="t">bring it to you, but you messed up picking the Coke can. You dropped it on the floor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3007" target="_blank">00:50:07.000</a></span> | <span class="t">It would still continue trying to bring it to you, put it aside, but all of that does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3010" target="_blank">00:50:10.020</a></span> | <span class="t">not really matter anymore because you dropped the Coke can. And so in this work, in our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3015" target="_blank">00:50:15.300</a></span> | <span class="t">monologue, we were really hoping to figure out how we could add closed loop dynamic feedback</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3020" target="_blank">00:50:20.180</a></span> | <span class="t">from the environment into this planning process. Let's take that exact same example. Now, instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3027" target="_blank">00:50:27.240</a></span> | <span class="t">of just directly printing every instruction, maybe we add back some feedback from the scene,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3031" target="_blank">00:50:31.980</a></span> | <span class="t">also conveyed using language as the universal API here. The scene can tell you what's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3036" target="_blank">00:50:36.980</a></span> | <span class="t">in there. Maybe the robot asks a question now. In the robot, this is a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3041" target="_blank">00:50:41.660</a></span> | <span class="t">asking the clarification question. Maybe here, a human responds or another language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3045" target="_blank">00:50:45.380</a></span> | <span class="t">model. Then you can predict the action or the next task to do once the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3049" target="_blank">00:50:49.860</a></span> | <span class="t">has enough context. And maybe you even add in stuff like success detection and so on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3054" target="_blank">00:50:54.660</a></span> | <span class="t">and so forth. How do we do this then? Well, the first thing that we implement is what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3060" target="_blank">00:51:00.700</a></span> | <span class="t">we call passive scene description. Just using either an off-the-shelf engineered heuristic,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3066" target="_blank">00:51:06.060</a></span> | <span class="t">using object detection models, something like Vylde, you can describe the scene in text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3070" target="_blank">00:51:10.980</a></span> | <span class="t">and just convey all that context to the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3075" target="_blank">00:51:15.420</a></span> | <span class="t">For active scene description, this is maybe similar to visual question answering if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3078" target="_blank">00:51:18.780</a></span> | <span class="t">familiar with that field. The language model can actually propose active queries that it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3084" target="_blank">00:51:24.740</a></span> | <span class="t">curious about in the scene, maybe to make sure that it has enough context to move on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3088" target="_blank">00:51:28.940</a></span> | <span class="t">And here, either a human can provide the answer, or in the future, a VQA model as they improve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3094" target="_blank">00:51:34.100</a></span> | <span class="t">can provide that. And finally, for success detection, this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3098" target="_blank">00:51:38.340</a></span> | <span class="t">very important to allow the language model planner to know when to try to retry something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3103" target="_blank">00:51:43.940</a></span> | <span class="t">Here we take in the first and last image, fine tune a clip success detector, and use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3108" target="_blank">00:51:48.420</a></span> | <span class="t">that to provide binary success/failure information back to our language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3116" target="_blank">00:51:56.620</a></span> | <span class="t">And for the results-wise, we can see a very similar SACAN long-horizon evaluation, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3122" target="_blank">00:52:02.340</a></span> | <span class="t">here what's interesting is that we're able to basically implement all these different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3128" target="_blank">00:52:08.340</a></span> | <span class="t">automated feedback mechanisms on the robot, and so that it's able to reason and recover</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3133" target="_blank">00:52:13.440</a></span> | <span class="t">from things. Here you see it's going to try to go to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3137" target="_blank">00:52:17.660</a></span> | <span class="t">table, but the human's actually been saying, "Hey, I changed my mind." And then the human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3143" target="_blank">00:52:23.300</a></span> | <span class="t">changes mind again, asking it to go back and forth. And the robot's able to, maybe we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3147" target="_blank">00:52:27.720</a></span> | <span class="t">kind of torturing the language model at this point, but the language model's able to replan</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3151" target="_blank">00:52:31.300</a></span> | <span class="t">and make sure that the human intent is satisfied. We also tried, I'm not sure if this video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3159" target="_blank">00:52:39.980</a></span> | <span class="t">shows it, but situations where we did adversarial inputs, where I walked around and just kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3164" target="_blank">00:52:44.780</a></span> | <span class="t">of knocking objects out of the robot's hands and forcing the success detector to tell it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3169" target="_blank">00:52:49.620</a></span> | <span class="t">"You messed up, try again." And we also tried this out on a couple of different domains,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3180" target="_blank">00:53:00.140</a></span> | <span class="t">a simulated tabletop manipulation domain, as well as a real-world manipulation domain,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3184" target="_blank">00:53:04.200</a></span> | <span class="t">and we found that this was much better than SACAN, or let's say just only using visual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3190" target="_blank">00:53:10.200</a></span> | <span class="t">features themselves with something like Clipboard. And I think here, it really speaks towards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3197" target="_blank">00:53:17.600</a></span> | <span class="t">a trend that I've really come to appreciate. In 2018, a robotics professor once said that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3203" target="_blank">00:53:23.200</a></span> | <span class="t">when they looked at all the different things preventing robot learning from scaling tremendously,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3207" target="_blank">00:53:27.160</a></span> | <span class="t">it thought the bottleneck was high-level semantic planning, about reasoning, about common sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3211" target="_blank">00:53:31.460</a></span> | <span class="t">And I think in 2022 and 2023, language models can provide a one path of how this can kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3218" target="_blank">00:53:38.020</a></span> | <span class="t">of be offloaded, at least in the interim. And I think if language models are the API,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3223" target="_blank">00:53:43.820</a></span> | <span class="t">then you can just bring in these vision language models as object detectors get better, as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3228" target="_blank">00:53:48.240</a></span> | <span class="t">success detectors, as VQA, as language models get better, you can bring them all into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3231" target="_blank">00:53:51.980</a></span> | <span class="t">fold and they act as kind of a life vest. If your robot currently does not have common</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3236" target="_blank">00:53:56.820</a></span> | <span class="t">sense reasoning, these other models can act as a scaffold and a life vest to bring you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3241" target="_blank">00:54:01.140</a></span> | <span class="t">up to par with what they currently love. And maybe then in the future, you'll get beyond</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3245" target="_blank">00:54:05.620</a></span> | <span class="t">what the language models know, but in the short term, it does seem that we can leverage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3248" target="_blank">00:54:08.620</a></span> | <span class="t">them to accelerate what we can do in the real world. Moving on now from, we saw now how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3255" target="_blank">00:54:15.840</a></span> | <span class="t">language models can do planning. We saw how vision language models can help planning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3259" target="_blank">00:54:19.860</a></span> | <span class="t">And now we're going to switch gears a bit and think about how vision language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3262" target="_blank">00:54:22.980</a></span> | <span class="t">can help other aspects of the bottlenecks that robot learning faces. One of these is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3269" target="_blank">00:54:29.140</a></span> | <span class="t">that data collection is very expensive. As we mentioned before, we did have this 130,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3276" target="_blank">00:54:36.140</a></span> | <span class="t">demonstration data set, but it was collected over a year and a half at significant cost,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3282" target="_blank">00:54:42.180</a></span> | <span class="t">both in resources and time and money and with many, many robots. And of course, these tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3289" target="_blank">00:54:49.140</a></span> | <span class="t">too were also a bit limited, right? We use 700 very templated commands, instructions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3295" target="_blank">00:54:55.020</a></span> | <span class="t">that we give to teleoperators, because we knew that this would scale, right? If we collected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3300" target="_blank">00:55:00.140</a></span> | <span class="t">enough data for each of these templated tasks, we could do that specific task. And here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3305" target="_blank">00:55:05.420</a></span> | <span class="t">the flow that someone was asking about earlier. We give this PICOCAN instruction, the operator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3309" target="_blank">00:55:09.820</a></span> | <span class="t">controls the robot in the real world, finish the task, marks the episode as terminate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3314" target="_blank">00:55:14.620</a></span> | <span class="t">and then shade that out to this big orange data set. And that big orange data set is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3318" target="_blank">00:55:18.580</a></span> | <span class="t">what we trained on in all of the previous projects for the control policies. What we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3323" target="_blank">00:55:23.060</a></span> | <span class="t">additionally considered was adding a bit of crowdsourced hindsight annotation. If you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3327" target="_blank">00:55:27.420</a></span> | <span class="t">familiar with it, with a hindsight experience replay and reinforcement learning with goal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3331" target="_blank">00:55:31.340</a></span> | <span class="t">conditioning with, you know, maybe the robot did something that wasn't just this high level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3336" target="_blank">00:55:36.500</a></span> | <span class="t">template instruction. We could ask a human to describe more verbosely what the robot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3341" target="_blank">00:55:41.500</a></span> | <span class="t">did. Maybe it picked up the COCAN that was on the right side of the table. Maybe it picked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3345" target="_blank">00:55:45.060</a></span> | <span class="t">it up and then knocked it over. Maybe it moved it very slowly to the middle. There's a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3349" target="_blank">00:55:49.500</a></span> | <span class="t">of semantic diversity encompassed in this demonstration that is not totally caught by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3356" target="_blank">00:55:56.460</a></span> | <span class="t">this high level templated PICOCAN instruction. So we labeled 3% of this big orange data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3362" target="_blank">00:56:02.580</a></span> | <span class="t">with these very verbose descriptions. And next, we kind of applied the pseudo-label</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3369" target="_blank">00:56:09.420</a></span> | <span class="t">strategy that's been seen in other fields, such as video pre-training with their inverse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3373" target="_blank">00:56:13.860</a></span> | <span class="t">dynamics model. But instead, we apply that to the instructions, to the semantics of what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3378" target="_blank">00:56:18.700</a></span> | <span class="t">contained in your data set. So step one, we pre-train a clip model on your small label</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3385" target="_blank">00:56:25.380</a></span> | <span class="t">data set of 3% of your main data. Then you go ahead and use that train BLM data to label</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3392" target="_blank">00:56:32.420</a></span> | <span class="t">all of the templated instruction demonstrations that you had before that 130,000 episode data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3398" target="_blank">00:56:38.540</a></span> | <span class="t">sets. Now you have a re-labeled data set, which has a large diversity of interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3403" target="_blank">00:56:43.380</a></span> | <span class="t">semantic instructions. And then we plug in all of these data sets into RT1 and just train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3410" target="_blank">00:56:50.140</a></span> | <span class="t">a language condition behavior cloning policy, similarly to how we would normally. But even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3415" target="_blank">00:56:55.740</a></span> | <span class="t">though normally we just use data set B, the orange one, now we use all three data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3421" target="_blank">00:57:01.540</a></span> | <span class="t">And then finally, we evaluate on entirely new unseen instructions. In the prior works,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3428" target="_blank">00:57:08.340</a></span> | <span class="t">we were evaluating mainly on the 700 templated instructions. But in this work, we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3433" target="_blank">00:57:13.100</a></span> | <span class="t">go beyond that. We can type in almost anything you want that you think might succeed. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3438" target="_blank">00:57:18.620</a></span> | <span class="t">you can phrase it how you can. You can add typos. You can even do it by referring to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3443" target="_blank">00:57:23.340</a></span> | <span class="t">semantic concepts. You can add spatial concepts. And we see how it does. The reason that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3450" target="_blank">00:57:30.540</a></span> | <span class="t">might work, maybe visually to represent this, is here are the t-SNE embeddings on the left</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3456" target="_blank">00:57:36.300</a></span> | <span class="t">and the right. It's the same embeddings. But on the left, they're colored by the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3461" target="_blank">00:57:41.340</a></span> | <span class="t">templated instruction that was used to collect that episode. And on the right is what the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3467" target="_blank">00:57:47.460</a></span> | <span class="t">vision language model thinks. If it's allowed to put a free form natural language caption</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3472" target="_blank">00:57:52.700</a></span> | <span class="t">and assign it to that episode, you see that on the left, you have these big clusters of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3476" target="_blank">00:57:56.660</a></span> | <span class="t">pick cocaine is like, you know, hundreds or thousands of episodes, but we all just call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3480" target="_blank">00:58:00.620</a></span> | <span class="t">them pick cocaine. On the right, then we can then expand those concepts and say, actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3484" target="_blank">00:58:04.860</a></span> | <span class="t">this episode is picking up the red cocaine. This episode is picking up the crumpled cocaine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3490" target="_blank">00:58:10.320</a></span> | <span class="t">This is picking up the cocaine that's next to the chip bag. And so you can get a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3494" target="_blank">00:58:14.160</a></span> | <span class="t">more mileage out of the same underlying data set by just using language as the diversity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3499" target="_blank">00:58:19.140</a></span> | <span class="t">mechanism through which you kind of expand the concepts that you're considering. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3503" target="_blank">00:58:23.300</a></span> | <span class="t">for example, in the middle, you see, you know, open top drawer can become hold and pull out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3507" target="_blank">00:58:27.500</a></span> | <span class="t">the top drawer. We have stuff like the center left for the middle, for the middle episode,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3512" target="_blank">00:58:32.740</a></span> | <span class="t">for the bottom one, pick green rice chips from white bowl becomes lift up the green</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3516" target="_blank">00:58:36.500</a></span> | <span class="t">chip bag from the bowl and drop it at the bottom left corner of the table. So you got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3519" target="_blank">00:58:39.860</a></span> | <span class="t">a lot of these semantic, you know, spatial concepts that are now going to be in your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3524" target="_blank">00:58:44.080</a></span> | <span class="t">target supervised labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3525" target="_blank">00:58:45.880</a></span> | <span class="t">I have a question. Yeah. Great question. So I guess if I can rephrase a bit, the problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3555" target="_blank">00:59:15.740</a></span> | <span class="t">is that like, it's actually a very difficult and perhaps even untrackable problem of how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3560" target="_blank">00:59:20.180</a></span> | <span class="t">you map all the linguistic concepts you see out in the wild down to like, maybe like embodied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3565" target="_blank">00:59:25.060</a></span> | <span class="t">specific types of episodes. And like, here, maybe I would say is that we are definitely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3570" target="_blank">00:59:30.180</a></span> | <span class="t">introducing a lot of our priors and our biases onto like, maybe what we call as left, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3575" target="_blank">00:59:35.980</a></span> | <span class="t">mean left 10 centimeters of two centimeters, like, like, what do words mean? And these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3581" target="_blank">00:59:41.420</a></span> | <span class="t">definitions, what do they mean to us, to the crowd compute raters that generated these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3585" target="_blank">00:59:45.980</a></span> | <span class="t">captions? What do they mean to the robot? What do they mean to the language models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3588" target="_blank">00:59:48.860</a></span> | <span class="t">Maybe these are all slightly different, but the hope is at least if they're roughly similar,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3594" target="_blank">00:59:54.180</a></span> | <span class="t">we'll get like directionally correct improvements. So I would say the nuances of this specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3599" target="_blank">00:59:59.660</a></span> | <span class="t">hard lines of definitions and like actual, like, you know, semantic meaning of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3604" target="_blank">01:00:04.900</a></span> | <span class="t">words, I think that's maybe out of scope right now, but maybe something we'll dive into further</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3610" target="_blank">01:00:10.500</a></span> | <span class="t">at a higher level, though. I think basically the bar is just so low. We have the 700 template</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3614" target="_blank">01:00:14.940</a></span> | <span class="t">instructions that are basically one hot IDs, and we just want to make those closer to natural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3620" target="_blank">01:00:20.100</a></span> | <span class="t">language, even if by a little. And I think at least we're, we're, we're trying to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3625" target="_blank">01:00:25.500</a></span> | <span class="t">towards that with these vision language models that are captioning automatically. Hope that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3630" target="_blank">01:00:30.180</a></span> | <span class="t">answers your question. And we also compare it to a few baselines on the top left here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3637" target="_blank">01:00:37.820</a></span> | <span class="t">We look at what if we only train on this 3% of these fancy human rated labels? What if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3643" target="_blank">01:00:43.580</a></span> | <span class="t">we only train on the original RT1 data sets? What if we train on both of these? And what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3648" target="_blank">01:00:48.820</a></span> | <span class="t">if we train on both of these plus all of the predictions given by our BLM? And what's interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3654" target="_blank">01:00:54.120</a></span> | <span class="t">here is that, you know, relabeling seems to universally help. We evaluated only on novel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3661" target="_blank">01:01:01.540</a></span> | <span class="t">instructions that was new for this project. It's the first time on a robotics project</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3665" target="_blank">01:01:05.020</a></span> | <span class="t">where we only tested on sentence, I could type whatever I thought I'll type it in. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3668" target="_blank">01:01:08.780</a></span> | <span class="t">that became the test set. And we just had to make sure that it was never contained in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3673" target="_blank">01:01:13.300</a></span> | <span class="t">the training coverage. And you see all these interesting examples on the right here of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3677" target="_blank">01:01:17.700</a></span> | <span class="t">stuff like move the lonely object to the others. I have no idea how this works. Stuff like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3683" target="_blank">01:01:23.580</a></span> | <span class="t">news, lifting the yellow rectangle, talking about colors, talking about move the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3687" target="_blank">01:01:27.700</a></span> | <span class="t">apple to the left. Here, we actually had two apples in the scene. And actually in our training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3692" target="_blank">01:01:32.420</a></span> | <span class="t">demonstration data, we never collected scenes with duplicate objects, just because, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3697" target="_blank">01:01:37.180</a></span> | <span class="t">know, we thought of this multi-modality problem. If you just say pick cocaine in this two cocaine,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3701" target="_blank">01:01:41.100</a></span> | <span class="t">it's going to be very difficult to figure out which one to do. But with language labeling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3704" target="_blank">01:01:44.860</a></span> | <span class="t">it seems like maybe we could do that now. So even though we never trained on scenes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3707" target="_blank">01:01:47.900</a></span> | <span class="t">of two apples, now you can evaluate on them and just specify with language, which apple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3712" target="_blank">01:01:52.180</a></span> | <span class="t">you want to go for. And it was working pretty reasonably. And finally, for the last example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3718" target="_blank">01:01:58.340</a></span> | <span class="t">here, I thought it was kind of interesting. A single cocaine, we try to do a novel behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3723" target="_blank">01:02:03.500</a></span> | <span class="t">Push towards the left was not a templated instruction. We only had move cocaine near</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3729" target="_blank">01:02:09.180</a></span> | <span class="t">Y, where Y is another object, move cocaine near apple, move cocaine near sponge. So pushing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3735" target="_blank">01:02:15.260</a></span> | <span class="t">this motion of just pushing the cocaine into air essentially was not something that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3740" target="_blank">01:02:20.340</a></span> | <span class="t">ever encompassed, but maybe it was in one of the labels. Maybe like if you've seen like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3744" target="_blank">01:02:24.700</a></span> | <span class="t">move cocaine near apple and apples on the left, and you saw move cocaine near sponge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3749" target="_blank">01:02:29.060</a></span> | <span class="t">and the sponge is on the left, you would general, the model can generalize and be like, oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3752" target="_blank">01:02:32.620</a></span> | <span class="t">left means this side of the table, not a specific object. So maybe that's what's happening,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3757" target="_blank">01:02:37.380</a></span> | <span class="t">but it's very unclear. This is, as I said, you know, just, I type, I thought of something,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3762" target="_blank">01:02:42.380</a></span> | <span class="t">I typed it and just saw what happened. And we definitely hope to explore this more quantitatively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3766" target="_blank">01:02:46.980</a></span> | <span class="t">in the future. Bottom left, of course, is I think comparing against non-visual augmentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3771" target="_blank">01:02:51.980</a></span> | <span class="t">So maybe you can also get these interesting concepts just from language alone, right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3776" target="_blank">01:02:56.380</a></span> | <span class="t">We had adding random noise or we do Madlib style, just swapping out words, or we even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3781" target="_blank">01:03:01.260</a></span> | <span class="t">use a LLM GPT-3 in this case to propose rephrasing of existing instructions. But I think my takeaway</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3787" target="_blank">01:03:07.620</a></span> | <span class="t">there is that you really need visual grounding for the visual language model to say, actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3792" target="_blank">01:03:12.260</a></span> | <span class="t">yeah, this caption is factually accurate at this given point in time. And that it's, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3797" target="_blank">01:03:17.300</a></span> | <span class="t">know, something perhaps that would be interesting for a robot. That fine-tuning process provides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3802" target="_blank">01:03:22.300</a></span> | <span class="t">both of those. Yeah, yeah, definitely. These are just some subsets of five of these evaluation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3817" target="_blank">01:03:37.860</a></span> | <span class="t">instructions, but we had over 60 of them. We didn't do a full quantitative ablation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3822" target="_blank">01:03:42.620</a></span> | <span class="t">for example, as we did in RT1. We had this like seen and unseen task set, and that was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3827" target="_blank">01:03:47.900</a></span> | <span class="t">compositional. You would see, you know, move Coke near Apple, and you would see move Apple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3832" target="_blank">01:03:52.500</a></span> | <span class="t">near sponge, but we'd hold out, move Coke near sponge, and we would test that out. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3836" target="_blank">01:03:56.420</a></span> | <span class="t">in this case, I think we can go much more beyond that. Because our language is completely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3839" target="_blank">01:03:59.860</a></span> | <span class="t">freeform, the compositional space of what you can kind of combine is just going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3844" target="_blank">01:04:04.700</a></span> | <span class="t">much larger. So we did try a little bit to answer your question. We tried some combinatorial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3848" target="_blank">01:04:08.860</a></span> | <span class="t">evaluations, but there's definitely a lot more thoroughness that we could do there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3854" target="_blank">01:04:14.140</a></span> | <span class="t">too. How am I doing on time? Okay, 10 minutes. Maybe I'll try to wrap up pretty soon, then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3860" target="_blank">01:04:20.180</a></span> | <span class="t">The dial-up takeaway, then, is that two parts, right? Lesson two, leverage foundation models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3865" target="_blank">01:04:25.260</a></span> | <span class="t">Let's use them as data augmentation. And lesson three, let's make sure that our offline data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3869" target="_blank">01:04:29.460</a></span> | <span class="t">set, you know, is robust enough where these different behaviors exist, and you can describe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3874" target="_blank">01:04:34.500</a></span> | <span class="t">them in language. If you don't have enough diverse behaviors, no matter how good your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3877" target="_blank">01:04:37.940</a></span> | <span class="t">labeling is, you probably can't elicit all of the interesting concepts that you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3881" target="_blank">01:04:41.540</a></span> | <span class="t">to learn from. And maybe most exciting for me here was that actually some label noise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3886" target="_blank">01:04:46.660</a></span> | <span class="t">is okay. Notoriously, in supervised learning and imitation learning, you need very clean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3890" target="_blank">01:04:50.980</a></span> | <span class="t">labels that are always 100% true, right? You don't want to be learning from, like, noisy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3895" target="_blank">01:04:55.540</a></span> | <span class="t">data where some, like, you know, large percentage is just not accurate. But in our case, it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3900" target="_blank">01:05:00.180</a></span> | <span class="t">seems that, like, some label noise was okay. The vision language model was not always predicting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3906" target="_blank">01:05:06.340</a></span> | <span class="t">factually accurate descriptions of the scene. And I think this definitely hurt when it got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3912" target="_blank">01:05:12.140</a></span> | <span class="t">too high, the noise, but at smaller levels, it definitely still seemed to be okay and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3917" target="_blank">01:05:17.060</a></span> | <span class="t">robust enough to handle that. So, that was a deep dive, then, on some individual works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3923" target="_blank">01:05:23.560</a></span> | <span class="t">that use this big recipe of language, foundation models, offline data sets in different parts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3929" target="_blank">01:05:29.460</a></span> | <span class="t">of the robot system. And this was the kind of pitch at the beginning, and I hope you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3935" target="_blank">01:05:35.260</a></span> | <span class="t">at least see a little bit of how our team has tried to take these principles and apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3939" target="_blank">01:05:39.580</a></span> | <span class="t">them to accelerating robot learning in the real world. As we see these different types</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3944" target="_blank">01:05:44.660</a></span> | <span class="t">of ingredients and lessons map onto different parts of the robot system altogether. For</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3950" target="_blank">01:05:50.300</a></span> | <span class="t">skill learning, right, that was RQ1 that we talked about. For planning, that was SACAN,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3954" target="_blank">01:05:54.260</a></span> | <span class="t">and then adding the closed-loop feedback with vision language models, that was inner monologue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3958" target="_blank">01:05:58.580</a></span> | <span class="t">For low-level control, we didn't talk about this today, but an exciting work from our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3961" target="_blank">01:06:01.700</a></span> | <span class="t">team is actually using language models to predict code that's executed on the robot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3966" target="_blank">01:06:06.180</a></span> | <span class="t">directly, perhaps as low-level controllers. Language models, you know, they read textbooks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3971" target="_blank">01:06:11.100</a></span> | <span class="t">they've read raw stocks, they've read, you know, UR5 documentation code, and they can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3974" target="_blank">01:06:14.860</a></span> | <span class="t">write code for these robots, and we can execute that. For data augmentation, we saw Dial with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3980" target="_blank">01:06:20.100</a></span> | <span class="t">vision language models. And also, I didn't talk about this here, but for object-centric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3985" target="_blank">01:06:25.100</a></span> | <span class="t">representations, for things like feature activation maps for specific objects, we can use those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3989" target="_blank">01:06:29.820</a></span> | <span class="t">as task representation for mapping a scene. And in NLMAP, they did that for object-centric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=3996" target="_blank">01:06:36.100</a></span> | <span class="t">navigation around the micro kitchen that we looked at. And I think, hopefully, in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4001" target="_blank">01:06:41.500</a></span> | <span class="t">next, you know, coming weeks and months, we have a few more rows and entries to add here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4005" target="_blank">01:06:45.780</a></span> | <span class="t">as well, but I think this kind of mindset is a very exciting research direction of how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4012" target="_blank">01:06:52.020</a></span> | <span class="t">you can apply these big high-level concepts about foundation models and offline data sets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4016" target="_blank">01:06:56.380</a></span> | <span class="t">when you look at what exists in the robot systems of today, and you find many gaps and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4020" target="_blank">01:07:00.700</a></span> | <span class="t">opportunities still available where we can do everything from exploratory pilots on how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4025" target="_blank">01:07:05.880</a></span> | <span class="t">this might look like, all the way to more extensive evaluations and really building</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4029" target="_blank">01:07:09.460</a></span> | <span class="t">out robust systems. I think both of these have value. So, I'll conclude with just saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4036" target="_blank">01:07:16.380</a></span> | <span class="t">that it was very fun exploring all of these complementary directions, but there are still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4041" target="_blank">01:07:21.160</a></span> | <span class="t">some major questions of how we can take these concepts even further, and how these trends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4045" target="_blank">01:07:25.680</a></span> | <span class="t">and ideas might even evolve moving forward as foundation models get better, as more data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4050" target="_blank">01:07:30.380</a></span> | <span class="t">set becomes available online, as more data becomes homogenized and tokenized and interoperable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4056" target="_blank">01:07:36.120</a></span> | <span class="t">And I think a lot of the concepts from other fields, like linguistics and vision, and from,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4060" target="_blank">01:07:40.660</a></span> | <span class="t">you know, all of the big scaling kind of level questions that are being pioneered in language-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4066" target="_blank">01:07:46.580</a></span> | <span class="t">foundation models, hopefully, those kind of ideas can trickle down to robotics. Maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4070" target="_blank">01:07:50.660</a></span> | <span class="t">even robotics can provide something back by providing embodied action causal data sets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4075" target="_blank">01:07:55.380</a></span> | <span class="t">that maybe might improve the quality of reasoning of some of these large language models that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4079" target="_blank">01:07:59.820</a></span> | <span class="t">are not embodied. With that, though, I guess I'd like to, you know, thank everyone for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4085" target="_blank">01:08:05.520</a></span> | <span class="t">your time and for Dave and Sia for inviting me, and open to any questions about the papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4091" target="_blank">01:08:11.000</a></span> | <span class="t">or just at a high level as well. Thanks so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4116" target="_blank">01:08:36.020</a></span> | <span class="t">Yeah great question. So the question, I guess, is like, what about tasks that require more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4119" target="_blank">01:08:39.560</a></span> | <span class="t">semantic reasoning, like, you know, operating at a certain speed or with maybe like, I don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4124" target="_blank">01:08:44.400</a></span> | <span class="t">know, numerical reasoning within the question, the prompt itself. I would say, so for a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4130" target="_blank">01:08:50.040</a></span> | <span class="t">of the more common sense reasoning, like, you know, throw away three co-cans, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4135" target="_blank">01:08:55.880</a></span> | <span class="t">after another, I think, you know, the language model is very good at that right now. So for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4140" target="_blank">01:09:00.160</a></span> | <span class="t">the secant planner, it will predict, you know, throw away the co-can three separate times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4145" target="_blank">01:09:05.260</a></span> | <span class="t">For the low level skill policy learning, though, I think that's more of a, that's more high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4151" target="_blank">01:09:11.120</a></span> | <span class="t">variance, I would say. And definitely for right now, we don't really condition on speed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4156" target="_blank">01:09:16.680</a></span> | <span class="t">or how you do it exactly. But that's definitely maybe something I could do if you could relabel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4162" target="_blank">01:09:22.420</a></span> | <span class="t">with, like, pick up the co-can slowly versus pick up the co-can quickly. Maybe that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4166" target="_blank">01:09:26.840</a></span> | <span class="t">something a vision language model could recognize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4195" target="_blank">01:09:55.660</a></span> | <span class="t">The question was, at what scale do we see like combinatorial generalization start to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4200" target="_blank">01:10:00.700</a></span> | <span class="t">occur, maybe between like, you've seen colors of one block, and then you want to evaluate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4204" target="_blank">01:10:04.820</a></span> | <span class="t">on a new color? And I think that's a great question. And unfortunately, my answer is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4209" target="_blank">01:10:09.060</a></span> | <span class="t">going to be very vague. And it depends. It depends on how you define your tasks. It depends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4213" target="_blank">01:10:13.240</a></span> | <span class="t">on the scale of your data set. And it depends on like, the concepts that you're trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4216" target="_blank">01:10:16.400</a></span> | <span class="t">generalize across. I think there have been numerous attempts to kind of basically formalize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4222" target="_blank">01:10:22.440</a></span> | <span class="t">what it means to generalize within, you know, learning and within robotics, even within</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4226" target="_blank">01:10:26.960</a></span> | <span class="t">like the specific settings we consider. And I don't think there are any clear trends be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4231" target="_blank">01:10:31.600</a></span> | <span class="t">like, of where you can say, oh, yeah, this is the number I need to hit where, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4235" target="_blank">01:10:35.480</a></span> | <span class="t">I can generalize across x, y, z dimensions. Like, you could evaluate all those, but I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4239" target="_blank">01:10:39.680</a></span> | <span class="t">don't think it will help you predict new trends, at least right now. I think we're probably,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4243" target="_blank">01:10:43.160</a></span> | <span class="t">you know, this is just me talking, I would say we're one order of magnitude off before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4247" target="_blank">01:10:47.480</a></span> | <span class="t">we can start to make very broadly generalizing statements about generalization capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4253" target="_blank">01:10:53.680</a></span> | <span class="t">I think, you know, add one or two more zeros to our data set size, and we can start to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4257" target="_blank">01:10:57.360</a></span> | <span class="t">talk about that in terms of task object skills. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4278" target="_blank">01:11:18.440</a></span> | <span class="t">Yeah, very astute observation. So the question, the question was that in SACAN, the value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4294" target="_blank">01:11:34.200</a></span> | <span class="t">functions that predict these scalars on the right here for the affordances are only storing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4299" target="_blank">01:11:39.280</a></span> | <span class="t">a certain limited number of tasks. So is that the bottleneck? And I would say yes, 100%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4304" target="_blank">01:11:44.000</a></span> | <span class="t">Scaling the number of tasks that your system is able to do that you can then give to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4308" target="_blank">01:11:48.240</a></span> | <span class="t">planner as its buffet of options to choose, that is the bottleneck, right? No matter how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4312" target="_blank">01:11:52.480</a></span> | <span class="t">good your planner is, if you can only do like three tasks, there's only certain like combinations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4318" target="_blank">01:11:58.320</a></span> | <span class="t">of those three tasks that it can do to, you know, map on to a high level instruction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4322" target="_blank">01:12:02.920</a></span> | <span class="t">So as you add more tasks, as the low level skill capabilities of your robot increases,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4328" target="_blank">01:12:08.160</a></span> | <span class="t">you're kind of like adding precision to like the coverage of the high level instructions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4333" target="_blank">01:12:13.040</a></span> | <span class="t">that your robot can try to do. So that's one of the main bottlenecks I see today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4352" target="_blank">01:12:32.080</a></span> | <span class="t">Great question. So have we tried RQ1 with RLHF or with RL? I think the short answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4359" target="_blank">01:12:39.920</a></span> | <span class="t">is I think we have some stuff in the works that is doing that. But right now, for all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4363" target="_blank">01:12:43.760</a></span> | <span class="t">of our projects, currently, we're just using this implementation learning loss. Again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4369" target="_blank">01:12:49.240</a></span> | <span class="t">I think I view this multitasking limitation bet that we're making is kind of an existence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4372" target="_blank">01:12:52.960</a></span> | <span class="t">proof. It works, it's not cheap, but it kind of does work and it does scale. And that at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4378" target="_blank">01:12:58.160</a></span> | <span class="t">least is a good starting point. And our main, you know, hope over the next months and years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4383" target="_blank">01:13:03.120</a></span> | <span class="t">is can we improve beyond that? Can we add back in offline improvement? You know, can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4387" target="_blank">01:13:07.040</a></span> | <span class="t">we add in RL back to the equation somehow? I'm an RL person at heart, so I really hope</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4391" target="_blank">01:13:11.520</a></span> | <span class="t">so. Sorry, could you repeat that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4417" target="_blank">01:13:37.040</a></span> | <span class="t">Yeah, good question. So regarding task balance and whether text-only data is sufficient for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4434" target="_blank">01:13:54.200</a></span> | <span class="t">helping motor control learning, I think my hope is that when, you know, when we experience</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4441" target="_blank">01:14:01.800</a></span> | <span class="t">emergence in both the robotics space and we've already seen emergence in the language space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4446" target="_blank">01:14:06.600</a></span> | <span class="t">at some point, maybe these reasoning concepts will start to transfer between the two. I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4450" target="_blank">01:14:10.520</a></span> | <span class="t">would point them to one interesting paper, which is, I think, can Wikipedia help reinforce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4455" target="_blank">01:14:15.120</a></span> | <span class="t">that learning from Shane and some other folks? They pre-train, you know, a large policy network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4461" target="_blank">01:14:21.360</a></span> | <span class="t">on like, you know, auto-aggressive token prediction on Wikipedia, just text only, and they use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4465" target="_blank">01:14:25.880</a></span> | <span class="t">that to initialize, like, control for Atari games with RL, and this actually helped. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4471" target="_blank">01:14:31.400</a></span> | <span class="t">you know, maybe this is philosophical, but maybe there's something about decision-making</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4474" target="_blank">01:14:34.800</a></span> | <span class="t">reasoning that transfers between text and action data, so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4482" target="_blank">01:14:42.760</a></span> | <span class="t">Great question. I definitely agree. You know, passing in six images is not going to be enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4498" target="_blank">01:14:58.840</a></span> | <span class="t">when you're executing tasks for minutes at a time. Like, clean my whole house, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4502" target="_blank">01:15:02.600</a></span> | <span class="t">you can only pass in the last, like, you know, two seconds. Like, come on. So, I think that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4507" target="_blank">01:15:07.400</a></span> | <span class="t">definitely going to be a limitation as our tasks set more complex and long horizon, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4512" target="_blank">01:15:12.280</a></span> | <span class="t">I think here, it's another open question, too, is context length. We have high-dimensional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4516" target="_blank">01:15:16.960</a></span> | <span class="t">images, even with token learning for reducing the number of patches that we pass through,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4522" target="_blank">01:15:22.100</a></span> | <span class="t">it's still, you know, very high-dimensional, and we quickly hit the context length cap.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4526" target="_blank">01:15:26.620</a></span> | <span class="t">Can we do, how do we, you know, improve beyond this? Maybe it's like retrieval transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4531" target="_blank">01:15:31.240</a></span> | <span class="t">or some other kind of mechanism. Great question. I think we are hoping to explore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4541" target="_blank">01:15:41.160</a></span> | <span class="t">that in the future, but with this, like, context length limitation, we are already near the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4545" target="_blank">01:15:45.280</a></span> | <span class="t">context length capacity with just these six images alone, much less, you know, passing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4550" target="_blank">01:15:50.140</a></span> | <span class="t">in whole trajectories of zero-shot behavior, few-shot behavior we wish to see. So, 2BD,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4557" target="_blank">01:15:57.800</a></span> | <span class="t">I think. Cool. Thank you, guys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4561" target="_blank">01:16:01.800</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4562" target="_blank">01:16:02.800</a></span> | <span class="t">[end of transcript]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ct4tdyyNDY4&t=4562" target="_blank">01:16:02.800</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>