<html><head><title>Building makemore Part 4: Becoming a Backprop Ninja</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Building makemore Part 4: Becoming a Backprop Ninja</h2><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI"><img src="https://i.ytimg.com/vi_webp/q8SA3rM6ckI/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=0">0:0</a> intro: why you should care & fun history<br><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=446">7:26</a> starter code<br><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=781">13:1</a> exercise 1: backproping the atomic compute graph<br><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3917">65:17</a> brief digression: besselâ€™s correction in batchnorm<br><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5191">86:31</a> exercise 2: cross entropy loss backward pass<br><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5797">96:37</a> exercise 3: batch norm layer backward pass<br><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6602">110:2</a> exercise 4: putting it all together<br><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6864">114:24</a> outro<br><br><div style="text-align: left;"><a href="./q8SA3rM6ckI.html">Whisper Transcript</a> | <a href="./transcript_q8SA3rM6ckI.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi everyone. So today we are once again continuing our implementation of Makemore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4" target="_blank">00:00:04.240</a></span> | <span class="t">Now so far we've come up to here, Montalio Perceptrons, and our neural net looked like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=10" target="_blank">00:00:10.720</a></span> | <span class="t">and we were implementing this over the last few lectures. Now I'm sure everyone is very excited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=14" target="_blank">00:00:14.880</a></span> | <span class="t">to go into recurrent neural networks and all of their variants and how they work, and the diagrams</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=19" target="_blank">00:00:19.360</a></span> | <span class="t">look cool and it's very exciting and interesting and we're going to get a better result, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=22" target="_blank">00:00:22.880</a></span> | <span class="t">unfortunately I think we have to remain here for one more lecture. And the reason for that is we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=28" target="_blank">00:00:28.880</a></span> | <span class="t">already trained this Montalio Perceptron, right, and we are getting pretty good loss, and I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=32" target="_blank">00:00:32.960</a></span> | <span class="t">we have a pretty decent understanding of the architecture and how it works, but the line of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=37" target="_blank">00:00:37.840</a></span> | <span class="t">code here that I take an issue with is here, lost at backward. That is, we are taking PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=43" target="_blank">00:00:43.760</a></span> | <span class="t">Autograd and using it to calculate all of our gradients along the way, and I would like to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=48" target="_blank">00:00:48.400</a></span> | <span class="t">remove the use of lost at backward, and I would like us to write our backward pass manually on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=52" target="_blank">00:00:52.800</a></span> | <span class="t">the level of tensors. And I think that this is a very useful exercise for the following reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=58" target="_blank">00:00:58.720</a></span> | <span class="t">I actually have an entire blog post on this topic, but I'd like to call backpropagation a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=63" target="_blank">00:01:03.520</a></span> | <span class="t">leaky abstraction. And what I mean by that is backpropagation doesn't just make your neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=69" target="_blank">00:01:09.120</a></span> | <span class="t">networks just work magically. It's not the case that you can just stack up arbitrary Lego blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=73" target="_blank">00:01:13.440</a></span> | <span class="t">of differentiable functions and just cross your fingers and backpropagate and everything is great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=77" target="_blank">00:01:17.680</a></span> | <span class="t">Things don't just work automatically. It is a leaky abstraction in the sense that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=82" target="_blank">00:01:22.880</a></span> | <span class="t">you can shoot yourself in the foot if you do not understand its internals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=86" target="_blank">00:01:26.640</a></span> | <span class="t">It will magically not work or not work optimally, and you will need to understand how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=92" target="_blank">00:01:32.240</a></span> | <span class="t">under the hood if you're hoping to debug it and if you are hoping to address it in your neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=96" target="_blank">00:01:36.480</a></span> | <span class="t">So this blog post here from a while ago goes into some of those examples. So for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=102" target="_blank">00:01:42.560</a></span> | <span class="t">we've already covered them, some of them already. For example, the flat tails of these functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=108" target="_blank">00:01:48.320</a></span> | <span class="t">and how you do not want to saturate them too much because your gradients will die.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=113" target="_blank">00:01:53.120</a></span> | <span class="t">The case of dead neurons, which I've already covered as well. The case of exploding or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=117" target="_blank">00:01:57.840</a></span> | <span class="t">vanishing gradients in the case of recurrent neural networks, which we are about to cover.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=121" target="_blank">00:02:01.680</a></span> | <span class="t">And then also, you will often come across some examples in the wild. This is a snippet that I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=128" target="_blank">00:02:08.480</a></span> | <span class="t">found in a random code base on the internet where they actually have a very subtle but pretty major</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=134" target="_blank">00:02:14.400</a></span> | <span class="t">bug in their implementation. And the bug points at the fact that the author of this code does not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=140" target="_blank">00:02:20.000</a></span> | <span class="t">actually understand backpropagation. So what they're trying to do here is they're trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=143" target="_blank">00:02:23.360</a></span> | <span class="t">clip the loss at a certain maximum value. But actually what they're trying to do is they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=148" target="_blank">00:02:28.000</a></span> | <span class="t">trying to clip the gradients to have a maximum value instead of trying to clip the loss at a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=152" target="_blank">00:02:32.000</a></span> | <span class="t">maximum value. And indirectly, they're basically causing some of the outliers to be actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=158" target="_blank">00:02:38.400</a></span> | <span class="t">ignored because when you clip a loss of an outlier, you are setting its gradient to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=164" target="_blank">00:02:44.080</a></span> | <span class="t">And so have a look through this and read through it. But there's basically a bunch of subtle issues</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=169" target="_blank">00:02:49.840</a></span> | <span class="t">that you're going to avoid if you actually know what you're doing. And that's why I don't think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=173" target="_blank">00:02:53.600</a></span> | <span class="t">it's the case that because PyTorch or other frameworks offer autograd, it is okay for us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=178" target="_blank">00:02:58.240</a></span> | <span class="t">to ignore how it works. Now, we've actually already covered autograd and we wrote micrograd.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=184" target="_blank">00:03:04.640</a></span> | <span class="t">But micrograd was an autograd engine only on the level of individual scalars. So the atoms were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=190" target="_blank">00:03:10.160</a></span> | <span class="t">single individual numbers. And I don't think it's enough. And I'd like us to basically think about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=195" target="_blank">00:03:15.120</a></span> | <span class="t">backpropagation on the level of tensors as well. And so in a summary, I think it's a good exercise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=200" target="_blank">00:03:20.240</a></span> | <span class="t">I think it is very, very valuable. You're going to become better at debugging neural networks and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=205" target="_blank">00:03:25.520</a></span> | <span class="t">making sure that you understand what you're doing. It is going to make everything fully explicit. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=209" target="_blank">00:03:29.920</a></span> | <span class="t">you're not going to be nervous about what is hidden away from you. And basically, in general,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=214" target="_blank">00:03:34.000</a></span> | <span class="t">we're going to emerge stronger. And so let's get into it. A bit of a fun historical note here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=219" target="_blank">00:03:39.760</a></span> | <span class="t">that today, writing your backward pass by hand and manually is not recommended and no one does it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=224" target="_blank">00:03:44.560</a></span> | <span class="t">except for the purposes of exercise. But about 10 years ago in deep learning, this was fairly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=229" target="_blank">00:03:49.360</a></span> | <span class="t">standard and in fact pervasive. So at the time, everyone used to write their own backward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=233" target="_blank">00:03:53.680</a></span> | <span class="t">by hand manually, including myself. And it's just what you would do. So we used to write backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=238" target="_blank">00:03:58.800</a></span> | <span class="t">pass by hand. And now everyone just called lost backward. We've lost something. I wanted to give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=244" target="_blank">00:04:04.400</a></span> | <span class="t">you a few examples of this. So here's a 2006 paper from Geoff Hinton and Roslyn Slavkinov</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=252" target="_blank">00:04:12.560</a></span> | <span class="t">in science that was influential at the time. And this was training some architectures called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=257" target="_blank">00:04:17.840</a></span> | <span class="t">restricted Boltzmann machines. And basically, it's an autoencoder trained here. And this is from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=264" target="_blank">00:04:24.160</a></span> | <span class="t">roughly 2010. I had a library for training restricted Boltzmann machines. And this was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=269" target="_blank">00:04:29.680</a></span> | <span class="t">at the time written in MATLAB. So Python was not used for deep learning pervasively. It was all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=274" target="_blank">00:04:34.080</a></span> | <span class="t">MATLAB. And MATLAB was this scientific computing package that everyone would use. So we would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=280" target="_blank">00:04:40.240</a></span> | <span class="t">write MATLAB, which is barely a programming language as well. But it had a very convenient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=285" target="_blank">00:04:45.680</a></span> | <span class="t">tensor class. And it was this computing environment. And you would run here. It would all run on a CPU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=290" target="_blank">00:04:50.480</a></span> | <span class="t">of course. But you would have very nice plots to go with it and a built-in debugger. And it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=294" target="_blank">00:04:54.560</a></span> | <span class="t">was pretty nice. Now, the code in this package in 2010 that I wrote for fitting restricted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=300" target="_blank">00:05:00.800</a></span> | <span class="t">Boltzmann machines, to a large extent, is recognizable. But I wanted to show you how you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=305" target="_blank">00:05:05.440</a></span> | <span class="t">would-- well, I'm creating the data and the xy batches. I'm initializing the neural net. So it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=311" target="_blank">00:05:11.520</a></span> | <span class="t">got weights and biases, just like we're used to. And then this is the training loop, where we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=315" target="_blank">00:05:15.920</a></span> | <span class="t">actually do the forward pass. And then here, at this time, they didn't even necessarily use back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=320" target="_blank">00:05:20.640</a></span> | <span class="t">propagation to train neural networks. So this, in particular, implements contrastive divergence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=325" target="_blank">00:05:25.920</a></span> | <span class="t">which estimates a gradient. And then here, we take that gradient and use it for a parameter update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=332" target="_blank">00:05:32.000</a></span> | <span class="t">along the lines that we're used to. Yeah, here. But you can see that, basically, people were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=337" target="_blank">00:05:37.920</a></span> | <span class="t">meddling with these gradients directly and inline and themselves. It wasn't that common to use an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=342" target="_blank">00:05:42.720</a></span> | <span class="t">autograd engine. Here's one more example from a paper of mine from 2014 called Deep Fragment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=348" target="_blank">00:05:48.720</a></span> | <span class="t">Embeddings. And here, what I was doing is I was aligning images and text. And so it's kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=354" target="_blank">00:05:54.160</a></span> | <span class="t">like a clip, if you're familiar with it. But instead of working on the level of entire images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=358" target="_blank">00:05:58.240</a></span> | <span class="t">and entire sentences, it was working on the level of individual objects and little pieces of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=362" target="_blank">00:06:02.240</a></span> | <span class="t">sentences. And I was embedding them and then calculating a very much like a clip-like loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=366" target="_blank">00:06:06.480</a></span> | <span class="t">And I dug up the code from 2014 of how I implemented this. And it was already in NumPy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=372" target="_blank">00:06:12.880</a></span> | <span class="t">and Python. And here, I'm implementing the cost function. And it was standard to implement not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=378" target="_blank">00:06:18.880</a></span> | <span class="t">just the cost, but also the backward pass manually. So here, I'm calculating the image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=383" target="_blank">00:06:23.680</a></span> | <span class="t">embeddings, sentence embeddings, the loss function. I calculate the scores. This is the loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=389" target="_blank">00:06:29.920</a></span> | <span class="t">And then once I have the loss function, I do the backward pass right here. So I backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=394" target="_blank">00:06:34.640</a></span> | <span class="t">through the loss function and through the neural net. And I append regularization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=398" target="_blank">00:06:38.720</a></span> | <span class="t">So everything was done by hand manually. And you would just write out the backward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=403" target="_blank">00:06:43.680</a></span> | <span class="t">And then you would use a gradient checker to make sure that your numerical estimate of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=407" target="_blank">00:06:47.840</a></span> | <span class="t">agrees with the one you calculated during the backpropagation. So this was very standard for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=412" target="_blank">00:06:52.080</a></span> | <span class="t">a long time. But today, of course, it is standard to use an autograd engine. But it was definitely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=417" target="_blank">00:06:57.680</a></span> | <span class="t">useful. And I think people sort of understood how these neural networks work on a very intuitive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=421" target="_blank">00:07:01.280</a></span> | <span class="t">level. And so I think it's a good exercise again. And this is where we want to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=425" target="_blank">00:07:05.280</a></span> | <span class="t">OK, so just as a reminder from our previous lecture, this is the Jupyter notebook that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=428" target="_blank">00:07:08.800</a></span> | <span class="t">we implemented at the time. And we're going to keep everything the same. So we're still going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=434" target="_blank">00:07:14.080</a></span> | <span class="t">to have a two-layer multilayer perceptron with a batch normalization layer. So the forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=438" target="_blank">00:07:18.480</a></span> | <span class="t">will be basically identical to this lecture. But here, we're going to get rid of loss.backward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=442" target="_blank">00:07:22.800</a></span> | <span class="t">And instead, we're going to write the backward pass manually. Now, here's the starter code for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=447" target="_blank">00:07:27.120</a></span> | <span class="t">this lecture. We are becoming a backprop ninja in this notebook. And the first few cells here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=453" target="_blank">00:07:33.600</a></span> | <span class="t">are identical to what we are used to. So we are doing some imports, loading in the data set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=458" target="_blank">00:07:38.160</a></span> | <span class="t">and processing the data set. None of this changed. Now, here I'm introducing a utility function that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=463" target="_blank">00:07:43.680</a></span> | <span class="t">we're going to use later to compare the gradients. So in particular, we are going to have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=467" target="_blank">00:07:47.360</a></span> | <span class="t">gradients that we estimate manually ourselves. And we're going to have gradients that PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=471" target="_blank">00:07:51.920</a></span> | <span class="t">calculates. And we're going to be checking for correctness, assuming, of course, that PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=476" target="_blank">00:07:56.240</a></span> | <span class="t">is correct. Then here, we have the initialization that we are quite used to. So we have our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=482" target="_blank">00:08:02.400</a></span> | <span class="t">embedding table for the characters, the first layer, second layer, and a batch normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=487" target="_blank">00:08:07.200</a></span> | <span class="t">in between. And here's where we create all the parameters. Now, you will note that I changed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=491" target="_blank">00:08:11.920</a></span> | <span class="t">the initialization a little bit to be small numbers. So normally, you would set the biases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=496" target="_blank">00:08:16.800</a></span> | <span class="t">to be all zero. Here, I am setting them to be small random numbers. And I'm doing this because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=501" target="_blank">00:08:21.760</a></span> | <span class="t">if your variables are initialized to exactly zero, sometimes what can happen is that can mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=507" target="_blank">00:08:27.120</a></span> | <span class="t">an incorrect implementation of a gradient. Because when everything is zero, it sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=512" target="_blank">00:08:32.560</a></span> | <span class="t">simplifies and gives you a much simpler expression of the gradient than you would otherwise get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=516" target="_blank">00:08:36.720</a></span> | <span class="t">And so by making it small numbers, I'm trying to unmask those potential errors in these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=521" target="_blank">00:08:41.520</a></span> | <span class="t">calculations. You also notice that I'm using b1 in the first layer. I'm using a bias, despite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=528" target="_blank">00:08:48.400</a></span> | <span class="t">batch normalization right afterwards. So this would typically not be what you do, because we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=533" target="_blank">00:08:53.280</a></span> | <span class="t">talked about the fact that you don't need a bias. But I'm doing this here just for fun, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=538" target="_blank">00:08:58.000</a></span> | <span class="t">we're going to have a gradient with respect to it. And we can check that we are still calculating it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=541" target="_blank">00:09:01.520</a></span> | <span class="t">correctly, even though this bias is spurious. So here, I'm calculating a single batch. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=547" target="_blank">00:09:07.680</a></span> | <span class="t">here, I am doing a forward pass. Now, you'll notice that the forward pass is significantly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=552" target="_blank">00:09:12.160</a></span> | <span class="t">expanded from what we are used to. Here, the forward pass was just here. Now, the reason that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=558" target="_blank">00:09:18.560</a></span> | <span class="t">the forward pass is longer is for two reasons. Number one, here, we just had an f dot cross</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=563" target="_blank">00:09:23.280</a></span> | <span class="t">entropy. But here, I am bringing back a explicit implementation of the loss function. And number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=568" target="_blank">00:09:28.720</a></span> | <span class="t">two, I've broken up the implementation into manageable chunks. So we have a lot more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=575" target="_blank">00:09:35.280</a></span> | <span class="t">intermediate tensors along the way in the forward pass. And that's because we are about to go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=579" target="_blank">00:09:39.440</a></span> | <span class="t">backwards and calculate the gradients in this back propagation from the bottom to the top.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=584" target="_blank">00:09:44.880</a></span> | <span class="t">So we're going to go upwards. And just like we have, for example, the log props tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=590" target="_blank">00:09:50.080</a></span> | <span class="t">in a forward pass, in a backward pass, we're going to have a d log props, which is going to store the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=594" target="_blank">00:09:54.560</a></span> | <span class="t">derivative of the loss with respect to the log props tensor. And so we're going to be pre-pending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=599" target="_blank">00:09:59.040</a></span> | <span class="t">d to every one of these tensors and calculating it along the way of this back propagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=604" target="_blank">00:10:04.960</a></span> | <span class="t">So as an example, we have a b in raw here. We're going to be calculating a d b in raw.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=609" target="_blank">00:10:09.360</a></span> | <span class="t">So here, I'm telling PyTorch that we want to retain the grad of all these intermediate values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=615" target="_blank">00:10:15.360</a></span> | <span class="t">because here in exercise one, we're going to calculate the backward pass. So we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=619" target="_blank">00:10:19.920</a></span> | <span class="t">to calculate all these d variables and use the CMP function I've introduced above to check our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=625" target="_blank">00:10:25.920</a></span> | <span class="t">correctness with respect to what PyTorch is telling us. This is going to be exercise one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=631" target="_blank">00:10:31.040</a></span> | <span class="t">where we sort of back propagate through this entire graph. Now, just to give you a very quick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=636" target="_blank">00:10:36.080</a></span> | <span class="t">preview of what's going to happen in exercise two and below, here we have fully broken up the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=642" target="_blank">00:10:42.240</a></span> | <span class="t">and back propagated through it manually in all the little atomic pieces that make it up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=647" target="_blank">00:10:47.040</a></span> | <span class="t">But here we're going to collapse the loss into a single cross entropy call.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=650" target="_blank">00:10:50.720</a></span> | <span class="t">And instead, we're going to analytically derive using math and paper and pencil, the gradient of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=658" target="_blank">00:10:58.480</a></span> | <span class="t">the loss with respect to the logits. And instead of back propagating through all of its little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=662" target="_blank">00:11:02.320</a></span> | <span class="t">chunks one at a time, we're just going to analytically derive what that gradient is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=666" target="_blank">00:11:06.160</a></span> | <span class="t">and we're going to implement that, which is much more efficient, as we'll see in a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=669" target="_blank">00:11:09.840</a></span> | <span class="t">Then we're going to do the exact same thing for batch normalization. So instead of breaking up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=674" target="_blank">00:11:14.880</a></span> | <span class="t">batch normalization into all the little tiny components, we're going to use pen and paper and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=679" target="_blank">00:11:19.760</a></span> | <span class="t">mathematics and calculus to derive the gradient through the batch normal layer. So we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=685" target="_blank">00:11:25.920</a></span> | <span class="t">calculate the backward pass through batch normal layer in a much more efficient expression,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=689" target="_blank">00:11:29.680</a></span> | <span class="t">instead of backward propagating through all of its little pieces independently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=692" target="_blank">00:11:32.720</a></span> | <span class="t">So that's going to be exercise three. And then in exercise four, we're going to put it all together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=698" target="_blank">00:11:38.800</a></span> | <span class="t">And this is the full code of training this two-layer MLP. And we're going to basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=703" target="_blank">00:11:43.680</a></span> | <span class="t">insert our manual backprop, and we're going to take out lost at backward. And you will basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=708" target="_blank">00:11:48.720</a></span> | <span class="t">see that you can get all the same results using fully your own code. And the only thing we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=715" target="_blank">00:11:55.440</a></span> | <span class="t">using from PyTorch is the torch.tensor to make the calculations efficient. But otherwise, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=721" target="_blank">00:12:01.360</a></span> | <span class="t">will understand fully what it means to forward and backward the neural net and train it. And I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=725" target="_blank">00:12:05.680</a></span> | <span class="t">that'll be awesome. So let's get to it. Okay, so I ran all the cells of this notebook all the way up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=731" target="_blank">00:12:11.120</a></span> | <span class="t">to here. And I'm going to erase this. And I'm going to start implementing backward pass, starting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=736" target="_blank">00:12:16.240</a></span> | <span class="t">with dlogprops. So we want to understand what should go here to calculate the gradient of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=741" target="_blank">00:12:21.360</a></span> | <span class="t">loss with respect to all the elements of the logprops tensor. Now, I'm going to give away the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=746" target="_blank">00:12:26.560</a></span> | <span class="t">answer here. But I wanted to put a quick note here that I think will be most pedagogically useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=751" target="_blank">00:12:31.120</a></span> | <span class="t">for you is to actually go into the description of this video and find the link to this Jupyter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=756" target="_blank">00:12:36.400</a></span> | <span class="t">notebook. You can find it both on GitHub, but you can also find Google Colab with it. So you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=760" target="_blank">00:12:40.480</a></span> | <span class="t">have to install anything, you'll just go to a website on Google Colab. And you can try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=764" target="_blank">00:12:44.640</a></span> | <span class="t">implement these derivatives or gradients yourself. And then if you are not able to come to my video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=770" target="_blank">00:12:50.800</a></span> | <span class="t">and see me do it, and so work in tandem and try it first yourself and then see me give away the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=776" target="_blank">00:12:56.640</a></span> | <span class="t">answer. And I think that'll be most valuable to you. And that's how I recommend you go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=780" target="_blank">00:13:00.080</a></span> | <span class="t">this lecture. So we are starting here with dlogprops. Now, dlogprops will hold the derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=787" target="_blank">00:13:07.120</a></span> | <span class="t">of the loss with respect to all the elements of logprops. What is inside logprops? The shape of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=793" target="_blank">00:13:13.360</a></span> | <span class="t">this is 32 by 27. So it's not going to surprise you that dlogprops should also be an array of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=800" target="_blank">00:13:20.400</a></span> | <span class="t">size 32 by 27, because we want the derivative of the loss with respect to all of its elements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=805" target="_blank">00:13:25.200</a></span> | <span class="t">So the sizes of those are always going to be equal. Now, how does logprops influence the loss?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=813" target="_blank">00:13:33.040</a></span> | <span class="t">Loss is negative logprops indexed with range of n and yb and then the mean of that. Now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=821" target="_blank">00:13:41.920</a></span> | <span class="t">just as a reminder, yb is just basically an array of all the correct indices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=829" target="_blank">00:13:49.760</a></span> | <span class="t">So what we're doing here is we're taking the logprops array of size 32 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=835" target="_blank">00:13:55.120</a></span> | <span class="t">And then we are going in every single row. And in each row, we are plucking out the index 8 and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=845" target="_blank">00:14:05.680</a></span> | <span class="t">then 14 and 15 and so on. So we're going down the rows. That's the iterator range of n. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=850" target="_blank">00:14:10.720</a></span> | <span class="t">then we are always plucking out the index at the column specified by this tensor yb. So in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=856" target="_blank">00:14:16.560</a></span> | <span class="t">zeroth row, we are taking the eighth column. In the first row, we're taking the 14th column, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=862" target="_blank">00:14:22.400</a></span> | <span class="t">And so logprops at this plucks out all those log probabilities of the correct next character in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=871" target="_blank">00:14:31.200</a></span> | <span class="t">sequence. So that's what that does. And the shape of this, or the size of it, is of course 32,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=877" target="_blank">00:14:37.040</a></span> | <span class="t">because our batch size is 32. So these elements get plucked out, and then their mean and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=884" target="_blank">00:14:44.720</a></span> | <span class="t">negative of that becomes loss. So I always like to work with simpler examples to understand the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=891" target="_blank">00:14:51.680</a></span> | <span class="t">numerical form of the derivative. What's going on here is once we've plucked out these examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=897" target="_blank">00:14:57.040</a></span> | <span class="t">we're taking the mean and then the negative. So the loss basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=903" target="_blank">00:15:03.120</a></span> | <span class="t">I can write it this way, is the negative of say a plus b plus c, and the mean of those three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=908" target="_blank">00:15:08.960</a></span> | <span class="t">numbers would be say negative, would divide three. That would be how we achieve the mean of three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=913" target="_blank">00:15:13.360</a></span> | <span class="t">numbers a, b, c, although we actually have 32 numbers here. And so what is basically the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=919" target="_blank">00:15:19.440</a></span> | <span class="t">by say like da, right? Well, if we simplify this expression mathematically, this is negative 1 over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=926" target="_blank">00:15:26.080</a></span> | <span class="t">3 of a plus negative 1 over 3 of b plus negative 1 over 3 of c. And so what is the loss by da?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=934" target="_blank">00:15:34.720</a></span> | <span class="t">It's just negative 1 over 3. And so you can see that if we don't just have a, b, and c,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=939" target="_blank">00:15:39.280</a></span> | <span class="t">but we have 32 numbers, then d loss by d, you know, every one of those numbers is going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=945" target="_blank">00:15:45.600</a></span> | <span class="t">1 over n more generally, because n is the size of the batch, 32 in this case. So d loss by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=955" target="_blank">00:15:55.040</a></span> | <span class="t">d logprobs is negative 1 over n in all these places. Now, what about the other elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=963" target="_blank">00:16:03.680</a></span> | <span class="t">inside logprobs? Because logprobs is a large array. You see that logprobs.sh is 32 by 27,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=969" target="_blank">00:16:09.360</a></span> | <span class="t">but only 32 of them participate in the loss calculation. So what's the derivative of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=975" target="_blank">00:16:15.600</a></span> | <span class="t">the other, most of the elements that do not get plucked out here? Well, their loss intuitively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=981" target="_blank">00:16:21.440</a></span> | <span class="t">is zero. Sorry, their gradient intuitively is zero. And that's because they do not participate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=986" target="_blank">00:16:26.160</a></span> | <span class="t">in the loss. So most of these numbers inside this tensor does not feed into the loss. And so if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=992" target="_blank">00:16:32.320</a></span> | <span class="t">were to change these numbers, then the loss doesn't change, which is the equivalent of us saying that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=998" target="_blank">00:16:38.320</a></span> | <span class="t">the derivative of the loss with respect to them is zero. They don't impact it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1001" target="_blank">00:16:41.600</a></span> | <span class="t">So here's a way to implement this derivative then. We start out with torsdat zeros of shape 32 by 27,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1010" target="_blank">00:16:50.160</a></span> | <span class="t">or let's just say, instead of doing this, because we don't want to hard code numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1014" target="_blank">00:16:54.000</a></span> | <span class="t">let's do torsdat zeros like logprobs. So basically, this is going to create an array of zeros exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1020" target="_blank">00:17:00.320</a></span> | <span class="t">in the shape of logprobs. And then we need to set the derivative of negative 1 over n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1026" target="_blank">00:17:06.160</a></span> | <span class="t">inside exactly these locations. So here's what we can do. The logprobs indexed in the identical way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1034" target="_blank">00:17:14.240</a></span> | <span class="t">will be just set to negative 1 over 0, divide n. Right, just like we derived here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1041" target="_blank">00:17:21.280</a></span> | <span class="t">So now let me erase all of this reasoning. And then this is the candidate derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1048" target="_blank">00:17:28.240</a></span> | <span class="t">for dlogprobs. Let's uncomment the first line and check that this is correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1052" target="_blank">00:17:32.240</a></span> | <span class="t">Okay, so CMP ran. And let's go back to CMP. And you see that what it's doing is it's calculating if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1062" target="_blank">00:17:42.880</a></span> | <span class="t">the calculated value by us, which is dt, is exactly equal to t.grad as calculated by PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1068" target="_blank">00:17:48.800</a></span> | <span class="t">And then this is making sure that all of the elements are exactly equal, and then converting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1074" target="_blank">00:17:54.400</a></span> | <span class="t">this to a single Boolean value, because we don't want a Boolean tensor, we just want a Boolean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1078" target="_blank">00:17:58.720</a></span> | <span class="t">value. And then here, we are making sure that, okay, if they're not exactly equal, maybe they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1084" target="_blank">00:18:04.320</a></span> | <span class="t">are approximately equal because of some floating point issues, but they're very, very close.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1088" target="_blank">00:18:08.960</a></span> | <span class="t">So here we are using torch.all_close, which has a little bit of a wiggle available, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1094" target="_blank">00:18:14.160</a></span> | <span class="t">sometimes you can get very, very close. But if you use a slightly different calculation, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1099" target="_blank">00:18:19.040</a></span> | <span class="t">of floating point arithmetic, you can get a slightly different result. So this is checking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1104" target="_blank">00:18:24.480</a></span> | <span class="t">if you get an approximately close result. And then here, we are checking the maximum,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1108" target="_blank">00:18:28.880</a></span> | <span class="t">basically the value that has the highest difference, and what is the difference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1114" target="_blank">00:18:34.480</a></span> | <span class="t">and the absolute value difference between those two. And so we are printing whether we have an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1118" target="_blank">00:18:38.800</a></span> | <span class="t">exact equality, an approximate equality, and what is the largest difference. And so here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1125" target="_blank">00:18:45.600</a></span> | <span class="t">we see that we actually have exact equality. And so therefore, of course, we also have an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1130" target="_blank">00:18:50.480</a></span> | <span class="t">approximate equality, and the maximum difference is exactly zero. So basically, our DLOG_PROPS</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1136" target="_blank">00:18:56.880</a></span> | <span class="t">is exactly equal to what PyTorch calculated to be log_props.grad in its backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1142" target="_blank">00:19:02.960</a></span> | <span class="t">So, so far, we're doing pretty well. Okay, so let's now continue our backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1148" target="_blank">00:19:08.640</a></span> | <span class="t">We have that log_props depends on probs through a log. So all the elements of probs are being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1154" target="_blank">00:19:14.080</a></span> | <span class="t">element-wise applied log_to. Now, if we want DPROPS, then, then remember your micrograd training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1161" target="_blank">00:19:21.280</a></span> | <span class="t">We have like a log node, it takes in probs and creates log_props. And DPROPS will be the local</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1168" target="_blank">00:19:28.880</a></span> | <span class="t">derivative of that individual operation, log, times the derivative of the loss with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1174" target="_blank">00:19:34.240</a></span> | <span class="t">to its output, which in this case is DLOG_PROPS. So what is the local derivative of this operation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1180" target="_blank">00:19:40.160</a></span> | <span class="t">Well, we are taking log element-wise, and we can come here and we can see, well, from alpha</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1184" target="_blank">00:19:44.400</a></span> | <span class="t">is your friend, that d by dx of log of x is just simply one over x. So therefore, in this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1190" target="_blank">00:19:50.640</a></span> | <span class="t">x is probs. So we have d by dx is one over x, which is one over probs, and then this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1196" target="_blank">00:19:56.720</a></span> | <span class="t">local derivative, and then times we want to chain it. So this is chain rule, times DLOG_PROPS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1203" target="_blank">00:20:03.440</a></span> | <span class="t">Then let me uncomment this and let me run the cell in place. And we see that the derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1208" target="_blank">00:20:08.880</a></span> | <span class="t">of probs as we calculated here is exactly correct. And so notice here how this works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1214" target="_blank">00:20:14.160</a></span> | <span class="t">Probs that are, probs is going to be inverted and then element-wise multiplied here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1219" target="_blank">00:20:19.840</a></span> | <span class="t">So if your probs is very, very close to one, that means you are, your network is currently predicting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1225" target="_blank">00:20:25.520</a></span> | <span class="t">the character correctly, then this will become one over one, and DLOG_PROPS just gets passed through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1231" target="_blank">00:20:31.520</a></span> | <span class="t">But if your probabilities are incorrectly assigned, so if the correct character here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1236" target="_blank">00:20:36.320</a></span> | <span class="t">is getting a very low probability, then 1.0 dividing by it will boost this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1242" target="_blank">00:20:42.240</a></span> | <span class="t">and then multiply by DLOG_PROPS. So basically what this line is doing intuitively is it's taking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1248" target="_blank">00:20:48.320</a></span> | <span class="t">the examples that have a very low probability currently assigned, and it's boosting their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1252" target="_blank">00:20:52.720</a></span> | <span class="t">gradient. You can look at it that way. Next up is COUNTSUM_INV. So we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1260" target="_blank">00:21:00.640</a></span> | <span class="t">the derivative of this. Now let me just pause here and kind of introduce what's happening here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1265" target="_blank">00:21:05.920</a></span> | <span class="t">in general, because I know it's a little bit confusing. We have the logits that come out of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1269" target="_blank">00:21:09.440</a></span> | <span class="t">the neural net. Here what I'm doing is I'm finding the maximum in each row, and I'm subtracting it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1275" target="_blank">00:21:15.440</a></span> | <span class="t">for the purpose of numerical stability. And we talked about how if you do not do this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1279" target="_blank">00:21:19.840</a></span> | <span class="t">you run into numerical issues if some of the logits take on too large values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1283" target="_blank">00:21:23.680</a></span> | <span class="t">because we end up exponentiating them. So this is done just for safety, numerically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1289" target="_blank">00:21:29.600</a></span> | <span class="t">Then here's the exponentiation of all the logits to create our counts. And then we want to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1296" target="_blank">00:21:36.640</a></span> | <span class="t">the sum of these counts and normalize so that all of the probs sum to 1. Now here, instead of using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1302" target="_blank">00:21:42.640</a></span> | <span class="t">1 over COUNTSUM, I use raised to the power of negative 1. Mathematically, they are identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1308" target="_blank">00:21:48.080</a></span> | <span class="t">I just found that there's something wrong with the PyTorch implementation of the backward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1311" target="_blank">00:21:51.520</a></span> | <span class="t">of division, and it gives a weird result. But that doesn't happen for **-1, so I'm using this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1319" target="_blank">00:21:59.120</a></span> | <span class="t">formula instead. But basically, all that's happening here is we got the logits, we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1324" target="_blank">00:22:04.080</a></span> | <span class="t">to exponentiate all of them, and we want to normalize the counts to create our probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1328" target="_blank">00:22:08.880</a></span> | <span class="t">It's just that it's happening across multiple lines. So now, here, we want to first take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1339" target="_blank">00:22:19.360</a></span> | <span class="t">derivative, we want to backpropagate into COUNTSUM_INF and then into COUNTS as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1344" target="_blank">00:22:24.320</a></span> | <span class="t">So what should be the COUNTSUM_INF? Now, we actually have to be careful here, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1349" target="_blank">00:22:29.840</a></span> | <span class="t">we have to scrutinize and be careful with the shapes. So COUNTS.shape and then COUNTSUM_INF.shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1357" target="_blank">00:22:37.040</a></span> | <span class="t">are different. So in particular, COUNTS is 32 by 27, but this COUNTSUM_INF is 32 by 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1365" target="_blank">00:22:45.360</a></span> | <span class="t">And so in this multiplication here, we also have an implicit broadcasting that PyTorch will do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1372" target="_blank">00:22:52.240</a></span> | <span class="t">because it needs to take this column tensor of 32 numbers and replicate it horizontally 27 times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1377" target="_blank">00:22:57.280</a></span> | <span class="t">to align these two tensors so it can do an element-wise multiply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1380" target="_blank">00:23:00.720</a></span> | <span class="t">So really what this looks like is the following, using a toy example again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1385" target="_blank">00:23:05.040</a></span> | <span class="t">What we really have here is just props is COUNTS times COUNTSUM_INF, so it's C equals A times B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1390" target="_blank">00:23:10.960</a></span> | <span class="t">but A is 3 by 3 and B is just 3 by 1, a column tensor. And so PyTorch internally replicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1398" target="_blank">00:23:18.320</a></span> | <span class="t">this elements of B, and it did that across all the columns. So for example, B1, which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1404" target="_blank">00:23:24.160</a></span> | <span class="t">first element of B, would be replicated here across all the columns in this multiplication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1408" target="_blank">00:23:28.240</a></span> | <span class="t">And now we're trying to backpropagate through this operation to COUNTSUM_INF.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1413" target="_blank">00:23:33.040</a></span> | <span class="t">So when we are calculating this derivative, it's important to realize that this looks like a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1420" target="_blank">00:23:40.880</a></span> | <span class="t">operation, but actually is two operations applied sequentially. The first operation that PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1426" target="_blank">00:23:46.480</a></span> | <span class="t">did is it took this column tensor and replicated it across all the columns, basically 27 times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1434" target="_blank">00:23:54.560</a></span> | <span class="t">So that's the first operation, it's a replication. And then the second operation is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1438" target="_blank">00:23:58.160</a></span> | <span class="t">multiplication. So let's first backpropagate through the multiplication. If these two arrays</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1444" target="_blank">00:24:04.480</a></span> | <span class="t">were of the same size and we just have A and B, both of them 3 by 3, then how do we backpropagate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1451" target="_blank">00:24:11.600</a></span> | <span class="t">through a multiplication? So if we just have scalars and not tensors, then if you have C</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1456" target="_blank">00:24:16.080</a></span> | <span class="t">equals A times B, then what is the derivative of C with respect to B? Well, it's just A.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1462" target="_blank">00:24:22.000</a></span> | <span class="t">So that's the local derivative. So here in our case, undoing the multiplication and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1468" target="_blank">00:24:28.240</a></span> | <span class="t">backpropagating through just the multiplication itself, which is element-wise, is going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1472" target="_blank">00:24:32.640</a></span> | <span class="t">the local derivative, which in this case is simply COUNTS, because COUNTS is the A.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1480" target="_blank">00:24:40.160</a></span> | <span class="t">So it's the local derivative, and then TIMES, because the chain rule, DPROPS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1484" target="_blank">00:24:44.240</a></span> | <span class="t">So this here is the derivative, or the gradient, but with respect to replicated B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1490" target="_blank">00:24:50.800</a></span> | <span class="t">But we don't have a replicated B, we just have a single B column. So how do we now backpropagate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1497" target="_blank">00:24:57.120</a></span> | <span class="t">through the replication? And intuitively, this B1 is the same variable, and it's just reused</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1503" target="_blank">00:25:03.440</a></span> | <span class="t">multiple times. And so you can look at it as being equivalent to a case we've encountered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1509" target="_blank">00:25:09.440</a></span> | <span class="t">in micrograd. And so here, I'm just pulling out a random graph we used in micrograd.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1514" target="_blank">00:25:14.320</a></span> | <span class="t">We had an example where a single node has its output feeding into two branches of basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1521" target="_blank">00:25:21.440</a></span> | <span class="t">the graph until the loss function. And we're talking about how the correct thing to do in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1525" target="_blank">00:25:25.520</a></span> | <span class="t">the backward pass is we need to sum all the gradients that arrive at any one node. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1530" target="_blank">00:25:30.960</a></span> | <span class="t">across these different branches, the gradients would sum. So if a node is used multiple times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1536" target="_blank">00:25:36.880</a></span> | <span class="t">the gradients for all of its uses sum during backpropagation. So here, B1 is used multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1543" target="_blank">00:25:43.120</a></span> | <span class="t">times in all these columns, and therefore the right thing to do here is to sum horizontally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1549" target="_blank">00:25:49.280</a></span> | <span class="t">across all the rows. So we want to sum in dimension 1, but we want to retain this dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1557" target="_blank">00:25:57.200</a></span> | <span class="t">so that countSumInv and its gradient are going to be exactly the same shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1561" target="_blank">00:26:01.600</a></span> | <span class="t">So we want to make sure that we keep them as true so we don't lose this dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1565" target="_blank">00:26:05.840</a></span> | <span class="t">And this will make the countSumInv be exactly shape 32 by 1. So revealing this comparison as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1574" target="_blank">00:26:14.080</a></span> | <span class="t">well and running this, we see that we get an exact match. So this derivative is exactly correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1582" target="_blank">00:26:22.080</a></span> | <span class="t">And let me erase this. Now let's also backpropagate into counts, which is the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1588" target="_blank">00:26:28.720</a></span> | <span class="t">variable here to create props. So from props to countSumInv, we just did that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1593" target="_blank">00:26:33.200</a></span> | <span class="t">Let's go into counts as well. So dcounts will be...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1597" target="_blank">00:26:37.120</a></span> | <span class="t">dcounts is our A, so dc by dA is just B. So therefore it's countSumInv.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1607" target="_blank">00:26:47.520</a></span> | <span class="t">And then times chain rule dprops. Now countSumInv is 32 by 1, dprops is 32 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1616" target="_blank">00:26:56.720</a></span> | <span class="t">So those will broadcast fine and will give us dcounts. There's no additional summation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1624" target="_blank">00:27:04.960</a></span> | <span class="t">required here. There will be a broadcasting that happens in this multiply here because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1631" target="_blank">00:27:11.040</a></span> | <span class="t">countSumInv needs to be replicated again to correctly multiply dprops. But that's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1636" target="_blank">00:27:16.560</a></span> | <span class="t">to give the correct result as far as this single operation is concerned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1640" target="_blank">00:27:20.960</a></span> | <span class="t">So we've backpropagated from props to counts, but we can't actually check the derivative of counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1647" target="_blank">00:27:27.920</a></span> | <span class="t">I have it much later on. And the reason for that is because countSumInv depends on counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1654" target="_blank">00:27:34.560</a></span> | <span class="t">And so there's a second branch here that we have to finish because countSumInv backpropagates into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1659" target="_blank">00:27:39.040</a></span> | <span class="t">countSum and countSum will backpropagate into counts. And so counts is a node that is being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1664" target="_blank">00:27:44.320</a></span> | <span class="t">used twice. It's used right here in two props and it goes through this other branch through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1668" target="_blank">00:27:48.800</a></span> | <span class="t">countSumInv. So even though we've calculated the first contribution of it, we still have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1673" target="_blank">00:27:53.840</a></span> | <span class="t">to calculate the second contribution of it later. Okay, so we're continuing with this branch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1679" target="_blank">00:27:59.120</a></span> | <span class="t">We have the derivative for countSumInv. Now we want the derivative of countSum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1682" target="_blank">00:28:02.640</a></span> | <span class="t">So dcountSum equals, what is the local derivative of this operation? So this is basically an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1688" target="_blank">00:28:08.560</a></span> | <span class="t">element-wise one over countsSum. So countSum raised to the power of negative one is the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1694" target="_blank">00:28:14.480</a></span> | <span class="t">as one over countsSum. If we go to WolframAlpha, we see that x to the negative one, d by dx of it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1701" target="_blank">00:28:21.360</a></span> | <span class="t">is basically negative x to the negative two. Negative one over x squared is the same as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1706" target="_blank">00:28:26.720</a></span> | <span class="t">negative x to the negative two. So dcountSum here will be, local derivative is going to be negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1716" target="_blank">00:28:36.240</a></span> | <span class="t">countsSum to the negative two, that's the local derivative, times chain rule, which is dcountSumInv.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1724" target="_blank">00:28:44.320</a></span> | <span class="t">So that's dcountSum. Let's uncomment this and check that I am correct. Okay, so we have perfect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1733" target="_blank">00:28:53.200</a></span> | <span class="t">equality. And there's no sketchiness going on here with any shapes because these are of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1739" target="_blank">00:28:59.440</a></span> | <span class="t">same shape. Okay, next up we want to back propagate through this line. We have that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1743" target="_blank">00:29:03.920</a></span> | <span class="t">countsSum is counts.sum along the rows. So I wrote out some help here. We have to keep in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1751" target="_blank">00:29:11.440</a></span> | <span class="t">mind that counts, of course, is 32 by 27, and countsSum is 32 by one. So in this back propagation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1757" target="_blank">00:29:17.680</a></span> | <span class="t">we need to take this column of derivatives and transform it into an array of derivatives,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1764" target="_blank">00:29:24.640</a></span> | <span class="t">two-dimensional array. So what is this operation doing? We're taking some kind of an input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1770" target="_blank">00:29:30.400</a></span> | <span class="t">like say a three-by-three matrix A, and we are summing up the rows into a column tensor B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1776" target="_blank">00:29:36.240</a></span> | <span class="t">B1, B2, B3, that is basically this. So now we have the derivatives of the loss with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1782" target="_blank">00:29:42.720</a></span> | <span class="t">B, all the elements of B. And now we want to derive the loss with respect to all these little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1788" target="_blank">00:29:48.400</a></span> | <span class="t">As. So how do the Bs depend on the As is basically what we're after. What is the local derivative of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1794" target="_blank">00:29:54.800</a></span> | <span class="t">this operation? Well, we can see here that B1 only depends on these elements here. The derivative of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1801" target="_blank">00:30:01.840</a></span> | <span class="t">B1 with respect to all of these elements down here is zero. But for these elements here, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1807" target="_blank">00:30:07.040</a></span> | <span class="t">A11, A12, etc., the local derivative is one, right? So DB1 by DA11, for example, is one. So it's one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1816" target="_blank">00:30:16.000</a></span> | <span class="t">one, and one. So when we have the derivative of loss with respect to B1, the local derivative of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1822" target="_blank">00:30:22.720</a></span> | <span class="t">B1 with respect to these inputs is zeroes here, but it's one on these guys. So in the chain rule,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1828" target="_blank">00:30:28.880</a></span> | <span class="t">we have the local derivative times the derivative of B1. And so because the local derivative is one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1837" target="_blank">00:30:37.360</a></span> | <span class="t">on these three elements, the local derivative multiplying the derivative of B1 will just be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1842" target="_blank">00:30:42.400</a></span> | <span class="t">the derivative of B1. And so you can look at it as a router. Basically, an addition is a router</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1849" target="_blank">00:30:49.440</a></span> | <span class="t">of gradient. Whatever gradient comes from above, it just gets routed equally to all the elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1853" target="_blank">00:30:53.840</a></span> | <span class="t">that participate in that addition. So in this case, the derivative of B1 will just flow equally to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1860" target="_blank">00:31:00.080</a></span> | <span class="t">the derivative of A11, A12, and A13. So if we have a derivative of all the elements of B in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1866" target="_blank">00:31:06.800</a></span> | <span class="t">column tensor, which is D counts sum that we've calculated just now, we basically see that what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1873" target="_blank">00:31:13.440</a></span> | <span class="t">that amounts to is all of these are now flowing to all these elements of A, and they're doing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1879" target="_blank">00:31:19.600</a></span> | <span class="t">horizontally. So basically what we want is we want to take the D counts sum of size 32 by 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1885" target="_blank">00:31:25.600</a></span> | <span class="t">and we just want to replicate it 27 times horizontally to create 32 by 27 array. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1892" target="_blank">00:31:32.320</a></span> | <span class="t">there's many ways to implement this operation. You could, of course, just replicate the tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1896" target="_blank">00:31:36.400</a></span> | <span class="t">but I think maybe one clean one is that D counts is simply torch.once like, so just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1904" target="_blank">00:31:44.560</a></span> | <span class="t">two-dimensional arrays of ones in the shape of counts, so 32 by 27, times D counts sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1912" target="_blank">00:31:52.080</a></span> | <span class="t">So this way we're letting the broadcasting here basically implement the replication. You can look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1917" target="_blank">00:31:57.840</a></span> | <span class="t">at it that way. But then we have to also be careful because D counts was all already calculated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1925" target="_blank">00:32:05.040</a></span> | <span class="t">We calculated earlier here, and that was just the first branch, and we're now finishing the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1930" target="_blank">00:32:10.000</a></span> | <span class="t">branch. So we need to make sure that these gradients add, so plus equals. And then here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1935" target="_blank">00:32:15.280</a></span> | <span class="t">let's comment out the comparison, and let's make sure, crossing fingers, that we have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1943" target="_blank">00:32:23.360</a></span> | <span class="t">correct result. So PyTorch agrees with us on this gradient as well. Okay, hopefully we're getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1948" target="_blank">00:32:28.880</a></span> | <span class="t">a hang of this now. Counts is an element-wise exp of normlogits. So now we want denormlogits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1955" target="_blank">00:32:35.440</a></span> | <span class="t">And because it's an element-wise operation, everything is very simple. What is the local</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1960" target="_blank">00:32:40.080</a></span> | <span class="t">derivative of e to the x? It's famously just e to the x. So this is the local derivative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1965" target="_blank">00:32:45.760</a></span> | <span class="t">That is the local derivative. Now we already calculated it, and it's inside counts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1972" target="_blank">00:32:52.240</a></span> | <span class="t">so we might as well potentially just reuse counts. That is the local derivative, times D counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1978" target="_blank">00:32:58.640</a></span> | <span class="t">[typing]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1981" target="_blank">00:33:01.840</a></span> | <span class="t">Funny as that looks. Counts times D counts is the derivative on the normlogits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1985" target="_blank">00:33:05.520</a></span> | <span class="t">And now let's erase this, and let's verify, and it looks good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1990" target="_blank">00:33:10.160</a></span> | <span class="t">So that's normlogits. Okay, so we are here on this line now, denormlogits. We have that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=1999" target="_blank">00:33:19.600</a></span> | <span class="t">and we're trying to calculate D logits and D logit maxes, so backpropagating through this line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2005" target="_blank">00:33:25.280</a></span> | <span class="t">Now we have to be careful here because the shapes, again, are not the same, and so there's an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2009" target="_blank">00:33:29.440</a></span> | <span class="t">implicit broadcasting happening here. So normlogits has the shape of 32 by 27. Logits does as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2016" target="_blank">00:33:36.240</a></span> | <span class="t">but logit maxes is only 32 by 1. So there's a broadcasting here in the minus. Now here I tried</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2023" target="_blank">00:33:43.920</a></span> | <span class="t">to sort of write out a toy example again. We basically have that this is our C equals A minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2029" target="_blank">00:33:49.680</a></span> | <span class="t">B, and we see that because of the shape, these are 3 by 3, but this one is just a column.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2035" target="_blank">00:33:55.120</a></span> | <span class="t">And so for example, every element of C, we have to look at how it came to be. And every element of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2040" target="_blank">00:34:00.880</a></span> | <span class="t">C is just the corresponding element of A minus basically that associated B. So it's very clear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2049" target="_blank">00:34:09.520</a></span> | <span class="t">now that the derivatives of every one of these Cs with respect to their inputs are 1 for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2056" target="_blank">00:34:16.560</a></span> | <span class="t">corresponding A, and it's a negative 1 for the corresponding B. And so therefore, the derivatives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2066" target="_blank">00:34:26.080</a></span> | <span class="t">on the C will flow equally to the corresponding As and then also to the corresponding Bs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2072" target="_blank">00:34:32.720</a></span> | <span class="t">but then in addition to that, the Bs are broadcast, so we'll have to do the additional sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2077" target="_blank">00:34:37.280</a></span> | <span class="t">just like we did before. And of course, the derivatives for Bs will undergo A minus because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2082" target="_blank">00:34:42.960</a></span> | <span class="t">the local derivative here is negative 1. So dC32 by dB3 is negative 1. So let's just implement that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2091" target="_blank">00:34:51.760</a></span> | <span class="t">Basically, dLogits will be exactly copying the derivative on normLogits. So dLogits equals</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2101" target="_blank">00:35:01.840</a></span> | <span class="t">dNormLogits, and I'll do a dot clone for safety, so we're just making a copy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2106" target="_blank">00:35:06.720</a></span> | <span class="t">And then we have that dLogitmaxis will be the negative of dNormLogits because of the negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2114" target="_blank">00:35:14.560</a></span> | <span class="t">sign. And then we have to be careful because Logitmaxis is a column. And so just like we saw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2122" target="_blank">00:35:22.320</a></span> | <span class="t">before, because we keep replicating the same elements across all the columns, then in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2129" target="_blank">00:35:29.280</a></span> | <span class="t">backward pass, because we keep reusing this, these are all just like separate branches of use of that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2135" target="_blank">00:35:35.120</a></span> | <span class="t">one variable. And so therefore, we have to do a sum along one, we'd keep them equals true,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2140" target="_blank">00:35:40.320</a></span> | <span class="t">so that we don't destroy this dimension. And then dLogitmaxis will be the same shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2145" target="_blank">00:35:45.680</a></span> | <span class="t">Now, we have to be careful because this dLogits is not the final dLogits, and that's because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2151" target="_blank">00:35:51.040</a></span> | <span class="t">not only do we get gradient signal into Logits through here, but Logitmaxis is a function of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2157" target="_blank">00:35:57.200</a></span> | <span class="t">Logits, and that's a second branch into Logits. So this is not yet our final derivative for Logits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2162" target="_blank">00:36:02.720</a></span> | <span class="t">we will come back later for the second branch. For now, dLogitmaxis is the final derivative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2168" target="_blank">00:36:08.080</a></span> | <span class="t">So let me uncomment this CMP here, and let's just run this. And Logitmaxis, if PyTorch agrees with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2175" target="_blank">00:36:15.600</a></span> | <span class="t">us. So that was the derivative into through this line. Now, before we move on, I want to pause here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2182" target="_blank">00:36:22.960</a></span> | <span class="t">briefly, and I want to look at these Logitmaxis and especially their gradients. We've talked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2187" target="_blank">00:36:27.680</a></span> | <span class="t">previously in the previous lecture, that the only reason we're doing this is for the numerical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2192" target="_blank">00:36:32.160</a></span> | <span class="t">stability of the softmax that we are implementing here. And we talked about how if you take these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2197" target="_blank">00:36:37.520</a></span> | <span class="t">Logits for any one of these examples, so one row of this Logits tensor, if you add or subtract any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2203" target="_blank">00:36:43.680</a></span> | <span class="t">value equally to all the elements, then the value of the probs will be unchanged. You're not changing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2209" target="_blank">00:36:49.760</a></span> | <span class="t">the softmax. The only thing that this is doing is it's making sure that exp doesn't overflow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2214" target="_blank">00:36:54.480</a></span> | <span class="t">And the reason we're using a max is because then we are guaranteed that each row of Logits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2219" target="_blank">00:36:59.200</a></span> | <span class="t">the highest number, is zero. And so this will be safe. And so basically what that has repercussions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2228" target="_blank">00:37:08.560</a></span> | <span class="t">If it is the case that changing Logitmaxis does not change the probs, and therefore does not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2234" target="_blank">00:37:14.480</a></span> | <span class="t">change the loss, then the gradient on Logitmaxis should be zero. Because saying those two things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2240" target="_blank">00:37:20.160</a></span> | <span class="t">is the same. So indeed, we hope that this is very, very small numbers. Indeed, we hope this is zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2246" target="_blank">00:37:26.000</a></span> | <span class="t">Now, because of floating point sort of wonkiness, this doesn't come out exactly zero. Only in some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2251" target="_blank">00:37:31.840</a></span> | <span class="t">of the rows it does. But we get extremely small values, like 1, e, -9, or 10. And so this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2257" target="_blank">00:37:37.440</a></span> | <span class="t">telling us that the values of Logitmaxis are not impacting the loss, as they shouldn't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2262" target="_blank">00:37:42.240</a></span> | <span class="t">It feels kind of weird to backpropagate through this branch, honestly, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2266" target="_blank">00:37:46.640</a></span> | <span class="t">if you have any implementation of like f.crossentropy in PyTorch, and you block together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2273" target="_blank">00:37:53.200</a></span> | <span class="t">all of these elements, and you're not doing the backpropagation piece by piece,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2276" target="_blank">00:37:56.640</a></span> | <span class="t">then you would probably assume that the derivative through here is exactly zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2280" target="_blank">00:38:00.560</a></span> | <span class="t">So you would be sort of skipping this branch, because it's only done for numerical stability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2289" target="_blank">00:38:09.200</a></span> | <span class="t">But it's interesting to see that even if you break up everything into the full atoms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2292" target="_blank">00:38:12.960</a></span> | <span class="t">and you still do the computation as you'd like with respect to numerical stability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2296" target="_blank">00:38:16.800</a></span> | <span class="t">the correct thing happens. And you still get very, very small gradients here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2301" target="_blank">00:38:21.680</a></span> | <span class="t">basically reflecting the fact that the values of these do not matter with respect to the final loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2306" target="_blank">00:38:26.960</a></span> | <span class="t">Okay, so let's now continue backpropagation through this line here. We've just calculated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2311" target="_blank">00:38:31.760</a></span> | <span class="t">the Logitmaxis, and now we want to backprop into Logits through this second branch. Now here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2317" target="_blank">00:38:37.040</a></span> | <span class="t">of course, we took Logits, and we took the max along all the rows, and then we looked at its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2321" target="_blank">00:38:41.920</a></span> | <span class="t">values here. Now the way this works is that in PyTorch, this thing here, the max returns both</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2330" target="_blank">00:38:50.800</a></span> | <span class="t">the values, and it returns the indices at which those values to count the maximum value. Now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2336" target="_blank">00:38:56.320</a></span> | <span class="t">in the forward pass, we only used values, because that's all we needed. But in the backward pass,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2340" target="_blank">00:39:00.720</a></span> | <span class="t">it's extremely useful to know about where those maximum values occurred. And we have the indices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2346" target="_blank">00:39:06.560</a></span> | <span class="t">at which they occurred. And this will, of course, help us do the backpropagation. Because what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2351" target="_blank">00:39:11.600</a></span> | <span class="t">should the backward pass be here in this case? We have the Logis tensor, which is 32 by 27,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2356" target="_blank">00:39:16.800</a></span> | <span class="t">and in each row, we find the maximum value, and then that value gets plucked out into Logitmaxis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2361" target="_blank">00:39:21.840</a></span> | <span class="t">And so intuitively, basically, the derivative flowing through here then should be 1 times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2372" target="_blank">00:39:32.320</a></span> | <span class="t">the local derivative is 1 for the appropriate entry that was plucked out, and then times the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2378" target="_blank">00:39:38.400</a></span> | <span class="t">global derivative of the Logitmaxis. So really what we're doing here, if you think through it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2382" target="_blank">00:39:42.960</a></span> | <span class="t">is we need to take the DLogitmaxis, and we need to scatter it to the correct positions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2388" target="_blank">00:39:48.320</a></span> | <span class="t">in these Logits from where the maximum values came. And so I came up with one line of code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2397" target="_blank">00:39:57.920</a></span> | <span class="t">that does that. Let me just erase a bunch of stuff here. So the line of-- you could do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2402" target="_blank">00:40:02.560</a></span> | <span class="t">very similar to what we've done here, where we create a zeros, and then we populate the correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2407" target="_blank">00:40:07.680</a></span> | <span class="t">elements. So we use the indices here, and we would set them to be 1. But you can also use one-hot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2414" target="_blank">00:40:14.400</a></span> | <span class="t">So f.one-hot, and then I'm taking the Logits.max over the first dimension, dot indices, and I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2422" target="_blank">00:40:22.400</a></span> | <span class="t">telling PyTorch that the dimension of every one of these tensors should be 27. And so what this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2431" target="_blank">00:40:31.600</a></span> | <span class="t">is going to do is-- okay, I apologize, this is crazy. plt.imshow of this. It's really just an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2440" target="_blank">00:40:40.640</a></span> | <span class="t">array of where the maxis came from in each row, and that element is 1, and all the other elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2446" target="_blank">00:40:46.320</a></span> | <span class="t">are 0. So it's a one-hot vector in each row, and these indices are now populating a single 1 in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2452" target="_blank">00:40:52.640</a></span> | <span class="t">the proper place. And then what I'm doing here is I'm multiplying by the Logitmaxis. And keep in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2458" target="_blank">00:40:58.480</a></span> | <span class="t">mind that this is a column of 32 by 1. And so when I'm doing this times the Logitmaxis, the Logitmaxis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2468" target="_blank">00:41:08.240</a></span> | <span class="t">will broadcast, and that column will get replicated, and then an element-wise multiply will ensure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2474" target="_blank">00:41:14.320</a></span> | <span class="t">that each of these just gets routed to whichever one of these bits is turned on. And so that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2479" target="_blank">00:41:19.760</a></span> | <span class="t">another way to implement this kind of an operation. And both of these can be used. I just thought I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2487" target="_blank">00:41:27.200</a></span> | <span class="t">would show an equivalent way to do it. And I'm using += because we already calculated the logits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2492" target="_blank">00:41:32.240</a></span> | <span class="t">here, and this is now the second branch. So let's look at logits and make sure that this is correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2499" target="_blank">00:41:39.600</a></span> | <span class="t">And we see that we have exactly the correct answer. Next up, we want to continue with logits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2506" target="_blank">00:41:46.720</a></span> | <span class="t">here. That is an outcome of a matrix multiplication and a bias offset in this linear layer. So I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2514" target="_blank">00:41:54.880</a></span> | <span class="t">printed out the shapes of all these intermediate tensors. We see that logits is of course 32 by 27,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2520" target="_blank">00:42:00.320</a></span> | <span class="t">as we've just seen. Then the h here is 32 by 64. So these are 64-dimensional hidden states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2527" target="_blank">00:42:07.120</a></span> | <span class="t">And then this w matrix projects those 64-dimensional vectors into 27 dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2533" target="_blank">00:42:13.040</a></span> | <span class="t">And then there's a 27-dimensional offset, which is a one-dimensional vector. Now we should note</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2539" target="_blank">00:42:19.120</a></span> | <span class="t">that this plus here actually broadcasts, because h multiplied by w2 will give us a 32 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2546" target="_blank">00:42:26.320</a></span> | <span class="t">And so then this plus b2 is a 27-dimensional vector here. Now in the rules of broadcasting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2553" target="_blank">00:42:33.520</a></span> | <span class="t">what's going to happen with this bias vector is that this one-dimensional vector of 27</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2558" target="_blank">00:42:38.240</a></span> | <span class="t">will get aligned with a padded dimension of 1 on the left. And it will basically become a row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2564" target="_blank">00:42:44.160</a></span> | <span class="t">vector. And then it will get replicated vertically 32 times to make it 32 by 27. And then there's an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2570" target="_blank">00:42:50.400</a></span> | <span class="t">element-wise multiply. Now the question is, how do we back propagate from logits to the hidden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2578" target="_blank">00:42:58.320</a></span> | <span class="t">states, the weight matrix w2, and the bias b2? And you might think that we need to go to some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2584" target="_blank">00:43:04.400</a></span> | <span class="t">matrix calculus, and then we have to look up the derivative for a matrix multiplication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2590" target="_blank">00:43:10.640</a></span> | <span class="t">But actually, you don't have to do any of that. And you can go back to first principles and derive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2594" target="_blank">00:43:14.240</a></span> | <span class="t">this yourself on a piece of paper. And specifically what I like to do, and what I find works well for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2599" target="_blank">00:43:19.680</a></span> | <span class="t">me, is you find a specific small example that you then fully write out. And then in the process of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2605" target="_blank">00:43:25.360</a></span> | <span class="t">analyzing how that individual small example works, you will understand the broader pattern.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2609" target="_blank">00:43:29.760</a></span> | <span class="t">And you'll be able to generalize and write out the full general formula for how these derivatives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2615" target="_blank">00:43:35.760</a></span> | <span class="t">flow in an expression like this. So let's try that out. So pardon the low budget production here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2621" target="_blank">00:43:41.120</a></span> | <span class="t">but what I've done here is I'm writing it out on a piece of paper. Really what we are interested</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2625" target="_blank">00:43:45.600</a></span> | <span class="t">in is we have a multiply b plus c, and that creates a d. And we have the derivative of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2632" target="_blank">00:43:52.800</a></span> | <span class="t">loss with respect to d, and we'd like to know what the derivative of the loss is with respect to a,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2636" target="_blank">00:43:56.480</a></span> | <span class="t">b, and c. Now these here are little two-dimensional examples of a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2641" target="_blank">00:44:01.200</a></span> | <span class="t">multiplication. 2 by 2 times a 2 by 2 plus a 2, a vector of just two elements, c1 and c2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2648" target="_blank">00:44:08.960</a></span> | <span class="t">gives me a 2 by 2. Now notice here that I have a bias vector here called c, and the bias vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2656" target="_blank">00:44:16.880</a></span> | <span class="t">is c1 and c2. But as I described over here, that bias vector will become a row vector in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2661" target="_blank">00:44:21.920</a></span> | <span class="t">broadcasting and will replicate vertically. So that's what's happening here as well. c1, c2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2666" target="_blank">00:44:26.880</a></span> | <span class="t">is replicated vertically, and we see how we have two rows of c1, c2 as a result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2671" target="_blank">00:44:31.760</a></span> | <span class="t">So now when I say write it out, I just mean like this. Basically break up this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2677" target="_blank">00:44:37.920</a></span> | <span class="t">multiplication into the actual thing that's going on under the hood. So as a result of matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2683" target="_blank">00:44:43.920</a></span> | <span class="t">multiplication and how it works, d11 is the result of a dot product between the first row of a and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2689" target="_blank">00:44:49.600</a></span> | <span class="t">the first column of b. So a11, b11 plus a12, b21 plus c1, and so on and so forth for all the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2699" target="_blank">00:44:59.840</a></span> | <span class="t">elements of d. And once you actually write it out, it becomes obvious this is just a bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2704" target="_blank">00:45:04.320</a></span> | <span class="t">multiplies and adds. And we know from micrograd how to differentiate multiplies and adds. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2711" target="_blank">00:45:11.440</a></span> | <span class="t">this is not scary anymore. It's not just matrix multiplication. It's just tedious, unfortunately,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2716" target="_blank">00:45:16.640</a></span> | <span class="t">but this is completely tractable. We have dl by d for all of these, and we want dl by all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2723" target="_blank">00:45:23.280</a></span> | <span class="t">little other variables. So how do we achieve that and how do we actually get the gradients?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2727" target="_blank">00:45:27.280</a></span> | <span class="t">Okay, so the low budget production continues here. So let's, for example, derive the derivative of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2733" target="_blank">00:45:33.040</a></span> | <span class="t">the loss with respect to a11. We see here that a11 occurs twice in our simple expression,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2739" target="_blank">00:45:39.440</a></span> | <span class="t">right here, right here, and influences d11 and d12. So what is dl by d a11? Well, it's dl by d11</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2749" target="_blank">00:45:49.440</a></span> | <span class="t">times the local derivative of d11, which in this case is just b11, because that's what's multiplying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2756" target="_blank">00:45:56.080</a></span> | <span class="t">a11 here. And likewise here, the local derivative of d12 with respect to a11 is just b12. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2764" target="_blank">00:46:04.240</a></span> | <span class="t">b12 will, in the chain rule, therefore, multiply dl by d12. And then because a11 is used both to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2771" target="_blank">00:46:11.440</a></span> | <span class="t">produce d11 and d12, we need to add up the contributions of both of those sort of chains</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2778" target="_blank">00:46:18.720</a></span> | <span class="t">that are running in parallel. And that's why we get a plus, just adding up those two contributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2785" target="_blank">00:46:25.680</a></span> | <span class="t">And that gives us dl by d a11. We can do the exact same analysis for the other one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2791" target="_blank">00:46:31.280</a></span> | <span class="t">for all the other elements of A. And when you simply write it out, it's just super simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2796" target="_blank">00:46:36.240</a></span> | <span class="t">taking of gradients on expressions like this. You find that this matrix dl by da that we're after,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2807" target="_blank">00:46:47.360</a></span> | <span class="t">right, if we just arrange all of them in the same shape as A takes, so A is just a 2x2 matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2813" target="_blank">00:46:53.680</a></span> | <span class="t">so dl by da here will be also just the same shape tensor with the derivatives now, so dl by da11,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2824" target="_blank">00:47:04.000</a></span> | <span class="t">etc. And we see that actually we can express what we've written out here as a matrix multiply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2829" target="_blank">00:47:09.920</a></span> | <span class="t">And so it just so happens that all of these formulas that we've derived here by taking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2836" target="_blank">00:47:16.400</a></span> | <span class="t">gradients can actually be expressed as a matrix multiplication. And in particular,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2840" target="_blank">00:47:20.720</a></span> | <span class="t">we see that it is the matrix multiplication of these two matrices. So it is the dl by d</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2848" target="_blank">00:47:28.480</a></span> | <span class="t">and then matrix multiplying B, but B transpose actually. So you see that B21 and B12 have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2856" target="_blank">00:47:36.640</a></span> | <span class="t">changed place, whereas before we had, of course, B11, B12, B21, B22. So you see that this other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2864" target="_blank">00:47:44.880</a></span> | <span class="t">matrix B is transposed. And so basically what we have, long story short, just by doing very simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2870" target="_blank">00:47:50.800</a></span> | <span class="t">reasoning here, by breaking up the expression in the case of a very simple example, is that dl by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2876" target="_blank">00:47:56.480</a></span> | <span class="t">da is, which is this, is simply equal to dl by dd matrix multiplied with B transpose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2883" target="_blank">00:48:03.520</a></span> | <span class="t">So that is what we have so far. Now we also want the derivative with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2890" target="_blank">00:48:10.800</a></span> | <span class="t">B and C. Now for B, I'm not actually doing the full derivation because honestly, it's not deep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2898" target="_blank">00:48:18.800</a></span> | <span class="t">It's just annoying. It's exhausting. You can actually do this analysis yourself. You'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2903" target="_blank">00:48:23.680</a></span> | <span class="t">also find that if you take these expressions and you differentiate with respect to B instead of A,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2908" target="_blank">00:48:28.240</a></span> | <span class="t">you will find that dl by db is also a matrix multiplication. In this case, you have to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2913" target="_blank">00:48:33.760</a></span> | <span class="t">the matrix A and transpose it and matrix multiply that with dl by dd. And that's what gives you the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2920" target="_blank">00:48:40.720</a></span> | <span class="t">dl by db. And then here for the offsets C1 and C2, if you again just differentiate with respect to C1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2928" target="_blank">00:48:48.560</a></span> | <span class="t">you will find an expression like this and C2, an expression like this. And basically you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2935" target="_blank">00:48:55.840</a></span> | <span class="t">find that dl by dc is simply, because they're just offsetting these expressions, you just have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2941" target="_blank">00:49:01.360</a></span> | <span class="t">to take the dl by dd matrix of the derivatives of d and you just have to sum across the columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2950" target="_blank">00:49:10.480</a></span> | <span class="t">And that gives you the derivatives for C. So long story short, the backward pass of a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2957" target="_blank">00:49:17.680</a></span> | <span class="t">multiply is a matrix multiply. And instead of, just like we had d equals A times B plus C,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2963" target="_blank">00:49:23.040</a></span> | <span class="t">in a scalar case, we sort of like arrive at something very, very similar, but now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2968" target="_blank">00:49:28.080</a></span> | <span class="t">with a matrix multiplication instead of a scalar multiplication. So the derivative of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2974" target="_blank">00:49:34.720</a></span> | <span class="t">d with respect to A is dl by dd matrix multiply B transpose. And here it's A transpose multiply dl</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2984" target="_blank">00:49:44.320</a></span> | <span class="t">by dd. But in both cases, it's a matrix multiplication with the derivative and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2989" target="_blank">00:49:49.760</a></span> | <span class="t">other term in the multiplication. And for C, it is a sum. Now I'll tell you a secret. I can never</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=2998" target="_blank">00:49:58.560</a></span> | <span class="t">remember the formulas that we just derived for backpropagating from matrix multiplication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3002" target="_blank">00:50:02.560</a></span> | <span class="t">and I can backpropagate through these expressions just fine. And the reason this works is because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3007" target="_blank">00:50:07.040</a></span> | <span class="t">the dimensions have to work out. So let me give you an example. Say I want to create dh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3012" target="_blank">00:50:12.640</a></span> | <span class="t">Then what should dh be? Number one, I have to know that the shape of dh must be the same as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3019" target="_blank">00:50:19.760</a></span> | <span class="t">the shape of h. And the shape of h is 32 by 64. And then the other piece of information I know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3025" target="_blank">00:50:25.760</a></span> | <span class="t">is that dh must be some kind of matrix multiplication of d logits with w2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3032" target="_blank">00:50:32.560</a></span> | <span class="t">And d logits is 32 by 27, and w2 is 64 by 27. There is only a single way to make the shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3040" target="_blank">00:50:40.880</a></span> | <span class="t">work out in this case, and it is indeed the correct result. In particular here, h needs to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3047" target="_blank">00:50:47.440</a></span> | <span class="t">be 32 by 64. The only way to achieve that is to take a d logits and matrix multiply it withâ€¦</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3054" target="_blank">00:50:54.400</a></span> | <span class="t">You see how I have to take w2, but I have to transpose it to make the dimensions work out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3060" target="_blank">00:51:00.240</a></span> | <span class="t">So w2 transpose. And it is the only way to matrix multiply those two pieces to make the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3065" target="_blank">00:51:05.680</a></span> | <span class="t">shapes work out. And that turns out to be the correct formula. So if we come here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3069" target="_blank">00:51:09.520</a></span> | <span class="t">we want dh, which is dA. And we see that dA is dL by dD matrix multiply B transpose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3077" target="_blank">00:51:17.120</a></span> | <span class="t">So that is d logits multiply, and B is w2, so w2 transpose, which is exactly what we have here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3084" target="_blank">00:51:24.880</a></span> | <span class="t">So there is no need to remember these formulas. Similarly, now if I want dw2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3090" target="_blank">00:51:30.960</a></span> | <span class="t">well I know that it must be a matrix multiplication of d logits and h. And maybe there is a few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3098" target="_blank">00:51:38.400</a></span> | <span class="t">transposeâ€¦ Like there is one transpose in there as well. And I do not know which way it is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3101" target="_blank">00:51:41.920</a></span> | <span class="t">so I have to come to w2. And I see that its shape is 64 by 27, and that has to come from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3108" target="_blank">00:51:48.480</a></span> | <span class="t">some matrix multiplication of these two. And so to get a 64 by 27, I need to take h, I need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3117" target="_blank">00:51:57.680</a></span> | <span class="t">transpose it, and then I need to matrix multiply it. So that will become 64 by 32. And then I need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3124" target="_blank">00:52:04.080</a></span> | <span class="t">to matrix multiply it with 32 by 27. And that is going to give me a 64 by 27. So I need to matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3129" target="_blank">00:52:09.440</a></span> | <span class="t">multiply this with d logits dot shape, just like that. That is the only way to make the dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3134" target="_blank">00:52:14.000</a></span> | <span class="t">work out, and just use matrix multiplication. And if we come here, we see that that is exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3139" target="_blank">00:52:19.680</a></span> | <span class="t">what is here. So a transpose, a for us is h, multiplied with d logits. So that is w2. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3148" target="_blank">00:52:28.480</a></span> | <span class="t">db2 is just the vertical sum. And actually, in the same way, there is only one way to make the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3157" target="_blank">00:52:37.120</a></span> | <span class="t">shapes work out. I do not have to remember that it is a vertical sum along the 0th axis, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3161" target="_blank">00:52:41.840</a></span> | <span class="t">that is the only way that this makes sense. Because b2 shape is 27, so in order to get a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3167" target="_blank">00:52:47.600</a></span> | <span class="t">d logits here, it is 32 by 27. So knowing that it is just sum over d logits in some direction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3176" target="_blank">00:52:56.480</a></span> | <span class="t">that direction must be 0, because I need to eliminate this dimension. So it is this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3184" target="_blank">00:53:04.720</a></span> | <span class="t">So this is kind of like the hacky way. Let me copy, paste, and delete that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3191" target="_blank">00:53:11.040</a></span> | <span class="t">And let me swing over here. And this is our backward pass for the linear layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3195" target="_blank">00:53:15.120</a></span> | <span class="t">hopefully. So now let us uncomment these three. And we are checking that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3201" target="_blank">00:53:21.040</a></span> | <span class="t">got all the three derivatives correct. And run. And we see that h, w2, and b2 are all exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3210" target="_blank">00:53:30.720</a></span> | <span class="t">correct. So we backpropagated through a linear layer. Now next up, we have derivative for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3218" target="_blank">00:53:38.960</a></span> | <span class="t">h already. And we need to backpropagate through tanh into hpreact. So we want to derive dhpreact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3226" target="_blank">00:53:46.000</a></span> | <span class="t">And here we have to backpropagate through a tanh. And we have already done this in micrograd.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3231" target="_blank">00:53:51.120</a></span> | <span class="t">And we remember that tanh is a very simple backward formula. Now unfortunately,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3235" target="_blank">00:53:55.600</a></span> | <span class="t">if I just put in d by dx of tanh of x into Boltram alpha, it lets us down. It tells us that it is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3240" target="_blank">00:54:00.960</a></span> | <span class="t">hyperbolic secant function squared of x. It is not exactly helpful. But luckily, Google image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3246" target="_blank">00:54:06.800</a></span> | <span class="t">search does not let us down. And it gives us the simpler formula. And in particular, if you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3251" target="_blank">00:54:11.440</a></span> | <span class="t">that a is equal to tanh of z, then da by dz, backpropagating through tanh, is just 1 minus a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3258" target="_blank">00:54:18.160</a></span> | <span class="t">squared. And take note that 1 minus a squared, a here is the output of the tanh, not the input to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3264" target="_blank">00:54:24.800</a></span> | <span class="t">the tanh, z. So the da by dz is here formulated in terms of the output of that tanh. And here also,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3272" target="_blank">00:54:32.960</a></span> | <span class="t">in Google image search, we have the full derivation if you want to actually take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3276" target="_blank">00:54:36.400</a></span> | <span class="t">actual definition of tanh and work through the math to figure out 1 minus tanh squared of z.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3281" target="_blank">00:54:41.440</a></span> | <span class="t">So 1 minus a squared is the local derivative. In our case, that is 1 minus the output of tanh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3290" target="_blank">00:54:50.720</a></span> | <span class="t">squared, which here is h. So it's h squared. And that is the local derivative. And then times the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3298" target="_blank">00:54:58.160</a></span> | <span class="t">chain rule, dh. So that is going to be our candidate implementation. So if we come here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3304" target="_blank">00:55:04.160</a></span> | <span class="t">and then uncomment this, let's hope for the best. And we have the right answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3311" target="_blank">00:55:11.280</a></span> | <span class="t">Okay, next up, we have dhpreact. And we want to backpropagate into the gain, the bn_raw,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3317" target="_blank">00:55:17.600</a></span> | <span class="t">and the bn_bias. So here, this is the bash norm parameters, bn_gain and bn_bias inside the bash</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3323" target="_blank">00:55:23.120</a></span> | <span class="t">norm that take the bn_raw that is exact unit Gaussian, and they scale it and shift it. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3329" target="_blank">00:55:29.520</a></span> | <span class="t">these are the parameters of the bash norm. Now, here, we have a multiplication. But it's worth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3334" target="_blank">00:55:34.880</a></span> | <span class="t">noting that this multiply is very, very different from this matrix multiply here. Matrix multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3339" target="_blank">00:55:39.920</a></span> | <span class="t">are dot products between rows and columns of these matrices involved. This is an element-wise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3345" target="_blank">00:55:45.200</a></span> | <span class="t">multiply. So things are quite a bit simpler. Now, we do have to be careful with some of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3349" target="_blank">00:55:49.440</a></span> | <span class="t">broadcasting happening in this line of code, though. So you see how bn_gain and bn_bias are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3355" target="_blank">00:55:55.760</a></span> | <span class="t">1 by 64, but dhpreact and bn_raw are 32 by 64. So we have to be careful with that and make sure that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3364" target="_blank">00:56:04.080</a></span> | <span class="t">all the shapes work out fine and that the broadcasting is correctly backpropagated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3367" target="_blank">00:56:07.600</a></span> | <span class="t">So in particular, let's start with dbn_gain. So dbn_gain should be, and here, this is again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3376" target="_blank">00:56:16.240</a></span> | <span class="t">element-wise multiply. And whenever we have a times b equals c, we saw that the local derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3381" target="_blank">00:56:21.680</a></span> | <span class="t">here is just, if this is a, the local derivative is just the b, the other one. So the local</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3386" target="_blank">00:56:26.880</a></span> | <span class="t">derivative is just bn_raw and then times chain rule. So dhpreact. So this is the candidate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3396" target="_blank">00:56:36.640</a></span> | <span class="t">gradient. Now, again, we have to be careful because bn_gain is of size 1 by 64. But this here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3405" target="_blank">00:56:45.280</a></span> | <span class="t">would be 32 by 64. And so the correct thing to do in this case, of course, is that bn_gain,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3413" target="_blank">00:56:53.120</a></span> | <span class="t">here is a rule vector of 64 numbers, it gets replicated vertically in this operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3417" target="_blank">00:56:57.840</a></span> | <span class="t">And so therefore, the correct thing to do is to sum because it's being replicated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3422" target="_blank">00:57:02.960</a></span> | <span class="t">And therefore, all the gradients in each of the rows that are now flowing backwards need to sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3429" target="_blank">00:57:09.040</a></span> | <span class="t">up to that same tensor dbn_gain. So we have to sum across all the zero, all the examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3436" target="_blank">00:57:16.640</a></span> | <span class="t">basically, which is the direction in which this gets replicated. And now we have to be also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3441" target="_blank">00:57:21.280</a></span> | <span class="t">careful because bn_gain is of shape 1 by 64. So in fact, I need to keep them as true. Otherwise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3449" target="_blank">00:57:29.840</a></span> | <span class="t">I would just get 64. Now, I don't actually really remember why the bn_gain and the bn_bias,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3456" target="_blank">00:57:36.320</a></span> | <span class="t">I made them be 1 by 64. But the biases b1 and b2, I just made them be one dimensional vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3465" target="_blank">00:57:45.280</a></span> | <span class="t">they're not two dimensional tensors. So I can't recall exactly why I left the gain and the bias</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3471" target="_blank">00:57:51.520</a></span> | <span class="t">as two dimensional. But it doesn't really matter as long as you are consistent and you're keeping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3475" target="_blank">00:57:55.440</a></span> | <span class="t">it the same. So in this case, we want to keep the dimension so that the tensor shapes work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3481" target="_blank">00:58:01.360</a></span> | <span class="t">Next up, we have bn_raw. So dbn_raw will be bn_gain multiplying dh_preact. That's our chain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3493" target="_blank">00:58:13.840</a></span> | <span class="t">rule. Now, what about the dimensions of this? We have to be careful, right? So dh_preact is 32 by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3502" target="_blank">00:58:22.960</a></span> | <span class="t">64, bn_gain is 1 by 64. So it will just get replicated to create this multiplication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3510" target="_blank">00:58:30.880</a></span> | <span class="t">which is the correct thing because in a forward pass, it also gets replicated in just the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3514" target="_blank">00:58:34.880</a></span> | <span class="t">way. So in fact, we don't need the brackets here, we're done. And the shapes are already correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3520" target="_blank">00:58:40.080</a></span> | <span class="t">And finally, for the bias, very similar. This bias here is very, very similar to the bias we saw in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3527" target="_blank">00:58:47.600</a></span> | <span class="t">the linear layer. And we see that the gradients from h_preact will simply flow into the biases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3533" target="_blank">00:58:53.280</a></span> | <span class="t">and add up because these are just offsets. And so basically, we want this to be dh_preact,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3540" target="_blank">00:59:00.000</a></span> | <span class="t">but it needs to sum along the right dimension. And in this case, similar to the gain,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3544" target="_blank">00:59:04.960</a></span> | <span class="t">we need to sum across the zeroth dimension, the examples, because of the way that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3549" target="_blank">00:59:09.280</a></span> | <span class="t">bias gets replicated vertically. And we also want to have keep_them as true. And so this will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3555" target="_blank">00:59:15.920</a></span> | <span class="t">basically take this and sum it up and give us a 1 by 64. So this is the candidate implementation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3562" target="_blank">00:59:22.880</a></span> | <span class="t">and makes all the shapes work. Let me bring it up down here. And then let me uncomment these three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3570" target="_blank">00:59:30.240</a></span> | <span class="t">lines to check that we are getting the correct result for all the three tensors. And indeed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3576" target="_blank">00:59:36.400</a></span> | <span class="t">we see that all of that got backpropagated correctly. So now we get to the batch norm layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3581" target="_blank">00:59:41.040</a></span> | <span class="t">We see how here bn_gain and bn_bias are the parameters, so the backpropagation ends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3586" target="_blank">00:59:46.640</a></span> | <span class="t">But bn_raw now is the output of the standardization. So here, what I'm doing, of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3592" target="_blank">00:59:52.960</a></span> | <span class="t">is I'm breaking up the batch norm into manageable pieces so we can backpropagate through each line</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3597" target="_blank">00:59:57.040</a></span> | <span class="t">individually. But basically, what's happening is bn_mean_i is the sum. So this is the bn_mean_i.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3606" target="_blank">01:00:06.160</a></span> | <span class="t">I apologize for the variable naming. bn_diff is x minus mu. bn_diff_2 is x minus mu squared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3614" target="_blank">01:00:14.560</a></span> | <span class="t">here inside the variance. bn_var is the variance, so sigma squared. This is bn_var.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3621" target="_blank">01:00:21.520</a></span> | <span class="t">And it's basically the sum of squares. So this is the x minus mu squared and then the sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3628" target="_blank">01:00:28.800</a></span> | <span class="t">Now, you'll notice one departure here. Here, it is normalized as 1 over m,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3634" target="_blank">01:00:34.160</a></span> | <span class="t">which is the number of examples. Here, I am normalizing as 1 over n minus 1 instead of m.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3640" target="_blank">01:00:40.720</a></span> | <span class="t">And this is deliberate, and I'll come back to that in a bit when we are at this line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3644" target="_blank">01:00:44.640</a></span> | <span class="t">It is something called the Bessel's correction, but this is how I want it in our case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3649" target="_blank">01:00:49.600</a></span> | <span class="t">bn_var_inv then becomes basically bn_var plus epsilon. Epsilon is 1, negative 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3657" target="_blank">01:00:57.120</a></span> | <span class="t">And then its 1 over square root is the same as raising to the power of negative 0.5,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3663" target="_blank">01:01:03.680</a></span> | <span class="t">right? Because 0.5 is square root. And then negative makes it 1 over square root.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3668" target="_blank">01:01:08.800</a></span> | <span class="t">So bn_var_inv is 1 over this denominator here. And then we can see that bn_raw, which is the x hat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3676" target="_blank">01:01:16.080</a></span> | <span class="t">here, is equal to the bn_diff, the numerator, multiplied by the bn_var_inv. And this line here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3686" target="_blank">01:01:26.160</a></span> | <span class="t">that creates H preact was the last piece we've already backpropagated through it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3689" target="_blank">01:01:29.920</a></span> | <span class="t">So now what we want to do is we are here, and we have bn_raw, and we have to first backpropagate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3697" target="_blank">01:01:37.040</a></span> | <span class="t">into bn_diff and bn_var_inv. So now we're here, and we have dbn_raw, and we need to backpropagate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3704" target="_blank">01:01:44.880</a></span> | <span class="t">through this line. Now, I've written out the shapes here, and indeed bn_var_inv is a shape 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3711" target="_blank">01:01:51.680</a></span> | <span class="t">by 64, so there is a broadcasting happening here that we have to be careful with. But it is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3717" target="_blank">01:01:57.520</a></span> | <span class="t">an element-wise simple multiplication. By now, we should be pretty comfortable with that. To get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3722" target="_blank">01:02:02.000</a></span> | <span class="t">dbn_diff, we know that this is just bn_var_inv multiplied with dbn_raw. And conversely, to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3733" target="_blank">01:02:13.200</a></span> | <span class="t">dbn_var_inv, we need to take bn_diff and multiply that by dbn_raw. So this is the candidate, but of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3744" target="_blank">01:02:24.240</a></span> | <span class="t">course we need to make sure that broadcasting is obeyed. So in particular, bn_var_inv multiplying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3750" target="_blank">01:02:30.080</a></span> | <span class="t">with dbn_raw will be okay and give us 32 by 64 as we expect. But dbn_var_inv would be taking a 32</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3760" target="_blank">01:02:40.160</a></span> | <span class="t">by 64, multiplying it by 32 by 64. So this is a 32 by 64. But of course this bn_var_inv is only 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3770" target="_blank">01:02:50.640</a></span> | <span class="t">by 64. So the second line here needs a sum across the examples, and because there's this dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3778" target="_blank">01:02:58.400</a></span> | <span class="t">here, we need to make sure that keep_dim is true. So this is the candidate. Let's erase this and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3786" target="_blank">01:03:06.480</a></span> | <span class="t">let's swing down here and implement it. And then let's comment out dbn_var_inv and dbn_diff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3794" target="_blank">01:03:14.800</a></span> | <span class="t">Now, we'll actually notice that dbn_diff, by the way, is going to be incorrect. So when I run this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3804" target="_blank">01:03:24.640</a></span> | <span class="t">bn_var_inv is correct. bn_diff is not correct. And this is actually expected, because we're not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3811" target="_blank">01:03:31.440</a></span> | <span class="t">done with bn_diff. So in particular, when we slide here, we see here that bn_raw is a function of bn_diff,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3818" target="_blank">01:03:38.480</a></span> | <span class="t">but actually bn_var_inv is a function of bn_var, which is a function of bn_diff_do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3823" target="_blank">01:03:43.360</a></span> | <span class="t">which is a function of bn_diff. So it comes here. So bdn_diff, these variable names are crazy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3830" target="_blank">01:03:50.400</a></span> | <span class="t">I'm sorry. It branches out into two branches, and we've only done one branch of it. We have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3835" target="_blank">01:03:55.600</a></span> | <span class="t">continue our backpropagation and eventually come back to bn_diff, and then we'll be able to do a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3839" target="_blank">01:03:59.680</a></span> | <span class="t">+= and get the actual correct gradient. For now, it is good to verify that cmp also works. It doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3846" target="_blank">01:04:06.160</a></span> | <span class="t">just lie to us and tell us that everything is always correct. It can in fact detect when your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3851" target="_blank">01:04:11.440</a></span> | <span class="t">gradient is not correct. So that's good to see as well. Okay, so now we have the derivative here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3856" target="_blank">01:04:16.480</a></span> | <span class="t">and we're trying to backpropagate through this line. And because we're raising to a power of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3860" target="_blank">01:04:20.880</a></span> | <span class="t">-0.5, I brought up the power rule. And we see that basically we have that the bn_var will now be,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3867" target="_blank">01:04:27.360</a></span> | <span class="t">we bring down the exponent, so -0.5 times x, which is this, and now raise to the power of -0.5-1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3877" target="_blank">01:04:37.360</a></span> | <span class="t">which is -1.5. Now, we would have to also apply a small chain rule here in our head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3884" target="_blank">01:04:44.320</a></span> | <span class="t">because we need to take further derivative of bn_var with respect to this expression here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3889" target="_blank">01:04:49.840</a></span> | <span class="t">inside the bracket. But because this is an element-wise operation, and everything is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3893" target="_blank">01:04:53.680</a></span> | <span class="t">fairly simple, that's just 1. And so there's nothing to do there. So this is the local</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3898" target="_blank">01:04:58.560</a></span> | <span class="t">derivative, and then times the global derivative to create the chain rule. This is just times the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3903" target="_blank">01:05:03.760</a></span> | <span class="t">bn_var. So this is our candidate. Let me bring this down and uncomment the check.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3911" target="_blank">01:05:11.920</a></span> | <span class="t">And we see that we have the correct result. Now, before we backpropagate through the next line,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3919" target="_blank">01:05:19.440</a></span> | <span class="t">I want to briefly talk about the note here, where I'm using the Bessel's correction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3922" target="_blank">01:05:22.640</a></span> | <span class="t">dividing by n-1, instead of dividing by n, when I normalize here the sum of squares.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3929" target="_blank">01:05:29.680</a></span> | <span class="t">Now, you'll notice that this is a departure from the paper, which uses 1/n instead,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3934" target="_blank">01:05:34.000</a></span> | <span class="t">not 1/n-1. There, m is our n. And so it turns out that there are two ways of estimating variance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3942" target="_blank">01:05:42.400</a></span> | <span class="t">of an array. One is the biased estimate, which is 1/n, and the other one is the unbiased estimate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3949" target="_blank">01:05:49.120</a></span> | <span class="t">which is 1/n-1. Now, confusingly, in the paper, this is not very clearly described,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3955" target="_blank">01:05:55.760</a></span> | <span class="t">and also it's a detail that kind of matters, I think. They are using the biased version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3960" target="_blank">01:06:00.400</a></span> | <span class="t">at training time, but later, when they are talking about the inference, they are mentioning that when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3965" target="_blank">01:06:05.360</a></span> | <span class="t">they do the inference, they are using the unbiased estimate, which is the n-1 version, basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3973" target="_blank">01:06:13.440</a></span> | <span class="t">for inference, and to calibrate the running mean and the running variance, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3980" target="_blank">01:06:20.080</a></span> | <span class="t">And so they actually introduce a train-test mismatch, where in training, they use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3984" target="_blank">01:06:24.320</a></span> | <span class="t">biased version, and in test time, they use the unbiased version. I find this extremely confusing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3990" target="_blank">01:06:30.000</a></span> | <span class="t">You can read more about the Bessel's correction and why dividing by n-1 gives you a better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=3995" target="_blank">01:06:35.680</a></span> | <span class="t">estimate of the variance in the case where you have population sizes or samples for a population</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4000" target="_blank">01:06:40.400</a></span> | <span class="t">that are very small. And that is indeed the case for us, because we are dealing with mini-batches,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4006" target="_blank">01:06:46.960</a></span> | <span class="t">and these mini-batches are a small sample of a larger population, which is the entire training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4011" target="_blank">01:06:51.760</a></span> | <span class="t">set. And so it just turns out that if you just estimate it using 1/n, that actually almost always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4018" target="_blank">01:06:58.000</a></span> | <span class="t">underestimates the variance. And it is a biased estimator, and it is advised that you use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4022" target="_blank">01:07:02.800</a></span> | <span class="t">unbiased version and divide by n-1. And you can go through this article here that I liked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4027" target="_blank">01:07:07.680</a></span> | <span class="t">that actually describes the full reasoning, and I'll link it in the video description.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4031" target="_blank">01:07:11.120</a></span> | <span class="t">Now, when you calculate the torsion variance, you'll notice that they take the unbiased flag,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4037" target="_blank">01:07:17.360</a></span> | <span class="t">whether or not you want to divide by n or n-1. Confusingly, they do not mention what the default</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4044" target="_blank">01:07:24.080</a></span> | <span class="t">is for unbiased, but I believe unbiased by default is true. I'm not sure why the docs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4049" target="_blank">01:07:29.440</a></span> | <span class="t">here don't cite that. Now, in the batch norm 1D, the documentation again is kind of wrong</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4055" target="_blank">01:07:35.840</a></span> | <span class="t">and confusing. It says that the standard deviation is calculated via the biased estimator,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4060" target="_blank">01:07:40.560</a></span> | <span class="t">but this is actually not exactly right, and people have pointed out that it is not right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4065" target="_blank">01:07:45.040</a></span> | <span class="t">in a number of issues since then, because actually the rabbit hole is deeper, and they follow the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4070" target="_blank">01:07:50.960</a></span> | <span class="t">paper exactly, and they use the biased version for training. But when they're estimating the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4075" target="_blank">01:07:55.840</a></span> | <span class="t">running standard deviation, they are using the unbiased version. So again, there's the train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4080" target="_blank">01:08:00.400</a></span> | <span class="t">test mismatch. So long story short, I'm not a fan of train test discrepancies. I basically kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4087" target="_blank">01:08:07.200</a></span> | <span class="t">consider the fact that we use the biased version, the training time, and the unbiased test time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4093" target="_blank">01:08:13.040</a></span> | <span class="t">I basically consider this to be a bug, and I don't think that there's a good reason for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4096" target="_blank">01:08:16.800</a></span> | <span class="t">They don't really go into the detail of the reasoning behind it in this paper. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4102" target="_blank">01:08:22.240</a></span> | <span class="t">why I basically prefer to use the Bessel's correction in my own work. Unfortunately,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4107" target="_blank">01:08:27.360</a></span> | <span class="t">batch norm does not take a keyword argument that tells you whether or not you want to use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4112" target="_blank">01:08:32.400</a></span> | <span class="t">unbiased version or the biased version in both train and test, and so therefore anyone using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4116" target="_blank">01:08:36.560</a></span> | <span class="t">batch normalization basically in my view has a bit of a bug in the code. And this turns out to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4123" target="_blank">01:08:43.040</a></span> | <span class="t">be much less of a problem if your mini batch sizes are a bit larger. But still, I just find it kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4128" target="_blank">01:08:48.560</a></span> | <span class="t">of unpalatable. So maybe someone can explain why this is okay. But for now, I prefer to use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4134" target="_blank">01:08:54.320</a></span> | <span class="t">unbiased version consistently both during training and at test time, and that's why I'm using 1/n-1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4140" target="_blank">01:09:00.720</a></span> | <span class="t">here. Okay, so let's now actually backpropagate through this line. So the first thing that I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4147" target="_blank">01:09:07.840</a></span> | <span class="t">always like to do is I like to scrutinize the shapes first. So in particular here, looking at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4152" target="_blank">01:09:12.480</a></span> | <span class="t">the shapes of what's involved, I see that bn_var shape is 1 by 64, so it's a row vector, and bn_div2.shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4160" target="_blank">01:09:20.640</a></span> | <span class="t">is 32 by 64. So clearly here we're doing a sum over the zeroth axis to squash the first dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4169" target="_blank">01:09:29.600</a></span> | <span class="t">of the shapes here using a sum. So that right away actually hints to me that there will be some kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4176" target="_blank">01:09:36.240</a></span> | <span class="t">of a replication or broadcasting in the backward pass. And maybe you're noticing the pattern here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4181" target="_blank">01:09:41.120</a></span> | <span class="t">but basically anytime you have a sum in the forward pass, that turns into a replication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4186" target="_blank">01:09:46.720</a></span> | <span class="t">or broadcasting in the backward pass along the same dimension. And conversely, when we have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4191" target="_blank">01:09:51.840</a></span> | <span class="t">replication or a broadcasting in the forward pass, that indicates a variable reuse. And so in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4198" target="_blank">01:09:58.880</a></span> | <span class="t">backward pass, that turns into a sum over the exact same dimension. And so hopefully you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4203" target="_blank">01:10:03.600</a></span> | <span class="t">noticing that duality, that those two are kind of like the opposites of each other in the forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4207" target="_blank">01:10:07.360</a></span> | <span class="t">and the backward pass. Now once we understand the shapes, the next thing I like to do always is I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4212" target="_blank">01:10:12.480</a></span> | <span class="t">like to look at a toy example in my head to sort of just like understand roughly how the variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4218" target="_blank">01:10:18.240</a></span> | <span class="t">dependencies go in the mathematical formula. So here we have a two-dimensional array, bn_div2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4225" target="_blank">01:10:25.520</a></span> | <span class="t">which we are scaling by a constant, and then we are summing vertically over the columns. So if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4231" target="_blank">01:10:31.840</a></span> | <span class="t">we have a 2x2 matrix A and then we sum over the columns and scale, we would get a row vector b1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4237" target="_blank">01:10:37.120</a></span> | <span class="t">b2, and b1 depends on A in this way, where it's just sum that is scaled of A, and b2 in this way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4245" target="_blank">01:10:45.200</a></span> | <span class="t">where it's the second column summed and scaled. And so looking at this basically, what we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4252" target="_blank">01:10:52.400</a></span> | <span class="t">to do now is we have the derivatives on b1 and b2, and we want to back propagate them into A's.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4257" target="_blank">01:10:57.600</a></span> | <span class="t">And so it's clear that just differentiating in your head, the local derivative here is 1 over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4261" target="_blank">01:11:01.920</a></span> | <span class="t">n minus 1 times 1 for each one of these A's. And basically the derivative of b1 has to flow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4271" target="_blank">01:11:11.600</a></span> | <span class="t">through the columns of A scaled by 1 over n minus 1. And that's roughly what's happening here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4278" target="_blank">01:11:18.560</a></span> | <span class="t">So intuitively, the derivative flow tells us that dbn_div2 will be the local derivative of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4287" target="_blank">01:11:27.280</a></span> | <span class="t">this operation. And there are many ways to do this, by the way, but I like to do something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4290" target="_blank">01:11:30.960</a></span> | <span class="t">like this, torch.once_like of bn_div2. So I'll create a large array, two-dimensional, of ones,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4298" target="_blank">01:11:38.640</a></span> | <span class="t">and then I will scale it. So 1.0 divide by n minus 1. So this is an array of 1 over n minus 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4308" target="_blank">01:11:48.480</a></span> | <span class="t">And that's sort of like the local derivative. And now for the chain rule, I will simply just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4313" target="_blank">01:11:53.360</a></span> | <span class="t">multiply it by dbn_var. And notice here what's going to happen. This is 32 by 64,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4321" target="_blank">01:12:01.440</a></span> | <span class="t">and this is just 1 by 64. So I'm letting the broadcasting do the replication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4327" target="_blank">01:12:07.040</a></span> | <span class="t">because internally in PyTorch, basically dbn_var, which is 1 by 64 row vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4333" target="_blank">01:12:13.040</a></span> | <span class="t">will in this multiplication get copied vertically until the two are of the same shape, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4338" target="_blank">01:12:18.960</a></span> | <span class="t">there will be an element-wise multiply. And so the broadcasting is basically doing the replication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4344" target="_blank">01:12:24.480</a></span> | <span class="t">And I will end up with the derivatives of dbn_div2 here. So this is the candidate solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4351" target="_blank">01:12:31.920</a></span> | <span class="t">Let's bring it down here. Let's uncomment this line where we check it, and let's hope for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4358" target="_blank">01:12:38.080</a></span> | <span class="t">best. And indeed, we see that this is the correct formula. Next up, let's differentiate here into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4364" target="_blank">01:12:44.320</a></span> | <span class="t">bn_div. So here we have that bn_div is element-wise squared to create bn_div2. So this is a relatively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4372" target="_blank">01:12:52.320</a></span> | <span class="t">simple derivative, because it's a simple element-wise operation. So it's kind of like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4375" target="_blank">01:12:55.920</a></span> | <span class="t">scalar case. And we have that dbn_div should be, if this is x squared, then the derivative of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4382" target="_blank">01:13:02.640</a></span> | <span class="t">is 2x. So it's simply 2 times bn_div, that's the local derivative, and then times chain rule. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4390" target="_blank">01:13:10.560</a></span> | <span class="t">the shape of these is the same. They are of the same shape. So times this. So that's the backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4396" target="_blank">01:13:16.800</a></span> | <span class="t">pass for this variable. Let me bring that down here. And now we have to be careful, because we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4401" target="_blank">01:13:21.840</a></span> | <span class="t">already calculated dbn_div, right? So this is just the end of the other branch coming back to bn_div,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4410" target="_blank">01:13:30.880</a></span> | <span class="t">because bn_div was already backpropagated to way over here from bn_raw. So we now completed the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4417" target="_blank">01:13:37.520</a></span> | <span class="t">second branch. And so that's why I have to do plus equals. And if you recall, we had an incorrect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4423" target="_blank">01:13:43.040</a></span> | <span class="t">derivative for bn_div before. And I'm hoping that once we append this last missing piece,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4428" target="_blank">01:13:48.320</a></span> | <span class="t">we have the exact correctness. So let's run. And bn_div2, bn_div now actually shows the exact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4435" target="_blank">01:13:55.360</a></span> | <span class="t">correct derivative. So that's comforting. Okay, so let's now backpropagate through this line here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4441" target="_blank">01:14:01.520</a></span> | <span class="t">The first thing we do, of course, is we check the shapes. And I wrote them out here. And basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4447" target="_blank">01:14:07.840</a></span> | <span class="t">the shape of this is 32 by 64. H_prebn is the same shape. But bn_mini is a row vector, 1 by 64.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4456" target="_blank">01:14:16.080</a></span> | <span class="t">So this minus here will actually do broadcasting. And so we have to be careful with that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4460" target="_blank">01:14:20.560</a></span> | <span class="t">And as a hint to us, again, because of the duality, a broadcasting in the forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4465" target="_blank">01:14:25.360</a></span> | <span class="t">means a variable reuse. And therefore, there will be a sum in the backward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4469" target="_blank">01:14:29.200</a></span> | <span class="t">So let's write out the backward pass here now. Backpropagate into the H_prebn. Because these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4478" target="_blank">01:14:38.000</a></span> | <span class="t">are the same shape, then the local derivative for each one of the elements here is just one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4482" target="_blank">01:14:42.640</a></span> | <span class="t">for the corresponding element in here. So basically, what this means is that the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4488" target="_blank">01:14:48.400</a></span> | <span class="t">just simply copies. It's just a variable assignment. It's equality. So I'm just going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4492" target="_blank">01:14:52.880</a></span> | <span class="t">to clone this tensor just for safety to create an exact copy of db_ndiff. And then here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4500" target="_blank">01:15:00.640</a></span> | <span class="t">to backpropagate into this one, what I'm inclined to do here is db_bn_mini will basically be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4508" target="_blank">01:15:08.400</a></span> | <span class="t">what is the local derivative? Well, it's negative torch dot once like of the shape of bn_diff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4519" target="_blank">01:15:19.280</a></span> | <span class="t">Right? And then times the derivative here, db_ndiff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4529" target="_blank">01:15:29.440</a></span> | <span class="t">And this here is the backpropagation for the replicated bn_mini. So I still have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4538" target="_blank">01:15:38.640</a></span> | <span class="t">backpropagate through the replication in the broadcasting, and I do that by doing a sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4544" target="_blank">01:15:44.240</a></span> | <span class="t">So I'm going to take this whole thing, and I'm going to do a sum over the zeroth dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4548" target="_blank">01:15:48.800</a></span> | <span class="t">which was the replication. So if you scrutinize this, by the way, you'll notice that this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4557" target="_blank">01:15:57.120</a></span> | <span class="t">the same shape as that. And so what I'm doing here doesn't actually make that much sense,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4561" target="_blank">01:16:01.920</a></span> | <span class="t">because it's just an array of ones multiplying db_ndiff. So in fact, I can just do this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4570" target="_blank">01:16:10.080</a></span> | <span class="t">and that is equivalent. So this is the candidate backward pass. Let me copy it here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4576" target="_blank">01:16:16.160</a></span> | <span class="t">And then let me comment out this one and this one. Enter. And it's wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4585" target="_blank">01:16:25.040</a></span> | <span class="t">Damn. Actually, sorry, this is supposed to be wrong. And it's supposed to be wrong because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4594" target="_blank">01:16:34.560</a></span> | <span class="t">we are backpropagating from a bn_diff into h_prebn, but we're not done because bn_mini</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4601" target="_blank">01:16:41.120</a></span> | <span class="t">depends on h_prebn, and there will be a second portion of that derivative coming from this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4605" target="_blank">01:16:45.840</a></span> | <span class="t">second branch. So we're not done yet, and we expect it to be incorrect. So there you go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4609" target="_blank">01:16:49.920</a></span> | <span class="t">So let's now backpropagate from bn_mini into h_prebn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4614" target="_blank">01:16:54.400</a></span> | <span class="t">And so here again, we have to be careful because there's a broadcasting along,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4621" target="_blank">01:17:01.280</a></span> | <span class="t">or there's a sum along the zeroth dimension. So this will turn into broadcasting in the backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4625" target="_blank">01:17:05.920</a></span> | <span class="t">pass now. And I'm going to go a little bit faster on this line because it is very similar to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4630" target="_blank">01:17:10.560</a></span> | <span class="t">line that we had before, multiple lines in the past, in fact. So dh_prebn will be,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4638" target="_blank">01:17:18.880</a></span> | <span class="t">the gradient will be scaled by 1/n, and then basically this gradient here, db_ndiff_mini,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4647" target="_blank">01:17:27.280</a></span> | <span class="t">is going to be scaled by 1/n, and then it's going to flow across all the columns and deposit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4653" target="_blank">01:17:33.120</a></span> | <span class="t">itself into dh_prebn. So what we want is this thing scaled by 1/n. Let me put the constant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4660" target="_blank">01:17:40.480</a></span> | <span class="t">up front here. So scale down the gradient, and now we need to replicate it across all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4672" target="_blank">01:17:52.400</a></span> | <span class="t">rows here. So I like to do that by torch.once_like of basically h_prebn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4681" target="_blank">01:18:01.760</a></span> | <span class="t">And I will let the broadcasting do the work of replication. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4694" target="_blank">01:18:14.960</a></span> | <span class="t">like that. So this is dh_prebn, and hopefully we can plus equals that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4702" target="_blank">01:18:22.560</a></span> | <span class="t">So this here is broadcasting, and then this is the scaling. So this should be correct. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4713" target="_blank">01:18:33.840</a></span> | <span class="t">So that completes the backpropagation of the bastrom layer, and we are now here. Let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4719" target="_blank">01:18:39.360</a></span> | <span class="t">backpropagate through the linear layer 1 here. Now because everything is getting a little vertically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4724" target="_blank">01:18:44.800</a></span> | <span class="t">crazy, I copy-pasted the line here, and let's just backpropagate through this one line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4728" target="_blank">01:18:48.960</a></span> | <span class="t">So first, of course, we inspect the shapes, and we see that this is 32 by 64. mcat is 32 by 30,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4737" target="_blank">01:18:57.760</a></span> | <span class="t">w1 is 30 by 64, and b1 is just 64. So as I mentioned, backpropagating through linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4746" target="_blank">01:19:06.880</a></span> | <span class="t">layers is fairly easy just by matching the shapes, so let's do that. We have that d_mpcat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4754" target="_blank">01:19:14.080</a></span> | <span class="t">should be some matrix multiplication of dh_prebn with w1 and one transpose thrown in there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4761" target="_blank">01:19:21.600</a></span> | <span class="t">So to make mcat be 32 by 30, I need to take dh_prebn, 32 by 64, and multiply it by w1 dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4776" target="_blank">01:19:36.000</a></span> | <span class="t">transpose. To get dw1, I need to end up with 30 by 64. So to get that, I need to take mcat transpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4789" target="_blank">01:19:49.680</a></span> | <span class="t">and multiply that by dh_prebn. And finally, to get db1, this is an addition, and we saw that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4804" target="_blank">01:20:04.640</a></span> | <span class="t">basically I need to just sum the elements in dh_prebn along some dimension. And to make the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4810" target="_blank">01:20:10.800</a></span> | <span class="t">dimensions work out, I need to sum along the 0th axis here to eliminate this dimension, and we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4817" target="_blank">01:20:17.280</a></span> | <span class="t">not keep dims, so that we want to just get a single one-dimensional vector of 64. So these are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4824" target="_blank">01:20:24.320</a></span> | <span class="t">claimed derivatives. Let me put that here, and let me uncomment three lines and cross our fingers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4834" target="_blank">01:20:34.000</a></span> | <span class="t">Everything is great. Okay, so we now continue almost there. We have the derivative of mcat,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4838" target="_blank">01:20:38.960</a></span> | <span class="t">and we want to backpropagate it into mb. So I again copied this line over here. So this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4847" target="_blank">01:20:47.040</a></span> | <span class="t">forward pass, and then this is the shapes. So remember that the shape here was 32 by 30,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4852" target="_blank">01:20:52.640</a></span> | <span class="t">and the original shape of mb was 32 by 3 by 10. So this layer in the forward pass, as you recall,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4858" target="_blank">01:20:58.400</a></span> | <span class="t">did the concatenation of these three 10-dimensional character vectors. And so now we just want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4865" target="_blank">01:21:05.200</a></span> | <span class="t">undo that. So this is actually a relatively straightforward operation, because the backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4871" target="_blank">01:21:11.120</a></span> | <span class="t">pass of the... What is a view? A view is just a representation of the array. It's just a logical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4876" target="_blank">01:21:16.640</a></span> | <span class="t">form of how you interpret the array. So let's just reinterpret it to be what it was before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4881" target="_blank">01:21:21.760</a></span> | <span class="t">So in other words, dmb is not 32 by 30. It is basically dmbcat, but if you view it as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4891" target="_blank">01:21:31.840</a></span> | <span class="t">the original shape, so just m.shape, you can pass in tuples into view. And so this should just be...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4901" target="_blank">01:21:41.440</a></span> | <span class="t">Okay, we just re-represent that view, and then we uncomment this line here, and hopefully...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4911" target="_blank">01:21:51.040</a></span> | <span class="t">Yeah, so the derivative of m is correct. So in this case, we just have to re-represent the shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4916" target="_blank">01:21:56.880</a></span> | <span class="t">of those derivatives into the original view. So now we are at the final line, and the only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4921" target="_blank">01:22:01.600</a></span> | <span class="t">thing that's left to backpropagate through is this indexing operation here, msc@xb. So as I did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4928" target="_blank">01:22:08.480</a></span> | <span class="t">before, I copy-pasted this line here, and let's look at the shapes of everything that's involved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4932" target="_blank">01:22:12.560</a></span> | <span class="t">and remind ourselves how this worked. So m.shape was 32 by 3 by 10. So it's 32 examples, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4941" target="_blank">01:22:21.680</a></span> | <span class="t">we have three characters. Each one of them has a 10-dimensional embedding, and this was achieved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4947" target="_blank">01:22:27.920</a></span> | <span class="t">by taking the lookup table C, which have 27 possible characters, each of them 10-dimensional,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4954" target="_blank">01:22:34.240</a></span> | <span class="t">and we looked up at the rows that were specified inside this tensor xb. So xb is 32 by 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4962" target="_blank">01:22:42.880</a></span> | <span class="t">and it's basically giving us, for each example, the identity or the index of which character</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4967" target="_blank">01:22:47.760</a></span> | <span class="t">is part of that example. And so here I'm showing the first five rows of this tensor xb.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4976" target="_blank">01:22:56.240</a></span> | <span class="t">And so we can see that, for example, here, it was the first example in this batch is that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4981" target="_blank">01:23:01.520</a></span> | <span class="t">first character, and the first character, and the fourth character comes into the neural net,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4985" target="_blank">01:23:05.440</a></span> | <span class="t">and then we want to predict the next character in a sequence after the character is 1, 1, 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4990" target="_blank">01:23:10.800</a></span> | <span class="t">So basically what's happening here is there are integers inside xb, and each one of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=4998" target="_blank">01:23:18.160</a></span> | <span class="t">integers is specifying which row of C we want to pluck out, right? And then we arrange those rows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5005" target="_blank">01:23:25.360</a></span> | <span class="t">that we've plucked out into 32 by 3 by 10 tensor, and we just package them into this tensor. And now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5013" target="_blank">01:23:33.920</a></span> | <span class="t">what's happening is that we have D_amp. So for every one of these basically plucked out rows,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5020" target="_blank">01:23:40.320</a></span> | <span class="t">we have their gradients now, but they're arranged inside this 32 by 3 by 10 tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5025" target="_blank">01:23:45.920</a></span> | <span class="t">So all we have to do now is we just need to route this gradient backwards through this assignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5031" target="_blank">01:23:51.600</a></span> | <span class="t">So we need to find which row of C did every one of these 10-dimensional embeddings come from,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5038" target="_blank">01:23:58.080</a></span> | <span class="t">and then we need to deposit them into D_c. So we just need to undo the indexing, and of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5046" target="_blank">01:24:06.240</a></span> | <span class="t">if any of these rows of C was used multiple times, which almost certainly is the case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5050" target="_blank">01:24:10.880</a></span> | <span class="t">like the row 1 and 1 was used multiple times, then we have to remember that the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5055" target="_blank">01:24:15.120</a></span> | <span class="t">that arrive there have to add. So for each occurrence, we have to have an addition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5061" target="_blank">01:24:21.200</a></span> | <span class="t">So let's now write this out. And I don't actually know of a much better way to do this than a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5065" target="_blank">01:24:25.360</a></span> | <span class="t">for loop, unfortunately, in Python. So maybe someone can come up with a vectorized efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5070" target="_blank">01:24:30.960</a></span> | <span class="t">operation, but for now, let's just use for loops. So let me create a torch.zeros_like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5076" target="_blank">01:24:36.080</a></span> | <span class="t">C to initialize just a 27 by 10 tensor of all zeros. And then honestly, for k in range,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5086" target="_blank">01:24:46.080</a></span> | <span class="t">xb.shape at 0. Maybe someone has a better way to do this, but for j in range, xb.shape at 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5094" target="_blank">01:24:54.320</a></span> | <span class="t">this is going to iterate over all the elements of xb, all these integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5102" target="_blank">01:25:02.000</a></span> | <span class="t">And then let's get the index at this position. So the index is basically xb at k, j.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5111" target="_blank">01:25:11.520</a></span> | <span class="t">So an example of that is 11 or 14 and so on. And now in a forward pass, we basically took</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5120" target="_blank">01:25:20.800</a></span> | <span class="t">the row of C at index, and we deposited it into emb at k, j. That's what happened. That's where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5132" target="_blank">01:25:32.000</a></span> | <span class="t">they are packaged. So now we need to go backwards, and we just need to route d_emb at the position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5138" target="_blank">01:25:38.400</a></span> | <span class="t">k, j. We now have these derivatives for each position, and it's 10-dimensional. And you just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5146" target="_blank">01:25:46.160</a></span> | <span class="t">need to go into the correct row of C. So d_C, rather, at i, x is this, but plus equals, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5155" target="_blank">01:25:55.920</a></span> | <span class="t">there could be multiple occurrences. Like the same row could have been used many, many times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5159" target="_blank">01:25:59.760</a></span> | <span class="t">And so all of those derivatives will just go backwards through the indexing, and they will add.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5167" target="_blank">01:26:07.840</a></span> | <span class="t">So this is my candidate solution. Let's copy it here. Let's uncomment this and cross our fingers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5179" target="_blank">01:26:19.520</a></span> | <span class="t">Yay! So that's it. We've backpropagated through this entire beast. So there we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5189" target="_blank">01:26:29.360</a></span> | <span class="t">Totally made sense. So now we come to exercise two. It basically turns out that in this first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5194" target="_blank">01:26:34.800</a></span> | <span class="t">exercise, we were doing way too much work. We were backpropagating way too much. And it was all good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5199" target="_blank">01:26:39.600</a></span> | <span class="t">practice and so on, but it's not what you would do in practice. And the reason for that is, for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5204" target="_blank">01:26:44.080</a></span> | <span class="t">example, here I separated out this loss calculation over multiple lines, and I broke it up all to its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5211" target="_blank">01:26:51.040</a></span> | <span class="t">smallest atomic pieces, and we backpropagated through all of those individually. But it turns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5215" target="_blank">01:26:55.600</a></span> | <span class="t">out that if you just look at the mathematical expression for the loss, then actually you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5221" target="_blank">01:27:01.440</a></span> | <span class="t">do the differentiation on pen and paper, and a lot of terms cancel and simplify. And the mathematical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5226" target="_blank">01:27:06.560</a></span> | <span class="t">expression you end up with can be significantly shorter and easier to implement than backpropagating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5231" target="_blank">01:27:11.440</a></span> | <span class="t">through all the little pieces of everything you've done. So before we had this complicated forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5236" target="_blank">01:27:16.240</a></span> | <span class="t">pass going from logits to the loss. But in PyTorch, everything can just be glued together into a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5241" target="_blank">01:27:21.920</a></span> | <span class="t">single call, f.crossentropy. You just pass in logits and the labels, and you get the exact same loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5247" target="_blank">01:27:27.120</a></span> | <span class="t">as I verify here. So our previous loss and the fast loss coming from the chunk of operations as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5252" target="_blank">01:27:32.880</a></span> | <span class="t">a single mathematical expression is the same, but it's much, much faster in a forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5257" target="_blank">01:27:37.840</a></span> | <span class="t">It's also much, much faster in backward pass. And the reason for that is, if you just look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5262" target="_blank">01:27:42.480</a></span> | <span class="t">the mathematical form of this and differentiate again, you will end up with a very small and short</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5266" target="_blank">01:27:46.800</a></span> | <span class="t">expression. So that's what we want to do here. We want to, in a single operation or in a single go,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5272" target="_blank">01:27:52.480</a></span> | <span class="t">or like very quickly, go directly into dlogits. And we need to implement dlogits as a function of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5279" target="_blank">01:27:59.600</a></span> | <span class="t">logits and ybs. But it will be significantly shorter than whatever we did here, where to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5286" target="_blank">01:28:06.640</a></span> | <span class="t">to dlogits, we had to go all the way here. So all of this work can be skipped in a much, much simpler</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5293" target="_blank">01:28:13.040</a></span> | <span class="t">mathematical expression that you can implement here. So you can give it a shot yourself. Basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5299" target="_blank">01:28:19.440</a></span> | <span class="t">look at what exactly is the mathematical expression of loss and differentiate with respect to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5304" target="_blank">01:28:24.480</a></span> | <span class="t">logits. So let me show you a hint. You can, of course, try it for yourself. But if not, I can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5312" target="_blank">01:28:32.000</a></span> | <span class="t">give you some hint of how to get started mathematically. So basically, what's happening</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5317" target="_blank">01:28:37.520</a></span> | <span class="t">here is we have logits. Then there's a softmax that takes the logits and gives you probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5322" target="_blank">01:28:42.480</a></span> | <span class="t">Then we are using the identity of the correct next character to pluck out a row of probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5329" target="_blank">01:28:49.600</a></span> | <span class="t">Take the negative log of it to get our negative log probability. And then we average up all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5334" target="_blank">01:28:54.880</a></span> | <span class="t">log probabilities or negative log probabilities to get our loss. So basically, what we have is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5340" target="_blank">01:29:00.960</a></span> | <span class="t">for a single individual example, rather, we have that loss is equal to negative log probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5346" target="_blank">01:29:06.560</a></span> | <span class="t">where p here is kind of like thought of as a vector of all the probabilities. So at the yth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5353" target="_blank">01:29:13.200</a></span> | <span class="t">position, where y is the label, and we have that p here, of course, is the softmax. So the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5361" target="_blank">01:29:21.200</a></span> | <span class="t">ith component of p, of this probability vector, is just the softmax function. So raising all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5368" target="_blank">01:29:28.080</a></span> | <span class="t">logits basically to the power of e and normalizing so everything sums to one. Now, if you write out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5376" target="_blank">01:29:36.480</a></span> | <span class="t">p of y here, you can just write out the softmax. And then basically what we're interested in is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5381" target="_blank">01:29:41.120</a></span> | <span class="t">we're interested in the derivative of the loss with respect to the ith logit. And so basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5388" target="_blank">01:29:48.800</a></span> | <span class="t">it's a d by d li of this expression here, where we have l indexed with the specific label y,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5395" target="_blank">01:29:55.680</a></span> | <span class="t">and on the bottom, we have a sum over j of e to the lj and the negative log of all that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5399" target="_blank">01:29:59.680</a></span> | <span class="t">So potentially, give it a shot, pen and paper, and see if you can actually derive the expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5404" target="_blank">01:30:04.720</a></span> | <span class="t">for the loss by d li. And then we're going to implement it here. Okay, so I'm going to give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5409" target="_blank">01:30:09.920</a></span> | <span class="t">away the result here. So this is some of the math I did to derive the gradients analytically. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5417" target="_blank">01:30:17.120</a></span> | <span class="t">we see here that I'm just applying the rules of calculus from your first or second year of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5420" target="_blank">01:30:20.800</a></span> | <span class="t">bachelor's degree, if you took it. And we see that the expressions actually simplify quite a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5425" target="_blank">01:30:25.920</a></span> | <span class="t">You have to separate out the analysis in the case where the ith index that you're interested in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5430" target="_blank">01:30:30.480</a></span> | <span class="t">inside logits is either equal to the label or it's not equal to the label. And then the expressions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5435" target="_blank">01:30:35.600</a></span> | <span class="t">simplify and cancel in a slightly different way. And what we end up with is something very,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5439" target="_blank">01:30:39.920</a></span> | <span class="t">very simple. We either end up with basically p at i, where p is again this vector of probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5447" target="_blank">01:30:47.120</a></span> | <span class="t">after a softmax, or p at i minus one, where we just simply subtract to one. But in any case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5452" target="_blank">01:30:52.800</a></span> | <span class="t">we just need to calculate the softmax p, and then in the correct dimension, we need to subtract to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5458" target="_blank">01:30:58.400</a></span> | <span class="t">one. And that's the gradient, the form that it takes analytically. So let's implement this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5463" target="_blank">01:31:03.280</a></span> | <span class="t">basically. And we have to keep in mind that this is only done for a single example. But here we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5467" target="_blank">01:31:07.680</a></span> | <span class="t">are working with batches of examples. So we have to be careful of that. And then the loss for a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5473" target="_blank">01:31:13.200</a></span> | <span class="t">batch is the average loss over all the examples. So in other words, is the example for all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5478" target="_blank">01:31:18.160</a></span> | <span class="t">individual examples, is the loss for each individual example summed up and then divided by n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5483" target="_blank">01:31:23.840</a></span> | <span class="t">And we have to back propagate through that as well and be careful with it. So d logits is going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5489" target="_blank">01:31:29.760</a></span> | <span class="t">f dot softmax. PyTorch has a softmax function that you can call. And we want to apply the softmax on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5497" target="_blank">01:31:37.120</a></span> | <span class="t">the logits. And we want to go in the dimension that is one. So basically, we want to do the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5503" target="_blank">01:31:43.440</a></span> | <span class="t">softmax along the rows of these logits. Then at the correct positions, we need to subtract a one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5509" target="_blank">01:31:49.840</a></span> | <span class="t">So d logits at iterating over all the rows and indexing into the columns provided by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5518" target="_blank">01:31:58.320</a></span> | <span class="t">correct labels inside yb, we need to subtract one. And then finally, it's the average loss that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5525" target="_blank">01:32:05.520</a></span> | <span class="t">the loss. And in the average, there's a one over n of all the losses added up. And so we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5531" target="_blank">01:32:11.440</a></span> | <span class="t">also back propagate through that division. So the gradient has to be scaled down by n as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5537" target="_blank">01:32:17.440</a></span> | <span class="t">because of the mean. But this otherwise should be the result. So now if we verify this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5544" target="_blank">01:32:24.720</a></span> | <span class="t">we see that we don't get an exact match. But at the same time, the maximum difference from logits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5551" target="_blank">01:32:31.600</a></span> | <span class="t">from PyTorch and rd logits here is on the order of 5e negative 9. So it's a tiny, tiny number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5558" target="_blank">01:32:38.960</a></span> | <span class="t">So because of floating point wonkiness, we don't get the exact bitwise result,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5564" target="_blank">01:32:44.320</a></span> | <span class="t">but we basically get the correct answer approximately. Now I'd like to pause here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5570" target="_blank">01:32:50.400</a></span> | <span class="t">briefly before we move on to the next exercise, because I'd like us to get an intuitive sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5574" target="_blank">01:32:54.480</a></span> | <span class="t">of what d logits is, because it has a beautiful and very simple explanation, honestly. So here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5581" target="_blank">01:33:01.200</a></span> | <span class="t">I'm taking d logits, and I'm visualizing it. And we can see that we have a batch of 32 examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5586" target="_blank">01:33:06.480</a></span> | <span class="t">of 27 characters. And what is d logits intuitively, right? d logits is the probabilities that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5593" target="_blank">01:33:13.600</a></span> | <span class="t">probabilities matrix in the forward pass. But then here, these black squares are the positions of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5598" target="_blank">01:33:18.240</a></span> | <span class="t">correct indices, where we subtracted a 1. And so what is this doing, right? These are the derivatives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5605" target="_blank">01:33:25.280</a></span> | <span class="t">on d logits. And so let's look at just the first row here. So that's what I'm doing here. I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5613" target="_blank">01:33:33.040</a></span> | <span class="t">calculating the probabilities of these logits, and then I'm taking just the first row. And this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5617" target="_blank">01:33:37.600</a></span> | <span class="t">the probability row. And then d logits of the first row, and multiplying by n just for us so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5624" target="_blank">01:33:44.320</a></span> | <span class="t">we don't have the scaling by n in here, and everything is more interpretable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5627" target="_blank">01:33:47.440</a></span> | <span class="t">We see that it's exactly equal to the probability, of course, but then the position of the correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5632" target="_blank">01:33:52.800</a></span> | <span class="t">index has a minus equals 1. So minus 1 on that position. And so notice that if you take d logits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5640" target="_blank">01:34:00.160</a></span> | <span class="t">at 0, and you sum it, it actually sums to 0. And so you should think of these gradients here at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5649" target="_blank">01:34:09.120</a></span> | <span class="t">each cell as like a force. We are going to be basically pulling down on the probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5657" target="_blank">01:34:17.280</a></span> | <span class="t">of the incorrect characters, and we're going to be pulling up on the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5661" target="_blank">01:34:21.120</a></span> | <span class="t">at the correct index. And that's what's basically happening in each row. And the amount of push and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5670" target="_blank">01:34:30.160</a></span> | <span class="t">pull is exactly equalized, because the sum is 0. So the amount to which we pull down on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5675" target="_blank">01:34:35.840</a></span> | <span class="t">probabilities, and the amount that we push up on the probability of the correct character is equal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5680" target="_blank">01:34:40.480</a></span> | <span class="t">So the repulsion and the attraction are equal. And think of the neural net now as a massive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5687" target="_blank">01:34:47.200</a></span> | <span class="t">pulley system or something like that. We're up here on top of d logits, and we're pulling up,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5692" target="_blank">01:34:52.800</a></span> | <span class="t">we're pulling down the probabilities of incorrect and pulling up the probability of the correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5696" target="_blank">01:34:56.480</a></span> | <span class="t">And in this complicated pulley system, because everything is mathematically just determined,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5701" target="_blank">01:35:01.920</a></span> | <span class="t">just think of it as sort of like this tension translating to this complicating pulley mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5706" target="_blank">01:35:06.720</a></span> | <span class="t">And then eventually we get a tug on the weights and the biases. And basically in each update,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5711" target="_blank">01:35:11.600</a></span> | <span class="t">we just kind of like tug in the direction that we like for each of these elements,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5715" target="_blank">01:35:15.600</a></span> | <span class="t">and the parameters are slowly given in to the tug. And that's what training a neural net</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5720" target="_blank">01:35:20.000</a></span> | <span class="t">kind of like looks like on a high level. And so I think the forces of push and pull in these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5725" target="_blank">01:35:25.280</a></span> | <span class="t">gradients are actually very intuitive here. We're pushing and pulling on the correct answer and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5730" target="_blank">01:35:30.720</a></span> | <span class="t">incorrect answers. And the amount of force that we're applying is actually proportional to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5736" target="_blank">01:35:36.400</a></span> | <span class="t">probabilities that came out in the forward pass. And so for example, if our probabilities came out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5741" target="_blank">01:35:41.520</a></span> | <span class="t">exactly correct, so they would have had zero everywhere except for one at the correct position,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5747" target="_blank">01:35:47.680</a></span> | <span class="t">then the d logits would be all a row of zeros for that example. There would be no push and pull.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5753" target="_blank">01:35:53.440</a></span> | <span class="t">So the amount to which your prediction is incorrect is exactly the amount by which you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5758" target="_blank">01:35:58.640</a></span> | <span class="t">going to get a pull or a push in that dimension. So if you have, for example, a very confidently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5764" target="_blank">01:36:04.240</a></span> | <span class="t">mispredicted element here, then what's going to happen is that element is going to be pulled down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5770" target="_blank">01:36:10.080</a></span> | <span class="t">very heavily, and the correct answer is going to be pulled up to the same amount. And the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5775" target="_blank">01:36:15.600</a></span> | <span class="t">characters are not going to be influenced too much. So the amount to which you mispredict is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5781" target="_blank">01:36:21.040</a></span> | <span class="t">then proportional to the strength of the pull. And that's happening independently in all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5786" target="_blank">01:36:26.240</a></span> | <span class="t">dimensions of this tensor. And it's sort of very intuitive and very easy to think through. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5791" target="_blank">01:36:31.440</a></span> | <span class="t">that's basically the magic of the cross-entropy loss and what it's doing dynamically in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5795" target="_blank">01:36:35.920</a></span> | <span class="t">backward pass of the neural net. So now we get to exercise number three, which is a very fun exercise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5801" target="_blank">01:36:41.600</a></span> | <span class="t">depending on your definition of fun. And we are going to do for batch normalization exactly what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5806" target="_blank">01:36:46.560</a></span> | <span class="t">we did for cross-entropy loss in exercise number two. That is, we are going to consider it as a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5811" target="_blank">01:36:51.280</a></span> | <span class="t">glued single mathematical expression and backpropagate through it in a very efficient manner,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5816" target="_blank">01:36:56.000</a></span> | <span class="t">because we are going to derive a much simpler formula for the backward pass of batch normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5820" target="_blank">01:37:00.320</a></span> | <span class="t">And we're going to do that using pen and paper. So previously, we've broken up batch normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5825" target="_blank">01:37:05.760</a></span> | <span class="t">into all of the little intermediate pieces and all the atomic operations inside it, and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5829" target="_blank">01:37:09.920</a></span> | <span class="t">backpropagate it through it one by one. Now we just have a single sort of forward pass of a batch norm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5837" target="_blank">01:37:17.760</a></span> | <span class="t">and it's all glued together, and we see that we get the exact same result as before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5842" target="_blank">01:37:22.320</a></span> | <span class="t">Now for the backward pass, we'd like to also implement a single formula basically for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5847" target="_blank">01:37:27.920</a></span> | <span class="t">backpropagating through this entire operation, that is the batch normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5851" target="_blank">01:37:31.360</a></span> | <span class="t">So in the forward pass previously, we took HPBN, the hidden states of the pre-batch normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5858" target="_blank">01:37:38.720</a></span> | <span class="t">and created HPREACT, which is the hidden states just before the activation. In the batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5864" target="_blank">01:37:44.320</a></span> | <span class="t">normalization paper, HPREBN is x and HPREACT is y. So in the backward pass, what we'd like to do now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5871" target="_blank">01:37:51.280</a></span> | <span class="t">is we have DHPREACT, and we'd like to produce DHPREBN, and we'd like to do that in a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5877" target="_blank">01:37:57.760</a></span> | <span class="t">efficient manner. So that's the name of the game, calculate DHPREBN given DHPREACT. And for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5884" target="_blank">01:38:04.320</a></span> | <span class="t">purposes of this exercise, we're going to ignore gamma and beta and their derivatives, because they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5889" target="_blank">01:38:09.440</a></span> | <span class="t">take on a very simple form in a very similar way to what we did up above. So let's calculate this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5895" target="_blank">01:38:15.920</a></span> | <span class="t">given that right here. So to help you a little bit like I did before, I started off the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5902" target="_blank">01:38:22.800</a></span> | <span class="t">implementation here on pen and paper, and I took two sheets of paper to derive the mathematical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5908" target="_blank">01:38:28.240</a></span> | <span class="t">formulas for the backward pass. And basically to set up the problem, just write out the mu,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5914" target="_blank">01:38:34.640</a></span> | <span class="t">sigma square, variance, xi hat, and yi, exactly as in the paper, except for the Bessel correction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5920" target="_blank">01:38:40.880</a></span> | <span class="t">And then in the backward pass, we have the derivative of the loss with respect to all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5926" target="_blank">01:38:46.160</a></span> | <span class="t">elements of y. And remember that y is a vector. There's multiple numbers here. So we have all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5933" target="_blank">01:38:53.520</a></span> | <span class="t">of the derivatives with respect to all the y's. And then there's a gamma and a beta, and this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5939" target="_blank">01:38:59.200</a></span> | <span class="t">kind of like the compute graph. The gamma and the beta, there's the x hat, and then the mu and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5944" target="_blank">01:39:04.480</a></span> | <span class="t">sigma squared, and the x. So we have dl by dyi, and we want dl by dxi for all the i's in these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5953" target="_blank">01:39:13.360</a></span> | <span class="t">vectors. So this is the compute graph, and you have to be careful because I'm trying to note here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5960" target="_blank">01:39:20.320</a></span> | <span class="t">that these are vectors. There's many nodes here inside x, x hat, and y, but mu and sigma, sorry,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5968" target="_blank">01:39:28.480</a></span> | <span class="t">sigma square are just individual scalars, single numbers. So you have to be careful with that. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5973" target="_blank">01:39:33.600</a></span> | <span class="t">have to imagine there's multiple nodes here, or you're going to get your math wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5976" target="_blank">01:39:36.400</a></span> | <span class="t">So as an example, I would suggest that you go in the following order, one, two, three, four,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5983" target="_blank">01:39:43.920</a></span> | <span class="t">in terms of the backpropagation. So backpropagate into x hat, then into sigma square, then into mu,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5989" target="_blank">01:39:49.280</a></span> | <span class="t">and then into x. Just like in a topological sort in micrograd, we would go from right to left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=5996" target="_blank">01:39:56.080</a></span> | <span class="t">You're doing the exact same thing, except you're doing it with symbols and on a piece of paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6000" target="_blank">01:40:00.240</a></span> | <span class="t">So for number one, I'm not giving away too much. If you want dl of dxi hat, then we just take dl</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6011" target="_blank">01:40:11.200</a></span> | <span class="t">by dyi and multiply it by gamma, because of this expression here, where any individual yi is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6017" target="_blank">01:40:17.440</a></span> | <span class="t">gamma times x i hat plus beta. So it didn't help you too much there, but this gives you basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6024" target="_blank">01:40:24.320</a></span> | <span class="t">the derivatives for all the x hats. And so now, try to go through this computational graph and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6031" target="_blank">01:40:31.360</a></span> | <span class="t">derive what is dl by d sigma square, and then what is dl by d mu, and then what is dl by dx,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6038" target="_blank">01:40:38.960</a></span> | <span class="t">eventually. So give it a go, and I'm going to be revealing the answer one piece at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6044" target="_blank">01:40:44.080</a></span> | <span class="t">Okay, so to get dl by d sigma square, we have to remember again, like I mentioned, that there are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6049" target="_blank">01:40:49.440</a></span> | <span class="t">many x hats here. And remember that sigma square is just a single individual number here. So when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6057" target="_blank">01:40:57.120</a></span> | <span class="t">we look at the expression for dl by d sigma square, we have to actually consider all the possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6064" target="_blank">01:41:04.400</a></span> | <span class="t">paths that we basically have that there's many x hats, and they all depend on sigma square.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6073" target="_blank">01:41:13.920</a></span> | <span class="t">So sigma square has a large fan out. There's lots of arrows coming out from sigma square into all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6079" target="_blank">01:41:19.040</a></span> | <span class="t">the x hats. And then there's a backpropagating signal from each x hat into sigma square. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6085" target="_blank">01:41:25.360</a></span> | <span class="t">that's why we actually need to sum over all those i's from i equal to one to m of the dl by d xi hat,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6094" target="_blank">01:41:34.720</a></span> | <span class="t">which is the global gradient, times the xi hat by d sigma square, which is the local gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6101" target="_blank">01:41:41.280</a></span> | <span class="t">of this operation here. And then mathematically, I'm just working it out here, and I'm simplifying,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6107" target="_blank">01:41:47.920</a></span> | <span class="t">and you get a certain expression for dl by d sigma square. And we're going to be using this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6112" target="_blank">01:41:52.560</a></span> | <span class="t">expression when we backpropagate into mu, and then eventually into x. So now let's continue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6117" target="_blank">01:41:57.200</a></span> | <span class="t">our backpropagation into mu. So what is dl by d mu? Now again, be careful that mu influences x hat,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6124" target="_blank">01:42:04.320</a></span> | <span class="t">and x hat is actually lots of values. So for example, if our mini-batch size is 32, as it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6129" target="_blank">01:42:09.520</a></span> | <span class="t">in our example that we were working on, then this is 32 numbers and 32 arrows going back to mu.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6135" target="_blank">01:42:15.840</a></span> | <span class="t">And then mu going to sigma square is just a single arrow, because sigma square is a scalar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6139" target="_blank">01:42:19.840</a></span> | <span class="t">So in total, there are 33 arrows emanating from mu, and then all of them have gradients coming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6146" target="_blank">01:42:26.160</a></span> | <span class="t">into mu, and they all need to be summed up. And so that's why when we look at the expression for dl</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6152" target="_blank">01:42:32.400</a></span> | <span class="t">by d mu, I am summing up over all the gradients of dl by d xi hat times d xi hat by d mu. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6161" target="_blank">01:42:41.040</a></span> | <span class="t">the that's this arrow, and that's 32 arrows here, and then plus the one arrow from here, which is dl</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6166" target="_blank">01:42:46.720</a></span> | <span class="t">by d sigma square times d sigma square by d mu. So now we have to work out that expression, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6172" target="_blank">01:42:52.880</a></span> | <span class="t">let me just reveal the rest of it. Simplifying here is not complicated, the first term, and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6179" target="_blank">01:42:59.280</a></span> | <span class="t">just get an expression here. For the second term though, there's something really interesting that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6183" target="_blank">01:43:03.120</a></span> | <span class="t">happens. When we look at d sigma square by d mu and we simplify, at one point if we assume that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6191" target="_blank">01:43:11.120</a></span> | <span class="t">in a special case where mu is actually the average of xi's, as it is in this case, then if we plug</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6198" target="_blank">01:43:18.800</a></span> | <span class="t">that in, then actually the gradient vanishes and becomes exactly zero. And that makes the entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6204" target="_blank">01:43:24.480</a></span> | <span class="t">second term cancel. And so these, if you just have a mathematical expression like this, and you look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6210" target="_blank">01:43:30.640</a></span> | <span class="t">at d sigma square by d mu, you would get some mathematical formula for how mu impacts sigma</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6216" target="_blank">01:43:36.720</a></span> | <span class="t">square. But if it is the special case that mu is actually equal to the average, as it is in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6221" target="_blank">01:43:41.920</a></span> | <span class="t">case of batch normalization, that gradient will actually vanish and become zero. So the whole</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6226" target="_blank">01:43:46.800</a></span> | <span class="t">term cancels, and we just get a fairly straightforward expression here for dl by d mu.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6231" target="_blank">01:43:51.600</a></span> | <span class="t">Okay, and now we get to the craziest part, which is deriving dl by d xi, which is ultimately what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6238" target="_blank">01:43:58.000</a></span> | <span class="t">we're after. Now let's count, first of all, how many numbers are there inside x? As I mentioned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6244" target="_blank">01:44:04.320</a></span> | <span class="t">there are 32 numbers. There are 32 little xi's. And let's count the number of arrows emanating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6249" target="_blank">01:44:09.440</a></span> | <span class="t">from each xi. There's an arrow going to mu, an arrow going to sigma square, and then there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6255" target="_blank">01:44:15.280</a></span> | <span class="t">an arrow going to x hat. But this arrow here, let's scrutinize that a little bit. Each xi hat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6261" target="_blank">01:44:21.440</a></span> | <span class="t">is just a function of xi and all the other scalars. So xi hat only depends on xi and none of the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6269" target="_blank">01:44:29.200</a></span> | <span class="t">x's. And so therefore, there are actually, in this single arrow, there are 32 arrows. But those 32</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6275" target="_blank">01:44:35.440</a></span> | <span class="t">arrows are going exactly parallel. They don't interfere. They're just going parallel between x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6280" target="_blank">01:44:40.720</a></span> | <span class="t">and x hat. You can look at it that way. And so how many arrows are emanating from each xi? There</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6285" target="_blank">01:44:45.360</a></span> | <span class="t">are three arrows, mu, sigma square, and the associated x hat. And so in backpropagation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6292" target="_blank">01:44:52.400</a></span> | <span class="t">we now need to apply the chain rule, and we need to add up those three contributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6297" target="_blank">01:44:57.120</a></span> | <span class="t">So here's what that looks like if I just write that out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6300" target="_blank">01:45:00.080</a></span> | <span class="t">We're chaining through mu, sigma square, and through x hat. And those three terms are just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6309" target="_blank">01:45:09.120</a></span> | <span class="t">here. Now, we already have three of these. We have dl by dx i hat. We have dl by d mu,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6316" target="_blank">01:45:16.880</a></span> | <span class="t">which we derived here. And we have dl by d sigma square, which we derived here. But we need three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6322" target="_blank">01:45:22.080</a></span> | <span class="t">other terms here. This one, this one, and this one. So I invite you to try to derive them. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6328" target="_blank">01:45:28.240</a></span> | <span class="t">not that complicated. You're just looking at these expressions here and differentiating with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6332" target="_blank">01:45:32.240</a></span> | <span class="t">to xi. So give it a shot, but here's the result, or at least what I got. I'm just differentiating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6343" target="_blank">01:45:43.920</a></span> | <span class="t">with respect to xi for all of these expressions. And honestly, I don't think there's anything too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6347" target="_blank">01:45:47.520</a></span> | <span class="t">tricky here. It's basic calculus. Now, what gets a little bit more tricky is we are now going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6352" target="_blank">01:45:52.960</a></span> | <span class="t">plug everything together. So all of these terms multiplied with all of these terms and add it up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6357" target="_blank">01:45:57.680</a></span> | <span class="t">according to this formula. And that gets a little bit hairy. So what ends up happening is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6362" target="_blank">01:46:02.240</a></span> | <span class="t">you get a large expression. And the thing to be very careful with here, of course, is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6370" target="_blank">01:46:10.320</a></span> | <span class="t">we are working with a dl by d xi for a specific i here. But when we are plugging in some of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6376" target="_blank">01:46:16.080</a></span> | <span class="t">terms, like say this term here, dl by d sigma squared, you see how dl by d sigma squared,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6384" target="_blank">01:46:24.400</a></span> | <span class="t">I end up with an expression. And I'm iterating over little i's here. But I can't use i as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6390" target="_blank">01:46:30.640</a></span> | <span class="t">variable when I plug in here, because this is a different i from this i. This i here is just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6396" target="_blank">01:46:36.240</a></span> | <span class="t">placeholder, like a local variable for a for loop in here. So here, when I plug that in, you notice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6401" target="_blank">01:46:41.840</a></span> | <span class="t">that I renamed the i to a j, because I need to make sure that this j is not this i. This j is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6408" target="_blank">01:46:48.640</a></span> | <span class="t">like a little local iterator over 32 terms. And so you have to be careful with that when you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6413" target="_blank">01:46:53.920</a></span> | <span class="t">plugging in the expressions from here to here. You may have to rename i's into j's. And you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6418" target="_blank">01:46:58.240</a></span> | <span class="t">to be very careful what is actually an i with respect to dl by d xi. So some of these are j's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6425" target="_blank">01:47:05.680</a></span> | <span class="t">some of these are i's. And then we simplify this expression. And I guess the big thing to notice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6433" target="_blank">01:47:13.520</a></span> | <span class="t">here is a bunch of terms just kind of come out to the front, and you can refactor them. There's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6437" target="_blank">01:47:17.840</a></span> | <span class="t">a sigma squared plus epsilon raised to the power of negative 3 over 2. This sigma squared plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6442" target="_blank">01:47:22.080</a></span> | <span class="t">epsilon can be actually separated out into three terms. Each of them are sigma squared plus epsilon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6448" target="_blank">01:47:28.000</a></span> | <span class="t">to the negative 1 over 2. So the three of them multiplied is equal to this. And then those three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6454" target="_blank">01:47:34.000</a></span> | <span class="t">terms can go different places because of the multiplication. So one of them actually comes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6458" target="_blank">01:47:38.480</a></span> | <span class="t">out to the front and will end up here outside. One of them joins up with this term, and one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6465" target="_blank">01:47:45.200</a></span> | <span class="t">them joins up with this other term. And then when you simplify the expression, you'll notice that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6470" target="_blank">01:47:50.400</a></span> | <span class="t">some of these terms that are coming out are just the xi hats. So you can simplify just by rewriting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6475" target="_blank">01:47:55.920</a></span> | <span class="t">that. And what we end up with at the end is a fairly simple mathematical expression over here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6480" target="_blank">01:48:00.800</a></span> | <span class="t">that I cannot simplify further. But basically, you'll notice that it only uses the stuff we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6486" target="_blank">01:48:06.080</a></span> | <span class="t">and it derives the thing we need. So we have dl by dy for all the i's, and those are used plenty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6493" target="_blank">01:48:13.680</a></span> | <span class="t">of times here. And also in addition, what we're using is these xi hats and xj hats, and they just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6498" target="_blank">01:48:18.720</a></span> | <span class="t">come from the forward pass. And otherwise, this is a simple expression, and it gives us dl by dxi</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6505" target="_blank">01:48:25.520</a></span> | <span class="t">for all the i's, and that's ultimately what we're interested in. So that's the end of BatchNorm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6511" target="_blank">01:48:31.440</a></span> | <span class="t">backward pass analytically. Let's now implement this final result. Okay, so I implemented the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6517" target="_blank">01:48:37.680</a></span> | <span class="t">expression into a single line of code here, and you can see that the max diff is tiny,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6523" target="_blank">01:48:43.120</a></span> | <span class="t">so this is the correct implementation of this formula. Now, I'll just basically tell you that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6529" target="_blank">01:48:49.760</a></span> | <span class="t">getting this formula here from this mathematical expression was not trivial, and there's a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6534" target="_blank">01:48:54.480</a></span> | <span class="t">going on packed into this one formula. And this is a whole exercise by itself, because you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6540" target="_blank">01:49:00.000</a></span> | <span class="t">to consider the fact that this formula here is just for a single neuron and a batch of 32 examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6545" target="_blank">01:49:05.440</a></span> | <span class="t">But what I'm doing here is we actually have 64 neurons, and so this expression has to in parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6551" target="_blank">01:49:11.920</a></span> | <span class="t">evaluate the BatchNorm backward pass for all of those 64 neurons in parallel and independently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6556" target="_blank">01:49:16.720</a></span> | <span class="t">So this has to happen basically in every single column of the inputs here. And in addition to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6565" target="_blank">01:49:25.200</a></span> | <span class="t">that, you see how there are a bunch of sums here, and we need to make sure that when I do those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6569" target="_blank">01:49:29.280</a></span> | <span class="t">sums that they broadcast correctly onto everything else that's here. And so getting this expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6574" target="_blank">01:49:34.800</a></span> | <span class="t">is just like highly non-trivial, and I invite you to basically look through it and step through it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6578" target="_blank">01:49:38.080</a></span> | <span class="t">and it's a whole exercise to make sure that this checks out. But once all the shapes agree,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6584" target="_blank">01:49:44.800</a></span> | <span class="t">and once you convince yourself that it's correct, you can also verify that PyTorch gets the exact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6588" target="_blank">01:49:48.800</a></span> | <span class="t">same answer as well. And so that gives you a lot of peace of mind that this mathematical formula</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6593" target="_blank">01:49:53.520</a></span> | <span class="t">is correctly implemented here and broadcasted correctly and replicated in parallel for all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6598" target="_blank">01:49:58.720</a></span> | <span class="t">of the 64 neurons inside this BatchNorm layer. Okay, and finally, exercise number four asks you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6605" target="_blank">01:50:05.280</a></span> | <span class="t">to put it all together. And here we have a redefinition of the entire problem. So you see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6610" target="_blank">01:50:10.320</a></span> | <span class="t">that we re-initialized the neural net from scratch and everything. And then here, instead of calling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6615" target="_blank">01:50:15.280</a></span> | <span class="t">the loss that backward, we want to have the manual backpropagation here as we derived it up above.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6620" target="_blank">01:50:20.160</a></span> | <span class="t">So go up, copy-paste all the chunks of code that we've already derived, put them here, and derive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6626" target="_blank">01:50:26.000</a></span> | <span class="t">your own gradients, and then optimize this neural net basically using your own gradients all the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6631" target="_blank">01:50:31.200</a></span> | <span class="t">to the calibration of the BatchNorm and the evaluation of the loss. And I was able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6635" target="_blank">01:50:35.840</a></span> | <span class="t">achieve quite a good loss, basically the same loss you would achieve before. And that shouldn't be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6640" target="_blank">01:50:40.240</a></span> | <span class="t">surprising because all we've done is we've really gotten into loss that backward, and we've pulled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6645" target="_blank">01:50:45.040</a></span> | <span class="t">out all the code and inserted it here. But those gradients are identical, and everything is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6650" target="_blank">01:50:50.160</a></span> | <span class="t">identical, and the results are identical. It's just that we have full visibility on exactly what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6655" target="_blank">01:50:55.040</a></span> | <span class="t">goes on under the hood of loss that backward in this specific case. Okay, and this is all of our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6661" target="_blank">01:51:01.200</a></span> | <span class="t">code. This is the full backward pass using basically the simplified backward pass for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6665" target="_blank">01:51:05.760</a></span> | <span class="t">the cross-entropy loss and the BatchNormalization. So backpropagating through cross-entropy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6671" target="_blank">01:51:11.120</a></span> | <span class="t">the second layer, the 10-H nonlinearity, the BatchNormalization through the first layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6678" target="_blank">01:51:18.480</a></span> | <span class="t">and through the embedding. And so you see that this is only maybe, what is this, 20 lines of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6682" target="_blank">01:51:22.880</a></span> | <span class="t">code or something like that? And that's what gives us gradients. And now we can potentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6687" target="_blank">01:51:27.600</a></span> | <span class="t">erase loss that backward. So the way I have the code set up is you should be able to run this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6692" target="_blank">01:51:32.560</a></span> | <span class="t">entire cell once you fill this in, and this will run for only 100 iterations and then break.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6697" target="_blank">01:51:37.280</a></span> | <span class="t">And it breaks because it gives you an opportunity to check your gradients against PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6701" target="_blank">01:51:41.760</a></span> | <span class="t">So here, our gradients we see are not exactly equal. They are approximately equal, and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6709" target="_blank">01:51:49.600</a></span> | <span class="t">differences are tiny, 1 and negative 9 or so. And I don't exactly know where they're coming from,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6714" target="_blank">01:51:54.240</a></span> | <span class="t">to be honest. So once we have some confidence that the gradients are basically correct,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6718" target="_blank">01:51:58.320</a></span> | <span class="t">we can take out the gradient checking. We can disable this breaking statement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6724" target="_blank">01:52:04.400</a></span> | <span class="t">And then we can basically disable loss that backward. We don't need it anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6731" target="_blank">01:52:11.200</a></span> | <span class="t">Feels amazing to say that. And then here, when we are doing the update, we're not going to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6737" target="_blank">01:52:17.520</a></span> | <span class="t">p.grad. This is the old way of PyTorch. We don't have that anymore because we're not doing backward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6743" target="_blank">01:52:23.360</a></span> | <span class="t">We are going to use this update where we, you see that I'm iterating over, I've arranged the grads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6750" target="_blank">01:52:30.080</a></span> | <span class="t">to be in the same order as the parameters, and I'm zipping them up, the gradients and the parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6755" target="_blank">01:52:35.120</a></span> | <span class="t">into p and grad. And then here, I'm going to step with just the grad that we derived manually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6760" target="_blank">01:52:40.880</a></span> | <span class="t">So the last piece is that none of this now requires gradients from PyTorch. And so one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6769" target="_blank">01:52:49.280</a></span> | <span class="t">thing you can do here is you can do withTorch.noGrad and offset this whole code block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6777" target="_blank">01:52:57.200</a></span> | <span class="t">And really what you're saying is you're telling PyTorch that, "Hey, I'm not going to call backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6781" target="_blank">01:53:01.200</a></span> | <span class="t">on any of this." And this allows PyTorch to be a bit more efficient with all of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6785" target="_blank">01:53:05.520</a></span> | <span class="t">And then we should be able to just run this. And it's running. And you see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6795" target="_blank">01:53:15.680</a></span> | <span class="t">lost at backward is commented out and we're optimizing. So we're going to leave this run,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6802" target="_blank">01:53:22.560</a></span> | <span class="t">and hopefully we get a good result. Okay, so I allowed the neural net to finish optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6807" target="_blank">01:53:27.920</a></span> | <span class="t">Then here, I calibrate the bastion parameters because I did not keep track of the running mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6815" target="_blank">01:53:35.040</a></span> | <span class="t">variance in their training loop. Then here, I ran the loss. And you see that we actually obtained a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6820" target="_blank">01:53:40.400</a></span> | <span class="t">pretty good loss, very similar to what we've achieved before. And then here, I'm sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6824" target="_blank">01:53:44.880</a></span> | <span class="t">from the model. And we see some of the name-like gibberish that we're sort of used to. So basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6829" target="_blank">01:53:49.760</a></span> | <span class="t">the model worked and samples pretty decent results compared to what we were used to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6835" target="_blank">01:53:55.520</a></span> | <span class="t">So everything is the same. But of course, the big deal is that we did not use lots of backward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6840" target="_blank">01:54:00.080</a></span> | <span class="t">We did not use PyTorch autograd, and we estimated our gradients ourselves by hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6844" target="_blank">01:54:04.400</a></span> | <span class="t">And so hopefully, you're looking at this, the backward pass of this neural net,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6848" target="_blank">01:54:08.320</a></span> | <span class="t">and you're thinking to yourself, actually, that's not too complicated. Each one of these layers is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6854" target="_blank">01:54:14.480</a></span> | <span class="t">like three lines of code or something like that. And most of it is fairly straightforward,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6858" target="_blank">01:54:18.960</a></span> | <span class="t">potentially with the notable exception of the batch normalization backward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6862" target="_blank">01:54:22.800</a></span> | <span class="t">Otherwise, it's pretty good. Okay, and that's everything I wanted to cover for this lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6867" target="_blank">01:54:27.440</a></span> | <span class="t">So hopefully, you found this interesting. And what I liked about it, honestly, is that it gave us a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6872" target="_blank">01:54:32.400</a></span> | <span class="t">very nice diversity of layers to backpropagate through. And I think it gives a pretty nice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6878" target="_blank">01:54:38.000</a></span> | <span class="t">and comprehensive sense of how these backward passes are implemented and how they work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6882" target="_blank">01:54:42.240</a></span> | <span class="t">And you'd be able to derive them yourself. But of course, in practice, you probably don't want to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6886" target="_blank">01:54:46.240</a></span> | <span class="t">and you want to use the PyTorch autograd. But hopefully, you have some intuition about how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6890" target="_blank">01:54:50.240</a></span> | <span class="t">gradients flow backwards through the neural net, starting at the loss, and how they flow through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6895" target="_blank">01:54:55.200</a></span> | <span class="t">all the variables and all the intermediate results. And if you understood a good chunk of it, and if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6900" target="_blank">01:55:00.640</a></span> | <span class="t">you have a sense of that, then you can count yourself as one of these buff dojis on the left,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6904" target="_blank">01:55:04.720</a></span> | <span class="t">instead of the dojis on the right here. Now, in the next lecture, we're actually going to go to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6910" target="_blank">01:55:10.400</a></span> | <span class="t">recurrent neural nets, LSTMs, and all the other variants of RNS. And we're going to start to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6916" target="_blank">01:55:16.240</a></span> | <span class="t">complexify the architecture and start to achieve better log likelihoods. And so I'm really looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6921" target="_blank">01:55:21.360</a></span> | <span class="t">forward to that. And I'll see you then.</span></div></div></body></html>