<html><head><title>MIT 6.S091: Introduction to Deep Reinforcement Learning (Deep RL)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>MIT 6.S091: Introduction to Deep Reinforcement Learning (Deep RL)</h2><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M"><img src="https://i.ytimg.com/vi_webp/zR11FLZ-O9M/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=134">2:14</a> Types of learning<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=395">6:35</a> Reinforcement learning in humans<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=502">8:22</a> What can be learned from data?<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=735">12:15</a> Reinforcement learning framework<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=846">14:6</a> Challenge for RL in real-world applications<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=940">15:40</a> Component of an RL agent<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1062">17:42</a> Example: robot in a room<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1385">23:5</a> AI safety and unintended consequences<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1581">26:21</a> Examples of RL systems<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1792">29:52</a> Takeaways for real-world impact<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1885">31:25</a> 3 types of RL: model-based, value-based, policy-based<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2128">35:28</a> Q-learning<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2320">38:40</a> Deep Q-Networks (DQN)<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2880">48:0</a> Policy Gradient (PG)<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3036">50:36</a> Advantage Actor-Critic (A2C & A3C)<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3172">52:52</a> Deep Deterministic Policy Gradient (DDPG)<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3252">54:12</a> Policy Optimization (TRPO and PPO)<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3363">56:3</a> AlphaZero<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3650">60:50</a> Deep RL in real-world applications<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3789">63:9</a> Closing the RL simulation gap<br><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3884">64:44</a> Next step in Deep RL<br><br><div style="text-align: left;"><a href="./zR11FLZ-O9M.html">Whisper Transcript</a> | <a href="./transcript_zR11FLZ-O9M.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Today I'd like to overview the exciting field of deep reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4" target="_blank">00:00:04.640</a></span> | <span class="t">Introduce, overview and provide you some of the basics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=8" target="_blank">00:00:08.240</a></span> | <span class="t">I think it's one of the most exciting fields in artificial intelligence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=14" target="_blank">00:00:14.960</a></span> | <span class="t">It's marrying the power and the ability of deep neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=20" target="_blank">00:00:20.320</a></span> | <span class="t">to represent and comprehend the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=23" target="_blank">00:00:23.120</a></span> | <span class="t">with the ability to act on that understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=29" target="_blank">00:00:29.040</a></span> | <span class="t">on that representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=31" target="_blank">00:00:31.040</a></span> | <span class="t">Taking as a whole, that's really what the creation of intelligent beings is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=36" target="_blank">00:00:36.480</a></span> | <span class="t">Understand the world and act</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=39" target="_blank">00:00:39.360</a></span> | <span class="t">And the exciting breakthroughs that recently have happened</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=42" target="_blank">00:00:42.000</a></span> | <span class="t">Captivate our imagination about what's possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=45" target="_blank">00:00:45.440</a></span> | <span class="t">And that's why this is my favorite area of deep learning and artificial intelligence in general</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=50" target="_blank">00:00:50.400</a></span> | <span class="t">And I hope you feel the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=52" target="_blank">00:00:52.560</a></span> | <span class="t">So what is deep reinforcement learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=55" target="_blank">00:00:55.120</a></span> | <span class="t">We've talked about deep learning which is taking samples of data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=60" target="_blank">00:01:00.080</a></span> | <span class="t">Being able to in a supervised way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=62" target="_blank">00:01:02.080</a></span> | <span class="t">compress, encode the representation of that data in a way that you can reason about it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=67" target="_blank">00:01:07.680</a></span> | <span class="t">And we take that power and apply it to the world where sequential decisions are to be made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=76" target="_blank">00:01:16.000</a></span> | <span class="t">So it's looking at problems and formulations of tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=82" target="_blank">00:01:22.640</a></span> | <span class="t">Where an agent, an intelligent system has to make a sequence of decisions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=88" target="_blank">00:01:28.240</a></span> | <span class="t">And the decisions that are made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=91" target="_blank">00:01:31.200</a></span> | <span class="t">Have an effect on the world around the agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=94" target="_blank">00:01:34.400</a></span> | <span class="t">How?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=96" target="_blank">00:01:36.880</a></span> | <span class="t">How do all of us?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=98" target="_blank">00:01:38.880</a></span> | <span class="t">Any intelligent being that is tasked with operating in the world. How do they learn anything?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=103" target="_blank">00:01:43.680</a></span> | <span class="t">Especially when you know very little in the beginning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=107" target="_blank">00:01:47.280</a></span> | <span class="t">It's trial and error is the fundamental process by which reinforcement learning agents learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=113" target="_blank">00:01:53.280</a></span> | <span class="t">And the deep part of deep reinforcement learning is neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=119" target="_blank">00:01:59.380</a></span> | <span class="t">It's using the frameworks and reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=122" target="_blank">00:02:02.720</a></span> | <span class="t">Where the neural network is doing the representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=127" target="_blank">00:02:07.460</a></span> | <span class="t">Of the world based on which the actions are made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=131" target="_blank">00:02:11.520</a></span> | <span class="t">And we have to take a step back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=135" target="_blank">00:02:15.760</a></span> | <span class="t">When we look at the types of learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=137" target="_blank">00:02:17.760</a></span> | <span class="t">Sometimes the terminology itself can confuse us to the fundamentals</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=142" target="_blank">00:02:22.340</a></span> | <span class="t">There is supervised learning, there's semi-supervised learning, there's unsupervised learning, there's reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=149" target="_blank">00:02:29.360</a></span> | <span class="t">And there's this feeling that supervised learning is really the only one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=153" target="_blank">00:02:33.360</a></span> | <span class="t">Where you have to perform the manual annotation, where you have to do the large-scale supervision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=158" target="_blank">00:02:38.500</a></span> | <span class="t">That's not the case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=162" target="_blank">00:02:42.640</a></span> | <span class="t">Every type of machine learning is supervised learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=165" target="_blank">00:02:45.680</a></span> | <span class="t">It's supervised by a loss function or a function that tells you what's good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=173" target="_blank">00:02:53.360</a></span> | <span class="t">And what's bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=175" target="_blank">00:02:55.680</a></span> | <span class="t">You know even looking at our own existence is how we humans figure out what's good and bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=180" target="_blank">00:03:00.240</a></span> | <span class="t">there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=182" target="_blank">00:03:02.080</a></span> | <span class="t">All kinds of sources direct and indirect by which our morals and ethics we figure out what's good and bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=188" target="_blank">00:03:08.320</a></span> | <span class="t">The difference between supervised and unsupervised and reinforcement learning is the source of that supervision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=193" target="_blank">00:03:13.780</a></span> | <span class="t">What's implied when you say unsupervised?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=196" target="_blank">00:03:16.560</a></span> | <span class="t">Is that the cost of human labor required to attain the supervision is low</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=202" target="_blank">00:03:22.720</a></span> | <span class="t">But it's never</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=205" target="_blank">00:03:25.440</a></span> | <span class="t">Turtles all the way down it's turtles and then there's a human at the bottom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=210" target="_blank">00:03:30.960</a></span> | <span class="t">There at some point there needs to be human intervention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=217" target="_blank">00:03:37.360</a></span> | <span class="t">Human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=218" target="_blank">00:03:38.480</a></span> | <span class="t">Input to provide what's good and what's bad and this will arise in reinforcement learning as well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=223" target="_blank">00:03:43.680</a></span> | <span class="t">we have to remember that because the challenges and the exciting opportunities of reinforcement learning lie in the fact of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=230" target="_blank">00:03:50.000</a></span> | <span class="t">How do we get that supervision?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=233" target="_blank">00:03:53.220</a></span> | <span class="t">In the most efficient way possible, but supervision nevertheless is required for any system that has an input and an output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=241" target="_blank">00:04:01.840</a></span> | <span class="t">That's trying to learn like a neural network does to provide an output. That's good. It needs somebody to say what's good and what's bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=249" target="_blank">00:04:09.360</a></span> | <span class="t">For you curious about that. There's been a few books a couple written throughout the last few centuries from Socrates to Nietzsche</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=256" target="_blank">00:04:16.720</a></span> | <span class="t">I recommend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=257" target="_blank">00:04:17.840</a></span> | <span class="t">the latter especially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=259" target="_blank">00:04:19.840</a></span> | <span class="t">So let's look at supervised learning and reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=263" target="_blank">00:04:23.620</a></span> | <span class="t">I'd like to propose a way to think about the difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=268" target="_blank">00:04:28.160</a></span> | <span class="t">That is illustrative and useful when we start talking about the techniques</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=273" target="_blank">00:04:33.300</a></span> | <span class="t">So supervised learning is taking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=275" target="_blank">00:04:35.760</a></span> | <span class="t">a bunch of examples of data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=278" target="_blank">00:04:38.160</a></span> | <span class="t">And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=281" target="_blank">00:04:41.200</a></span> | <span class="t">Learning from those examples where ground truth provides you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=284" target="_blank">00:04:44.640</a></span> | <span class="t">the compressed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=287" target="_blank">00:04:47.440</a></span> | <span class="t">Semantic meaning of what's in that data and from those examples one by one whether it's sequences or single samples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=296" target="_blank">00:04:56.720</a></span> | <span class="t">We learn what how to then take future such samples and interpret them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=302" target="_blank">00:05:02.240</a></span> | <span class="t">Reinforcement learning is teaching what we teach an agent through experience</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=309" target="_blank">00:05:09.700</a></span> | <span class="t">Not by showing a singular sample of a data set but by putting them out into the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=315" target="_blank">00:05:15.840</a></span> | <span class="t">The distinction there the essential element of reinforcement learning then for us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=320" target="_blank">00:05:20.880</a></span> | <span class="t">Now we'll talk about a bunch of algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=324" target="_blank">00:05:24.640</a></span> | <span class="t">But the essential design step is to provide the world in which to experience</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=331" target="_blank">00:05:31.140</a></span> | <span class="t">The agent learns from the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=334" target="_blank">00:05:34.320</a></span> | <span class="t">The from the world it gets the dynamics of that world the physics of the world from that world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=341" target="_blank">00:05:41.040</a></span> | <span class="t">It gets the rewards what's good and bad and us as designers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=344" target="_blank">00:05:44.660</a></span> | <span class="t">Of that agent do not just have to do the algorithm. We have to do design the the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=353" target="_blank">00:05:53.040</a></span> | <span class="t">In which that agent is trying to solve a task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=357" target="_blank">00:05:57.600</a></span> | <span class="t">The design of the world is the process of reinforcement learning the design of examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=363" target="_blank">00:06:03.520</a></span> | <span class="t">The annotation of examples is the world of supervised learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=366" target="_blank">00:06:06.500</a></span> | <span class="t">And the essential perhaps the most difficult element of reinforcement learning is the reward the good versus bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=375" target="_blank">00:06:15.840</a></span> | <span class="t">Here a baby starts walking across the room</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=381" target="_blank">00:06:21.280</a></span> | <span class="t">We want to define success as a baby</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=383" target="_blank">00:06:23.840</a></span> | <span class="t">walking across the room</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=386" target="_blank">00:06:26.400</a></span> | <span class="t">And reaching the destination that's success and failure is the inability to reach that destination</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=392" target="_blank">00:06:32.100</a></span> | <span class="t">simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=393" target="_blank">00:06:33.760</a></span> | <span class="t">and reinforcement learning in humans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=395" target="_blank">00:06:35.760</a></span> | <span class="t">The way we learn from these very few examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=400" target="_blank">00:06:40.980</a></span> | <span class="t">Appear to learn from very few examples through trial and error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=406" target="_blank">00:06:46.240</a></span> | <span class="t">Is a mystery a beautiful mystery full of open questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=409" target="_blank">00:06:49.280</a></span> | <span class="t">It could be from the huge amount of data 230 million years worth of bipedal data that we've been walking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=415" target="_blank">00:06:55.760</a></span> | <span class="t">Mammals walking ability to walk or 500 million years the ability to see having eyes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=422" target="_blank">00:07:02.240</a></span> | <span class="t">So that's the the hardware side somehow genetically encoded in us is the ability to comprehend this world extremely efficiently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=430" target="_blank">00:07:10.260</a></span> | <span class="t">It could be through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=432" target="_blank">00:07:12.640</a></span> | <span class="t">not the hardware not the 500 million years, but the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=436" target="_blank">00:07:16.000</a></span> | <span class="t">the few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=438" target="_blank">00:07:18.720</a></span> | <span class="t">minutes hours days months</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=440" target="_blank">00:07:20.880</a></span> | <span class="t">Maybe even years in the very beginning when we're born</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=444" target="_blank">00:07:24.720</a></span> | <span class="t">The ability to learn really quickly through observation to aggregate that information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=449" target="_blank">00:07:29.940</a></span> | <span class="t">Filter all the junk that you don't need and be able to learn really quickly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=454" target="_blank">00:07:34.480</a></span> | <span class="t">Through imitation learning through observation the way for walking that might mean observing others to walk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=462" target="_blank">00:07:42.080</a></span> | <span class="t">The idea there is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=463" target="_blank">00:07:43.760</a></span> | <span class="t">If there was no others</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=465" target="_blank">00:07:45.760</a></span> | <span class="t">Around we would never be able to learn this the fundamentals of this walking or as efficiently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=470" target="_blank">00:07:50.420</a></span> | <span class="t">It's through observation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=473" target="_blank">00:07:53.200</a></span> | <span class="t">And then it could be the algorithm totally not understood is the algorithm that our brain uses to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=481" target="_blank">00:08:01.520</a></span> | <span class="t">The back propagation that's an artificial neural networks the same kind of processes not understood in the brain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=488" target="_blank">00:08:08.960</a></span> | <span class="t">That could be the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=491" target="_blank">00:08:11.520</a></span> | <span class="t">So I want you to think about that as we talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=493" target="_blank">00:08:13.760</a></span> | <span class="t">the very trivial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=496" target="_blank">00:08:16.160</a></span> | <span class="t">By comparison accomplishments and reinforcement learning and how do we take the next steps?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=500" target="_blank">00:08:20.880</a></span> | <span class="t">But it nevertheless is exciting to have machines that learn how to act in the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=510" target="_blank">00:08:30.080</a></span> | <span class="t">the process of learning for those who have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=514" target="_blank">00:08:34.560</a></span> | <span class="t">fallen in love with artificial intelligence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=518" target="_blank">00:08:38.000</a></span> | <span class="t">The process of learning is thought of as intelligence. It's the ability to know very little and through experience examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=525" target="_blank">00:08:45.620</a></span> | <span class="t">Interaction with the world in whatever medium whether it's data or simulation so on be able to form much richer and interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=533" target="_blank">00:08:53.780</a></span> | <span class="t">Representations of that world be able to act in that world. That's that's the dream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=538" target="_blank">00:08:58.080</a></span> | <span class="t">So let's look at this stack of what an age what it means to be an agent in this world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=543" target="_blank">00:09:03.280</a></span> | <span class="t">from top</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=545" target="_blank">00:09:05.040</a></span> | <span class="t">The input to the bottom the output is there's an environment. We have to sense that environment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=551" target="_blank">00:09:11.280</a></span> | <span class="t">We have just a few tools us humans have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=553" target="_blank">00:09:13.360</a></span> | <span class="t">Several sensory systems on cars you can have lidar camera</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=560" target="_blank">00:09:20.080</a></span> | <span class="t">Stereo vision audio microphone networking gps imu sensor so on whatever robot you can think about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=567" target="_blank">00:09:27.440</a></span> | <span class="t">There's a way to sense that world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=569" target="_blank">00:09:29.760</a></span> | <span class="t">and you have this raw sensory data and then once you have the raw sensory data you're tasked with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=574" target="_blank">00:09:34.720</a></span> | <span class="t">representing that data in such a way that you can make sense of it as opposed to all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=580" target="_blank">00:09:40.240</a></span> | <span class="t">The the raw sensors in the eye the cones and so on that taken as just giant stream of high bandwidth information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=588" target="_blank">00:09:48.000</a></span> | <span class="t">We have to be able to form</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=590" target="_blank">00:09:50.240</a></span> | <span class="t">higher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=592" target="_blank">00:09:52.400</a></span> | <span class="t">Abstractions of features based on which we can reason from edges to corners to faces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=597" target="_blank">00:09:57.760</a></span> | <span class="t">And so on that's exactly what deep learning neural networks have stepped in to be able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=602" target="_blank">00:10:02.480</a></span> | <span class="t">In an automated fashion with as little human input as possible be able to form higher order representations of that information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=610" target="_blank">00:10:10.100</a></span> | <span class="t">Then there's the the learning aspect building on top of the greater abstractions formed through representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=617" target="_blank">00:10:17.620</a></span> | <span class="t">Be able to accomplish something useful whether it's discriminative task a generative task and so on based on the representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=625" target="_blank">00:10:25.040</a></span> | <span class="t">Be able to make sense of the data be able to generate new data and so on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=629" target="_blank">00:10:29.440</a></span> | <span class="t">From sequence to sequence to sequence to sample from sample to sequence and so on and so forth to actions as we'll talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=637" target="_blank">00:10:37.040</a></span> | <span class="t">and then there is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=639" target="_blank">00:10:39.920</a></span> | <span class="t">ability to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=642" target="_blank">00:10:42.780</a></span> | <span class="t">Aggregate all the information that's been received in the past to the useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=648" target="_blank">00:10:48.480</a></span> | <span class="t">information that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=651" target="_blank">00:10:51.440</a></span> | <span class="t">Pertinent to the task at hand. It's the thing the old</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=655" target="_blank">00:10:55.040</a></span> | <span class="t">It looks like a duck quacks like a duck swims like a duck</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=658" target="_blank">00:10:58.240</a></span> | <span class="t">Three different data sets i'm sure there's state-of-the-art algorithms for the three image classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=664" target="_blank">00:11:04.420</a></span> | <span class="t">audio recognition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=666" target="_blank">00:11:06.720</a></span> | <span class="t">video classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=668" target="_blank">00:11:08.560</a></span> | <span class="t">Activity recognition so on aggregating those three together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=672" target="_blank">00:11:12.000</a></span> | <span class="t">Is still an open problem and that could be the last piece again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=676" target="_blank">00:11:16.480</a></span> | <span class="t">I want you to think about as we think about reinforcement learning agents. How do we play?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=680" target="_blank">00:11:20.880</a></span> | <span class="t">How do we transfer from the game of atari to the game of go to the game of dota to the game of a robot?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=689" target="_blank">00:11:29.040</a></span> | <span class="t">Navigating an uncertain environment in the real world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=692" target="_blank">00:11:32.720</a></span> | <span class="t">And once you have that once you sense the raw world once you have a representation of that world then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=700" target="_blank">00:11:40.240</a></span> | <span class="t">We need to act</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=703" target="_blank">00:11:43.440</a></span> | <span class="t">Which is provide actions within the constraints of the world in such a way that we believe can get us towards success</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=711" target="_blank">00:11:51.680</a></span> | <span class="t">The promise excitement of deep learning is is the part of the stack that converts raw data into meaningful representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=718" target="_blank">00:11:58.900</a></span> | <span class="t">The promise the dream of deeper enforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=722" target="_blank">00:12:02.560</a></span> | <span class="t">Is going beyond</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=725" target="_blank">00:12:05.040</a></span> | <span class="t">And building an agent that uses that representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=728" target="_blank">00:12:08.020</a></span> | <span class="t">And acts achieve success in the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=731" target="_blank">00:12:11.120</a></span> | <span class="t">That's super exciting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=733" target="_blank">00:12:13.920</a></span> | <span class="t">The framework and the formulation of reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=738" target="_blank">00:12:18.800</a></span> | <span class="t">At its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=740" target="_blank">00:12:20.800</a></span> | <span class="t">Simplest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=742" target="_blank">00:12:22.240</a></span> | <span class="t">Is that there's an environment and there's an agent that acts in that environment?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=746" target="_blank">00:12:26.500</a></span> | <span class="t">the agent senses the environment by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=749" target="_blank">00:12:29.600</a></span> | <span class="t">by some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=751" target="_blank">00:12:31.580</a></span> | <span class="t">observation whether it's partial or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=753" target="_blank">00:12:33.840</a></span> | <span class="t">complete observation of the environment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=756" target="_blank">00:12:36.660</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=758" target="_blank">00:12:38.880</a></span> | <span class="t">It gives the environment an action it acts in that environment and through the action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=763" target="_blank">00:12:43.920</a></span> | <span class="t">The environment changes in some way and then a new observation occurs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=769" target="_blank">00:12:49.360</a></span> | <span class="t">And then also as you provide the action make the observations you receive a reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=773" target="_blank">00:12:53.940</a></span> | <span class="t">In most formulations of this of this framework</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=778" target="_blank">00:12:58.000</a></span> | <span class="t">This entire system has no memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=780" target="_blank">00:13:00.720</a></span> | <span class="t">That the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=784" target="_blank">00:13:04.080</a></span> | <span class="t">The only thing you need to be concerned about is the state you came from the state you arrived in and the reward received</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=789" target="_blank">00:13:09.780</a></span> | <span class="t">The open question here is what can't be modeled in this kind of way. Can we model all of it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=796" target="_blank">00:13:16.160</a></span> | <span class="t">from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=798" target="_blank">00:13:18.000</a></span> | <span class="t">From human life to the game of go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=800" target="_blank">00:13:20.000</a></span> | <span class="t">Can all of this be modeled in this way?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=802" target="_blank">00:13:22.480</a></span> | <span class="t">And what are is this a good way to formulate the learning problem of robotic systems?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=809" target="_blank">00:13:29.520</a></span> | <span class="t">In the real world in the simulated world. Those are the the open questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=813" target="_blank">00:13:33.220</a></span> | <span class="t">The environment could be fully observable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=817" target="_blank">00:13:37.040</a></span> | <span class="t">Or partially observable like in poker</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=820" target="_blank">00:13:40.960</a></span> | <span class="t">It could be single agent or multi-agent atari versus driving like deep traffic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=827" target="_blank">00:13:47.340</a></span> | <span class="t">deterministic or stochastic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=829" target="_blank">00:13:49.340</a></span> | <span class="t">static versus dynamic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=832" target="_blank">00:13:52.060</a></span> | <span class="t">Static as in chess dynamic again and driving in most real world applications discrete versus continuous like games</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=839" target="_blank">00:13:59.100</a></span> | <span class="t">Chess or continuous and carpool balancing a pull on a cart</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=843" target="_blank">00:14:03.500</a></span> | <span class="t">The challenge for RL in real world applications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=847" target="_blank">00:14:07.840</a></span> | <span class="t">Is that as a reminder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=856" target="_blank">00:14:16.280</a></span> | <span class="t">Supervised learning is teaching by example learning by example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=861" target="_blank">00:14:21.100</a></span> | <span class="t">teaching from our perspective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=863" target="_blank">00:14:23.860</a></span> | <span class="t">Reinforcement learning is teaching by experience</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=866" target="_blank">00:14:26.300</a></span> | <span class="t">And the way we provide experience to reinforcement learning agents currently for the most part is through simulation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=873" target="_blank">00:14:33.100</a></span> | <span class="t">Or through highly constrained real world scenarios</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=877" target="_blank">00:14:37.260</a></span> | <span class="t">So the challenge is in the fact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=880" target="_blank">00:14:40.840</a></span> | <span class="t">that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=882" target="_blank">00:14:42.600</a></span> | <span class="t">most of the successes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=884" target="_blank">00:14:44.920</a></span> | <span class="t">is with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=886" target="_blank">00:14:46.920</a></span> | <span class="t">Systems environments that are simulatable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=889" target="_blank">00:14:49.180</a></span> | <span class="t">So there's two ways to then close this gap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=894" target="_blank">00:14:54.280</a></span> | <span class="t">two directions of research and work one is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=898" target="_blank">00:14:58.440</a></span> | <span class="t">improve the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=900" target="_blank">00:15:00.740</a></span> | <span class="t">algorithms improve the ability of the algorithms to then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=903" target="_blank">00:15:03.640</a></span> | <span class="t">To form policies that are transferable across all kinds of domains including the real world including especially the real world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=910" target="_blank">00:15:10.920</a></span> | <span class="t">So train and simulation transfer to the real world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=914" target="_blank">00:15:14.920</a></span> | <span class="t">or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=916" target="_blank">00:15:16.920</a></span> | <span class="t">As we improve the simulation in such a way that the fidelity of the simulation increase increases to the point where the gap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=924" target="_blank">00:15:24.040</a></span> | <span class="t">between reality and simulation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=926" target="_blank">00:15:26.280</a></span> | <span class="t">is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=928" target="_blank">00:15:28.760</a></span> | <span class="t">Minimal to a degree that things learned in simulation are directly trivially transferable to the real world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=937" target="_blank">00:15:37.000</a></span> | <span class="t">Okay, the major components of an RL agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=944" target="_blank">00:15:44.040</a></span> | <span class="t">an agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=945" target="_blank">00:15:45.960</a></span> | <span class="t">Operates based on a strategy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=947" target="_blank">00:15:47.960</a></span> | <span class="t">called a policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=950" target="_blank">00:15:50.440</a></span> | <span class="t">It sees the world it makes a decision. That's a policy makes a decision how to act sees the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=956" target="_blank">00:15:56.520</a></span> | <span class="t">Sees a new state acts sees a reward sees new states and acts and this repeats</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=963" target="_blank">00:16:03.080</a></span> | <span class="t">forever until a terminal state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=966" target="_blank">00:16:06.200</a></span> | <span class="t">the value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=968" target="_blank">00:16:08.520</a></span> | <span class="t">is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=969" target="_blank">00:16:09.880</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=971" target="_blank">00:16:11.300</a></span> | <span class="t">estimate of how good a state is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=973" target="_blank">00:16:13.620</a></span> | <span class="t">or how good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=976" target="_blank">00:16:16.260</a></span> | <span class="t">A state action pair is meaning taking an action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=979" target="_blank">00:16:19.540</a></span> | <span class="t">In a particular state. How good is that ability to evaluate that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=984" target="_blank">00:16:24.660</a></span> | <span class="t">and then the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=987" target="_blank">00:16:27.540</a></span> | <span class="t">Different from the environment from the perspective of the agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=990" target="_blank">00:16:30.340</a></span> | <span class="t">So the environment has a model based on which it operates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=993" target="_blank">00:16:33.640</a></span> | <span class="t">And then the agent has a representation best understanding of that model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=999" target="_blank">00:16:39.300</a></span> | <span class="t">So the purpose for an RL agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1001" target="_blank">00:16:41.620</a></span> | <span class="t">In this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1004" target="_blank">00:16:44.340</a></span> | <span class="t">Simply formulated framework is to maximize reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1007" target="_blank">00:16:47.380</a></span> | <span class="t">the way that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1009" target="_blank">00:16:49.780</a></span> | <span class="t">The reward mathematically and practically is talked about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1013" target="_blank">00:16:53.140</a></span> | <span class="t">Is with a discounted framework, so we discount further and further future reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1020" target="_blank">00:17:00.740</a></span> | <span class="t">So the reward that's farther into the future is means less to us in terms of maximization than reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1027" target="_blank">00:17:07.460</a></span> | <span class="t">That's in the near term. And so why do we discount it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1031" target="_blank">00:17:11.220</a></span> | <span class="t">So first a lot of it is a math trick to be able to prove certain aspects analyze certain aspects of conversions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1037" target="_blank">00:17:17.160</a></span> | <span class="t">And in general on a more philosophical sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1040" target="_blank">00:17:20.820</a></span> | <span class="t">Because environments either are or can be thought of a stochastic random. It's very difficult</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1047" target="_blank">00:17:27.720</a></span> | <span class="t">To there's a degree of uncertainty, which makes it difficult to really estimate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1053" target="_blank">00:17:33.080</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1055" target="_blank">00:17:35.940</a></span> | <span class="t">The reward they'll be in the future because of the ripple effect of the uncertainty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1060" target="_blank">00:17:40.280</a></span> | <span class="t">Let's look at an example a simple one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1063" target="_blank">00:17:43.860</a></span> | <span class="t">Helps us understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1066" target="_blank">00:17:46.640</a></span> | <span class="t">policies rewards actions, there's a robot in the room there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1070" target="_blank">00:17:50.980</a></span> | <span class="t">12 cells in which you can step it starts in the bottom left. It tries to get rewards on the top, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1078" target="_blank">00:17:58.740</a></span> | <span class="t">There's a plus one. It's a really good thing at the top, right? It wants to get there by walking around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1085" target="_blank">00:18:05.300</a></span> | <span class="t">There's a negative one, which is really bad. It wants to avoid that square and the choice of actions is up down left right four actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1093" target="_blank">00:18:13.140</a></span> | <span class="t">so you could think of uh, they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1096" target="_blank">00:18:16.260</a></span> | <span class="t">Being a negative reward of 0.04 for each step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1100" target="_blank">00:18:20.260</a></span> | <span class="t">So there's a cost to each step and there's a stochastic nature to this world potentially we'll talk about both deterministic stochastic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1106" target="_blank">00:18:26.920</a></span> | <span class="t">So in the stochastic case when you choose the action up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1110" target="_blank">00:18:30.660</a></span> | <span class="t">with an 80% probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1114" target="_blank">00:18:34.580</a></span> | <span class="t">With an 80% chance you move up but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1116" target="_blank">00:18:36.660</a></span> | <span class="t">With 10% chance you move left and another 10 move right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1121" target="_blank">00:18:41.300</a></span> | <span class="t">So that's stochastic nature, even though you try to go up you might end up in a blocks the left into the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1126" target="_blank">00:18:46.340</a></span> | <span class="t">so for a deterministic world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1128" target="_blank">00:18:48.980</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1131" target="_blank">00:18:51.280</a></span> | <span class="t">optimal policy here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1133" target="_blank">00:18:53.280</a></span> | <span class="t">Given that we always start in the bottom left is really shortest path</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1136" target="_blank">00:18:56.580</a></span> | <span class="t">Is you know, you can't ever because there's no stochasticity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1140" target="_blank">00:19:00.980</a></span> | <span class="t">you're never going to screw up and just fall into the hole negative one hole that you just compute the shortest path and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1147" target="_blank">00:19:07.140</a></span> | <span class="t">Walk along that shortest path why shortest path because every single step hurts. There's a negative reward to it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1153" target="_blank">00:19:13.860</a></span> | <span class="t">0.04 so shortest path is the thing that minimizes the reward shortest path to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1160" target="_blank">00:19:20.340</a></span> | <span class="t">to the plus one block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1162" target="_blank">00:19:22.820</a></span> | <span class="t">Okay, let's look at a stochastic world. Like I mentioned the 80% up and then split 10% to the left and right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1169" target="_blank">00:19:29.860</a></span> | <span class="t">How does the policy change? Well, first of all we need to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1172" target="_blank">00:19:32.660</a></span> | <span class="t">uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1174" target="_blank">00:19:34.980</a></span> | <span class="t">We need to have a plan for every single block in the area because you might end up there due to the stochasticity of the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1180" target="_blank">00:19:40.980</a></span> | <span class="t">Okay, the the basic addition there is that we're trying to go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1185" target="_blank">00:19:45.940</a></span> | <span class="t">Avoid up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1189" target="_blank">00:19:49.300</a></span> | <span class="t">The closer you get to the negative one hole. So just try to avoid up because up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1196" target="_blank">00:19:56.100</a></span> | <span class="t">The stochastic nature of up means you might fall into the hole with a 10% chance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1200" target="_blank">00:20:00.340</a></span> | <span class="t">And given the 0.04 step reward you're willing to take the long way home</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1205" target="_blank">00:20:05.620</a></span> | <span class="t">In some cases in order to avoid that possibility the negative one possibility</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1210" target="_blank">00:20:10.760</a></span> | <span class="t">Now, let's look at a reward for each step if it decreases to negative two. It really hurts to take every step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1217" target="_blank">00:20:17.380</a></span> | <span class="t">Then again, we go to the shortest path despite the fact that uh, there's a stochastic nature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1224" target="_blank">00:20:24.180</a></span> | <span class="t">In fact, you don't really care that you step into the negative one hole because every step really hurts. You just want to get home</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1230" target="_blank">00:20:30.260</a></span> | <span class="t">And then you can play with this reward structure right yes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1235" target="_blank">00:20:35.380</a></span> | <span class="t">instead of uh negative two or negative 0.04 you can look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1241" target="_blank">00:20:41.140</a></span> | <span class="t">Negative 0.1 and you can see immediately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1244" target="_blank">00:20:44.600</a></span> | <span class="t">that the structure of the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1247" target="_blank">00:20:47.620</a></span> | <span class="t">It changes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1250" target="_blank">00:20:50.180</a></span> | <span class="t">So with a higher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1252" target="_blank">00:20:52.420</a></span> | <span class="t">Value the higher negative reward for each step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1254" target="_blank">00:20:54.980</a></span> | <span class="t">immediately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1256" target="_blank">00:20:56.980</a></span> | <span class="t">the urgency of the agent increases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1259" target="_blank">00:20:59.160</a></span> | <span class="t">Versus the less urgency the lower the negative reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1263" target="_blank">00:21:03.780</a></span> | <span class="t">And when the reward flips</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1267" target="_blank">00:21:07.380</a></span> | <span class="t">So it's positive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1270" target="_blank">00:21:10.980</a></span> | <span class="t">The every step is a positive so the entire system which is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1277" target="_blank">00:21:17.140</a></span> | <span class="t">Quite common in reinforcement learning the entire system is full of positive rewards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1281" target="_blank">00:21:21.940</a></span> | <span class="t">And so then the optimum policy becomes the longest path</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1285" target="_blank">00:21:25.220</a></span> | <span class="t">Is a grad school taking as long as possible never reaching the destination</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1291" target="_blank">00:21:31.880</a></span> | <span class="t">So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1296" target="_blank">00:21:36.900</a></span> | <span class="t">What lessons do we draw from robot in the room two things?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1300" target="_blank">00:21:40.660</a></span> | <span class="t">The environment model the dynamics is just there in the trivial example the stochastic nature the difference between 80 percent 100 percent and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1309" target="_blank">00:21:49.140</a></span> | <span class="t">50 percent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1311" target="_blank">00:21:51.060</a></span> | <span class="t">The model of the world the environment has a big impact on what the optimal policy is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1315" target="_blank">00:21:55.780</a></span> | <span class="t">And the reward structure most importantly the thing we can often control</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1322" target="_blank">00:22:02.440</a></span> | <span class="t">More</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1326" target="_blank">00:22:06.900</a></span> | <span class="t">in our constructs of the task we try to solve in reinforcement learning is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1331" target="_blank">00:22:11.140</a></span> | <span class="t">What is good and what is bad and how bad is it and how good is it the reward structure is a big?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1337" target="_blank">00:22:17.300</a></span> | <span class="t">Impact and that has a complete change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1341" target="_blank">00:22:21.140</a></span> | <span class="t">like like uh, robert frost said a complete change on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1345" target="_blank">00:22:25.540</a></span> | <span class="t">Policy the choices the agent makes so when you formulate a reinforcement learning framework</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1353" target="_blank">00:22:33.380</a></span> | <span class="t">As researchers as students what you often do is you design the environment you design the world in which the system learns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1361" target="_blank">00:22:41.140</a></span> | <span class="t">Even when your ultimate goal is the physical robot you just still there's a lot of work still done in simulation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1368" target="_blank">00:22:48.100</a></span> | <span class="t">So you design the world the parameters of that world and you also design the reward structure and it can have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1373" target="_blank">00:22:53.780</a></span> | <span class="t">A transformative results slight variations in those parameters can be huge results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1381" target="_blank">00:23:01.060</a></span> | <span class="t">On huge differences on the policy that's arrived and of course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1385" target="_blank">00:23:05.060</a></span> | <span class="t">The example i've shown before I really love is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1391" target="_blank">00:23:11.060</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1392" target="_blank">00:23:12.580</a></span> | <span class="t">impact of the the changing reward structure might have unintended consequences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1397" target="_blank">00:23:17.320</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1399" target="_blank">00:23:19.940</a></span> | <span class="t">those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1401" target="_blank">00:23:21.140</a></span> | <span class="t">Consequences for real world system can have obviously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1404" target="_blank">00:23:24.200</a></span> | <span class="t">highly detrimental</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1407" target="_blank">00:23:27.680</a></span> | <span class="t">Costs that are more than just a failed game of atari</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1411" target="_blank">00:23:31.220</a></span> | <span class="t">So here's a human performing the task get playing the game of coast runners racing around the track</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1417" target="_blank">00:23:37.380</a></span> | <span class="t">and so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1419" target="_blank">00:23:39.620</a></span> | <span class="t">uh, when you finish first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1421" target="_blank">00:23:41.860</a></span> | <span class="t">And you finish fast you get a lot of points and so it's natural to then okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1427" target="_blank">00:23:47.780</a></span> | <span class="t">Let's do an rl agent and then optimize this for those points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1431" target="_blank">00:23:51.700</a></span> | <span class="t">And what you find out in the game is that you also get points by picking up the little green turbo things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1439" target="_blank">00:23:59.380</a></span> | <span class="t">And what the agent figures out is that you can actually get a lot more points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1444" target="_blank">00:24:04.180</a></span> | <span class="t">even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1446" target="_blank">00:24:06.500</a></span> | <span class="t">By simply focusing on the green turbos</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1449" target="_blank">00:24:09.380</a></span> | <span class="t">focusing on the green turbos</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1452" target="_blank">00:24:12.020</a></span> | <span class="t">Just rotating over and over slamming into the wall fire and everything just picking it up, especially because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1457" target="_blank">00:24:17.540</a></span> | <span class="t">ability to pick up those turbos</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1461" target="_blank">00:24:21.540</a></span> | <span class="t">Can avoid the terminal state at the end of finishing the race in fact finishing the race means you stop collecting positive reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1468" target="_blank">00:24:28.580</a></span> | <span class="t">So you never want to finish collect the turbos</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1470" target="_blank">00:24:30.900</a></span> | <span class="t">And though that's a trivial example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1474" target="_blank">00:24:34.680</a></span> | <span class="t">It's not actually easy to find such examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1478" target="_blank">00:24:38.360</a></span> | <span class="t">But they're out there of unintended consequences that can have highly negative detrimental effects when put in the real world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1485" target="_blank">00:24:45.940</a></span> | <span class="t">We'll talk about a little bit of robotics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1488" target="_blank">00:24:48.740</a></span> | <span class="t">When you put robots four-wheeled ones like autonomous vehicles into the real world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1493" target="_blank">00:24:53.780</a></span> | <span class="t">And you have objective functions that have to navigate difficult intersections full of pedestrians</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1499" target="_blank">00:24:59.080</a></span> | <span class="t">So you have to form intent models of those pedestrians here. You see cars asserting themselves through dense intersections</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1506" target="_blank">00:25:06.280</a></span> | <span class="t">taking risks and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1508" target="_blank">00:25:08.820</a></span> | <span class="t">Within those risks that are taken by us humans when we drive vehicles</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1514" target="_blank">00:25:14.260</a></span> | <span class="t">we have to then encode that ability to take subtle risk into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1518" target="_blank">00:25:18.580</a></span> | <span class="t">into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1521" target="_blank">00:25:21.780</a></span> | <span class="t">AI-based control algorithms perception</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1523" target="_blank">00:25:23.960</a></span> | <span class="t">Then you have to think about at the end of the day. There's an objective function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1529" target="_blank">00:25:29.540</a></span> | <span class="t">and if that objective function does not anticipate the green turbos that are to be collected and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1536" target="_blank">00:25:36.020</a></span> | <span class="t">then result in some unintended consequences could have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1538" target="_blank">00:25:38.980</a></span> | <span class="t">very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1543" target="_blank">00:25:43.300</a></span> | <span class="t">Negative effects especially in situations that involve human life</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1547" target="_blank">00:25:47.300</a></span> | <span class="t">That's the field of AI safety and some of the folks who talk about deep mind and open AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1552" target="_blank">00:25:52.980</a></span> | <span class="t">That are doing incredible work in RL also have groups that are working in AI safety for a very good reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1560" target="_blank">00:26:00.180</a></span> | <span class="t">this is a problem that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1562" target="_blank">00:26:02.900</a></span> | <span class="t">I believe that artificial intelligence will define some of the most impactful positive things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1569" target="_blank">00:26:09.060</a></span> | <span class="t">In the 21st century, but I also believe we are nowhere close</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1573" target="_blank">00:26:13.700</a></span> | <span class="t">To solving some of the fundamental problems of AI safety that we also need to address as we develop those algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1580" target="_blank">00:26:20.440</a></span> | <span class="t">So okay examples of reinforcement learning systems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1583" target="_blank">00:26:23.860</a></span> | <span class="t">All of it has to do with formulation of rewards formulation of state and actions. You have the traditional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1591" target="_blank">00:26:31.320</a></span> | <span class="t">The often used benchmark of a cart</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1597" target="_blank">00:26:37.600</a></span> | <span class="t">Balancing a pole continuous. So the action is the horizontal force of the cart</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1602" target="_blank">00:26:42.000</a></span> | <span class="t">The goal is to balance the pole</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1604" target="_blank">00:26:44.160</a></span> | <span class="t">So it stays top in the moving cart and the reward is one at each time step if the pole is upright</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1609" target="_blank">00:26:49.920</a></span> | <span class="t">And the state measured by the cart by the agent is the pole angle angular speed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1615" target="_blank">00:26:55.920</a></span> | <span class="t">And of course self sensing of the cart position and the horizontal velocity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1621" target="_blank">00:27:01.460</a></span> | <span class="t">Another example here didn't want to include the video because it's really disturbing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1627" target="_blank">00:27:07.360</a></span> | <span class="t">but I do want to include this slide because it's really important to think about is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1631" target="_blank">00:27:11.440</a></span> | <span class="t">by sensing the the raw pixels learning and teaching an agent to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1636" target="_blank">00:27:16.560</a></span> | <span class="t">Play a game of doom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1639" target="_blank">00:27:19.520</a></span> | <span class="t">So the goal there is to eliminate all opponents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1642" target="_blank">00:27:22.260</a></span> | <span class="t">The state is the raw game pixels the actions up down shoot reload and so on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1648" target="_blank">00:27:28.000</a></span> | <span class="t">And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1651" target="_blank">00:27:31.680</a></span> | <span class="t">The positive reward is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1654" target="_blank">00:27:34.000</a></span> | <span class="t">When an opponent is eliminated and negative when the agent is eliminated simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1658" target="_blank">00:27:38.800</a></span> | <span class="t">I added it here because again on the topic of AI safety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1664" target="_blank">00:27:44.420</a></span> | <span class="t">We have to think about objective functions and how that translate into the world of not just autonomous vehicles</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1673" target="_blank">00:27:53.940</a></span> | <span class="t">but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1676" target="_blank">00:27:56.720</a></span> | <span class="t">Things that even more directly have harm like autonomous weapon systems. We have a lecture on this in the AGI series</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1684" target="_blank">00:28:04.240</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1685" target="_blank">00:28:05.280</a></span> | <span class="t">The on the robotics platform the manipulate object manipulation grasping objects. There's a few benchmarks. There's a few interesting applications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1693" target="_blank">00:28:13.140</a></span> | <span class="t">learning the problem of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1696" target="_blank">00:28:16.080</a></span> | <span class="t">grabbing objects moving objects</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1698" target="_blank">00:28:18.320</a></span> | <span class="t">Manipulating objects rotating and so on especially when those objects don't have have complicated shapes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1705" target="_blank">00:28:25.680</a></span> | <span class="t">And so the goal is to pick up an object in the purely in the grasping object challenge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1711" target="_blank">00:28:31.200</a></span> | <span class="t">The state is the visual information. So it's visual visual based the raw pixels of the objects</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1716" target="_blank">00:28:36.800</a></span> | <span class="t">The action is to move the arm grasp the object pick it up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1720" target="_blank">00:28:40.160</a></span> | <span class="t">And obviously it's positive when the pickup is successful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1724" target="_blank">00:28:44.340</a></span> | <span class="t">The reason i'm personally excited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1727" target="_blank">00:28:47.120</a></span> | <span class="t">by this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1728" target="_blank">00:28:48.960</a></span> | <span class="t">is because it will finally allow us to solve the problem of the the claw which has been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1735" target="_blank">00:28:55.040</a></span> | <span class="t">Torturing me for many years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1738" target="_blank">00:28:58.880</a></span> | <span class="t">I don't know. That's not at all why i'm excited but okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1742" target="_blank">00:29:02.080</a></span> | <span class="t">And then we have to think about as we get greater and greater degree of application in the real world with the robotics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1748" target="_blank">00:29:08.180</a></span> | <span class="t">Like cars</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1751" target="_blank">00:29:11.520</a></span> | <span class="t">The the main focus of my passion in terms of robotics is how do we encode some of the things that us humans encode?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1757" target="_blank">00:29:17.920</a></span> | <span class="t">How do we you know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1759" target="_blank">00:29:19.840</a></span> | <span class="t">We have to think about our own objective function our own reward structure our own model of the environment about which we perceive and reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1767" target="_blank">00:29:27.200</a></span> | <span class="t">About in order to then encode machines that are doing the same and I believe autonomous driving is in that category</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1773" target="_blank">00:29:33.040</a></span> | <span class="t">We have to ask questions of ethics. We have to ask questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1776" target="_blank">00:29:36.080</a></span> | <span class="t">of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1778" target="_blank">00:29:38.080</a></span> | <span class="t">of risk value of human life value of efficiency money and so on all these are fundamental questions that an autonomous vehicle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1785" target="_blank">00:29:45.600</a></span> | <span class="t">Unfortunately has to solve before it becomes fully autonomous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1789" target="_blank">00:29:49.220</a></span> | <span class="t">So here are the key takeaways of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1794" target="_blank">00:29:54.960</a></span> | <span class="t">the real world impact of reinforcement learning agents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1797" target="_blank">00:29:57.680</a></span> | <span class="t">On the deep learning side</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1801" target="_blank">00:30:01.920</a></span> | <span class="t">Okay, these neural networks that form higher representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1804" target="_blank">00:30:04.400</a></span> | <span class="t">The fun part is the algorithms all the different architectures the different encoder decoder structures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1810" target="_blank">00:30:10.020</a></span> | <span class="t">all the attention self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1813" target="_blank">00:30:13.140</a></span> | <span class="t">recurrence LSTMs GRUs all the fun architectures and the data and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1819" target="_blank">00:30:19.680</a></span> | <span class="t">the ability to leverage different data sets in order to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1823" target="_blank">00:30:23.180</a></span> | <span class="t">to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1825" target="_blank">00:30:25.180</a></span> | <span class="t">discriminate better than uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1827" target="_blank">00:30:27.180</a></span> | <span class="t">Perform discriminatory tasks better than you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1830" target="_blank">00:30:30.640</a></span> | <span class="t">MIT does better than Stanford that kind of thing. That's the fun part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1834" target="_blank">00:30:34.800</a></span> | <span class="t">The hard part is asking good questions and collecting huge amounts of data that's representative of the task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1841" target="_blank">00:30:41.920</a></span> | <span class="t">That's for real world impact not CVPR publication real world impact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1846" target="_blank">00:30:46.320</a></span> | <span class="t">A huge amount of data on the deeper enforcement learning side the key challenge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1853" target="_blank">00:30:53.040</a></span> | <span class="t">The fun part again is the algorithms. How do we learn from data some of the stuff i'll talk about today?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1857" target="_blank">00:30:57.520</a></span> | <span class="t">The hard part is defining the environment defining the access space and the reward structure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1863" target="_blank">00:31:03.600</a></span> | <span class="t">As I mentioned this is the big challenge and the hardest part is how to crack the gap between simulation in the real world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1870" target="_blank">00:31:10.640</a></span> | <span class="t">the leaping lizard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1872" target="_blank">00:31:12.960</a></span> | <span class="t">That's the hardest part. We don't even know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1874" target="_blank">00:31:14.960</a></span> | <span class="t">How to solve that transfer learning problem yet for the real world impact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1878" target="_blank">00:31:18.880</a></span> | <span class="t">The three types of reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1882" target="_blank">00:31:22.240</a></span> | <span class="t">There's countless algorithms and there's a lot of ways to taxonomize them, but at the highest level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1890" target="_blank">00:31:30.960</a></span> | <span class="t">There's model-based and there's model-free</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1893" target="_blank">00:31:33.760</a></span> | <span class="t">model-based algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1897" target="_blank">00:31:37.020</a></span> | <span class="t">Learn the model of the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1899" target="_blank">00:31:39.280</a></span> | <span class="t">So as you interact with the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1901" target="_blank">00:31:41.280</a></span> | <span class="t">You construct your estimate of how you believe the dynamics of that world operates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1907" target="_blank">00:31:47.940</a></span> | <span class="t">The nice thing about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1911" target="_blank">00:31:51.680</a></span> | <span class="t">Doing that is once you have a model or an estimate of a model you're able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1916" target="_blank">00:31:56.480</a></span> | <span class="t">Anticipate you're able to plan into the future. You're able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1921" target="_blank">00:32:01.200</a></span> | <span class="t">use the model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1924" target="_blank">00:32:04.320</a></span> | <span class="t">In a branching way predict how your actions will change the world so you can plan far into the future</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1930" target="_blank">00:32:10.240</a></span> | <span class="t">This is the mechanism by which you you can you can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1933" target="_blank">00:32:13.360</a></span> | <span class="t">chess</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1935" target="_blank">00:32:15.520</a></span> | <span class="t">Uh in the simplest form because in chess, you don't even need to learn the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1938" target="_blank">00:32:18.880</a></span> | <span class="t">The model is learned is given to you chess go and so on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1941" target="_blank">00:32:21.520</a></span> | <span class="t">The most important way in which they're different I think is the sample efficiency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1946" target="_blank">00:32:26.500</a></span> | <span class="t">Is how many examples of data are needed to be able to successfully operate in the world?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1952" target="_blank">00:32:32.000</a></span> | <span class="t">And so model-based methods because they're constructing a model if they can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1956" target="_blank">00:32:36.480</a></span> | <span class="t">Are extremely sample efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1959" target="_blank">00:32:39.360</a></span> | <span class="t">Because once you have a model you can do all kinds of reasoning that doesn't require</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1964" target="_blank">00:32:44.100</a></span> | <span class="t">experiencing every possibility of that model you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1968" target="_blank">00:32:48.720</a></span> | <span class="t">Unroll the model to to see how the world changes based on your actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1973" target="_blank">00:32:53.600</a></span> | <span class="t">Value-based methods are ones that look to estimate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1978" target="_blank">00:32:58.880</a></span> | <span class="t">The quality of states the quality of state taking a certain action in a certain state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1984" target="_blank">00:33:04.320</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1986" target="_blank">00:33:06.320</a></span> | <span class="t">They're called off policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1988" target="_blank">00:33:08.320</a></span> | <span class="t">Versus the last category that's on policy. What does it mean to be off policy? It means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=1996" target="_blank">00:33:16.080</a></span> | <span class="t">They constantly a value-based agents constantly update how good is taking action in a state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2002" target="_blank">00:33:22.800</a></span> | <span class="t">and they have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2006" target="_blank">00:33:26.000</a></span> | <span class="t">model of that goodness of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2008" target="_blank">00:33:28.240</a></span> | <span class="t">Taking action in a state and they use that to pick the optimal action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2012" target="_blank">00:33:32.000</a></span> | <span class="t">They don't directly learn a policy a strategy of how to act they learn how good it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2019" target="_blank">00:33:39.600</a></span> | <span class="t">to be in a state and use that goodness information to then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2024" target="_blank">00:33:44.960</a></span> | <span class="t">pick the best one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2026" target="_blank">00:33:46.960</a></span> | <span class="t">And then every once in a while flip a coin in order to explore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2029" target="_blank">00:33:49.920</a></span> | <span class="t">And then policy-based methods are ones that directly learn a policy function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2036" target="_blank">00:33:56.320</a></span> | <span class="t">so they take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2038" target="_blank">00:33:58.640</a></span> | <span class="t">as input the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2040" target="_blank">00:34:00.640</a></span> | <span class="t">the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2042" target="_blank">00:34:02.140</a></span> | <span class="t">representation of that world neural networks and is output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2044" target="_blank">00:34:04.720</a></span> | <span class="t">a action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2047" target="_blank">00:34:07.360</a></span> | <span class="t">Where the action is stochastic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2049" target="_blank">00:34:09.360</a></span> | <span class="t">So, okay, that's the range of model-based value-based and policy-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2054" target="_blank">00:34:14.820</a></span> | <span class="t">Here's an image from openai that I really like I encourage you to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2059" target="_blank">00:34:19.700</a></span> | <span class="t">As we further explore here to look up spinning up in deeper enforcement learning from openai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2066" target="_blank">00:34:26.120</a></span> | <span class="t">Here's an image that taxonomizes in the way that I described some of the recent developments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2070" target="_blank">00:34:30.920</a></span> | <span class="t">in rl</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2072" target="_blank">00:34:32.660</a></span> | <span class="t">so at the very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2074" target="_blank">00:34:34.420</a></span> | <span class="t">top the distinction between model free rl and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2077" target="_blank">00:34:37.060</a></span> | <span class="t">model-based rl</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2080" target="_blank">00:34:40.180</a></span> | <span class="t">In model free rl, which is what we'll focus on today. There is a distinction between policy optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2086" target="_blank">00:34:46.680</a></span> | <span class="t">So on policy methods and q learning which is all policy methods policy optimization methods that directly optimize the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2096" target="_blank">00:34:56.340</a></span> | <span class="t">Directly learn the policy in some way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2098" target="_blank">00:34:58.900</a></span> | <span class="t">and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2100" target="_blank">00:35:00.500</a></span> | <span class="t">Q learning all policy methods learn like I mentioned the value of taking a certain action in a state and from that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2107" target="_blank">00:35:07.540</a></span> | <span class="t">learned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2110" target="_blank">00:35:10.260</a></span> | <span class="t">that learned q value be able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2112" target="_blank">00:35:12.980</a></span> | <span class="t">Choose how to act in the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2116" target="_blank">00:35:16.020</a></span> | <span class="t">So let's look at a few sample representative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2119" target="_blank">00:35:19.880</a></span> | <span class="t">approaches in this space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2123" target="_blank">00:35:23.120</a></span> | <span class="t">Let's start with the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2125" target="_blank">00:35:25.620</a></span> | <span class="t">that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2127" target="_blank">00:35:27.940</a></span> | <span class="t">Really was one of the first great breakthroughs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2130" target="_blank">00:35:30.600</a></span> | <span class="t">From google deep mind on the deep rl side and solving atari games dqn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2135" target="_blank">00:35:35.320</a></span> | <span class="t">deep q learning networks deep q networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2140" target="_blank">00:35:40.500</a></span> | <span class="t">And let's take a step back and think about what q learning is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2143" target="_blank">00:35:43.380</a></span> | <span class="t">Q learning looks at the state action value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2147" target="_blank">00:35:47.300</a></span> | <span class="t">Q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2150" target="_blank">00:35:50.260</a></span> | <span class="t">That estimates based on a particular policy or based on an optimal policy. How good is it to take an action?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2156" target="_blank">00:35:56.900</a></span> | <span class="t">in this state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2160" target="_blank">00:36:00.020</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2161" target="_blank">00:36:01.360</a></span> | <span class="t">estimated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2162" target="_blank">00:36:02.720</a></span> | <span class="t">Reward if I take an action in this state and continue operating under an optimal policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2169" target="_blank">00:36:09.300</a></span> | <span class="t">It gives you directly a way to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2171" target="_blank">00:36:11.620</a></span> | <span class="t">Amongst all the actions I have which action should I take to maximize the reward?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2176" target="_blank">00:36:16.660</a></span> | <span class="t">Now in the beginning, you know, nothing, you know, you don't have this value estimation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2181" target="_blank">00:36:21.800</a></span> | <span class="t">You don't have this q function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2184" target="_blank">00:36:24.660</a></span> | <span class="t">So you have to learn it and you learn it with a bellman equation of updating it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2188" target="_blank">00:36:28.980</a></span> | <span class="t">You take your current estimate and update it with the reward you see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2192" target="_blank">00:36:32.180</a></span> | <span class="t">Received after you take an action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2194" target="_blank">00:36:34.820</a></span> | <span class="t">Here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2197" target="_blank">00:36:37.700</a></span> | <span class="t">It's off policy and model free. You don't have to have any estimate or knowledge of the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2203" target="_blank">00:36:43.380</a></span> | <span class="t">You don't have to have any policy whatsoever. All you're doing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2207" target="_blank">00:36:47.460</a></span> | <span class="t">Roaming about the world collecting data when you took a certain action. Here's the word you received and you're updating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2213" target="_blank">00:36:53.800</a></span> | <span class="t">gradually this table</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2216" target="_blank">00:36:56.800</a></span> | <span class="t">Where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2219" target="_blank">00:36:59.620</a></span> | <span class="t">the table has state states on the y-axis and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2223" target="_blank">00:37:03.860</a></span> | <span class="t">actions on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2227" target="_blank">00:37:07.140</a></span> | <span class="t">the x-axis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2229" target="_blank">00:37:09.140</a></span> | <span class="t">and the key part there is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2231" target="_blank">00:37:11.540</a></span> | <span class="t">Because you always have an estimate of what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2234" target="_blank">00:37:14.820</a></span> | <span class="t">To take an action of the value of taking that action so you can always take the optimal one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2240" target="_blank">00:37:20.740</a></span> | <span class="t">but because you know very little in the beginning that optimal is going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2245" target="_blank">00:37:25.140</a></span> | <span class="t">You have no way of knowing that's good or not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2248" target="_blank">00:37:28.020</a></span> | <span class="t">So there's some degree of exploration the fundamental aspect of value-based methods or any RL methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2254" target="_blank">00:37:34.180</a></span> | <span class="t">Like I said, it's trial and error is exploration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2256" target="_blank">00:37:36.920</a></span> | <span class="t">So for value-based methods like Q learning the way that's done is with a flip of a coin epsilon greedy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2264" target="_blank">00:37:44.340</a></span> | <span class="t">With a flip of a coin you can choose to just take a random action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2269" target="_blank">00:37:49.140</a></span> | <span class="t">and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2272" target="_blank">00:37:52.020</a></span> | <span class="t">Slowly decrease epsilon to zero as your agent learns more and more and more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2277" target="_blank">00:37:57.940</a></span> | <span class="t">so in the beginning you explore a lot an epsilon of one an epsilon of zero in the end when you're just acting greedy based on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2285" target="_blank">00:38:05.540</a></span> | <span class="t">Your understanding of the world as represented by the Q value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2289" target="_blank">00:38:09.300</a></span> | <span class="t">For non-neural network approaches. This is simply a table the Q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2294" target="_blank">00:38:14.420</a></span> | <span class="t">This Q function is a table. Like I said on the y state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2299" target="_blank">00:38:19.300</a></span> | <span class="t">x actions and in each cell you have a reward that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2306" target="_blank">00:38:26.500</a></span> | <span class="t">A discount or reward you estimate to be received there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2309" target="_blank">00:38:29.060</a></span> | <span class="t">And as you walk around with this Bellamy equation, you can update that table</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2313" target="_blank">00:38:33.140</a></span> | <span class="t">but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2314" target="_blank">00:38:34.740</a></span> | <span class="t">It's a table nevertheless number of states times number of actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2318" target="_blank">00:38:38.580</a></span> | <span class="t">Now if you look at any practical real-world problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2321" target="_blank">00:38:41.460</a></span> | <span class="t">And an arcade game with raw sensory input is a very crude first step towards the real world so raw sensory information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2332" target="_blank">00:38:52.660</a></span> | <span class="t">This kind of value iteration and updating a table is impractical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2337" target="_blank">00:38:57.160</a></span> | <span class="t">Because here's for a game of breakout if you look at four consecutive frames of a game of breakout</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2343" target="_blank">00:39:03.080</a></span> | <span class="t">Size of the of the raw sensory input is 84 by 84 pixels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2350" target="_blank">00:39:10.500</a></span> | <span class="t">grayscale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2352" target="_blank">00:39:12.660</a></span> | <span class="t">Every pixel has 256 values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2354" target="_blank">00:39:14.980</a></span> | <span class="t">that's 256 to the power of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2361" target="_blank">00:39:21.700</a></span> | <span class="t">Whatever 84 times 84 times 4 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2364" target="_blank">00:39:24.180</a></span> | <span class="t">Whatever it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2367" target="_blank">00:39:27.140</a></span> | <span class="t">It's significantly larger than the number of atoms in the universe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2369" target="_blank">00:39:29.720</a></span> | <span class="t">So the size of this Q table if we use the traditional approach is intractable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2375" target="_blank">00:39:35.640</a></span> | <span class="t">Neural networks to the rescue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2381" target="_blank">00:39:41.700</a></span> | <span class="t">Deep RL is RL plus neural networks where the neural networks is tasked with taking this in value-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2389" target="_blank">00:39:49.860</a></span> | <span class="t">Methods taking this Q table and learning a compressed representation of it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2393" target="_blank">00:39:53.860</a></span> | <span class="t">Learning an approximator for the function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2397" target="_blank">00:39:57.540</a></span> | <span class="t">from state and action to the value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2400" target="_blank">00:40:00.660</a></span> | <span class="t">That's what previously talked about the ability the powerful ability of neural networks to form</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2407" target="_blank">00:40:07.700</a></span> | <span class="t">representations from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2410" target="_blank">00:40:10.080</a></span> | <span class="t">extremely high dimensional complex raw sensory information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2414" target="_blank">00:40:14.740</a></span> | <span class="t">So it's simple the framework remains for the most part the same in reinforcement learning. It's just that this Q function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2421" target="_blank">00:40:21.300</a></span> | <span class="t">For value-based methods becomes a neural network and becomes an approximator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2427" target="_blank">00:40:27.240</a></span> | <span class="t">where the hope is as you navigate the world and you pick up new knowledge through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2432" target="_blank">00:40:32.900</a></span> | <span class="t">The back propagating the gradient and the loss function that you're able to form a good representation of the optimal Q function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2442" target="_blank">00:40:42.340</a></span> | <span class="t">So use neural networks with neural networks are good at which is function approximators</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2445" target="_blank">00:40:45.960</a></span> | <span class="t">And that's DQN deep Q network was used to have the initial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2451" target="_blank">00:40:51.780</a></span> | <span class="t">Incredible nice results on the arcade games where the input is the raw sensory pixels with a few convolutional layers fully connected layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2460" target="_blank">00:41:00.740</a></span> | <span class="t">And the output is a set of actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2463" target="_blank">00:41:03.540</a></span> | <span class="t">You know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2466" target="_blank">00:41:06.640</a></span> | <span class="t">Probability of taking that action and then you sample that and you choose the best action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2470" target="_blank">00:41:10.500</a></span> | <span class="t">And so this simple agent with a neural network that estimates that Q function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2474" target="_blank">00:41:14.260</a></span> | <span class="t">Very simple network is able to achieve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2478" target="_blank">00:41:18.100</a></span> | <span class="t">A superhuman performance on many of these arcade games that excited the world because it's taking raw sensory information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2485" target="_blank">00:41:25.400</a></span> | <span class="t">with a pretty simple network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2488" target="_blank">00:41:28.020</a></span> | <span class="t">That doesn't in the beginning understand any of the physics of the world any of the dynamics of the environment and through that intractable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2494" target="_blank">00:41:34.920</a></span> | <span class="t">space the intractable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2499" target="_blank">00:41:39.300</a></span> | <span class="t">State space is able to learn how to actually do pretty well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2502" target="_blank">00:41:42.900</a></span> | <span class="t">The loss function for DQN has two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2508" target="_blank">00:41:48.980</a></span> | <span class="t">Q functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2511" target="_blank">00:41:51.860</a></span> | <span class="t">One is the expected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2514" target="_blank">00:41:54.200</a></span> | <span class="t">The predicted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2517" target="_blank">00:41:57.940</a></span> | <span class="t">Q value of taking an action in a particular state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2520" target="_blank">00:42:00.580</a></span> | <span class="t">and the other is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2523" target="_blank">00:42:03.300</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2524" target="_blank">00:42:04.420</a></span> | <span class="t">Target against which the loss function is calculated. Which is what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2529" target="_blank">00:42:09.380</a></span> | <span class="t">value that you got once you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2532" target="_blank">00:42:12.660</a></span> | <span class="t">Taken that action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2535" target="_blank">00:42:15.780</a></span> | <span class="t">and once you've taken that action the way you calculate the value is by looking at the next step and choosing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2540" target="_blank">00:42:20.900</a></span> | <span class="t">Maximum choosing if you take the best action in the next state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2545" target="_blank">00:42:25.620</a></span> | <span class="t">What is going to be the Q function? So there's two estimators going on in terms of neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2551" target="_blank">00:42:31.620</a></span> | <span class="t">There's two forward passes here. There's two Q's in this equation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2554" target="_blank">00:42:34.900</a></span> | <span class="t">so in traditional DQN, that's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2558" target="_blank">00:42:38.660</a></span> | <span class="t">That's done by a single neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2561" target="_blank">00:42:41.780</a></span> | <span class="t">With a few tricks and double DQN that's done by two neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2566" target="_blank">00:42:46.200</a></span> | <span class="t">And I mentioned tricks because with this and with most of RL tricks tell a lot of the story</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2575" target="_blank">00:42:55.940</a></span> | <span class="t">A lot of what makes systems work is the details</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2580" target="_blank">00:43:00.100</a></span> | <span class="t">in games and robotic systems in these cases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2584" target="_blank">00:43:04.180</a></span> | <span class="t">The two biggest tricks for DQN that will reappear in a lot of value-based methods is experience replay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2592" target="_blank">00:43:12.180</a></span> | <span class="t">So think of an agent that plays through these games</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2596" target="_blank">00:43:16.580</a></span> | <span class="t">as also collecting memories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2599" target="_blank">00:43:19.220</a></span> | <span class="t">You collect this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2601" target="_blank">00:43:21.700</a></span> | <span class="t">Bank of memories that can then be replayed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2605" target="_blank">00:43:25.380</a></span> | <span class="t">The power of that one of the central elements of what makes value-based methods attractive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2610" target="_blank">00:43:30.980</a></span> | <span class="t">is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2613" target="_blank">00:43:33.300</a></span> | <span class="t">Because you're not directly estimating the policy but are learning the quality of taking an action in a particular state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2619" target="_blank">00:43:39.140</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2621" target="_blank">00:43:41.380</a></span> | <span class="t">You're able to then jump around through your memory and and play</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2625" target="_blank">00:43:45.460</a></span> | <span class="t">different aspects of that memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2628" target="_blank">00:43:48.100</a></span> | <span class="t">So learn, train the network through the historical data and then the other trick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2634" target="_blank">00:43:54.980</a></span> | <span class="t">Simple is like I said that there is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2638" target="_blank">00:43:58.100</a></span> | <span class="t">So the loss function has two queues</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2640" target="_blank">00:44:00.740</a></span> | <span class="t">So you're it's it's a dragon chasing its own tail</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2645" target="_blank">00:44:05.140</a></span> | <span class="t">It's easy for the loss function to become unstable. So the training does not converge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2650" target="_blank">00:44:10.580</a></span> | <span class="t">So the trick of fixing a target network is taking one of the queues</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2655" target="_blank">00:44:15.060</a></span> | <span class="t">And only updating it every x steps every thousand steps and so on and taking the same kind of network is just fixing it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2662" target="_blank">00:44:22.180</a></span> | <span class="t">So for the target network that defines the loss function just keeping it fixed and only updating irregularly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2667" target="_blank">00:44:27.800</a></span> | <span class="t">So you're chasing a fixed target with a loss function as opposed to a dynamic one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2673" target="_blank">00:44:33.780</a></span> | <span class="t">So you can solve a lot of the Atari games</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2677" target="_blank">00:44:37.300</a></span> | <span class="t">With minimal effort come up with some creative solutions here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2681" target="_blank">00:44:41.220</a></span> | <span class="t">Break out here after 10 minutes of training on the left after two of two hours of training on the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2686" target="_blank">00:44:46.900</a></span> | <span class="t">It's coming up with some creative solutions again. It's pretty cool because this is raw pixels, right? We're now like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2693" target="_blank">00:44:53.780</a></span> | <span class="t">There's been a few years since this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2696" target="_blank">00:44:56.740</a></span> | <span class="t">Breakthrough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2698" target="_blank">00:44:58.740</a></span> | <span class="t">So kind of take it for granted, but I still for the most part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2703" target="_blank">00:45:03.140</a></span> | <span class="t">Captivated by just how beautiful it is that from raw sensory information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2708" target="_blank">00:45:08.360</a></span> | <span class="t">neural networks are able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2711" target="_blank">00:45:11.460</a></span> | <span class="t">learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2712" target="_blank">00:45:12.340</a></span> | <span class="t">to act in a way that actually supersedes humans in terms of creativity in terms of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2716" target="_blank">00:45:16.420</a></span> | <span class="t">In terms of actual raw performance. It's really exciting and games of simple form is the cleanest way to demonstrate that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2724" target="_blank">00:45:24.660</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2726" target="_blank">00:45:26.660</a></span> | <span class="t">The same kind of DQN network is able to achieve superhuman performance on a bunch of different games</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2731" target="_blank">00:45:31.700</a></span> | <span class="t">There's improvements to this like dual DQN again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2736" target="_blank">00:45:36.100</a></span> | <span class="t">The Q function can be decomposed which is useful into the value estimate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2741" target="_blank">00:45:41.140</a></span> | <span class="t">Of being in that state and what's called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2744" target="_blank">00:45:44.020</a></span> | <span class="t">And in future slides will be called advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2747" target="_blank">00:45:47.000</a></span> | <span class="t">So the advantage of taking action in that state the nice thing of the advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2751" target="_blank">00:45:51.960</a></span> | <span class="t">as a measure is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2754" target="_blank">00:45:54.900</a></span> | <span class="t">It's a measure of the action quality relative to the average</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2760" target="_blank">00:46:00.020</a></span> | <span class="t">Action that could be taken there. So if it's that's very useful advantage versus sort of raw reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2766" target="_blank">00:46:06.900</a></span> | <span class="t">Is that if all the actions you have to take are pretty good?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2770" target="_blank">00:46:10.100</a></span> | <span class="t">You want to know well how much better it is?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2772" target="_blank">00:46:12.980</a></span> | <span class="t">in terms of optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2775" target="_blank">00:46:15.620</a></span> | <span class="t">That's a better measure for choosing actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2778" target="_blank">00:46:18.740</a></span> | <span class="t">in a value-based sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2781" target="_blank">00:46:21.540</a></span> | <span class="t">So when you have these two estimates you have these two streams for neural network and a dueling DQN</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2788" target="_blank">00:46:28.000</a></span> | <span class="t">DDQN where one estimates the value the other the advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2792" target="_blank">00:46:32.900</a></span> | <span class="t">And that's again that dueling nature is useful for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2799" target="_blank">00:46:39.120</a></span> | <span class="t">Also when the there are many states in which the action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2803" target="_blank">00:46:43.360</a></span> | <span class="t">Is decoupled the quality of the actions is decoupled from the state. So many states it doesn't matter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2810" target="_blank">00:46:50.720</a></span> | <span class="t">Which action you take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2814" target="_blank">00:46:54.000</a></span> | <span class="t">So you don't need to learn all the different complexities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2817" target="_blank">00:46:57.300</a></span> | <span class="t">All the topology of different actions when you in a particular state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2821" target="_blank">00:47:01.920</a></span> | <span class="t">And another one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2825" target="_blank">00:47:05.280</a></span> | <span class="t">Is prioritize experience replay like I said experience replay is really key to these algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2830" target="_blank">00:47:10.660</a></span> | <span class="t">And the thing that syncs some of the policy optimization methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2835" target="_blank">00:47:15.040</a></span> | <span class="t">And experience replay is collecting different memories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2838" target="_blank">00:47:18.500</a></span> | <span class="t">But if you just sample randomly in those memories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2843" target="_blank">00:47:23.520</a></span> | <span class="t">You're now affected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2845" target="_blank">00:47:25.520</a></span> | <span class="t">the sampled experiences are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2847" target="_blank">00:47:27.680</a></span> | <span class="t">Really affected by the frequency of those experience occurred not their importance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2852" target="_blank">00:47:32.720</a></span> | <span class="t">So prioritize experience replay assigns a priority</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2856" target="_blank">00:47:36.500</a></span> | <span class="t">a value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2858" target="_blank">00:47:38.320</a></span> | <span class="t">based on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2860" target="_blank">00:47:40.320</a></span> | <span class="t">magnitude of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2862" target="_blank">00:47:42.540</a></span> | <span class="t">temporal difference learned error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2864" target="_blank">00:47:44.640</a></span> | <span class="t">So the the stuff you have learned the most from is given a higher priority and therefore you get to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2872" target="_blank">00:47:52.480</a></span> | <span class="t">through the experience replay process that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2874" target="_blank">00:47:54.800</a></span> | <span class="t">That particular experience more often</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2878" target="_blank">00:47:58.320</a></span> | <span class="t">Okay, moving on to policy gradients this is on policy versus Q learning off policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2887" target="_blank">00:48:07.760</a></span> | <span class="t">Policy gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2891" target="_blank">00:48:11.360</a></span> | <span class="t">Is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2893" target="_blank">00:48:13.040</a></span> | <span class="t">Directly optimizing the policy where the input is the raw pixels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2896" target="_blank">00:48:16.640</a></span> | <span class="t">And the policy network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2899" target="_blank">00:48:19.360</a></span> | <span class="t">represents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2901" target="_blank">00:48:21.920</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2903" target="_blank">00:48:23.520</a></span> | <span class="t">Forms of representations of that environment space and its output produces a stochastic estimate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2909" target="_blank">00:48:29.280</a></span> | <span class="t">A probability of the different actions here in the pong the pixels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2913" target="_blank">00:48:33.460</a></span> | <span class="t">A single output that produces a probability of moving the paddle up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2918" target="_blank">00:48:38.800</a></span> | <span class="t">So how do policy gradients vanilla policy gradient very basic works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2923" target="_blank">00:48:43.120</a></span> | <span class="t">Is you unroll the environment you play through the environment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2929" target="_blank">00:48:49.600</a></span> | <span class="t">Here pong moving the paddle up and down and so on collecting no rewards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2934" target="_blank">00:48:54.100</a></span> | <span class="t">And only collecting reward at the very end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2937" target="_blank">00:48:57.840</a></span> | <span class="t">Based on whether you win or lose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2941" target="_blank">00:49:01.760</a></span> | <span class="t">Every single action you're taking along the way gets either punished or rewarded based on whether it led to victory or defeat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2947" target="_blank">00:49:07.680</a></span> | <span class="t">This also is remarkable that this works at all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2953" target="_blank">00:49:13.120</a></span> | <span class="t">because the credit assignment there is a is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2957" target="_blank">00:49:17.680</a></span> | <span class="t">I mean every single thing you did along the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2960" target="_blank">00:49:20.080</a></span> | <span class="t">Is averaged out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2963" target="_blank">00:49:23.520</a></span> | <span class="t">It's like muddied. It's the reason that policy gradient methods are more inefficient, but it's still very surprising that it works at all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2970" target="_blank">00:49:30.800</a></span> | <span class="t">So the pros versus DQN the value-based methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2975" target="_blank">00:49:35.920</a></span> | <span class="t">Is that if the world is so messy that you can't learn a Q function the nice thing about policy gradient because it's learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2982" target="_blank">00:49:42.080</a></span> | <span class="t">The policy directly that it will at least learn a pretty good policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2986" target="_blank">00:49:46.480</a></span> | <span class="t">Usually in many cases faster convergence. It's able to deal with stochastic policies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2991" target="_blank">00:49:51.120</a></span> | <span class="t">So value-based methods cannot learn stochastic policies and it's much more naturally able to deal with continuous actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=2997" target="_blank">00:49:57.920</a></span> | <span class="t">The cons is it's inefficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3001" target="_blank">00:50:01.380</a></span> | <span class="t">versus DQN</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3004" target="_blank">00:50:04.000</a></span> | <span class="t">it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3005" target="_blank">00:50:05.360</a></span> | <span class="t">It can become highly unstable as we'll talk about some solutions to this during the training process and the credit assignment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3013" target="_blank">00:50:13.040</a></span> | <span class="t">So if we look at the chain of actions that lead to a positive reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3017" target="_blank">00:50:17.600</a></span> | <span class="t">Some might be awesome actions. Some might be good actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3021" target="_blank">00:50:21.520</a></span> | <span class="t">Some might be terrible actions, but that doesn't matter as long as the destination was good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3026" target="_blank">00:50:26.400</a></span> | <span class="t">And that's then every single action along the way gets a positive reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3031" target="_blank">00:50:31.060</a></span> | <span class="t">That's the downside and there's now improvements to that advantage actor critic methods A2C combining the best of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3042" target="_blank">00:50:42.880</a></span> | <span class="t">Value-based methods and policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3045" target="_blank">00:50:45.120</a></span> | <span class="t">based methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3048" target="_blank">00:50:48.320</a></span> | <span class="t">So having an actor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3050" target="_blank">00:50:50.560</a></span> | <span class="t">two networks an actor which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3053" target="_blank">00:50:53.440</a></span> | <span class="t">Policy-based and that's the one that takes the actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3056" target="_blank">00:50:56.880</a></span> | <span class="t">Samples the actions from the policy network and the critic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3060" target="_blank">00:51:00.880</a></span> | <span class="t">That measures how good those actions are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3063" target="_blank">00:51:03.520</a></span> | <span class="t">And the critic is value-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3066" target="_blank">00:51:06.640</a></span> | <span class="t">All right. So as opposed to in the policy update the first equation there the reward coming from the destination</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3073" target="_blank">00:51:13.300</a></span> | <span class="t">The the reward being from whether you won the game or not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3077" target="_blank">00:51:17.200</a></span> | <span class="t">Every single step along the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3079" target="_blank">00:51:19.920</a></span> | <span class="t">You now learn a Q value function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3082" target="_blank">00:51:22.800</a></span> | <span class="t">QSA state and action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3085" target="_blank">00:51:25.740</a></span> | <span class="t">using the critic network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3088" target="_blank">00:51:28.480</a></span> | <span class="t">So you're able to now learn about the environment about evaluating your own actions at every step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3095" target="_blank">00:51:35.280</a></span> | <span class="t">So you're much more sample efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3097" target="_blank">00:51:37.280</a></span> | <span class="t">There's asynchronous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3099" target="_blank">00:51:39.280</a></span> | <span class="t">From deep mind and synchronous from open ai variants of this but of the actor advantage actor critic framework</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3107" target="_blank">00:51:47.040</a></span> | <span class="t">But both are highly parallelizable the difference with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3110" target="_blank">00:51:50.800</a></span> | <span class="t">A3C the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3115" target="_blank">00:51:55.280</a></span> | <span class="t">asynchronous one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3117" target="_blank">00:51:57.200</a></span> | <span class="t">Is that every single agent so you just throw these agents operating in the environment and they're learning they're rolling out the games and getting the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3125" target="_blank">00:52:05.840</a></span> | <span class="t">They're updating the original network asynchronously the global network parameters asynchronously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3132" target="_blank">00:52:12.420</a></span> | <span class="t">And as a result, they're also operating constantly on outdated versions of that network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3138" target="_blank">00:52:18.960</a></span> | <span class="t">The open ai approach that fixes this is that there's a coordinator that there's these rounds where everybody</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3146" target="_blank">00:52:26.320</a></span> | <span class="t">All the agents in parallel are rolling out the episode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3150" target="_blank">00:52:30.240</a></span> | <span class="t">but then the coordinator waits for everybody to finish in order to make the update to the global network and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3156" target="_blank">00:52:36.080</a></span> | <span class="t">distributes all the same parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3158" target="_blank">00:52:38.620</a></span> | <span class="t">To all the agents and so that means that every iteration starts with the same global parameters and that has really nice properties</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3166" target="_blank">00:52:46.880</a></span> | <span class="t">in terms of conversions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3169" target="_blank">00:52:49.520</a></span> | <span class="t">and stability of the training process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3171" target="_blank">00:52:51.520</a></span> | <span class="t">Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3173" target="_blank">00:52:53.200</a></span> | <span class="t">from google deep mind the deep deterministic policy gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3177" target="_blank">00:52:57.520</a></span> | <span class="t">Is combining the ideas of dqn but dealing with continuous action spaces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3183" target="_blank">00:53:03.120</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3185" target="_blank">00:53:05.360</a></span> | <span class="t">taking a policy network, but instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3187" target="_blank">00:53:07.360</a></span> | <span class="t">the actor actor critic framework</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3190" target="_blank">00:53:10.640</a></span> | <span class="t">but instead of picking a stochastic policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3195" target="_blank">00:53:15.040</a></span> | <span class="t">Having the actor operating in a stochastic nature is picking the best picking a deterministic policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3201" target="_blank">00:53:21.440</a></span> | <span class="t">So it's always choosing the best action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3205" target="_blank">00:53:25.760</a></span> | <span class="t">But okay with that the problem quite naturally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3209" target="_blank">00:53:29.060</a></span> | <span class="t">Is that when the policy is now deterministic it's able to do a continuous action space, but because it's deterministic it's never exploring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3216" target="_blank">00:53:36.580</a></span> | <span class="t">So the way we inject exploration into the system is by adding noise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3220" target="_blank">00:53:40.960</a></span> | <span class="t">either adding noise into the action space on the output or adding noise into the parameters of the network that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3227" target="_blank">00:53:47.200</a></span> | <span class="t">Have then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3230" target="_blank">00:53:50.720</a></span> | <span class="t">that create perturbations in the actions such that the final result is that you try different kinds of things and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3237" target="_blank">00:53:57.120</a></span> | <span class="t">The scale of the noise just like with the epsilon greedy in the exploration for dqn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3241" target="_blank">00:54:01.680</a></span> | <span class="t">The scale of the noise decreases as you learn more and more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3244" target="_blank">00:54:04.240</a></span> | <span class="t">so on the policy optimization side from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3247" target="_blank">00:54:07.760</a></span> | <span class="t">Openai and others</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3250" target="_blank">00:54:10.880</a></span> | <span class="t">We'll do a lecture just on this there's been a lot of exciting work here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3256" target="_blank">00:54:16.800</a></span> | <span class="t">the basic idea of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3259" target="_blank">00:54:19.980</a></span> | <span class="t">optimization on policy optimization with ppo and trpo</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3263" target="_blank">00:54:23.120</a></span> | <span class="t">is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3265" target="_blank">00:54:25.760</a></span> | <span class="t">First of all, we want to formulate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3267" target="_blank">00:54:27.760</a></span> | <span class="t">A reinforcement learning as purely an optimization problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3273" target="_blank">00:54:33.520</a></span> | <span class="t">And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3276" target="_blank">00:54:36.640</a></span> | <span class="t">second of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3278" target="_blank">00:54:38.080</a></span> | <span class="t">if policy optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3280" target="_blank">00:54:40.080</a></span> | <span class="t">the actions you take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3282" target="_blank">00:54:42.320</a></span> | <span class="t">Influences the rest of your the optimization process. You have to be very careful about the actions you take in particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3290" target="_blank">00:54:50.660</a></span> | <span class="t">You have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3292" target="_blank">00:54:52.020</a></span> | <span class="t">Avoid taking really bad actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3294" target="_blank">00:54:54.020</a></span> | <span class="t">When your convergence the the training performance in general</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3298" target="_blank">00:54:58.260</a></span> | <span class="t">collapses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3300" target="_blank">00:55:00.580</a></span> | <span class="t">So, how do we do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3302" target="_blank">00:55:02.100</a></span> | <span class="t">There's the line search methods, which is where gradient descent or gradient ascent falls under which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3307" target="_blank">00:55:07.700</a></span> | <span class="t">Which is the how we train deep neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3311" target="_blank">00:55:11.460</a></span> | <span class="t">is you first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3314" target="_blank">00:55:14.020</a></span> | <span class="t">Pick a direction of the gradient and then pick the step size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3319" target="_blank">00:55:19.620</a></span> | <span class="t">The problem with that is that can get you into trouble here. There's a nice visualization walking along a ridge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3326" target="_blank">00:55:26.420</a></span> | <span class="t">Is it can it can result in you stepping off that ridge again the collapsing of the training process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3333" target="_blank">00:55:33.620</a></span> | <span class="t">the performance the trust region is is the underlying idea here for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3339" target="_blank">00:55:39.380</a></span> | <span class="t">For the policy optimization methods that first pick the step size so the constraint in various kinds of ways the the magnitude</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3348" target="_blank">00:55:48.100</a></span> | <span class="t">Of the difference to to the weights that's applied and then the direction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3352" target="_blank">00:55:52.980</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3355" target="_blank">00:55:55.140</a></span> | <span class="t">Placing a much higher priority not choosing bad actions that can throw you off the optimization path trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3361" target="_blank">00:56:01.220</a></span> | <span class="t">We should take to that path</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3363" target="_blank">00:56:03.060</a></span> | <span class="t">and finally the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3364" target="_blank">00:56:04.740</a></span> | <span class="t">On the model-based methods and we'll also talk about them in the robotics side. There's a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3369" target="_blank">00:56:09.380</a></span> | <span class="t">interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3371" target="_blank">00:56:11.120</a></span> | <span class="t">approaches now where deep learning is starting to be used for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3375" target="_blank">00:56:15.140</a></span> | <span class="t">Model-based methods when the model has to be learned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3377" target="_blank">00:56:17.620</a></span> | <span class="t">But of course when the model doesn't have to be learned is given inherent to the game</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3382" target="_blank">00:56:22.100</a></span> | <span class="t">You know the model like in go and chess and so on alpha zero has really done incredible stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3388" target="_blank">00:56:28.500</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3390" target="_blank">00:56:30.660</a></span> | <span class="t">What's why is what is the model here? So the way that a lot of these games are approached</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3396" target="_blank">00:56:36.340</a></span> | <span class="t">You know game of go it's turn-based one person goes and another person goes and there's this game tree</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3402" target="_blank">00:56:42.260</a></span> | <span class="t">At every point there's a set of actions that can be taken and quickly if you look at that game tree</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3406" target="_blank">00:56:46.900</a></span> | <span class="t">It's it becomes you know, it grows exponentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3410" target="_blank">00:56:50.020</a></span> | <span class="t">So it becomes huge the game of go is the hugest of all in terms of because the number of choices you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3415" target="_blank">00:56:55.620</a></span> | <span class="t">Is the largest and there's chess</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3418" target="_blank">00:56:58.340</a></span> | <span class="t">And then, you know, it gets to checkers and then tic-tac-toe and it's just the the degree at every step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3424" target="_blank">00:57:04.580</a></span> | <span class="t">increases decreases based on the game structure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3428" target="_blank">00:57:08.180</a></span> | <span class="t">And so the task for neural network there is to learn the quality of the board. It's to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3434" target="_blank">00:57:14.100</a></span> | <span class="t">which boards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3436" target="_blank">00:57:16.820</a></span> | <span class="t">which game positions are most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3439" target="_blank">00:57:19.540</a></span> | <span class="t">likely to result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3442" target="_blank">00:57:22.420</a></span> | <span class="t">in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3444" target="_blank">00:57:24.100</a></span> | <span class="t">Most useful to explore and result in a highly successful state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3448" target="_blank">00:57:28.500</a></span> | <span class="t">So that choice of what's good to explore what what branch is good to go down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3453" target="_blank">00:57:33.940</a></span> | <span class="t">Is where we can have neural networks step in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3456" target="_blank">00:57:36.980</a></span> | <span class="t">And with alpha go it was pre-trained the first success that beat the world champion was pre-trained on expert games</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3464" target="_blank">00:57:44.580</a></span> | <span class="t">then with alpha go zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3467" target="_blank">00:57:47.140</a></span> | <span class="t">It was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3470" target="_blank">00:57:50.580</a></span> | <span class="t">No pre-training on expert systems. So no imitation learning is just purely through self-play through suggesting through playing itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3478" target="_blank">00:57:58.900</a></span> | <span class="t">New board positions many of these systems use Monte Carlo tree search and during this search</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3485" target="_blank">00:58:05.120</a></span> | <span class="t">balancing exploitation exploration so going deep on promising positions based on the estimation in neural network or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3491" target="_blank">00:58:11.120</a></span> | <span class="t">With a flip of a coin playing underplayed positions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3495" target="_blank">00:58:15.380</a></span> | <span class="t">And so this kind of here you could think of as an intuition of looking at a board and estimating how good that board is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3504" target="_blank">00:58:24.320</a></span> | <span class="t">And also estimating how good that board is likely to lead to victory down the end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3511" target="_blank">00:58:31.520</a></span> | <span class="t">So estimating just general quality and probability of leading to victory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3515" target="_blank">00:58:35.680</a></span> | <span class="t">then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3517" target="_blank">00:58:37.120</a></span> | <span class="t">The next step forward is alpha zero using the same similar architecture with mcts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3523" target="_blank">00:58:43.380</a></span> | <span class="t">Monte Carlo tree search but applying it to different games</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3527" target="_blank">00:58:47.360</a></span> | <span class="t">And applying it and competing against</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3530" target="_blank">00:58:50.560</a></span> | <span class="t">other engines state-of-the-art engines in go and shogi in chess</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3535" target="_blank">00:58:55.760</a></span> | <span class="t">And outperforming them with very few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3538" target="_blank">00:58:58.640</a></span> | <span class="t">very few steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3540" target="_blank">00:59:00.480</a></span> | <span class="t">so here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3542" target="_blank">00:59:02.480</a></span> | <span class="t">This model-based approaches which are really extremely simple efficient if you can construct such a model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3548" target="_blank">00:59:08.800</a></span> | <span class="t">And in in robotics if you can learn such a model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3552" target="_blank">00:59:12.160</a></span> | <span class="t">It can be exceptionally powerful here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3555" target="_blank">00:59:15.760</a></span> | <span class="t">Beating the the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3559" target="_blank">00:59:19.660</a></span> | <span class="t">Engines which are far superior to humans already stockfish can destroy most humans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3564" target="_blank">00:59:24.320</a></span> | <span class="t">on earth at the game of chess</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3567" target="_blank">00:59:27.360</a></span> | <span class="t">the ability through learning through through estimating the quality of a board to be able to defeat these engines is incredible and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3574" target="_blank">00:59:34.400</a></span> | <span class="t">the exciting aspect here versus engines that don't use neural networks is that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3582" target="_blank">00:59:42.160</a></span> | <span class="t">number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3584" target="_blank">00:59:44.320</a></span> | <span class="t">It really has to do with based on the neural network you explore certain positions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3590" target="_blank">00:59:50.100</a></span> | <span class="t">You explore certain parts of the tree</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3593" target="_blank">00:59:53.840</a></span> | <span class="t">And if you look at grandmasters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3597" target="_blank">00:59:57.440</a></span> | <span class="t">human players</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3599" target="_blank">00:59:59.440</a></span> | <span class="t">In chess, they seem to explore very few moves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3603" target="_blank">01:00:03.120</a></span> | <span class="t">They have a really good neural network at estimating which are the likely branches which would provide value to explore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3611" target="_blank">01:00:11.120</a></span> | <span class="t">and on the other side</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3614" target="_blank">01:00:14.080</a></span> | <span class="t">Stockfish and so on are much more brute force in their estimation for the mcts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3619" target="_blank">01:00:19.620</a></span> | <span class="t">And then alpha zero is a step towards the grandmaster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3623" target="_blank">01:00:23.600</a></span> | <span class="t">Because the number of branches need to be explored is much much fewer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3626" target="_blank">01:00:26.480</a></span> | <span class="t">A lot of the work is done in the representation formed by the neural network, which is super exciting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3632" target="_blank">01:00:32.480</a></span> | <span class="t">And then it's able to outperform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3634" target="_blank">01:00:34.960</a></span> | <span class="t">Stockfish and chess. It's able to outperform elmo and shogi and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3640" target="_blank">01:00:40.080</a></span> | <span class="t">It's itself in go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3643" target="_blank">01:00:43.600</a></span> | <span class="t">Or the previous iterations of alpha goes zero and so on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3647" target="_blank">01:00:47.280</a></span> | <span class="t">Now the challenge here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3653" target="_blank">01:00:53.440</a></span> | <span class="t">The sobering truth is that majority of real world application</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3657" target="_blank">01:00:57.220</a></span> | <span class="t">Of agents that have to act in this world perceive the world and act in this world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3661" target="_blank">01:01:01.680</a></span> | <span class="t">Are for the most part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3664" target="_blank">01:01:04.000</a></span> | <span class="t">Not based have no rl involved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3666" target="_blank">01:01:06.480</a></span> | <span class="t">So the action is not learned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3669" target="_blank">01:01:09.680</a></span> | <span class="t">You use neural networks to perceive certain aspects of the world, but ultimately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3673" target="_blank">01:01:13.940</a></span> | <span class="t">the action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3676" target="_blank">01:01:16.720</a></span> | <span class="t">Is not is not learned from data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3679" target="_blank">01:01:19.520</a></span> | <span class="t">That's true for all most of the autonomous vehicle companies or all of the autonomous vehicle companies operating today</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3685" target="_blank">01:01:25.680</a></span> | <span class="t">and it's true for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3688" target="_blank">01:01:28.320</a></span> | <span class="t">robotic manipulation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3690" target="_blank">01:01:30.220</a></span> | <span class="t">Industrial robotics and any of the humanoid robots have to navigate in this world under uncertain conditions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3695" target="_blank">01:01:35.780</a></span> | <span class="t">All the work from boston dynamics doesn't involve any machine learning as far as we know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3700" target="_blank">01:01:40.080</a></span> | <span class="t">Now that's beginning to change here with animal the the recent development</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3709" target="_blank">01:01:49.360</a></span> | <span class="t">Where the certain aspects of the control</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3712" target="_blank">01:01:52.640</a></span> | <span class="t">Of robotics is being learned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3715" target="_blank">01:01:55.280</a></span> | <span class="t">You're trying to learn more efficient movement. You're trying to learn more robust movement on top of the other controllers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3721" target="_blank">01:02:01.940</a></span> | <span class="t">So it's quite exciting through rl to be able to learn some of the control dynamics here. That's able to teach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3729" target="_blank">01:02:09.040</a></span> | <span class="t">Uh this particular robot to be able to get up from arbitrary positions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3733" target="_blank">01:02:13.780</a></span> | <span class="t">So it's less hard coding in order to be able to deal with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3737" target="_blank">01:02:17.740</a></span> | <span class="t">uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3739" target="_blank">01:02:19.740</a></span> | <span class="t">Unexpected initial conditions and unexpected perturbations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3742" target="_blank">01:02:22.820</a></span> | <span class="t">So it's exciting there in terms of learning the control dynamics and some of the driving policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3749" target="_blank">01:02:29.280</a></span> | <span class="t">So making behavioral driving behavior decisions changing lanes turning and so on that if you uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3756" target="_blank">01:02:36.400</a></span> | <span class="t">If you were here last week heard from waymo</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3758" target="_blank">01:02:38.960</a></span> | <span class="t">They they're starting to use some rl in terms of the driving policy in order to especially predict the future</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3764" target="_blank">01:02:44.560</a></span> | <span class="t">They're trying to anticipate intent modeling predict where the pedestrians where the cars are going to be based in the environment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3770" target="_blank">01:02:50.000</a></span> | <span class="t">They're trying to unroll what's happened recently into the future and beginning to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3774" target="_blank">01:02:54.720</a></span> | <span class="t">Move beyond sort of pure end-to-end on nvidia end-to-end learning approach of the control decisions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3782" target="_blank">01:03:02.480</a></span> | <span class="t">Are actually moving to rl and making long-term planning decisions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3786" target="_blank">01:03:06.660</a></span> | <span class="t">but again, the challenge is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3792" target="_blank">01:03:12.800</a></span> | <span class="t">The gap the leap needed to go from simulation to real world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3796" target="_blank">01:03:16.560</a></span> | <span class="t">all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3798" target="_blank">01:03:18.080</a></span> | <span class="t">Most of the work is done from the design of the environment and the design of the reward structure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3802" target="_blank">01:03:22.480</a></span> | <span class="t">And because most of that work now is in simulation. We need to either develop better algorithms for transfer learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3809" target="_blank">01:03:29.040</a></span> | <span class="t">or close the distance between simulation and the real world and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3813" target="_blank">01:03:33.520</a></span> | <span class="t">Also, we could think outside the box a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3818" target="_blank">01:03:38.240</a></span> | <span class="t">I had the conversation with peter abeel recently one of the leading researchers in deep rl</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3822" target="_blank">01:03:42.640</a></span> | <span class="t">It kind of on the side quickly mentioned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3826" target="_blank">01:03:46.240</a></span> | <span class="t">The the idea is that we don't need to make simulation more realistic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3832" target="_blank">01:03:52.420</a></span> | <span class="t">What we could do is just create an infinite number of simulations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3838" target="_blank">01:03:58.180</a></span> | <span class="t">Or a very large number of simulations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3843" target="_blank">01:04:03.360</a></span> | <span class="t">And the naturally the regularization aspect of having all those simulations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3847" target="_blank">01:04:07.860</a></span> | <span class="t">Will make it so that our our reality is just another sample from those simulations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3852" target="_blank">01:04:12.900</a></span> | <span class="t">And so maybe the solution isn't to create higher fidelity simulation or to create transfer learning algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3859" target="_blank">01:04:19.140</a></span> | <span class="t">Maybe it's to build</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3861" target="_blank">01:04:21.920</a></span> | <span class="t">a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3864" target="_blank">01:04:24.240</a></span> | <span class="t">Arbitrary number of simulations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3866" target="_blank">01:04:26.700</a></span> | <span class="t">So then that step towards creating a agent that work that works in the real world is a trivial one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3873" target="_blank">01:04:33.680</a></span> | <span class="t">And maybe that's exactly whoever created the simulation we're living in and the multiverse that we're living in did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3880" target="_blank">01:04:40.640</a></span> | <span class="t">Next steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3885" target="_blank">01:04:45.200</a></span> | <span class="t">The lecture videos we have several in rl will be made all available on deeplearning.mit.edu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3890" target="_blank">01:04:50.580</a></span> | <span class="t">We'll have several tutorials in rl on github</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3894" target="_blank">01:04:54.400</a></span> | <span class="t">The link is there and I really like the essay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3898" target="_blank">01:04:58.800</a></span> | <span class="t">From openai on spinning up as a deep rl researcher, you know, if you're interested in getting into research in rl</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3905" target="_blank">01:05:05.360</a></span> | <span class="t">What are the steps you need to take from the background of developing the mathematical background prob stat and multivariate calculus?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3913" target="_blank">01:05:13.300</a></span> | <span class="t">To some of the basics like it's covered last week on deep learning some of the basics ideas in rl</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3919" target="_blank">01:05:19.040</a></span> | <span class="t">Just terminology and so on some basic concepts then picking a framework tensorflow or pytorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3925" target="_blank">01:05:25.540</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3927" target="_blank">01:05:27.280</a></span> | <span class="t">Learn by doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3929" target="_blank">01:05:29.040</a></span> | <span class="t">All right implement the algorithms I mentioned today. Those are the core rl algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3933" target="_blank">01:05:33.540</a></span> | <span class="t">So implement all of them from scratch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3936" target="_blank">01:05:36.080</a></span> | <span class="t">It should only take about 200 300 lines of code. They're actually when you put it down on paper are quite simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3942" target="_blank">01:05:42.720</a></span> | <span class="t">intuitive algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3944" target="_blank">01:05:44.960</a></span> | <span class="t">and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3946" target="_blank">01:05:46.720</a></span> | <span class="t">Read papers about those algorithms that follow after looking not for the big waving performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3954" target="_blank">01:05:54.000</a></span> | <span class="t">The hand waving performance but for the tricks that were used to train these algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3958" target="_blank">01:05:58.000</a></span> | <span class="t">The tricks tell a lot of the story and that's the useful parts that you need to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3963" target="_blank">01:06:03.280</a></span> | <span class="t">And iterate fast on simple benchmark environments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3967" target="_blank">01:06:07.200</a></span> | <span class="t">So openai gem has provided a lot of easy to use environments that you can play with that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3972" target="_blank">01:06:12.480</a></span> | <span class="t">You can train an agent in minutes hours as opposed to days and weeks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3977" target="_blank">01:06:17.200</a></span> | <span class="t">And so iterating fast is the best way to learn these algorithms and then on the research side</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3982" target="_blank">01:06:22.480</a></span> | <span class="t">There's three ways to get a best paper award, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3985" target="_blank">01:06:25.440</a></span> | <span class="t">To publish and to contribute and have an impact in the research community</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3991" target="_blank">01:06:31.120</a></span> | <span class="t">In rl one is improve an existing approach given a particular benchmark. There's a few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=3997" target="_blank">01:06:37.360</a></span> | <span class="t">Benchmark data sets environments that are emerging. So you want to improve an existing approach some aspect of the convergence of the performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4004" target="_blank">01:06:44.900</a></span> | <span class="t">You can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4006" target="_blank">01:06:46.400</a></span> | <span class="t">Focus on an unsolved task. There's certain games that just haven't been solved through the rl</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4012" target="_blank">01:06:52.560</a></span> | <span class="t">Formulation or you can come up with a totally new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4015" target="_blank">01:06:55.600</a></span> | <span class="t">Problem that hasn't been addressed by rl before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4019" target="_blank">01:06:59.200</a></span> | <span class="t">So with that I'd like to thank you very much tomorrow. I hope to see you here for deep traffic. Thanks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4026" target="_blank">01:07:06.980</a></span> | <span class="t">You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4029" target="_blank">01:07:09.700</a></span> | <span class="t">You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4031" target="_blank">01:07:11.700</a></span> | <span class="t">You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4034" target="_blank">01:07:14.660</a></span> | <span class="t">You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4036" target="_blank">01:07:16.660</a></span> | <span class="t">You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4038" target="_blank">01:07:18.660</a></span> | <span class="t">You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4040" target="_blank">01:07:20.660</a></span> | <span class="t">You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4042" target="_blank">01:07:22.660</a></span> | <span class="t">You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zR11FLZ-O9M&t=4044" target="_blank">01:07:24.660</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>