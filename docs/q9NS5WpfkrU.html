<html><head><title>Training BERT #1 - Masked-Language Modeling (MLM)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Training BERT #1 - Masked-Language Modeling (MLM)</h2><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU"><img src="https://i.ytimg.com/vi_webp/q9NS5WpfkrU/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=135">2:15</a> Walkthrough<br><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=262">4:22</a> Tutorial<br><br><div style="text-align: left;"><a href="./q9NS5WpfkrU.html">Whisper Transcript</a> | <a href="./transcript_q9NS5WpfkrU.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Okay, so in this video, we're going to have a look at what I think is the more interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=5" target="_blank">00:00:05.400</a></span> | <span class="t">side of transformers, which is how we actually train those. So typically with transforming,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=12" target="_blank">00:00:12.160</a></span> | <span class="t">what we do is we download a pre-trained model from Hugging Face. And then at that point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=18" target="_blank">00:00:18.400</a></span> | <span class="t">we can either use a pre-trained model as is, which in a lot of cases, it will be good enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=24" target="_blank">00:00:24.120</a></span> | <span class="t">to actually do that. But then at other times, we might want to actually fine-tune the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=32" target="_blank">00:00:32.560</a></span> | <span class="t">And that is what I'll be showing you how to do here. So core of BERT, there are two different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=40" target="_blank">00:00:40.760</a></span> | <span class="t">training or fine-tuning approaches that we can use. And we can even use both of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=45" target="_blank">00:00:45.920</a></span> | <span class="t">together. But for this video, what we're going to have a look at is how to use a mass language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=51" target="_blank">00:00:51.920</a></span> | <span class="t">modeling, which is called MLM. And MLM is really the, probably the most important of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=60" target="_blank">00:01:00.000</a></span> | <span class="t">those two core training approaches. The other one being next sentence prediction. So what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=68" target="_blank">00:01:08.760</a></span> | <span class="t">MLM is, is we essentially give BERT a input sequence. So like this, so this would be our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=79" target="_blank">00:01:19.440</a></span> | <span class="t">input sequence. And we ask BERT to predict the same input sequence as the output. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=87" target="_blank">00:01:27.720</a></span> | <span class="t">BERT will optimize the weights within its encoder layers in order to produce this output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=96" target="_blank">00:01:36.080</a></span> | <span class="t">Now obviously that's pretty easy. So what we do is we mask some, some random tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=105" target="_blank">00:01:45.200</a></span> | <span class="t">within the input. So here we might mask one. And what we do is replace that with another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=114" target="_blank">00:01:54.000</a></span> | <span class="t">token, which is a special token called a mask token, which looks like that. And when we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=122" target="_blank">00:02:02.400</a></span> | <span class="t">doing MLM, we would typically mask around 15% of the input tokens. So if we take a look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=135" target="_blank">00:02:15.840</a></span> | <span class="t">at how that looks, so this might look a little complex, but it's pretty straightforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=140" target="_blank">00:02:20.960</a></span> | <span class="t">So down here, we have our input from the previous slide. We process that through our tokenizer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=149" target="_blank">00:02:29.600</a></span> | <span class="t">like we normally would with transformers. And then in the middle here, I haven't drawn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=154" target="_blank">00:02:34.760</a></span> | <span class="t">it, but in the middle, there's a masking function. And that masking function will mask around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=162" target="_blank">00:02:42.320</a></span> | <span class="t">15% of the tokens in the input IDs tensor. So here we have a mask token and they will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=171" target="_blank">00:02:51.720</a></span> | <span class="t">then get processed by BERT in the middle here. And BERT will output a set of vectors, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=179" target="_blank">00:02:59.580</a></span> | <span class="t">all have the length 768. Usually there's, there's different BERT models. They have different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=187" target="_blank">00:03:07.480</a></span> | <span class="t">lengths. We'll go with the 768 here, and then we pass them through a feed forward network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=194" target="_blank">00:03:14.360</a></span> | <span class="t">and that will output our, our output logits up here. And each one of those is of the size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=205" target="_blank">00:03:25.440</a></span> | <span class="t">equal to the vocab size. And with this model, I think the vocab size is something around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=212" target="_blank">00:03:32.040</a></span> | <span class="t">I think three or 30,500, something like that. And then from there to get the predicted token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=221" target="_blank">00:03:41.260</a></span> | <span class="t">for each one of those logits, we apply a softmax function to get a probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=229" target="_blank">00:03:49.480</a></span> | <span class="t">And then we apply a argmax function, which is what you can see here. So this is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=236" target="_blank">00:03:56.280</a></span> | <span class="t">an example of one of those logits over here. We have the softmax, we get the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=243" target="_blank">00:04:03.500</a></span> | <span class="t">distribution, and then we apply our argmax to get our final token ID, which we can then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=250" target="_blank">00:04:10.500</a></span> | <span class="t">map or we can then decode using our tokenizer to get an actual word in English. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=258" target="_blank">00:04:18.880</a></span> | <span class="t">how it works. Let's have a look at how we actually do that in code. Okay, so first we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=265" target="_blank">00:04:25.200</a></span> | <span class="t">need to import everything we need. So we're using transformers here, where we're using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=269" target="_blank">00:04:29.960</a></span> | <span class="t">the BERT tokenizer and BERT format LM classes. And then we'll also be importing Torch as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=277" target="_blank">00:04:37.600</a></span> | <span class="t">well. So from transformers, import our tokenizer, and also our BERT for mass LM, which is MLM,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=294" target="_blank">00:04:54.240</a></span> | <span class="t">the mass language modeling. And then we also want to import Torch as well. Okay. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=306" target="_blank">00:05:06.280</a></span> | <span class="t">what I want to do here is initialize our two models, well our tokenizer and model. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=313" target="_blank">00:05:13.960</a></span> | <span class="t">I do that just as we normally would with Hugging Face transformers. So we do BERT tokenizer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=323" target="_blank">00:05:23.400</a></span> | <span class="t">from pre-trained. And here we have BERT base on case. And then we also want our model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=339" target="_blank">00:05:39.000</a></span> | <span class="t">which is BERT for mass LM. And this will also be from pre-trained. Again, using the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=349" target="_blank">00:05:49.560</a></span> | <span class="t">model, so BERT base on case. Okay. So that's our tokenizer and model. And I'm also going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=358" target="_blank">00:05:58.800</a></span> | <span class="t">to use this example text here. So we see here, so this should be election, this mask, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=369" target="_blank">00:06:09.840</a></span> | <span class="t">this one here should be attacked. Okay. Now, execute that. I've made a typo here. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=390" target="_blank">00:06:30.840</a></span> | <span class="t">And now what we want to do is actually tokenize that chunk of text. So to do that, we would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=399" target="_blank">00:06:39.880</a></span> | <span class="t">write inputs. We have our tokenizer, and all we do is pass our text in there. We're using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=405" target="_blank">00:06:45.840</a></span> | <span class="t">PyTorch here, so we want to return tensors, PT. Okay. And let's have a look at what tensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=414" target="_blank">00:06:54.840</a></span> | <span class="t">we return from that. So you see we have our input IDs, token type IDs, and attention mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=422" target="_blank">00:07:02.200</a></span> | <span class="t">Now we don't need to worry about token type IDs whatsoever for MLM. And attention mask,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=432" target="_blank">00:07:12.240</a></span> | <span class="t">MLM does use that, but I'm not going to go into any details. So all we want to focus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=439" target="_blank">00:07:19.440</a></span> | <span class="t">on this video is input IDs. So let's have a look at what we have there. So there's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=448" target="_blank">00:07:28.200</a></span> | <span class="t">few things that I want to point out. First, we have our special tokens. So we have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=453" target="_blank">00:07:33.880</a></span> | <span class="t">CLS or classified token here. We have the separated token, SCP. And we also have our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=461" target="_blank">00:07:41.080</a></span> | <span class="t">mass tokens, one here and one here. And everything in between are actual real tokens from our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=472" target="_blank">00:07:52.320</a></span> | <span class="t">text. So what we have now, we have our inputs. And what we do is use these inputs initially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=483" target="_blank">00:08:03.800</a></span> | <span class="t">to create our labels. But what I've done here is already amassed our inputs. So what I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=491" target="_blank">00:08:11.880</a></span> | <span class="t">going to do is just actually replace these with the actual words. So this is election.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=501" target="_blank">00:08:21.140</a></span> | <span class="t">And this one is attacked. So just rerun that and that. Okay. And now what we can do with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=513" target="_blank">00:08:33.860</a></span> | <span class="t">that is actually create our target labels. So the target labels needs to be contained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=521" target="_blank">00:08:41.700</a></span> | <span class="t">within a tensor called labels. Create like that. And it just needs to be a copy of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=531" target="_blank">00:08:51.700</a></span> | <span class="t">input IDs tensor. And to create a copy of that, we write detach. And then we clone it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=545" target="_blank">00:09:05.300</a></span> | <span class="t">Okay. So that creates our copy, which is not going to be connected to our input IDs. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=555" target="_blank">00:09:15.980</a></span> | <span class="t">now if we just have a look at our inputs, we can see input IDs at the top, and we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=561" target="_blank">00:09:21.340</a></span> | <span class="t">labels at the bottom. They're just copies. Okay. Now what we want to do is mask a random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=569" target="_blank">00:09:29.620</a></span> | <span class="t">number of input IDs or tokens within the input IDs tensor, but not the labels tensor. Now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=579" target="_blank">00:09:39.660</a></span> | <span class="t">to do that, what we can do is use the PyTorch random function. And using that, what we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=589" target="_blank">00:09:49.380</a></span> | <span class="t">do is create a random array of floats that have equal dimensions to input IDs tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=598" target="_blank">00:09:58.100</a></span> | <span class="t">So all we do is we pass input IDs dot shape into there. And if we can check the shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=607" target="_blank">00:10:07.600</a></span> | <span class="t">of it afterwards, we get this one by 62, which is equal to this here. And we can have a look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=616" target="_blank">00:10:16.820</a></span> | <span class="t">at what we have there. It's just a set of floats between zero and one. Now, if we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=625" target="_blank">00:10:25.180</a></span> | <span class="t">to select a random 15% of those, what we do is we'll create a new array, mask array, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=636" target="_blank">00:10:36.020</a></span> | <span class="t">this will be equal to rand where rand is greater than or less than 0.15. Okay. And this will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=651" target="_blank">00:10:51.540</a></span> | <span class="t">select 15% of those. And let me show you what that looks like. So this will create a Boolean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=658" target="_blank">00:10:58.740</a></span> | <span class="t">array and he'll say all of these faults and then these true values are, that's where we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=668" target="_blank">00:11:08.140</a></span> | <span class="t">put our mask tokens later on. Now there's one here and this one is covering our separator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=676" target="_blank">00:11:16.420</a></span> | <span class="t">token. Now we don't want to mask our separator or classifier token. We don't want to mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=681" target="_blank">00:11:21.340</a></span> | <span class="t">any special tokens. So what we can do is add an extra little bit of logic there, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=690" target="_blank">00:11:30.860</a></span> | <span class="t">will like this. So we do inputs, input IDs, and we say not equal to one zero one, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=703" target="_blank">00:11:43.220</a></span> | <span class="t">is our classifier token. And let's just have a look at what this looks like and see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=708" target="_blank">00:11:48.020</a></span> | <span class="t">now we get true for everything, except for my classifier token. And we multiply this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=714" target="_blank">00:11:54.980</a></span> | <span class="t">by the same rule, but for our separator token. So now you see that we have faults here and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=722" target="_blank">00:12:02.180</a></span> | <span class="t">faults here. Now all we need to do is add this to our mask array logic up here. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=731" target="_blank">00:12:11.180</a></span> | <span class="t">we also put brackets around this and this will make sure that these two are always faults</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=739" target="_blank">00:12:19.300</a></span> | <span class="t">no matter what. Okay. Now what I want to do is actually get the index positions of each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=750" target="_blank">00:12:30.220</a></span> | <span class="t">one of these true values and do that. We write torch flatten. So this is going to just flatten</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=757" target="_blank">00:12:37.980</a></span> | <span class="t">the tensor that we will get out from this next bit of code and maybe it would make sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=765" target="_blank">00:12:45.940</a></span> | <span class="t">Okay. Let's start from the first part of the code. So we're going to go mask array here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=775" target="_blank">00:12:55.260</a></span> | <span class="t">That gets us our mask array. We want to say non zero. And that will get us a vector of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=787" target="_blank">00:13:07.700</a></span> | <span class="t">indices where we have true values or non zero values. And what we want to do is convert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=794" target="_blank">00:13:14.340</a></span> | <span class="t">that into a list like that. But you see that we have a list within a list. So this is where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=804" target="_blank">00:13:24.420</a></span> | <span class="t">the torch flatten comes in. So we add another bracket around this and we do torch flatten.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=815" target="_blank">00:13:35.940</a></span> | <span class="t">And then we convert it to a list. And that gives us a list of indices where we have these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=822" target="_blank">00:13:42.380</a></span> | <span class="t">true values. So that's our selection. And now what we want to do is use that selection</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=837" target="_blank">00:13:57.780</a></span> | <span class="t">to select a certain number or select those specific indices within our input ID tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=852" target="_blank">00:14:12.540</a></span> | <span class="t">So we want to select the first part of that. So the zero index followed by selection. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=861" target="_blank">00:14:21.820</a></span> | <span class="t">we set those equal to one zero three. And then let's have a look and see if that works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=868" target="_blank">00:14:28.220</a></span> | <span class="t">So one zero three is our mask token, by the way. And you can see here now we have those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=876" target="_blank">00:14:36.900</a></span> | <span class="t">mask tokens in those positions. So we just masked random, roughly 15% of those tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=886" target="_blank">00:14:46.140</a></span> | <span class="t">And then from there, we can pass all of this into our model and the model will calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=891" target="_blank">00:14:51.500</a></span> | <span class="t">out loss and the logits that we saw before. So we do that as we normally would when we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=898" target="_blank">00:14:58.220</a></span> | <span class="t">using HuggingFace and Torch. So we have models, we pass our inputs as keyword arguments. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=909" target="_blank">00:15:09.540</a></span> | <span class="t">look at what output is given us. And we'll see we have these two tensors, we have loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=915" target="_blank">00:15:15.380</a></span> | <span class="t">and we have logits. Now, let's have a look at what that loss looks like. Okay, so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=923" target="_blank">00:15:23.300</a></span> | <span class="t">get this value here. So that is our loss. And of course, with that loss, we can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=929" target="_blank">00:15:29.440</a></span> | <span class="t">optimize our model. Okay, so that's how mass language modeling works. Now, when we're actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=941" target="_blank">00:15:41.740</a></span> | <span class="t">training a model using mass language modeling, obviously, the code is slightly different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=948" target="_blank">00:15:48.660</a></span> | <span class="t">But there's also a reasonable amount of depth that we need to go into for that. So I'm not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=954" target="_blank">00:15:54.540</a></span> | <span class="t">going to include in this video, but I am going to do a video on that, actually training a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=961" target="_blank">00:16:01.740</a></span> | <span class="t">model using mass language modeling pretty soon. And I'll leave a link to that in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=967" target="_blank">00:16:07.660</a></span> | <span class="t">description because I know some of you probably want to watch that to understand how to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=972" target="_blank">00:16:12.380</a></span> | <span class="t">train your own models using this. But that's it for this video. I hope it's been useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=q9NS5WpfkrU&t=980" target="_blank">00:16:20.260</a></span> | <span class="t">And I will see you again in the next one.</span></div></div></body></html>