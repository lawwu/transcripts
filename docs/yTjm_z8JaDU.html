<html><head><title>Survey Paper Club: Long Context, Reasoning Economy, Leaderboard Illusion</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Survey Paper Club: Long Context, Reasoning Economy, Leaderboard Illusion</h2><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU"><img src="https://i.ytimg.com/vi/yTjm_z8JaDU/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./yTjm_z8JaDU.html">Whisper Transcript</a> | <a href="./transcript_yTjm_z8JaDU.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Go first, then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2" target="_blank">00:00:02.080</a></span> | <span class="t">Nope.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3" target="_blank">00:00:03.860</a></span> | <span class="t">All right, so I've been thinking a lot about Q&A evals,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=9" target="_blank">00:00:09.000</a></span> | <span class="t">questions I'm answering, especially for long context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=11" target="_blank">00:00:11.620</a></span> | <span class="t">So I've gone through maybe about 12, 18 papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=15" target="_blank">00:00:15.320</a></span> | <span class="t">I wanted to share with you what I thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=19" target="_blank">00:00:19.080</a></span> | <span class="t">was really good from these papers as well as some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=21" target="_blank">00:00:21.240</a></span> | <span class="t">of the key highlights and how I thought about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=23" target="_blank">00:00:23.620</a></span> | <span class="t">I think first we'll start with narrative Q&A.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=26" target="_blank">00:00:26.500</a></span> | <span class="t">And don't worry, I will actually try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=27" target="_blank">00:00:27.920</a></span> | <span class="t">to extract all the links for these five papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=30" target="_blank">00:00:30.920</a></span> | <span class="t">and I'll share it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=32" target="_blank">00:00:32.020</a></span> | <span class="t">So don't try to look for it, just focus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=34" target="_blank">00:00:34.040</a></span> | <span class="t">on what was important in here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=37" target="_blank">00:00:37.460</a></span> | <span class="t">And I can't monitor the chat, so yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=40" target="_blank">00:00:40.400</a></span> | <span class="t">Just stop me anytime.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=41" target="_blank">00:00:41.780</a></span> | <span class="t">So this is the first paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=43" target="_blank">00:00:43.400</a></span> | <span class="t">So you might think, Eugene, it's from 2017.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=47" target="_blank">00:00:47.720</a></span> | <span class="t">Is it still relevant?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=48" target="_blank">00:00:48.980</a></span> | <span class="t">I personally think this is the OG.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=52" target="_blank">00:00:52.060</a></span> | <span class="t">This is a really, really good paper from DeepMind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=56" target="_blank">00:00:56.900</a></span> | <span class="t">So let's go through it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=57" target="_blank">00:00:57.960</a></span> | <span class="t">For each paper, we'll talk about the data set, the methodology, and then how it helps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=63" target="_blank">00:01:03.020</a></span> | <span class="t">So this data set consists of stories, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=66" target="_blank">00:01:06.140</a></span> | <span class="t">So it's both books and movie scripts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=68" target="_blank">00:01:08.860</a></span> | <span class="t">And it's with human written questions based on human abstractive summaries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=74" target="_blank">00:01:14.680</a></span> | <span class="t">So what does this mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=76" target="_blank">00:01:16.420</a></span> | <span class="t">Essentially, from the book and from the movie, we'll generate an abstract summary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=80" target="_blank">00:01:20.800</a></span> | <span class="t">Then, based on the summary, solely on the summary, we try to generate a question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=85" target="_blank">00:01:25.740</a></span> | <span class="t">What this means is that we do not deliberately generate questions that are just extractive from the text, whereby you just answer solely based on the text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=97" target="_blank">00:01:37.900</a></span> | <span class="t">It requires further thinking because it's based on the abstract and therefore requires evidence across the entire book.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=107" target="_blank">00:01:47.140</a></span> | <span class="t">So, for example, here's a question, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=109" target="_blank">00:01:49.100</a></span> | <span class="t">How is Oscar related to Dana?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=111" target="_blank">00:01:51.420</a></span> | <span class="t">The answer is that it's her son.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=113" target="_blank">00:01:53.260</a></span> | <span class="t">So here's the summary snippet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=114" target="_blank">00:01:54.480</a></span> | <span class="t">Peter's former girlfriend had a son, Dana.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=116" target="_blank">00:01:56.380</a></span> | <span class="t">And then, you know, throughout the book, this is the movie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=120" target="_blank">00:02:00.200</a></span> | <span class="t">That's a good looking kid you got there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=121" target="_blank">00:02:01.520</a></span> | <span class="t">Like, based on this, you have to make the inference that this is the son.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=124" target="_blank">00:02:04.920</a></span> | <span class="t">And, you know, they were already thinking about this in 2017, before HIVT could do even anything like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=131" target="_blank">00:02:11.640</a></span> | <span class="t">So the data set 150, I mean, 1,500 stories from Project Gutenberg and movie scripts, script from the web.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=144" target="_blank">00:02:24.860</a></span> | <span class="t">So they ask annotators to provide them with the summaries, essentially what I just mentioned, so that they don't just pick localized questions that can be identified from the local question itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=155" target="_blank">00:02:35.920</a></span> | <span class="t">There are a few things here that we don't really like, but couldn't, can't be helped.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=159" target="_blank">00:02:39.660</a></span> | <span class="t">This was probably stay out of the eye at that point in time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=161" target="_blank">00:02:41.620</a></span> | <span class="t">They use blue, meaty, and rouge metric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=163" target="_blank">00:02:43.460</a></span> | <span class="t">And they provide two golden references for each of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=167" target="_blank">00:02:47.300</a></span> | <span class="t">We'll see that this actually doesn't work very well in the future papers we go through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=173" target="_blank">00:02:53.320</a></span> | <span class="t">Yeah, so that's the, oh, go, go for it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=177" target="_blank">00:02:57.180</a></span> | <span class="t">Why we don't like these metrics, blue and rouge and stuff?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=180" target="_blank">00:03:00.120</a></span> | <span class="t">That's a good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=181" target="_blank">00:03:01.480</a></span> | <span class="t">What we found is that they don't really correlate well with human judgments of whether an answer is good or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=190" target="_blank">00:03:10.060</a></span> | <span class="t">Because it's purely engram based.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=194" target="_blank">00:03:14.080</a></span> | <span class="t">I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=194" target="_blank">00:03:14.840</a></span> | <span class="t">So how are you judging whether answers are good or not?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=198" target="_blank">00:03:18.680</a></span> | <span class="t">Is it like a factual QA?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=200" target="_blank">00:03:20.840</a></span> | <span class="t">Or is it like, you know, this is something that is like the right tone of what I would want an answer to be?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=207" target="_blank">00:03:27.640</a></span> | <span class="t">How I think about it is mostly factual, summarization, inferential.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=213" target="_blank">00:03:33.420</a></span> | <span class="t">We don't really go into the, I don't really think about the tone or the style.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=218" target="_blank">00:03:38.680</a></span> | <span class="t">I think right now, what I think is probably going to be good, there's probably going to be two sets of metrics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=225" target="_blank">00:03:45.160</a></span> | <span class="t">There's going to be reference-free metrics and reference-based.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=228" target="_blank">00:03:48.460</a></span> | <span class="t">What does this mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=229" target="_blank">00:03:49.140</a></span> | <span class="t">Reference-based means that you have a right answer, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=232" target="_blank">00:03:52.960</a></span> | <span class="t">You have the right answer, and you're just checking to see if it matches the right answer a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=236" target="_blank">00:03:56.560</a></span> | <span class="t">I think that's very straightforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=237" target="_blank">00:03:57.820</a></span> | <span class="t">Reference-free metric is that there may be no right answer, but you just want to check whether it's good enough a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=243" target="_blank">00:04:03.580</a></span> | <span class="t">So an example of a reference-based metric is this, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=248" target="_blank">00:04:08.560</a></span> | <span class="t">How are they related?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=250" target="_blank">00:04:10.440</a></span> | <span class="t">It's her son.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=251" target="_blank">00:04:11.040</a></span> | <span class="t">Maybe if the answer is it's her child or saying something like, oh, she is his mom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=262" target="_blank">00:04:22.580</a></span> | <span class="t">If the answer is not, even if the answer is not in the right way, if you use an LLM evaluator, it's able to say that, hey, no, this is a hit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=271" target="_blank">00:04:31.440</a></span> | <span class="t">An example of a reference-free metric could be a summarization whereby we already know LLMs already surpass human performance in terms of summarization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=281" target="_blank">00:04:41.140</a></span> | <span class="t">So it could be that here's a summary, and then here's the text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=284" target="_blank">00:04:44.720</a></span> | <span class="t">Is this summary a faithful representation of the text?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=289" target="_blank">00:04:49.440</a></span> | <span class="t">So for those, there's an example in that, one of the future papers, one of the papers I'll be covering, that actually talks about this reference-free metric for evaluating faithfulness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=300" target="_blank">00:05:00.460</a></span> | <span class="t">Did I answer the question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=305" target="_blank">00:05:05.020</a></span> | <span class="t">So this paper is in the realm of fiction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=312" target="_blank">00:05:12.520</a></span> | <span class="t">Now let's go into non-fiction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=314" target="_blank">00:05:14.160</a></span> | <span class="t">Here's another paper that I quite like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=317" target="_blank">00:05:17.140</a></span> | <span class="t">Data set of information-seeking papers, anchored in research papers, by Allen Institute for AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=325" target="_blank">00:05:25.500</a></span> | <span class="t">Their approach is also very interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=330" target="_blank">00:05:30.880</a></span> | <span class="t">They have academics, read, title, and abstract, only title and abstract, and then ask questions off the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=338" target="_blank">00:05:38.580</a></span> | <span class="t">That's it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=339" target="_blank">00:05:39.620</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=340" target="_blank">00:05:40.800</a></span> | <span class="t">So it's a very similar approach whereby they force annotators to only look at limited data and then try to see if they can ask the question or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=353" target="_blank">00:05:53.080</a></span> | <span class="t">And of course, the reason is because when you give annotators the full text and you ask them to generate questions, they often will generate questions that can be answered just within one or two words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=361" target="_blank">00:06:01.820</a></span> | <span class="t">But by forcing annotators to only look at the summary data, you actually force them to generate questions that are more open-ended.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=369" target="_blank">00:06:09.680</a></span> | <span class="t">And in this case, this specific data set actually includes the evidence to arrive at that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=374" target="_blank">00:06:14.320</a></span> | <span class="t">And if you think about it, including the evidence, this evidence can be used in a retrieval evaluation pipeline, which is, did our retrieval find the right evidence for this question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=385" target="_blank">00:06:25.260</a></span> | <span class="t">So this is quite useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=387" target="_blank">00:06:27.620</a></span> | <span class="t">5,000 papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=390" target="_blank">00:06:30.720</a></span> | <span class="t">No, 5,000 questions on 1,500 papers on NLP, on NLP papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=395" target="_blank">00:06:35.780</a></span> | <span class="t">And I think they actually collected these questions themselves, the authors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=400" target="_blank">00:06:40.120</a></span> | <span class="t">So 55% of the questions require evidence for multiple paragraphs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=404" target="_blank">00:06:44.420</a></span> | <span class="t">And this is the only, this is why when you ask a question from the abstract, the abstract, because it condenses data from multiple paragraphs, you actually have to answer questions from those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=415" target="_blank">00:06:55.700</a></span> | <span class="t">And 13 of them are actually super challenging, requiring answers from tables and figures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=420" target="_blank">00:07:00.660</a></span> | <span class="t">So I think that's probably the main thing I want to answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=426" target="_blank">00:07:06.660</a></span> | <span class="t">So again, annotators, they don't just generate questions that cannot be answered, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=432" target="_blank">00:07:12.400</a></span> | <span class="t">First, they generate questions, then they try to answer this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=435" target="_blank">00:07:15.860</a></span> | <span class="t">So if the question is answerable, they select the minimal set of evidence snippets to try to answer it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=441" target="_blank">00:07:21.480</a></span> | <span class="t">And of course, if the question is not answerable, they just throw it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=444" target="_blank">00:07:24.780</a></span> | <span class="t">So in a sense, I think this data set is fairly carefully curated to ensure that all the answers actually can be answered, even though they took their approach whereby they focus on the summary first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=453" target="_blank">00:07:33.120</a></span> | <span class="t">And then there's a couple of questions that you can see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=459" target="_blank">00:07:39.200</a></span> | <span class="t">You can see that even when you do this, a lot of the questions are going to be extractive, which is you just extract from the text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=469" target="_blank">00:07:49.920</a></span> | <span class="t">What we really want to aim on is for the abstractive answers, which requires some form of reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=476" target="_blank">00:07:56.700</a></span> | <span class="t">And I think that's what this methodology is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=480" target="_blank">00:08:00.320</a></span> | <span class="t">And of course, there's a yes or no and unanswerable questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=485" target="_blank">00:08:05.080</a></span> | <span class="t">They just skip it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=485" target="_blank">00:08:05.900</a></span> | <span class="t">Yeah, that's all I want to share about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=491" target="_blank">00:08:11.980</a></span> | <span class="t">Next, I want to talk about L-eval, which is long e-val.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=495" target="_blank">00:08:15.720</a></span> | <span class="t">This is slightly more recent, 2023.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=500" target="_blank">00:08:20.220</a></span> | <span class="t">Discarding unanswerable questions seems like part of the reason LLM's answers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=506" target="_blank">00:08:26.260</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=508" target="_blank">00:08:28.100</a></span> | <span class="t">I think it's important in the sense that we just need to make sure that those questions we don't mark it against the LLM or that we don't force the LLM to make up bullshit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=515" target="_blank">00:08:35.140</a></span> | <span class="t">So it's 20 sub-pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=518" target="_blank">00:08:38.260</a></span> | <span class="t">They have 500 long documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=520" target="_blank">00:08:40.120</a></span> | <span class="t">And these documents range from 3K to 200K.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=522" target="_blank">00:08:42.960</a></span> | <span class="t">What we're really more interested in is in the 20K.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=526" target="_blank">00:08:46.900</a></span> | <span class="t">But beyond the data set that they have here, here's the data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=532" target="_blank">00:08:52.000</a></span> | <span class="t">The data set is really interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=533" target="_blank">00:08:53.080</a></span> | <span class="t">They almost created a lot of new data sets themselves where they script Coursera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=537" target="_blank">00:08:57.560</a></span> | <span class="t">This is, I think, a sci-fi citation, science fiction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=543" target="_blank">00:09:03.260</a></span> | <span class="t">Science fiction data set, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=545" target="_blank">00:09:05.160</a></span> | <span class="t">It's a science fiction data set of true-false questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=548" target="_blank">00:09:08.920</a></span> | <span class="t">And then they have a code data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=551" target="_blank">00:09:11.120</a></span> | <span class="t">And then there's a long context question answering data set based on investor relations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=555" target="_blank">00:09:15.760</a></span> | <span class="t">So it's finance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=556" target="_blank">00:09:16.700</a></span> | <span class="t">So it's a really mixed bag from educational fiction and long context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=564" target="_blank">00:09:24.140</a></span> | <span class="t">So then, of course, they also combine several publicly available data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=572" target="_blank">00:09:32.600</a></span> | <span class="t">Now, what's really interesting here is this finding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=575" target="_blank">00:09:35.400</a></span> | <span class="t">Of course, they use an LLM as a judge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=579" target="_blank">00:09:39.560</a></span> | <span class="t">I mean, who doesn't nowadays?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=580" target="_blank">00:09:40.580</a></span> | <span class="t">But what they found was this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=585" target="_blank">00:09:45.680</a></span> | <span class="t">Oh, no, where is it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=588" target="_blank">00:09:48.740</a></span> | <span class="t">Sorry, it's a little bit messy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=593" target="_blank">00:09:53.120</a></span> | <span class="t">So what they had to do was this, which is length instruction enhanced evaluation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=602" target="_blank">00:10:02.560</a></span> | <span class="t">So the problem with a lot of the N-gram-based benchmarks is that it's very much reliant on length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=609" target="_blank">00:10:09.780</a></span> | <span class="t">All the N-gram-based benchmarks are based on some recall and some precision approach, which you need some kind of denominator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=617" target="_blank">00:10:17.560</a></span> | <span class="t">And usually the denominator is the number of tokens in the answer, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=621" target="_blank">00:10:21.000</a></span> | <span class="t">And therefore, longer answers will therefore be penalized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=625" target="_blank">00:10:25.000</a></span> | <span class="t">So what they found is that they try to constrain the LLMs to the number of words in the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=634" target="_blank">00:10:34.440</a></span> | <span class="t">And they found that when they did this, firstly, all the automated metrics, this is a key point, failed to correlate with human judgment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=645" target="_blank">00:10:45.880</a></span> | <span class="t">And compared, and of course, LLM judges are more accurate and robust to length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=649" target="_blank">00:10:49.840</a></span> | <span class="t">They also showed that with this length instruction enhanced evaluation, they found that they were able to improve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=662" target="_blank">00:11:02.040</a></span> | <span class="t">With the length instructions, they were able to improve the rouge by 0.5 to 0.8.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=669" target="_blank">00:11:09.000</a></span> | <span class="t">And, you know, the data of the GPT-4 evaluator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=672" target="_blank">00:11:12.360</a></span> | <span class="t">Essentially, what it means is that when you try different pipelines or different models to try to get your answers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=678" target="_blank">00:11:18.420</a></span> | <span class="t">you want to try to make sure that the answers, it's not unfairly biasing a certain LLM that has longer answers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=687" target="_blank">00:11:27.900</a></span> | <span class="t">So I think this is also a pretty standard now, but it's a good reminder and they show data on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=692" target="_blank">00:11:32.400</a></span> | <span class="t">The next one, I mean, since Swix likes fiction so much, there's another fiction paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=699" target="_blank">00:11:39.380</a></span> | <span class="t">So this is novel QA, question answering documents, exceeding 200K documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=707" target="_blank">00:11:47.120</a></span> | <span class="t">So this is based on English documents, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=709" target="_blank">00:11:49.600</a></span> | <span class="t">And they have a mix of various complexity, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=717" target="_blank">00:11:57.600</a></span> | <span class="t">So what is really interesting here is they use, based on English, but they also try to, and they also, they have golden answers, which is great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=727" target="_blank">00:12:07.880</a></span> | <span class="t">Having these reference answers is really important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=730" target="_blank">00:12:10.680</a></span> | <span class="t">But what is really interesting is the, how they try to create the data set, which is multi-hop, single-hop, and detailed answers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=738" target="_blank">00:12:18.120</a></span> | <span class="t">So single-hop means you need to find, you locate some evidence somewhere and you need to connect it with another evidence somewhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=746" target="_blank">00:12:26.380</a></span> | <span class="t">Multi-hop means you need to jump all over the place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=748" target="_blank">00:12:28.440</a></span> | <span class="t">So this is actually pretty challenging, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=751" target="_blank">00:12:31.580</a></span> | <span class="t">And I think a lot of regular Q&A, if you're asking questions of your documents or of your book, et cetera,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=756" target="_blank">00:12:36.880</a></span> | <span class="t">these are the kind of very difficult questions you're asking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=759" target="_blank">00:12:39.960</a></span> | <span class="t">You're probably not asking for extractive questions, like what did Tom do or where did Dumbledore destroy the first Horcrux?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=769" target="_blank">00:12:49.720</a></span> | <span class="t">Like even where did he destroy the first Horcrux, you actually need to do a lot of abstractive and reasoning thinking behind that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=777" target="_blank">00:12:57.380</a></span> | <span class="t">So what is really interesting is that they tried to show what happens with the loss-in-the-middle phenomena, and that's the data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=786" target="_blank">00:13:06.000</a></span> | <span class="t">That's the graph I want to focus on here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=787" target="_blank">00:13:07.880</a></span> | <span class="t">Also in normal Q&A, they actually have two kinds of answers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=791" target="_blank">00:13:11.180</a></span> | <span class="t">The first answer is multiple choice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=794" target="_blank">00:13:14.280</a></span> | <span class="t">And you can see over here, the answer is multiple choice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=799" target="_blank">00:13:19.000</a></span> | <span class="t">And then the second answer is generative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=800" target="_blank">00:13:20.660</a></span> | <span class="t">For the answer, the model actually has to choose some kind of answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=806" target="_blank">00:13:26.280</a></span> | <span class="t">And you can see, this is probably not the best graph, but essentially both of these are multiple choice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=813" target="_blank">00:13:33.860</a></span> | <span class="t">Do you try and match it with the answer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=815" target="_blank">00:13:35.420</a></span> | <span class="t">They reference L-Eval, they reference Bamboo and Long Bench.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=819" target="_blank">00:13:39.760</a></span> | <span class="t">We won't have time to go through a lot of these, but L-Eval, which is the paper that we just mentioned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=823" target="_blank">00:13:43.800</a></span> | <span class="t">they actually reuse the same length instruction enhanced evaluation based on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=830" target="_blank">00:13:50.240</a></span> | <span class="t">So token size, 200k tokens, this exceeds what Claude is able to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=835" target="_blank">00:13:55.700</a></span> | <span class="t">But we don't know what Claude's tokenizer is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=838" target="_blank">00:13:58.160</a></span> | <span class="t">Anyway, so this data set, now over here, they are very explicit on how they constructed a data set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=843" target="_blank">00:14:03.980</a></span> | <span class="t">which is very helpful if you want to construct a data set yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=847" target="_blank">00:14:07.140</a></span> | <span class="t">Essentially, you need a few columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=849" target="_blank">00:14:09.360</a></span> | <span class="t">You need the question, obviously, and the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=851" target="_blank">00:14:11.240</a></span> | <span class="t">You also need the golden reference answer, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=855" target="_blank">00:14:15.600</a></span> | <span class="t">The gold answer and the evidence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=858" target="_blank">00:14:18.340</a></span> | <span class="t">So now evidence, you can probably just say the evidence is probably the entire document up to the point that I read.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=862" target="_blank">00:14:22.800</a></span> | <span class="t">Or you can actually go the extra mile by saying that these are the paragraphs where the evidence is or the spans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=867" target="_blank">00:14:27.520</a></span> | <span class="t">And you can also try to label the questions to try to help you understand,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=872" target="_blank">00:14:32.100</a></span> | <span class="t">hey, is my pipeline performing better on the more complex questions, multi-hop questions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=876" target="_blank">00:14:36.640</a></span> | <span class="t">abstractive summarization questions, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=879" target="_blank">00:14:39.400</a></span> | <span class="t">I kind of ignore the multiple choice answers because that's not very representative if you're trying to build a chatbot on top of long context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=888" target="_blank">00:14:48.820</a></span> | <span class="t">So very interestingly, they use public domain novels from Project Gutenberg.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=894" target="_blank">00:14:54.820</a></span> | <span class="t">They also purchase e-books.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=896" target="_blank">00:14:56.720</a></span> | <span class="t">I'm not sure what it means to purchase the e-book and then release it as part of a golden data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=902" target="_blank">00:15:02.480</a></span> | <span class="t">I don't know if you purchase the e-book, you actually have the copyright and the license to do that, to share it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=907" target="_blank">00:15:07.360</a></span> | <span class="t">But I know that's what they did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=909" target="_blank">00:15:09.000</a></span> | <span class="t">I don't want to question that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=910" target="_blank">00:15:10.740</a></span> | <span class="t">So here's how the distribution of the questions look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=916" target="_blank">00:15:16.240</a></span> | <span class="t">Multi-hop, which is the most challenging one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=918" target="_blank">00:15:18.220</a></span> | <span class="t">Single-hop and then followed by detail, which is just really extractive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=920" target="_blank">00:15:20.800</a></span> | <span class="t">Now, there's three really interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=926" target="_blank">00:15:26.960</a></span> | <span class="t">I'm not sure if I would agree with their interpretation of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=932" target="_blank">00:15:32.860</a></span> | <span class="t">So what they found over here, oh, sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=935" target="_blank">00:15:35.780</a></span> | <span class="t">Figure three is that accuracy drops beyond 100k tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=944" target="_blank">00:15:44.720</a></span> | <span class="t">So once your data is in, beyond 100k, accuracy drops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=950" target="_blank">00:15:50.540</a></span> | <span class="t">So that's what the graph on the left shows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=952" target="_blank">00:15:52.720</a></span> | <span class="t">I think that's somewhat clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=955" target="_blank">00:15:55.020</a></span> | <span class="t">It feels like, I think they're right to say that, okay, if you average all the three numbers, you do see a stepwise drop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=960" target="_blank">00:16:00.780</a></span> | <span class="t">Now, figure 3b, this is the one that's actually really interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=967" target="_blank">00:16:07.820</a></span> | <span class="t">In a sense, what they say is that if I give it some text and the answer is in some percentile of the text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=976" target="_blank">00:16:16.360</a></span> | <span class="t">the answer is in the first 10% or the answer is the last 90% or the last answer is right in the middle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=981" target="_blank">00:16:21.400</a></span> | <span class="t">What this graph says is that there is no loss in the middle effect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=986" target="_blank">00:16:26.400</a></span> | <span class="t">Regardless of where the actual answer, the actual evidence is, there's actually no loss in the middle effect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=992" target="_blank">00:16:32.840</a></span> | <span class="t">And this is actually, this actually suggests a hint.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=995" target="_blank">00:16:35.280</a></span> | <span class="t">If you are trying to build such a benchmark yourself, you want your benchmark to be positionally robust in the sense that you want to ask questions that can be answered by the first quarter of the book, the half of the book, the third of the book, and at the end of the book.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1011" target="_blank">00:16:51.640</a></span> | <span class="t">You don't want to just formulate questions that are either just all at the start or all in the middle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1017" target="_blank">00:16:57.020</a></span> | <span class="t">You want to have this concept of positional robustness to understand, hey, where does my pipeline start to fail?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1023" target="_blank">00:17:03.020</a></span> | <span class="t">According to this graph, they say that it does not fail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1027" target="_blank">00:17:07.180</a></span> | <span class="t">But now let's look at figure 7, which is in the appendix, where they split this up into two stages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1038" target="_blank">00:17:18.040</a></span> | <span class="t">The first one on top is when the, and maybe let's just focus on the GPT-4 and Claude 2.1 numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1044" target="_blank">00:17:24.980</a></span> | <span class="t">The first one on top is when the context length is below 100k.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1050" target="_blank">00:17:30.340</a></span> | <span class="t">So you can see when the context length is below 100k, there may be some kind of loss in the middle effect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1056" target="_blank">00:17:36.100</a></span> | <span class="t">If you look at this GPT, if you really squint at this GPT-4 graph, and maybe this intern LLM graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1063" target="_blank">00:17:43.380</a></span> | <span class="t">But when the context length exceeds 100k, you start to see it's really just dropping throughout all the way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1069" target="_blank">00:17:49.120</a></span> | <span class="t">So I think maybe there's two stories here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1072" target="_blank">00:17:52.160</a></span> | <span class="t">When the context length is medium, look at us, 100k context is already considered medium now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1076" target="_blank">00:17:56.800</a></span> | <span class="t">When the context length is medium, maybe the loss in the middle effect is present.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1081" target="_blank">00:18:01.100</a></span> | <span class="t">When the context length is long, I guess the longest one they have here, which is more than 100k,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1087" target="_blank">00:18:07.800</a></span> | <span class="t">you start to see that long context start to drop off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1092" target="_blank">00:18:12.600</a></span> | <span class="t">So that's a nuanced finding that they found here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1095" target="_blank">00:18:15.920</a></span> | <span class="t">Now the last one, I know I'm going super fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1100" target="_blank">00:18:20.360</a></span> | <span class="t">I will just want to finish this and maybe in just three minutes, and then we can go into questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1103" target="_blank">00:18:23.660</a></span> | <span class="t">The last one is especially interesting because it goes into multi-document Q&A.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1111" target="_blank">00:18:31.120</a></span> | <span class="t">Up to so far, we've always only been focusing on a single document, a single movie script, a single book, a single paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1119" target="_blank">00:18:39.440</a></span> | <span class="t">But what if the answer requires multiple documents?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1123" target="_blank">00:18:43.500</a></span> | <span class="t">So here's the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1124" target="_blank">00:18:44.820</a></span> | <span class="t">List the current assets of each of the above companies in order.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1127" target="_blank">00:18:47.700</a></span> | <span class="t">So you do need to go through multiple documents to try to answer this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1131" target="_blank">00:18:51.560</a></span> | <span class="t">And it depends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1132" target="_blank">00:18:52.860</a></span> | <span class="t">Different documents could have different contexts, could have different styles of writing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1136" target="_blank">00:18:56.500</a></span> | <span class="t">How is the LLM able to deal with this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1139" target="_blank">00:18:59.320</a></span> | <span class="t">So I won't go through this very much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1144" target="_blank">00:19:04.240</a></span> | <span class="t">This is the long benchmark, L-O-O-N-G.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1147" target="_blank">00:19:07.800</a></span> | <span class="t">Pretty interesting because it also means dragon in Chinese.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1150" target="_blank">00:19:10.860</a></span> | <span class="t">Maybe that's what they intend to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1154" target="_blank">00:19:14.260</a></span> | <span class="t">And you can see these are the different tasks that you try to look at, which is given multiple docs, can you pick the right one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1161" target="_blank">00:19:21.140</a></span> | <span class="t">Given multiple docs, can you do some kind of comparison?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1163" target="_blank">00:19:23.920</a></span> | <span class="t">Hey, is Ali Baba better than Pai Tu or Tencent?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1166" target="_blank">00:19:26.560</a></span> | <span class="t">Comparison.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1167" target="_blank">00:19:27.180</a></span> | <span class="t">Given multiple docs, can you do clustering and do some kind of summarization on top of that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1171" target="_blank">00:19:31.960</a></span> | <span class="t">And then given multiple docs, can you just do some kind of chain of reasoning across the docs?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1176" target="_blank">00:19:36.160</a></span> | <span class="t">But one thing that's really interesting in this is not just how the benchmark was built, but this result that they have in this paper, in this graph over here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1186" target="_blank">00:19:46.600</a></span> | <span class="t">So this paper, I don't know, again, this is a question that I often debate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1190" target="_blank">00:19:50.820</a></span> | <span class="t">I don't know what the right answer is and I'm curious to hear what people think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1193" target="_blank">00:19:53.980</a></span> | <span class="t">The blue line, the blue round line with the blue dots, this is the baseline accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1200" target="_blank">00:20:00.480</a></span> | <span class="t">And you can see baseline accuracy just keeps dropping.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1203" target="_blank">00:20:03.200</a></span> | <span class="t">It's fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1204" target="_blank">00:20:04.080</a></span> | <span class="t">So this is the top five, GPT-40 and QAN 272B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1206" target="_blank">00:20:06.740</a></span> | <span class="t">Now, the other lines, the other colors, these are when you use OpenAI embeddings and get the top 10 documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1214" target="_blank">00:20:14.640</a></span> | <span class="t">And when you use, I can't remember what BGE stands for, but it's another embedding model and use the top five.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1221" target="_blank">00:20:21.240</a></span> | <span class="t">Top five and of course, OpenAI top five and OpenAI top 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1226" target="_blank">00:20:26.000</a></span> | <span class="t">What do you see here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1226" target="_blank">00:20:26.940</a></span> | <span class="t">Can anyone just shout out?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1228" target="_blank">00:20:28.820</a></span> | <span class="t">What's the big trend here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1229" target="_blank">00:20:29.820</a></span> | <span class="t">Longer context, lower?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1234" target="_blank">00:20:34.700</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1235" target="_blank">00:20:35.760</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1237" target="_blank">00:20:37.260</a></span> | <span class="t">What is the other big trend?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1239" target="_blank">00:20:39.140</a></span> | <span class="t">The other big trend is maybe we don't need RAG.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1245" target="_blank">00:20:45.280</a></span> | <span class="t">I know maybe that is controversial, but you can see that using RAG, you always get lower accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1254" target="_blank">00:20:54.900</a></span> | <span class="t">Now, this, I don't know if it's controversial or not, but if you think about it, if you think about it two years into the future,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1264" target="_blank">00:21:04.620</a></span> | <span class="t">will models have standard half a million or one million contacts?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1268" target="_blank">00:21:08.080</a></span> | <span class="t">I think that's very likely, yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1269" target="_blank">00:21:09.680</a></span> | <span class="t">Will pre-cash cost drop further?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1273" target="_blank">00:21:13.600</a></span> | <span class="t">I think the answer is yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1274" target="_blank">00:21:14.980</a></span> | <span class="t">Will long contacts capability increase?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1278" target="_blank">00:21:18.180</a></span> | <span class="t">I think the answer is probably yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1279" target="_blank">00:21:19.600</a></span> | <span class="t">So it doesn't mean that we don't need RAG.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1281" target="_blank">00:21:21.800</a></span> | <span class="t">It means that if your single document, if you have a single document or single book,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1287" target="_blank">00:21:27.400</a></span> | <span class="t">or even maybe a Game of Thrones style, that length of book,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1290" target="_blank">00:21:30.140</a></span> | <span class="t">maybe you just need the whole thing in the context and you maybe don't need RAG.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1293" target="_blank">00:21:33.480</a></span> | <span class="t">Now, you still need RAG if you're trying to query on the Library of Congress,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1296" target="_blank">00:21:36.540</a></span> | <span class="t">which is all the financial reports.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1299" target="_blank">00:21:39.800</a></span> | <span class="t">But from this example here, you can see that at 50K or 100K or even 200K,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1304" target="_blank">00:21:44.600</a></span> | <span class="t">RAG actually didn't help, but it actually hurt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1312" target="_blank">00:21:52.340</a></span> | <span class="t">So that's all I had to share.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1313" target="_blank">00:21:53.780</a></span> | <span class="t">Any questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1314" target="_blank">00:21:54.380</a></span> | <span class="t">Quick question on this last one, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1316" target="_blank">00:21:56.840</a></span> | <span class="t">So is this a single document or is this multiple documents?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1321" target="_blank">00:22:01.700</a></span> | <span class="t">I suspect this is multiple documents because their data sets are all multiple documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1328" target="_blank">00:22:08.120</a></span> | <span class="t">So in the long context, they're just throwing in multiple documents?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1332" target="_blank">00:22:12.500</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1333" target="_blank">00:22:13.400</a></span> | <span class="t">All the documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1334" target="_blank">00:22:14.860</a></span> | <span class="t">I suspect it's multiple documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1344" target="_blank">00:22:24.020</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1345" target="_blank">00:22:25.400</a></span> | <span class="t">So one thing that I ran into in the past with long context evals was so I was working on like a legal long context QA task and they had overfit on trying to fine tune a menstrual model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1362" target="_blank">00:22:42.980</a></span> | <span class="t">And it was really good at documents at the start, the middle and the end, but not at different quartiles because they tried to fix this long context QA by training on facts that were injected in the middle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1375" target="_blank">00:22:55.300</a></span> | <span class="t">And then we had overfit to that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1378" target="_blank">00:22:58.480</a></span> | <span class="t">But then what happened was when we did chain of thought questions with multiple documents, we noticed that it would just chain of thought through the whole context and be like, okay, I need this source and then I will do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1392" target="_blank">00:23:12.960</a></span> | <span class="t">So giving in multiple kind of had an internal chain of thought even when not prompted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1397" target="_blank">00:23:17.160</a></span> | <span class="t">But then when we did it on one large document, it still wouldn't work well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1402" target="_blank">00:23:22.480</a></span> | <span class="t">So interesting little like note that we had on this specific thing because we tried, let's just throw in the whole couple documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1409" target="_blank">00:23:29.320</a></span> | <span class="t">And it was, you know, I think it was 32K context, but that could still fit multiple, multiple documents and it could reason through, okay, I need to look at this subsection because now it has internal breaks, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1420" target="_blank">00:23:40.360</a></span> | <span class="t">So it could easily find, okay, I need like, you know, 17th document is here and then it would look into that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1425" target="_blank">00:23:45.320</a></span> | <span class="t">But if we gave one document that was 32K, it wouldn't do any well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1429" target="_blank">00:23:49.340</a></span> | <span class="t">So interesting little note on this even.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1432" target="_blank">00:23:52.240</a></span> | <span class="t">Yeah, I think maybe that's how the fine tuning was done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1433" target="_blank">00:23:53.980</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1436" target="_blank">00:23:56.160</a></span> | <span class="t">So the question I always ask myself and I mean, not to be spicy here, but the question I always ask myself is a reg or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1446" target="_blank">00:24:06.200</a></span> | <span class="t">So now here's the practical considerations, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1449" target="_blank">00:24:09.760</a></span> | <span class="t">If you want to build a reg, you need to eval the reg.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1451" target="_blank">00:24:11.740</a></span> | <span class="t">And building the reg, evaluating the reg, maybe that's two headcount.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1456" target="_blank">00:24:16.260</a></span> | <span class="t">And we know evaluation is really hard, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1458" target="_blank">00:24:18.140</a></span> | <span class="t">Evaluation and retrieval is especially hard because it's always the cold start problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1462" target="_blank">00:24:22.460</a></span> | <span class="t">If you're asking a question of a document base, everyone has their own document base and there's only one person who's answering document base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1468" target="_blank">00:24:28.960</a></span> | <span class="t">It's really hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1469" target="_blank">00:24:29.920</a></span> | <span class="t">Unlike if you're doing recommendation systems or search, you have free data coming from everywhere where customers are just saying yes or no, whether I like this product that you've written or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1479" target="_blank">00:24:39.520</a></span> | <span class="t">But for evaluation or retrieval, I think it's really, really, really hard and really expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1484" target="_blank">00:24:44.240</a></span> | <span class="t">So as far as I can, as much as I would like to not build a reg and evaluate the retrieval component of the reg, I try to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1493" target="_blank">00:24:53.220</a></span> | <span class="t">Now, if anyone here wants to tell me, Eugene, you're wrong, I would love to hear it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1498" target="_blank">00:24:58.020</a></span> | <span class="t">And feel free to ping me on Discord or Twitter anywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1502" target="_blank">00:25:02.100</a></span> | <span class="t">But in my mind, I think evaluation retrieval is actually extremely hard, especially in reg, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1506" target="_blank">00:25:06.440</a></span> | <span class="t">Because you're retrieving the documents and then you're summarizing into an answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1509" target="_blank">00:25:09.560</a></span> | <span class="t">The user will never give you feedback on whether the documents you have retrieved are actually useful or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1514" target="_blank">00:25:14.680</a></span> | <span class="t">There's no built-in data flyer will feedback you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1517" target="_blank">00:25:17.660</a></span> | <span class="t">The user will only say the answer is good or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1520" target="_blank">00:25:20.180</a></span> | <span class="t">But you actually don't, unless you actually have a dedicated team that actually says, this document is good, this document is not good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1525" target="_blank">00:25:25.800</a></span> | <span class="t">It's really hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1526" target="_blank">00:25:26.800</a></span> | <span class="t">And how would they actually know whether a document is good or not?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1529" target="_blank">00:25:29.420</a></span> | <span class="t">Unless they have absolute, firstly, they will never build track.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1533" target="_blank">00:25:33.560</a></span> | <span class="t">You'll never build measure recall.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1535" target="_blank">00:25:35.220</a></span> | <span class="t">The best that you can do is measure precision, but precision is also pretty, it's not bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1539" target="_blank">00:25:39.360</a></span> | <span class="t">It's actually, it goes a long way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1541" target="_blank">00:25:41.100</a></span> | <span class="t">So that's how this last graph here is really making me think a lot about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1547" target="_blank">00:25:47.740</a></span> | <span class="t">And yeah, so I'll stop here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1550" target="_blank">00:25:50.360</a></span> | <span class="t">I have two quick comments on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1553" target="_blank">00:25:53.320</a></span> | <span class="t">One is, I wonder how other models will shape up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1557" target="_blank">00:25:57.140</a></span> | <span class="t">Like we have LamaGuard as a hallucination detection model, which you can implement live.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1562" target="_blank">00:26:02.540</a></span> | <span class="t">And then it's great now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1563" target="_blank">00:26:03.540</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1564" target="_blank">00:26:04.240</a></span> | <span class="t">So will there be a sort of, you know, rag checker type model that we can implement in systems?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1569" target="_blank">00:26:09.800</a></span> | <span class="t">And how much will systems have an effect to this, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1572" target="_blank">00:26:12.460</a></span> | <span class="t">Models that are specialized at this sort of QA task of, is this the correct document?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1577" target="_blank">00:26:17.200</a></span> | <span class="t">And it's past just embedding and similarity search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1581" target="_blank">00:26:21.600</a></span> | <span class="t">But, you know, can we have a quick, small model that just references documents and sees if we can optimize them?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1586" target="_blank">00:26:26.960</a></span> | <span class="t">Similar to how we currently have like, you know, rag with re-rank or hide embedding, stuff like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1591" target="_blank">00:26:31.840</a></span> | <span class="t">Will we just have models that abstract this away?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1594" target="_blank">00:26:34.940</a></span> | <span class="t">And then the other point is, in two pretty large use cases that I built in medical and legal, we actually had it pretty easy to have a human in the loop for feedback for if documents are suggested.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1608" target="_blank">00:26:48.180</a></span> | <span class="t">So, for example, for lawyers, we had a quick QA sort of chatbot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1613" target="_blank">00:26:53.660</a></span> | <span class="t">And what happened was, if we, you know, automatically just had an output, basically everyone would just take the LLM's word, even if it was incorrect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1623" target="_blank">00:27:03.240</a></span> | <span class="t">So we had to scale back and be like, okay, what we ended up on was a sort of fill-in-the-blank multiple-choice style output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1630" target="_blank">00:27:10.180</a></span> | <span class="t">So you would have a reference document, you would have a subset, and then you would have to fill in sort of what would happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1636" target="_blank">00:27:16.000</a></span> | <span class="t">This was kind of our middle ground to make sure that there was a human in the loop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1639" target="_blank">00:27:19.800</a></span> | <span class="t">Because if you just have, you know, is this correct? Check or X is what we did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1644" target="_blank">00:27:24.960</a></span> | <span class="t">Everyone would just press the check, and we had like 90-something percent success rate on just, yep, this is right, and people wouldn't really check the document.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1652" target="_blank">00:27:32.160</a></span> | <span class="t">But if we gave them a snippet, and then they had to kind of find the result, they could either, you know, leave a comment that this is not, like the result isn't here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1660" target="_blank">00:27:40.100</a></span> | <span class="t">which we had like a significant greater than 10% of the time, or they could get the reference there, and it was still like highly impactful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1667" target="_blank">00:27:47.900</a></span> | <span class="t">I know we kind of used that data to build our own guardrail for the system, but I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1672" target="_blank">00:27:52.280</a></span> | <span class="t">I found that there were pretty clever ways to get these human in the loop of, do we have the right document?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1676" target="_blank">00:27:56.920</a></span> | <span class="t">And we actually did it from a guardrail of just, we can't have people just accept every AI response, because models were pretty shit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1683" target="_blank">00:28:03.680</a></span> | <span class="t">We didn't have great LM as a judge, but that was just one use case we came around for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1689" target="_blank">00:28:09.520</a></span> | <span class="t">That's pretty cool, Vibu. I hope you have nothing on plan next Saturday and Sunday, because that's the only thing that we'll be talking about when I'm in SF next week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1698" target="_blank">00:28:18.680</a></span> | <span class="t">Yeah, I don't know if I can actually get regular non-paying users to do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1705" target="_blank">00:28:25.000</a></span> | <span class="t">You can imagine, you know, I'm just reading my book. Do I actually want to provide the right data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1710" target="_blank">00:28:30.620</a></span> | <span class="t">I don't know. But okay, over to whoever's next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1713" target="_blank">00:28:33.420</a></span> | <span class="t">Okay. Six, you want to go next? I got next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1716" target="_blank">00:28:36.240</a></span> | <span class="t">Ted, question. Open.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1718" target="_blank">00:28:38.460</a></span> | <span class="t">Yeah, I think Vibu's next, but yeah, go ahead, Ted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1721" target="_blank">00:28:41.440</a></span> | <span class="t">Yeah, just a quick question, Eugene. I'm curious if you've looked at, like, there was the writing in the margins papers, one that I know about, and, you know, sort of like trying to improve rag instead of just doing vanilla rag.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1735" target="_blank">00:28:55.000</a></span> | <span class="t">I think Sam actually got his folks to present the writing in the margins paper. So yeah, I'm familiar with that. I'm familiar with that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1744" target="_blank">00:29:04.700</a></span> | <span class="t">I have not invested too much time experimenting and implementing that. But yes, I'm aware of that. Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1751" target="_blank">00:29:11.300</a></span> | <span class="t">Also, it's another note when you bring up, do we need rag or all the context? So once again, shout out, Sam.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1758" target="_blank">00:29:18.040</a></span> | <span class="t">Ryder team shipped their new model this week. One thing I gave feedback was, if I like to just share my screen right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1765" target="_blank">00:29:25.900</a></span> | <span class="t">Sorry, Sam, I'm calling you out live. But, so, where is this? Is this the right tab? This is the right tab.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1777" target="_blank">00:29:37.300</a></span> | <span class="t">Basically, they mentioned that, you know, it takes 22 seconds to process a million tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1785" target="_blank">00:29:45.860</a></span> | <span class="t">And my question is basically, like, how many people know how long any other model takes to process a million tokens, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1791" target="_blank">00:29:51.620</a></span> | <span class="t">Like, as you go to, let's say I want 10 million tokens in context, you know, does this scale to now 10x longer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1799" target="_blank">00:29:59.120</a></span> | <span class="t">Is it, like, over two to three minutes per query? And, yeah, do people really, like, people are used to pretty quick answers, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1806" target="_blank">00:30:06.460</a></span> | <span class="t">But when I'm building a rag system, like, sure, we have agentic stuff, but, like, are we okay with 22 million seconds for a million tokens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1815" target="_blank">00:30:15.900</a></span> | <span class="t">And if you're saying this, is this, like, a good thing? Like, is 22 seconds quick? How long are other models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1822" target="_blank">00:30:22.020</a></span> | <span class="t">I don't know this, and I thought I would go through it. But, yeah, I feel like it's also just another consideration, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1828" target="_blank">00:30:28.900</a></span> | <span class="t">But, like, as we throw more tokens, yeah, these things take a while. Like, we're used to that instant response. And, yeah, I don't know, just something to know if we just do long context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1839" target="_blank">00:30:39.440</a></span> | <span class="t">I'm assuming you guys do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1842" target="_blank">00:30:42.360</a></span> | <span class="t">This was surprisingly difficult to find comparisons on. I was trying to build, like, a little, like, homegrown benchmarking thing to test other models, but I couldn't, I didn't have, like, the API levels or the, like, credits to be able to actually make it work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1857" target="_blank">00:30:57.920</a></span> | <span class="t">I could test it on, like, some of the smaller models, because I could get my context window limitations up higher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1863" target="_blank">00:31:03.720</a></span> | <span class="t">But it was, yeah, it's actually not a solved problem yet of, like, testing models on million token requests right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1873" target="_blank">00:31:13.780</a></span> | <span class="t">Yeah, a lot of it also just meant, like, matters on, you know, are you doing your own inference? What hardware? Is the API provider optimizing for throughput? Time the first token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1885" target="_blank">00:31:25.940</a></span> | <span class="t">So, I guess there's, like, yeah, it's just another parameter to think about. If you're doing long context versus RAG, we don't think much about what goes into RAG, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1893" target="_blank">00:31:33.880</a></span> | <span class="t">They're just short responses. But, yeah, now if you're doing something like this, you know, you got to think about, oh, shit, I have millions of tokens per call.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1902" target="_blank">00:31:42.160</a></span> | <span class="t">How do I optimize on this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1905" target="_blank">00:31:45.240</a></span> | <span class="t">But, yeah, that's enough of my writer thing. But check out the model. It seems cool. I haven't tried it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1912" target="_blank">00:31:52.880</a></span> | <span class="t">Thanks, FeeBoo. Your check is in the mail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1914" target="_blank">00:31:54.540</a></span> | <span class="t">Definitely not sponsored. But they have sick merch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1919" target="_blank">00:31:59.840</a></span> | <span class="t">Swix, you want to go next? Should I go next?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1923" target="_blank">00:32:03.160</a></span> | <span class="t">I thought we said it was you, but I'm easygoing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1927" target="_blank">00:32:07.960</a></span> | <span class="t">Okay, I'm struggling on my screen share. There we go. We're good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1932" target="_blank">00:32:12.660</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1932" target="_blank">00:32:12.880</a></span> | <span class="t">No, no, I got it. It works now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1934" target="_blank">00:32:14.940</a></span> | <span class="t">Okay, so I think I shared the paper at the beginning of the call. I'll share it again real quick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1945" target="_blank">00:32:25.460</a></span> | <span class="t">So basically, this is a little survey paper. Something that I was kind of annoyed by is how before, here we go, we used to kind of have the default be that, you know, big companies make big model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1965" target="_blank">00:32:45.820</a></span> | <span class="t">Big model is fast and smart. And also, is my screen share working? It wasn't working on this before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1971" target="_blank">00:32:51.820</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1972" target="_blank">00:32:52.260</a></span> | <span class="t">Okay, sick. So before, you know, models would get smarter, faster, cheaper, and that's great. Like stuff is getting cheaper for us. Now that we have reasoning models, well, they've kind of passed the cost on to us, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1984" target="_blank">00:33:04.680</a></span> | <span class="t">Now I have to pay more for simple reasoning tokens, even though I have basic questions. And I don't want to pay more. I want the big open AI Gemini to pay that cost, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=1993" target="_blank">00:33:13.420</a></span> | <span class="t">And I also don't like how stuff gets slower. So I was like, someone needs to figure out, like, we got to stop throwing reasoning models at everything. We still want great next token predictors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2005" target="_blank">00:33:25.840</a></span> | <span class="t">And like, too many people are using reasoning models for the wrong thing. So like last week, this reasoning survey paper came out. It wasn't the best, but you know, it'll be a good little fill in. So basically, they're trying to coin this term of reasoning economy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2023" target="_blank">00:33:43.760</a></span> | <span class="t">When should we use reasoning models? And they're just like an overview of what are different techniques for post-training reasoning, pre-training reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2030" target="_blank">00:33:50.720</a></span> | <span class="t">So like chain of thought is a version of this. And then how do these systems perform? How can we prune out chain of thought?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2037" target="_blank">00:33:57.360</a></span> | <span class="t">And they just like, this is kind of the first reasoning paper that we found here. So I figured we might as well go through it a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2043" target="_blank">00:34:03.760</a></span> | <span class="t">So they kind of have these two systems of reasoning where they're system one and system two. System one is like computationally efficient, but suboptimal, where we have, let's see, what is this again?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2058" target="_blank">00:34:18.520</a></span> | <span class="t">Oh, they also have a GitHub repo where they're tracking all this, but I forgot what system one, system two was system two. Oh, so it's like reasoning models and then regular text decoding models, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2072" target="_blank">00:34:32.620</a></span> | <span class="t">So they kind of start out by saying like, there are some inefficiencies in this, right? We have models wasting tokens. We have like fake thinking where models are just outputting tokens. There's problems in RL where we've RL models and now they're over optimizing on length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2090" target="_blank">00:34:50.140</a></span> | <span class="t">Chain of thought can like help, but it's not a training time. So how do we kind of like look at what's going on here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2100" target="_blank">00:35:00.140</a></span> | <span class="t">There's the two stages, right? There's post-training and then there's the test time. So test time thinking is like, you know, MCTC, Monte Carlo Tree Search MCTS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2109" target="_blank">00:35:09.480</a></span> | <span class="t">There's techniques like you could do speculative decoding, have a little model do stuff fast and then have a big model check it. You could do chain of thought. You could run multiple queries and kind of condense them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2120" target="_blank">00:35:20.960</a></span> | <span class="t">They also have the agent concept where you can have like an agentic model. Then there's the post-training stage where you could do this natively in the model. You can have SFT to kind of do reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2131" target="_blank">00:35:31.960</a></span> | <span class="t">And then they show, you know, like you can have a thousand samples of reasoning data, a thousand sample reasoning data set kind of train in reasoning. You could do proper RL. They dissect a lot of DeepSeq R1 in this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2143" target="_blank">00:35:43.860</a></span> | <span class="t">And then there's inefficiencies, right? So RL, you have a length bias. You have deceptive behaviors when you're doing RL. We don't know how to optimize all this or test time when you're doing it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2154" target="_blank">00:35:54.360</a></span> | <span class="t">That inference, you know, we're doing like inefficient usage of computation. So for example, there's a paper that cites, they do up to a thousand to 10,000 parallel calls of Lama 8B. And they're like, yeah, you know, we don't need 10,000 Lama 8B calls for one question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2171" target="_blank">00:36:11.160</a></span> | <span class="t">Then there's like, okay, how do we address these changes, right? So there's the architecture level. What are people working on for model level stuff for reasoning? Those algorithmic stuff, there's the data that goes into it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2182" target="_blank">00:36:22.700</a></span> | <span class="t">And then for the inference side, you know, there's stuff where you can force in an adaptive budget. So for example, you can give in a token that says, how much budget do you want in this question? And then the model learns to follow it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2194" target="_blank">00:36:34.700</a></span> | <span class="t">There's basic routers where you can route to a small, medium, high model, and they sort of cite how O3 does this. So there's O3 mini, O3 high, regular O3. You can have a router.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2205" target="_blank">00:36:45.800</a></span> | <span class="t">There's agents. There's the decoding side. So you can do this at inference time. There's different techniques you can do. And they just kind of go through all this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2214" target="_blank">00:36:54.480</a></span> | <span class="t">Let's continue on through here. So this is kind of, you know, a bit of background. What's going on here? What are some of the issues? What are the methods?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2223" target="_blank">00:37:03.740</a></span> | <span class="t">So post-training, yeah, we have SFT. You can do SFT and get basic reasoning. They have this cool little diagram chart here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2232" target="_blank">00:37:12.360</a></span> | <span class="t">It kind of shows different processes that we can do, right? So for the reasoning in training time, we can do post-training and we can do test time models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2243" target="_blank">00:37:23.940</a></span> | <span class="t">So we can do SFT, RL. There's parallel methods where you can kind of, you know, shoot out a bunch of models, shoot out a bunch of queries, condense them down in a sort of pipeline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2254" target="_blank">00:37:34.500</a></span> | <span class="t">Then there's, and they show kind of papers that do this, how they perform, how we can prune out chain of thought, how we can make it more efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2261" target="_blank">00:37:41.160</a></span> | <span class="t">There's sequential methods. So this is kind of like your pipeline approach, you know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2265" target="_blank">00:37:45.920</a></span> | <span class="t">You have human in the loop. You have guardrails. You have different tool usage. There's different sequential methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2271" target="_blank">00:37:51.860</a></span> | <span class="t">For post-training, there's stuff like length biases. So, you know, models, there's overly cautious language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2281" target="_blank">00:38:01.920</a></span> | <span class="t">There's fake thinking that comes in when you do RL. This is just because we don't have the best RL methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2286" target="_blank">00:38:06.920</a></span> | <span class="t">Then inefficient model usage. So unreasonable algorithmic section, you know, they don't always choose the right algorithm to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2297" target="_blank">00:38:17.840</a></span> | <span class="t">Are we using the right pipeline for the right task? So are we sending the right queries to the right models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2305" target="_blank">00:38:25.540</a></span> | <span class="t">Unreasonable compute allocation. Of course, you know, this is basically, do you need the biggest model for the basic task?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2311" target="_blank">00:38:31.920</a></span> | <span class="t">And can you optimize little models? Then we've got on the other side, you know, the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2319" target="_blank">00:38:39.560</a></span> | <span class="t">So how can we improve data? They've got algorithmic changes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2322" target="_blank">00:38:42.820</a></span> | <span class="t">So you can have a length penalty while training, procedure reward where you can, they kind of break down what are the two types of ways to affect length output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2334" target="_blank">00:38:54.360</a></span> | <span class="t">and reasoning. So there's, you know, end output where you can kind of judge what is the end output?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2340" target="_blank">00:39:00.200</a></span> | <span class="t">Is the math correct? Is the code correct? Does it compile?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2342" target="_blank">00:39:02.680</a></span> | <span class="t">And then there's also a process of what is the process code? How do we affect that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2346" target="_blank">00:39:06.760</a></span> | <span class="t">Long to short RL, adaptive budget aware tuning. So this is kind of fine tuning with a budget.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2352" target="_blank">00:39:12.300</a></span> | <span class="t">Then on the architect, there's also a chain of thought compression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2355" target="_blank">00:39:15.540</a></span> | <span class="t">So explicit and implicit. Explicit is where you directly prune 70% of the wrong chain of thought.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2362" target="_blank">00:39:22.740</a></span> | <span class="t">And then, you know, you train on more efficient chain of thought.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2365" target="_blank">00:39:25.300</a></span> | <span class="t">Implicit is where in the model itself, the architecture makes it such that different queries go through more efficient parts of the architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2376" target="_blank">00:39:36.340</a></span> | <span class="t">There's recurrent layers that you can kind of, you know, you can recurrently keep this memory state to do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2382" target="_blank">00:39:42.120</a></span> | <span class="t">There's dynamic depth, model routing, multimodal stuff, knowledge distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2387" target="_blank">00:39:47.540</a></span> | <span class="t">So they kind of talk about how you can basically distill out reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2392" target="_blank">00:39:52.220</a></span> | <span class="t">So DeepSeq did this with base quant models, llama models, and how effective that can be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2397" target="_blank">00:39:57.520</a></span> | <span class="t">Then improvement in test time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2400" target="_blank">00:40:00.080</a></span> | <span class="t">So on the input side, you know, you tell it, budget allocation, there's adaptive decoding algorithm, so kind of speculative decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2408" target="_blank">00:40:08.460</a></span> | <span class="t">On the output side, early stopping, search with pruning, constrained decoding, basically fun little chart of different stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2416" target="_blank">00:40:16.640</a></span> | <span class="t">If you're interested in any of this, they have a bunch of papers linked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2421" target="_blank">00:40:21.540</a></span> | <span class="t">Okay, so they start off with, you know, DeepSeq showed that you don't need to do SFT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2427" target="_blank">00:40:27.820</a></span> | <span class="t">You can perform RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2430" target="_blank">00:40:30.540</a></span> | <span class="t">They did find that SFT still accelerates stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2434" target="_blank">00:40:34.200</a></span> | <span class="t">So in the final run, they did a little bit of SFT before their RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2438" target="_blank">00:40:38.420</a></span> | <span class="t">The core focus of RL currently lies in the design of reward signals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2447" target="_blank">00:40:47.000</a></span> | <span class="t">So there's two type of rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2448" target="_blank">00:40:48.260</a></span> | <span class="t">There's process reward models and outcome reward models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2451" target="_blank">00:40:51.800</a></span> | <span class="t">Process kind of enables more fine-grained learning signals guiding the LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2456" target="_blank">00:40:56.480</a></span> | <span class="t">And then there's the outcome, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2459" target="_blank">00:40:59.200</a></span> | <span class="t">This is kind of where you look at what is the final thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2461" target="_blank">00:41:01.560</a></span> | <span class="t">So is the math correct and whatnot?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2463" target="_blank">00:41:03.140</a></span> | <span class="t">ORM provides supervision signal at outcome level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2469" target="_blank">00:41:09.480</a></span> | <span class="t">Then they kind of talk about the test time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2472" target="_blank">00:41:12.200</a></span> | <span class="t">So they show how, like, you know, one is kind of still pretty strong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2475" target="_blank">00:41:15.320</a></span> | <span class="t">So even though you can, like, instead of looking at the process and kind of, like, pruning the chain of thought itself, if you only do ORM, so if you only do RL on outputs, you still have really good capabilities, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2487" target="_blank">00:41:27.500</a></span> | <span class="t">Like, DeepSeq still had this aha moment, and it's still doing good reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2492" target="_blank">00:41:32.660</a></span> | <span class="t">Test time methods are kind of, you know, parallel methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2495" target="_blank">00:41:35.220</a></span> | <span class="t">So parallel is you have LLMs generate several calls, sequential methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2500" target="_blank">00:41:40.040</a></span> | <span class="t">So it's just kind of your free of thought, chain of thought, MCTS, beam search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2505" target="_blank">00:41:45.880</a></span> | <span class="t">And then their kind of, like, takeaway here is that, yeah, you know, the full potential reasoning is not achieved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2514" target="_blank">00:41:54.120</a></span> | <span class="t">They have cool statistics here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2518" target="_blank">00:41:58.460</a></span> | <span class="t">Then in section three, we talk about, like, inefficiencies in this model training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2526" target="_blank">00:42:06.540</a></span> | <span class="t">So there's inefficient model behaviors from post-training, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2531" target="_blank">00:42:11.380</a></span> | <span class="t">So first one is basically a length bias where you can have reasoning that's overly cautious, and this affects simple questions, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2538" target="_blank">00:42:18.520</a></span> | <span class="t">So LLMs trained with RL tend to produce longer responses than SFT, that makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2545" target="_blank">00:42:25.140</a></span> | <span class="t">Now, there's two questions that they have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2547" target="_blank">00:42:27.520</a></span> | <span class="t">So what are the reasons for longer responses, and does this increased length indicate a bias or enhancement of model capabilities?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2555" target="_blank">00:42:35.240</a></span> | <span class="t">So overly cautious reasoning models, you know, model excessively has unnecessary verification steps and redundant reasoning on easy-to-handle questions for meaningless paraphrases and deviations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2572" target="_blank">00:42:52.140</a></span> | <span class="t">We don't want that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2573" target="_blank">00:42:53.400</a></span> | <span class="t">Deceptive behaviors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2575" target="_blank">00:42:55.480</a></span> | <span class="t">There was some work done that, you know, models don't output the real thinking that they're doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2581" target="_blank">00:43:01.380</a></span> | <span class="t">Fake thinking happens where, you know, a model is just outputting tokens, even though we can see that it has the answer pretty early on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2588" target="_blank">00:43:08.740</a></span> | <span class="t">That's kind of section 3.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2591" target="_blank">00:43:11.220</a></span> | <span class="t">So what's happening inefficiently?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2593" target="_blank">00:43:13.840</a></span> | <span class="t">Then there's the test time inefficient stuff, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2596" target="_blank">00:43:16.240</a></span> | <span class="t">So are we using the right hyperparameters?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2598" target="_blank">00:43:18.780</a></span> | <span class="t">Do we have the right pipelines?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2600" target="_blank">00:43:20.400</a></span> | <span class="t">Are we kind of at test time doing the right thing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2603" target="_blank">00:43:23.860</a></span> | <span class="t">Unreasonable computation allocation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2606" target="_blank">00:43:26.720</a></span> | <span class="t">So, you know, this is that example of scaling computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2611" target="_blank">00:43:31.280</a></span> | <span class="t">So scaling LAMA 3AB instructs generating 100,000 to 10,000 samples for simple questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2617" target="_blank">00:43:37.620</a></span> | <span class="t">There's reasoning boundaries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2621" target="_blank">00:43:41.000</a></span> | <span class="t">But basically, they want to say that they emphasize the importance of adaptive computation allocation based on task complexity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2630" target="_blank">00:43:50.400</a></span> | <span class="t">So for more complex tasks, we should have, you know, a better way to allocate resources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2639" target="_blank">00:43:59.220</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2640" target="_blank">00:44:00.580</a></span> | <span class="t">Section 4 is kind of on two things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2643" target="_blank">00:44:03.560</a></span> | <span class="t">So part one is the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2645" target="_blank">00:44:05.020</a></span> | <span class="t">So what is the data that's used for reasoning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2648" target="_blank">00:44:08.740</a></span> | <span class="t">Basically, there's explicitly encoding in desired reasoning patterns is one way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2653" target="_blank">00:44:13.820</a></span> | <span class="t">And we can do this with even basic SFT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2655" target="_blank">00:44:15.980</a></span> | <span class="t">So there was that paper where they showed, you know, a thousand diverse SFT samples can do basic reasoning that's on par with O1 preview at the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2665" target="_blank">00:44:25.260</a></span> | <span class="t">But what's most important is quality, diversity, and difficulty of this data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2670" target="_blank">00:44:30.680</a></span> | <span class="t">Then there's kind of the algorithms that approach this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2673" target="_blank">00:44:33.660</a></span> | <span class="t">So they have this long to short RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2675" target="_blank">00:44:35.820</a></span> | <span class="t">These are strategies like, you know, where you have model merging of different models that try different things, shortest rejection, sampling, DPO, optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2685" target="_blank">00:44:45.500</a></span> | <span class="t">This one kind of showed that you can have about a 30 to 40 percent drop in tokens with no accuracy drop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2694" target="_blank">00:44:54.520</a></span> | <span class="t">Budget aware tuning was another one where they shaved off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2698" target="_blank">00:44:58.260</a></span> | <span class="t">Where's budget aware?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2700" target="_blank">00:45:00.060</a></span> | <span class="t">So budget aware tuning, this is another approach where a budget prediction allocation was implemented.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2709" target="_blank">00:45:09.060</a></span> | <span class="t">This approach achieved a 67 percent reduction in response length with only a 3 percent loss in accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2716" target="_blank">00:45:16.420</a></span> | <span class="t">They kind of have a little cute little diagram of these different things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2721" target="_blank">00:45:21.860</a></span> | <span class="t">So how they're budget aware is basically, you know, similar to O1, you have easy, middle, hard stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2728" target="_blank">00:45:28.920</a></span> | <span class="t">You have an output allocation that you want towards this question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2732" target="_blank">00:45:32.480</a></span> | <span class="t">Shave off a bunch of response output tokens and 3 percent loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2737" target="_blank">00:45:37.360</a></span> | <span class="t">Chain of thought compression is another one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2739" target="_blank">00:45:39.700</a></span> | <span class="t">You've got explicit and implicit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2742" target="_blank">00:45:42.600</a></span> | <span class="t">Kind of as they seem, implicit is where you implicitly compress out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2748" target="_blank">00:45:48.120</a></span> | <span class="t">Explicit, sorry, implicit is model based where, you know, you have an architecture that kind of tries to optimize this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2756" target="_blank">00:45:56.460</a></span> | <span class="t">Explicit is where you where you do it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2760" target="_blank">00:46:00.340</a></span> | <span class="t">Both of these work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2762" target="_blank">00:46:02.340</a></span> | <span class="t">There's different benefits to both and they just have papers that you could follow along with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2766" target="_blank">00:46:06.560</a></span> | <span class="t">OK, architecture wise, there's system one and system two cooperation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2772" target="_blank">00:46:12.660</a></span> | <span class="t">So this is where you kind of have this routing layer where you can have a system where you have something like O1 where you have three distinct thinking models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2782" target="_blank">00:46:22.040</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2782" target="_blank">00:46:22.260</a></span> | <span class="t">O1 low, middle, high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2784" target="_blank">00:46:24.000</a></span> | <span class="t">You kind of have this model routing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2786" target="_blank">00:46:26.020</a></span> | <span class="t">There's model to model collaboration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2788" target="_blank">00:46:28.220</a></span> | <span class="t">So you have pipelines that have different models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2791" target="_blank">00:46:31.140</a></span> | <span class="t">This is stuff where you have things like speculative decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2794" target="_blank">00:46:34.960</a></span> | <span class="t">So speculative decoding, there's first a small model that generates candidate tokens, then a big model that verifies them in parallel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2802" target="_blank">00:46:42.180</a></span> | <span class="t">This has two to three X speed up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2804" target="_blank">00:46:44.880</a></span> | <span class="t">Then they have the whole topic of knowledge distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2807" target="_blank">00:46:47.840</a></span> | <span class="t">So you have a big model, you distill it down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2811" target="_blank">00:46:51.120</a></span> | <span class="t">And then, you know, DeepSeq R1 outperforms applying RL on base model to Quen 2.532B where you can distill out and do RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2821" target="_blank">00:47:01.680</a></span> | <span class="t">That was very effective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2822" target="_blank">00:47:02.860</a></span> | <span class="t">Then they have like more architecture, architecture stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2826" target="_blank">00:47:06.100</a></span> | <span class="t">So you've got adaptive active parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2833" target="_blank">00:47:13.660</a></span> | <span class="t">So this is more so like where you can have a recurrent layer and you can sort of add different depth to different queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2840" target="_blank">00:47:20.880</a></span> | <span class="t">So the model itself has different depth that different queries are passed through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2844" target="_blank">00:47:24.600</a></span> | <span class="t">There's different research going on in this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2847" target="_blank">00:47:27.040</a></span> | <span class="t">Dynamic depth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2849" target="_blank">00:47:29.460</a></span> | <span class="t">What else have we got?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2852" target="_blank">00:47:32.600</a></span> | <span class="t">Then we've got test time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2854" target="_blank">00:47:34.780</a></span> | <span class="t">So inference time, how do we add inference time outside of the model, outside of RL, how do we adapt how much compute is spent?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2862" target="_blank">00:47:42.240</a></span> | <span class="t">So you've got adaptive budget decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2865" target="_blank">00:47:45.120</a></span> | <span class="t">So this is stuff where you've got like budget prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2867" target="_blank">00:47:47.860</a></span> | <span class="t">Budget prediction is where, you know, we basically train in something where you tell or you basically tell it, you know, I want this many tokens to be produced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2876" target="_blank">00:47:56.940</a></span> | <span class="t">How well does that do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2878" target="_blank">00:47:58.420</a></span> | <span class="t">There's budget constraint generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2880" target="_blank">00:48:00.620</a></span> | <span class="t">There's early stopping, search with pruning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2883" target="_blank">00:48:03.740</a></span> | <span class="t">There's adaptive selection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2886" target="_blank">00:48:06.820</a></span> | <span class="t">What else?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2888" target="_blank">00:48:08.540</a></span> | <span class="t">Early stopping, pruning, constraint decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2893" target="_blank">00:48:13.980</a></span> | <span class="t">And then that's kind of like survey of what's happened so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2898" target="_blank">00:48:18.860</a></span> | <span class="t">Then they go into discussion of like what else is still not being done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2903" target="_blank">00:48:23.260</a></span> | <span class="t">So then there's this topic of multimodal reasoning, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2906" target="_blank">00:48:26.560</a></span> | <span class="t">Right now, language models, language reasoning models, they call LRMs, they're only text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2912" target="_blank">00:48:32.740</a></span> | <span class="t">So what happens when we have multimodal reasoning models, like vision models that also reason?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2917" target="_blank">00:48:37.740</a></span> | <span class="t">How are we optimizing for that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2920" target="_blank">00:48:40.360</a></span> | <span class="t">In their survey currently for multimodal reasoning models, all they're doing is like current architecture level stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2928" target="_blank">00:48:48.980</a></span> | <span class="t">So model architecture optimization, so lightweight vision encoders, vision token compression, vision language productors, smaller language models, efficient structures, efficient vision technique adaptation, VIT quantization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2946" target="_blank">00:49:06.160</a></span> | <span class="t">So, you know, evaluation of this like isn't really being optimized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2952" target="_blank">00:49:12.340</a></span> | <span class="t">Then there's efficient agentic reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2954" target="_blank">00:49:14.720</a></span> | <span class="t">So stuff like deep research, how do we efficiently do agentic reasoning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2961" target="_blank">00:49:21.180</a></span> | <span class="t">They bring up some benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2963" target="_blank">00:49:23.520</a></span> | <span class="t">So like DNA bench is one, humanity last exam.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2967" target="_blank">00:49:27.200</a></span> | <span class="t">How do we sort of start to efficiently benchmark these things, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2970" target="_blank">00:49:30.900</a></span> | <span class="t">And then are we optimizing for the outcome versus process efficiency?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2975" target="_blank">00:49:35.400</a></span> | <span class="t">And then there's different benchmarks that they bring up for these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2980" target="_blank">00:49:40.700</a></span> | <span class="t">This is kind of like the Mechinterp side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2982" target="_blank">00:49:42.980</a></span> | <span class="t">So Anthropic is doing some interesting work on what's actually happening during this reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2988" target="_blank">00:49:48.460</a></span> | <span class="t">And then that's kind of it, you know, they kind of bring up, here's a bunch of sources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2993" target="_blank">00:49:53.300</a></span> | <span class="t">Here's different stuff on reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2994" target="_blank">00:49:54.740</a></span> | <span class="t">Here's how we can optimize different things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=2996" target="_blank">00:49:56.660</a></span> | <span class="t">And they've got a bit of different sorts, but not the deepest paper, you know, 15 minute overview.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3002" target="_blank">00:50:02.820</a></span> | <span class="t">That's some stuff happening in reasoning model optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3008" target="_blank">00:50:08.300</a></span> | <span class="t">Okay, I want to give Suik's five, six minutes, but any one, two questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3012" target="_blank">00:50:12.740</a></span> | <span class="t">Not from me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3020" target="_blank">00:50:20.660</a></span> | <span class="t">There's not much time or so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3022" target="_blank">00:50:22.380</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3022" target="_blank">00:50:22.720</a></span> | <span class="t">Okay, Suik's passed to you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3025" target="_blank">00:50:25.160</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3027" target="_blank">00:50:27.080</a></span> | <span class="t">Today we cover a one-day-old paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3031" target="_blank">00:50:31.640</a></span> | <span class="t">So very of the moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3037" target="_blank">00:50:37.840</a></span> | <span class="t">Can you see my screen?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3038" target="_blank">00:50:38.700</a></span> | <span class="t">Yes, you can.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3039" target="_blank">00:50:39.340</a></span> | <span class="t">So this is the leaderboard illusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3041" target="_blank">00:50:41.120</a></span> | <span class="t">I think basically it is Cohere not doing well on LM Arena and then Cohere saying this is fucked up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3047" target="_blank">00:50:47.960</a></span> | <span class="t">But they are correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3050" target="_blank">00:50:50.460</a></span> | <span class="t">I mean, this is open secret for a long time that you can somewhat game LM Arena and it is somewhat pay-to-play.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3061" target="_blank">00:51:01.520</a></span> | <span class="t">But it is, I think I'm not as negative as some of the others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3066" target="_blank">00:51:06.120</a></span> | <span class="t">I think this is just capitalism at work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3068" target="_blank">00:51:08.300</a></span> | <span class="t">Anyway, I do also probably think that it was a bad idea for LM Arena to announce that they are becoming a company at the exact same time that people are questioning their commercial business model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3082" target="_blank">00:51:22.600</a></span> | <span class="t">And this is the result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3084" target="_blank">00:51:24.700</a></span> | <span class="t">This could be the end of them if they don't handle it very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3088" target="_blank">00:51:28.540</a></span> | <span class="t">So Cohere says four things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3094" target="_blank">00:51:34.880</a></span> | <span class="t">One, this is the most obvious one that was obviously going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3100" target="_blank">00:51:40.280</a></span> | <span class="t">At some point in time, Gemini had like three or four different variations on LM Arena at any one point in time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3106" target="_blank">00:51:46.500</a></span> | <span class="t">And then they just released whatever score is the highest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3108" target="_blank">00:51:48.340</a></span> | <span class="t">Therefore, on a normal distribution, you would just get a very skewed like p-hack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3113" target="_blank">00:51:53.160</a></span> | <span class="t">This is literally just p-hacking of results, which is not fair to everyone else who does not have that capability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3119" target="_blank">00:51:59.420</a></span> | <span class="t">And basically, they were alleging that only four labs had access to that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3122" target="_blank">00:52:02.620</a></span> | <span class="t">I know this from off-the-record stuff as well from other people who are trying to submit things to LM Arena.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3131" target="_blank">00:52:11.520</a></span> | <span class="t">They also were able to, they also like sold data access.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3134" target="_blank">00:52:14.840</a></span> | <span class="t">They, I think there's some arguments about like the kinds of battles that LM Arena was exposing to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3144" target="_blank">00:52:24.120</a></span> | <span class="t">And then the, and then they also, they also accused LM Arena of silently removing models, even though there were some official removed models as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3152" target="_blank">00:52:32.900</a></span> | <span class="t">So about 66% were silently removed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3155" target="_blank">00:52:35.180</a></span> | <span class="t">So I think this graphic is just really good overview of what the accusations were.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3160" target="_blank">00:52:40.760</a></span> | <span class="t">They have some evidence about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3162" target="_blank">00:52:42.840</a></span> | <span class="t">I think some of them are stronger than others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3164" target="_blank">00:52:44.420</a></span> | <span class="t">But I just would highlight for folks the, the, the conclusions as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3171" target="_blank">00:52:51.320</a></span> | <span class="t">Because I think like the, the sensitive thing about this is that Cohera itself is a pretty large lab.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3177" target="_blank">00:52:57.600</a></span> | <span class="t">And for them to criticize LM Arena, which is basically two Stanford, two UC Berkeley guys, is, you know, punching down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3186" target="_blank">00:53:06.660</a></span> | <span class="t">So they, but they obviously, LM Arena has a lot of influence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3190" target="_blank">00:53:10.000</a></span> | <span class="t">So I think they, the important thing is to have constructive suggestions around what to do, given that LM Arena is a thing, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3196" target="_blank">00:53:16.380</a></span> | <span class="t">Like I think in, in a fair world, maybe a lot of people would rather that LM Arena just doesn't exist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3201" target="_blank">00:53:21.720</a></span> | <span class="t">But now that it does exist and people do use it, what do we do about it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3205" target="_blank">00:53:25.300</a></span> | <span class="t">So they, they had some really good suggestions, I thought, which was, don't allow people to retract scores that do, do badly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3213" target="_blank">00:53:33.680</a></span> | <span class="t">It's like, you know, just because you did badly, you don't get to hide it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3218" target="_blank">00:53:38.380</a></span> | <span class="t">Two, limit, limit, limit model submissions, so that you don't get to spam models and only promote the best ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3226" target="_blank">00:53:46.220</a></span> | <span class="t">Three, have equal levels of model removals of, between closed source and open source.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3233" target="_blank">00:53:53.340</a></span> | <span class="t">So don't favor closed source.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3234" target="_blank">00:53:54.800</a></span> | <span class="t">Four, implement fair sampling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3237" target="_blank">00:53:57.040</a></span> | <span class="t">I love this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3238" target="_blank">00:53:58.080</a></span> | <span class="t">Okay, so this is very fun, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3240" target="_blank">00:54:00.060</a></span> | <span class="t">So LM Arena is a, is a sampling problem, meaning they, they, they have to like find workloads and then like sort of match them up and, and try to, try to arrive at some reasonable ELO number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3254" target="_blank">00:54:14.300</a></span> | <span class="t">It turns out that the authors of LM Arena originally had a methodology which was more like active learning and they abandoned it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3263" target="_blank">00:54:23.560</a></span> | <span class="t">So I really like the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3266" target="_blank">00:54:26.360</a></span> | <span class="t">This formulation avoids simply favoring large proprietary providers and instead effectively prioritizes under-evaluated and high-variance pairs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3274" target="_blank">00:54:34.440</a></span> | <span class="t">And this, this word, high-variance pairs, made me realize like, oh yeah, I mean, that's obviously what you should focus on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3279" target="_blank">00:54:39.940</a></span> | <span class="t">This is very active learning in the sense of like, okay, if, if you have a lot of, if you have a lot of, if you have a place with a lot of disagreements, you should focus your battles on those pairs to lower the variance by increasing the, the sample size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3292" target="_blank">00:54:52.640</a></span> | <span class="t">And, and apparently they had a paper on this and they ended up not doing it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3299" target="_blank">00:54:59.980</a></span> | <span class="t">So it's kind of interesting there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3301" target="_blank">00:55:01.380</a></span> | <span class="t">And then finally, provide transparency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3304" target="_blank">00:55:04.560</a></span> | <span class="t">This is fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3305" target="_blank">00:55:05.500</a></span> | <span class="t">This is normal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3307" target="_blank">00:55:07.020</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3307" target="_blank">00:55:07.420</a></span> | <span class="t">Any questions or debates about this one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3310" target="_blank">00:55:10.340</a></span> | <span class="t">What's the Lama T?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3314" target="_blank">00:55:14.260</a></span> | <span class="t">What happened with Lama 4?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3315" target="_blank">00:55:15.600</a></span> | <span class="t">I think they published this before Lama 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3318" target="_blank">00:55:18.780</a></span> | <span class="t">I don't think, I don't think Lama 4 was discussed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3320" target="_blank">00:55:20.260</a></span> | <span class="t">Oh, yeah, there you go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3323" target="_blank">00:55:23.440</a></span> | <span class="t">Oh, in the lead up to Lama 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3324" target="_blank">00:55:24.920</a></span> | <span class="t">But I don't think, I don't think they had the Lama chat issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3328" target="_blank">00:55:28.100</a></span> | <span class="t">That first paragraph is crazy though.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3332" target="_blank">00:55:32.780</a></span> | <span class="t">27 variants.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3335" target="_blank">00:55:35.800</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3336" target="_blank">00:55:36.160</a></span> | <span class="t">I think it's interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3339" target="_blank">00:55:39.480</a></span> | <span class="t">Like they say substantially higher sampling rates for OpenAI, Google, XAI and Meta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3345" target="_blank">00:55:45.980</a></span> | <span class="t">So one, two, three, four, but XAI is like a lot lower than the other two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3350" target="_blank">00:55:50.400</a></span> | <span class="t">And then Meta is even lower than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3351" target="_blank">00:55:51.960</a></span> | <span class="t">Amazon, I think decently, nicely treated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3354" target="_blank">00:55:54.600</a></span> | <span class="t">But yeah, I mean, so, you know, I think now it's kosher to say that the ones that were treated the worst was RECA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3362" target="_blank">00:56:02.360</a></span> | <span class="t">And I heard about it directly from RECA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3364" target="_blank">00:56:04.140</a></span> | <span class="t">So it's very sad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3368" target="_blank">00:56:08.420</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3371" target="_blank">00:56:11.480</a></span> | <span class="t">At the top level, they also put out the most, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3374" target="_blank">00:56:14.560</a></span> | <span class="t">Google OpenAI have the most models compared to Meta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3377" target="_blank">00:56:17.420</a></span> | <span class="t">Yeah, but RECA had trouble submitting RECA 2 and RECA 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3384" target="_blank">00:56:24.940</a></span> | <span class="t">So yeah, that's, I think this is a useful pushback on LM Arena.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3394" target="_blank">00:56:34.080</a></span> | <span class="t">And I really liked that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3395" target="_blank">00:56:35.540</a></span> | <span class="t">I thought it was very classy that they, first, they actually shared this with the LM Arena team before publishing this paper, which I thought is just responsible disclosure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3405" target="_blank">00:56:45.460</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3405" target="_blank">00:56:45.900</a></span> | <span class="t">That's a short paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3407" target="_blank">00:56:47.220</a></span> | <span class="t">I thought it was very interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3408" target="_blank">00:56:48.980</a></span> | <span class="t">Senpai actually said like, oh yeah, LM Arena is dead to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3413" target="_blank">00:56:53.680</a></span> | <span class="t">Now OpenRouter is my best friend.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3415" target="_blank">00:56:55.340</a></span> | <span class="t">So now the same dynamics are going to apply to OpenRouter because that's how these things work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3424" target="_blank">00:57:04.680</a></span> | <span class="t">So yeah, so now he's talking about OpenRouter rankings and basically this favors cost is my general takeaway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3433" target="_blank">00:57:13.720</a></span> | <span class="t">So when Gemini launched, Gemini 2 flashed launch and it was free, suddenly it shot out to, you know, one of the most popular APIs, which is not a surprise because it's free.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3449" target="_blank">00:57:29.860</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3450" target="_blank">00:57:30.300</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3454" target="_blank">00:57:34.660</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3455" target="_blank">00:57:35.560</a></span> | <span class="t">Did people have questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3456" target="_blank">00:57:36.980</a></span> | <span class="t">I'm not seeing the chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3458" target="_blank">00:57:38.320</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3461" target="_blank">00:57:41.960</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3462" target="_blank">00:57:42.400</a></span> | <span class="t">I think that's it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3463" target="_blank">00:57:43.860</a></span> | <span class="t">We're out of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3464" target="_blank">00:57:44.660</a></span> | <span class="t">I'm happy to talk about AI News another time, but there was somebody who had a question about the search on AI News.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3472" target="_blank">00:57:52.460</a></span> | <span class="t">And basically it is all pre-built.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3475" target="_blank">00:57:55.960</a></span> | <span class="t">That's why it's fast because there's no compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3479" target="_blank">00:57:59.100</a></span> | <span class="t">Everything's pre-indexed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3482" target="_blank">00:58:02.560</a></span> | <span class="t">So if I do like...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3484" target="_blank">00:58:04.200</a></span> | <span class="t">I mean, doesn't, isn't that search whereby you type a keyword and it was actually able to do the lexical side?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3497" target="_blank">00:58:17.100</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3498" target="_blank">00:58:18.100</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3498" target="_blank">00:58:18.580</a></span> | <span class="t">I don't think, I don't think, I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3500" target="_blank">00:58:20.320</a></span> | <span class="t">Someone was impressed by that, but I don't think it's particularly much to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3503" target="_blank">00:58:23.700</a></span> | <span class="t">Like I have a year's worth of content in here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3505" target="_blank">00:58:25.900</a></span> | <span class="t">It's not that much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3507" target="_blank">00:58:27.040</a></span> | <span class="t">You know, everything can fit in a JSON file.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3510" target="_blank">00:58:30.340</a></span> | <span class="t">Seems like we didn't have time to discuss what people are next week, but I think it's the Lama series, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3515" target="_blank">00:58:35.800</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3516" target="_blank">00:58:36.260</a></span> | <span class="t">The Lama series.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3519" target="_blank">00:58:39.980</a></span> | <span class="t">So, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3520" target="_blank">00:58:40.740</a></span> | <span class="t">Next week we're talking Lama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3522" target="_blank">00:58:42.380</a></span> | <span class="t">Thank you, Rafa.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3523" target="_blank">00:58:43.800</a></span> | <span class="t">I thought I could confirm with him, but...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3525" target="_blank">00:58:45.600</a></span> | <span class="t">Yeah, most likely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3526" target="_blank">00:58:46.760</a></span> | <span class="t">I'll never talk this in a while.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3529" target="_blank">00:58:49.200</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3529" target="_blank">00:58:49.520</a></span> | <span class="t">Either it's Tom or Vibu, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3531" target="_blank">00:58:51.120</a></span> | <span class="t">No, I'm kidding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3531" target="_blank">00:58:51.640</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3534" target="_blank">00:58:54.380</a></span> | <span class="t">Take care.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3535" target="_blank">00:58:55.100</a></span> | <span class="t">Thank you, Rafa.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yTjm_z8JaDU&t=3535" target="_blank">00:58:55.540</a></span> | <span class="t">Bye.</span></div></div></body></html>