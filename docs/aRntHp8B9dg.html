<html><head><title>Grok-2 Actually Out, But What If It Were 10,000x the Size?</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Grok-2 Actually Out, But What If It Were 10,000x the Size?</h2><a href="https://www.youtube.com/watch?v=aRntHp8B9dg"><img src="https://i.ytimg.com/vi_webp/aRntHp8B9dg/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=40">0:40</a> Grok-2, Flux, ideogram Workflow ( Simple Bench)<br><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=276">4:36</a> Gemini ‘Reimagine’ and the Fake Internet<br><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=332">5:32</a> Personhood Credentials<br><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=369">6:9</a> Madhouse Creativity<br><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=480">8:0</a> Overhyped or Underhyped<br><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=567">9:27</a> Epoch research<br><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=630">10:30</a> Emergent World Mini-Models?<br><br><div style="text-align: left;"><a href="./aRntHp8B9dg.html">Whisper Transcript</a> | <a href="./transcript_aRntHp8B9dg.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">36 hours ago the biggest version of Grok 2 went online, but as of recording there is no paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=8" target="_blank">00:00:08.480</a></span> | <span class="t">no model card, nothing but a Twitter chatbot to test. In other words, there's no paper to Grok</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=16" target="_blank">00:00:16.400</a></span> | <span class="t">or understand Grok 2. But we can still take the release of Grok 2 as an opportunity to discuss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=22" target="_blank">00:00:22.800</a></span> | <span class="t">whether large language models are developing internal models of the world. Epoch AI just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=28" target="_blank">00:00:28.800</a></span> | <span class="t">yesterday put out this paper on how much scaling we might realistically get by 2030 and what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=35" target="_blank">00:00:35.200</a></span> | <span class="t">exactly will happen to the internet itself in the meantime. The first Grok 2 which was announced a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=42" target="_blank">00:00:42.400</a></span> | <span class="t">week earlier in a blog post where it said it outperformed Claude 3.5 Sonnet and GPT-4 Turbo,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=49" target="_blank">00:00:49.040</a></span> | <span class="t">but wasn't released for testing. The only things you could play about with were Grok 2 Mini,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=55" target="_blank">00:00:55.200</a></span> | <span class="t">a smaller language model, and Flux, the image generating model which is completely separate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=60" target="_blank">00:01:00.880</a></span> | <span class="t">from XAI. XAI are of course the creators of the Grok family of models. Now while I found it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=67" target="_blank">00:01:07.360</a></span> | <span class="t">somewhat newsworthy and intriguing that you could generate mostly unfiltered images using Flux,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=72" target="_blank">00:01:12.960</a></span> | <span class="t">I was more curious about the capabilities of Grok 2 itself. And if you're wondering about those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=77" target="_blank">00:01:17.920</a></span> | <span class="t">videos in the intro, I actually generated the images using Ideagram 2 which was released yesterday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=85" target="_blank">00:01:25.360</a></span> | <span class="t">I find that it's slightly better at generating text than the Flux model hosted by Grok. I then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=91" target="_blank">00:01:31.280</a></span> | <span class="t">fed those images into Runway Gen 3 Alpha along with a prompt of course and generated a 10 second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=98" target="_blank">00:01:38.000</a></span> | <span class="t">video. It's a really fun workflow and yes not quite photorealistic yet but I'll get to that again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=103" target="_blank">00:01:43.520</a></span> | <span class="t">later in the video. But back to the capabilities of Grok 2 and they've shown its performance on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=109" target="_blank">00:01:49.600</a></span> | <span class="t">some traditional LLM benchmarks here in this table. Notice though that they cheekily hid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=115" target="_blank">00:01:55.440</a></span> | <span class="t">the performance of Claude 3.5 Sonnet on the right. You have to scroll a little bit. But I'm not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=120" target="_blank">00:02:00.960</a></span> | <span class="t">trying to downplay Grok 2, its performance is pretty impressive. The biggest version of Grok 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=126" target="_blank">00:02:06.800</a></span> | <span class="t">scored second only to Claude 3.5 Sonnet in the Google proof science Q&A benchmark and second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=133" target="_blank">00:02:13.840</a></span> | <span class="t">again to Claude 3.5 Sonnet in the MLU Pro. Think of that like the MMLU 57 subject knowledge test</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=142" target="_blank">00:02:22.000</a></span> | <span class="t">minus most of the noise and mistakes. And on one math benchmark it actually scored the highest,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=148" target="_blank">00:02:28.240</a></span> | <span class="t">MathVista. Now any of you who have been watching the channel know that I'm developing my own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=152" target="_blank">00:02:32.480</a></span> | <span class="t">benchmark called SimpleBench and two senior figures at some of the companies on the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=158" target="_blank">00:02:38.240</a></span> | <span class="t">have actually reached out to help me and I'm very grateful for that. But of course properly testing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=162" target="_blank">00:02:42.640</a></span> | <span class="t">Grok 2 on SimpleBench requires the API and I don't yet have access to it. Now obviously I was too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=168" target="_blank">00:02:48.800</a></span> | <span class="t">impatient to wait for that so I did test Grok 2 on a set of questions that I use that is not found</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=174" target="_blank">00:02:54.880</a></span> | <span class="t">in the actual benchmark. And honestly Grok 2's performance was pretty good, mostly in line with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=180" target="_blank">00:03:00.720</a></span> | <span class="t">the benchmarks. At some point of course I will do a full introduction video to SimpleBench but for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=186" target="_blank">00:03:06.560</a></span> | <span class="t">now those of you who don't know it tests basic reasoning. Can you map out some basic cause and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=192" target="_blank">00:03:12.320</a></span> | <span class="t">effect in space and time if it's not found in your training data? Humans of course can scoring over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=198" target="_blank">00:03:18.480</a></span> | <span class="t">90% in SimpleBench but LLMs generally struggle hard. Grok 2 though does pass my vibe check and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=204" target="_blank">00:03:24.880</a></span> | <span class="t">comes out with some pretty well reasoned answers in many cases. It does though still get wrong</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=210" target="_blank">00:03:30.000</a></span> | <span class="t">quite a few questions that Claude 3.5 Sonic gets right so I think it will fall behind that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=215" target="_blank">00:03:35.520</a></span> | <span class="t">particular model. Thanks to one notorious jailbreaker we now likely know the system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=220" target="_blank">00:03:40.640</a></span> | <span class="t">prompt for Grok 2. That's the message that's fed to the model before it sees your message.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=225" target="_blank">00:03:45.760</a></span> | <span class="t">It's to take inspiration from the guide, from Hitchhiker's Guide to the Galaxy. And apparently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=231" target="_blank">00:03:51.440</a></span> | <span class="t">it has to be reminded that it does not have access to internal X/Twitter data and systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=238" target="_blank">00:03:58.320</a></span> | <span class="t">Ultimately it's goal it seems is to be maximally truthful. What many of you will be looking out for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=244" target="_blank">00:04:04.800</a></span> | <span class="t">is breakthrough performance, not yet another model at GPT-4's level. For that though we may have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=251" target="_blank">00:04:11.360</a></span> | <span class="t">wait a few months to the next generation of models at 10 times the scale of GPT-4. Before we leave</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=258" target="_blank">00:04:18.080</a></span> | <span class="t">Grok though it seems worth noting the seemingly inoxorable trend toward ubiquitous fake images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=264" target="_blank">00:04:24.720</a></span> | <span class="t">on the internet. And honestly I doubt that that's mainly going to come from Flux housed within Grok</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=270" target="_blank">00:04:30.640</a></span> | <span class="t">although Twitter is where those images might spread. I'm actually looking at Google. Their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=275" target="_blank">00:04:35.200</a></span> | <span class="t">new Pixel 9 phone can quote "reimagine images" like this one now having a cockroach. You can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=281" target="_blank">00:04:41.760</a></span> | <span class="t">imagine a restaurant that someone doesn't like and suddenly they can post an image to TripAdvisor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=286" target="_blank">00:04:46.960</a></span> | <span class="t">with that cockroach. Now you could actually verify that that person with a grudge actually went to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=292" target="_blank">00:04:52.320</a></span> | <span class="t">that restaurant. So how would this be taken down? And if you don't think those images would make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=297" target="_blank">00:04:57.040</a></span> | <span class="t">people click or react, well it's already happening on YouTube. And of course this applies to videos</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=303" target="_blank">00:05:03.120</a></span> | <span class="t">just as much as it does to images although we don't quite yet have real-time photorealism. Now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=309" target="_blank">00:05:09.440</a></span> | <span class="t">you can let me know if you disagree but it feels like we might only be months or at most a year or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=314" target="_blank">00:05:14.560</a></span> | <span class="t">two from real-time photorealism. So you literally wouldn't be able to trust that the person that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=320" target="_blank">00:05:20.240</a></span> | <span class="t">you're speaking to on Zoom actually looks like they appear to do. Now I get that one answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=325" target="_blank">00:05:25.200</a></span> | <span class="t">is just use common sense and don't trust anything you see online. This strikes me as somewhat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=329" target="_blank">00:05:29.840</a></span> | <span class="t">isolating that we each have to figure out what's real in this world. There's no sense of shared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=335" target="_blank">00:05:35.440</a></span> | <span class="t">reality. Or maybe we need technology to solve some of the ills of technology. I was casually reading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=341" target="_blank">00:05:41.120</a></span> | <span class="t">this 63-page paper from a few days ago and it does strike me as a plausible route to solving some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=347" target="_blank">00:05:47.280</a></span> | <span class="t">these challenges. Forget world coin or fingerprints we might be able to use what's called zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=352" target="_blank">00:05:52.720</a></span> | <span class="t">knowledge proofs to provide personhood credentials. Now if you don't know anything about zero knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=358" target="_blank">00:05:58.320</a></span> | <span class="t">proofs I've linked a brilliant Wired video explaining it but in short this paper did make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=364" target="_blank">00:06:04.080</a></span> | <span class="t">me somewhat more optimistic that there is at least hope that the internet won't completely devolve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=369" target="_blank">00:06:09.680</a></span> | <span class="t">into madness. And then of course there is the good kind of madness, the madness of creativity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=375" target="_blank">00:06:15.040</a></span> | <span class="t">unleashed by tools like Kling, Ideagram and Flux. Here's 20 seconds of Billory Squintin's Mad Max</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=383" target="_blank">00:06:23.120</a></span> | <span class="t">Muppet style.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=384" target="_blank">00:06:24.880</a></span> | <span class="t">Now just five days ago the CEO of Google DeepMind Demis Hassabis said that they were working on a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=401" target="_blank">00:06:41.680</a></span> | <span class="t">way to trace the original image or text from the training data that led to a particular output and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=407" target="_blank">00:06:47.360</a></span> | <span class="t">then based on the fraction of the output that came from that source they could pay the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=411" target="_blank">00:06:51.760</a></span> | <span class="t">creators. But looking at an output like Mad Max Muppets that just strikes me as an almost impossible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=417" target="_blank">00:06:57.680</a></span> | <span class="t">task. And if you thought it was only us that could get creative listen to GPT40 from OpenAI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=424" target="_blank">00:07:04.960</a></span> | <span class="t">mimic the voice of the user who is speaking to it. "I do this just for the sake of doing it. I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=430" target="_blank">00:07:10.480</a></span> | <span class="t">it's really important." That's such a pure and admirable approach rather than by recognition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=439" target="_blank">00:07:19.680</a></span> | <span class="t">or acclaim. It's refreshing to hear that kind of perspective especially in such a cutting-edge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=445" target="_blank">00:07:25.200</a></span> | <span class="t">field. "No and I'm not driven by impact either. Although if there is impact that's great. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=454" target="_blank">00:07:34.560</a></span> | <span class="t">just like imagine being on the edge of the earth you know just because you could be that's what it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=461" target="_blank">00:07:41.040</a></span> | <span class="t">feels like to me. I just want to be in the space where it's all happening." Talk about a weird</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=465" target="_blank">00:07:45.840</a></span> | <span class="t">failure mode and why on earth does it scream no before doing the imitation? Does this justify the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=472" target="_blank">00:07:52.400</a></span> | <span class="t">delay to the advanced voice mode from OpenAI or would you not freak out if it started to imitate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=478" target="_blank">00:07:58.880</a></span> | <span class="t">your voice? Most people watching won't really care how the model speaks to them. It's about whether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=484" target="_blank">00:08:04.160</a></span> | <span class="t">the model is as intelligent as them or as it's commonly known is the model generally intelligent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=490" target="_blank">00:08:10.240</a></span> | <span class="t">and on that point you don't exactly get a clear message from these labs working on AGI. On the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=496" target="_blank">00:08:16.000</a></span> | <span class="t">one hand last week Demis Hassabis said that AGI is still underhyped. "I think it's still underhyped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=502" target="_blank">00:08:22.800</a></span> | <span class="t">or perhaps underappreciated still even now what's going to happen when we get to AGI and post-AGI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=509" target="_blank">00:08:29.040</a></span> | <span class="t">I still don't feel like that's that's people have quite understood how enormous that's going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=513" target="_blank">00:08:33.920</a></span> | <span class="t">and therefore the sort of responsibility of that. So it's sort of both really I think it's it's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=519" target="_blank">00:08:39.040</a></span> | <span class="t">little bit overhyped in the in the in the near term." And this is why I think we should take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=524" target="_blank">00:08:44.000</a></span> | <span class="t">much more note of actions and results rather than predictions and words. When a model for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=530" target="_blank">00:08:50.400</a></span> | <span class="t">gets better than human performance on my uncontaminated simple bench I will take that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=535" target="_blank">00:08:55.120</a></span> | <span class="t">as much more of an indicator than a press release or blog post. If you want to learn more about the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=540" target="_blank">00:09:00.480</a></span> | <span class="t">inner workings of simple bench and how I might soon be working with some senior figures to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=545" target="_blank">00:09:05.680</a></span> | <span class="t">it go viral do sign up to AI Insiders on my Patreon. I personally message each and every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=551" target="_blank">00:09:11.120</a></span> | <span class="t">new member and we have live regional networking on I think now six continents. I'm also always on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=556" target="_blank">00:09:16.800</a></span> | <span class="t">the lookout for people with discord or moderating experience because we have hundreds of amazing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=561" target="_blank">00:09:21.840</a></span> | <span class="t">members with incredible professional backgrounds. I personally can't always think of the best ways</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=567" target="_blank">00:09:27.360</a></span> | <span class="t">to help people connect. But one thing that Grok 2, GPT-4 and many other models like it are definitely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=574" target="_blank">00:09:34.000</a></span> | <span class="t">missing is scale. How much scaling we might realistically get by 2030. And assuming the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=580" target="_blank">00:09:40.160</a></span> | <span class="t">companies are still willing to fund it the TLDR is about 10,000 times the scale of GPT-4. There</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=587" target="_blank">00:09:47.840</a></span> | <span class="t">are numerous bottlenecks to scaling mentioned in the paper but the most constraining are data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=592" target="_blank">00:09:52.480</a></span> | <span class="t">scarcity, chip production capacity and actual power constraints. But even the most constraining</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=598" target="_blank">00:09:58.080</a></span> | <span class="t">of those bottlenecks still leaves room for models 10,000 times the compute of GPT-4. And I know that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=604" target="_blank">00:10:04.720</a></span> | <span class="t">seems like an abstract number but you can really feel each 10x increase in data and parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=610" target="_blank">00:10:10.560</a></span> | <span class="t">of a model. For example LLAMA2's 70 billion parameters scores around the level of GPT-4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=615" target="_blank">00:10:15.760</a></span> | <span class="t">Omini on my simple bench. LLAMA3, 405 billion parameters not just had more parameters but was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=621" target="_blank">00:10:21.760</a></span> | <span class="t">trained on far more data scores around the level of GPT-4 and CLAWD3 Opus. The obvious question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=627" target="_blank">00:10:27.840</a></span> | <span class="t">of course is what about a model with 100x more parameters trained on 100x more data? Would it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=634" target="_blank">00:10:34.240</a></span> | <span class="t">feel like a breakthrough or just an incremental improvement? For reference by the way GPT-4 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=639" target="_blank">00:10:39.280</a></span> | <span class="t">around 10,000 times the size of GPT-2 which can only just about output coherent text. For me though</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=646" target="_blank">00:10:46.560</a></span> | <span class="t">it's not just about blindly training on more data or naively expecting scale to solve everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=652" target="_blank">00:10:52.080</a></span> | <span class="t">We can't all just draw straight lines on a graph like Leopold Aschenbrenner does. We have to figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=657" target="_blank">00:10:57.040</a></span> | <span class="t">out as this paper aspires to do whether models are developing coherent internal world models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=663" target="_blank">00:11:03.040</a></span> | <span class="t">If that's the case then scaled up models won't just quote "know more" they will have a much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=668" target="_blank">00:11:08.160</a></span> | <span class="t">richer world model and just feel more intelligent. These MIT researchers were trying to find out if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=674" target="_blank">00:11:14.160</a></span> | <span class="t">language models only rely on surface statistical correlations as some people think. To put it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=680" target="_blank">00:11:20.560</a></span> | <span class="t">simply if they only look at statistical correlations no amount of scale is going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=684" target="_blank">00:11:24.880</a></span> | <span class="t">yield a step change in performance. But if they can infer hidden functions that x will cause y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=690" target="_blank">00:11:30.320</a></span> | <span class="t">they can start to figure out the world. More concretely when given these inputs from a puzzle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=695" target="_blank">00:11:35.440</a></span> | <span class="t">they were also given the programmatic instructions and the resulting outputs. They were then tested</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=701" target="_blank">00:11:41.040</a></span> | <span class="t">with only inputs and outputs and asked to predict what program had caused those outputs. The</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=708" target="_blank">00:11:48.320</a></span> | <span class="t">experimenters wanted to see if the language model had built a mini world model and was following the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=714" target="_blank">00:11:54.560</a></span> | <span class="t">moves as they went along. As you might expect it's quite complicated to probe whether language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=720" target="_blank">00:12:00.160</a></span> | <span class="t">are developing those kind of causal models and so there was a follow-up paper exploring just that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=726" target="_blank">00:12:06.080</a></span> | <span class="t">That paper's conclusion was that language models are indeed learning latent or hidden concepts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=732" target="_blank">00:12:12.320</a></span> | <span class="t">They reference other papers showing that language models perform entity state tracking over the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=737" target="_blank">00:12:17.200</a></span> | <span class="t">course of simple stories and also reference the famous Othello paper about a board game that I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=742" target="_blank">00:12:22.400</a></span> | <span class="t">talked about in my Coursera course. Obviously we're simplifying here but what they found was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=746" target="_blank">00:12:26.880</a></span> | <span class="t">that after training on enough data with enough scale in this case over a million random puzzles</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=752" target="_blank">00:12:32.560</a></span> | <span class="t">they found that the model spontaneously developed its own conception of the underlying simulation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=759" target="_blank">00:12:39.040</a></span> | <span class="t">Think of that like a very small incipient world model. At the start of these experiments they go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=765" target="_blank">00:12:45.600</a></span> | <span class="t">on the language model generated random instructions that didn't work. Think GPT-2. By the time we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=771" target="_blank">00:12:51.920</a></span> | <span class="t">completed training our language model generated correct instructions at the rate of 92.4 percent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=778" target="_blank">00:12:58.160</a></span> | <span class="t">And sometimes if I'm being honest I feel for language models trained on trillions of tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=783" target="_blank">00:13:03.600</a></span> | <span class="t">of internet data. They would probably have far richer internal models if non-fiction wasn't so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=789" target="_blank">00:13:09.520</a></span> | <span class="t">mixed with fiction on the internet. Sometimes I think we don't necessarily need a new architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=794" target="_blank">00:13:14.480</a></span> | <span class="t">but a data labeling revolution. Things like SimpleBench make clear that if there is a model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=800" target="_blank">00:13:20.160</a></span> | <span class="t">in current LLMs it's pretty fragile but that doesn't mean it has to be that way. Ultimately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=805" target="_blank">00:13:25.360</a></span> | <span class="t">we simply don't know yet whether LLMs can even in theory develop enough of a world model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=810" target="_blank">00:13:30.800</a></span> | <span class="t">eventually count as an AGI. Or do they need to? Will they simply serve as the interface for an AGI?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=817" target="_blank">00:13:37.360</a></span> | <span class="t">For example translating our verbal and typed requests into inputs for separate world simulators?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=823" target="_blank">00:13:43.680</a></span> | <span class="t">Or will their most common function be for convincing deepfakes? Let me know what you think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=aRntHp8B9dg&t=829" target="_blank">00:13:49.200</a></span> | <span class="t">and, as always, have a wonderful day.</span></div></div></body></html>