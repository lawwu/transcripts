<html><head><title>LongNet: Scaling Transformers to 1,000,000,000 tokens: Python Code + Explanation</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>LongNet: Scaling Transformers to 1,000,000,000 tokens: Python Code + Explanation</h2><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ"><img src="https://i.ytimg.com/vi/nC2nU9j9DVQ/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=330">5:30</a> How it works<br><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=840">14:0</a> Computational Complexity<br><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1157">19:17</a> Attention visualization<br><br><div style="text-align: left;"><a href="./nC2nU9j9DVQ.html">Whisper Transcript</a> | <a href="./transcript_nC2nU9j9DVQ.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys, welcome to my channel. Today we will be talking about a new model that came recently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=5" target="_blank">00:00:05.440</a></span> | <span class="t">it's called LongNet and it's a model based on the transformer that came from Microsoft Research Asia</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=12" target="_blank">00:00:12.720</a></span> | <span class="t">just two weeks ago I guess and the basic idea is that they want to scale the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=20" target="_blank">00:00:20.800</a></span> | <span class="t">but the wonderful idea is that they managed to scale it to 1 billion tokens. Now if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=27" target="_blank">00:00:27.120</a></span> | <span class="t">familiar with language models you know that the sequence length makes a huge impact on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=33" target="_blank">00:00:33.120</a></span> | <span class="t">performance of the model because the sequence length tells you how many tokens can relate to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=40" target="_blank">00:00:40.080</a></span> | <span class="t">each other when performing the attention mechanism which allows the model for example to have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=45" target="_blank">00:00:45.920</a></span> | <span class="t">longer context or a shorter context for example for a model like GPT you want to be able to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=51" target="_blank">00:00:51.760</a></span> | <span class="t">a long context so that the model can watch words that were written a long time ago to calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=60" target="_blank">00:01:00.080</a></span> | <span class="t">the next token for example and the LongNet is able to scale this to 1 billion token so to show you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=67" target="_blank">00:01:07.760</a></span> | <span class="t">how amazing this is I want to show you this graph in which we show that for example the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=72" target="_blank">00:01:12.960</a></span> | <span class="t">length of GPT was just 512 and then we have this pulse transformer 12,064,262,1 million but however</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=84" target="_blank">00:01:24.160</a></span> | <span class="t">they go 1,000 times more with LongNet 1 billion tokens and just to imagine the scale it's really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=93" target="_blank">00:01:33.760</a></span> | <span class="t">amazing because you can basically feed all the wikipedia of the text of wikipedia to the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=102" target="_blank">00:01:42.160</a></span> | <span class="t">and the model will be able to calculate the attention using all these tokens but let's see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=108" target="_blank">00:01:48.160</a></span> | <span class="t">how this all works first of all LongNet as claimed by the authors has significant advantages the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=114" target="_blank">00:01:54.640</a></span> | <span class="t">first is that it is linear the computation the computational complexity is linear with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=120" target="_blank">00:02:00.320</a></span> | <span class="t">sequence length and we will see why and how there is a logarithmic dependency between tokens so it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=127" target="_blank">00:02:07.600</a></span> | <span class="t">means that basically the more distance the tokens the dependency is less powerful is less they are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=136" target="_blank">00:02:16.240</a></span> | <span class="t">how to say the the attention mechanism is less powerful between two tokens that are very far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=142" target="_blank">00:02:22.160</a></span> | <span class="t">from each other and more strong between two tokens that are close to each other and it can be trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=149" target="_blank">00:02:29.120</a></span> | <span class="t">on a distributed network it means that we can calculate this attention mechanism on a distributed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=155" target="_blank">00:02:35.040</a></span> | <span class="t">system so multiple gpus or multiple computers and it is a drop-in replacement for standard attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=162" target="_blank">00:02:42.240</a></span> | <span class="t">which means that if we already have a model that uses the attention mechanism we can basically just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=167" target="_blank">00:02:47.440</a></span> | <span class="t">replace the attention mechanism without changing all the rest of the model and it will work as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=173" target="_blank">00:02:53.280</a></span> | <span class="t">before but with this new improved attention that can use longer sequence lengths and if you're not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=180" target="_blank">00:03:00.720</a></span> | <span class="t">familiar with the transformer models i kindly ask you to watch my previous video in which i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=185" target="_blank">00:03:05.360</a></span> | <span class="t">explained the attention model the attention mechanism and the transformer model and i will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=191" target="_blank">00:03:11.600</a></span> | <span class="t">review it basically here to show you what was the problem with the attention mechanism before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=196" target="_blank">00:03:16.240</a></span> | <span class="t">so here i have the slides from my previous video and we can see here the self-attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=202" target="_blank">00:03:22.080</a></span> | <span class="t">with the self-attention we had before we had the matrix called the q k and v and the q was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=207" target="_blank">00:03:27.440</a></span> | <span class="t">basically the sentence which is a matrix of sequence length by d model d model is the size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=213" target="_blank">00:03:33.440</a></span> | <span class="t">of the vector of the representing the embedding of each word and when we do the multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=219" target="_blank">00:03:39.840</a></span> | <span class="t">of query multiplied by the k or the transpose of the k to produce this matrix requires a number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=226" target="_blank">00:03:46.880</a></span> | <span class="t">operations that is n to the power of 2 multiplied by the d model why because so it's n to the power</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=235" target="_blank">00:03:55.920</a></span> | <span class="t">of 2 multiplied by the d model why is this the case well to produce for example this item here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=242" target="_blank">00:04:02.000</a></span> | <span class="t">in this the output of the softmax we need to do the dot product of this word so the word your with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=247" target="_blank">00:04:07.760</a></span> | <span class="t">the word your so the word with itself and the vector the dot product of two vectors that are d</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=256" target="_blank">00:04:16.400</a></span> | <span class="t">d model long is d model and we need to do this for all the items in this matrix and the items</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=264" target="_blank">00:04:24.400</a></span> | <span class="t">in this matrix are n to the power of 2 so the sequence to the power of 2 and this is the reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=270" target="_blank">00:04:30.320</a></span> | <span class="t">why the complexity of the self-attention before but also of the attention of the cross attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=275" target="_blank">00:04:35.600</a></span> | <span class="t">before was in the order of n to the power of 2 by d and this table is also this comparison is also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=282" target="_blank">00:04:42.400</a></span> | <span class="t">present in the paper here so the vanilla attention had a complexity of n to the power of 2 multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=288" target="_blank">00:04:48.000</a></span> | <span class="t">by d but with this new model the long net we have an attention model an attention mechanism that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=295" target="_blank">00:04:55.360</a></span> | <span class="t">in the order of n multiplied by so it grows linearly with the sequence length and we will see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=301" target="_blank">00:05:01.920</a></span> | <span class="t">how so here the introduction the authors claim that actually the sequence length is one of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=308" target="_blank">00:05:08.000</a></span> | <span class="t">main problems with the language models and how scaling it is a priority and here they showed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=315" target="_blank">00:05:15.600</a></span> | <span class="t">how this work is better than the other basically because we reduce the number of computations to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=320" target="_blank">00:05:20.720</a></span> | <span class="t">calculate the attention and how they scale it to 1 billion we will see all of this in detail i will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=326" target="_blank">00:05:26.640</a></span> | <span class="t">also also show some visualizations of how this works the basic principle is attention allocation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=333" target="_blank">00:05:33.200</a></span> | <span class="t">decreases exponentially as the distance between the tokens grow now let's have a look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=339" target="_blank">00:05:39.040</a></span> | <span class="t">picture to see how this works before we had let's go to my previous slide before we had a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=347" target="_blank">00:05:47.520</a></span> | <span class="t">like this so we calculated the attention between all tokens with all other tokens but with the log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=353" target="_blank">00:05:53.840</a></span> | <span class="t">net we don't do this imagine we have a sequence length of 16 and of course in the upper part of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=360" target="_blank">00:06:00.240</a></span> | <span class="t">this matrix we don't calculate the attention because we want the model the attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=364" target="_blank">00:06:04.240</a></span> | <span class="t">to be causal so we don't want the token number one to be related to the token number eight but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=369" target="_blank">00:06:09.040</a></span> | <span class="t">of course we want the token number eight to be able to watch the token number one for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=373" target="_blank">00:06:13.680</a></span> | <span class="t">and so the oldest part is empty and the second thing is instead of calculating all the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=385" target="_blank">00:06:25.040</a></span> | <span class="t">all the dot products of all the tokens with all other tokens what we do is we split the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=392" target="_blank">00:06:32.560</a></span> | <span class="t">into small windows of different sizes so first we start with the size of four in this case the n is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=399" target="_blank">00:06:39.680</a></span> | <span class="t">the number of tokens of this sentence we split into four segments and here are called segments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=406" target="_blank">00:06:46.800</a></span> | <span class="t">each one of size four and we calculate the attention between all the words in this small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=413" target="_blank">00:06:53.280</a></span> | <span class="t">box with all the other words and we do it for all these small segments here here we also see another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=420" target="_blank">00:07:00.880</a></span> | <span class="t">parameter called deleted rate because we are not skipping any token so we are calculating all token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=426" target="_blank">00:07:06.400</a></span> | <span class="t">with all other token we do it again this time however by increasing the size of the window so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=432" target="_blank">00:07:12.320</a></span> | <span class="t">we don't use a window of size four we do use a window of size eight and we calculate the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=438" target="_blank">00:07:18.240</a></span> | <span class="t">between each word and every other word in this window so basically and we do it all for all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=447" target="_blank">00:07:27.200</a></span> | <span class="t">windows until they cover all the sequence length again then we increase the we double the segment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=455" target="_blank">00:07:35.200</a></span> | <span class="t">length so the size of the window that we watch but we also double the deletion rate so how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=461" target="_blank">00:07:41.360</a></span> | <span class="t">tokens we skip so we relate the token number zero with the token number four and then we skip three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=469" target="_blank">00:07:49.040</a></span> | <span class="t">and then we do again the dot product and we then we skip three and we do again the dot product so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=473" target="_blank">00:07:53.600</a></span> | <span class="t">we skip every three here we skip everyone here we skip zero why do we do this because we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=481" target="_blank">00:08:01.280</a></span> | <span class="t">smaller window we want the attention mechanism to be more precise because if you for example when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=486" target="_blank">00:08:06.720</a></span> | <span class="t">you read a book you know when you read a paragraph the words in the paragraph should be very related</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=492" target="_blank">00:08:12.080</a></span> | <span class="t">to each other because they're talking about where something very specific this one can be thought of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=498" target="_blank">00:08:18.000</a></span> | <span class="t">as a chapter so the in the chapter we don't want to relate all the words of all the chapters to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=504" target="_blank">00:08:24.160</a></span> | <span class="t">each other but maybe some parts of the chapters because basically in the same chapter the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=510" target="_blank">00:08:30.960</a></span> | <span class="t">paragraph more or less will talk about the same topics but it's not like we need the the dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=517" target="_blank">00:08:37.440</a></span> | <span class="t">product between all the words in the chapter with all the other words in the same chapter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=522" target="_blank">00:08:42.160</a></span> | <span class="t">and then if we go to the book level we don't want the the dot product between every word of the book</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=529" target="_blank">00:08:49.120</a></span> | <span class="t">with all the other words but we want some general idea so basically we want some words to so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=533" target="_blank">00:08:53.600</a></span> | <span class="t">general theme of the book should be present but not every word with other words so this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=538" target="_blank">00:08:58.960</a></span> | <span class="t">idea that we use also for this attention mechanism here for small windows so words that are very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=546" target="_blank">00:09:06.000</a></span> | <span class="t">close to each other we do the dot product so words that are more far from each other we don't do all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=551" target="_blank">00:09:11.760</a></span> | <span class="t">the dot products and for very big windows we do even less another thing is that we the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=559" target="_blank">00:09:19.760</a></span> | <span class="t">dot products in each window no matter the size of the window or the relation rate is always the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=565" target="_blank">00:09:25.280</a></span> | <span class="t">so here for example we have four plus three plus two plus one dot products in this window and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=571" target="_blank">00:09:31.760</a></span> | <span class="t">the same number of dot products that we have here and it's the same number of dot products that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=576" target="_blank">00:09:36.160</a></span> | <span class="t">have this that we have here now you may be wondering well this is not relating the token number one to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=586" target="_blank">00:09:46.160</a></span> | <span class="t">the token number 16 for example right yeah but what if we overlapped all of them together we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=593" target="_blank">00:09:53.120</a></span> | <span class="t">obtain something like this and you can see here that still the token number one is not related</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=599" target="_blank">00:09:59.280</a></span> | <span class="t">to the token number 16 but we can always find a connection a way of going from token number one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=608" target="_blank">00:10:08.240</a></span> | <span class="t">to token number 16 by using intermediate tokens and we will see later how this is possible i also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=613" target="_blank">00:10:13.920</a></span> | <span class="t">made a tool to visualize this and let's watch some details from the paper so first we start by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=621" target="_blank">00:10:21.920</a></span> | <span class="t">introducing the vanilla transformer so this is the basically the the attention mechanism as in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=629" target="_blank">00:10:29.280</a></span> | <span class="t">paper attention is all you need and it's the same one that we saw here then basically here they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=635" target="_blank">00:10:35.760</a></span> | <span class="t">describe what is the deleted attention so in the deleted attention we choose a w and the r so a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=642" target="_blank">00:10:42.800</a></span> | <span class="t">window size and the deletion rate and we divide our sequence into n divided by w boxes like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=652" target="_blank">00:10:52.240</a></span> | <span class="t">here n is 16 if the segment length is 4 we will have 4 boxes if the segment length is 8 we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=659" target="_blank">00:10:59.520</a></span> | <span class="t">have 2 boxes etc and we also skip every r tokens actually r minus 1 tokens and all of this actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=670" target="_blank">00:11:10.720</a></span> | <span class="t">as you can see they are independent because here the attention mechanism to be calculated in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=676" target="_blank">00:11:16.080</a></span> | <span class="t">box for example you only need the to have available the embedding of the tokens that are in this box</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=683" target="_blank">00:11:23.520</a></span> | <span class="t">because there is no interconnection between these two boxes so this one and this one can be calculated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=690" target="_blank">00:11:30.240</a></span> | <span class="t">in parallel okay and the next thing is that they calculate the softmax for each boxes so they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=697" target="_blank">00:11:37.600</a></span> | <span class="t">calculate the attention in each of these boxes here and then they combine them together basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=703" target="_blank">00:11:43.760</a></span> | <span class="t">with the concatenation and the another important interesting thing the one we saw before is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=710" target="_blank">00:11:50.400</a></span> | <span class="t">they don't use just one r or one w they use a sequence of r and w's and we will see here that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=718" target="_blank">00:11:58.640</a></span> | <span class="t">this sequence of r and w are geometric sequences with an alpha constant here so in this case the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=725" target="_blank">00:12:05.280</a></span> | <span class="t">alpha constant is 2 basically what they do is they start with a small window so w1 for example equal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=731" target="_blank">00:12:11.520</a></span> | <span class="t">to 4 then each time you multiply the previous window by 2 and also the dilation rate by 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=737" target="_blank">00:12:17.040</a></span> | <span class="t">so from 4 we go to 8 from 8 we go to 16 until we reach the sequence length the same happens with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=742" target="_blank">00:12:22.880</a></span> | <span class="t">the dilation rate at the beginning we don't skip any word then we start skipping 1 then we multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=748" target="_blank">00:12:28.800</a></span> | <span class="t">it by 2 and we skip every 3 and they combine all of this together using these two equations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=756" target="_blank">00:12:36.720</a></span> | <span class="t">basically they calculate the denominator of the softmax for each of this attention so all of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=764" target="_blank">00:12:44.000</a></span> | <span class="t">this and this and then they use it as a weight for a weighted average we can see it here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=770" target="_blank">00:12:50.800</a></span> | <span class="t">and how to transform this into a multi-head attention well basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=776" target="_blank">00:12:56.560</a></span> | <span class="t">before we were start we for each of this combination of segment length and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=784" target="_blank">00:13:04.800</a></span> | <span class="t">dilation rate suppose we have four heads the segment length is 8 and the dilation rate is 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=790" target="_blank">00:13:10.720</a></span> | <span class="t">as you know with the dilation rate of 2 we need to skip every second token so we can calculate it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=799" target="_blank">00:13:19.280</a></span> | <span class="t">like this for the head number one we start from zero and we skip every other token so we calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=804" target="_blank">00:13:24.880</a></span> | <span class="t">from zero and then we skip the one and then we skip we arrive to the two and then three we skip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=811" target="_blank">00:13:31.120</a></span> | <span class="t">and four etc otherwise we can skip the zero and we start from the one so we pass we skip one and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=818" target="_blank">00:13:38.880</a></span> | <span class="t">then we do the other and we keep the same dilation the dilated rate for the head number three and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=824" target="_blank">00:13:44.960</a></span> | <span class="t">head number four we do the same in this case the head number one and the head number three are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=829" target="_blank">00:13:49.920</a></span> | <span class="t">same because actually the the stride is smaller than the number of heads if we had a stride that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=836" target="_blank">00:13:56.720</a></span> | <span class="t">was bigger than the number of heads or equal to the number of heads we would see four different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=841" target="_blank">00:14:01.280</a></span> | <span class="t">patterns here but the basic idea of the multi-head is this one and let's look at the computational</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=847" target="_blank">00:14:07.920</a></span> | <span class="t">complexity of this model and how it is calculated well the computational complexity of this attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=854" target="_blank">00:14:14.000</a></span> | <span class="t">mechanism is basically given by the dot product that we do to calculate the attention so the soft</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=860" target="_blank">00:14:20.240</a></span> | <span class="t">max of the query multiplied by the key and with the vanilla transformer we had n to the power of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=866" target="_blank">00:14:26.560</a></span> | <span class="t">2 multiplied by d but here we have w divided by r to the power of 2 multiplied by d so w is our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=874" target="_blank">00:14:34.800</a></span> | <span class="t">segment size so let's go here r is the dilation rate and what we can see here is that the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=886" target="_blank">00:14:46.240</a></span> | <span class="t">of dot products that we are doing is this one so this the size of the window is w divided by r</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=894" target="_blank">00:14:54.800</a></span> | <span class="t">so w divided by r is 4 so this is the size of the window right and w divided by r is also the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=903" target="_blank">00:15:03.440</a></span> | <span class="t">of tokens in this window for which we will calculate the dot product because you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=909" target="_blank">00:15:09.600</a></span> | <span class="t">that this matrix here even if the size is 8 the number of actual dot product that we will do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=916" target="_blank">00:15:16.000</a></span> | <span class="t">actually not 8 by 8 but 4 by 4 because we are skipping every other token and even if the size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=924" target="_blank">00:15:24.480</a></span> | <span class="t">of this window is 16 by 16 we will not be calculating 16 by 16 dot products we will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=930" target="_blank">00:15:30.880</a></span> | <span class="t">calculating 4 by 4 dot products because we are skipping three tokens and this is the idea behind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=937" target="_blank">00:15:37.440</a></span> | <span class="t">the calculation of the complexity the fact that we are not calculating the dot product between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=943" target="_blank">00:15:43.280</a></span> | <span class="t">all the window all the tokens in a window but only w divided by r multiplied by 2 is the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=954" target="_blank">00:15:54.000</a></span> | <span class="t">of the size of the of the dot products that we will do and each dot product is involves a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=961" target="_blank">00:16:01.600</a></span> | <span class="t">of dimension d so we also multiplied by d and divided by w is the number of boxes so for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=968" target="_blank">00:16:08.640</a></span> | <span class="t">if we are here if when w is 4 the number of boxes is also 4 because n divided by w 4 and when the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=976" target="_blank">00:16:16.960</a></span> | <span class="t">number the w is 8 the number of boxes is 2 because 16 divided by 2 and when the sequence length is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=983" target="_blank">00:16:23.520</a></span> | <span class="t">16 so w is equal to 16 the number of boxes that we get is 16 by 16 so only one so we can see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=992" target="_blank">00:16:32.880</a></span> | <span class="t">the the number of floating point operations that we are doing is proportional to n divided by 2 so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=999" target="_blank">00:16:39.200</a></span> | <span class="t">the number of boxes and in each box we will do w divided by r to the power of 2 multiplied by d</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1004" target="_blank">00:16:44.720</a></span> | <span class="t">operations because of the dot product and you may be wondering that this this the window size is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1012" target="_blank">00:16:52.720</a></span> | <span class="t">still very big right so if you do it in numpy or in pytorch actually the the number of operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1019" target="_blank">00:16:59.520</a></span> | <span class="t">you will do for example for this window of size 16 by 16 is still 16 by 16 but there are better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1025" target="_blank">00:17:05.520</a></span> | <span class="t">ways to represent what are called sparse matrices so this actually is a matrix that is sparse so if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1031" target="_blank">00:17:11.600</a></span> | <span class="t">you create a matrix multiplication algorithm that knows this and that can take into consideration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1038" target="_blank">00:17:18.800</a></span> | <span class="t">that this matrix is sparse then you can do many less operations first you can store less information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1046" target="_blank">00:17:26.800</a></span> | <span class="t">because you know that most of the matrix is zero and the second thing is that you you can perform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1052" target="_blank">00:17:32.400</a></span> | <span class="t">less operations so if you can just skip calculating the dot product for all the positions of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1058" target="_blank">00:17:38.080</a></span> | <span class="t">matrix that are you know are zero then you do less operations and i think the on the authors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1064" target="_blank">00:17:44.800</a></span> | <span class="t">of the paper they created some custom kernel for cuda to do this the another thing the author shows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1071" target="_blank">00:17:51.200</a></span> | <span class="t">okay here this is the number of floating point operations for one window size so for one w but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1078" target="_blank">00:17:58.560</a></span> | <span class="t">we don't have one w we have many w's and we also know that these w's are according to a geometric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1085" target="_blank">00:18:05.280</a></span> | <span class="t">sequence as written in the paper written here we set w and r to geometric sequences geometric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1092" target="_blank">00:18:12.080</a></span> | <span class="t">sequence means that we take the previous w and to get the next w we multiply it by one alpha and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1097" target="_blank">00:18:17.600</a></span> | <span class="t">this alpha is fixed starting from w zero or here for example w zero is equal to four and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1105" target="_blank">00:18:25.360</a></span> | <span class="t">dilation rate r zero is equal to one and every time they multiply by two and let's go back here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1113" target="_blank">00:18:33.600</a></span> | <span class="t">okay so they need to combine the floating point operations for all of this w and r's and they do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1120" target="_blank">00:18:40.400</a></span> | <span class="t">it here but considering that this w and r are actually the result of a geometric sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1126" target="_blank">00:18:46.240</a></span> | <span class="t">this becomes depending on alpha and w zero so the initial w that you choose and if we watch this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1133" target="_blank">00:18:53.440</a></span> | <span class="t">expression here we can see that it's the number of floating point operations that you need to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1139" target="_blank">00:18:59.520</a></span> | <span class="t">calculate this combined attention here so the combination of all these w and r's here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1147" target="_blank">00:19:07.120</a></span> | <span class="t">proportional to n and d it linear it grows linearly with n and d so just like it's written here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1155" target="_blank">00:19:15.760</a></span> | <span class="t">another interesting fact is that even if two words are not connected to each other directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1162" target="_blank">00:19:22.240</a></span> | <span class="t">by a dot product we can calculate the information distance between them that is how many jumps you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1169" target="_blank">00:19:29.920</a></span> | <span class="t">need to make to go from one token to the next let me explain this better for example let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1176" target="_blank">00:19:36.640</a></span> | <span class="t">watch my notebook that i made here this is a notebook that i made specifically for learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1182" target="_blank">00:19:42.160</a></span> | <span class="t">this model and i wanted to actually test how it works so we imagine we have a sequence length of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1188" target="_blank">00:19:48.480</a></span> | <span class="t">16 and so here in the my representation is from 0 to 15 not from 1 to 16 but the idea is the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1195" target="_blank">00:19:55.840</a></span> | <span class="t">and we know that we will be calculating for example the first attention will be calculated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1201" target="_blank">00:20:01.920</a></span> | <span class="t">for this box this box this box this box then another one that will be this one this one and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1209" target="_blank">00:20:09.040</a></span> | <span class="t">the last one that is this one and this is exactly the same is the combined attention that we see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1216" target="_blank">00:20:16.960</a></span> | <span class="t">here so the overlapping of this attention this attention this attention is exactly the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1222" target="_blank">00:20:22.480</a></span> | <span class="t">just the colors are different now let's how let's look how the words for example the token number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1229" target="_blank">00:20:29.440</a></span> | <span class="t">0 and the token number 15 so the last token are related the idea is that we cannot go from token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1235" target="_blank">00:20:35.920</a></span> | <span class="t">number 0 to token number 15 directly because there is no dot product between 0 and 15 but we can find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1242" target="_blank">00:20:42.320</a></span> | <span class="t">a path to go there so from 0 to 15 we can go from 0 to 12 and from 12 to 15 let's see so from 0 to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1250" target="_blank">00:20:50.640</a></span> | <span class="t">12 there is a dot product right then there is a dot product between 12 and itself because in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1256" target="_blank">00:20:56.400</a></span> | <span class="t">attention mechanism we are always making the dot product between every node every token and itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1261" target="_blank">00:21:01.440</a></span> | <span class="t">and the token number 12 it's related to the token number 15 so there is a dot product between the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1268" target="_blank">00:21:08.640</a></span> | <span class="t">token number 12 and 15 so actually the token number 0 it is related to the token number 15</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1274" target="_blank">00:21:14.000</a></span> | <span class="t">through the token number 12 and we can find this path for all the tokens and i can prove it i i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1280" target="_blank">00:21:20.960</a></span> | <span class="t">show it in this notebook that for example all the nodes are reachable from the node number zero so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1287" target="_blank">00:21:27.680</a></span> | <span class="t">from the token number zero we can reach all the tokens by using different uh tokens as intermediate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1294" target="_blank">00:21:34.160</a></span> | <span class="t">and in this paper let's go to the paper uh here they show that the maximum number of jumps that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1302" target="_blank">00:21:42.080</a></span> | <span class="t">you need to make to go from one token to any other token is uh less than is gross with the logarithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1311" target="_blank">00:21:51.520</a></span> | <span class="t">of the sequence length that is if the sequence length is uh let's say 10 times bigger you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1319" target="_blank">00:21:59.520</a></span> | <span class="t">need 10 times you don't need to make 10 times bigger jumps to to go from one token to the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1325" target="_blank">00:22:05.680</a></span> | <span class="t">this also so why do we are we talking about jumps because it means also how strong is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1331" target="_blank">00:22:11.840</a></span> | <span class="t">relationship between two tokens because if we calculate the dot product between two tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1336" target="_blank">00:22:16.880</a></span> | <span class="t">then that means that the model will find that immediately that that dot product so the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1342" target="_blank">00:22:22.720</a></span> | <span class="t">will learn to relate immediately that two tokens but if we have intermediate tokens the model will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1348" target="_blank">00:22:28.960</a></span> | <span class="t">make will take more iterations to find this connection between tokens so it will the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1353" target="_blank">00:22:33.520</a></span> | <span class="t">connection between those two tokens will be more weak and this is what the authors claimed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1358" target="_blank">00:22:38.320</a></span> | <span class="t">they claim that the attention mechanism is spread in such a way that the strength of the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1365" target="_blank">00:22:45.360</a></span> | <span class="t">mechanism becomes weaker exponentially with the by increasing the sequence length and or in other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1374" target="_blank">00:22:54.560</a></span> | <span class="t">words we can say that the number of jumps that you need to make grows with the logarithm of n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1379" target="_blank">00:22:59.440</a></span> | <span class="t">and we can do the same for example with other length of tokens for example here i use the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1386" target="_blank">00:23:06.160</a></span> | <span class="t">sequence of sequence length of 16 but we can use 32 for example and visualize it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1392" target="_blank">00:23:12.080</a></span> | <span class="t">and let's see if it's visualizable yeah so basically our um our log net will do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1400" target="_blank">00:23:20.480</a></span> | <span class="t">he will start with um small boxes of size four then he will also calculate the attention for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1407" target="_blank">00:23:27.440</a></span> | <span class="t">the box size eight then also for the box size 16 and also for the box size 32 here we can see the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1416" target="_blank">00:23:36.720</a></span> | <span class="t">overlap attention maxi so all the different sizes but also all the single groups so for example the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1423" target="_blank">00:23:43.520</a></span> | <span class="t">the all the tokens that are directly connected to each other with different color so the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1429" target="_blank">00:23:49.600</a></span> | <span class="t">number zero is directly connected to the known to the token number three and and also the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1436" target="_blank">00:23:56.320</a></span> | <span class="t">number four is directly connected to the token number five because they are part of the same box</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1440" target="_blank">00:24:00.320</a></span> | <span class="t">when they are calculated but the other tokens they have to be inferred for example with a sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1445" target="_blank">00:24:05.600</a></span> | <span class="t">length of 32 we can see that still the token number zero is reachable from every other token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1452" target="_blank">00:24:12.880</a></span> | <span class="t">but by different number of steps for example to go from token number zero to token number 17</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1459" target="_blank">00:24:19.200</a></span> | <span class="t">we need to pass from 16 let's see from 0 to 17 we cannot go directly because there is no dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1466" target="_blank">00:24:26.880</a></span> | <span class="t">here but we can go to 16 there is a dot product here and 16 is related to itself also and 16 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1473" target="_blank">00:24:33.760</a></span> | <span class="t">also related to 17 so we actually can go from 0 to 17 by passing from 16 and this can be done for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1481" target="_blank">00:24:41.440</a></span> | <span class="t">all the nodes and i also made a graph here to visualize this so from 0 we cannot go directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1486" target="_blank">00:24:46.400</a></span> | <span class="t">to 17 but we can go for to 16 and from 16 we can go to 17 and this is the idea of the long net we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1496" target="_blank">00:24:56.880</a></span> | <span class="t">let's go back okay we don't calculate all the dot products to each other with each other so all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1504" target="_blank">00:25:04.240</a></span> | <span class="t">tokens with all the other tokens but we spread this attention mechanism in such a way that words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1511" target="_blank">00:25:11.440</a></span> | <span class="t">that are very close to each other are directly connected and words that are far from each other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1516" target="_blank">00:25:16.800</a></span> | <span class="t">are connected through other tokens and let's watch also in the paper they also show how the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1528" target="_blank">00:25:28.240</a></span> | <span class="t">model can be trained in a distributed way well we already saw it because all of these boxes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1536" target="_blank">00:25:36.080</a></span> | <span class="t">are actually independent from each other so to calculate for example the attention in this box</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1541" target="_blank">00:25:41.440</a></span> | <span class="t">here you need only the embedding of the token number 0 1 2 and 3 and that's it to calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1548" target="_blank">00:25:48.400</a></span> | <span class="t">the attention of mechanism of this box here you need to have the embedding of the token number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1554" target="_blank">00:25:54.720</a></span> | <span class="t">0 2 and 4 and 6 but not of the other and to calculate this one the same etc and another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1565" target="_blank">00:26:05.200</a></span> | <span class="t">interesting thing is that the number of dot products in each box is always constant so if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1570" target="_blank">00:26:10.800</a></span> | <span class="t">we have we can choose the model in such a way that we each computer can hold at most that number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1580" target="_blank">00:26:20.240</a></span> | <span class="t">dot products and so this this mechanism is quite parallelizable and it's really important okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1589" target="_blank">00:26:29.680</a></span> | <span class="t">it's really important because it allows us to scale greatly with the because parallelization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1595" target="_blank">00:26:35.920</a></span> | <span class="t">is very important for us because we can compute the model on the cloud or on different gpus and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1600" target="_blank">00:26:40.880</a></span> | <span class="t">we take can take advantage of this parallelization another interesting thing is that the runtime</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1608" target="_blank">00:26:48.160</a></span> | <span class="t">we can see here that with the sequence length increasing we grow linearly with the with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1614" target="_blank">00:26:54.080</a></span> | <span class="t">runtime but not like exponentially with the vanilla transformer you can see here and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1619" target="_blank">00:26:59.280</a></span> | <span class="t">in the rest of the paper they show how the how the model performs to other previous models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1625" target="_blank">00:27:05.760</a></span> | <span class="t">now my my point is also not to show actually the the results which you can look by yourself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1631" target="_blank">00:27:11.840</a></span> | <span class="t">my my my goal was to actually show the attention mechanism of this new long net and i hope it was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1638" target="_blank">00:27:18.640</a></span> | <span class="t">clear i hope also you will use my python notebook to experiment by yourself i show you how basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1646" target="_blank">00:27:26.240</a></span> | <span class="t">it works here you define the sequence length that you want to visualize and the notebook will only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1651" target="_blank">00:27:31.360</a></span> | <span class="t">visualize short sequence length i think i set this to 32 so if it's bigger than 32 it will not be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1657" target="_blank">00:27:37.040</a></span> | <span class="t">visualized because it's not easy to visualize it also and basically to calculate the the the distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1665" target="_blank">00:27:45.680</a></span> | <span class="t">between one token and the other token i just basically do a bfs breadth-first search it's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1671" target="_blank">00:27:51.760</a></span> | <span class="t">very ugly one unoptimized one doesn't matter because i built this notebook in half hour</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1676" target="_blank">00:27:56.800</a></span> | <span class="t">just for showing how the model works so you can you are invited to make it better if you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1683" target="_blank">00:28:03.520</a></span> | <span class="t">last thing that i didn't show and that is very interesting is that we can see that the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1689" target="_blank">00:28:09.920</a></span> | <span class="t">node distance from the node from zero to any other node is three and it's changing with the logarithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1696" target="_blank">00:28:16.640</a></span> | <span class="t">of n as you can see so if we are for example if our n is equal to let's say 16 let's do it again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1704" target="_blank">00:28:24.000</a></span> | <span class="t">if the the sequence length is 16 we can see here that this is the path to go from the node number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1712" target="_blank">00:28:32.080</a></span> | <span class="t">zero to any or any other node and the maximum distance to go from node number zero to any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1716" target="_blank">00:28:36.960</a></span> | <span class="t">other node is two and it's just like the logarithm of n which is 16 you can also change which node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1723" target="_blank">00:28:43.680</a></span> | <span class="t">you want to go from so if you want to calculate other paths for example here we say that i want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1728" target="_blank">00:28:48.480</a></span> | <span class="t">to go from node number five to every other node and here we prove that the node all the nodes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1735" target="_blank">00:28:55.120</a></span> | <span class="t">are reachable from the node number five and here we display the paths okay and this is the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1741" target="_blank">00:29:01.360</a></span> | <span class="t">distance from node number five to any other node and this is the graph i hope you like my video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1748" target="_blank">00:29:08.160</a></span> | <span class="t">guys and i hope it was more or less clear how this this mechanism works i didn't explain all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1755" target="_blank">00:29:15.120</a></span> | <span class="t">the equations i here i have a lot of i have a lot of comments reason on the sides because i like to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1761" target="_blank">00:29:21.840</a></span> | <span class="t">write take notes when i read the paper mostly because i want to understand also the maths</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1766" target="_blank">00:29:26.960</a></span> | <span class="t">behind it so if you're interested in some parts just write in the comments and i will try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1770" target="_blank">00:29:30.960</a></span> | <span class="t">explain it better but i think most people just want to understand the mechanism and they are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1774" target="_blank">00:29:34.960</a></span> | <span class="t">waiting for the official code to be released to actually watch how it works and i hope i didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1780" target="_blank">00:29:40.080</a></span> | <span class="t">make any mistakes because basically there is no information online about the long net so everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1785" target="_blank">00:29:45.040</a></span> | <span class="t">i told you is all because of my research and i hope you enjoyed the video so please come back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ&t=1792" target="_blank">00:29:52.320</a></span> | <span class="t">to my channel for more videos about deep learning and machine learning and have a great day</span></div></div></body></html>