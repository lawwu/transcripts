<html><head><title>Stanford XCS224U I Analysis NLU, Pt 4: Casual Abstraction & Interchange Intervention Training (IIT)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U I Analysis NLU, Pt 4: Casual Abstraction & Interchange Intervention Training (IIT)</h2><a href="https://www.youtube.com/watch?v=6pwpOOj33aw"><img src="https://i.ytimg.com/vi/6pwpOOj33aw/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=27">0:27</a> Recipe for causal abstraction<br><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=360">6:0</a> Interchange intervention accuracy (IIA)<br><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=483">8:3</a> Findings from causal abstraction<br><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=634">10:34</a> Connections to the literature<br><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=662">11:2</a> Summary<br><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=696">11:36</a> Method<br><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=825">13:45</a> Findings from IIT<br><br><div style="text-align: left;"><a href="./6pwpOOj33aw.html">Whisper Transcript</a> | <a href="./transcript_6pwpOOj33aw.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome back everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=6" target="_blank">00:00:06.120</a></span> | <span class="t">This is part 4 in our series on analysis methods for NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=9" target="_blank">00:00:09.680</a></span> | <span class="t">We've come to our third set of methods, causal abstraction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=13" target="_blank">00:00:13.920</a></span> | <span class="t">I've been heavily involved with developing these methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=16" target="_blank">00:00:16.960</a></span> | <span class="t">I think they're tremendously exciting because they offer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=19" target="_blank">00:00:19.360</a></span> | <span class="t">a real opportunity for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=21" target="_blank">00:00:21.160</a></span> | <span class="t">causal concept level explanations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=24" target="_blank">00:00:24.080</a></span> | <span class="t">of how our NLP models are behaving.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=26" target="_blank">00:00:26.960</a></span> | <span class="t">Let's begin with a recipe for this causal abstraction analysis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=31" target="_blank">00:00:31.880</a></span> | <span class="t">Step 1, you state a hypothesis about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=34" target="_blank">00:00:34.720</a></span> | <span class="t">some aspect of your target model's causal structure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=38" target="_blank">00:00:38.320</a></span> | <span class="t">and you could express this as a small computer program.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=42" target="_blank">00:00:42.640</a></span> | <span class="t">In step 2, we're going to search for an alignment between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=46" target="_blank">00:00:46.520</a></span> | <span class="t">variables in this causal model we've defined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=49" target="_blank">00:00:49.480</a></span> | <span class="t">and sets of neurons in the target model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=52" target="_blank">00:00:52.740</a></span> | <span class="t">This is a hypothesis about how the roles for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=55" target="_blank">00:00:55.600</a></span> | <span class="t">those variables and sets of neurons align with each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=59" target="_blank">00:00:59.880</a></span> | <span class="t">To do this analysis,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=62" target="_blank">00:01:02.360</a></span> | <span class="t">to assess these alignments,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=63" target="_blank">00:01:03.760</a></span> | <span class="t">we perform the fundamental operation of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=66" target="_blank">00:01:06.000</a></span> | <span class="t">causal abstraction analysis, the interchange intervention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=70" target="_blank">00:01:10.200</a></span> | <span class="t">Much of this screencast is going to be devoted to giving you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=73" target="_blank">00:01:13.180</a></span> | <span class="t">for a feel for how interchange interventions work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=77" target="_blank">00:01:17.120</a></span> | <span class="t">For a running example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=79" target="_blank">00:01:19.100</a></span> | <span class="t">let's return to our simple neural network that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=81" target="_blank">00:01:21.240</a></span> | <span class="t">takes in three numbers and adds them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=84" target="_blank">00:01:24.360</a></span> | <span class="t">We assume that this network is successful at its task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=87" target="_blank">00:01:27.720</a></span> | <span class="t">and the question is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=88" target="_blank">00:01:28.880</a></span> | <span class="t">in human interpretable terms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=90" target="_blank">00:01:30.960</a></span> | <span class="t">how does the network perform this function?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=94" target="_blank">00:01:34.320</a></span> | <span class="t">As before, we can hypothesize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=96" target="_blank">00:01:36.960</a></span> | <span class="t">a causal model that's given in green here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=99" target="_blank">00:01:39.720</a></span> | <span class="t">The idea behind this causal model is that the network is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=103" target="_blank">00:01:43.260</a></span> | <span class="t">adding together the first two inputs to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=105" target="_blank">00:01:45.400</a></span> | <span class="t">form an intermediate variable S1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=108" target="_blank">00:01:48.180</a></span> | <span class="t">and then the third input is copied over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=110" target="_blank">00:01:50.740</a></span> | <span class="t">into an intermediate variable W,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=112" target="_blank">00:01:52.980</a></span> | <span class="t">and S1 and W are the elements that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=115" target="_blank">00:01:55.880</a></span> | <span class="t">directly contribute to the output of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=118" target="_blank">00:01:58.920</a></span> | <span class="t">That's a hypothesis about what might be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=121" target="_blank">00:02:01.200</a></span> | <span class="t">happening with our otherwise opaque neural model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=124" target="_blank">00:02:04.240</a></span> | <span class="t">and the question is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=125" target="_blank">00:02:05.420</a></span> | <span class="t">is the hypothesis correct?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=127" target="_blank">00:02:07.600</a></span> | <span class="t">We're going to use interchange interventions to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=130" target="_blank">00:02:10.120</a></span> | <span class="t">help us assess that hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=132" target="_blank">00:02:12.580</a></span> | <span class="t">We'll break this down into a few pieces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=134" target="_blank">00:02:14.600</a></span> | <span class="t">First, we hypothesize that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=136" target="_blank">00:02:16.760</a></span> | <span class="t">the neural representation L3 plays the same role as S1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=142" target="_blank">00:02:22.360</a></span> | <span class="t">Let's assess that idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=143" target="_blank">00:02:23.960</a></span> | <span class="t">The first intervention happens on the causal model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=147" target="_blank">00:02:27.280</a></span> | <span class="t">We take our causal model and we process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=149" target="_blank">00:02:29.160</a></span> | <span class="t">example 1, 3, 5, and we get 9.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=151" target="_blank">00:02:31.800</a></span> | <span class="t">We use that same causal model to process 4,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=154" target="_blank">00:02:34.880</a></span> | <span class="t">5, 6, and we get 15.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=157" target="_blank">00:02:37.640</a></span> | <span class="t">Now the intervention comes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=159" target="_blank">00:02:39.380</a></span> | <span class="t">We're going to target the S1 variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=161" target="_blank">00:02:41.560</a></span> | <span class="t">for the right-hand example that has value 9,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=164" target="_blank">00:02:44.180</a></span> | <span class="t">literally take that value and place it in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=166" target="_blank">00:02:46.980</a></span> | <span class="t">the corresponding place in the left-hand example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=170" target="_blank">00:02:50.120</a></span> | <span class="t">The causal model is completely understood by us,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=173" target="_blank">00:02:53.000</a></span> | <span class="t">and so we know exactly what will happen now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=175" target="_blank">00:02:55.400</a></span> | <span class="t">The output will change to 14.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=177" target="_blank">00:02:57.680</a></span> | <span class="t">The child nodes below</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=179" target="_blank">00:02:59.520</a></span> | <span class="t">the variable that we intervened on don't matter in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=182" target="_blank">00:03:02.400</a></span> | <span class="t">The intervention fully wipes them out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=184" target="_blank">00:03:04.480</a></span> | <span class="t">and we're just adding 9 and 5 together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=187" target="_blank">00:03:07.640</a></span> | <span class="t">That's the causal model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=189" target="_blank">00:03:09.320</a></span> | <span class="t">We assume that we understand it before we begin the analysis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=193" target="_blank">00:03:13.160</a></span> | <span class="t">The interesting part comes when we think about the neural model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=196" target="_blank">00:03:16.560</a></span> | <span class="t">We don't know how this neural model works,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=198" target="_blank">00:03:18.760</a></span> | <span class="t">and we're going to try to use these interventions to uncover that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=202" target="_blank">00:03:22.200</a></span> | <span class="t">We process 1, 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=204" target="_blank">00:03:24.120</a></span> | <span class="t">5 with our neural model and we get 9.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=206" target="_blank">00:03:26.800</a></span> | <span class="t">We process 4, 5, 6,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=208" target="_blank">00:03:28.800</a></span> | <span class="t">and we get 15.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=210" target="_blank">00:03:30.320</a></span> | <span class="t">Now we're going to intervene on the L3 state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=213" target="_blank">00:03:33.480</a></span> | <span class="t">We target that in the right-hand example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=215" target="_blank">00:03:35.960</a></span> | <span class="t">and we literally take those values and place</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=218" target="_blank">00:03:38.600</a></span> | <span class="t">them in the corresponding spot in the left-hand example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=222" target="_blank">00:03:42.440</a></span> | <span class="t">We study the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=224" target="_blank">00:03:44.400</a></span> | <span class="t">If the output after that intervention is 14,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=228" target="_blank">00:03:48.380</a></span> | <span class="t">then we have one piece of evidence that L3 plays the same causal role as S1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=234" target="_blank">00:03:54.920</a></span> | <span class="t">If we repeat this intervention for every conceivable input to these models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=240" target="_blank">00:04:00.320</a></span> | <span class="t">and we always see this alignment between causal model and neural model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=244" target="_blank">00:04:04.840</a></span> | <span class="t">we have proven that L3 plays the same causal role as S1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=250" target="_blank">00:04:10.160</a></span> | <span class="t">We can continue this for other variables.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=252" target="_blank">00:04:12.280</a></span> | <span class="t">Let's target now L1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=253" target="_blank">00:04:13.680</a></span> | <span class="t">Suppose we hypothesize that it plays the same role as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=257" target="_blank">00:04:17.160</a></span> | <span class="t">W in the causal model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=259" target="_blank">00:04:19.200</a></span> | <span class="t">Again, let's first intervene on the causal model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=261" target="_blank">00:04:21.680</a></span> | <span class="t">We target that W variable on the right-hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=264" target="_blank">00:04:24.440</a></span> | <span class="t">We take that value and we place it in the corresponding place in the left-hand model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=269" target="_blank">00:04:29.480</a></span> | <span class="t">We study the output that has changed the output to 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=273" target="_blank">00:04:33.160</a></span> | <span class="t">Then we return to our neural models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=275" target="_blank">00:04:35.240</a></span> | <span class="t">Parallel operation, target L1 on the right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=278" target="_blank">00:04:38.480</a></span> | <span class="t">take that value and literally place it into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=281" target="_blank">00:04:41.200</a></span> | <span class="t">the corresponding spot in the left and we study the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=284" target="_blank">00:04:44.920</a></span> | <span class="t">Again, if the output is 10,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=286" target="_blank">00:04:46.560</a></span> | <span class="t">we have a single piece of evidence that L1 and W are causally aligned in this way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=292" target="_blank">00:04:52.400</a></span> | <span class="t">If we repeat this intervention for every possible input and always see this correspondence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=297" target="_blank">00:04:57.500</a></span> | <span class="t">we have proven that L1 and W play the same causal roles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=302" target="_blank">00:05:02.480</a></span> | <span class="t">We could go one step further.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=304" target="_blank">00:05:04.180</a></span> | <span class="t">Suppose we think about L2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=305" target="_blank">00:05:05.940</a></span> | <span class="t">Suppose we intervene on L2 in every way we can think of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=310" target="_blank">00:05:10.060</a></span> | <span class="t">and we never see an impact on the output behavior of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=314" target="_blank">00:05:14.220</a></span> | <span class="t">In that way, we have proven that L2 plays</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=317" target="_blank">00:05:17.160</a></span> | <span class="t">no causal role in the input-output behavior of this network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=321" target="_blank">00:05:21.000</a></span> | <span class="t">Since we can assume that the input variables are aligned across causal and neural models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=325" target="_blank">00:05:25.960</a></span> | <span class="t">and we can assume that the output variables are aligned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=328" target="_blank">00:05:28.700</a></span> | <span class="t">we have now fully proven via all these intervention experiments that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=333" target="_blank">00:05:33.240</a></span> | <span class="t">that causal model in green is an abstraction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=336" target="_blank">00:05:36.000</a></span> | <span class="t">of the otherwise more complex neural model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=339" target="_blank">00:05:39.440</a></span> | <span class="t">That is exciting. If we have actually established this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=342" target="_blank">00:05:42.400</a></span> | <span class="t">then we are licensed to allow the neural model to fall away,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=346" target="_blank">00:05:46.120</a></span> | <span class="t">and we can reason entirely in terms of the causal model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=349" target="_blank">00:05:49.540</a></span> | <span class="t">secure that the two models are causally aligned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=353" target="_blank">00:05:53.580</a></span> | <span class="t">They have the same underlying mechanisms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=357" target="_blank">00:05:57.080</a></span> | <span class="t">Now, that is a ideal of causal abstraction analysis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=362" target="_blank">00:06:02.060</a></span> | <span class="t">There are a few things from the real world that are going to intervene.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=365" target="_blank">00:06:05.540</a></span> | <span class="t">The first is that we can never perform the full set of interventions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=369" target="_blank">00:06:09.480</a></span> | <span class="t">For all realistic cases,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=370" target="_blank">00:06:10.920</a></span> | <span class="t">there are too many inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=372" target="_blank">00:06:12.440</a></span> | <span class="t">Even for the case of my tiny addition network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=375" target="_blank">00:06:15.120</a></span> | <span class="t">there is an infinitude of possible inputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=378" target="_blank">00:06:18.260</a></span> | <span class="t">we can't check them all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=379" target="_blank">00:06:19.440</a></span> | <span class="t">We have to pick a small subset of examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=383" target="_blank">00:06:23.320</a></span> | <span class="t">Then otherwise, for real models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=385" target="_blank">00:06:25.380</a></span> | <span class="t">we're never going to see perfect causal abstraction relationships because of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=389" target="_blank">00:06:29.360</a></span> | <span class="t">the messy nature of naturally trained models that we use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=394" target="_blank">00:06:34.280</a></span> | <span class="t">We need some graded notion of success,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=396" target="_blank">00:06:36.720</a></span> | <span class="t">and I think interchange intervention accuracy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=399" target="_blank">00:06:39.200</a></span> | <span class="t">is a good initial baseline metric for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=402" target="_blank">00:06:42.720</a></span> | <span class="t">The IIA is the percentage of interchange interventions that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=406" target="_blank">00:06:46.820</a></span> | <span class="t">performed that lead to outputs that match</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=409" target="_blank">00:06:49.680</a></span> | <span class="t">those of the causal model under the chosen alignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=412" target="_blank">00:06:52.560</a></span> | <span class="t">You can think of it as an accuracy measure for your hypothesized alignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=418" target="_blank">00:06:58.280</a></span> | <span class="t">IIA is scaled in 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=420" target="_blank">00:07:00.720</a></span> | <span class="t">1 as with a normal accuracy metric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=423" target="_blank">00:07:03.440</a></span> | <span class="t">It can actually be above task performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=426" target="_blank">00:07:06.720</a></span> | <span class="t">This is striking,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=428" target="_blank">00:07:08.160</a></span> | <span class="t">and it has happened to us in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=430" target="_blank">00:07:10.380</a></span> | <span class="t">If the interchange interventions put the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=432" target="_blank">00:07:12.880</a></span> | <span class="t">into a better state than it was in originally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=435" target="_blank">00:07:15.280</a></span> | <span class="t">then you might actually see a boost in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=437" target="_blank">00:07:17.560</a></span> | <span class="t">performance from these Frankenstein examples that you have created.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=441" target="_blank">00:07:21.720</a></span> | <span class="t">This is really fundamental here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=444" target="_blank">00:07:24.040</a></span> | <span class="t">IIA is extremely sensitive to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=446" target="_blank">00:07:26.360</a></span> | <span class="t">the set of interchange interventions that you decided to perform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=449" target="_blank">00:07:29.440</a></span> | <span class="t">If you can't perform all of them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=450" target="_blank">00:07:30.900</a></span> | <span class="t">you have to pick a subset,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=452" target="_blank">00:07:32.340</a></span> | <span class="t">and that will be a factor in shaping your accuracy results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=456" target="_blank">00:07:36.920</a></span> | <span class="t">In particular, pay particular attention to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=460" target="_blank">00:07:40.040</a></span> | <span class="t">how many interchange interventions should change the output label.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=463" target="_blank">00:07:43.880</a></span> | <span class="t">Those are the ones that are really providing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=465" target="_blank">00:07:45.920</a></span> | <span class="t">causal insights because you see exactly what should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=468" target="_blank">00:07:48.840</a></span> | <span class="t">happen in terms of changes once you have performed the intervention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=473" target="_blank">00:07:53.800</a></span> | <span class="t">Having an abundance of these causally insightful interventions is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=478" target="_blank">00:07:58.160</a></span> | <span class="t">the most powerful thing you can do in terms of building an argument.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=482" target="_blank">00:08:02.960</a></span> | <span class="t">Let me briefly summarize some findings from causal abstraction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=487" target="_blank">00:08:07.200</a></span> | <span class="t">These are mostly from our work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=489" target="_blank">00:08:09.500</a></span> | <span class="t">Fine-tuned BERT models succeed at hard out-of-domain examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=493" target="_blank">00:08:13.880</a></span> | <span class="t">involving lexical entailment and negation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=496" target="_blank">00:08:16.000</a></span> | <span class="t">because they are abstracted by simple monotonicity programs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=499" target="_blank">00:08:19.720</a></span> | <span class="t">I emphasize because,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=501" target="_blank">00:08:21.240</a></span> | <span class="t">and I wrote it in blue there because I am not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=503" target="_blank">00:08:23.440</a></span> | <span class="t">being casual with that causal language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=505" target="_blank">00:08:25.840</a></span> | <span class="t">I really intend a causal claim.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=508" target="_blank">00:08:28.280</a></span> | <span class="t">That is the kind of thing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=510" target="_blank">00:08:30.160</a></span> | <span class="t">causal abstraction licenses you to be able to say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=514" target="_blank">00:08:34.080</a></span> | <span class="t">Relatedly, fine-tuned BERT models succeed at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=517" target="_blank">00:08:37.760</a></span> | <span class="t">the MQNLI task because they find compositional solutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=522" target="_blank">00:08:42.240</a></span> | <span class="t">MQNLI is the multiply quantified NLI benchmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=525" target="_blank">00:08:45.880</a></span> | <span class="t">It's a synthetic benchmark full of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=527" target="_blank">00:08:47.800</a></span> | <span class="t">very intricate compositional analyses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=530" target="_blank">00:08:50.640</a></span> | <span class="t">between quantifiers and modifiers and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=533" target="_blank">00:08:53.840</a></span> | <span class="t">A challenging benchmark,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=535" target="_blank">00:08:55.360</a></span> | <span class="t">and we show with causal abstraction that models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=537" target="_blank">00:08:57.920</a></span> | <span class="t">succeed to the extent that they actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=540" target="_blank">00:09:00.280</a></span> | <span class="t">find compositional solutions to the task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=543" target="_blank">00:09:03.760</a></span> | <span class="t">Models succeed at the MNIST pointer value retrieval task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=547" target="_blank">00:09:07.880</a></span> | <span class="t">because they are abstracted by simple programs like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=551" target="_blank">00:09:11.060</a></span> | <span class="t">if the digit is six,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=552" target="_blank">00:09:12.300</a></span> | <span class="t">then the label is in the lower left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=554" target="_blank">00:09:14.720</a></span> | <span class="t">A brief digression there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=556" target="_blank">00:09:16.700</a></span> | <span class="t">I love these explanations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=559" target="_blank">00:09:19.040</a></span> | <span class="t">That simple program that I described is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=561" target="_blank">00:09:21.400</a></span> | <span class="t">more or less a description of the task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=563" target="_blank">00:09:23.720</a></span> | <span class="t">It's wonderfully reassuring to see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=566" target="_blank">00:09:26.040</a></span> | <span class="t">our explanations actually align with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=568" target="_blank">00:09:28.640</a></span> | <span class="t">the task structure for these very successful models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=571" target="_blank">00:09:31.920</a></span> | <span class="t">Another nice point here is that we're starting to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=574" target="_blank">00:09:34.840</a></span> | <span class="t">a blurring of the distinction between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=577" target="_blank">00:09:37.360</a></span> | <span class="t">neural models and symbolic models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=579" target="_blank">00:09:39.800</a></span> | <span class="t">After all, if you can show that the two are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=582" target="_blank">00:09:42.120</a></span> | <span class="t">aligned via causal abstraction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=584" target="_blank">00:09:44.400</a></span> | <span class="t">then there really is no meaningful difference between the two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=588" target="_blank">00:09:48.040</a></span> | <span class="t">which leads you to wonder whether there's truly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=590" target="_blank">00:09:50.480</a></span> | <span class="t">a meaningful difference between symbolic AI and neural AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=594" target="_blank">00:09:54.860</a></span> | <span class="t">They can certainly come together and you see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=597" target="_blank">00:09:57.400</a></span> | <span class="t">them coming together in these analyses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=600" target="_blank">00:10:00.620</a></span> | <span class="t">Finally, Bart and T5 use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=603" target="_blank">00:10:03.240</a></span> | <span class="t">coherent entity and situation representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=605" target="_blank">00:10:05.800</a></span> | <span class="t">that evolve as the discourse unfolds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=608" target="_blank">00:10:08.400</a></span> | <span class="t">Liatal 2021 use causal abstraction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=611" target="_blank">00:10:11.300</a></span> | <span class="t">in order to substantiate that claim.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=614" target="_blank">00:10:14.220</a></span> | <span class="t">Very exciting to see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=616" target="_blank">00:10:16.440</a></span> | <span class="t">If you would like to get hands-on with these ideas,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=619" target="_blank">00:10:19.600</a></span> | <span class="t">I would encourage you to check out our notebook.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=621" target="_blank">00:10:21.760</a></span> | <span class="t">It's called IIT Equality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=623" target="_blank">00:10:23.560</a></span> | <span class="t">It walks through causal abstraction analysis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=625" target="_blank">00:10:25.840</a></span> | <span class="t">using simple toy examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=628" target="_blank">00:10:28.240</a></span> | <span class="t">and then also shows you how to apply IIT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=631" target="_blank">00:10:31.000</a></span> | <span class="t">which is the next topic we'll discuss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=634" target="_blank">00:10:34.200</a></span> | <span class="t">There isn't time to cover this in detail,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=636" target="_blank">00:10:36.760</a></span> | <span class="t">but I did want to call out that causal abstraction is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=639" target="_blank">00:10:39.980</a></span> | <span class="t">a toolkit corresponding to a large family</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=643" target="_blank">00:10:43.360</a></span> | <span class="t">of intervention-based methods for understanding our models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=647" target="_blank">00:10:47.140</a></span> | <span class="t">I've listed a few other exciting entries in this literature here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=651" target="_blank">00:10:51.200</a></span> | <span class="t">If you would like even more connections to the literature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=654" target="_blank">00:10:54.200</a></span> | <span class="t">I recommend this blog post that we did,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=656" target="_blank">00:10:56.440</a></span> | <span class="t">which relates a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=657" target="_blank">00:10:57.840</a></span> | <span class="t">these methods to causal abstraction itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=661" target="_blank">00:11:01.600</a></span> | <span class="t">Let's return to our summary scorecard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=664" target="_blank">00:11:04.940</a></span> | <span class="t">We're talking about intervention-based methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=667" target="_blank">00:11:07.600</a></span> | <span class="t">I claim that they can characterize representations richly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=670" target="_blank">00:11:10.920</a></span> | <span class="t">After all, we show how those representations correspond</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=673" target="_blank">00:11:13.840</a></span> | <span class="t">to interpretable high-level variables.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=676" target="_blank">00:11:16.920</a></span> | <span class="t">I've also tried to argue that this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=678" target="_blank">00:11:18.920</a></span> | <span class="t">a causal inference method,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=681" target="_blank">00:11:21.080</a></span> | <span class="t">and I still have a smiley under improved models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=683" target="_blank">00:11:23.960</a></span> | <span class="t">I have not substantiated that for you next,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=686" target="_blank">00:11:26.720</a></span> | <span class="t">but that is the next task under</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=688" target="_blank">00:11:28.760</a></span> | <span class="t">the heading of interchange intervention training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=691" target="_blank">00:11:31.860</a></span> | <span class="t">Let's turn to that now, IIT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=695" target="_blank">00:11:35.360</a></span> | <span class="t">The method is quite simple and builds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=698" target="_blank">00:11:38.360</a></span> | <span class="t">directly on causal abstraction with interchange interventions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=702" target="_blank">00:11:42.120</a></span> | <span class="t">Here's a summary diagram of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=704" target="_blank">00:11:44.840</a></span> | <span class="t">interchange intervention using our addition example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=708" target="_blank">00:11:48.120</a></span> | <span class="t">with the one twist that you'll notice that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=710" target="_blank">00:11:50.800</a></span> | <span class="t">my intervention now for L3 has led to an incorrect result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=715" target="_blank">00:11:55.140</a></span> | <span class="t">We wanted 14 and we got four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=718" target="_blank">00:11:58.720</a></span> | <span class="t">We have in some sense shown that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=721" target="_blank">00:12:01.360</a></span> | <span class="t">our hypothesized alignment between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=723" target="_blank">00:12:03.520</a></span> | <span class="t">these variables is not correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=725" target="_blank">00:12:05.640</a></span> | <span class="t">But I think you can also see in here an opportunity to do better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=729" target="_blank">00:12:09.680</a></span> | <span class="t">We can correct this misalignment if we want to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=732" target="_blank">00:12:12.680</a></span> | <span class="t">After all, we know what the label</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=735" target="_blank">00:12:15.080</a></span> | <span class="t">should have been and we know what it was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=738" target="_blank">00:12:18.160</a></span> | <span class="t">That gives us a gradient signal that we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=741" target="_blank">00:12:21.280</a></span> | <span class="t">use to update the parameters of this model and make it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=744" target="_blank">00:12:24.920</a></span> | <span class="t">more conform to our underlying causal model under this alignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=749" target="_blank">00:12:29.860</a></span> | <span class="t">Let's see how that would play out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=751" target="_blank">00:12:31.360</a></span> | <span class="t">We get our error signal and that flows back as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=753" target="_blank">00:12:33.840</a></span> | <span class="t">usual to the hidden states L1, L2, and L3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=758" target="_blank">00:12:38.080</a></span> | <span class="t">For L1, the gradients flow back as usual to the input states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=763" target="_blank">00:12:43.080</a></span> | <span class="t">The same thing is true for L2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=765" target="_blank">00:12:45.480</a></span> | <span class="t">But for L3, we have a more complicated update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=768" target="_blank">00:12:48.780</a></span> | <span class="t">We have literally copied over the full computation graph in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=772" target="_blank">00:12:52.800</a></span> | <span class="t">the PyTorch sense including all the gradient information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=776" target="_blank">00:12:56.720</a></span> | <span class="t">What we get for L3 is a double update coming from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=780" target="_blank">00:13:00.560</a></span> | <span class="t">our current example as well as the source example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=783" target="_blank">00:13:03.800</a></span> | <span class="t">which also processed that representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=786" target="_blank">00:13:06.520</a></span> | <span class="t">We get a double update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=788" target="_blank">00:13:08.260</a></span> | <span class="t">The result of repeatedly performing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=791" target="_blank">00:13:11.360</a></span> | <span class="t">these IIT updates on these models using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=794" target="_blank">00:13:14.160</a></span> | <span class="t">the causal model for the labels as we've done here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=797" target="_blank">00:13:17.340</a></span> | <span class="t">is that we push the model to modularize information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=801" target="_blank">00:13:21.000</a></span> | <span class="t">about S1 in this case in the L3 variable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=805" target="_blank">00:13:25.000</a></span> | <span class="t">The importance of alignments falls away and the emphasis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=809" target="_blank">00:13:29.200</a></span> | <span class="t">here is on actually pushing models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=811" target="_blank">00:13:31.120</a></span> | <span class="t">improving them by making them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=813" target="_blank">00:13:33.360</a></span> | <span class="t">have the causal structure that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=815" target="_blank">00:13:35.440</a></span> | <span class="t">hypothesized in the hopes that they will then perform in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=818" target="_blank">00:13:38.600</a></span> | <span class="t">more systematic ways and be better at the tasks we've set for them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=823" target="_blank">00:13:43.640</a></span> | <span class="t">Findings from IIT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=826" target="_blank">00:13:46.960</a></span> | <span class="t">We showed that IIT achieve state-of-the-art results on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=831" target="_blank">00:13:51.440</a></span> | <span class="t">that MNIST pointer value retrieval task that I mentioned before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=834" target="_blank">00:13:54.560</a></span> | <span class="t">as well as ReScan,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=835" target="_blank">00:13:55.760</a></span> | <span class="t">which is a grounded language understanding benchmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=838" target="_blank">00:13:58.660</a></span> | <span class="t">We also showed that IIT can be used as a distillation objective,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=844" target="_blank">00:14:04.000</a></span> | <span class="t">where essentially what we do is distill</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=846" target="_blank">00:14:06.640</a></span> | <span class="t">teacher models into student models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=848" target="_blank">00:14:08.720</a></span> | <span class="t">forcing them not only to conform in their input-output behavior,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=852" target="_blank">00:14:12.280</a></span> | <span class="t">but also conform at the level of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=854" target="_blank">00:14:14.680</a></span> | <span class="t">their internal representations under</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=857" target="_blank">00:14:17.060</a></span> | <span class="t">the counterfactuals that we create for IIT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=859" target="_blank">00:14:19.900</a></span> | <span class="t">This is exciting to me because I think it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=861" target="_blank">00:14:21.840</a></span> | <span class="t">a powerful distillation method and it also shows you that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=865" target="_blank">00:14:25.560</a></span> | <span class="t">the causal model that we use for IIT can be quite abstract.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=869" target="_blank">00:14:29.480</a></span> | <span class="t">In this case, it's just a high-level constraint on what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=872" target="_blank">00:14:32.780</a></span> | <span class="t">want the teacher and student models to look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=876" target="_blank">00:14:36.600</a></span> | <span class="t">We also showed that IIT can be used to induce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=879" target="_blank">00:14:39.760</a></span> | <span class="t">internal representations of characters in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=882" target="_blank">00:14:42.280</a></span> | <span class="t">language models that are based in subword tokenization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=885" target="_blank">00:14:45.760</a></span> | <span class="t">We showed that this helps with a variety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=887" target="_blank">00:14:47.760</a></span> | <span class="t">of character level games and tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=890" target="_blank">00:14:50.060</a></span> | <span class="t">This is IIT being used to strike a balance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=892" target="_blank">00:14:52.860</a></span> | <span class="t">Subword models seem to be our best language models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=896" target="_blank">00:14:56.240</a></span> | <span class="t">but we have tasks that require knowledge of characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=900" target="_blank">00:15:00.000</a></span> | <span class="t">What we do with IIT is imbue these models with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=902" target="_blank">00:15:02.640</a></span> | <span class="t">knowledge of characters in their internal states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=906" target="_blank">00:15:06.560</a></span> | <span class="t">Finally, we recently used IIT to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=909" target="_blank">00:15:09.760</a></span> | <span class="t">concept level methods for explaining model behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=913" target="_blank">00:15:13.280</a></span> | <span class="t">That's a technique that we call causal proxy models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=916" target="_blank">00:15:16.160</a></span> | <span class="t">and it essentially leverages the core insight of IIT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=920" target="_blank">00:15:20.960</a></span> | <span class="t">Again, we have this course notebook, IIT equality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=924" target="_blank">00:15:24.840</a></span> | <span class="t">It covers abstraction analyses and then also shows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=927" target="_blank">00:15:27.960</a></span> | <span class="t">you how to train models in this IIT mode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=931" target="_blank">00:15:31.920</a></span> | <span class="t">We can return to our scorecard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=934" target="_blank">00:15:34.320</a></span> | <span class="t">Now I have smileys across the board,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=936" target="_blank">00:15:36.640</a></span> | <span class="t">and I claim that I have justified all of those smileys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=939" target="_blank">00:15:39.840</a></span> | <span class="t">I feel that this does point to intervention-based methods as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=943" target="_blank">00:15:43.280</a></span> | <span class="t">the best bet we have for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=945" target="_blank">00:15:45.200</a></span> | <span class="t">deeply understanding how NLP models work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=6pwpOOj33aw&t=949" target="_blank">00:15:49.840</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>