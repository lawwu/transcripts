<html><head><title>Stanford CS25: V1 I Transformers United: DL Models that have revolutionized NLP, CV, RL</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V1 I Transformers United: DL Models that have revolutionized NLP, CV, RL</h2><a href="https://www.youtube.com/watch?v=P127jhj-8-Y"><img src="https://i.ytimg.com/vi/P127jhj-8-Y/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=163">2:43</a> Overview of Transformers<br><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=363">6:3</a> Attention mechanisms<br><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=473">7:53</a> Self retention<br><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=698">11:38</a> Other necessary ingredients<br><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=812">13:32</a> Encoder Decoder Architecture<br><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=962">16:2</a> Advantages & Disadvantages<br><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1084">18:4</a> Applications of Transformers<br><br><div style="text-align: left;"><a href="./P127jhj-8-Y.html">Whisper Transcript</a> | <a href="./transcript_P127jhj-8-Y.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">[BLANK_AUDIO]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=5" target="_blank">00:00:05.880</a></span> | <span class="t">>> Hey everyone, welcome to the first and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=8" target="_blank">00:00:08.640</a></span> | <span class="t">introductory lecture for CS25, Transformers United.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=13" target="_blank">00:00:13.000</a></span> | <span class="t">So CS25 was a class that the three of us created and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=16" target="_blank">00:00:16.480</a></span> | <span class="t">taught at Stanford in the fall of 2021.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=19" target="_blank">00:00:19.760</a></span> | <span class="t">And the subject of the class is not as the picture might suggest,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=23" target="_blank">00:00:23.800</a></span> | <span class="t">it's not about robots that can transform into cars.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=27" target="_blank">00:00:27.480</a></span> | <span class="t">It's about deep learning models and specifically a particular kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=31" target="_blank">00:00:31.620</a></span> | <span class="t">deep learning models that have revolutionized multiple fields.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=35" target="_blank">00:00:35.340</a></span> | <span class="t">Starting from natural language processing to things like computer vision and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=39" target="_blank">00:00:39.840</a></span> | <span class="t">reinforcement learning, to name a few.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=41" target="_blank">00:00:41.480</a></span> | <span class="t">We have an exciting set of videos lined up for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=45" target="_blank">00:00:45.880</a></span> | <span class="t">We have some truly fantastic speakers come and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=48" target="_blank">00:00:48.400</a></span> | <span class="t">give talks about how they were applying transformers in their own research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=53" target="_blank">00:00:53.280</a></span> | <span class="t">And we hope you will enjoy and learn from these talks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=56" target="_blank">00:00:56.440</a></span> | <span class="t">So this video is purely an introductory lecture to talk a little bit about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=60" target="_blank">00:01:00.560</a></span> | <span class="t">transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=61" target="_blank">00:01:01.800</a></span> | <span class="t">And before we get started, I'd like to introduce the instructors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=65" target="_blank">00:01:05.560</a></span> | <span class="t">So my name is Abwehr.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=67" target="_blank">00:01:07.000</a></span> | <span class="t">I am a software engineer at a company called Applied Intuition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=70" target="_blank">00:01:10.320</a></span> | <span class="t">Before this, I was a master's student in CS at Stanford.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=74" target="_blank">00:01:14.320</a></span> | <span class="t">And I am one of the co-instructors for CS25.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=79" target="_blank">00:01:19.680</a></span> | <span class="t">Chaitanya, Dev, if the two of you could introduce yourselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=82" target="_blank">00:01:22.480</a></span> | <span class="t">>> So hi, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=83" target="_blank">00:01:23.880</a></span> | <span class="t">I am a PhD student at Stanford.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=86" target="_blank">00:01:26.280</a></span> | <span class="t">Before this, I was pursuing a master's here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=89" target="_blank">00:01:29.520</a></span> | <span class="t">I'm researching a lot in generative modeling, reinforcement learning, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=92" target="_blank">00:01:32.760</a></span> | <span class="t">robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=93" target="_blank">00:01:33.880</a></span> | <span class="t">So nice to meet you all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=95" target="_blank">00:01:35.280</a></span> | <span class="t">>> Yeah, that was Dev, since he didn't say his name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=98" target="_blank">00:01:38.400</a></span> | <span class="t">Chaitanya, if you want to introduce yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=100" target="_blank">00:01:40.560</a></span> | <span class="t">>> Yeah, hi, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=101" target="_blank">00:01:41.920</a></span> | <span class="t">My name is Chaitanya, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=103" target="_blank">00:01:43.520</a></span> | <span class="t">I'm currently working as an ML engineer at a startup called Moveworks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=108" target="_blank">00:01:48.360</a></span> | <span class="t">Before that, I was a master's student at Stanford specializing in NLP and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=112" target="_blank">00:01:52.360</a></span> | <span class="t">was a member of the prize-winning Stanford's team for the Alexa Prize Challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=116" target="_blank">00:01:56.280</a></span> | <span class="t">>> All right, awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=119" target="_blank">00:01:59.680</a></span> | <span class="t">So moving on to the rest of this talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=124" target="_blank">00:02:04.520</a></span> | <span class="t">Essentially, what we hope you will learn watching these videos, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=128" target="_blank">00:02:08.840</a></span> | <span class="t">what we hope the people who took our class in the fall of 2021 learned, is three things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=135" target="_blank">00:02:15.320</a></span> | <span class="t">One is we hope you will have an understanding of how transformers work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=139" target="_blank">00:02:19.720</a></span> | <span class="t">Secondly, we hope you will learn, and by the end of these talks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=143" target="_blank">00:02:23.560</a></span> | <span class="t">understand how transformers are being applied beyond just natural language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=148" target="_blank">00:02:28.080</a></span> | <span class="t">processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=149" target="_blank">00:02:29.460</a></span> | <span class="t">And thirdly, we hope that some of these talks will spark some new ideas within you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=154" target="_blank">00:02:34.360</a></span> | <span class="t">and hopefully lead to new directions of research,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=157" target="_blank">00:02:37.320</a></span> | <span class="t">new kinds of innovation, and things of that sort.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=160" target="_blank">00:02:40.280</a></span> | <span class="t">And to begin, we're going to talk a little bit about transformers and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=168" target="_blank">00:02:48.280</a></span> | <span class="t">introduce some of the context behind transformers as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=172" target="_blank">00:02:52.280</a></span> | <span class="t">And for that, I'd like to hand it off to Dev.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=173" target="_blank">00:02:53.960</a></span> | <span class="t">>> So hi, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=182" target="_blank">00:03:02.040</a></span> | <span class="t">So welcome to our transformer seminar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=184" target="_blank">00:03:04.960</a></span> | <span class="t">So I will start first with an overview of the attention timeline and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=188" target="_blank">00:03:08.860</a></span> | <span class="t">how it came to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=189" target="_blank">00:03:09.960</a></span> | <span class="t">The key idea about transformers was the self-attention mechanism that was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=194" target="_blank">00:03:14.260</a></span> | <span class="t">developed in 2017, and it all started with this one paper called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=197" target="_blank">00:03:17.220</a></span> | <span class="t">Attention is All You Need by Vava Swanyatal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=199" target="_blank">00:03:19.900</a></span> | <span class="t">Before 2017, we used to have this prehistoric era where we had older models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=204" target="_blank">00:03:24.420</a></span> | <span class="t">like RNNs, LSTMs, and simpler attention mechanisms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=208" target="_blank">00:03:28.620</a></span> | <span class="t">And eventually, the growth in transformers has exploded into other fields and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=212" target="_blank">00:03:32.580</a></span> | <span class="t">has become prominent in all of machine learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=215" target="_blank">00:03:35.540</a></span> | <span class="t">And I'll go and see and show how this has been used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=219" target="_blank">00:03:39.900</a></span> | <span class="t">So in the prehistoric era, there used to be RNNs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=223" target="_blank">00:03:43.660</a></span> | <span class="t">There were different models, like the sequence-to-sequence, LSTMs, GRUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=227" target="_blank">00:03:47.700</a></span> | <span class="t">They were good at encoding some sort of memory, but they did not work for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=231" target="_blank">00:03:51.840</a></span> | <span class="t">encoding long sequences, and they were very bad at encoding context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=235" target="_blank">00:03:55.220</a></span> | <span class="t">So here is an example where if you have a sentence like, I grew up in France,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=239" target="_blank">00:03:59.180</a></span> | <span class="t">dot, dot, dot, so I speak fluent dash.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=242" target="_blank">00:04:02.380</a></span> | <span class="t">Then you want to fill this with French based on the context, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=245" target="_blank">00:04:05.340</a></span> | <span class="t">a LSTM model might not know what it is and might just make a very big mistake here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=249" target="_blank">00:04:09.780</a></span> | <span class="t">Similarly, we can show some sort of correlation map here where if you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=253" target="_blank">00:04:13.940</a></span> | <span class="t">a pronoun like it, we want it to correlate to one of the past nouns that we have seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=258" target="_blank">00:04:18.420</a></span> | <span class="t">so far, like animal, but again, older models were really not good at this context encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=266" target="_blank">00:04:26.100</a></span> | <span class="t">So where we are currently now is on the verge of takeoff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=269" target="_blank">00:04:29.580</a></span> | <span class="t">We're beginning to realize the potential of transformers in different fields.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=272" target="_blank">00:04:32.900</a></span> | <span class="t">We have started to use them to solve long sequence problems and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=276" target="_blank">00:04:36.740</a></span> | <span class="t">protein folding, such as the alpha fold model from DeepMind,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=282" target="_blank">00:04:42.780</a></span> | <span class="t">which gets 95% accuracy on different challenges in offline RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=287" target="_blank">00:04:47.580</a></span> | <span class="t">We can use it for few-shot and zero-shot generalization for text and image generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=292" target="_blank">00:04:52.100</a></span> | <span class="t">And we can also use this for content generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=293" target="_blank">00:04:53.860</a></span> | <span class="t">So here's an example from OpenAI, where you can give a different text prompt and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=298" target="_blank">00:04:58.580</a></span> | <span class="t">have an AI-generated fictional image for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=301" target="_blank">00:05:01.260</a></span> | <span class="t">And so there's a talk on this that you can also watch on YouTube,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=306" target="_blank">00:05:06.500</a></span> | <span class="t">which basically says that LSTMs are dead and long-lived transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=309" target="_blank">00:05:09.940</a></span> | <span class="t">So what's the future?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=313" target="_blank">00:05:13.340</a></span> | <span class="t">So we can enable a lot more applications for transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=317" target="_blank">00:05:17.660</a></span> | <span class="t">They can be applied to any form of sequence modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=320" target="_blank">00:05:20.740</a></span> | <span class="t">So we could use them for real understanding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=323" target="_blank">00:05:23.340</a></span> | <span class="t">We can use them for finance and a lot more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=325" target="_blank">00:05:25.740</a></span> | <span class="t">So basically imagine all sorts of genetic modeling problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=329" target="_blank">00:05:29.180</a></span> | <span class="t">Nevertheless, there are a lot of missing ingredients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=331" target="_blank">00:05:31.660</a></span> | <span class="t">So like the human brain, we need some sort of external memory unit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=335" target="_blank">00:05:35.860</a></span> | <span class="t">which is the hippocampus for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=337" target="_blank">00:05:37.860</a></span> | <span class="t">And there are some early works here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=340" target="_blank">00:05:40.460</a></span> | <span class="t">So one nice work you might want to check out is called Neural Turing Machines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=344" target="_blank">00:05:44.340</a></span> | <span class="t">Similarly, the current attention mechanisms are very competitionally complex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=349" target="_blank">00:05:49.100</a></span> | <span class="t">in terms of time, and they scale quadratically, which we'll discuss later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=352" target="_blank">00:05:52.500</a></span> | <span class="t">And we want to make them more linear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=354" target="_blank">00:05:54.700</a></span> | <span class="t">And the third problem is that we want to align our current sort of language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=358" target="_blank">00:05:58.500</a></span> | <span class="t">with how the human brain works and human values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=361" target="_blank">00:06:01.340</a></span> | <span class="t">And this is also a big issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=363" target="_blank">00:06:03.260</a></span> | <span class="t">OK, so now I will deep dive deeper into the attention mechanisms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=370" target="_blank">00:06:10.260</a></span> | <span class="t">and show how they came out to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=372" target="_blank">00:06:12.260</a></span> | <span class="t">So initially, they used to be very simple mechanisms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=377" target="_blank">00:06:17.780</a></span> | <span class="t">Their attention was inspired by the process of importance fitting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=381" target="_blank">00:06:21.220</a></span> | <span class="t">or putting attention on different parts of an image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=384" target="_blank">00:06:24.420</a></span> | <span class="t">where like similar to a human, where you might focus more on like a foreground,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=388" target="_blank">00:06:28.380</a></span> | <span class="t">if you have an image of a dog compared to like the rest of the background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=391" target="_blank">00:06:31.060</a></span> | <span class="t">So in the case of soft attention, what you do is you learn the simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=394" target="_blank">00:06:34.340</a></span> | <span class="t">soft attention weighting for each pixel, which can be a weight between 0 to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=399" target="_blank">00:06:39.100</a></span> | <span class="t">The problem over here is that this is a very expensive computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=402" target="_blank">00:06:42.380</a></span> | <span class="t">And then you can, as shown in the figure on the left,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=406" target="_blank">00:06:46.540</a></span> | <span class="t">you can see we are calculating this attention map for the whole image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=408" target="_blank">00:06:48.740</a></span> | <span class="t">What you can do instead is you can just calculate a 0 to 1 attention map,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=415" target="_blank">00:06:55.500</a></span> | <span class="t">where we directly put a 1 on wherever the dog is and a 0 wherever it's a background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=420" target="_blank">00:07:00.780</a></span> | <span class="t">This is like less computationally expensive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=423" target="_blank">00:07:03.260</a></span> | <span class="t">but the problem is it's not differentiable and makes things harder to train.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=426" target="_blank">00:07:06.140</a></span> | <span class="t">Going forward, we also have different varieties of basic attention mechanisms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=430" target="_blank">00:07:10.980</a></span> | <span class="t">that came, that were proposed before self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=434" target="_blank">00:07:14.140</a></span> | <span class="t">So the first variety here is global attention models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=437" target="_blank">00:07:17.500</a></span> | <span class="t">So in global attention models for each hidden layer input, hidden layer output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=443" target="_blank">00:07:23.420</a></span> | <span class="t">you learn an attention weight, a of p.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=446" target="_blank">00:07:26.300</a></span> | <span class="t">And this is element-wise multiplied with your current output to calculate your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=449" target="_blank">00:07:29.660</a></span> | <span class="t">final output, yt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=451" target="_blank">00:07:31.260</a></span> | <span class="t">Similarly, you have local attention models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=455" target="_blank">00:07:35.340</a></span> | <span class="t">where instead of calculating the global attention over the whole sequence length,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=459" target="_blank">00:07:39.980</a></span> | <span class="t">you only calculate the attention over a small window.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=463" target="_blank">00:07:43.500</a></span> | <span class="t">And then you weight by the attention of the window into the current output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=469" target="_blank">00:07:49.100</a></span> | <span class="t">to get the final output you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=470" target="_blank">00:07:50.340</a></span> | <span class="t">So moving on, I'll pass on to Chaitanya to discuss self-attention mechanisms and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=476" target="_blank">00:07:56.340</a></span> | <span class="t">transforms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=478" target="_blank">00:07:58.580</a></span> | <span class="t">>> Yeah, thank you, Div, for covering a brief overview of how the primitive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=483" target="_blank">00:08:03.660</a></span> | <span class="t">versions of attention work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=485" target="_blank">00:08:05.340</a></span> | <span class="t">Now, just before we talk about self-attention, just a bit of a trivia that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=490" target="_blank">00:08:10.060</a></span> | <span class="t">this term was first introduced by a paper from Lin et al,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=494" target="_blank">00:08:14.060</a></span> | <span class="t">which provided a framework for a self-attentive mechanism for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=498" target="_blank">00:08:18.620</a></span> | <span class="t">our sentence embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=502" target="_blank">00:08:22.540</a></span> | <span class="t">And now moving on to the main crux of the transformers paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=506" target="_blank">00:08:26.420</a></span> | <span class="t">which was the self-attention block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=508" target="_blank">00:08:28.380</a></span> | <span class="t">So self-attention is the basis, is the main comp building block for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=513" target="_blank">00:08:33.300</a></span> | <span class="t">what makes the transformers model work so well and to enable them and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=518" target="_blank">00:08:38.220</a></span> | <span class="t">make them so powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=520" target="_blank">00:08:40.260</a></span> | <span class="t">So to think of it more easily,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=522" target="_blank">00:08:42.140</a></span> | <span class="t">we can break down the self-attention as a search retrieval problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=526" target="_blank">00:08:46.980</a></span> | <span class="t">So the problem is that given a query Q, and we need to find a set of keys K,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=533" target="_blank">00:08:53.260</a></span> | <span class="t">which are most similar to Q and return the corresponding key values called V.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=538" target="_blank">00:08:58.220</a></span> | <span class="t">Now, these three vectors can be drawn from the same source.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=541" target="_blank">00:09:01.060</a></span> | <span class="t">For example, we can have that Q, K, and V are all equal to a single vector X,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=545" target="_blank">00:09:05.620</a></span> | <span class="t">where X can be output of a previous layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=548" target="_blank">00:09:08.580</a></span> | <span class="t">In transformers, these vectors are obtained by applying different linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=552" target="_blank">00:09:12.820</a></span> | <span class="t">transformations to X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=554" target="_blank">00:09:14.460</a></span> | <span class="t">So as to enable the model to capture more complex interactions between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=558" target="_blank">00:09:18.780</a></span> | <span class="t">the different tokens at different places of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=562" target="_blank">00:09:22.860</a></span> | <span class="t">Now, how attention is computed is just a weighted summation of the similarities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=567" target="_blank">00:09:27.540</a></span> | <span class="t">in the query and key vectors, which is weighted by the respective value for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=571" target="_blank">00:09:31.860</a></span> | <span class="t">those keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=573" target="_blank">00:09:33.260</a></span> | <span class="t">And in the transformers paper, they use the scale dot product as a similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=577" target="_blank">00:09:37.820</a></span> | <span class="t">function for the queries and keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=580" target="_blank">00:09:40.260</a></span> | <span class="t">And another important aspect of the transformers was the introduction of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=584" target="_blank">00:09:44.580</a></span> | <span class="t">multi-head self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=586" target="_blank">00:09:46.420</a></span> | <span class="t">So what multi-head self-attention means is that the self-attention is for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=590" target="_blank">00:09:50.700</a></span> | <span class="t">at every layer, the self-attention is performed multiple times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=594" target="_blank">00:09:54.260</a></span> | <span class="t">which enables the model to learn multiple representation subspaces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=598" target="_blank">00:09:58.660</a></span> | <span class="t">So in a way, you can think of it that each head has a power to look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=605" target="_blank">00:10:05.820</a></span> | <span class="t">different things and to learn different semantics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=608" target="_blank">00:10:08.580</a></span> | <span class="t">For example, one head can be learning to try to predict what is the part of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=613" target="_blank">00:10:13.580</a></span> | <span class="t">speech for those tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=615" target="_blank">00:10:15.220</a></span> | <span class="t">One head might be learning what is the syntactic structure of the sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=619" target="_blank">00:10:19.660</a></span> | <span class="t">and all those things that are there to understand what the upcoming sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=626" target="_blank">00:10:26.860</a></span> | <span class="t">means.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=628" target="_blank">00:10:28.740</a></span> | <span class="t">Now, to better understand what the self-attention works and what are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=632" target="_blank">00:10:32.020</a></span> | <span class="t">different computations, there is a short video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=635" target="_blank">00:10:35.140</a></span> | <span class="t">So as you can see, there are three incoming tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=641" target="_blank">00:10:41.180</a></span> | <span class="t">So input 1, input 2, input 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=643" target="_blank">00:10:43.220</a></span> | <span class="t">We apply linear transformations to get the key value vectors for each input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=649" target="_blank">00:10:49.100</a></span> | <span class="t">and then once a query queue comes, we calculate its similarity with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=653" target="_blank">00:10:53.220</a></span> | <span class="t">respective key vectors, and then multiply those scores with the value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=658" target="_blank">00:10:58.900</a></span> | <span class="t">vector, and then add them all up to get the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=662" target="_blank">00:11:02.740</a></span> | <span class="t">The same computation is then performed on all the tokens, and we get the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=668" target="_blank">00:11:08.420</a></span> | <span class="t">of the self-attention layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=670" target="_blank">00:11:10.380</a></span> | <span class="t">So as you can see here, the final output of the self-attention layer is in dark</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=674" target="_blank">00:11:14.580</a></span> | <span class="t">green that's at the top of the screen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=677" target="_blank">00:11:17.660</a></span> | <span class="t">So now again, for the final token, we perform everything same, queries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=681" target="_blank">00:11:21.500</a></span> | <span class="t">multiplied by keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=682" target="_blank">00:11:22.820</a></span> | <span class="t">We get the similarity scores, and then those similarity scores weigh the value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=686" target="_blank">00:11:26.820</a></span> | <span class="t">vectors, and then we finally perform the addition to get the self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=691" target="_blank">00:11:31.300</a></span> | <span class="t">output of the transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=699" target="_blank">00:11:39.220</a></span> | <span class="t">Apart from self-attention, there are some other necessary ingredients that makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=704" target="_blank">00:11:44.620</a></span> | <span class="t">the transformer so powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=706" target="_blank">00:11:46.540</a></span> | <span class="t">One important aspect is the presence of positional representations or the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=710" target="_blank">00:11:50.700</a></span> | <span class="t">embedding layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=711" target="_blank">00:11:51.740</a></span> | <span class="t">So the way RNNs work very well was that since they process each of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=718" target="_blank">00:11:58.020</a></span> | <span class="t">information in a sequential ordering, so there was this notion of ordering,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=723" target="_blank">00:12:03.220</a></span> | <span class="t">right, and which is also very important in understanding language because we all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=726" target="_blank">00:12:06.980</a></span> | <span class="t">know that we read any piece of text from left to right in most of the languages,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=734" target="_blank">00:12:14.940</a></span> | <span class="t">and also right to left in some languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=737" target="_blank">00:12:17.220</a></span> | <span class="t">So there is a notion of ordering, which is lost in kind of self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=740" target="_blank">00:12:20.740</a></span> | <span class="t">because every word is attending to every other word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=744" target="_blank">00:12:24.060</a></span> | <span class="t">That's why this paper introduced a separate embedding layer for introducing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=748" target="_blank">00:12:28.900</a></span> | <span class="t">positional representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=750" target="_blank">00:12:30.980</a></span> | <span class="t">The second important aspect is having nonlinearities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=754" target="_blank">00:12:34.300</a></span> | <span class="t">So if you think of all the computation that is happening in the self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=758" target="_blank">00:12:38.100</a></span> | <span class="t">layer, it's all linear because it's all matrix multiplication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=761" target="_blank">00:12:41.220</a></span> | <span class="t">But as we all know, that deep learning models work well when they are able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=767" target="_blank">00:12:47.700</a></span> | <span class="t">learn more complex mappings between input and output, which can be attained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=772" target="_blank">00:12:52.140</a></span> | <span class="t">by a simple MLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=774" target="_blank">00:12:54.220</a></span> | <span class="t">And the third important component of the transformers is the masking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=779" target="_blank">00:12:59.020</a></span> | <span class="t">So masking is what allows to parallelize the operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=783" target="_blank">00:13:03.020</a></span> | <span class="t">Since every word can attend to every other word, in the decoder part of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=787" target="_blank">00:13:07.220</a></span> | <span class="t">transformers, which Advai is going to be talking about later, is the problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=791" target="_blank">00:13:11.380</a></span> | <span class="t">comes that you don't want the decoder to look into the future because that can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=796" target="_blank">00:13:16.540</a></span> | <span class="t">result in data leakage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=798" target="_blank">00:13:18.340</a></span> | <span class="t">So that's why masking helps the decoder to avoid that future information and learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=804" target="_blank">00:13:24.860</a></span> | <span class="t">only what the model has processed so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=809" target="_blank">00:13:29.580</a></span> | <span class="t">So now on to the encoder-decoder architecture of the transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=814" target="_blank">00:13:34.780</a></span> | <span class="t">Advai?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=816" target="_blank">00:13:36.580</a></span> | <span class="t">- Yeah, thanks, Saithanya, for talking about self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=819" target="_blank">00:13:39.620</a></span> | <span class="t">So self-attention is sort of the key ingredient or one of the key ingredients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=824" target="_blank">00:13:44.940</a></span> | <span class="t">that allows transformers to work so well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=827" target="_blank">00:13:47.540</a></span> | <span class="t">But at a very high level, the model that was proposed in the Vaswani et al.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=831" target="_blank">00:13:51.940</a></span> | <span class="t">paper of 2017 was like previous language models in the sense that it had an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=837" target="_blank">00:13:57.540</a></span> | <span class="t">encoder-decoder architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=839" target="_blank">00:13:59.540</a></span> | <span class="t">What that means is, let's say you're working on a translation problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=842" target="_blank">00:14:02.900</a></span> | <span class="t">You want to translate English to French.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=844" target="_blank">00:14:04.940</a></span> | <span class="t">The way that would work is you would read in the entire input of your English</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=849" target="_blank">00:14:09.340</a></span> | <span class="t">sentence, you would encode that input, so that's the encoder part of the network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=853" target="_blank">00:14:13.780</a></span> | <span class="t">And then you would generate token by token the corresponding French translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=858" target="_blank">00:14:18.420</a></span> | <span class="t">And the decoder is the part of the network that is responsible for generating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=862" target="_blank">00:14:22.980</a></span> | <span class="t">those tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=864" target="_blank">00:14:24.420</a></span> | <span class="t">So you can think of these encoder blocks and decoder blocks as essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=869" target="_blank">00:14:29.980</a></span> | <span class="t">something like Lego.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=870" target="_blank">00:14:30.980</a></span> | <span class="t">They have these sub-components that make them up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=874" target="_blank">00:14:34.580</a></span> | <span class="t">And in particular, the encoder block has three main sub-components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=878" target="_blank">00:14:38.540</a></span> | <span class="t">The first is a self-attention layer that Saithanya talked about earlier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=883" target="_blank">00:14:43.180</a></span> | <span class="t">And as talked about earlier as well, you need a feed-forward layer after that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=888" target="_blank">00:14:48.500</a></span> | <span class="t">because the self-attention layer only performs linear operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=892" target="_blank">00:14:52.140</a></span> | <span class="t">And so you need something that can capture the non-linearities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=895" target="_blank">00:14:55.540</a></span> | <span class="t">You also have a layer norm after this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=898" target="_blank">00:14:58.060</a></span> | <span class="t">And lastly, there are residual connections between different encoder blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=902" target="_blank">00:15:02.740</a></span> | <span class="t">The decoder is very similar to the encoder, but there's one difference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=906" target="_blank">00:15:06.340</a></span> | <span class="t">which is that it has this extra layer because the decoder doesn't just do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=910" target="_blank">00:15:10.100</a></span> | <span class="t">multi-head attention on the output of the previous layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=913" target="_blank">00:15:13.900</a></span> | <span class="t">So for context, the encoder does multi-head attention for each self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=919" target="_blank">00:15:19.340</a></span> | <span class="t">layer in the encoder block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=921" target="_blank">00:15:21.300</a></span> | <span class="t">In each of the encoder blocks, it does multi-head attention looking at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=926" target="_blank">00:15:26.020</a></span> | <span class="t">previous layers of the encoder blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=929" target="_blank">00:15:29.420</a></span> | <span class="t">The decoder, however, does that in the sense that it also looks at the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=934" target="_blank">00:15:34.380</a></span> | <span class="t">layers of the decoder, but it also looks at the output of the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=938" target="_blank">00:15:38.420</a></span> | <span class="t">And so it needs a multi-head attention layer over the encoder blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=943" target="_blank">00:15:43.740</a></span> | <span class="t">And lastly, there's masking as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=946" target="_blank">00:15:46.700</a></span> | <span class="t">So if you are-- because every token can look at every other token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=951" target="_blank">00:15:51.300</a></span> | <span class="t">you want to make sure in the decoder that you're not looking into the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=955" target="_blank">00:15:55.100</a></span> | <span class="t">So if you're in position 3, for instance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=957" target="_blank">00:15:57.340</a></span> | <span class="t">you shouldn't be able to look at position 4 and position 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=960" target="_blank">00:16:00.020</a></span> | <span class="t">So those are sort of all the components that led to the creation of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=968" target="_blank">00:16:08.060</a></span> | <span class="t">in the Vaswani et al paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=970" target="_blank">00:16:10.700</a></span> | <span class="t">And let's talk a little bit about the advantages and drawbacks of this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=976" target="_blank">00:16:16.540</a></span> | <span class="t">So the two main advantages, which are huge advantages and which are why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=980" target="_blank">00:16:20.660</a></span> | <span class="t">transformers have done such a good job of revolutionizing many,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=985" target="_blank">00:16:25.740</a></span> | <span class="t">many fields within deep learning, are as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=989" target="_blank">00:16:29.660</a></span> | <span class="t">So the first is there is this constant path length between any two positions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=994" target="_blank">00:16:34.260</a></span> | <span class="t">in a sequence because every token in the sequence is looking at every other token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=999" target="_blank">00:16:39.700</a></span> | <span class="t">And this basically solves the problem that Dev talked about earlier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1003" target="_blank">00:16:43.060</a></span> | <span class="t">with long sequences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1004" target="_blank">00:16:44.580</a></span> | <span class="t">You don't have this problem with long sequences where if you're trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1007" target="_blank">00:16:47.940</a></span> | <span class="t">predict a token that depends on a word that was far, far behind in a sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1014" target="_blank">00:16:54.220</a></span> | <span class="t">you don't have the problem of losing that context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1015" target="_blank">00:16:55.940</a></span> | <span class="t">Now, the distance between them is only one in terms of the path length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1020" target="_blank">00:17:00.700</a></span> | <span class="t">Also, because of the nature of the computation that's happening,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1023" target="_blank">00:17:03.900</a></span> | <span class="t">transformer models lend themselves really well to parallelization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1027" target="_blank">00:17:07.380</a></span> | <span class="t">And because of the advances that we've had with GPUs, basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1030" target="_blank">00:17:10.900</a></span> | <span class="t">if you take a transformer model with n parameters and you take a model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1034" target="_blank">00:17:14.660</a></span> | <span class="t">isn't a transformer, say like an MSTM, also with n parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1038" target="_blank">00:17:18.340</a></span> | <span class="t">training the transformer model is going to be much faster because of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1042" target="_blank">00:17:22.020</a></span> | <span class="t">parallelization that it leverages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1044" target="_blank">00:17:24.500</a></span> | <span class="t">So those are the advantages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1046" target="_blank">00:17:26.180</a></span> | <span class="t">The disadvantages are basically self-attention takes quadratic time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1051" target="_blank">00:17:31.220</a></span> | <span class="t">because every token looks at every other token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1053" target="_blank">00:17:33.620</a></span> | <span class="t">Order n squared, as you might know, does not scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1056" target="_blank">00:17:36.500</a></span> | <span class="t">And there's actually been a lot of work in trying to tackle this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1060" target="_blank">00:17:40.220</a></span> | <span class="t">So we've linked to some here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1061" target="_blank">00:17:41.500</a></span> | <span class="t">Big Bird, Linformer, and Reformer are all approaches to try and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1064" target="_blank">00:17:44.580</a></span> | <span class="t">make this linear or quasi-linear, essentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1067" target="_blank">00:17:47.580</a></span> | <span class="t">And yeah, we highly recommend going through Jay Allamer's blog,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1075" target="_blank">00:17:55.820</a></span> | <span class="t">the Illustrated Transformer, which provides great visualizations and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1079" target="_blank">00:17:59.940</a></span> | <span class="t">explains everything that we just talked about in great detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1082" target="_blank">00:18:02.260</a></span> | <span class="t">Yeah, and I'd like to pass it on to Chaitanya for applications of transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1090" target="_blank">00:18:10.460</a></span> | <span class="t">So now moving on to like some of the recent work, some of the work that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1094" target="_blank">00:18:14.780</a></span> | <span class="t">very shortly followed the Transformers paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1098" target="_blank">00:18:18.060</a></span> | <span class="t">So one of the models that came out was GPT, the GPT architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1103" target="_blank">00:18:23.740</a></span> | <span class="t">which was released by OpenAI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1104" target="_blank">00:18:24.940</a></span> | <span class="t">So OpenAI had the latest model that OpenAI has in the GPT series is the GPT-3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1111" target="_blank">00:18:31.260</a></span> | <span class="t">So it consists of only the decoder blocks from Transformers and is trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1115" target="_blank">00:18:35.500</a></span> | <span class="t">on a traditional language modeling task, which is predicting the current token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1120" target="_blank">00:18:40.300</a></span> | <span class="t">which is predicting the next token given the last T tokens that the model has seen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1125" target="_blank">00:18:45.900</a></span> | <span class="t">And for any downstream tasks, now the model can just,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1129" target="_blank">00:18:49.900</a></span> | <span class="t">you can just train a classification layer on the last hidden state,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1133" target="_blank">00:18:53.260</a></span> | <span class="t">which can have any number of labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1137" target="_blank">00:18:57.660</a></span> | <span class="t">And since the model is generative in nature, you can also use the pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1143" target="_blank">00:19:03.420</a></span> | <span class="t">network as for generative kind of tasks, such as summarization and natural language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1149" target="_blank">00:19:09.100</a></span> | <span class="t">and natural language generation for that instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1152" target="_blank">00:19:12.380</a></span> | <span class="t">Another important aspect that GPT-3 gained popularity was its ability to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1158" target="_blank">00:19:18.540</a></span> | <span class="t">be able to perform in-context learning, what the authors called in-context learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1163" target="_blank">00:19:23.580</a></span> | <span class="t">So this is the ability wherein the model can perform, can learn under few short settings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1168" target="_blank">00:19:28.780</a></span> | <span class="t">what the task is to complete the task without performing any gradient updates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1173" target="_blank">00:19:33.820</a></span> | <span class="t">For example, let's say the model is shown a bunch of addition examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1178" target="_blank">00:19:38.620</a></span> | <span class="t">And then if you pass in a new input and leave the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1182" target="_blank">00:19:42.220</a></span> | <span class="t">and just leave it at equal to sign, the model tries to predict the next token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1189" target="_blank">00:19:49.020</a></span> | <span class="t">which very well comes out to be the sum of the numbers that is shown.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1195" target="_blank">00:19:55.660</a></span> | <span class="t">Another example can be also the spell correction task or the translation task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1200" target="_blank">00:20:00.540</a></span> | <span class="t">So this was the ability that made GPT-3 so much talked about in the NLP world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1208" target="_blank">00:20:08.060</a></span> | <span class="t">And right now also, many applications have been made using GPT-3, which includes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1213" target="_blank">00:20:13.820</a></span> | <span class="t">one of them being the VS Code Copilot, which tries to generate a piece of code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1221" target="_blank">00:20:21.820</a></span> | <span class="t">given a docstring kind of natural language text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1226" target="_blank">00:20:26.140</a></span> | <span class="t">Another major model that came out that was based on the Transformers architecture was BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1232" target="_blank">00:20:32.220</a></span> | <span class="t">So BERT lends its name from, it's an acronym for Bidirectional Encoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1237" target="_blank">00:20:37.020</a></span> | <span class="t">encoder representations of Transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1238" target="_blank">00:20:38.940</a></span> | <span class="t">It consists of only the encoder blocks of the Transformers, which is unlike GPT-3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1244" target="_blank">00:20:44.620</a></span> | <span class="t">which had only the decoder blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1246" target="_blank">00:20:46.620</a></span> | <span class="t">Now, because of this change, there comes a problem because BERT has only the encoder blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1255" target="_blank">00:20:55.100</a></span> | <span class="t">So it sees the entire piece of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1257" target="_blank">00:20:57.260</a></span> | <span class="t">It cannot be pre-trained on a naive language modeling task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1260" target="_blank">00:21:00.220</a></span> | <span class="t">because of the problem of data leakage from the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1262" target="_blank">00:21:02.940</a></span> | <span class="t">So what the authors came up with was a clever idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1266" target="_blank">00:21:06.700</a></span> | <span class="t">And they came up with a novel task called mass language modeling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1270" target="_blank">00:21:10.860</a></span> | <span class="t">which included to replace certain words with a placeholder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1275" target="_blank">00:21:15.180</a></span> | <span class="t">And then the model tries to predict those words given the entire context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1279" target="_blank">00:21:19.980</a></span> | <span class="t">Now, apart from this token-level task, the authors also added a second objective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1285" target="_blank">00:21:25.900</a></span> | <span class="t">called the next sentence prediction, which was a sentence-level task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1288" target="_blank">00:21:28.780</a></span> | <span class="t">Wherein, given two chunks of text, the model tried to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1294" target="_blank">00:21:34.300</a></span> | <span class="t">whether the second sentence followed the other sentence or not,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1297" target="_blank">00:21:37.740</a></span> | <span class="t">followed the first sentence or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1299" target="_blank">00:21:39.180</a></span> | <span class="t">And now, after pre-training this model for any downstream task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1303" target="_blank">00:21:43.580</a></span> | <span class="t">the model can be further fine-tuned with an additional classification layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1306" target="_blank">00:21:46.700</a></span> | <span class="t">just like it was in GPT-3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1309" target="_blank">00:21:49.580</a></span> | <span class="t">So these are the two models that have been very popular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1314" target="_blank">00:21:54.300</a></span> | <span class="t">and have made a lot of applications, made their way in a lot of applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1318" target="_blank">00:21:58.940</a></span> | <span class="t">But the landscape has changed quite a lot since we have taken this class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1322" target="_blank">00:22:02.540</a></span> | <span class="t">There are models with different pre-training techniques, like Electra, D-BERTA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1327" target="_blank">00:22:07.020</a></span> | <span class="t">And there are also models that do well in other modalities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1332" target="_blank">00:22:12.140</a></span> | <span class="t">and which we are going to be talking about in other lecture series as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1335" target="_blank">00:22:15.260</a></span> | <span class="t">So yeah, that's all from this lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1338" target="_blank">00:22:18.700</a></span> | <span class="t">And thank you for tuning in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1340" target="_blank">00:22:20.140</a></span> | <span class="t">- Yeah, just want to end by saying thank you all for watching this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1345" target="_blank">00:22:25.580</a></span> | <span class="t">And we have a really exciting set of videos with truly amazing speakers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1350" target="_blank">00:22:30.940</a></span> | <span class="t">And we hope you are able to derive value from that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1353" target="_blank">00:22:33.180</a></span> | <span class="t">- Okay, thanks a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1355" target="_blank">00:22:35.100</a></span> | <span class="t">- Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1355" target="_blank">00:22:35.820</a></span> | <span class="t">- Thank you, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1357" target="_blank">00:22:37.500</a></span> | <span class="t">- Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1358" target="_blank">00:22:38.800</a></span> | <span class="t">- Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P127jhj-8-Y&t=1359" target="_blank">00:22:39.300</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>