<html><head><title>Language Diffusion Survey</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Language Diffusion Survey</h2><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk"><img src="https://i.ytimg.com/vi/tRf0H-P2Jfk/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./tRf0H-P2Jfk.html">Whisper Transcript</a> | <a href="./transcript_tRf0H-P2Jfk.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">All right. So jumping into it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=5" target="_blank">00:00:05.080</a></span> | <span class="t">So just general, yeah, intro yourself. Don't forget. We want to know about you and your personal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=11" target="_blank">00:00:11.640</a></span> | <span class="t">why. Okay. Yeah, there you go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=13" target="_blank">00:00:13.480</a></span> | <span class="t">Yeah, for sure. Yeah, so I'm currently a master student at Georgia Tech. I know a handful of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=20" target="_blank">00:00:20.040</a></span> | <span class="t">folks in this community have also taken some classes there. But yeah, I quit my job about a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=27" target="_blank">00:00:27.100</a></span> | <span class="t">year ago to go full time just to speed up the process and take some harder classes and kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=31" target="_blank">00:00:31.120</a></span> | <span class="t">of dig a little deeper. So I've got about a year left trying to focus on machine learning stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=39" target="_blank">00:00:39.280</a></span> | <span class="t">and a bunch of systems courses. I didn't have like a CS undergrad, so I'm really trying to fill in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=44" target="_blank">00:00:44.440</a></span> | <span class="t">some gaps there. But did some web dev stuff, some data engineering, some information retrieval stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=50" target="_blank">00:00:50.080</a></span> | <span class="t">at a legal company, and then at Rose set a stone before that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=54" target="_blank">00:00:54.200</a></span> | <span class="t">Let's see. So I guess sort of the motivation for this is definitely like the Gemini diffusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=62" target="_blank">00:01:02.700</a></span> | <span class="t">announcement. I know that Mercury got introduced by Inception Labs, I think back in January,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=70" target="_blank">00:01:10.680</a></span> | <span class="t">which they're not a big outfit. But one of the people on some of these papers is one of the founders</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=79" target="_blank">00:01:19.020</a></span> | <span class="t">there. And one of their big claims is just like they can achieve similar levels of accuracy at much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=85" target="_blank">00:01:25.520</a></span> | <span class="t">faster output speed. So I think that's just an interesting dimension. I guess sort of for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=94" target="_blank">00:01:34.360</a></span> | <span class="t">structure of this task, I think about this as like, how do we teach in general, spend a little bit of time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=102" target="_blank">00:01:42.260</a></span> | <span class="t">as teaching at a coding boot camp. And I'm trying to keep you guys in that flow state between boredom and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=108" target="_blank">00:01:48.060</a></span> | <span class="t">anxiety. So, you know, if it's going too fast, feel free to ask questions. If you're bored, I'm sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=112" target="_blank">00:01:52.580</a></span> | <span class="t">You know, take a look at some of the papers that are referenced. This is definitely optimized for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=118" target="_blank">00:01:58.580</a></span> | <span class="t">exploration as opposed to exploitation. So I'm going over a bunch of different papers, but not very deep,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=123" target="_blank">00:02:03.620</a></span> | <span class="t">but happy to kind of pause and redirect as needed. Don't want to keep this like super formal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=128" target="_blank">00:02:08.420</a></span> | <span class="t">nice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=133" target="_blank">00:02:13.620</a></span> | <span class="t">Did he freeze?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=139" target="_blank">00:02:19.780</a></span> | <span class="t">Did in fact freeze. Oh, no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=144" target="_blank">00:02:24.260</a></span> | <span class="t">Yeah. Okay. It's okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=146" target="_blank">00:02:26.260</a></span> | <span class="t">We broke out a flow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=147" target="_blank">00:02:27.620</a></span> | <span class="t">Flow is still going. Keep keep it going. Who's seen Gemini diffusion? Thousands of tokens per second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=154" target="_blank">00:02:34.260</a></span> | <span class="t">So smart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=155" target="_blank">00:02:35.060</a></span> | <span class="t">I don't think thousands. I think I think a thousand, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=158" target="_blank">00:02:38.420</a></span> | <span class="t">We shall see. Let's ask Gemini. How many tokens per second is Gemini diffusion?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=168" target="_blank">00:02:48.420</a></span> | <span class="t">Yeah, let's let's also ping him on discord.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=172" target="_blank">00:02:52.660</a></span> | <span class="t">I think that there's quite a few ways to still optimize it. Wow. Gemini is so smart. Gemini says that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=180" target="_blank">00:03:00.100</a></span> | <span class="t">it says it is 1479 tokens per second. But there's a brief overhead delay of 0.84 seconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=189" target="_blank">00:03:09.300</a></span> | <span class="t">That's rough. You got to wait a second for your thousand tokens per second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=195" target="_blank">00:03:15.380</a></span> | <span class="t">There's a question here from IO about are diffusion models auto-regressive? Typically not. But you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=201" target="_blank">00:03:21.860</a></span> | <span class="t">have an auto-regressive body and a diffusion head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=207" target="_blank">00:03:27.540</a></span> | <span class="t">Okay. Sorry about that. I think Zoom crashed on me. That's the last of it. Cool. So I'll try to run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=219" target="_blank">00:03:39.540</a></span> | <span class="t">through this. So really just kind of motivating this with some of the foundations. What is a generative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=225" target="_blank">00:03:45.860</a></span> | <span class="t">model in the abstract? We're trying to model kind of an underlying probability distribution of data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=233" target="_blank">00:03:53.140</a></span> | <span class="t">that's unknown. And then once we learned the distribution, then we're able to do stuff with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=238" target="_blank">00:03:58.340</a></span> | <span class="t">it. So this slide is based on a talk from Yang Song, who did some foundational stuff starting kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=246" target="_blank">00:04:06.020</a></span> | <span class="t">in 2019 and has been a big contributor to diffusion models in general since then. But I thought his graphic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=253" target="_blank">00:04:13.380</a></span> | <span class="t">was pretty cool. So we've got our probability of a data point X given the -- based on the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=272" target="_blank">00:04:32.420</a></span> | <span class="t">of the data. And then we're trying to find a model that's able to learn that distribution based on a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=279" target="_blank">00:04:39.700</a></span> | <span class="t">laminal parameters theta. And during training, the p theta X is treated as a likelihood function and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=286" target="_blank">00:04:46.580</a></span> | <span class="t">measuring it that deserves observed data under the current parameters theta. We can think about this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=292" target="_blank">00:04:52.020</a></span> | <span class="t">as a KL divergence between the two probability distributions and sort of the optimization function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=297" target="_blank">00:04:57.460</a></span> | <span class="t">we're trying to do is minimize the difference between these two distributions. Another equivalent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=303" target="_blank">00:05:03.940</a></span> | <span class="t">framing is to say that we're trying to find the parameters theta that maximize the likelihood of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=312" target="_blank">00:05:12.100</a></span> | <span class="t">estimating expected data. And then we can do this equivalence relation. The problem is that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=318" target="_blank">00:05:18.020</a></span> | <span class="t">for most data distributions that we care about, the shape of the distribution is really complex and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=328" target="_blank">00:05:28.340</a></span> | <span class="t">hard to model. And then it's also hard to find a set of weights that are able to appropriately model it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=336" target="_blank">00:05:36.180</a></span> | <span class="t">So in that naive case, we can say we're using a Gaussian distribution, which isn't very expressive and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=346" target="_blank">00:05:46.020</a></span> | <span class="t">isn't able to model complex spaces. Or we can use kind of a neural net, which is based on universal approximation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=356" target="_blank">00:05:56.420</a></span> | <span class="t">theorem is able to model our data. And then we can sample from it. Sort of the challenge becomes, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=362" target="_blank">00:06:02.420</a></span> | <span class="t">how do we make sure that it's appropriately expressive, but isn't too expressive. So another view of this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=372" target="_blank">00:06:12.820</a></span> | <span class="t">just like a narrow slice of a two-dimensional mapping of sort of the feature space that Claude 3 learns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=385" target="_blank">00:06:25.140</a></span> | <span class="t">So just another kind of perspective into what the general modeling task is. We're trying to learn this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=390" target="_blank">00:06:30.980</a></span> | <span class="t">distribution or a distribution that's able to model the true space of the data that we care about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=400" target="_blank">00:06:40.100</a></span> | <span class="t">Cool. So jumping into a bunch of different types of general models. Again, this is sort of more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=408" target="_blank">00:06:48.980</a></span> | <span class="t">background information. This is inspired by an old talk by Ian Goodfellow, but breaking down the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=418" target="_blank">00:06:58.500</a></span> | <span class="t">general models into explicit densities of implicit density. I'm starting kind of on the left branch here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=424" target="_blank">00:07:04.740</a></span> | <span class="t">We're saying that a generative model aims to learn the underlying probability distribution data. Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=430" target="_blank">00:07:10.100</a></span> | <span class="t">So that's what we just discussed. And then the explicit density</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=433" target="_blank">00:07:13.780</a></span> | <span class="t">is we're defining p theta x. Again, using the maximum likelihood. These are split, again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=445" target="_blank">00:07:25.380</a></span> | <span class="t">into tractable density models and approximate density models. The tractable density models include</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=452" target="_blank">00:07:32.660</a></span> | <span class="t">auto-aggressive models and normalizing flows. So sort of the classic formulation for auto-aggressive models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=460" target="_blank">00:07:40.260</a></span> | <span class="t">RNNs, LSTMs, GPT, et cetera, kind of follows this joint probability distribution of a single token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=475" target="_blank">00:07:55.300</a></span> | <span class="t">given the previous tokens. And that just kind of continues auto-aggressively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=479" target="_blank">00:07:59.140</a></span> | <span class="t">For explicit density functions, they're modifying the density itself, but it's intractable to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=496" target="_blank">00:08:16.340</a></span> | <span class="t">that directly. So we use an approximation on that. And so these both use lower bounds on what the density</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=507" target="_blank">00:08:27.060</a></span> | <span class="t">function we're trying to learn is. So we talked about VAE or VQVAE recently. I think Ted did a talk on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=516" target="_blank">00:08:36.900</a></span> | <span class="t">And then today we're getting into diffusion models, but this is sort of just like framing it with other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=523" target="_blank">00:08:43.300</a></span> | <span class="t">types. And then, you know, going back up to our tree, we've got implicit density functions, which includes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=530" target="_blank">00:08:50.740</a></span> | <span class="t">GANs, energy-based models, which aren't used a ton, but sort of inspire or motivate a lot of the -- or some of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=538" target="_blank">00:08:58.900</a></span> | <span class="t">approaches that are used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=539" target="_blank">00:08:59.780</a></span> | <span class="t">So with GANs, you've got a generator discriminator, and you're trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=549" target="_blank">00:09:09.380</a></span> | <span class="t">generate a sample that's able to fool the discriminator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=554" target="_blank">00:09:14.500</a></span> | <span class="t">GANs, but with each of all of these, we can view it as forming some sort of lean representation of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=564" target="_blank">00:09:24.180</a></span> | <span class="t">what our model -- our data distribution is, compressing it into a different form and then trying to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=574" target="_blank">00:09:34.900</a></span> | <span class="t">how to reassemble it from that later representation. There's a lot of kind of back and forth between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=584" target="_blank">00:09:44.500</a></span> | <span class="t">the different generative modeling approaches. The space is pretty rich and people pollinate ideas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=594" target="_blank">00:09:54.020</a></span> | <span class="t">between all of this stuff. Again, this is sort of hopefully not too much in the boredom side of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=599" target="_blank">00:09:59.860</a></span> | <span class="t">things. But for folks that are newer to the space, I think it's helpful to kind of have the preliminaries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=606" target="_blank">00:10:06.260</a></span> | <span class="t">So jumping into diffusion models foundations. Actually, let me pause for questions real quick and check chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=618" target="_blank">00:10:18.420</a></span> | <span class="t">Just let me know if anybody's got something I can pause for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=622" target="_blank">00:10:22.020</a></span> | <span class="t">There's a question about elbow. I vaguely remember elbow. I forget what elbows are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=627" target="_blank">00:10:27.300</a></span> | <span class="t">Yeah, I think it's the expression lower bounds. So it comes up in VE's and saw it in some of the diffusion stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=640" target="_blank">00:10:40.100</a></span> | <span class="t">But it's sort of a trick for how to work around learning, like the true probability distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=648" target="_blank">00:10:48.340</a></span> | <span class="t">because that's intractable. So we're saying we can get as close as this approximation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=654" target="_blank">00:10:54.180</a></span> | <span class="t">of what the distribution should be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=662" target="_blank">00:11:02.980</a></span> | <span class="t">On my side, I didn't super get implicit versus explicit. I guess like we're used to explicit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=669" target="_blank">00:11:09.780</a></span> | <span class="t">Yeah, for sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=673" target="_blank">00:11:13.460</a></span> | <span class="t">Also, how did you make these?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=676" target="_blank">00:11:16.180</a></span> | <span class="t">This was an obsidian canvas. And I just took screenshots of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=682" target="_blank">00:11:22.900</a></span> | <span class="t">God.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=684" target="_blank">00:11:24.100</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=684" target="_blank">00:11:24.100</a></span> | <span class="t">It looks great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=685" target="_blank">00:11:25.620</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=690" target="_blank">00:11:30.100</a></span> | <span class="t">Like, everyone is explicit, right? Based on the diagram that you...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=694" target="_blank">00:11:34.740</a></span> | <span class="t">Everyone except for GANs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=698" target="_blank">00:11:38.020</a></span> | <span class="t">Yeah. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=699" target="_blank">00:11:39.780</a></span> | <span class="t">Because they do like the weird generator discriminator min/max game to like try to optimize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=708" target="_blank">00:11:48.260</a></span> | <span class="t">I don't think it's weird at all. I think we're about to see that with multi-agent. That's basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=713" target="_blank">00:11:53.620</a></span> | <span class="t">what OpenAI is working on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=714" target="_blank">00:11:54.580</a></span> | <span class="t">Oh, really? That's cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=718" target="_blank">00:11:58.900</a></span> | <span class="t">Well, I'll bite my tongue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=720" target="_blank">00:12:00.100</a></span> | <span class="t">It's a little spoiler that we recorded an episode with Gnome Brown.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=726" target="_blank">00:12:06.660</a></span> | <span class="t">Damn. That's cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=728" target="_blank">00:12:08.100</a></span> | <span class="t">Anyway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=728" target="_blank">00:12:08.100</a></span> | <span class="t">So, trying to go back as far as I could. The first reference I found to kind of iteratively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=740" target="_blank">00:12:20.260</a></span> | <span class="t">applying noise was in extracting this paper, extracting and composing robust features with denoising autoencoders.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=746" target="_blank">00:12:26.820</a></span> | <span class="t">So, this is before like diffusion became a thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=749" target="_blank">00:12:29.140</a></span> | <span class="t">Yashio Bengio's one here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=751" target="_blank">00:12:31.380</a></span> | <span class="t">And then Pascal Vincent also shows up a lot of kind of earlier work out of University of Montreal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=757" target="_blank">00:12:37.860</a></span> | <span class="t">So, a quote from the paper is destruction. For each input X, a fixed number of VD of components are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=766" target="_blank">00:12:46.500</a></span> | <span class="t">chosen at random and their value is forced to zero while the others are left untouched. All information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=771" target="_blank">00:12:51.780</a></span> | <span class="t">about the chosen components is thus removed from the particular input pattern. The autoencoder will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=776" target="_blank">00:12:56.820</a></span> | <span class="t">trained to fill in those particular introduced blanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=782" target="_blank">00:13:02.020</a></span> | <span class="t">So, they're just randomly picking elements and masking them out and then trying to learn a representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=792" target="_blank">00:13:12.180</a></span> | <span class="t">based on the mask stuff. They found that it improves sort of the performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=796" target="_blank">00:13:16.660</a></span> | <span class="t">Benjio picked that up a couple of years later. So, that was 2008. He had a handful of papers between this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=805" target="_blank">00:13:25.700</a></span> | <span class="t">And then, in 2013, this one I thought was kind of interesting where definitely in the same vein,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=815" target="_blank">00:13:35.140</a></span> | <span class="t">but they've taken a bit further. We have proven that training a model to denoise is a way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=819" target="_blank">00:13:39.380</a></span> | <span class="t">implicitly estimate the underlying data generating process and that a simple Markov chain that alternates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=825" target="_blank">00:13:45.380</a></span> | <span class="t">sampling from the denoising model and from the corrupting process converges to the estimator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=830" target="_blank">00:13:50.420</a></span> | <span class="t">This provides a mean for generating data from any denoising autoencoder. So, they've got, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=837" target="_blank">00:13:57.620</a></span> | <span class="t">they had a figure from MNIST where they're iteratively applying noise on one side and then walking it back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=847" target="_blank">00:14:07.060</a></span> | <span class="t">on the other side. So, can we take a noisy image and reconstruct it? Which definitely leaves the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=853" target="_blank">00:14:13.380</a></span> | <span class="t">foundation for some of the stuff to come. This was like the first paper that people associate with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=859" target="_blank">00:14:19.300</a></span> | <span class="t">Diffusion process. So, kind of like the past ones, a lot of this comes from physics. Like a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=870" target="_blank">00:14:30.740</a></span> | <span class="t">old school ML, like early 2000s and before is like statistics and statistical mechanics. So, one of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=880" target="_blank">00:14:40.500</a></span> | <span class="t">examples I've heard to describe this is like how do you like a stochastic process is like dropping a drop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=887" target="_blank">00:14:47.700</a></span> | <span class="t">of ink in water and seeing it kind of like spread out. And then the reverse process is like how would you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=892" target="_blank">00:14:52.660</a></span> | <span class="t">like imagine where the drop came from? Shit. Okay. Okay. So, similar to the past ones, they iteratively add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=909" target="_blank">00:15:09.220</a></span> | <span class="t">noise. They talk about stochastic differential equations, which the math is deeper than I fully</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=917" target="_blank">00:15:17.860</a></span> | <span class="t">understand or want to try to get into. But it's the underpinnings are just kind of on these stochastic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=923" target="_blank">00:15:23.540</a></span> | <span class="t">processes, which were heavily inspired by physics and some of the stuff into like how do particles move?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=932" target="_blank">00:15:32.820</a></span> | <span class="t">How does heat and entropy associated with heat kind of perform?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=937" target="_blank">00:15:37.860</a></span> | <span class="t">And some of the examples from that paper, they've got like on the left, figure A, we've got C410 holdout</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=950" target="_blank">00:15:50.420</a></span> | <span class="t">images. They corrupt them in figure B with like Gaussian noise. And then they denoise the images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=960" target="_blank">00:16:00.420</a></span> | <span class="t">and try to reconstruct them. And we can see that the reconstructions are pretty decent. And I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=966" target="_blank">00:16:06.020</a></span> | <span class="t">this was with a two layer network. So, not a ton of parameters, but they're still able to get some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=972" target="_blank">00:16:12.900</a></span> | <span class="t">interesting results. And again, yeah, we're gradually applying noise in the forward diffusion and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=979" target="_blank">00:16:19.540</a></span> | <span class="t">reverse diffusion, recovering the distribution. Kind of speed summary of 2015 to 2019.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=986" target="_blank">00:16:26.900</a></span> | <span class="t">2017. GANs got released. I think Bingio was also on the GAN paper with Ian Goodfellow, I think around 2014.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=995" target="_blank">00:16:35.460</a></span> | <span class="t">They blew up, they had kind of leading results for the Fisher inception, just to whatever the FID score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1007" target="_blank">00:16:47.620</a></span> | <span class="t">for a while. Their GANs kind of struggle with a mode collapse. So, I did a project on GANs a few months ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1019" target="_blank">00:16:59.300</a></span> | <span class="t">And I was trying to reconstruct the fashion in this. And like half of my results came out as boots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1028" target="_blank">00:17:08.180</a></span> | <span class="t">when there should be shirts and hats and all kinds of stuff. VQVAE was released, I think, 2018. We talked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1039" target="_blank">00:17:19.060</a></span> | <span class="t">about that recently. But it compresses images into a discrete one-dimensional space. And then tries to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1044" target="_blank">00:17:24.740</a></span> | <span class="t">predict it from the compressed space. And these kind of like went back and forth on what the state-of-the-art</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1054" target="_blank">00:17:34.100</a></span> | <span class="t">results were. Then in 2020, Song and Ehrman took another stab at diffusion models. They framed it in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1067" target="_blank">00:17:47.700</a></span> | <span class="t">terms of this thing called score matching. Where their approach to this was very math heavy. But they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1079" target="_blank">00:17:59.540</a></span> | <span class="t">trying to understand how a diffusion process is able to learn a function to score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1084" target="_blank">00:18:04.820</a></span> | <span class="t">the data in a high-dimensional space. And then they also introduced some processes for scaling that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1093" target="_blank">00:18:13.940</a></span> | <span class="t">And then they introduced a slightly larger network. And based on their principled approach, we're able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1104" target="_blank">00:18:24.340</a></span> | <span class="t">increase the sample size and quality of the images that they could generate. So just a snapshot from this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1110" target="_blank">00:18:30.740</a></span> | <span class="t">So still kind of look bad, but better. And then their FID score was 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1119" target="_blank">00:18:39.940</a></span> | <span class="t">The big breakout paper for diffusion was this one, which is usually called DDPMs, but denoising diffusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1131" target="_blank">00:18:51.140</a></span> | <span class="t">of probabilistic models. Similar to previous work. This one uses a forward and refuse deversion process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1139" target="_blank">00:18:59.860</a></span> | <span class="t">One of the key breakouts here was that they restricted this to only predicting the mean of the Gaussian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1149" target="_blank">00:19:09.860</a></span> | <span class="t">distribution from the noise was added. And then we're able to show with a bunch of math that the mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1159" target="_blank">00:19:19.460</a></span> | <span class="t">predicting the mean is equivalent to predicting the sample X or the image sample that they're trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1164" target="_blank">00:19:24.980</a></span> | <span class="t">to find. So they effectively, they're trying to predict the sample X, but they do that by predicting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1177" target="_blank">00:19:37.460</a></span> | <span class="t">the noise applied to X. And then they also introduced a much larger network. So this is a unit that they pulled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1185" target="_blank">00:19:45.380</a></span> | <span class="t">from image segmentation tasks. And they achieved a state of the art for FID. Again, this is 2020.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1194" target="_blank">00:19:54.100</a></span> | <span class="t">I'm not going to go through all this. Song came back pretty shortly afterwards. And it iterated on his last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1210" target="_blank">00:20:10.580</a></span> | <span class="t">framework. More stuff with stochastic differential equations. Some of these papers or all these papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1217" target="_blank">00:20:17.220</a></span> | <span class="t">that I've talked about sort of form the underpinnings for current work. They're really kind of like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1222" target="_blank">00:20:22.420</a></span> | <span class="t">landmark ones. And people keep referencing particularly this, the DDPM paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1233" target="_blank">00:20:33.220</a></span> | <span class="t">And again, the results look even better. You can see here, this is the score function in the middle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1239" target="_blank">00:20:39.700</a></span> | <span class="t">From the song paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1250" target="_blank">00:20:50.340</a></span> | <span class="t">Okay. So jumping ahead to actually pause there. Any questions? A lot of the stuff is kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1259" target="_blank">00:20:59.060</a></span> | <span class="t">background, so trying to go through quickly. But that doesn't mean it's not interesting or important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1263" target="_blank">00:21:03.540</a></span> | <span class="t">I had a side tangent on video diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1270" target="_blank">00:21:10.020</a></span> | <span class="t">And then also, I think the other thing I was waiting for you to cover, but it looks like you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1276" target="_blank">00:21:16.740</a></span> | <span class="t">stopped it like 2020, right? Did the paper be covered?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1281" target="_blank">00:21:21.300</a></span> | <span class="t">Yeah, I think so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1283" target="_blank">00:21:23.620</a></span> | <span class="t">2021. You didn't cover latent diffusion? Was that latent diffusion?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1288" target="_blank">00:21:28.500</a></span> | <span class="t">No, I didn't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1289" target="_blank">00:21:29.620</a></span> | <span class="t">Well, latent diffusion is very important. And then consistency models and flow matching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1294" target="_blank">00:21:34.900</a></span> | <span class="t">Those are the three things that I think the last three years of diffusion have kind of thought taught us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1299" target="_blank">00:21:39.300</a></span> | <span class="t">For sure. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1305" target="_blank">00:21:45.940</a></span> | <span class="t">We've covered those things in previous paperclips.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1311" target="_blank">00:21:51.620</a></span> | <span class="t">Yeah, I think that the background was super helpful. Thank you, Tyler.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1317" target="_blank">00:21:57.780</a></span> | <span class="t">It was a good catch up for me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1319" target="_blank">00:21:59.860</a></span> | <span class="t">Okay, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1324" target="_blank">00:22:04.580</a></span> | <span class="t">Cool. So jumping into this one is 2021.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1330" target="_blank">00:22:10.980</a></span> | <span class="t">So after the DDPM paper got state of the art on FID scores for ImageNet, it got a lot of attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1342" target="_blank">00:22:22.180</a></span> | <span class="t">The research kind of spiked afterwards. A lot of the focus was still on applications with image generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1352" target="_blank">00:22:32.180</a></span> | <span class="t">But increasingly, it started to become with text modeling and other modalities. So audio, video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1358" target="_blank">00:22:38.180</a></span> | <span class="t">This was one of the first ones I could find with reference to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1364" target="_blank">00:22:44.740</a></span> | <span class="t">text modeling. And this is the Austin et al. 2021 structure denoising models in discrete spaces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1376" target="_blank">00:22:56.020</a></span> | <span class="t">So let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1380" target="_blank">00:23:00.020</a></span> | <span class="t">One of the question paper is we develop a structured corruption process for appropriate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1387" target="_blank">00:23:07.060</a></span> | <span class="t">processes appropriate for text using similarity between tokens to enable gradual corruption and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1392" target="_blank">00:23:12.580</a></span> | <span class="t">denoising. Expanding further, we also explore a corruption process that insert mask tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1397" target="_blank">00:23:17.620</a></span> | <span class="t">allow us to draw parallels to autoregressive and mass-based generative models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1400" target="_blank">00:23:20.900</a></span> | <span class="t">I'm actually going to skip the slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1408" target="_blank">00:23:28.980</a></span> | <span class="t">So the process they introduce is they think of, you know, a string of text as a sequence of discrete</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1421" target="_blank">00:23:41.300</a></span> | <span class="t">tokens. We can do that as a reshape it into a matrix. And then if we're randomly masking a token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1432" target="_blank">00:23:52.020</a></span> | <span class="t">we can do that by sampling Gaussian noise. But each one of these samples is discrete versus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1439" target="_blank">00:23:59.460</a></span> | <span class="t">some of the previous approaches we talked about, or assume that the distribution is continuous,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1444" target="_blank">00:24:04.740</a></span> | <span class="t">which is more appropriate for like color values or from, you know, an audio signal,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1451" target="_blank">00:24:11.860</a></span> | <span class="t">we're text. We're treating this as just one of the values from a row cab. And we can iteratively apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1459" target="_blank">00:24:19.780</a></span> | <span class="t">noise using these masks. So again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1464" target="_blank">00:24:24.500</a></span> | <span class="t">this is the four process Q, XT condition on XS. And they use like a transition matrix, which I think is from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1479" target="_blank">00:24:39.700</a></span> | <span class="t">Markov, Markov, like a Markov process. But I'm feel free to jump in if someone has a better understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1490" target="_blank">00:24:50.020</a></span> | <span class="t">of that. But each time step, they determine, you know, randomly, which based on a probability from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1498" target="_blank">00:24:58.580</a></span> | <span class="t">from this transition basics, should we mask a token at each time step T.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1508" target="_blank">00:25:08.020</a></span> | <span class="t">And then the reverse process is similar to that, where if we know what the true distribution is of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1515" target="_blank">00:25:15.300</a></span> | <span class="t">the data XO, we know like what the unmasked value should be. And so we're able to sample from like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1524" target="_blank">00:25:24.340</a></span> | <span class="t">the reverse of the distribution to unmask it. But of course, you know, in actual gender modeling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1533" target="_blank">00:25:33.460</a></span> | <span class="t">we don't know what the true distribution of XO is. So in our, you know, our reverse process,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1541" target="_blank">00:25:41.380</a></span> | <span class="t">probability of X given theta, we can said, approximate this using our neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1557" target="_blank">00:25:57.140</a></span> | <span class="t">to determine what the probability should be. So this is, this is what the network learns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1562" target="_blank">00:26:02.500</a></span> | <span class="t">And here's kind of a sample of like what the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1574" target="_blank">00:26:14.100</a></span> | <span class="t">what the model is, this is a figure from the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1576" target="_blank">00:26:16.260</a></span> | <span class="t">And based on the LM1B task to generate new sentences. And then the bottom is the bottom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1586" target="_blank">00:26:26.820</a></span> | <span class="t">portion on the right is reconstruction samples. And so what this is trying to show is like on the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1593" target="_blank">00:26:33.780</a></span> | <span class="t">on the top as T goes up, more stuff is masked. And then on the bottom as T moves forward, more stuff is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1600" target="_blank">00:26:40.340</a></span> | <span class="t">unmasked. And then if we learned a good representation of the, of our data, then what gets reconstructed is, is like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1610" target="_blank">00:26:50.660</a></span> | <span class="t">pretty close to what the data should be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1615" target="_blank">00:26:55.540</a></span> | <span class="t">So we can see, you know, it masks. The original is Caterpillar is eager to expand in Asia. And then the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1623" target="_blank">00:27:03.940</a></span> | <span class="t">the reconstructed version of Caterpillar is eager to expand in China.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1626" target="_blank">00:27:06.820</a></span> | <span class="t">Some of the claims in the paper, this is a big block quote that I'm not going to read,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1638" target="_blank">00:27:18.260</a></span> | <span class="t">but I encourage people to check out because I think it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1640" target="_blank">00:27:20.340</a></span> | <span class="t">pretty, pretty interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1645" target="_blank">00:27:25.140</a></span> | <span class="t">But they, they claim that that BERT viewed through this, this lens that they've, they've established</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1651" target="_blank">00:27:31.220</a></span> | <span class="t">is a one-step diffusion model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1653" target="_blank">00:27:33.540</a></span> | <span class="t">And the auto aggressive models are discrete diffusion models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1659" target="_blank">00:27:39.060</a></span> | <span class="t">And the generative masculine drink models. So the type of model they've proposed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1667" target="_blank">00:27:47.700</a></span> | <span class="t">or, or, or mass language models in general. So like BERT, um, or diffusion models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1673" target="_blank">00:27:53.780</a></span> | <span class="t">So interesting parallel with kind of like previous NLP research, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1679" target="_blank">00:27:59.460</a></span> | <span class="t">and trying to like establish a background for, for their approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1684" target="_blank">00:28:04.420</a></span> | <span class="t">This plays out in some of the, um, the future papers. So I wasn't sure how much depth to go in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1691" target="_blank">00:28:11.780</a></span> | <span class="t">So I cut out a bunch of it stuff. Um, so this one was 2021. Um, there's a ton of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1697" target="_blank">00:28:17.620</a></span> | <span class="t">Sorry Tyler. Oh yeah. I was going to actually ask a question on this. I think this feels actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1703" target="_blank">00:28:23.060</a></span> | <span class="t">kind of important. Um, so I want to talk through my thinking of it. And I think you, you will know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1710" target="_blank">00:28:30.580</a></span> | <span class="t">this a lot more than me. I think BERT is a one-step diffusion model because we add noise, right? We corrupt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1716" target="_blank">00:28:36.660</a></span> | <span class="t">the tokens. That's why it's a one-step diffusion model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1720" target="_blank">00:28:40.980</a></span> | <span class="t">I'm a little bit stuck on how autoregressive models are discrete diffusion models. Could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1725" target="_blank">00:28:45.460</a></span> | <span class="t">you talk a little bit more about that piece? Yeah, totally. Um, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1732" target="_blank">00:28:52.260</a></span> | <span class="t">I think there, um, I'm not as, as firm on this as, as I, as I could be. Um, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1746" target="_blank">00:29:06.980</a></span> | <span class="t">you know way more than me at this point. I don't know. I went really broad and skimmed a ton of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1752" target="_blank">00:29:12.580</a></span> | <span class="t">papers. Um, and I can, I can share a bit of a high level about what they're staying here. So what's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1757" target="_blank">00:29:17.940</a></span> | <span class="t">intuition? Yes, please. Discrete diffusion in this sense is basically them. Uh, so at each step there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1767" target="_blank">00:29:27.220</a></span> | <span class="t">there's a deterministic answer and they're just masking the next token, right? So when you train the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1772" target="_blank">00:29:32.660</a></span> | <span class="t">other aggressive model, what you're basically doing is you're predicting one token at a time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1778" target="_blank">00:29:38.020</a></span> | <span class="t">and in a sense, right, that's, that's just, you have deterministic outputs of what the token should be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1783" target="_blank">00:29:43.940</a></span> | <span class="t">You're just now masking one token at a time. So it's deterministic in the sense of, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1789" target="_blank">00:29:49.940</a></span> | <span class="t">you know exactly what it is. It's equivalent to like a single diffusion step, but instead of stochastic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1795" target="_blank">00:29:55.620</a></span> | <span class="t">diffusion, it's deterministic because you know what the token should be. But yeah, I like read a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1802" target="_blank">00:30:02.420</a></span> | <span class="t">bit more about this and that's, that's kind of all they're saying. They're just saying you're masking token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1806" target="_blank">00:30:06.580</a></span> | <span class="t">by token and that's all it is. Right. So it is discrete as opposed to probabilistic or continuous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1814" target="_blank">00:30:14.980</a></span> | <span class="t">like images because images, image pixels are continuous, right? Right. In this case, it's text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1819" target="_blank">00:30:19.540</a></span> | <span class="t">tokens. That's why it's discrete. Is that it? Images, images are continuous in the sense of you're predicting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1826" target="_blank">00:30:26.180</a></span> | <span class="t">how much noise was applied and what the noise is. In this case, you have exactly one token of measurable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1832" target="_blank">00:30:32.180</a></span> | <span class="t">predictable, discrete change. Right. So that's, that's how they're framing the idea, which, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1838" target="_blank">00:30:38.420</a></span> | <span class="t">kind of in some sense makes sense, right? Like how much noise did I apply over an image? There's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1843" target="_blank">00:30:43.060</a></span> | <span class="t">spectrum of answers, right? And then your loss is measured differently. In this case, it's a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1847" target="_blank">00:30:47.540</a></span> | <span class="t">discrete. Yeah. I also wonder in this case, like maybe the word big, it could be large or huge. I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1854" target="_blank">00:30:54.420</a></span> | <span class="t">all of them, all of them are actually, the word big itself may not be the, you have many synonyms that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1861" target="_blank">00:31:01.380</a></span> | <span class="t">could also be correct answers. So in that sense, framing as discrete loss or discrete diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1867" target="_blank">00:31:07.860</a></span> | <span class="t">Never mind. I don't know enough of this to comment more. Sorry, please go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1872" target="_blank">00:31:12.020</a></span> | <span class="t">So in that case, why are mass language models? Oh, RJ, go. Yeah, sorry. One interesting thing that I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1881" target="_blank">00:31:21.220</a></span> | <span class="t">learned when I did the consistency paper was that the diffusion process is actually the model from the, from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1890" target="_blank">00:31:30.020</a></span> | <span class="t">beginning, given the predictions up until now. Right. So like, it's not, it's not just like given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1897" target="_blank">00:31:37.860</a></span> | <span class="t">the last prediction, what's the next step. It's given, what's the next step, given all the predictions up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1905" target="_blank">00:31:45.700</a></span> | <span class="t">until now. And this is the same for our language model. And so that it, in, from that perspective,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1910" target="_blank">00:31:50.580</a></span> | <span class="t">it looks very much like an auto regressive model. Right. That makes sense. Yeah. And, and, and the other thing that I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1918" target="_blank">00:31:58.180</a></span> | <span class="t">I just like, maybe, maybe there's a statement, the obvious, but BERT is it like with the mass language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1924" target="_blank">00:32:04.660</a></span> | <span class="t">modeling is just sort of like a generalization of masking only the next token. Right. So it's sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1930" target="_blank">00:32:10.900</a></span> | <span class="t">like, you're just, you're just, you just happen to have all masked the next token every time for the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1936" target="_blank">00:32:16.980</a></span> | <span class="t">um, for the auto aggressive model. Whereas with a mass language model, you're doing anywhere in,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1944" target="_blank">00:32:24.580</a></span> | <span class="t">or like a chunk inside of the, and like tokens inside of the, inside of the text, instead of at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1952" target="_blank">00:32:32.740</a></span> | <span class="t">Right. That makes sense. I kind of think like, but it's like, you're masking tokens in the middle,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1959" target="_blank">00:32:39.380</a></span> | <span class="t">like 15% of the time. Yeah. Whereas for auto regressive, you're masking everything ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1963" target="_blank">00:32:43.860</a></span> | <span class="t">Yeah, exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1965" target="_blank">00:32:45.460</a></span> | <span class="t">Um, everything's a mask. Okay. And then they denoise one position at a time, if you think of it that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1970" target="_blank">00:32:50.100</a></span> | <span class="t">But, but, but maybe more. Yeah. So maybe more just like you mask whatever the next token is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1975" target="_blank">00:32:55.540</a></span> | <span class="t">Or, or maybe it, maybe it's better. Maybe what you said is better way to look at it in the sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1981" target="_blank">00:33:01.460</a></span> | <span class="t">it, because it parallels what I said about diffusion models in the, um, like for whatever the stable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1989" target="_blank">00:33:09.220</a></span> | <span class="t">diffusion or whatever. No, that's totally right. Yeah. And then going to get into it in a minute. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=1994" target="_blank">00:33:14.420</a></span> | <span class="t">at least a little bit is that, um, some of the state of the art stuff for language diffusion,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2000" target="_blank">00:33:20.180</a></span> | <span class="t">um, of course they, they lean heavier on big transformer models. Um, and they're able to remove</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2005" target="_blank">00:33:25.780</a></span> | <span class="t">kind of the, the causal, causal mask, um, that's applied for attention. Um, so you think of like the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2012" target="_blank">00:33:32.500</a></span> | <span class="t">you know, the bottom triangle, um, that we usually see for attention, or if you have like group query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2016" target="_blank">00:33:36.980</a></span> | <span class="t">attention, it's like the little squares, um, down your, your triangular matrix, um, for, for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2023" target="_blank">00:33:43.380</a></span> | <span class="t">diffusion language models, they're able to look at, you know, the whole, um, uh, KV, um, for,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2033" target="_blank">00:33:53.700</a></span> | <span class="t">for the attention, um, operator. So, so if I'm jumping ahead here, I'm sorry,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2040" target="_blank">00:34:00.020</a></span> | <span class="t">sorry, please finish, uh, yeah, no, that's it. So if I'm jumping ahead here, let's say I have a code base</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2046" target="_blank">00:34:06.260</a></span> | <span class="t">and I say, I want to refactor this specific object that's being used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2053" target="_blank">00:34:13.060</a></span> | <span class="t">Discrete model, uh, a generative model just fills in the blanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2057" target="_blank">00:34:17.060</a></span> | <span class="t">And that's why you don't actually have to go left to right, like autoregressive models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2061" target="_blank">00:34:21.620</a></span> | <span class="t">which you have to go left to right, even when they're just filling in the blanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2064" target="_blank">00:34:24.340</a></span> | <span class="t">Actually, I don't know if there's, uh, better techniques for filling in the blanks with an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2069" target="_blank">00:34:29.060</a></span> | <span class="t">autoregressive model for code models, but discrete model can just say, okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2072" target="_blank">00:34:32.180</a></span> | <span class="t">someone will just draft out, okay, these are all the definitions I need, all the functions I need,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2076" target="_blank">00:34:36.500</a></span> | <span class="t">some autoregressive model draft that out and then discrete model just fill in the blanks and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2081" target="_blank">00:34:41.060</a></span> | <span class="t">it can do that very fast. I'm, I'm just kind of trying to match it to what we saw at Google I/O</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2086" target="_blank">00:34:46.980</a></span> | <span class="t">when they were demoing this, um, sorry, I mean a diffusion model for coding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2092" target="_blank">00:34:52.660</a></span> | <span class="t">Totally. Yeah. I mean, I think one of the challenges is too, is that the context for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2097" target="_blank">00:34:57.220</a></span> | <span class="t">things that are ahead. Um, so if you're, you know, in that example, and I, I don't know the specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2105" target="_blank">00:35:05.940</a></span> | <span class="t">process that are applied for, for code models and assuming it's, you know, mom with a, with a similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2110" target="_blank">00:35:10.660</a></span> | <span class="t">loss function, um, and that they're just sampling from, um, saying, here's what it is, here's what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2118" target="_blank">00:35:18.580</a></span> | <span class="t">want. And it's able to interpret that there should be a larger output space, um, or the output space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2125" target="_blank">00:35:25.060</a></span> | <span class="t">sequence should be longer. Um, that's true. Expanding that dynamically, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2129" target="_blank">00:35:29.860</a></span> | <span class="t">Yeah, totally. Yeah. But if you're, if you have like, you know, your, your hundred line file,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2135" target="_blank">00:35:35.460</a></span> | <span class="t">if you're doing it auto aggressively, you, you have to feed in the, like, and you're making edit at a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2143" target="_blank">00:35:43.460</a></span> | <span class="t">character 50 or something, you have to feed in the first 50 lines in context and say, we want the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2149" target="_blank">00:35:49.700</a></span> | <span class="t">edit at line 50, and then also feed in the remaining 50 lines, but it, you know, so, and then the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2158" target="_blank">00:35:58.740</a></span> | <span class="t">your auto aggressive model that has to attend to the stuff that's ahead of it, um, with reference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2164" target="_blank">00:36:04.740</a></span> | <span class="t">to the stuff that's behind it versus if we don't have the, the causal mask, it can attend to all of it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2170" target="_blank">00:36:10.100</a></span> | <span class="t">at once, um, or it's able to attend to it at once in a different way. Um, but hopefully I'm not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2177" target="_blank">00:36:17.060</a></span> | <span class="t">Makes sense. Thank you, Tyler. This was helpful for my intuition. Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2180" target="_blank">00:36:20.740</a></span> | <span class="t">For sure. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2182" target="_blank">00:36:22.660</a></span> | <span class="t">cool. So yeah, there's a bunch of papers that happened from 2021. I'm jumping ahead to 2024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2191" target="_blank">00:36:31.220</a></span> | <span class="t">Um, the state of the art kind of like kept ticking up a little bit, but still significantly worse than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2198" target="_blank">00:36:38.340</a></span> | <span class="t">auto aggressive models. Um, even for comparable model sizes, um, there was stuff that like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2206" target="_blank">00:36:46.500</a></span> | <span class="t">back and forth between discrete state spaces and then continuous state spaces where they have like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2211" target="_blank">00:36:51.460</a></span> | <span class="t">a continuous representation and then they have like a separate process. So the continuous representation is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2216" target="_blank">00:36:56.180</a></span> | <span class="t">like an embedding. Um, and then they have a separate process to sample from the embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2220" target="_blank">00:37:00.740</a></span> | <span class="t">to determine what the token should be. Um, which hasn't really panned out in terms of like what the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2229" target="_blank">00:37:09.700</a></span> | <span class="t">what is now state of the art. So that's part of the reason I chose to skip over it. Um, but, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2235" target="_blank">00:37:15.140</a></span> | <span class="t">encourage folks that are interested to look into it. There's a bunch of papers that came out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2240" target="_blank">00:37:20.180</a></span> | <span class="t">um, kind of in this time span that I'm glossing over. Um, one of the next ones that I thought was,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2250" target="_blank">00:37:30.740</a></span> | <span class="t">was pretty cool. Um, and I think this is kind of looking backwards. It's sort of what this year,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2255" target="_blank">00:37:35.380</a></span> | <span class="t">the art is and what's referenced from that. So looking at, um, um, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2261" target="_blank">00:37:41.380</a></span> | <span class="t">the study in October of last year, um, where they started scaling up, um, mass diffusion models. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2274" target="_blank">00:37:54.100</a></span> | <span class="t">and they were able to achieve results, um, that are competitive with, um, auto aggressive model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2281" target="_blank">00:38:01.780</a></span> | <span class="t">um, language models, um, of similar sizes, um, and with relatively similar, um, levels of, of training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2291" target="_blank">00:38:11.220</a></span> | <span class="t">compute. Um, so they trained up to a 1.1 billion parameter model. Um, and then for, depending on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2300" target="_blank">00:38:20.980</a></span> | <span class="t">different benchmark was, was competitive with GPT two, um, the 1.5 B version and then llama two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2307" target="_blank">00:38:27.460</a></span> | <span class="t">um, the seven B version. Um, so those weren't quite state of the art when this was released. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2312" target="_blank">00:38:32.820</a></span> | <span class="t">but still kind of a, um, a step forward. Um, and then it's, I don't know, I kind of like this scaling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2319" target="_blank">00:38:39.460</a></span> | <span class="t">um, scaling work just to see like the number goes down as compute goes up. Um, there was pretty graphs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2329" target="_blank">00:38:49.620</a></span> | <span class="t">Um, so, you know, the classic Kaplan, um, chinchilla isoflop curves, um, based on our,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2337" target="_blank">00:38:57.700</a></span> | <span class="t">our training budget, we make stuff go down. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2343" target="_blank">00:39:03.140</a></span> | <span class="t">and here they have, like, they're saying they followed a similar, um, scaling law to auto aggressive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2354" target="_blank">00:39:14.740</a></span> | <span class="t">models with their mass diffusion models. Um, but with some constant multiplier. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2360" target="_blank">00:39:20.900</a></span> | <span class="t">but the, the general curve, you know, on our, our log log plot was, was pretty similar for, for their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2368" target="_blank">00:39:28.580</a></span> | <span class="t">approach. Um, digging into this a little bit more, um, the, to achieve a similar validation loss, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2380" target="_blank">00:39:40.900</a></span> | <span class="t">they had to have 16 X, um, more compute, um, than the auto aggressive model and on their,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2388" target="_blank">00:39:48.020</a></span> | <span class="t">their approach that they model here. So that's the, the left plot. And then on the right plot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2391" target="_blank">00:39:51.780</a></span> | <span class="t">um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2393" target="_blank">00:39:53.300</a></span> | <span class="t">they were able to achieve similar, um, what is this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2403" target="_blank">00:40:03.460</a></span> | <span class="t">I think better performance with fewer parameters is the right plot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2406" target="_blank">00:40:06.900</a></span> | <span class="t">And then, yeah, here's their, the results, um, which are competitive with the, the models we listed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2418" target="_blank">00:40:18.020</a></span> | <span class="t">Um, this one we, we talked about last, last week. Um, so I'm not going to dig into it too much. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2424" target="_blank">00:40:24.420</a></span> | <span class="t">so I just want to hit some things that I thought were cool that we didn't quite touch on. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2428" target="_blank">00:40:28.580</a></span> | <span class="t">so this is the same, uh, most of the same authors as the scaling loss paper, um, where they continue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2436" target="_blank">00:40:36.820</a></span> | <span class="t">the trend and scale up to, um, 7 billion parameters. Um, and then the results got even better. Um, it's now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2445" target="_blank">00:40:45.460</a></span> | <span class="t">as good or better than llama two, seven B or llama three, eight B, um, and handful of different benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2450" target="_blank">00:40:50.420</a></span> | <span class="t">Um, one of the things, you know, we were just talking about is that the bi-directional reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2456" target="_blank">00:40:56.900</a></span> | <span class="t">Um, so one of the tasks they, um, they looked at was, um, reversing a poem. Um, so if you have like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2470" target="_blank">00:41:10.740</a></span> | <span class="t">the last couple of lines in a poem, can you predict the lines that came before it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2474" target="_blank">00:41:14.820</a></span> | <span class="t">Um, and for this specific task, um, this model significantly outperformed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2481" target="_blank">00:41:21.140</a></span> | <span class="t">most of the other models, they, they tested against including, um, 4.0. Um, and they, they attribute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2488" target="_blank">00:41:28.340</a></span> | <span class="t">that to kind of the, um, the lack of the casual mask, um, for tension that, you know, just talking about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2494" target="_blank">00:41:34.260</a></span> | <span class="t">Um, so again, a figure from the paper where they're, they're masking stuff. Um, they also introduced this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2502" target="_blank">00:41:42.180</a></span> | <span class="t">step where, um, they're able to score the probability of a token, um, that's predicted by the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2509" target="_blank">00:41:49.620</a></span> | <span class="t">the mass predictor. Um, and things that are low probability, they can then potentially remask and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2514" target="_blank">00:41:54.820</a></span> | <span class="t">then try to resample again to get a better prediction. Um, so that's, um, the right or part C of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2522" target="_blank">00:42:02.500</a></span> | <span class="t">figure, um, where they're remasking, um, the figure and then re-predicting it again, which is kind of neat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2527" target="_blank">00:42:07.940</a></span> | <span class="t">Um, and then more scaling loss stuff, um, for different tasks, um, their model was able to, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2540" target="_blank">00:42:20.020</a></span> | <span class="t">achieve better performance at, at lower, um, training compute. Um, and sometimes it's worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2546" target="_blank">00:42:26.820</a></span> | <span class="t">So for the middle bottom plot GSM, GSM 8K, um, which I think is a math focused task, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2554" target="_blank">00:42:34.740</a></span> | <span class="t">they outperformed, um, their autoregressive baseline and then to the plot to the left of it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2559" target="_blank">00:42:39.380</a></span> | <span class="t">the bottom left plot there, um, we can see all the orange stars are sort of below, um, the, the blue dots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2568" target="_blank">00:42:48.420</a></span> | <span class="t">Um, and so lower is a worse accuracy and this is your zero shot task, um, for the same level of compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2575" target="_blank">00:42:55.380</a></span> | <span class="t">Um, so definitely some trade-offs there. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2578" target="_blank">00:42:58.260</a></span> | <span class="t">This next one, uh, block diffusion. Um, so this paper, this is 20, um, January, this one came out in, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2588" target="_blank">00:43:08.660</a></span> | <span class="t">March of this year. Um, this uses, um, a hybrid architecture where it, um, has, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2598" target="_blank">00:43:18.340</a></span> | <span class="t">has blocks, um, that in each block is generated auto aggressively, but with any in the block,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2604" target="_blank">00:43:24.020</a></span> | <span class="t">it's, um, generated using diffusion. Um, part of the reason they did that is to, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2610" target="_blank">00:43:30.740</a></span> | <span class="t">I think part of it was to make their, um, the sampling task easier. Um, but they were also able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2619" target="_blank">00:43:39.220</a></span> | <span class="t">to take advantage of KB caching, um, versus this, this paper, um, despite the results, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2628" target="_blank">00:43:48.420</a></span> | <span class="t">doesn't, hasn't been optimized to fulfill a lot of the tricks, like KB caching. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2633" target="_blank">00:43:53.540</a></span> | <span class="t">I didn't dig into the specifics of why KB caching doesn't work for this previous paper. Um, kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2640" target="_blank">00:44:00.660</a></span> | <span class="t">thinking about it, um, my intuition is that, um, if you have like a, a dialogue, um, you know, between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2649" target="_blank">00:44:09.860</a></span> | <span class="t">the system and a user, um, that it's, it has to generate like the next response starting from zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2656" target="_blank">00:44:16.500</a></span> | <span class="t">um, as opposed to finding a way to, to cache like the previous, um, tokens like you can, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2663" target="_blank">00:44:23.300</a></span> | <span class="t">if you're generating auto aggressively, um, and they're able to, so block diffusion was able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2668" target="_blank">00:44:28.180</a></span> | <span class="t">get around that by having, um, like these, these chunks of a fixed length that they then generate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2674" target="_blank">00:44:34.100</a></span> | <span class="t">So we, again, we talked about this last week that, um, the large language diffusion paper, um, applied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2686" target="_blank">00:44:46.100</a></span> | <span class="t">pretty similar pre-training. Um, I think they use like a 2.3 trillion tokens. Um, and we saw 10 to the 23rd,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2693" target="_blank">00:44:53.860</a></span> | <span class="t">um, flops on H 100, um, but they didn't do any post training. Um, and then some of the, the new work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2705" target="_blank">00:45:05.380</a></span> | <span class="t">that's coming out is improving kind of the, the post training, um, uh, especially with our reasoning slant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2711" target="_blank">00:45:11.860</a></span> | <span class="t">Um, so this paper, um, so this paper, uh, uses this, this is a base model, the, the LADA that they introduce,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2720" target="_blank">00:45:20.660</a></span> | <span class="t">um, and then applies, um, this, their custom GRPO, um, post training. Um, and then they use like the S1, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2734" target="_blank">00:45:34.580</a></span> | <span class="t">reasoning dataset to do supervised foreign tuning, um, with a slant towards, um, like math and reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2742" target="_blank">00:45:42.020</a></span> | <span class="t">and code tasks. Um, and based on that, they're able to, um, you know, dramatically improve the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2747" target="_blank">00:45:47.620</a></span> | <span class="t">performance on those specific tasks over the base model. Um, so that's the, the bottom table here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2757" target="_blank">00:45:57.940</a></span> | <span class="t">they're in green. Um, the top row is the, the base model. Um, so they, they bumped up the numbers a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2765" target="_blank">00:46:05.380</a></span> | <span class="t">good amount. Um, let's see, and that's, that's all. So thanks all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2776" target="_blank">00:46:16.020</a></span> | <span class="t">This is really good. Um, Tyler, I, there's a question that Eric posed in the channel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2787" target="_blank">00:46:27.540</a></span> | <span class="t">that I'm also curious about. That's a compute for the output window scale as O log. Is it N squared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2795" target="_blank">00:46:35.140</a></span> | <span class="t">for diffusion models or is it linear? Oh, would you know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2799" target="_blank">00:46:39.460</a></span> | <span class="t">I don't know offhand. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2805" target="_blank">00:46:45.940</a></span> | <span class="t">Some of the big ones that we hear about like stable diffusion, dolly image gen, uh, diffusion transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2817" target="_blank">00:46:57.220</a></span> | <span class="t">video diffusion transformers. Those are all transformer based. So there is, um, there is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2823" target="_blank">00:47:03.780</a></span> | <span class="t">that, um, quadratic scaling issue, but some of them are not. So some basic denoising</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2830" target="_blank">00:47:10.740</a></span> | <span class="t">diffusion probabilistic models, they're more CNN based. And then there's no longer that transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2836" target="_blank">00:47:16.820</a></span> | <span class="t">complexity issue. So depending on what you're doing, like some early work trying to do diffusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2843" target="_blank">00:47:23.460</a></span> | <span class="t">for completion. So like short completion for cogen stuff is not transformer based. So you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2850" target="_blank">00:47:30.100</a></span> | <span class="t">no longer complexity bound. But I found it weird. Cause like, you know, that's completion. It's like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2855" target="_blank">00:47:35.940</a></span> | <span class="t">not long, long context. So I don't know. It's just what they did though. But, um, some of the latent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2862" target="_blank">00:47:42.980</a></span> | <span class="t">diffusion models, like from stability, those I believe are also not transformer based. So they're,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2870" target="_blank">00:47:50.260</a></span> | <span class="t">you know, that's more popular. You've probably heard of some latent diffusion stuff. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2874" target="_blank">00:47:54.740</a></span> | <span class="t">they don't use, um, they don't use transformers for the diffusion itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2881" target="_blank">00:48:01.380</a></span> | <span class="t">I think they just use it for the, um, text encoding. So, you know, it, it kind of depends on where you see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2889" target="_blank">00:48:09.700</a></span> | <span class="t">the quadratic scaling complexity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2900" target="_blank">00:48:20.660</a></span> | <span class="t">that's cool. Um, that's cool. Yeah, I guess we have a few follow open questions now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2905" target="_blank">00:48:25.620</a></span> | <span class="t">Yeah. I know RJ had a question. I don't know if you want to come on camera and just ask it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2910" target="_blank">00:48:30.500</a></span> | <span class="t">Oh, no, I was, I was come, I was commenting that I think that most, uh, or stable diffusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2921" target="_blank">00:48:41.300</a></span> | <span class="t">anyway, has a, has attention blocks in inside of the diffusion block. So I think it, but I don't know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2928" target="_blank">00:48:48.580</a></span> | <span class="t">that would be, uh, so that would be across all the, um, sort of latent space tokens. So that's a fixed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2939" target="_blank">00:48:59.620</a></span> | <span class="t">size and wouldn't be impacted. So I don't really, I'm not sure that this, that impacts like a, a text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2945" target="_blank">00:49:05.380</a></span> | <span class="t">diffusion model, except because you would, yeah. Um, if you look at like BERT being a diffusion model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2953" target="_blank">00:49:13.220</a></span> | <span class="t">right, then it is obviously, it has a transformer block. And so therefore would, uh, so I think that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2958" target="_blank">00:49:18.500</a></span> | <span class="t">the question about that is it's a little bit orthogonal, right? Because I, I view diffusion as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2963" target="_blank">00:49:23.780</a></span> | <span class="t">like a alternative process to, uh, autoregressive modeling kind of, and not so much transformer versus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2974" target="_blank">00:49:34.900</a></span> | <span class="t">non-transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2975" target="_blank">00:49:35.700</a></span> | <span class="t">Yeah, there was, the people have incorporated transformers into diffusion models in a handful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2987" target="_blank">00:49:47.300</a></span> | <span class="t">of different ways. Um, so let me see if we can pull this up. Um, uh, I think that's my old desktop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=2995" target="_blank">00:49:55.860</a></span> | <span class="t">Let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3001" target="_blank">00:50:01.780</a></span> | <span class="t">Well, so this is the, the unit image that I pulled from one of the slides. Um, this is from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3007" target="_blank">00:50:07.380</a></span> | <span class="t">Prince's understanding deep learning, which is, which is pretty solid. Um, so starting with DDPM,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3013" target="_blank">00:50:13.780</a></span> | <span class="t">this is sort of the, the base model architecture that they use to predict, um, the noise at each time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3019" target="_blank">00:50:19.780</a></span> | <span class="t">step. Um, so they have, um, you know, the previous time step and then it's fed into this. And then the output is the, um, the image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3031" target="_blank">00:50:31.540</a></span> | <span class="t">minus the noise. Um, or maybe it's just, sorry, I think it's actually just the noise itself, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3037" target="_blank">00:50:37.140</a></span> | <span class="t">as a difference from what the image of you, but regardless, um, they've got like a bunch of, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3044" target="_blank">00:50:44.340</a></span> | <span class="t">convolutional blocks, um, scaling it down and then back up. Um, and they, they don't make it very clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3051" target="_blank">00:50:51.460</a></span> | <span class="t">Um, this color's not very good, but even within the original DDPM model from 2020, um, there's attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3058" target="_blank">00:50:58.980</a></span> | <span class="t">operators kind of at the 16 by 16, um, chunks here. So it's able to attend to what came in. Um, and then as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3069" target="_blank">00:51:09.300</a></span> | <span class="t">models got bigger, um, and more complicated, um, people started tossing in more complicated model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3075" target="_blank">00:51:15.780</a></span> | <span class="t">architectures and a lot of more attention at different parts of this. Um, a lot of them still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3080" target="_blank">00:51:20.020</a></span> | <span class="t">kind of retained this unit shape, um, but stuff kind of got fancier from there. So that's one approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3086" target="_blank">00:51:26.900</a></span> | <span class="t">And then I don't know if I can find the right. And then, can I just add to that? Yeah, please.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3091" target="_blank">00:51:31.460</a></span> | <span class="t">Yeah. So, so as Vibu was saying, uh, the attention is primarily used in the, some of these earlier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3097" target="_blank">00:51:37.540</a></span> | <span class="t">image diffusion models to, uh, to the time emitting space, as well as the prompt that would guide the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3107" target="_blank">00:51:47.060</a></span> | <span class="t">image generation. Uh, sometimes these prompts are text images or text prompts, but sometimes these are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3114" target="_blank">00:51:54.180</a></span> | <span class="t">image prompts like depth maps or contours or outlines of different things. So the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3121" target="_blank">00:52:01.060</a></span> | <span class="t">attention is basically used as a grounding mechanism, uh, to flow forward, but the actual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3126" target="_blank">00:52:06.980</a></span> | <span class="t">process itself is diffusion. So we give, there is a separation that we can make. The attention is used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3132" target="_blank">00:52:12.980</a></span> | <span class="t">for helping the model understand the semantics of the image as well as for the generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3139" target="_blank">00:52:19.700</a></span> | <span class="t">But the actual diffusion process itself is orthogonal to that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3143" target="_blank">00:52:23.380</a></span> | <span class="t">Yeah. The term for that is conditioning. So they call it conditioning sometimes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3149" target="_blank">00:52:29.300</a></span> | <span class="t">Yeah. And there's even class conditioning. So you can have like class based labels and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3154" target="_blank">00:52:34.660</a></span> | <span class="t">guide, um, generation towards that. So like stylistic labels, right? I want anime and that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3160" target="_blank">00:52:40.500</a></span> | <span class="t">separate than tax, text conditioning of like ultra realistic. And you know, you're basically using text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3167" target="_blank">00:52:47.220</a></span> | <span class="t">embeddings, but the, the attention there is it's typically done, um, with something like cross</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3174" target="_blank">00:52:54.180</a></span> | <span class="t">attention over a text embedding dimension. And that's, that's separate than diffusion scaling, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3181" target="_blank">00:53:01.460</a></span> | <span class="t">you know, a different complexity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3184" target="_blank">00:53:04.100</a></span> | <span class="t">Correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3185" target="_blank">00:53:05.860</a></span> | <span class="t">And then things get much more complex. They have something called as control nets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3189" target="_blank">00:53:09.460</a></span> | <span class="t">which is slightly different concept than conditioning. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3193" target="_blank">00:53:13.860</a></span> | <span class="t">It's interesting where like, you know, we basically had our like GPT three moment where we used to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3202" target="_blank">00:53:22.820</a></span> | <span class="t">like all these temporal nets to fix consistency when you scale stuff up. And then with Sora, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3208" target="_blank">00:53:28.340</a></span> | <span class="t">it turns out that video generation is just scaled up diffusion and you just scale it up a lot and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3213" target="_blank">00:53:33.700</a></span> | <span class="t">you solve a lot of these little like nuances. And it just kind of works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3218" target="_blank">00:53:38.260</a></span> | <span class="t">So I guess I don't want to be argumentative, but, uh, I think that, um, unless I'm misinterpreting this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3227" target="_blank">00:53:47.460</a></span> | <span class="t">image, uh, that I put in the chat, I'm pretty sure it's saying that there is actually, um, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3236" target="_blank">00:53:56.900</a></span> | <span class="t">attention blocks in the backbone of the diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3240" target="_blank">00:54:00.180</a></span> | <span class="t">Um, not, not just in the conditioning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3244" target="_blank">00:54:04.260</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3248" target="_blank">00:54:08.180</a></span> | <span class="t">So I don't know if I can pull it up and, or someone can share it if you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3251" target="_blank">00:54:11.540</a></span> | <span class="t">Yeah. I mean, it's been a while since the last week did it, but, uh, that conditioning is primarily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3256" target="_blank">00:54:16.980</a></span> | <span class="t">for, uh, that attention is primarily for the conditioning, whether it's text or, or image prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3265" target="_blank">00:54:25.540</a></span> | <span class="t">Okay. Uh, okay. Uh, well, okay. We, I guess we can argue about it offline. I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3276" target="_blank">00:54:36.820</a></span> | <span class="t">I think that, uh, it's far as I can tell it as, as, uh, Tyler was noting, I think it's actually part of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3284" target="_blank">00:54:44.500</a></span> | <span class="t">the unit backbone or whatever, uh, the backbone is made of. And there's like in the stable diffusion,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3290" target="_blank">00:54:50.500</a></span> | <span class="t">the fusion is unit, right? Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3292" target="_blank">00:54:52.500</a></span> | <span class="t">Anyway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3294" target="_blank">00:54:54.820</a></span> | <span class="t">I'll always help pull for more background information. Um, thanks again, Tyler. Really, really fun one this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3304" target="_blank">00:55:04.740</a></span> | <span class="t">time. Um, I think Cirque said to take a call. So just wrapping things up here next week, we have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3312" target="_blank">00:55:12.820</a></span> | <span class="t">AI engineer world's fair. So if any of you guys are around, you know, we'll share something on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3317" target="_blank">00:55:17.940</a></span> | <span class="t">discord. We'll, we'll do like a little meetup. I think we have time for like an in-person paper club</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3323" target="_blank">00:55:23.140</a></span> | <span class="t">workshop thing. Uh, we're supposed to be announcing our test of time paper club V2. So we'll share it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3330" target="_blank">00:55:30.500</a></span> | <span class="t">remote too, but, um, TBD on what the paper is, but we'll have some sort of session in person and also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3339" target="_blank">00:55:39.140</a></span> | <span class="t">remote next week. So if you're around at the conference, come by, otherwise, you know, same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3344" target="_blank">00:55:44.020</a></span> | <span class="t">zoom thing and then it'll just be a different one. But yeah, thanks everyone for coming. Thanks, Tyler,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3349" target="_blank">00:55:49.300</a></span> | <span class="t">for sharing. Thank you. Uh, Tyler, someone asked about slides. Oh yeah. I'll post those in the discord.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3359" target="_blank">00:55:59.300</a></span> | <span class="t">Yeah. Thanks everyone. Perfect. Yeah. Thanks. Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3367" target="_blank">00:56:07.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3369" target="_blank">00:56:09.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3371" target="_blank">00:56:11.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3373" target="_blank">00:56:13.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3375" target="_blank">00:56:15.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3377" target="_blank">00:56:17.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3379" target="_blank">00:56:19.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3381" target="_blank">00:56:21.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3383" target="_blank">00:56:23.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3385" target="_blank">00:56:25.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3387" target="_blank">00:56:27.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3389" target="_blank">00:56:29.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3391" target="_blank">00:56:31.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3393" target="_blank">00:56:33.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3395" target="_blank">00:56:35.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3397" target="_blank">00:56:37.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3399" target="_blank">00:56:39.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3401" target="_blank">00:56:41.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3403" target="_blank">00:56:43.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3405" target="_blank">00:56:45.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3407" target="_blank">00:56:47.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3409" target="_blank">00:56:49.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3411" target="_blank">00:56:51.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3413" target="_blank">00:56:53.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3415" target="_blank">00:56:55.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3417" target="_blank">00:56:57.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3417" target="_blank">00:56:57.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3419" target="_blank">00:56:59.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3419" target="_blank">00:56:59.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3421" target="_blank">00:57:01.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3421" target="_blank">00:57:01.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3423" target="_blank">00:57:03.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3423" target="_blank">00:57:03.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3425" target="_blank">00:57:05.060</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=tRf0H-P2Jfk&t=3427" target="_blank">00:57:07.060</a></span> | <span class="t">Thank you.</span></div></div></body></html>