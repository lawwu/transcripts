<html><head><title>Stanford CS224N NLP with Deep Learning | 2023 | Lecture 8 - Self-Attention and Transformers</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS224N NLP with Deep Learning | 2023 | Lecture 8 - Self-Attention and Transformers</h2><a href="https://www.youtube.com/watch?v=LWMzyfvuehA"><img src="https://i.ytimg.com/vi/LWMzyfvuehA/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./LWMzyfvuehA.html">Whisper Transcript</a> | <a href="./transcript_LWMzyfvuehA.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=7" target="_blank">00:00:07.280</a></span> | <span class="t">Welcome to CS224N.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=9" target="_blank">00:00:09.280</a></span> | <span class="t">We're about two minutes in, so let's get started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=12" target="_blank">00:00:12.880</a></span> | <span class="t">So today, we've got what I think is quite an exciting lecture topic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=17" target="_blank">00:00:17.240</a></span> | <span class="t">We're going to talk about self-attention and transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=21" target="_blank">00:00:21.760</a></span> | <span class="t">So these are some ideas that are sort of the foundation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=25" target="_blank">00:00:25.320</a></span> | <span class="t">of most of the modern advances in natural language processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=29" target="_blank">00:00:29.400</a></span> | <span class="t">And actually, AI systems in a broad range of fields.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=34" target="_blank">00:00:34.640</a></span> | <span class="t">So it's a very, very fun topic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=37" target="_blank">00:00:37.600</a></span> | <span class="t">Before we get into that--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=39" target="_blank">00:00:39.480</a></span> | <span class="t">OK, before we get into that, we're going to have a couple of reminders.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=49" target="_blank">00:00:49.240</a></span> | <span class="t">So there are brand new lecture notes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=51" target="_blank">00:00:51.120</a></span> | <span class="t">Woo!</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=51" target="_blank">00:00:51.620</a></span> | <span class="t">[CHEERING]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=53" target="_blank">00:00:53.560</a></span> | <span class="t">Nice, thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=54" target="_blank">00:00:54.640</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=57" target="_blank">00:00:57.140</a></span> | <span class="t">I'm very excited about them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=59" target="_blank">00:00:59.280</a></span> | <span class="t">They go into-- they pretty much follow along</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=62" target="_blank">00:01:02.460</a></span> | <span class="t">with what I'll be talking about today, but go into considerably more detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=67" target="_blank">00:01:07.700</a></span> | <span class="t">Assignment four is due a week from today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=71" target="_blank">00:01:11.780</a></span> | <span class="t">Yeah, so the issues with Azure continue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=75" target="_blank">00:01:15.380</a></span> | <span class="t">Thankfully-- woo!</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=76" target="_blank">00:01:16.540</a></span> | <span class="t">Thankfully, our TAs especially has tested that this works on Colab,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=85" target="_blank">00:01:25.100</a></span> | <span class="t">and the amount of training is such that a Colab session will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=89" target="_blank">00:01:29.040</a></span> | <span class="t">allow you to train your machine translation system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=93" target="_blank">00:01:33.340</a></span> | <span class="t">So if you don't have a GPU, use Colab.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=95" target="_blank">00:01:35.180</a></span> | <span class="t">We're continuing to work on getting access</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=97" target="_blank">00:01:37.100</a></span> | <span class="t">to more GPUs for assignment five in the final project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=101" target="_blank">00:01:41.900</a></span> | <span class="t">We'll continue to update you as we're able to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=104" target="_blank">00:01:44.660</a></span> | <span class="t">But the usual systems this year are no longer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=109" target="_blank">00:01:49.100</a></span> | <span class="t">holding because companies are changing their minds about things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=112" target="_blank">00:01:52.140</a></span> | <span class="t">OK, so our final project proposal, you have a proposal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=117" target="_blank">00:01:57.540</a></span> | <span class="t">of what you want to work on for your final project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=120" target="_blank">00:02:00.460</a></span> | <span class="t">We will give you feedback on whether we think it's a feasible idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=124" target="_blank">00:02:04.400</a></span> | <span class="t">or how to change it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=125" target="_blank">00:02:05.260</a></span> | <span class="t">So this is very important because we want you to work on something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=127" target="_blank">00:02:07.980</a></span> | <span class="t">that we think has a good chance of success for the rest of the quarter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=131" target="_blank">00:02:11.160</a></span> | <span class="t">That's going to be out tonight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=132" target="_blank">00:02:12.460</a></span> | <span class="t">We'll have an ad announcement when it is out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=135" target="_blank">00:02:15.660</a></span> | <span class="t">And we want to get you feedback on that pretty quickly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=139" target="_blank">00:02:19.020</a></span> | <span class="t">because you'll be working on this after assignment five is done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=142" target="_blank">00:02:22.140</a></span> | <span class="t">Really, the major core component of the course after that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=146" target="_blank">00:02:26.340</a></span> | <span class="t">is the final project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=149" target="_blank">00:02:29.380</a></span> | <span class="t">OK, any questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=152" target="_blank">00:02:32.860</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=153" target="_blank">00:02:33.360</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=153" target="_blank">00:02:33.860</a></span> | <span class="t">So let's take a look back into what we've done so far in this course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=161" target="_blank">00:02:41.900</a></span> | <span class="t">and see what we were doing in natural language processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=167" target="_blank">00:02:47.120</a></span> | <span class="t">What was our strategy?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=168" target="_blank">00:02:48.080</a></span> | <span class="t">If you had a natural language processing problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=170" target="_blank">00:02:50.080</a></span> | <span class="t">and you wanted to take your best effort attempt at it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=173" target="_blank">00:02:53.060</a></span> | <span class="t">without doing anything too fancy, you would have said, OK,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=176" target="_blank">00:02:56.040</a></span> | <span class="t">I'm going to have a bidirectional LSTM instead of a simple RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=181" target="_blank">00:03:01.180</a></span> | <span class="t">I'm going to use an LSTM to encode my sentences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=184" target="_blank">00:03:04.220</a></span> | <span class="t">I get bidirectional context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=186" target="_blank">00:03:06.140</a></span> | <span class="t">And if I have an output that I'm trying to generate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=189" target="_blank">00:03:09.300</a></span> | <span class="t">I'll have a unidirectional LSTM that I was going to generate one by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=194" target="_blank">00:03:14.100</a></span> | <span class="t">So you have a translation or a parse or whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=197" target="_blank">00:03:17.140</a></span> | <span class="t">And so maybe I've encoded in a bidirectional LSTM the source sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=200" target="_blank">00:03:20.500</a></span> | <span class="t">and I'm sort of one by one decoding out the target</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=204" target="_blank">00:03:24.260</a></span> | <span class="t">with my unidirectional LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=206" target="_blank">00:03:26.480</a></span> | <span class="t">And then also, I was going to use something like attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=210" target="_blank">00:03:30.680</a></span> | <span class="t">to give flexible access to memory if I felt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=214" target="_blank">00:03:34.800</a></span> | <span class="t">like I needed to do this sort of look back and see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=217" target="_blank">00:03:37.080</a></span> | <span class="t">where I want to translate from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=219" target="_blank">00:03:39.040</a></span> | <span class="t">And this was just working exceptionally well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=221" target="_blank">00:03:41.960</a></span> | <span class="t">And we motivated attention through wanting to do machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=226" target="_blank">00:03:46.200</a></span> | <span class="t">And you have this bottleneck where you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=228" target="_blank">00:03:48.140</a></span> | <span class="t">want to have to encode the whole source sentence in a single vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=232" target="_blank">00:03:52.840</a></span> | <span class="t">And in this lecture, we have the same goal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=235" target="_blank">00:03:55.000</a></span> | <span class="t">So we're going to be looking at a lot of the same problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=237" target="_blank">00:03:57.380</a></span> | <span class="t">that we did previously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=238" target="_blank">00:03:58.520</a></span> | <span class="t">But we're going to use different building blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=240" target="_blank">00:04:00.560</a></span> | <span class="t">We're going to say, if 2014 to 2017-ish I was using recurrence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=247" target="_blank">00:04:07.480</a></span> | <span class="t">through lots of trial and error, years later,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=250" target="_blank">00:04:10.400</a></span> | <span class="t">it had these brand new building blocks that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=252" target="_blank">00:04:12.720</a></span> | <span class="t">can plug in, direct replacement for LSTMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=257" target="_blank">00:04:17.160</a></span> | <span class="t">And they're going to allow for just a huge range of much more successful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=262" target="_blank">00:04:22.120</a></span> | <span class="t">applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=263" target="_blank">00:04:23.320</a></span> | <span class="t">And so what are the issues with the recurrent neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=268" target="_blank">00:04:28.680</a></span> | <span class="t">we used to use?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=269" target="_blank">00:04:29.680</a></span> | <span class="t">And what are the new systems that we're going to use from this point moving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=272" target="_blank">00:04:32.720</a></span> | <span class="t">forward?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=275" target="_blank">00:04:35.160</a></span> | <span class="t">So one of the issues with a recurrent neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=278" target="_blank">00:04:38.880</a></span> | <span class="t">is what we're going to call linear interaction distance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=281" target="_blank">00:04:41.680</a></span> | <span class="t">So as we know, RNNs are unrolled left to right or right to left,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=287" target="_blank">00:04:47.200</a></span> | <span class="t">depending on the language and the direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=289" target="_blank">00:04:49.840</a></span> | <span class="t">But it encodes the notion of linear locality, which is useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=293" target="_blank">00:04:53.080</a></span> | <span class="t">Because if two words occur right next to each other,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=295" target="_blank">00:04:55.600</a></span> | <span class="t">sometimes they're actually quite related.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=297" target="_blank">00:04:57.320</a></span> | <span class="t">So tasty pizza.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=298" target="_blank">00:04:58.640</a></span> | <span class="t">They're nearby.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=299" target="_blank">00:04:59.680</a></span> | <span class="t">And in the recurrent neural network, you encode tasty.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=304" target="_blank">00:05:04.360</a></span> | <span class="t">And then you walk one step, and you encode pizza.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=308" target="_blank">00:05:08.720</a></span> | <span class="t">So nearby words do often affect each other's meanings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=312" target="_blank">00:05:12.600</a></span> | <span class="t">But you have this problem where very long distance dependencies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=317" target="_blank">00:05:17.200</a></span> | <span class="t">can take a very long time to interact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=318" target="_blank">00:05:18.960</a></span> | <span class="t">So if I have the sentence, the chef--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=321" target="_blank">00:05:21.400</a></span> | <span class="t">so those are nearby.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=322" target="_blank">00:05:22.600</a></span> | <span class="t">Those interact with each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=325" target="_blank">00:05:25.120</a></span> | <span class="t">And then who, and then a bunch of stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=328" target="_blank">00:05:28.680</a></span> | <span class="t">Like the chef who went to the stores and picked up the ingredients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=332" target="_blank">00:05:32.520</a></span> | <span class="t">and loves garlic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=335" target="_blank">00:05:35.320</a></span> | <span class="t">And then was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=337" target="_blank">00:05:37.160</a></span> | <span class="t">Like I actually have an RNN step, this sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=340" target="_blank">00:05:40.440</a></span> | <span class="t">of application of the recurrent weight matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=343" target="_blank">00:05:43.000</a></span> | <span class="t">and some element-wise nonlinearities once, twice, three times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=347" target="_blank">00:05:47.320</a></span> | <span class="t">As many times as there is potentially the length of the sequence between chef</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=352" target="_blank">00:05:52.520</a></span> | <span class="t">and was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=353" target="_blank">00:05:53.760</a></span> | <span class="t">And it's the chef who was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=354" target="_blank">00:05:54.960</a></span> | <span class="t">So this is a long distance dependency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=356" target="_blank">00:05:56.840</a></span> | <span class="t">Should feel kind of related to the stuff that we did in dependency syntax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=361" target="_blank">00:06:01.120</a></span> | <span class="t">But it's quite difficult to learn potentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=366" target="_blank">00:06:06.440</a></span> | <span class="t">that these words should be related.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=369" target="_blank">00:06:09.080</a></span> | <span class="t">So if you have a lot of steps between words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=379" target="_blank">00:06:19.440</a></span> | <span class="t">it can be difficult to learn the dependencies between them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=382" target="_blank">00:06:22.400</a></span> | <span class="t">We talked about all these gradient problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=384" target="_blank">00:06:24.240</a></span> | <span class="t">LSTMs do a lot better at modeling the gradients across long distances</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=389" target="_blank">00:06:29.680</a></span> | <span class="t">than simple recurrent neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=391" target="_blank">00:06:31.280</a></span> | <span class="t">But it's not perfect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=393" target="_blank">00:06:33.960</a></span> | <span class="t">And we already know that this linear order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=396" target="_blank">00:06:36.680</a></span> | <span class="t">isn't sort of the right way to think about sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=400" target="_blank">00:06:40.520</a></span> | <span class="t">So if I wanted to learn that it's the chef who was,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=406" target="_blank">00:06:46.920</a></span> | <span class="t">then I might have a hard time doing it because the gradients have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=411" target="_blank">00:06:51.720</a></span> | <span class="t">to propagate from was to chef.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=413" target="_blank">00:06:53.160</a></span> | <span class="t">And really, I'd like more direct connection</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=416" target="_blank">00:06:56.440</a></span> | <span class="t">between words that might be related in the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=419" target="_blank">00:06:59.080</a></span> | <span class="t">Or in a document even, if these are going to get much longer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=424" target="_blank">00:07:04.000</a></span> | <span class="t">So this is this linear interaction distance problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=426" target="_blank">00:07:06.160</a></span> | <span class="t">We would like words that might be related</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=428" target="_blank">00:07:08.400</a></span> | <span class="t">to be able to interact with each other in the neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=431" target="_blank">00:07:11.000</a></span> | <span class="t">computation graph more easily than being linearly far away</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=439" target="_blank">00:07:19.800</a></span> | <span class="t">so that we can learn these long distance dependencies better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=443" target="_blank">00:07:23.000</a></span> | <span class="t">And there's a related problem too that again comes back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=445" target="_blank">00:07:25.560</a></span> | <span class="t">to the recurrent neural networks dependence on the index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=448" target="_blank">00:07:28.640</a></span> | <span class="t">On the index into the sequence, often called a dependence on time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=452" target="_blank">00:07:32.880</a></span> | <span class="t">So in a recurrent neural network, the forward and backward passes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=456" target="_blank">00:07:36.800</a></span> | <span class="t">have O of sequence length many.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=459" target="_blank">00:07:39.520</a></span> | <span class="t">So that means just roughly sequence, in this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=461" target="_blank">00:07:41.600</a></span> | <span class="t">just sequence length many unparallelizable operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=465" target="_blank">00:07:45.000</a></span> | <span class="t">So we know GPUs are great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=467" target="_blank">00:07:47.240</a></span> | <span class="t">They can do a lot of operations at once,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=470" target="_blank">00:07:50.520</a></span> | <span class="t">as long as there's no dependency between the operations in terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=473" target="_blank">00:07:53.840</a></span> | <span class="t">of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=474" target="_blank">00:07:54.360</a></span> | <span class="t">You have to compute one and then compute the other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=477" target="_blank">00:07:57.680</a></span> | <span class="t">But in a recurrent neural network, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=479" target="_blank">00:07:59.800</a></span> | <span class="t">can't actually compute the RNN hidden state for time step 5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=483" target="_blank">00:08:03.800</a></span> | <span class="t">before you compute the RNN hidden state for time step 4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=486" target="_blank">00:08:06.920</a></span> | <span class="t">or time step 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=488" target="_blank">00:08:08.560</a></span> | <span class="t">And so you get this graph that looks very similar,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=491" target="_blank">00:08:11.320</a></span> | <span class="t">where if I want to compute this hidden state,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=493" target="_blank">00:08:13.200</a></span> | <span class="t">so I've got some word, I have zero operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=496" target="_blank">00:08:16.160</a></span> | <span class="t">I need to do before I can compute this state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=498" target="_blank">00:08:18.600</a></span> | <span class="t">I have one operation I can do before I can compute this state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=502" target="_blank">00:08:22.560</a></span> | <span class="t">And as my sequence length grows, I've got--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=505" target="_blank">00:08:25.280</a></span> | <span class="t">OK, here I've got three operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=507" target="_blank">00:08:27.040</a></span> | <span class="t">I need to do before I can compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=508" target="_blank">00:08:28.880</a></span> | <span class="t">the state with the number 3, because I need to compute this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=512" target="_blank">00:08:32.080</a></span> | <span class="t">and this and that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=513" target="_blank">00:08:33.880</a></span> | <span class="t">So there's three unparallelizable operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=517" target="_blank">00:08:37.000</a></span> | <span class="t">that I'm glomming all the matrix multiplies and stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=519" target="_blank">00:08:39.640</a></span> | <span class="t">into a single one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=520" target="_blank">00:08:40.880</a></span> | <span class="t">So 1, 2, 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=522" target="_blank">00:08:42.480</a></span> | <span class="t">And of course, this grows with the sequence length as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=525" target="_blank">00:08:45.320</a></span> | <span class="t">So down over here, as the sequence length grows,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=528" target="_blank">00:08:48.720</a></span> | <span class="t">I can't parallelize--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=530" target="_blank">00:08:50.520</a></span> | <span class="t">I can't just have a big GPU just kachanka</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=533" target="_blank">00:08:53.600</a></span> | <span class="t">with the matrix multiply to compute this state,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=536" target="_blank">00:08:56.840</a></span> | <span class="t">because I need to compute all the previous states beforehand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=539" target="_blank">00:08:59.600</a></span> | <span class="t">OK, any questions about that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=543" target="_blank">00:09:03.520</a></span> | <span class="t">So these are these two related problems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=546" target="_blank">00:09:06.040</a></span> | <span class="t">both with the dependence on time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=547" target="_blank">00:09:07.960</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=548" target="_blank">00:09:08.800</a></span> | <span class="t">Yeah, so I have a question on the linear interaction issues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=551" target="_blank">00:09:11.360</a></span> | <span class="t">I thought that was the whole point of the attention network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=553" target="_blank">00:09:13.880</a></span> | <span class="t">and then how maybe you want, during the training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=557" target="_blank">00:09:17.960</a></span> | <span class="t">of the actual cells that depend more on each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=561" target="_blank">00:09:21.080</a></span> | <span class="t">Can't we do something like the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=562" target="_blank">00:09:22.840</a></span> | <span class="t">and then work our way around that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=566" target="_blank">00:09:26.200</a></span> | <span class="t">So the question is, with the linear interaction distance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=568" target="_blank">00:09:28.760</a></span> | <span class="t">wasn't this the point of attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=570" target="_blank">00:09:30.440</a></span> | <span class="t">that gets around that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=571" target="_blank">00:09:31.720</a></span> | <span class="t">Can't we use something with attention to help,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=573" target="_blank">00:09:33.880</a></span> | <span class="t">or does that just help?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=575" target="_blank">00:09:35.040</a></span> | <span class="t">So it won't solve the parallelizability problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=577" target="_blank">00:09:37.480</a></span> | <span class="t">And in fact, everything we do in the rest of this lecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=579" target="_blank">00:09:39.840</a></span> | <span class="t">will be attention-based.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=581" target="_blank">00:09:41.160</a></span> | <span class="t">But we'll get rid of the recurrence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=582" target="_blank">00:09:42.620</a></span> | <span class="t">and just do attention, more or less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=584" target="_blank">00:09:44.360</a></span> | <span class="t">So well, yeah, it's a great intuition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=588" target="_blank">00:09:48.720</a></span> | <span class="t">Any other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=589" target="_blank">00:09:49.920</a></span> | <span class="t">OK, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=594" target="_blank">00:09:54.480</a></span> | <span class="t">So if not recurrence, what about attention?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=597" target="_blank">00:09:57.920</a></span> | <span class="t">See, I'm just a slide back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=600" target="_blank">00:10:00.040</a></span> | <span class="t">And so we're going to get deep into attention today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=604" target="_blank">00:10:04.440</a></span> | <span class="t">But just for the second, attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=606" target="_blank">00:10:06.520</a></span> | <span class="t">treats each word's representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=608" target="_blank">00:10:08.160</a></span> | <span class="t">as a query to access and incorporate information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=611" target="_blank">00:10:11.480</a></span> | <span class="t">from a set of values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=612" target="_blank">00:10:12.840</a></span> | <span class="t">So previously, we were in a decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=614" target="_blank">00:10:14.880</a></span> | <span class="t">We were decoding out a translation of a sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=617" target="_blank">00:10:17.360</a></span> | <span class="t">And we attended to the encoder so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=619" target="_blank">00:10:19.280</a></span> | <span class="t">that we didn't have to store the entire representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=621" target="_blank">00:10:21.520</a></span> | <span class="t">of the source sentence into a single vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=624" target="_blank">00:10:24.020</a></span> | <span class="t">And here, today, we'll think about attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=626" target="_blank">00:10:26.120</a></span> | <span class="t">within a single sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=627" target="_blank">00:10:27.520</a></span> | <span class="t">So I've got this sentence written out here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=629" target="_blank">00:10:29.840</a></span> | <span class="t">with a word 1 through word t, in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=632" target="_blank">00:10:32.680</a></span> | <span class="t">And right on these integers in the boxes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=635" target="_blank">00:10:35.960</a></span> | <span class="t">I'm writing out the number of unparallelizable operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=638" target="_blank">00:10:38.840</a></span> | <span class="t">that you need to do before you can compute these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=641" target="_blank">00:10:41.880</a></span> | <span class="t">So for each word, you can independently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=643" target="_blank">00:10:43.600</a></span> | <span class="t">compute its embedding without doing anything else previously,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=646" target="_blank">00:10:46.920</a></span> | <span class="t">because the embedding just depends on the word identity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=650" target="_blank">00:10:50.320</a></span> | <span class="t">And then with attention, if I wanted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=653" target="_blank">00:10:53.460</a></span> | <span class="t">to build an attention representation of this word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=655" target="_blank">00:10:55.580</a></span> | <span class="t">by looking at all the other words in the sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=657" target="_blank">00:10:57.780</a></span> | <span class="t">that's one big operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=659" target="_blank">00:10:59.740</a></span> | <span class="t">And I can do them in parallel for all the words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=662" target="_blank">00:11:02.460</a></span> | <span class="t">So the attention for this word, I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=664" target="_blank">00:11:04.460</a></span> | <span class="t">can do for the attention for this word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=666" target="_blank">00:11:06.100</a></span> | <span class="t">I don't need to walk left to right like I did for an RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=669" target="_blank">00:11:09.100</a></span> | <span class="t">Again, we'll get much deeper into this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=670" target="_blank">00:11:10.940</a></span> | <span class="t">But you should have the intuition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=673" target="_blank">00:11:13.940</a></span> | <span class="t">that it solves the linear interaction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=676" target="_blank">00:11:16.220</a></span> | <span class="t">problem and the non-parallelizability problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=678" target="_blank">00:11:18.860</a></span> | <span class="t">Because now, no matter how far away words are from each other,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=682" target="_blank">00:11:22.120</a></span> | <span class="t">I am potentially interacting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=683" target="_blank">00:11:23.980</a></span> | <span class="t">I might just attend to you, even if you're very, very far away,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=687" target="_blank">00:11:27.600</a></span> | <span class="t">sort of independent of how far away you are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=689" target="_blank">00:11:29.840</a></span> | <span class="t">And I also don't need to sort of walk along the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=693" target="_blank">00:11:33.120</a></span> | <span class="t">linearly long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=694" target="_blank">00:11:34.320</a></span> | <span class="t">So I'm treating the whole sequence at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=696" target="_blank">00:11:36.840</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=698" target="_blank">00:11:38.320</a></span> | <span class="t">So the intuition is that attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=700" target="_blank">00:11:40.520</a></span> | <span class="t">allows you to look very far away at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=702" target="_blank">00:11:42.280</a></span> | <span class="t">And it doesn't have this dependence on the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=704" target="_blank">00:11:44.440</a></span> | <span class="t">index that keeps us from parallelizing operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=707" target="_blank">00:11:47.120</a></span> | <span class="t">And so now, the rest of the lecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=708" target="_blank">00:11:48.620</a></span> | <span class="t">will talk in great depth about attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=711" target="_blank">00:11:51.780</a></span> | <span class="t">So maybe let's just move on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=715" target="_blank">00:11:55.220</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=716" target="_blank">00:11:56.300</a></span> | <span class="t">So let's think more deeply about attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=720" target="_blank">00:12:00.180</a></span> | <span class="t">One thing that you might think of with attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=722" target="_blank">00:12:02.660</a></span> | <span class="t">is that it's sort of performing kind of a fuzzy lookup</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=725" target="_blank">00:12:05.540</a></span> | <span class="t">in a key value store.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=727" target="_blank">00:12:07.180</a></span> | <span class="t">So you have a bunch of keys, a bunch of values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=729" target="_blank">00:12:09.540</a></span> | <span class="t">and it's going to help you sort of access that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=732" target="_blank">00:12:12.140</a></span> | <span class="t">So in an actual lookup table, just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=734" target="_blank">00:12:14.300</a></span> | <span class="t">like a dictionary in Python, for example, very simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=738" target="_blank">00:12:18.220</a></span> | <span class="t">You have a table of keys that each key maps to a value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=742" target="_blank">00:12:22.100</a></span> | <span class="t">And then you give it a query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=743" target="_blank">00:12:23.500</a></span> | <span class="t">And the query matches one of the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=746" target="_blank">00:12:26.300</a></span> | <span class="t">And then you return the value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=747" target="_blank">00:12:27.940</a></span> | <span class="t">So I've got a bunch of keys here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=751" target="_blank">00:12:31.420</a></span> | <span class="t">And my query matches the key.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=753" target="_blank">00:12:33.220</a></span> | <span class="t">So I return the value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=754" target="_blank">00:12:34.660</a></span> | <span class="t">Simple, fair, easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=756" target="_blank">00:12:36.940</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=757" target="_blank">00:12:37.740</a></span> | <span class="t">Good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=759" target="_blank">00:12:39.500</a></span> | <span class="t">And in attention, so just like we saw before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=764" target="_blank">00:12:44.060</a></span> | <span class="t">the query matches all keys softly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=766" target="_blank">00:12:46.660</a></span> | <span class="t">There's no exact match.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=768" target="_blank">00:12:48.940</a></span> | <span class="t">You sort of compute some sort of similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=770" target="_blank">00:12:50.780</a></span> | <span class="t">between the key and all of the--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=772" target="_blank">00:12:52.660</a></span> | <span class="t">sorry, the query and all of the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=774" target="_blank">00:12:54.620</a></span> | <span class="t">And then you sort of weight the results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=776" target="_blank">00:12:56.260</a></span> | <span class="t">So you've got a query again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=777" target="_blank">00:12:57.780</a></span> | <span class="t">You've got a bunch of keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=780" target="_blank">00:13:00.020</a></span> | <span class="t">The query, to different extents, is similar to each of the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=784" target="_blank">00:13:04.500</a></span> | <span class="t">And you will sort of measure that similarity between 0 and 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=788" target="_blank">00:13:08.460</a></span> | <span class="t">through a softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=790" target="_blank">00:13:10.140</a></span> | <span class="t">And then you get the values out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=792" target="_blank">00:13:12.380</a></span> | <span class="t">So you average them via the weights of the similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=795" target="_blank">00:13:15.660</a></span> | <span class="t">between the key and the query and the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=798" target="_blank">00:13:18.780</a></span> | <span class="t">You do a weighted sum with those weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=800" target="_blank">00:13:20.580</a></span> | <span class="t">And you get an output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=801" target="_blank">00:13:21.660</a></span> | <span class="t">So it really is quite a bit like a lookup table,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=804" target="_blank">00:13:24.780</a></span> | <span class="t">but in this sort of soft vector space, mushy sort of sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=809" target="_blank">00:13:29.940</a></span> | <span class="t">So I'm really doing some kind of accessing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=812" target="_blank">00:13:32.140</a></span> | <span class="t">into this information that's stored in the key value store.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=815" target="_blank">00:13:35.820</a></span> | <span class="t">But I'm sort of softly looking at all of the results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=821" target="_blank">00:13:41.220</a></span> | <span class="t">OK, any questions there?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=822" target="_blank">00:13:42.300</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=826" target="_blank">00:13:46.940</a></span> | <span class="t">So what might this look like?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=828" target="_blank">00:13:48.620</a></span> | <span class="t">So if I was trying to represent this sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=830" target="_blank">00:13:50.980</a></span> | <span class="t">I went to Stanford CS224n and learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=834" target="_blank">00:13:54.260</a></span> | <span class="t">So I'm trying to build a representation of learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=836" target="_blank">00:13:56.260</a></span> | <span class="t">I have a key for each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=841" target="_blank">00:14:01.580</a></span> | <span class="t">So this is this self-attention thing that we'll get into.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=844" target="_blank">00:14:04.500</a></span> | <span class="t">I have a key for each word, a value for each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=846" target="_blank">00:14:06.740</a></span> | <span class="t">I've got the query for learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=848" target="_blank">00:14:08.380</a></span> | <span class="t">And I've got these sort of tealish bars up top,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=851" target="_blank">00:14:11.620</a></span> | <span class="t">which sort of might say how much you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=853" target="_blank">00:14:13.620</a></span> | <span class="t">going to try to access each of the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=855" target="_blank">00:14:15.500</a></span> | <span class="t">Like, oh, maybe 224n is not that important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=858" target="_blank">00:14:18.300</a></span> | <span class="t">CS, maybe that determines what I learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=860" target="_blank">00:14:20.500</a></span> | <span class="t">You know, Stanford.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=862" target="_blank">00:14:22.700</a></span> | <span class="t">And then learned, maybe that's important to representing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=865" target="_blank">00:14:25.180</a></span> | <span class="t">itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=865" target="_blank">00:14:25.900</a></span> | <span class="t">So you sort of look across at the whole sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=868" target="_blank">00:14:28.100</a></span> | <span class="t">and build up this sort of soft accessing of information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=871" target="_blank">00:14:31.020</a></span> | <span class="t">across the sentence in order to represent learned in context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=875" target="_blank">00:14:35.860</a></span> | <span class="t">So this is just a toy diagram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=878" target="_blank">00:14:38.860</a></span> | <span class="t">So let's get into the math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=880" target="_blank">00:14:40.460</a></span> | <span class="t">So we're going to look at a sequence of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=883" target="_blank">00:14:43.540</a></span> | <span class="t">So that's w1 to n, a sequence of words in a vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=886" target="_blank">00:14:46.860</a></span> | <span class="t">So this is like, you know, Zuko made his uncle tea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=889" target="_blank">00:14:49.140</a></span> | <span class="t">That's a good sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=890" target="_blank">00:14:50.340</a></span> | <span class="t">And for each word, we're going to embed it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=892" target="_blank">00:14:52.580</a></span> | <span class="t">with this embedding matrix, just like we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=894" target="_blank">00:14:54.620</a></span> | <span class="t">been doing in this class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=896" target="_blank">00:14:56.180</a></span> | <span class="t">So I have this embedding matrix that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=897" target="_blank">00:14:57.780</a></span> | <span class="t">goes from the vocabulary size to the dimensionality d.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=902" target="_blank">00:15:02.460</a></span> | <span class="t">So each word has a non-contextual,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=904" target="_blank">00:15:04.660</a></span> | <span class="t">only dependent on itself, word embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=907" target="_blank">00:15:07.500</a></span> | <span class="t">And now I'm going to transform each word with one of three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=911" target="_blank">00:15:11.340</a></span> | <span class="t">different weight matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=912" target="_blank">00:15:12.500</a></span> | <span class="t">So this is often called key query value self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=916" target="_blank">00:15:16.820</a></span> | <span class="t">So I have a matrix Q, which is an rd to d.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=919" target="_blank">00:15:19.980</a></span> | <span class="t">So this maps xi, which is a vector of dimensionality d,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=923" target="_blank">00:15:23.420</a></span> | <span class="t">to another vector of dimensionality d.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=925" target="_blank">00:15:25.780</a></span> | <span class="t">And that's going to be a query vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=928" target="_blank">00:15:28.500</a></span> | <span class="t">So it takes an xi and it sort of rotates it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=931" target="_blank">00:15:31.260</a></span> | <span class="t">shuffles it around, stretches it, squishes it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=933" target="_blank">00:15:33.940</a></span> | <span class="t">Makes it different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=935" target="_blank">00:15:35.060</a></span> | <span class="t">And now it's a query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=935" target="_blank">00:15:35.940</a></span> | <span class="t">And now for a different learnable parameter, k--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=938" target="_blank">00:15:38.300</a></span> | <span class="t">so that's another matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=939" target="_blank">00:15:39.500</a></span> | <span class="t">I'm going to come up with my keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=941" target="_blank">00:15:41.940</a></span> | <span class="t">And with a different learnable parameter, v,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=945" target="_blank">00:15:45.220</a></span> | <span class="t">I'm going to come up with my values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=947" target="_blank">00:15:47.100</a></span> | <span class="t">So I'm taking each of the non-contextual word embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=949" target="_blank">00:15:49.640</a></span> | <span class="t">each of these xi's, and I'm transforming each of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=953" target="_blank">00:15:53.300</a></span> | <span class="t">to come up with my query for that word, my key for that word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=957" target="_blank">00:15:57.140</a></span> | <span class="t">and my value for that word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=960" target="_blank">00:16:00.220</a></span> | <span class="t">So every word is doing each of these roles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=963" target="_blank">00:16:03.700</a></span> | <span class="t">Next, I'm going to compute all pairs of similarities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=966" target="_blank">00:16:06.660</a></span> | <span class="t">between the keys and queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=968" target="_blank">00:16:08.220</a></span> | <span class="t">So in the toy example we saw, I was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=970" target="_blank">00:16:10.500</a></span> | <span class="t">computing the similarity between a single query for the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=973" target="_blank">00:16:13.260</a></span> | <span class="t">learned and all of the keys for the entire sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=977" target="_blank">00:16:17.380</a></span> | <span class="t">In this context, I'm computing all pairs of similarities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=980" target="_blank">00:16:20.380</a></span> | <span class="t">between all keys and all values because I want to represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=984" target="_blank">00:16:24.300</a></span> | <span class="t">all of these sums.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=985" target="_blank">00:16:25.140</a></span> | <span class="t">So I've got this sort of dot--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=987" target="_blank">00:16:27.620</a></span> | <span class="t">I'm just going to take the dot product between these two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=989" target="_blank">00:16:29.780</a></span> | <span class="t">vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=990" target="_blank">00:16:30.420</a></span> | <span class="t">So I've got qi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=991" target="_blank">00:16:31.820</a></span> | <span class="t">So this is saying the query for word i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=994" target="_blank">00:16:34.140</a></span> | <span class="t">dotted with the key for word j.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=996" target="_blank">00:16:36.100</a></span> | <span class="t">And I get this score, which is a real value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1000" target="_blank">00:16:40.980</a></span> | <span class="t">Might be very large negative, might be zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1002" target="_blank">00:16:42.820</a></span> | <span class="t">might be very large and positive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1004" target="_blank">00:16:44.660</a></span> | <span class="t">And so that's like, how much should I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1006" target="_blank">00:16:46.580</a></span> | <span class="t">look at j in this lookup table?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1010" target="_blank">00:16:50.140</a></span> | <span class="t">And then I do the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1011" target="_blank">00:16:51.340</a></span> | <span class="t">So I softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1012" target="_blank">00:16:52.580</a></span> | <span class="t">So I say that the actual weight that I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1015" target="_blank">00:16:55.140</a></span> | <span class="t">going to look at j from i is softmax of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1018" target="_blank">00:16:58.660</a></span> | <span class="t">over all of the possible indices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1020" target="_blank">00:17:00.900</a></span> | <span class="t">So it's like the affinity between i and j</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1023" target="_blank">00:17:03.780</a></span> | <span class="t">normalized by the affinity between i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1026" target="_blank">00:17:06.020</a></span> | <span class="t">and all of the possible j prime in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1028" target="_blank">00:17:08.220</a></span> | <span class="t">And then my output is just the weighted sum of values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1033" target="_blank">00:17:13.940</a></span> | <span class="t">So I've got this output for word i.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1036" target="_blank">00:17:16.060</a></span> | <span class="t">So maybe i is like 1 for Zuko.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1038" target="_blank">00:17:18.420</a></span> | <span class="t">And I'm representing it as the sum of these weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1042" target="_blank">00:17:22.140</a></span> | <span class="t">for all j.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1043" target="_blank">00:17:23.140</a></span> | <span class="t">So Zuko and maid and his and uncle and t.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1046" target="_blank">00:17:26.180</a></span> | <span class="t">And the value vector for that word j.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1050" target="_blank">00:17:30.140</a></span> | <span class="t">I'm looking from i to j as much as alpha ij.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1054" target="_blank">00:17:34.940</a></span> | <span class="t">What's the dimension of Wi?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1057" target="_blank">00:17:37.380</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1059" target="_blank">00:17:39.380</a></span> | <span class="t">Oh, Wi, you can either think of it as a symbol in vocab v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1064" target="_blank">00:17:44.900</a></span> | <span class="t">So that's like, you could think of it as a one-hot vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1067" target="_blank">00:17:47.660</a></span> | <span class="t">And yeah, in this case, we are, I guess, thinking of it as--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1071" target="_blank">00:17:51.220</a></span> | <span class="t">so one-hot vector in dimensionality size of vocab.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1074" target="_blank">00:17:54.380</a></span> | <span class="t">So in the matrix E, you see that it's r d by bars around v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1079" target="_blank">00:17:59.660</a></span> | <span class="t">That's size of the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1081" target="_blank">00:18:01.700</a></span> | <span class="t">So when I do E multiplied by Wi, that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1085" target="_blank">00:18:05.100</a></span> | <span class="t">taking E, which is d by v, multiplying it by w, which is v,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1090" target="_blank">00:18:10.540</a></span> | <span class="t">and returning a vector that's dimensionality d.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1093" target="_blank">00:18:13.100</a></span> | <span class="t">So w in that first line, like w1n,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1096" target="_blank">00:18:16.940</a></span> | <span class="t">that's a matrix where it has maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1100" target="_blank">00:18:20.700</a></span> | <span class="t">like a column for every word in that sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1103" target="_blank">00:18:23.500</a></span> | <span class="t">And each column is a length v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1105" target="_blank">00:18:25.820</a></span> | <span class="t">Yeah, usually, I guess we think of it as having a--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1108" target="_blank">00:18:28.460</a></span> | <span class="t">I mean, if I'm putting the sequence length index first,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1111" target="_blank">00:18:31.780</a></span> | <span class="t">you might think of it as having a row for each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1113" target="_blank">00:18:33.980</a></span> | <span class="t">But similarly, yeah, it's n, which is the sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1117" target="_blank">00:18:37.020</a></span> | <span class="t">And then the second dimension would be v,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1119" target="_blank">00:18:39.020</a></span> | <span class="t">which is the vocabulary size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1120" target="_blank">00:18:40.740</a></span> | <span class="t">And then that gets mapped to this thing, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1122" target="_blank">00:18:42.740</a></span> | <span class="t">is sequence length by d.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1126" target="_blank">00:18:46.380</a></span> | <span class="t">Why do we learn two different matrices, q and k,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1129" target="_blank">00:18:49.460</a></span> | <span class="t">when q transpose--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1131" target="_blank">00:18:51.420</a></span> | <span class="t">qi transpose kj is really just one matrix in the middle?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1136" target="_blank">00:18:56.100</a></span> | <span class="t">That's a great question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1137" target="_blank">00:18:57.100</a></span> | <span class="t">It ends up being because this will end up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1139" target="_blank">00:18:59.500</a></span> | <span class="t">being a low-rank approximation to that matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1142" target="_blank">00:19:02.060</a></span> | <span class="t">So it is for computational efficiency reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1145" target="_blank">00:19:05.500</a></span> | <span class="t">Although it also, I think, feels kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1147" target="_blank">00:19:07.660</a></span> | <span class="t">of nice in the presentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1149" target="_blank">00:19:09.940</a></span> | <span class="t">But yeah, what we'll end up doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1151" target="_blank">00:19:11.300</a></span> | <span class="t">is having a very low-rank approximation to qk transpose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1154" target="_blank">00:19:14.860</a></span> | <span class="t">And so you actually do do it like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1157" target="_blank">00:19:17.380</a></span> | <span class="t">It's a good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1159" target="_blank">00:19:19.780</a></span> | <span class="t">Is vii, so the query with any specific?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1166" target="_blank">00:19:26.140</a></span> | <span class="t">Sorry, could you repeat that for me?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1167" target="_blank">00:19:27.620</a></span> | <span class="t">This eii, so the query of the word dotted with the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1172" target="_blank">00:19:32.620</a></span> | <span class="t">by itself, does it look like an identity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1174" target="_blank">00:19:34.980</a></span> | <span class="t">or does it look like anything in particular?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1177" target="_blank">00:19:37.420</a></span> | <span class="t">That's a good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1178" target="_blank">00:19:38.340</a></span> | <span class="t">OK, let me remember to repeat questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1180" target="_blank">00:19:40.500</a></span> | <span class="t">So does eii, for j equal to i, so looking at itself,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1184" target="_blank">00:19:44.660</a></span> | <span class="t">look like anything in particular?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1186" target="_blank">00:19:46.080</a></span> | <span class="t">Does it look like the identity?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1187" target="_blank">00:19:47.660</a></span> | <span class="t">Is that the question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1188" target="_blank">00:19:48.820</a></span> | <span class="t">OK, so right, it's unclear, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1193" target="_blank">00:19:53.020</a></span> | <span class="t">This question of should you look at yourself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1194" target="_blank">00:19:54.940</a></span> | <span class="t">for representing yourself, well, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1197" target="_blank">00:19:57.020</a></span> | <span class="t">going to be encoded by the matrices q and k.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1200" target="_blank">00:20:00.780</a></span> | <span class="t">If I didn't have q and k in there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1202" target="_blank">00:20:02.940</a></span> | <span class="t">if those were the identity matrices,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1204" target="_blank">00:20:04.940</a></span> | <span class="t">if q is identity, k is identity, then this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1207" target="_blank">00:20:07.460</a></span> | <span class="t">would be sort of dot product with yourself, which is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1209" target="_blank">00:20:09.860</a></span> | <span class="t">to be high on average, like you're pointing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1212" target="_blank">00:20:12.180</a></span> | <span class="t">in the same direction as yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1213" target="_blank">00:20:13.860</a></span> | <span class="t">But it could be that qxi and kxi might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1218" target="_blank">00:20:18.460</a></span> | <span class="t">be sort of arbitrarily different from each other,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1221" target="_blank">00:20:21.060</a></span> | <span class="t">because q could be the identity, and k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1224" target="_blank">00:20:24.140</a></span> | <span class="t">could map you to the negative of yourself,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1227" target="_blank">00:20:27.060</a></span> | <span class="t">for example, so that you don't look at yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1229" target="_blank">00:20:29.060</a></span> | <span class="t">So this is all learned in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1230" target="_blank">00:20:30.940</a></span> | <span class="t">So you end up--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1232" target="_blank">00:20:32.380</a></span> | <span class="t">it can sort of decide by learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1235" target="_blank">00:20:35.860</a></span> | <span class="t">whether you should be looking at yourself or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1238" target="_blank">00:20:38.180</a></span> | <span class="t">And that's some of the flexibility</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1239" target="_blank">00:20:39.580</a></span> | <span class="t">that parametrizing it as q and k gives you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1242" target="_blank">00:20:42.820</a></span> | <span class="t">that wouldn't be there if I just used xis everywhere</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1246" target="_blank">00:20:46.180</a></span> | <span class="t">in this equation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1249" target="_blank">00:20:49.820</a></span> | <span class="t">I'm going to try to move on, I'm afraid,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1251" target="_blank">00:20:51.860</a></span> | <span class="t">because there's a lot to get on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1253" target="_blank">00:20:53.340</a></span> | <span class="t">But we'll keep talking about self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1255" target="_blank">00:20:55.540</a></span> | <span class="t">And so as more questions come up,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1257" target="_blank">00:20:57.660</a></span> | <span class="t">I can also potentially return back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1261" target="_blank">00:21:01.460</a></span> | <span class="t">OK, so this is our basic building block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1265" target="_blank">00:21:05.140</a></span> | <span class="t">But there are a bunch of barriers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1266" target="_blank">00:21:06.860</a></span> | <span class="t">to using it as a replacement for LSTMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1270" target="_blank">00:21:10.260</a></span> | <span class="t">And so what we're going to do for this portion of the lecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1273" target="_blank">00:21:13.020</a></span> | <span class="t">is talk about the minimal components</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1274" target="_blank">00:21:14.740</a></span> | <span class="t">that we need in order to use self-attention as sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1278" target="_blank">00:21:18.180</a></span> | <span class="t">of this very fundamental building block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1281" target="_blank">00:21:21.540</a></span> | <span class="t">So we can't use it as it stands as I've presented it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1285" target="_blank">00:21:25.260</a></span> | <span class="t">because there are a couple of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1286" target="_blank">00:21:26.740</a></span> | <span class="t">that we need to sort of solve or fix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1289" target="_blank">00:21:29.340</a></span> | <span class="t">One of them is that there's no notion of sequence order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1292" target="_blank">00:21:32.180</a></span> | <span class="t">in self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1293" target="_blank">00:21:33.500</a></span> | <span class="t">So what does this mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1297" target="_blank">00:21:37.580</a></span> | <span class="t">If I have a sentence like--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1300" target="_blank">00:21:40.140</a></span> | <span class="t">I'm going to move over here to the whiteboard briefly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1302" target="_blank">00:21:42.380</a></span> | <span class="t">and hopefully I'll write quite large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1306" target="_blank">00:21:46.820</a></span> | <span class="t">If I have a sentence like, Zuko made his uncle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1315" target="_blank">00:21:55.700</a></span> | <span class="t">And let's say, his uncle made Zuko.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1325" target="_blank">00:22:05.780</a></span> | <span class="t">If I were to embed each of these words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1328" target="_blank">00:22:08.700</a></span> | <span class="t">using its embedding matrix, the embedding matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1330" target="_blank">00:22:10.700</a></span> | <span class="t">isn't dependent on the index of the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1334" target="_blank">00:22:14.980</a></span> | <span class="t">So this is the word index 1, 2, 3, 4,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1338" target="_blank">00:22:18.060</a></span> | <span class="t">versus now his is over here, and uncle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1341" target="_blank">00:22:21.900</a></span> | <span class="t">And so when I compute the self-attention--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1343" target="_blank">00:22:23.900</a></span> | <span class="t">and there's a lot more on this in the lecture notes that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1346" target="_blank">00:22:26.240</a></span> | <span class="t">goes through a full example--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1349" target="_blank">00:22:29.860</a></span> | <span class="t">the actual self-attention operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1352" target="_blank">00:22:32.260</a></span> | <span class="t">will give you exactly the same representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1354" target="_blank">00:22:34.380</a></span> | <span class="t">for this sequence, Zuko made his uncle, as for this sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1358" target="_blank">00:22:38.060</a></span> | <span class="t">his uncle made Zuko.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1360" target="_blank">00:22:40.020</a></span> | <span class="t">And that's bad, because they're sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1361" target="_blank">00:22:41.680</a></span> | <span class="t">that mean different things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1363" target="_blank">00:22:43.900</a></span> | <span class="t">And so it's this idea that self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1366" target="_blank">00:22:46.820</a></span> | <span class="t">is an operation on sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1368" target="_blank">00:22:48.580</a></span> | <span class="t">You have a set of vectors that you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1370" target="_blank">00:22:50.780</a></span> | <span class="t">going to perform self-attention on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1372" target="_blank">00:22:52.660</a></span> | <span class="t">and nowhere does the exact position of the words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1375" target="_blank">00:22:55.740</a></span> | <span class="t">come into play directly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1379" target="_blank">00:22:59.140</a></span> | <span class="t">So we're going to encode the position of words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1382" target="_blank">00:23:02.540</a></span> | <span class="t">through the keys, queries, and values that we have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1386" target="_blank">00:23:06.100</a></span> | <span class="t">So consider now representing each sequence index--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1390" target="_blank">00:23:10.060</a></span> | <span class="t">our sequences are going from 1 to n--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1392" target="_blank">00:23:12.100</a></span> | <span class="t">as a vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1393" target="_blank">00:23:13.380</a></span> | <span class="t">So don't worry so far about how it's being made,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1397" target="_blank">00:23:17.220</a></span> | <span class="t">but you can imagine representing the number 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1400" target="_blank">00:23:20.300</a></span> | <span class="t">the position 1, the position 2, the position 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1403" target="_blank">00:23:23.540</a></span> | <span class="t">as a vector in the dimensionality d,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1405" target="_blank">00:23:25.580</a></span> | <span class="t">just like we're representing our keys, queries, and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1409" target="_blank">00:23:29.260</a></span> | <span class="t">And so these are position vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1413" target="_blank">00:23:33.060</a></span> | <span class="t">If you were to want to incorporate the information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1417" target="_blank">00:23:37.940</a></span> | <span class="t">represented by these positions into our self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1422" target="_blank">00:23:42.380</a></span> | <span class="t">you could just add these vectors, these p i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1425" target="_blank">00:23:45.140</a></span> | <span class="t">vectors, to the inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1428" target="_blank">00:23:48.020</a></span> | <span class="t">So if I have this xi embedding of a word, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1433" target="_blank">00:23:53.340</a></span> | <span class="t">is the word at position i, but really just represents,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1436" target="_blank">00:23:56.060</a></span> | <span class="t">oh, the word zuko is here, now I can say, oh, it's the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1439" target="_blank">00:23:59.100</a></span> | <span class="t">zuko, and it's at position 5, because this vector represents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1443" target="_blank">00:24:03.860</a></span> | <span class="t">position 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1444" target="_blank">00:24:04.660</a></span> | <span class="t">So how do we do this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1451" target="_blank">00:24:11.260</a></span> | <span class="t">And we might only have to do this once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1452" target="_blank">00:24:12.860</a></span> | <span class="t">So we can do it once at the very input to the network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1456" target="_blank">00:24:16.420</a></span> | <span class="t">and then that is sufficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1458" target="_blank">00:24:18.260</a></span> | <span class="t">We don't have to do it at every layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1459" target="_blank">00:24:19.840</a></span> | <span class="t">because it knows from the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1463" target="_blank">00:24:23.660</a></span> | <span class="t">So one way in which people have done this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1466" target="_blank">00:24:26.060</a></span> | <span class="t">is look at these sinusoidal position representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1469" target="_blank">00:24:29.500</a></span> | <span class="t">So this looks a little bit like this, where you have--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1472" target="_blank">00:24:32.100</a></span> | <span class="t">so this is a vector p i, which is in dimensionality d.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1475" target="_blank">00:24:35.980</a></span> | <span class="t">And each one of the dimensions, you take the value i,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1480" target="_blank">00:24:40.500</a></span> | <span class="t">you modify it by some constant, and you pass it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1485" target="_blank">00:24:45.740</a></span> | <span class="t">to the sine or cosine function, and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1487" target="_blank">00:24:47.780</a></span> | <span class="t">get these sort of values that vary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1489" target="_blank">00:24:49.900</a></span> | <span class="t">according to the period, differing periods depending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1493" target="_blank">00:24:53.260</a></span> | <span class="t">on the dimensionality's d.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1494" target="_blank">00:24:54.340</a></span> | <span class="t">So I've got this sort of a representation of a matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1497" target="_blank">00:24:57.500</a></span> | <span class="t">where d is the vertical dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1499" target="_blank">00:24:59.580</a></span> | <span class="t">and then n is the horizontal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1501" target="_blank">00:25:01.500</a></span> | <span class="t">And you can see that there's sort of like, oh, as I walk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1505" target="_blank">00:25:05.580</a></span> | <span class="t">along, you see the period of the sine function going up and down,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1508" target="_blank">00:25:08.300</a></span> | <span class="t">and each of the dimensions d has a different period.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1511" target="_blank">00:25:11.220</a></span> | <span class="t">And so together, you can represent a bunch of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1513" target="_blank">00:25:13.620</a></span> | <span class="t">sort of position indices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1515" target="_blank">00:25:15.140</a></span> | <span class="t">And it gives this intuition that, oh, maybe sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1520" target="_blank">00:25:20.020</a></span> | <span class="t">of the absolute position of a word isn't as important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1522" target="_blank">00:25:22.780</a></span> | <span class="t">You've got the sort of periodicity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1524" target="_blank">00:25:24.220</a></span> | <span class="t">of the sines and cosines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1526" target="_blank">00:25:26.220</a></span> | <span class="t">And maybe that allows you to extrapolate to longer sequences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1529" target="_blank">00:25:29.500</a></span> | <span class="t">But in practice, that doesn't work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1532" target="_blank">00:25:32.140</a></span> | <span class="t">But this is sort of like an early notion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1534" target="_blank">00:25:34.220</a></span> | <span class="t">that is still sometimes used for how to represent position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1537" target="_blank">00:25:37.140</a></span> | <span class="t">in transformers and self-attention networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1540" target="_blank">00:25:40.700</a></span> | <span class="t">in general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1543" target="_blank">00:25:43.260</a></span> | <span class="t">So that's one idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1545" target="_blank">00:25:45.180</a></span> | <span class="t">You might think it's a little bit complicated,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1548" target="_blank">00:25:48.580</a></span> | <span class="t">a little bit unintuitive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1550" target="_blank">00:25:50.300</a></span> | <span class="t">Here's something that feels a little bit more deep learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1554" target="_blank">00:25:54.380</a></span> | <span class="t">So we're just going to say, oh, I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1557" target="_blank">00:25:57.460</a></span> | <span class="t">got a maximum sequence length of n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1559" target="_blank">00:25:59.860</a></span> | <span class="t">And I'm just going to learn a matrix that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1561" target="_blank">00:26:01.940</a></span> | <span class="t">dimensionality d by n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1563" target="_blank">00:26:03.700</a></span> | <span class="t">And that's going to represent my positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1565" target="_blank">00:26:05.500</a></span> | <span class="t">And I'm going to learn it as a parameter, just like I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1567" target="_blank">00:26:07.780</a></span> | <span class="t">learn every other parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1569" target="_blank">00:26:09.100</a></span> | <span class="t">And what do they mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1570" target="_blank">00:26:10.020</a></span> | <span class="t">Oh, I have no idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1570" target="_blank">00:26:10.860</a></span> | <span class="t">But it represents position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1573" target="_blank">00:26:13.100</a></span> | <span class="t">So you just sort of add this matrix to the xi's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1579" target="_blank">00:26:19.420</a></span> | <span class="t">your input embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1582" target="_blank">00:26:22.140</a></span> | <span class="t">And it learns to fit to data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1584" target="_blank">00:26:24.180</a></span> | <span class="t">So whatever representation of position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1586" target="_blank">00:26:26.300</a></span> | <span class="t">that's linear, sort of index-based that you want,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1590" target="_blank">00:26:30.300</a></span> | <span class="t">you can learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1591" target="_blank">00:26:31.460</a></span> | <span class="t">And the cons are that, well, you definitely now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1593" target="_blank">00:26:33.900</a></span> | <span class="t">can't represent anything that's longer than n words long, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1597" target="_blank">00:26:37.980</a></span> | <span class="t">No sequence longer than n you can handle because, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1601" target="_blank">00:26:41.660</a></span> | <span class="t">you only learned a matrix of this many positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1604" target="_blank">00:26:44.420</a></span> | <span class="t">And so in practice, you'll get a model error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1607" target="_blank">00:26:47.660</a></span> | <span class="t">if you pass a self-attention model, something longer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1610" target="_blank">00:26:50.500</a></span> | <span class="t">than length n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1611" target="_blank">00:26:51.620</a></span> | <span class="t">It will just sort of crash and say, I can't do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1616" target="_blank">00:26:56.620</a></span> | <span class="t">And so this is sort of what most systems nowadays use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1619" target="_blank">00:26:59.660</a></span> | <span class="t">There are more flexible representations of position,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1622" target="_blank">00:27:02.220</a></span> | <span class="t">including a couple in the lecture notes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1624" target="_blank">00:27:04.940</a></span> | <span class="t">You might want to look at the relative linear position,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1628" target="_blank">00:27:08.020</a></span> | <span class="t">or words before or after each other,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1629" target="_blank">00:27:09.900</a></span> | <span class="t">but not their absolute position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1631" target="_blank">00:27:11.500</a></span> | <span class="t">There's also some sort of representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1633" target="_blank">00:27:13.300</a></span> | <span class="t">that harken back to our dependency syntax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1636" target="_blank">00:27:16.660</a></span> | <span class="t">Because, oh, maybe words that are close in the dependency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1639" target="_blank">00:27:19.100</a></span> | <span class="t">parse tree should be the things that are sort of close</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1641" target="_blank">00:27:21.620</a></span> | <span class="t">in the self-attention operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1645" target="_blank">00:27:25.060</a></span> | <span class="t">OK, questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1648" target="_blank">00:27:28.340</a></span> | <span class="t">In practice, do we typically just make n large enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1652" target="_blank">00:27:32.420</a></span> | <span class="t">that we don't run into the issue of having something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1656" target="_blank">00:27:36.500</a></span> | <span class="t">that can be input longer than n?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1659" target="_blank">00:27:39.060</a></span> | <span class="t">So the question is, in practice, do we just make n long enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1661" target="_blank">00:27:41.900</a></span> | <span class="t">that we don't run into the problem where we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1664" target="_blank">00:27:44.100</a></span> | <span class="t">to look at a text longer than n?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1666" target="_blank">00:27:46.660</a></span> | <span class="t">No, in practice, it's actually quite a problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1669" target="_blank">00:27:49.420</a></span> | <span class="t">Even today, even in the largest, biggest language models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1672" target="_blank">00:27:52.540</a></span> | <span class="t">and can I fit this prompt into chat GPT or whatever?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1678" target="_blank">00:27:58.020</a></span> | <span class="t">It's a thing that you might see on Twitter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1679" target="_blank">00:27:59.820</a></span> | <span class="t">These continue to be issues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1681" target="_blank">00:28:01.980</a></span> | <span class="t">And part of it is because the self-attention operation--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1684" target="_blank">00:28:04.980</a></span> | <span class="t">and we'll get into this later in the lecture--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1686" target="_blank">00:28:06.900</a></span> | <span class="t">it's quadratic complexity in the sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1690" target="_blank">00:28:10.060</a></span> | <span class="t">So you're going to spend n squared memory budget in order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1694" target="_blank">00:28:14.420</a></span> | <span class="t">to make sequence lengths longer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1695" target="_blank">00:28:15.740</a></span> | <span class="t">So in practice, this might be on a large model, say, 4,000 or so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1701" target="_blank">00:28:21.420</a></span> | <span class="t">n is 4,000, so you can fit 4,000 words, which feels like a lot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1704" target="_blank">00:28:24.620</a></span> | <span class="t">but it's not going to fit a novel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1706" target="_blank">00:28:26.220</a></span> | <span class="t">It's not going to fit a Wikipedia page.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1709" target="_blank">00:28:29.740</a></span> | <span class="t">And there are models that do longer sequences, for sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1713" target="_blank">00:28:33.740</a></span> | <span class="t">And again, we'll talk a bit about it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1715" target="_blank">00:28:35.280</a></span> | <span class="t">but no, this actually is an issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1716" target="_blank">00:28:36.700</a></span> | <span class="t">How do you know that the p you learned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1723" target="_blank">00:28:43.140</a></span> | <span class="t">is the position, which is not any other?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1727" target="_blank">00:28:47.580</a></span> | <span class="t">I don't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1728" target="_blank">00:28:48.220</a></span> | <span class="t">It's yours.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1729" target="_blank">00:28:49.100</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1729" target="_blank">00:28:49.700</a></span> | <span class="t">So how do you know that the p that you've learned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1731" target="_blank">00:28:51.580</a></span> | <span class="t">this matrix that you've learned, is representing position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1733" target="_blank">00:28:53.960</a></span> | <span class="t">as opposed to anything else?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1735" target="_blank">00:28:55.540</a></span> | <span class="t">And the reason is the only thing it correlates is position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1738" target="_blank">00:28:58.700</a></span> | <span class="t">So when I see these vectors, I'm adding this p matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1742" target="_blank">00:29:02.180</a></span> | <span class="t">to my x matrix, the word embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1745" target="_blank">00:29:05.460</a></span> | <span class="t">I'm adding them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1746" target="_blank">00:29:06.720</a></span> | <span class="t">And the words that show up at each index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1748" target="_blank">00:29:08.420</a></span> | <span class="t">will vary depending on what word actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1750" target="_blank">00:29:10.900</a></span> | <span class="t">showed up there in the example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1752" target="_blank">00:29:12.380</a></span> | <span class="t">But the p matrix never differs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1753" target="_blank">00:29:13.820</a></span> | <span class="t">It's always exactly the same at every index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1756" target="_blank">00:29:16.300</a></span> | <span class="t">And so it's the only thing in the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1758" target="_blank">00:29:18.300</a></span> | <span class="t">that it correlates with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1759" target="_blank">00:29:19.260</a></span> | <span class="t">So you're learning it implicitly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1761" target="_blank">00:29:21.260</a></span> | <span class="t">This vector at index 1 is always at index 1 for every example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1764" target="_blank">00:29:24.540</a></span> | <span class="t">for every gradient update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1766" target="_blank">00:29:26.300</a></span> | <span class="t">And nothing else co-occurs like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1771" target="_blank">00:29:31.900</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1772" target="_blank">00:29:32.380</a></span> | <span class="t">So what do you end up learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1773" target="_blank">00:29:33.820</a></span> | <span class="t">I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1774" target="_blank">00:29:34.340</a></span> | <span class="t">It's unclear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1774" target="_blank">00:29:34.900</a></span> | <span class="t">But it definitely allows you to know, oh, this word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1777" target="_blank">00:29:37.820</a></span> | <span class="t">is with this index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1779" target="_blank">00:29:39.020</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1781" target="_blank">00:29:41.700</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1782" target="_blank">00:29:42.200</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1782" target="_blank">00:29:42.700</a></span> | <span class="t">Just quickly, when you say quadratic constant in space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1787" target="_blank">00:29:47.100</a></span> | <span class="t">is a sequence right now defined as a sequence?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1789" target="_blank">00:29:49.580</a></span> | <span class="t">Is there a sequence of words?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1791" target="_blank">00:29:51.180</a></span> | <span class="t">Or I'm trying to figure out what unit is using it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1797" target="_blank">00:29:57.300</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1797" target="_blank">00:29:57.800</a></span> | <span class="t">So the question is, when this is quadratic in the sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1800" target="_blank">00:30:00.300</a></span> | <span class="t">is that a sequence of words?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1801" target="_blank">00:30:01.420</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1801" target="_blank">00:30:01.920</a></span> | <span class="t">Think of it as a sequence of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1803" target="_blank">00:30:03.700</a></span> | <span class="t">Sometimes there'll be pieces that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1805" target="_blank">00:30:05.120</a></span> | <span class="t">are smaller than words, which we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1806" target="_blank">00:30:06.620</a></span> | <span class="t">go into in the next lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1808" target="_blank">00:30:08.180</a></span> | <span class="t">But yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1808" target="_blank">00:30:08.700</a></span> | <span class="t">Think of this as a sequence of words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1810" target="_blank">00:30:10.220</a></span> | <span class="t">but not necessarily just for a sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1812" target="_blank">00:30:12.100</a></span> | <span class="t">maybe for an entire paragraph, or an entire document,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1815" target="_blank">00:30:15.380</a></span> | <span class="t">or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1816" target="_blank">00:30:16.980</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1817" target="_blank">00:30:17.480</a></span> | <span class="t">But the attention is where it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1819" target="_blank">00:30:19.820</a></span> | <span class="t">Yeah, the attention is based words to words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1823" target="_blank">00:30:23.700</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1824" target="_blank">00:30:24.260</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1825" target="_blank">00:30:25.060</a></span> | <span class="t">I'm going to move on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1827" target="_blank">00:30:27.100</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1828" target="_blank">00:30:28.700</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1829" target="_blank">00:30:29.200</a></span> | <span class="t">So we have another problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1830" target="_blank">00:30:30.540</a></span> | <span class="t">Another is that, based on the presentation of self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1834" target="_blank">00:30:34.060</a></span> | <span class="t">that we've done, there's really no nonlinearities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1836" target="_blank">00:30:36.900</a></span> | <span class="t">for deep learning magic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1838" target="_blank">00:30:38.940</a></span> | <span class="t">We're just computing weighted averages of stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1843" target="_blank">00:30:43.420</a></span> | <span class="t">So if I apply self-attention, and then apply self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1847" target="_blank">00:30:47.820</a></span> | <span class="t">again, and then again, and again, and again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1850" target="_blank">00:30:50.660</a></span> | <span class="t">you should look at the next lecture notes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1852" target="_blank">00:30:52.820</a></span> | <span class="t">if you're interested in this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1853" target="_blank">00:30:53.540</a></span> | <span class="t">It's actually quite cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1854" target="_blank">00:30:54.580</a></span> | <span class="t">But what you end up doing is you're just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1856" target="_blank">00:30:56.280</a></span> | <span class="t">re-averaging value vectors together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1858" target="_blank">00:30:58.240</a></span> | <span class="t">So you're computing averages of value vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1860" target="_blank">00:31:00.940</a></span> | <span class="t">and it ends up looking like one big self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1863" target="_blank">00:31:03.700</a></span> | <span class="t">But there's an easy fix to this if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1865" target="_blank">00:31:05.400</a></span> | <span class="t">want the traditional deep learning magic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1867" target="_blank">00:31:07.980</a></span> | <span class="t">And you can just add a feed-forward network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1870" target="_blank">00:31:10.180</a></span> | <span class="t">to post-process each output vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1871" target="_blank">00:31:11.940</a></span> | <span class="t">So I've got a word here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1873" target="_blank">00:31:13.500</a></span> | <span class="t">That's the output of self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1875" target="_blank">00:31:15.460</a></span> | <span class="t">And I'm going to pass it through--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1877" target="_blank">00:31:17.460</a></span> | <span class="t">in this case, I'm calling it a multilayer perceptron MLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1880" target="_blank">00:31:20.380</a></span> | <span class="t">So this is a vector in Rd that's going to be--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1883" target="_blank">00:31:23.820</a></span> | <span class="t">and it's taking in as input a vector in Rd.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1886" target="_blank">00:31:26.420</a></span> | <span class="t">And you do the usual multilayer perceptron thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1890" target="_blank">00:31:30.020</a></span> | <span class="t">where you have the output, and you multiply it by a matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1892" target="_blank">00:31:32.460</a></span> | <span class="t">pass it through a nonlinearity, multiply it by another matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1896" target="_blank">00:31:36.020</a></span> | <span class="t">And so what this looks like in self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1898" target="_blank">00:31:38.300</a></span> | <span class="t">is that I've got this sentence, the chef who--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1900" target="_blank">00:31:40.580</a></span> | <span class="t">da, da, da, da, da-- food.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1902" target="_blank">00:31:42.140</a></span> | <span class="t">And I've got my embeddings for it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1904" target="_blank">00:31:44.040</a></span> | <span class="t">I pass it through this whole big self-attention block, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1906" target="_blank">00:31:46.780</a></span> | <span class="t">looks at the whole sequence and incorporates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1909" target="_blank">00:31:49.140</a></span> | <span class="t">context and all that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1910" target="_blank">00:31:50.540</a></span> | <span class="t">And then I pass each one individually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1912" target="_blank">00:31:52.660</a></span> | <span class="t">through a feed-forward layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1915" target="_blank">00:31:55.300</a></span> | <span class="t">So this embedding, that's the output of the self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1918" target="_blank">00:31:58.420</a></span> | <span class="t">for the word "the," is passed independently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1921" target="_blank">00:32:01.060</a></span> | <span class="t">through a multilayer perceptron here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1923" target="_blank">00:32:03.540</a></span> | <span class="t">And you can think of it as combining together or processing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1929" target="_blank">00:32:09.540</a></span> | <span class="t">the result of attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1931" target="_blank">00:32:11.600</a></span> | <span class="t">So there's a number of reasons why we do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1934" target="_blank">00:32:14.360</a></span> | <span class="t">One of them also is that you can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1936" target="_blank">00:32:16.020</a></span> | <span class="t">stack a ton of computation into these feed-forward networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1940" target="_blank">00:32:20.120</a></span> | <span class="t">very, very efficiently, very parallelizable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1942" target="_blank">00:32:22.600</a></span> | <span class="t">very good for GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1943" target="_blank">00:32:23.880</a></span> | <span class="t">But this is what's done in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1945" target="_blank">00:32:25.600</a></span> | <span class="t">So you do self-attention, and then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1947" target="_blank">00:32:27.280</a></span> | <span class="t">can pass it through this position-wise feed-forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1951" target="_blank">00:32:31.320</a></span> | <span class="t">layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1951" target="_blank">00:32:31.760</a></span> | <span class="t">Every word is processed independently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1954" target="_blank">00:32:34.000</a></span> | <span class="t">by this feed-forward network to process the result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1960" target="_blank">00:32:40.360</a></span> | <span class="t">So that's adding our classical deep learning nonlinearities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1963" target="_blank">00:32:43.740</a></span> | <span class="t">for self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1965" target="_blank">00:32:45.980</a></span> | <span class="t">And that's an easy fix for this no nonlinearities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1969" target="_blank">00:32:49.260</a></span> | <span class="t">problem in self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1970" target="_blank">00:32:50.920</a></span> | <span class="t">And then we have a last issue before we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1972" target="_blank">00:32:52.860</a></span> | <span class="t">have our final minimal self-attention building block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1976" target="_blank">00:32:56.300</a></span> | <span class="t">with which we can replace RNNs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1979" target="_blank">00:32:59.740</a></span> | <span class="t">And that's that-- well, when I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1982" target="_blank">00:33:02.100</a></span> | <span class="t">been writing out all of these examples of self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1984" target="_blank">00:33:04.940</a></span> | <span class="t">you can look at the entire sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1987" target="_blank">00:33:07.700</a></span> | <span class="t">And in practice, for some tasks, such as machine translation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1992" target="_blank">00:33:12.400</a></span> | <span class="t">or language modeling, whenever you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1993" target="_blank">00:33:13.940</a></span> | <span class="t">want to define a probability distribution over a sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1996" target="_blank">00:33:16.700</a></span> | <span class="t">you can't cheat and look at the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=1998" target="_blank">00:33:18.820</a></span> | <span class="t">So at every time step, I could define the set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2004" target="_blank">00:33:24.860</a></span> | <span class="t">of keys and queries and values to only include past words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2009" target="_blank">00:33:29.260</a></span> | <span class="t">But this is inefficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2011" target="_blank">00:33:31.140</a></span> | <span class="t">Bear with me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2011" target="_blank">00:33:31.780</a></span> | <span class="t">It's inefficient because you can't parallelize it so well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2014" target="_blank">00:33:34.660</a></span> | <span class="t">So instead, we compute the entire n by n matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2017" target="_blank">00:33:37.940</a></span> | <span class="t">just like I showed in the slide discussing self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2021" target="_blank">00:33:41.220</a></span> | <span class="t">And then I mask out words in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2022" target="_blank">00:33:42.940</a></span> | <span class="t">So for this score, eij--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2025" target="_blank">00:33:45.500</a></span> | <span class="t">and I computed eij for all n by n pairs of words--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2029" target="_blank">00:33:49.900</a></span> | <span class="t">is equal to whatever it was before if the word that you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2034" target="_blank">00:33:54.820</a></span> | <span class="t">looking at, index j, is an index that is less than or equal to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2039" target="_blank">00:33:59.380</a></span> | <span class="t">where you are, index i.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2041" target="_blank">00:34:01.740</a></span> | <span class="t">And it's equal to negative infinity-ish otherwise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2044" target="_blank">00:34:04.820</a></span> | <span class="t">if it's in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2046" target="_blank">00:34:06.220</a></span> | <span class="t">And when you softmax the eij, negative infinity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2048" target="_blank">00:34:08.640</a></span> | <span class="t">gets mapped to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2051" target="_blank">00:34:11.220</a></span> | <span class="t">So now my attention is weighted 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2053" target="_blank">00:34:13.980</a></span> | <span class="t">My weighted average is 0 on the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2056" target="_blank">00:34:16.500</a></span> | <span class="t">So I can't look at it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2058" target="_blank">00:34:18.740</a></span> | <span class="t">What does this look like?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2060" target="_blank">00:34:20.020</a></span> | <span class="t">So in order to encode these words, the chef who--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2064" target="_blank">00:34:24.140</a></span> | <span class="t">maybe the start symbol there--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2067" target="_blank">00:34:27.940</a></span> | <span class="t">I can look at these words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2069" target="_blank">00:34:29.940</a></span> | <span class="t">That's all pairs of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2071" target="_blank">00:34:31.620</a></span> | <span class="t">And then I just gray out--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2072" target="_blank">00:34:32.700</a></span> | <span class="t">I negative infinity out the words I can't look at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2075" target="_blank">00:34:35.860</a></span> | <span class="t">So when encoding the start symbol,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2077" target="_blank">00:34:37.300</a></span> | <span class="t">I can just look at the start symbol.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2079" target="_blank">00:34:39.260</a></span> | <span class="t">When encoding the, I can look at the start symbol and the.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2083" target="_blank">00:34:43.220</a></span> | <span class="t">When encoding chef, I can look at start the chef.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2086" target="_blank">00:34:46.220</a></span> | <span class="t">But I can't look at who.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2088" target="_blank">00:34:48.960</a></span> | <span class="t">And so with this representation of chef that is only looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2093" target="_blank">00:34:53.780</a></span> | <span class="t">at start the chef, I can define a probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2097" target="_blank">00:34:57.940</a></span> | <span class="t">using this vector that allows me to predict who</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2101" target="_blank">00:35:01.100</a></span> | <span class="t">without having cheated by already looking ahead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2103" target="_blank">00:35:03.180</a></span> | <span class="t">and seeing that, well, who is the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2105" target="_blank">00:35:05.660</a></span> | <span class="t">Questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2111" target="_blank">00:35:11.900</a></span> | <span class="t">So it says for using it in decoders.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2115" target="_blank">00:35:15.020</a></span> | <span class="t">Do we do this for both the encoding layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2117" target="_blank">00:35:17.020</a></span> | <span class="t">and the decoding layer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2118" target="_blank">00:35:18.140</a></span> | <span class="t">Or for the encoding layer, are we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2119" target="_blank">00:35:19.700</a></span> | <span class="t">allowing ourselves to look for--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2121" target="_blank">00:35:21.700</a></span> | <span class="t">The question is, it says here that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2123" target="_blank">00:35:23.580</a></span> | <span class="t">using this in a decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2124" target="_blank">00:35:24.540</a></span> | <span class="t">Do we also use it in the encoder?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2126" target="_blank">00:35:26.700</a></span> | <span class="t">So this is the distinction between a bidirectional LSTM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2131" target="_blank">00:35:31.060</a></span> | <span class="t">and a unidirectional LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2133" target="_blank">00:35:33.140</a></span> | <span class="t">So wherever you don't need this constraint,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2137" target="_blank">00:35:37.100</a></span> | <span class="t">you probably don't use it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2138" target="_blank">00:35:38.180</a></span> | <span class="t">So if you're using an encoder on the source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2140" target="_blank">00:35:40.180</a></span> | <span class="t">sentence of your machine translation problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2142" target="_blank">00:35:42.460</a></span> | <span class="t">you probably don't do this masking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2144" target="_blank">00:35:44.380</a></span> | <span class="t">because it's probably good to let everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2146" target="_blank">00:35:46.220</a></span> | <span class="t">look at each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2147" target="_blank">00:35:47.140</a></span> | <span class="t">And then whenever you do need to use it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2148" target="_blank">00:35:48.800</a></span> | <span class="t">because you have this autoregressive probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2151" target="_blank">00:35:51.620</a></span> | <span class="t">of word one, probability of two given one, three given two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2154" target="_blank">00:35:54.980</a></span> | <span class="t">and one, then you would use this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2156" target="_blank">00:35:56.340</a></span> | <span class="t">So traditionally, yes, in decoders, you will use it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2158" target="_blank">00:35:58.740</a></span> | <span class="t">In encoders, you will not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2162" target="_blank">00:36:02.260</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2164" target="_blank">00:36:04.380</a></span> | <span class="t">My question is a little bit philosophical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2167" target="_blank">00:36:07.500</a></span> | <span class="t">How humans actually generate sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2170" target="_blank">00:36:10.780</a></span> | <span class="t">by having some notion of the probability of future words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2174" target="_blank">00:36:14.980</a></span> | <span class="t">before they say the words that--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2179" target="_blank">00:36:19.020</a></span> | <span class="t">or before they choose the words that they are currently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2184" target="_blank">00:36:24.620</a></span> | <span class="t">speaking or writing, generating?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2186" target="_blank">00:36:26.940</a></span> | <span class="t">Good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2187" target="_blank">00:36:27.520</a></span> | <span class="t">So the question is, isn't looking ahead a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2190" target="_blank">00:36:30.460</a></span> | <span class="t">and predicting or getting an idea of the words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2192" target="_blank">00:36:32.900</a></span> | <span class="t">that you might say in the future sort of how humans generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2195" target="_blank">00:36:35.460</a></span> | <span class="t">language instead of the strict constraint of not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2198" target="_blank">00:36:38.660</a></span> | <span class="t">seeing it into the future?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2199" target="_blank">00:36:39.940</a></span> | <span class="t">Is that what you're--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2200" target="_blank">00:36:40.740</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2201" target="_blank">00:36:41.580</a></span> | <span class="t">So right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2203" target="_blank">00:36:43.180</a></span> | <span class="t">Trying to plan ahead to see what I should do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2206" target="_blank">00:36:46.420</a></span> | <span class="t">is definitely an interesting idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2208" target="_blank">00:36:48.820</a></span> | <span class="t">But when I am training the network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2211" target="_blank">00:36:51.180</a></span> | <span class="t">I can't-- if I'm teaching it to try to predict the next word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2215" target="_blank">00:36:55.460</a></span> | <span class="t">and if I give it the answer, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2217" target="_blank">00:36:57.100</a></span> | <span class="t">not going to learn anything useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2219" target="_blank">00:36:59.820</a></span> | <span class="t">So in practice, when I'm generating text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2221" target="_blank">00:37:01.680</a></span> | <span class="t">maybe it would be a good idea to make some guesses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2223" target="_blank">00:37:03.980</a></span> | <span class="t">far into the future or have a high-level plan or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2227" target="_blank">00:37:07.700</a></span> | <span class="t">But in training the network, I can't encode that intuition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2231" target="_blank">00:37:11.180</a></span> | <span class="t">about how humans build--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2233" target="_blank">00:37:13.580</a></span> | <span class="t">like, generate sequences of language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2235" target="_blank">00:37:15.260</a></span> | <span class="t">by just giving it the answer of the future</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2237" target="_blank">00:37:17.020</a></span> | <span class="t">directly, at least, because then it's just too easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2239" target="_blank">00:37:19.820</a></span> | <span class="t">There's nothing to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2241" target="_blank">00:37:21.900</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2242" target="_blank">00:37:22.380</a></span> | <span class="t">But there might be interesting ideas about maybe giving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2244" target="_blank">00:37:24.460</a></span> | <span class="t">the network a hint as to what kind of thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2246" target="_blank">00:37:26.540</a></span> | <span class="t">could come next, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2248" target="_blank">00:37:28.220</a></span> | <span class="t">But that's out of scope for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2249" target="_blank">00:37:29.660</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2251" target="_blank">00:37:31.180</a></span> | <span class="t">Yeah, question over here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2252" target="_blank">00:37:32.220</a></span> | <span class="t">So I understand why we want to mask the future for stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2255" target="_blank">00:37:35.820</a></span> | <span class="t">like language models, but how does it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2257" target="_blank">00:37:37.460</a></span> | <span class="t">apply to machine translation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2259" target="_blank">00:37:39.260</a></span> | <span class="t">Like, why would we use it there?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2260" target="_blank">00:37:40.500</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2261" target="_blank">00:37:41.000</a></span> | <span class="t">So in machine translation--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2263" target="_blank">00:37:43.500</a></span> | <span class="t">I'm going to come over to this board</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2266" target="_blank">00:37:46.020</a></span> | <span class="t">and hopefully get a better marker.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2269" target="_blank">00:37:49.380</a></span> | <span class="t">Nice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2270" target="_blank">00:37:50.020</a></span> | <span class="t">In machine translation, I have a sentence like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2274" target="_blank">00:37:54.980</a></span> | <span class="t">"I like pizza."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2279" target="_blank">00:37:59.500</a></span> | <span class="t">And I want to be able to translate it--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2284" target="_blank">00:38:04.820</a></span> | <span class="t">"Je me pizza."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2288" target="_blank">00:38:08.820</a></span> | <span class="t">Nice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2289" target="_blank">00:38:09.940</a></span> | <span class="t">And so when I'm looking at "I like pizza,"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2294" target="_blank">00:38:14.980</a></span> | <span class="t">I get this as the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2296" target="_blank">00:38:16.380</a></span> | <span class="t">And so I want self-attention without masking,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2302" target="_blank">00:38:22.820</a></span> | <span class="t">because I want "I" to look at "like" and "I" to look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2306" target="_blank">00:38:26.660</a></span> | <span class="t">"pizza" and "like" to look at "pizza."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2309" target="_blank">00:38:29.100</a></span> | <span class="t">And then when I'm generating this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2311" target="_blank">00:38:31.100</a></span> | <span class="t">if my tokens are like "Je m la pizza,"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2315" target="_blank">00:38:35.180</a></span> | <span class="t">I want to, in encoding this word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2317" target="_blank">00:38:37.740</a></span> | <span class="t">I want to be able to look only at myself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2320" target="_blank">00:38:40.600</a></span> | <span class="t">And we'll talk about encoder-decoder architectures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2323" target="_blank">00:38:43.100</a></span> | <span class="t">in this later in the lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2326" target="_blank">00:38:46.120</a></span> | <span class="t">But I want to be able to look at myself, none of the future,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2328" target="_blank">00:38:48.620</a></span> | <span class="t">and all of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2330" target="_blank">00:38:50.020</a></span> | <span class="t">And so what I'm talking about right now in this masking case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2332" target="_blank">00:38:52.780</a></span> | <span class="t">is masking out with negative infinity all of these words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2339" target="_blank">00:38:59.380</a></span> | <span class="t">So that attention score from "Je" to everything else</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2342" target="_blank">00:39:02.540</a></span> | <span class="t">should be negative infinity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2345" target="_blank">00:39:05.820</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2346" target="_blank">00:39:06.300</a></span> | <span class="t">Does that answer your question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2347" target="_blank">00:39:07.780</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2349" target="_blank">00:39:09.380</a></span> | <span class="t">OK, let's move ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2351" target="_blank">00:39:11.380</a></span> | <span class="t">OK, so that was our last big building block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2356" target="_blank">00:39:16.500</a></span> | <span class="t">issue with self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2357" target="_blank">00:39:17.660</a></span> | <span class="t">So this is what I would call--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2359" target="_blank">00:39:19.220</a></span> | <span class="t">and this is my personal opinion-- a minimal self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2362" target="_blank">00:39:22.220</a></span> | <span class="t">building block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2362" target="_blank">00:39:22.900</a></span> | <span class="t">You have self-attention, the basis of the method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2365" target="_blank">00:39:25.620</a></span> | <span class="t">So that's sort of here in the red.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2369" target="_blank">00:39:29.260</a></span> | <span class="t">And maybe we had the inputs to the sequence here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2371" target="_blank">00:39:31.980</a></span> | <span class="t">And then you embed it with that embedding matrix E.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2374" target="_blank">00:39:34.620</a></span> | <span class="t">And then you add position embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2376" target="_blank">00:39:36.780</a></span> | <span class="t">And then these three arrows represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2378" target="_blank">00:39:38.580</a></span> | <span class="t">using the key, the value, and the query that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2382" target="_blank">00:39:42.980</a></span> | <span class="t">sort of stylized there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2384" target="_blank">00:39:44.140</a></span> | <span class="t">This is often how you see these diagrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2387" target="_blank">00:39:47.300</a></span> | <span class="t">And so you pass it to self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2390" target="_blank">00:39:50.420</a></span> | <span class="t">with the position representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2393" target="_blank">00:39:53.100</a></span> | <span class="t">So that specifies the sequence order,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2394" target="_blank">00:39:54.980</a></span> | <span class="t">because otherwise you'd have no idea what order the words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2397" target="_blank">00:39:57.460</a></span> | <span class="t">showed up in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2399" target="_blank">00:39:59.260</a></span> | <span class="t">You have the nonlinearities in sort of the TLFeedForward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2401" target="_blank">00:40:01.940</a></span> | <span class="t">network there to sort of provide that sort of squashing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2405" target="_blank">00:40:05.820</a></span> | <span class="t">and sort of deep learning expressivity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2408" target="_blank">00:40:08.900</a></span> | <span class="t">And then you have masking in order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2410" target="_blank">00:40:10.700</a></span> | <span class="t">to have parallelizable operations that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2413" target="_blank">00:40:13.500</a></span> | <span class="t">don't look at the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2415" target="_blank">00:40:15.460</a></span> | <span class="t">So this is sort of our minimal architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2418" target="_blank">00:40:18.100</a></span> | <span class="t">And then up at the top above here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2420" target="_blank">00:40:20.180</a></span> | <span class="t">so you have this thing-- maybe you repeat this sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2422" target="_blank">00:40:22.380</a></span> | <span class="t">self-attention and feedforward many times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2424" target="_blank">00:40:24.460</a></span> | <span class="t">So self-attention, feedforward, self-attention, feedforward,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2427" target="_blank">00:40:27.380</a></span> | <span class="t">self-attention, feedforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2428" target="_blank">00:40:28.940</a></span> | <span class="t">That's what I'm calling this block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2431" target="_blank">00:40:31.140</a></span> | <span class="t">And then maybe at the end of it, you predict something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2433" target="_blank">00:40:33.540</a></span> | <span class="t">I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2433" target="_blank">00:40:33.860</a></span> | <span class="t">We haven't really talked about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2435" target="_blank">00:40:35.340</a></span> | <span class="t">But you have these representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2436" target="_blank">00:40:36.940</a></span> | <span class="t">And then you predict the next word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2438" target="_blank">00:40:38.500</a></span> | <span class="t">or you predict the sentiment, or you predict whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2440" target="_blank">00:40:40.740</a></span> | <span class="t">So this is like a self-attention architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2444" target="_blank">00:40:44.640</a></span> | <span class="t">OK, we're going to move on to the transformer next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2446" target="_blank">00:40:46.760</a></span> | <span class="t">So if there are any questions-- yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2448" target="_blank">00:40:48.260</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2452" target="_blank">00:40:52.180</a></span> | <span class="t">Other way around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2453" target="_blank">00:40:53.380</a></span> | <span class="t">We will use masking for decoders,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2456" target="_blank">00:40:56.140</a></span> | <span class="t">where I want to decode out a sequence where I have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2460" target="_blank">00:41:00.420</a></span> | <span class="t">an informational constraint, where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2462" target="_blank">00:41:02.380</a></span> | <span class="t">to represent this word properly, I cannot have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2465" target="_blank">00:41:05.140</a></span> | <span class="t">the information of the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2466" target="_blank">00:41:06.460</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2468" target="_blank">00:41:08.580</a></span> | <span class="t">Yeah, OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2469" target="_blank">00:41:09.080</a></span> | <span class="t">OK, great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2476" target="_blank">00:41:16.220</a></span> | <span class="t">So now let's talk about the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2477" target="_blank">00:41:17.860</a></span> | <span class="t">So what I've pitched to you is what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2480" target="_blank">00:41:20.660</a></span> | <span class="t">I call a minimal self-attention architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2484" target="_blank">00:41:24.740</a></span> | <span class="t">And I quite like pitching it that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2488" target="_blank">00:41:28.980</a></span> | <span class="t">But really, no one uses the architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2490" target="_blank">00:41:30.820</a></span> | <span class="t">that was just up on the slide, the previous slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2494" target="_blank">00:41:34.100</a></span> | <span class="t">It doesn't work quite as well as it could.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2496" target="_blank">00:41:36.060</a></span> | <span class="t">And there's a bunch of important details</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2498" target="_blank">00:41:38.660</a></span> | <span class="t">that we'll talk about now that goes into the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2501" target="_blank">00:41:41.700</a></span> | <span class="t">What I would hope, though, to have you take away from that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2506" target="_blank">00:41:46.220</a></span> | <span class="t">is that the transformer architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2508" target="_blank">00:41:48.020</a></span> | <span class="t">as I'll present it now, is not necessarily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2511" target="_blank">00:41:51.420</a></span> | <span class="t">the end point of our search for better and better ways</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2514" target="_blank">00:41:54.580</a></span> | <span class="t">of representing language, even though it's now ubiquitous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2517" target="_blank">00:41:57.940</a></span> | <span class="t">and has been for a couple of years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2520" target="_blank">00:42:00.060</a></span> | <span class="t">So think about these sort of ideas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2521" target="_blank">00:42:01.500</a></span> | <span class="t">of the problems of using self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2525" target="_blank">00:42:05.020</a></span> | <span class="t">and maybe ways of fixing some of the issues with transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2528" target="_blank">00:42:08.940</a></span> | <span class="t">OK, so a transformer decoder is how we'll build systems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2533" target="_blank">00:42:13.500</a></span> | <span class="t">like language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2534" target="_blank">00:42:14.460</a></span> | <span class="t">And so we've discussed this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2535" target="_blank">00:42:15.740</a></span> | <span class="t">It's like our decoder with our self-attention-only sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2538" target="_blank">00:42:18.940</a></span> | <span class="t">of minimal architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2540" target="_blank">00:42:20.300</a></span> | <span class="t">It's got a couple of extra components,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2541" target="_blank">00:42:21.940</a></span> | <span class="t">some of which I've grayed out here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2543" target="_blank">00:42:23.400</a></span> | <span class="t">that we'll go over one by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2545" target="_blank">00:42:25.220</a></span> | <span class="t">The first that's actually different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2548" target="_blank">00:42:28.900</a></span> | <span class="t">is that we'll replace our self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2551" target="_blank">00:42:31.660</a></span> | <span class="t">with masking with masked multi-head self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2555" target="_blank">00:42:35.820</a></span> | <span class="t">This ends up being crucial.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2556" target="_blank">00:42:36.940</a></span> | <span class="t">It's probably the most important distinction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2559" target="_blank">00:42:39.820</a></span> | <span class="t">between the transformer and this sort of minimal architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2562" target="_blank">00:42:42.360</a></span> | <span class="t">that I've presented.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2563" target="_blank">00:42:43.820</a></span> | <span class="t">So let's come back to our toy example of attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2566" target="_blank">00:42:46.740</a></span> | <span class="t">where we've been trying to represent the word learned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2569" target="_blank">00:42:49.060</a></span> | <span class="t">in the context of the sequence, I went to Stanford CS224N</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2572" target="_blank">00:42:52.660</a></span> | <span class="t">and learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2574" target="_blank">00:42:54.740</a></span> | <span class="t">And I was sort of giving these teal bars to say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2577" target="_blank">00:42:57.580</a></span> | <span class="t">oh, maybe intuitively you look at various things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2581" target="_blank">00:43:01.020</a></span> | <span class="t">to build up your representation of learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2584" target="_blank">00:43:04.340</a></span> | <span class="t">But really, there are varying ways</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2586" target="_blank">00:43:06.540</a></span> | <span class="t">in which I want to look back at the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2589" target="_blank">00:43:09.660</a></span> | <span class="t">to see varying sort of aspects of information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2593" target="_blank">00:43:13.660</a></span> | <span class="t">that I want to incorporate into my representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2596" target="_blank">00:43:16.300</a></span> | <span class="t">So maybe in this way, I sort of want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2599" target="_blank">00:43:19.340</a></span> | <span class="t">to look at Stanford CS224N, because, oh, it's like entities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2605" target="_blank">00:43:25.900</a></span> | <span class="t">You learn different stuff at Stanford CS224N</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2608" target="_blank">00:43:28.300</a></span> | <span class="t">than you do at other courses or other universities or whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2611" target="_blank">00:43:31.940</a></span> | <span class="t">And so maybe I want to look here for this reason.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2615" target="_blank">00:43:35.180</a></span> | <span class="t">And maybe in another sense, I actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2617" target="_blank">00:43:37.700</a></span> | <span class="t">want to look at the word learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2619" target="_blank">00:43:39.380</a></span> | <span class="t">And I want to look at I. I went and learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2623" target="_blank">00:43:43.380</a></span> | <span class="t">And I want to see maybe syntactically relevant words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2626" target="_blank">00:43:46.300</a></span> | <span class="t">It's very different reasons for which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2627" target="_blank">00:43:47.940</a></span> | <span class="t">I might want to look at different things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2629" target="_blank">00:43:49.640</a></span> | <span class="t">in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2630" target="_blank">00:43:50.900</a></span> | <span class="t">And so trying to average it all out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2632" target="_blank">00:43:52.740</a></span> | <span class="t">with a single operation of self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2635" target="_blank">00:43:55.020</a></span> | <span class="t">ends up being maybe somewhat too difficult in a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2638" target="_blank">00:43:58.340</a></span> | <span class="t">that will make precise in assignment 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2640" target="_blank">00:44:00.180</a></span> | <span class="t">Nice, we'll do a little bit more math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2643" target="_blank">00:44:03.940</a></span> | <span class="t">OK, so any questions about this intuition?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2651" target="_blank">00:44:11.580</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2654" target="_blank">00:44:14.340</a></span> | <span class="t">Yeah, so it should be an application of attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2657" target="_blank">00:44:17.140</a></span> | <span class="t">just as I've presented it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2659" target="_blank">00:44:19.140</a></span> | <span class="t">So one independent define the keys, define the queries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2662" target="_blank">00:44:22.300</a></span> | <span class="t">define the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2663" target="_blank">00:44:23.020</a></span> | <span class="t">I'll define it more precisely here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2664" target="_blank">00:44:24.940</a></span> | <span class="t">But think of it as I do attention once,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2667" target="_blank">00:44:27.460</a></span> | <span class="t">and then I do it again with different parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2671" target="_blank">00:44:31.620</a></span> | <span class="t">being able to look at different things, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2673" target="_blank">00:44:33.740</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2676" target="_blank">00:44:36.220</a></span> | <span class="t">How do we ensure that they look at different things?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2678" target="_blank">00:44:38.660</a></span> | <span class="t">We do not-- OK, so the question is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2680" target="_blank">00:44:40.060</a></span> | <span class="t">if we have two separate sets of weights trying to learn,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2681" target="_blank">00:44:41.980</a></span> | <span class="t">say, to do this and to do that, how do we ensure that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2684" target="_blank">00:44:44.700</a></span> | <span class="t">learn different things?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2685" target="_blank">00:44:45.940</a></span> | <span class="t">We do not ensure that they learn different things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2689" target="_blank">00:44:49.100</a></span> | <span class="t">And in practice, they do, although not perfectly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2692" target="_blank">00:44:52.780</a></span> | <span class="t">So it ends up being the case that you have some redundancy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2695" target="_blank">00:44:55.660</a></span> | <span class="t">and you can cut out some of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2697" target="_blank">00:44:57.420</a></span> | <span class="t">But that's out of scope for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2699" target="_blank">00:44:59.140</a></span> | <span class="t">But we hope, just like we hope that different dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2702" target="_blank">00:45:02.300</a></span> | <span class="t">in our feedforward layers will learn different things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2704" target="_blank">00:45:04.500</a></span> | <span class="t">because of lack of symmetry and whatever,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2706" target="_blank">00:45:06.740</a></span> | <span class="t">that we hope that the heads will start to specialize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2709" target="_blank">00:45:09.380</a></span> | <span class="t">And that will mean they'll specialize even more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2711" target="_blank">00:45:11.460</a></span> | <span class="t">And yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2714" target="_blank">00:45:14.380</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2716" target="_blank">00:45:16.340</a></span> | <span class="t">All right, so in order to discuss multi-head self</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2718" target="_blank">00:45:18.620</a></span> | <span class="t">attention well, we really need to talk about the matrices,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2722" target="_blank">00:45:22.100</a></span> | <span class="t">how we're going to implement this in GPUs efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2725" target="_blank">00:45:25.220</a></span> | <span class="t">We're going to talk about the sequence-stacked form</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2727" target="_blank">00:45:27.780</a></span> | <span class="t">of attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2729" target="_blank">00:45:29.260</a></span> | <span class="t">So we've been talking about each word sort of individually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2731" target="_blank">00:45:31.660</a></span> | <span class="t">as a vector in dimensionality D. But really, we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2735" target="_blank">00:45:35.140</a></span> | <span class="t">going to be working on these as big matrices that are stacked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2738" target="_blank">00:45:38.900</a></span> | <span class="t">So I take all of my word embeddings, x1 to xn,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2742" target="_blank">00:45:42.340</a></span> | <span class="t">and I stack them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2743" target="_blank">00:45:43.900</a></span> | <span class="t">And now I have a big matrix that is in dimensionality Rn by D.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2749" target="_blank">00:45:49.860</a></span> | <span class="t">OK, and now with my matrices K, Q, and V,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2755" target="_blank">00:45:55.180</a></span> | <span class="t">I can just multiply them on this side of x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2758" target="_blank">00:45:58.220</a></span> | <span class="t">So x is Rn by D. K is Rd by D. So n by D times d by D</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2764" target="_blank">00:46:04.300</a></span> | <span class="t">gives you n by D again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2767" target="_blank">00:46:07.060</a></span> | <span class="t">So I can just compute a big matrix multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2770" target="_blank">00:46:10.460</a></span> | <span class="t">on my whole sequence to multiply each one of the words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2773" target="_blank">00:46:13.340</a></span> | <span class="t">of my key query and value matrices very efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2777" target="_blank">00:46:17.100</a></span> | <span class="t">So this is sort of this vectorization idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2778" target="_blank">00:46:18.860</a></span> | <span class="t">I don't want to for loop over the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2780" target="_blank">00:46:20.780</a></span> | <span class="t">I represent the sequence as a big matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2783" target="_blank">00:46:23.540</a></span> | <span class="t">and I just do one big matrix multiply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2787" target="_blank">00:46:27.580</a></span> | <span class="t">Then the output is defined as this sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2789" target="_blank">00:46:29.440</a></span> | <span class="t">of inscrutable bit of math, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2791" target="_blank">00:46:31.660</a></span> | <span class="t">I'm going to go over visually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2795" target="_blank">00:46:35.260</a></span> | <span class="t">So first, we're going to take the key query dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2797" target="_blank">00:46:37.820</a></span> | <span class="t">products in one matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2799" target="_blank">00:46:39.460</a></span> | <span class="t">So we've got xq, which is Rn by D.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2807" target="_blank">00:46:47.100</a></span> | <span class="t">And I've got xk transpose, which is Rd by n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2810" target="_blank">00:46:50.660</a></span> | <span class="t">So n by D, d by n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2813" target="_blank">00:46:53.140</a></span> | <span class="t">This is computing all of the eij's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2815" target="_blank">00:46:55.400</a></span> | <span class="t">these scores for self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2818" target="_blank">00:46:58.100</a></span> | <span class="t">So this is all pairs of attention scores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2820" target="_blank">00:47:00.620</a></span> | <span class="t">computed in one big matrix multiply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2823" target="_blank">00:47:03.580</a></span> | <span class="t">OK?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2824" target="_blank">00:47:04.580</a></span> | <span class="t">So this is this big matrix here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2826" target="_blank">00:47:06.180</a></span> | <span class="t">Next, I use the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2829" target="_blank">00:47:09.620</a></span> | <span class="t">So I softmax this over the second dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2833" target="_blank">00:47:13.860</a></span> | <span class="t">the second n dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2835" target="_blank">00:47:15.980</a></span> | <span class="t">And I get my sort of normalized scores,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2839" target="_blank">00:47:19.060</a></span> | <span class="t">and then I multiply with xv.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2840" target="_blank">00:47:20.700</a></span> | <span class="t">So this is an n by n matrix multiplied by an n by D matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2846" target="_blank">00:47:26.180</a></span> | <span class="t">And what do I get?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2846" target="_blank">00:47:26.900</a></span> | <span class="t">Well, this is just doing the weighted average.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2849" target="_blank">00:47:29.540</a></span> | <span class="t">So this is one big weighted average.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2852" target="_blank">00:47:32.340</a></span> | <span class="t">Big weighted average contribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2854" target="_blank">00:47:34.220</a></span> | <span class="t">on the whole matrix, giving me my whole self-attention output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2857" target="_blank">00:47:37.380</a></span> | <span class="t">in Rn by D. So I've just restated identically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2861" target="_blank">00:47:41.620</a></span> | <span class="t">the self-attention operations, but computed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2863" target="_blank">00:47:43.980</a></span> | <span class="t">in terms of matrices so that you could do this efficiently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2867" target="_blank">00:47:47.220</a></span> | <span class="t">on a GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2867" target="_blank">00:47:47.860</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2872" target="_blank">00:47:52.060</a></span> | <span class="t">So multi-headed attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2874" target="_blank">00:47:54.180</a></span> | <span class="t">This is going to give us--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2875" target="_blank">00:47:55.500</a></span> | <span class="t">and it's going to be important to compute this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2877" target="_blank">00:47:57.460</a></span> | <span class="t">in terms of the matrices, which we'll see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2879" target="_blank">00:47:59.580</a></span> | <span class="t">This is going to give us the ability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2881" target="_blank">00:48:01.000</a></span> | <span class="t">to look in multiple places at once for different reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2884" target="_blank">00:48:04.220</a></span> | <span class="t">So for self-attention looks where this dot product here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2889" target="_blank">00:48:09.060</a></span> | <span class="t">is high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2890" target="_blank">00:48:10.580</a></span> | <span class="t">This xi, the Q matrix, the key matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2895" target="_blank">00:48:15.380</a></span> | <span class="t">But maybe we want to look in different places</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2898" target="_blank">00:48:18.020</a></span> | <span class="t">for different reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2899" target="_blank">00:48:19.300</a></span> | <span class="t">So we actually define multiple query, key, and value matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2904" target="_blank">00:48:24.540</a></span> | <span class="t">So I'm going to have a bunch of heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2906" target="_blank">00:48:26.740</a></span> | <span class="t">I'm going to have h self-attention heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2910" target="_blank">00:48:30.260</a></span> | <span class="t">And for each head, I'm going to define an independent query,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2913" target="_blank">00:48:33.060</a></span> | <span class="t">key, and value matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2914" target="_blank">00:48:34.860</a></span> | <span class="t">And I'm going to say that its shape is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2917" target="_blank">00:48:37.220</a></span> | <span class="t">going to map from the model dimensionality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2919" target="_blank">00:48:39.340</a></span> | <span class="t">to the model dimensionality over h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2921" target="_blank">00:48:41.300</a></span> | <span class="t">So each one of these is doing projection down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2923" target="_blank">00:48:43.260</a></span> | <span class="t">to a lower dimensional space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2925" target="_blank">00:48:45.340</a></span> | <span class="t">This is going to be for computational efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2927" target="_blank">00:48:47.700</a></span> | <span class="t">And I'll just apply self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2931" target="_blank">00:48:51.260</a></span> | <span class="t">independently for each output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2933" target="_blank">00:48:53.300</a></span> | <span class="t">So this equation here is identical to the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2936" target="_blank">00:48:56.100</a></span> | <span class="t">we saw for single-headed self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2938" target="_blank">00:48:58.540</a></span> | <span class="t">except I've got these sort of l indices everywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2942" target="_blank">00:49:02.860</a></span> | <span class="t">So I've got this lower dimensional thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2944" target="_blank">00:49:04.540</a></span> | <span class="t">I'm mapping to a lower dimensional space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2946" target="_blank">00:49:06.540</a></span> | <span class="t">And then I do have my lower dimensional value vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2949" target="_blank">00:49:09.100</a></span> | <span class="t">there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2949" target="_blank">00:49:09.700</a></span> | <span class="t">So my output is an rd by h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2951" target="_blank">00:49:11.900</a></span> | <span class="t">But really, you're doing exactly the same kind of operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2954" target="_blank">00:49:14.860</a></span> | <span class="t">I'm just doing it h different times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2957" target="_blank">00:49:17.620</a></span> | <span class="t">And then you combine the outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2959" target="_blank">00:49:19.700</a></span> | <span class="t">So I've done sort of look in different places</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2962" target="_blank">00:49:22.140</a></span> | <span class="t">with the different key, query, and value matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2964" target="_blank">00:49:24.900</a></span> | <span class="t">And then I get each of their outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2968" target="_blank">00:49:28.340</a></span> | <span class="t">And then I concatenate them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2971" target="_blank">00:49:31.020</a></span> | <span class="t">So each one is dimensionality d by h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2973" target="_blank">00:49:33.540</a></span> | <span class="t">And I concatenate them together and then sort of mix them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2976" target="_blank">00:49:36.140</a></span> | <span class="t">together with the final linear transformation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2979" target="_blank">00:49:39.820</a></span> | <span class="t">And so each head gets to look at different things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2983" target="_blank">00:49:43.040</a></span> | <span class="t">and construct their value vectors differently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2985" target="_blank">00:49:45.420</a></span> | <span class="t">And then I sort of combine the result all together at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2989" target="_blank">00:49:49.500</a></span> | <span class="t">Let's go through this visually, because it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2991" target="_blank">00:49:51.660</a></span> | <span class="t">at least helpful for me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2995" target="_blank">00:49:55.820</a></span> | <span class="t">It's actually not more costly to do this, really,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=2998" target="_blank">00:49:58.540</a></span> | <span class="t">than it is to compute a single head of self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3001" target="_blank">00:50:01.060</a></span> | <span class="t">And we'll see through the pictures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3002" target="_blank">00:50:02.520</a></span> | <span class="t">So in single-headed self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3007" target="_blank">00:50:07.940</a></span> | <span class="t">we computed xq.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3009" target="_blank">00:50:09.460</a></span> | <span class="t">And in multi-headed self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3011" target="_blank">00:50:11.100</a></span> | <span class="t">we'll also compute xq the same way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3013" target="_blank">00:50:13.860</a></span> | <span class="t">So xq is rn by d.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3016" target="_blank">00:50:16.260</a></span> | <span class="t">And then we can reshape it into rn, that's sequence length,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3024" target="_blank">00:50:24.500</a></span> | <span class="t">times the number of heads, times the model dimensionality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3029" target="_blank">00:50:29.420</a></span> | <span class="t">over the number of heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3030" target="_blank">00:50:30.500</a></span> | <span class="t">So I've just reshaped it to say, now I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3032" target="_blank">00:50:32.580</a></span> | <span class="t">got a big three-axis tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3035" target="_blank">00:50:35.540</a></span> | <span class="t">The first axis is the sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3037" target="_blank">00:50:37.820</a></span> | <span class="t">The second one is the number of heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3039" target="_blank">00:50:39.420</a></span> | <span class="t">The third is this reduced model dimensionality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3042" target="_blank">00:50:42.020</a></span> | <span class="t">And that costs nothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3043" target="_blank">00:50:43.780</a></span> | <span class="t">And do the same thing for x and v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3045" target="_blank">00:50:45.940</a></span> | <span class="t">And then I transpose so that I've got the head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3048" target="_blank">00:50:48.460</a></span> | <span class="t">axis as the first axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3050" target="_blank">00:50:50.740</a></span> | <span class="t">And now I can compute all my other operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3053" target="_blank">00:50:53.460</a></span> | <span class="t">with the head axis, kind of like a batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3058" target="_blank">00:50:58.020</a></span> | <span class="t">So what does this look like in practice?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3061" target="_blank">00:51:01.780</a></span> | <span class="t">Instead of having one big xq matrix that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3065" target="_blank">00:51:05.100</a></span> | <span class="t">model dimensionality d, I've got, in this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3068" target="_blank">00:51:08.180</a></span> | <span class="t">three xq matrices of model dimensionality d by 3, d by 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3072" target="_blank">00:51:12.780</a></span> | <span class="t">d by 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3073" target="_blank">00:51:13.860</a></span> | <span class="t">Same thing with the key matrix here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3076" target="_blank">00:51:16.500</a></span> | <span class="t">So everything looks almost identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3078" target="_blank">00:51:18.540</a></span> | <span class="t">It's just the reshaping of the tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3081" target="_blank">00:51:21.000</a></span> | <span class="t">And now, at the output of this, I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3083" target="_blank">00:51:23.340</a></span> | <span class="t">got three sets of attention scores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3086" target="_blank">00:51:26.860</a></span> | <span class="t">just by doing this reshape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3089" target="_blank">00:51:29.060</a></span> | <span class="t">And the cost is that, well, each of my attention heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3093" target="_blank">00:51:33.420</a></span> | <span class="t">has only a d by h vector to work with instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3095" target="_blank">00:51:35.900</a></span> | <span class="t">of a d-dimensional vector to work with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3098" target="_blank">00:51:38.020</a></span> | <span class="t">So I get the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3098" target="_blank">00:51:38.860</a></span> | <span class="t">I get these three sets of pairs of scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3103" target="_blank">00:51:43.340</a></span> | <span class="t">I compute the softmax independently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3105" target="_blank">00:51:45.660</a></span> | <span class="t">for each of the three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3107" target="_blank">00:51:47.100</a></span> | <span class="t">And then I have three value matrices there as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3110" target="_blank">00:51:50.680</a></span> | <span class="t">each of them lower dimensional.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3112" target="_blank">00:51:52.660</a></span> | <span class="t">And then finally, I get my three different output vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3116" target="_blank">00:51:56.700</a></span> | <span class="t">And I have a final linear transformation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3118" target="_blank">00:51:58.460</a></span> | <span class="t">to mush them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3121" target="_blank">00:52:01.020</a></span> | <span class="t">And I get an output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3122" target="_blank">00:52:02.660</a></span> | <span class="t">And in summary, what this allows you to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3124" target="_blank">00:52:04.620</a></span> | <span class="t">is exactly what I gave in the toy example, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3128" target="_blank">00:52:08.220</a></span> | <span class="t">was I can have each of these heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3130" target="_blank">00:52:10.240</a></span> | <span class="t">look at different parts of a sequence for different reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3132" target="_blank">00:52:12.820</a></span> | <span class="t">So this is at a given block, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3141" target="_blank">00:52:21.020</a></span> | <span class="t">All of these attention heads are for a given transformer block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3143" target="_blank">00:52:23.780</a></span> | <span class="t">A next block could also have three attention heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3146" target="_blank">00:52:26.980</a></span> | <span class="t">The question is, are all of these for a given block?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3150" target="_blank">00:52:30.340</a></span> | <span class="t">And we'll talk about a block again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3151" target="_blank">00:52:31.800</a></span> | <span class="t">But this block was this sort of pair of self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3155" target="_blank">00:52:35.060</a></span> | <span class="t">and feed-forward network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3156" target="_blank">00:52:36.300</a></span> | <span class="t">So you do self-attention, feed-forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3157" target="_blank">00:52:37.920</a></span> | <span class="t">That's one block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3158" target="_blank">00:52:38.780</a></span> | <span class="t">Another block is another self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3160" target="_blank">00:52:40.120</a></span> | <span class="t">another feed-forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3161" target="_blank">00:52:41.340</a></span> | <span class="t">And the question is, are the parameters shared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3163" target="_blank">00:52:43.260</a></span> | <span class="t">between the blocks or not?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3164" target="_blank">00:52:44.900</a></span> | <span class="t">Generally, they are not shared.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3166" target="_blank">00:52:46.180</a></span> | <span class="t">You'll have independent parameters at every block,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3168" target="_blank">00:52:48.660</a></span> | <span class="t">although there are some exceptions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3172" target="_blank">00:52:52.700</a></span> | <span class="t">Voting on that, is it typically the case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3175" target="_blank">00:52:55.380</a></span> | <span class="t">that you have the same number of heads at each block?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3178" target="_blank">00:52:58.820</a></span> | <span class="t">Or do you vary the number of heads across blocks?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3181" target="_blank">00:53:01.380</a></span> | <span class="t">You have this-- you definitely could vary it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3184" target="_blank">00:53:04.020</a></span> | <span class="t">People haven't found reason to vary--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3185" target="_blank">00:53:05.540</a></span> | <span class="t">so the question is, do you have different numbers of heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3187" target="_blank">00:53:07.960</a></span> | <span class="t">across the different blocks?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3189" target="_blank">00:53:09.340</a></span> | <span class="t">Or do you have the same number of heads across all blocks?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3192" target="_blank">00:53:12.780</a></span> | <span class="t">The simplest thing is to just have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3194" target="_blank">00:53:14.540</a></span> | <span class="t">it be the same everywhere, which is what people have done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3196" target="_blank">00:53:16.940</a></span> | <span class="t">I haven't yet found a good reason to vary it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3199" target="_blank">00:53:19.100</a></span> | <span class="t">but it could be interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3201" target="_blank">00:53:21.860</a></span> | <span class="t">It's definitely the case that after training these networks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3205" target="_blank">00:53:25.540</a></span> | <span class="t">you can actually just totally zero out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3207" target="_blank">00:53:27.900</a></span> | <span class="t">remove some of the attention heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3210" target="_blank">00:53:30.900</a></span> | <span class="t">And I'd be curious to know if you could remove more or less,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3215" target="_blank">00:53:35.700</a></span> | <span class="t">depending on the layer index, which might then say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3219" target="_blank">00:53:39.380</a></span> | <span class="t">oh, we should just have fewer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3220" target="_blank">00:53:40.580</a></span> | <span class="t">But again, it's not actually more expensive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3222" target="_blank">00:53:42.420</a></span> | <span class="t">to have a bunch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3223" target="_blank">00:53:43.700</a></span> | <span class="t">So people tend to instead set the number of heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3226" target="_blank">00:53:46.740</a></span> | <span class="t">to be roughly so that you have a reasonable number of dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3231" target="_blank">00:53:51.180</a></span> | <span class="t">per head, given the total model dimensionality d that you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3235" target="_blank">00:53:55.460</a></span> | <span class="t">So for example, I might want at least 64 dimensions per head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3240" target="_blank">00:54:00.260</a></span> | <span class="t">which if d is 128, that tells me how many heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3244" target="_blank">00:54:04.340</a></span> | <span class="t">I'm going to have, roughly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3246" target="_blank">00:54:06.020</a></span> | <span class="t">So people tend to scale the number of heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3247" target="_blank">00:54:07.860</a></span> | <span class="t">up with the model dimensionality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3249" target="_blank">00:54:09.620</a></span> | <span class="t">Yeah, with that xq, by slicing it into different columns,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3255" target="_blank">00:54:15.820</a></span> | <span class="t">you're reducing the rank of the final matrix, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3259" target="_blank">00:54:19.020</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3259" target="_blank">00:54:19.820</a></span> | <span class="t">But that doesn't really have any effect on the results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3263" target="_blank">00:54:23.180</a></span> | <span class="t">So the question is, by having these reduced xq and xk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3269" target="_blank">00:54:29.300</a></span> | <span class="t">matrices, this is a very low rank approximation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3272" target="_blank">00:54:32.940</a></span> | <span class="t">This little sliver and this little sliver</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3275" target="_blank">00:54:35.340</a></span> | <span class="t">defining this whole big matrix, it's very low rank.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3278" target="_blank">00:54:38.020</a></span> | <span class="t">Is that not bad?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3279" target="_blank">00:54:39.780</a></span> | <span class="t">In practice, no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3280" target="_blank">00:54:40.940</a></span> | <span class="t">I mean, again, it's the reason why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3282" target="_blank">00:54:42.700</a></span> | <span class="t">we limit the number of heads depending on the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3285" target="_blank">00:54:45.820</a></span> | <span class="t">dimensionality, because you want intuitively at least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3289" target="_blank">00:54:49.620</a></span> | <span class="t">some number of dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3291" target="_blank">00:54:51.220</a></span> | <span class="t">So 64 is sometimes done, 128, something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3295" target="_blank">00:54:55.820</a></span> | <span class="t">But if you're not giving each head too much to do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3298" target="_blank">00:54:58.300</a></span> | <span class="t">and it's got sort of a simple job, you've got a lot of heads,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3301" target="_blank">00:55:01.020</a></span> | <span class="t">it ends up sort of being OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3304" target="_blank">00:55:04.260</a></span> | <span class="t">All we really know is that empirically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3305" target="_blank">00:55:05.980</a></span> | <span class="t">it's way better to have more heads than one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3312" target="_blank">00:55:12.420</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3314" target="_blank">00:55:14.140</a></span> | <span class="t">I'm wondering, have there been studies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3316" target="_blank">00:55:16.300</a></span> | <span class="t">to see if information in one of the sets of the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3322" target="_blank">00:55:22.140</a></span> | <span class="t">scores, like information that one of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3325" target="_blank">00:55:25.100</a></span> | <span class="t">learns is consistent and related to each other,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3329" target="_blank">00:55:29.540</a></span> | <span class="t">or how are they related?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3332" target="_blank">00:55:32.380</a></span> | <span class="t">So the question is, have there been studies to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3334" target="_blank">00:55:34.580</a></span> | <span class="t">if there's consistent information encoded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3337" target="_blank">00:55:37.140</a></span> | <span class="t">by the attention heads?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3338" target="_blank">00:55:38.780</a></span> | <span class="t">And yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3340" target="_blank">00:55:40.740</a></span> | <span class="t">Actually, there's been quite a lot of study</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3342" target="_blank">00:55:42.580</a></span> | <span class="t">and interpretability and analysis of these models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3344" target="_blank">00:55:44.980</a></span> | <span class="t">to try to figure out what roles, what sort of mechanistic roles</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3348" target="_blank">00:55:48.420</a></span> | <span class="t">each of these heads takes on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3350" target="_blank">00:55:50.180</a></span> | <span class="t">And there's quite a bit of exciting results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3352" target="_blank">00:55:52.740</a></span> | <span class="t">there around some attention heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3355" target="_blank">00:55:55.140</a></span> | <span class="t">learning to pick out the syntactic dependencies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3359" target="_blank">00:55:59.820</a></span> | <span class="t">or maybe doing a global averaging of context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3363" target="_blank">00:56:03.780</a></span> | <span class="t">The question is quite nuanced, though,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3365" target="_blank">00:56:05.420</a></span> | <span class="t">because in a deep network, it's unclear--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3367" target="_blank">00:56:07.780</a></span> | <span class="t">and we should talk about this more offline--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3369" target="_blank">00:56:09.620</a></span> | <span class="t">it's unclear if you look at a word 10 layers deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3372" target="_blank">00:56:12.580</a></span> | <span class="t">in a network what you're really looking at,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3374" target="_blank">00:56:14.900</a></span> | <span class="t">because it's already incorporated context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3377" target="_blank">00:56:17.060</a></span> | <span class="t">from everyone else, and it's a little bit unclear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3380" target="_blank">00:56:20.140</a></span> | <span class="t">Active area of research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3381" target="_blank">00:56:21.300</a></span> | <span class="t">But I think I should move on now to keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3385" target="_blank">00:56:25.100</a></span> | <span class="t">discussing transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3386" target="_blank">00:56:26.780</a></span> | <span class="t">But yeah, if you want to talk more about it, I'm happy to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3390" target="_blank">00:56:30.700</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3391" target="_blank">00:56:31.420</a></span> | <span class="t">So another sort of hack that I'm going to toss in here--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3394" target="_blank">00:56:34.580</a></span> | <span class="t">I mean, maybe they wouldn't call it hack,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3396" target="_blank">00:56:36.300</a></span> | <span class="t">but it's a nice little method to improve things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3399" target="_blank">00:56:39.620</a></span> | <span class="t">It's called scaled dot product attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3402" target="_blank">00:56:42.060</a></span> | <span class="t">So one of the issues with this sort of key query value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3405" target="_blank">00:56:45.500</a></span> | <span class="t">self-attention is that when the model dimensionality becomes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3407" target="_blank">00:56:47.940</a></span> | <span class="t">large, the dot products between vectors, even random vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3411" target="_blank">00:56:51.700</a></span> | <span class="t">tend to become large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3415" target="_blank">00:56:55.060</a></span> | <span class="t">And when that happens, the inputs to the softmax function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3418" target="_blank">00:56:58.060</a></span> | <span class="t">can be very large, making the gradient small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3421" target="_blank">00:57:01.380</a></span> | <span class="t">So intuitively, if you have two random vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3423" target="_blank">00:57:03.300</a></span> | <span class="t">and model dimensionality d, and you just dot product them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3426" target="_blank">00:57:06.540</a></span> | <span class="t">together, as d grows, their dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3429" target="_blank">00:57:09.140</a></span> | <span class="t">grows in expectation to be very large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3431" target="_blank">00:57:11.540</a></span> | <span class="t">And so you sort of want to start out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3433" target="_blank">00:57:13.940</a></span> | <span class="t">with everyone's attention being very uniform, very flat,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3436" target="_blank">00:57:16.900</a></span> | <span class="t">sort of look everywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3438" target="_blank">00:57:18.780</a></span> | <span class="t">But if some dot products are very large,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3440" target="_blank">00:57:20.620</a></span> | <span class="t">then learning will be inhibited.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3443" target="_blank">00:57:23.660</a></span> | <span class="t">And so what you end up doing is you just sort of--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3446" target="_blank">00:57:26.300</a></span> | <span class="t">for each of your heads, you just sort of divide all the scores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3449" target="_blank">00:57:29.700</a></span> | <span class="t">by this constant that's determined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3451" target="_blank">00:57:31.380</a></span> | <span class="t">by the model dimensionality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3453" target="_blank">00:57:33.060</a></span> | <span class="t">So as the vectors grow very large,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3455" target="_blank">00:57:35.660</a></span> | <span class="t">their dot products don't, at least at initialization time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3460" target="_blank">00:57:40.500</a></span> | <span class="t">So this is sort of like a nice little important,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3465" target="_blank">00:57:45.020</a></span> | <span class="t">but maybe not--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3466" target="_blank">00:57:46.020</a></span> | <span class="t">yeah, it's important to know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3472" target="_blank">00:57:52.260</a></span> | <span class="t">And so that's called scaled dot product attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3475" target="_blank">00:57:55.500</a></span> | <span class="t">From here on out, we'll just assume that we do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3478" target="_blank">00:57:58.340</a></span> | <span class="t">It's quite easy to implement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3479" target="_blank">00:57:59.540</a></span> | <span class="t">You just do a little division in all of your computations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3485" target="_blank">00:58:05.060</a></span> | <span class="t">OK, so now in the transformer decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3487" target="_blank">00:58:07.260</a></span> | <span class="t">we've got a couple of other things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3488" target="_blank">00:58:08.780</a></span> | <span class="t">that I have unfaded out here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3492" target="_blank">00:58:12.660</a></span> | <span class="t">We have two big optimization tricks, or optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3495" target="_blank">00:58:15.220</a></span> | <span class="t">methods, I should say, really, because these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3497" target="_blank">00:58:17.060</a></span> | <span class="t">are quite important, that end up being very important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3500" target="_blank">00:58:20.100</a></span> | <span class="t">We've got residual connections and layer normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3502" target="_blank">00:58:22.940</a></span> | <span class="t">And in transformer diagrams that you see sort of around the web,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3506" target="_blank">00:58:26.980</a></span> | <span class="t">they're often written together as this add and norm box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3512" target="_blank">00:58:32.380</a></span> | <span class="t">And in practice, in the transformer decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3514" target="_blank">00:58:34.540</a></span> | <span class="t">I'm going to apply mask multi-head attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3518" target="_blank">00:58:38.620</a></span> | <span class="t">and then do this sort of optimization add a norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3521" target="_blank">00:58:41.340</a></span> | <span class="t">Then I'll do a feed forward application</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3523" target="_blank">00:58:43.340</a></span> | <span class="t">and then add a norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3524" target="_blank">00:58:44.660</a></span> | <span class="t">So this is quite important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3527" target="_blank">00:58:47.660</a></span> | <span class="t">So let's go over these two individual components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3531" target="_blank">00:58:51.820</a></span> | <span class="t">The first is residual connections.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3533" target="_blank">00:58:53.300</a></span> | <span class="t">I mean, I think we've talked about residual connections</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3535" target="_blank">00:58:55.460</a></span> | <span class="t">before, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3536" target="_blank">00:58:56.100</a></span> | <span class="t">So it's worth doing it again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3538" target="_blank">00:58:58.140</a></span> | <span class="t">But it's really a good trick to help models train better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3541" target="_blank">00:59:01.820</a></span> | <span class="t">So just to recap, we're going to take--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3544" target="_blank">00:59:04.660</a></span> | <span class="t">instead of having this sort of-- you have a layer, layer i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3548" target="_blank">00:59:08.220</a></span> | <span class="t">minus 1, and you pass it through a thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3550" target="_blank">00:59:10.540</a></span> | <span class="t">Maybe it's self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3551" target="_blank">00:59:11.620</a></span> | <span class="t">Maybe it's a feed forward network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3553" target="_blank">00:59:13.060</a></span> | <span class="t">Now you've got layer i.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3556" target="_blank">00:59:16.380</a></span> | <span class="t">I'm going to add the result of layer i to its input here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3563" target="_blank">00:59:23.060</a></span> | <span class="t">So now I'm saying I'm just going to compute the layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3565" target="_blank">00:59:25.320</a></span> | <span class="t">and I'm going to add in the input to the layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3567" target="_blank">00:59:27.740</a></span> | <span class="t">so that I only have to learn the residual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3570" target="_blank">00:59:30.780</a></span> | <span class="t">from the previous layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3572" target="_blank">00:59:32.020</a></span> | <span class="t">So I've got this sort of connection here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3573" target="_blank">00:59:33.720</a></span> | <span class="t">It's often written as this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3574" target="_blank">00:59:34.860</a></span> | <span class="t">It's sort of like, boop, connection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3578" target="_blank">00:59:38.940</a></span> | <span class="t">It goes around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3579" target="_blank">00:59:39.860</a></span> | <span class="t">And you should think that the gradient is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3581" target="_blank">00:59:41.740</a></span> | <span class="t">really great through the residual connection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3583" target="_blank">00:59:43.820</a></span> | <span class="t">Like, ah, if I've got vanishing or exploding gradient--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3587" target="_blank">00:59:47.700</a></span> | <span class="t">vanishing gradients through this layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3589" target="_blank">00:59:49.500</a></span> | <span class="t">well, I can at least learn everything behind it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3591" target="_blank">00:59:51.740</a></span> | <span class="t">because I've got this residual connection where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3594" target="_blank">00:59:54.160</a></span> | <span class="t">the gradient is 1 because it's the identity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3598" target="_blank">00:59:58.060</a></span> | <span class="t">This is really nice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3599" target="_blank">00:59:59.180</a></span> | <span class="t">And it also maybe is like a--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3601" target="_blank">01:00:01.460</a></span> | <span class="t">at least at initialization, everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3603" target="_blank">01:00:03.980</a></span> | <span class="t">looks a little bit like the identity function now, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3606" target="_blank">01:00:06.660</a></span> | <span class="t">Because if the contribution of the layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3608" target="_blank">01:00:08.920</a></span> | <span class="t">is somewhat small because all of your weights are small,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3611" target="_blank">01:00:11.680</a></span> | <span class="t">and I have the addition from the input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3613" target="_blank">01:00:13.980</a></span> | <span class="t">maybe the whole thing looks a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3615" target="_blank">01:00:15.620</a></span> | <span class="t">like the identity, which might be a good sort of place</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3618" target="_blank">01:00:18.380</a></span> | <span class="t">to start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3620" target="_blank">01:00:20.340</a></span> | <span class="t">And there are really nice visualizations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3622" target="_blank">01:00:22.100</a></span> | <span class="t">I just love this visualization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3624" target="_blank">01:00:24.800</a></span> | <span class="t">So this is your lost landscape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3626" target="_blank">01:00:26.420</a></span> | <span class="t">So you're gradient descent, and you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3628" target="_blank">01:00:28.000</a></span> | <span class="t">trying to traverse the mountains of the lost landscape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3630" target="_blank">01:00:30.860</a></span> | <span class="t">This is like the parameter space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3632" target="_blank">01:00:32.580</a></span> | <span class="t">And down is better in your loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3634" target="_blank">01:00:34.620</a></span> | <span class="t">And it's really hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3635" target="_blank">01:00:35.500</a></span> | <span class="t">So you get stuck in some local optima,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3638" target="_blank">01:00:38.060</a></span> | <span class="t">and you can't sort of find your way to get out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3641" target="_blank">01:00:41.180</a></span> | <span class="t">And then this is with residual connections.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3643" target="_blank">01:00:43.140</a></span> | <span class="t">I mean, come on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3644" target="_blank">01:00:44.300</a></span> | <span class="t">You just sort of walk down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3647" target="_blank">01:00:47.060</a></span> | <span class="t">I mean, that's not actually, I guess,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3648" target="_blank">01:00:48.860</a></span> | <span class="t">really how it works all the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3650" target="_blank">01:00:50.380</a></span> | <span class="t">But I really love this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3652" target="_blank">01:00:52.140</a></span> | <span class="t">It's great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3652" target="_blank">01:00:52.640</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3658" target="_blank">01:00:58.540</a></span> | <span class="t">So yeah, we've seen residual connections.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3660" target="_blank">01:01:00.260</a></span> | <span class="t">We should move on to layer normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3662" target="_blank">01:01:02.860</a></span> | <span class="t">So layer norm is another thing to help your model train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3666" target="_blank">01:01:06.500</a></span> | <span class="t">faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3668" target="_blank">01:01:08.380</a></span> | <span class="t">And the intuitions around layer normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3674" target="_blank">01:01:14.760</a></span> | <span class="t">and sort of the empiricism of it working very well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3677" target="_blank">01:01:17.040</a></span> | <span class="t">maybe aren't perfectly, let's say, connected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3681" target="_blank">01:01:21.020</a></span> | <span class="t">But you should imagine, I suppose,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3685" target="_blank">01:01:25.700</a></span> | <span class="t">that we want to say this variation within each layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3689" target="_blank">01:01:29.860</a></span> | <span class="t">Things can get very big.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3691" target="_blank">01:01:31.180</a></span> | <span class="t">Things can get very small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3693" target="_blank">01:01:33.140</a></span> | <span class="t">That's not actually informative because of variations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3696" target="_blank">01:01:36.700</a></span> | <span class="t">between maybe the gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3699" target="_blank">01:01:39.940</a></span> | <span class="t">Or I've got sort of weird things going on in my layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3703" target="_blank">01:01:43.860</a></span> | <span class="t">that I can't totally control.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3705" target="_blank">01:01:45.140</a></span> | <span class="t">I haven't been able to sort of make everything behave sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3707" target="_blank">01:01:47.740</a></span> | <span class="t">of nicely where everything stays roughly the same norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3710" target="_blank">01:01:50.460</a></span> | <span class="t">Maybe some things explode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3711" target="_blank">01:01:51.660</a></span> | <span class="t">Maybe some things shrink.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3714" target="_blank">01:01:54.660</a></span> | <span class="t">And I want to cut down on sort of uninformative variation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3719" target="_blank">01:01:59.580</a></span> | <span class="t">between layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3720" target="_blank">01:02:00.940</a></span> | <span class="t">So I'm going to let x and rd be an individual word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3723" target="_blank">01:02:03.740</a></span> | <span class="t">vector in the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3725" target="_blank">01:02:05.380</a></span> | <span class="t">So this is like I have a single index, one vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3729" target="_blank">01:02:09.100</a></span> | <span class="t">And what I'm going to try to do is just normalize it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3732" target="_blank">01:02:12.660</a></span> | <span class="t">Normalize it in the sense of it's got a bunch of variation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3735" target="_blank">01:02:15.540</a></span> | <span class="t">And I'm going to cut out on everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3737" target="_blank">01:02:17.700</a></span> | <span class="t">I'm going to normalize it to unit mean and standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3740" target="_blank">01:02:20.340</a></span> | <span class="t">deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3741" target="_blank">01:02:21.020</a></span> | <span class="t">So I'm going to estimate the mean here across--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3746" target="_blank">01:02:26.700</a></span> | <span class="t">so for all of the dimensions in the vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3750" target="_blank">01:02:30.180</a></span> | <span class="t">so j equals 1 to the model dimensionality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3752" target="_blank">01:02:32.660</a></span> | <span class="t">I'm going to sum up the value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3753" target="_blank">01:02:33.900</a></span> | <span class="t">So I've got this one big word vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3755" target="_blank">01:02:35.860</a></span> | <span class="t">And I sum up all the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3757" target="_blank">01:02:37.540</a></span> | <span class="t">Division by d here, that's the mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3760" target="_blank">01:02:40.220</a></span> | <span class="t">I'm going to have my estimate of the standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3763" target="_blank">01:02:43.540</a></span> | <span class="t">Again, these should say estimates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3765" target="_blank">01:02:45.020</a></span> | <span class="t">This is my simple estimate of the standard deviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3767" target="_blank">01:02:47.300</a></span> | <span class="t">or the values within this one vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3770" target="_blank">01:02:50.500</a></span> | <span class="t">And I'm just going to--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3773" target="_blank">01:02:53.700</a></span> | <span class="t">and then possibly, I guess I can have learned parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3778" target="_blank">01:02:58.060</a></span> | <span class="t">to try to scale back out in terms of multiplicatively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3782" target="_blank">01:03:02.780</a></span> | <span class="t">and additively here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3784" target="_blank">01:03:04.500</a></span> | <span class="t">That's optional.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3785" target="_blank">01:03:05.540</a></span> | <span class="t">We're going to compute this standardization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3788" target="_blank">01:03:08.380</a></span> | <span class="t">I'm going to take my vector x, subtract out the mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3791" target="_blank">01:03:11.340</a></span> | <span class="t">divide by the standard deviation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3792" target="_blank">01:03:12.780</a></span> | <span class="t">plus this epsilon constant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3794" target="_blank">01:03:14.820</a></span> | <span class="t">If there's not a lot of variation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3796" target="_blank">01:03:16.300</a></span> | <span class="t">I don't want things to explode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3797" target="_blank">01:03:17.900</a></span> | <span class="t">So I'm going to have this epsilon there that's close to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3801" target="_blank">01:03:21.700</a></span> | <span class="t">So this part here, x minus mu over square root sigma</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3805" target="_blank">01:03:25.500</a></span> | <span class="t">plus epsilon, is saying take all the variation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3808" target="_blank">01:03:28.540</a></span> | <span class="t">and normalize it to unit mean and standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3812" target="_blank">01:03:32.600</a></span> | <span class="t">And then maybe I want to scale it, stretch it back out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3817" target="_blank">01:03:37.080</a></span> | <span class="t">and then maybe add an offset beta that I've learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3820" target="_blank">01:03:40.860</a></span> | <span class="t">Although in practice, actually, this part-- and discuss this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3823" target="_blank">01:03:43.300</a></span> | <span class="t">in the lecture notes--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3824" target="_blank">01:03:44.580</a></span> | <span class="t">in practice, this part maybe isn't actually that important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3827" target="_blank">01:03:47.940</a></span> | <span class="t">But so layer normalization, yeah, you're sort of--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3831" target="_blank">01:03:51.220</a></span> | <span class="t">you can think of this as when I get the output of layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3834" target="_blank">01:03:54.000</a></span> | <span class="t">normalization, it's going to be--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3835" target="_blank">01:03:55.940</a></span> | <span class="t">sort of look nice and look similar to the next layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3838" target="_blank">01:03:58.940</a></span> | <span class="t">independent of what's gone on because it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3840" target="_blank">01:04:00.940</a></span> | <span class="t">going to be unit mean and standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3842" target="_blank">01:04:02.780</a></span> | <span class="t">So maybe that makes for a better thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3844" target="_blank">01:04:04.660</a></span> | <span class="t">to learn off of for the next layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3846" target="_blank">01:04:06.260</a></span> | <span class="t">OK, any questions for residual or layer norm?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3852" target="_blank">01:04:12.340</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3853" target="_blank">01:04:13.220</a></span> | <span class="t">What would it mean to subtract the scalar mu from the vector x?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3857" target="_blank">01:04:17.340</a></span> | <span class="t">Yeah, it's a good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3858" target="_blank">01:04:18.780</a></span> | <span class="t">When I subtract the scalar mu from the vector x,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3861" target="_blank">01:04:21.580</a></span> | <span class="t">I broadcast mu to dimensionality d and remove mu from all d.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3867" target="_blank">01:04:27.980</a></span> | <span class="t">Yeah, good point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3869" target="_blank">01:04:29.300</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3869" target="_blank">01:04:29.900</a></span> | <span class="t">That was unclear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3870" target="_blank">01:04:30.580</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3871" target="_blank">01:04:31.080</a></span> | <span class="t">In the fourth bullet, maybe I'm confused.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3877" target="_blank">01:04:37.420</a></span> | <span class="t">Is it divided?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3878" target="_blank">01:04:38.300</a></span> | <span class="t">Should it be divided by d or from mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3882" target="_blank">01:04:42.500</a></span> | <span class="t">Sorry, can you repeat that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3883" target="_blank">01:04:43.620</a></span> | <span class="t">In the fourth bullet point when you're calculating the mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3887" target="_blank">01:04:47.220</a></span> | <span class="t">is it divided by d or is it--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3889" target="_blank">01:04:49.660</a></span> | <span class="t">or maybe I'm just confused.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3891" target="_blank">01:04:51.180</a></span> | <span class="t">I think it is divided by d.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3892" target="_blank">01:04:52.340</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3892" target="_blank">01:04:52.820</a></span> | <span class="t">Oh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3895" target="_blank">01:04:55.180</a></span> | <span class="t">These are-- so this is the average deviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3897" target="_blank">01:04:57.300</a></span> | <span class="t">from the mean of all of the-- yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3900" target="_blank">01:05:00.260</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3900" target="_blank">01:05:00.760</a></span> | <span class="t">So if you have five words in a sentence by their norm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3904" target="_blank">01:05:04.700</a></span> | <span class="t">do you normalize based on the statistics of these five words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3909" target="_blank">01:05:09.460</a></span> | <span class="t">or do you want one word by one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3911" target="_blank">01:05:11.700</a></span> | <span class="t">So the question is, if I have five words in the sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3914" target="_blank">01:05:14.500</a></span> | <span class="t">do I normalize by aggregating the statistics to estimate mu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3918" target="_blank">01:05:18.820</a></span> | <span class="t">and sigma across all the five words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3921" target="_blank">01:05:21.060</a></span> | <span class="t">share their statistics, or do it independently for each word?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3924" target="_blank">01:05:24.140</a></span> | <span class="t">This is a great question, which I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3925" target="_blank">01:05:25.700</a></span> | <span class="t">think in all the papers that discuss transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3928" target="_blank">01:05:28.260</a></span> | <span class="t">is under specified.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3930" target="_blank">01:05:30.140</a></span> | <span class="t">You do not share across the five words, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3933" target="_blank">01:05:33.060</a></span> | <span class="t">is somewhat confusing to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3935" target="_blank">01:05:35.500</a></span> | <span class="t">So each of the five words is done completely independently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3939" target="_blank">01:05:39.180</a></span> | <span class="t">You could have shared across the five words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3941" target="_blank">01:05:41.380</a></span> | <span class="t">and said that my estimate of the statistics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3943" target="_blank">01:05:43.300</a></span> | <span class="t">are just based on all five, but you do not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3949" target="_blank">01:05:49.740</a></span> | <span class="t">I can't pretend I understand totally why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3951" target="_blank">01:05:51.380</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3951" target="_blank">01:05:51.880</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3954" target="_blank">01:05:54.360</a></span> | <span class="t">For example, per batch or per output of the same position?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3961" target="_blank">01:06:01.400</a></span> | <span class="t">So similar question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3962" target="_blank">01:06:02.840</a></span> | <span class="t">The question is, if you have a batch of sequences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3966" target="_blank">01:06:06.760</a></span> | <span class="t">so just like we were doing batch-based training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3970" target="_blank">01:06:10.040</a></span> | <span class="t">do you for a single word--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3971" target="_blank">01:06:11.840</a></span> | <span class="t">now, we don't share across the sequence index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3973" target="_blank">01:06:13.760</a></span> | <span class="t">for sharing the statistics, but do you share across the batch?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3976" target="_blank">01:06:16.680</a></span> | <span class="t">And the answer is no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3977" target="_blank">01:06:17.680</a></span> | <span class="t">You also do not share across the batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3979" target="_blank">01:06:19.480</a></span> | <span class="t">In fact, layer normalization was sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3982" target="_blank">01:06:22.160</a></span> | <span class="t">invented as a replacement for batch normalization, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3985" target="_blank">01:06:25.360</a></span> | <span class="t">did just that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3986" target="_blank">01:06:26.320</a></span> | <span class="t">And the issue with batch normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3987" target="_blank">01:06:27.900</a></span> | <span class="t">is that now your forward pass sort of depends in a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3990" target="_blank">01:06:30.960</a></span> | <span class="t">that you don't like on examples that should be not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3994" target="_blank">01:06:34.080</a></span> | <span class="t">related to your example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3995" target="_blank">01:06:35.400</a></span> | <span class="t">And so, yeah, you don't share statistics across the batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=3997" target="_blank">01:06:37.800</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4001" target="_blank">01:06:41.640</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4004" target="_blank">01:06:44.240</a></span> | <span class="t">OK, so now we have our full transformer decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4008" target="_blank">01:06:48.600</a></span> | <span class="t">and we have our blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4010" target="_blank">01:06:50.520</a></span> | <span class="t">So in this sort of slightly grayed out thing here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4012" target="_blank">01:06:52.960</a></span> | <span class="t">that says repeat for a number of decoder blocks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4018" target="_blank">01:06:58.640</a></span> | <span class="t">each block consists of--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4020" target="_blank">01:07:00.520</a></span> | <span class="t">I pass it through self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4022" target="_blank">01:07:02.400</a></span> | <span class="t">and then my add and norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4024" target="_blank">01:07:04.720</a></span> | <span class="t">So I've got this residual connection here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4026" target="_blank">01:07:06.480</a></span> | <span class="t">that goes around, add.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4028" target="_blank">01:07:08.400</a></span> | <span class="t">I've got the layer normalization there, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4030" target="_blank">01:07:10.780</a></span> | <span class="t">a feed-forward layer, and then another add and norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4035" target="_blank">01:07:15.240</a></span> | <span class="t">And so that sort of set of four operations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4038" target="_blank">01:07:18.040</a></span> | <span class="t">I apply for some number of times, number of blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4041" target="_blank">01:07:21.720</a></span> | <span class="t">So that whole thing is called a single block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4044" target="_blank">01:07:24.000</a></span> | <span class="t">And that's it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4044" target="_blank">01:07:24.960</a></span> | <span class="t">That's the transformer decoder as it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4049" target="_blank">01:07:29.040</a></span> | <span class="t">Cool, so that's a whole architecture right there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4054" target="_blank">01:07:34.040</a></span> | <span class="t">We've solved things like needing to represent position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4056" target="_blank">01:07:36.800</a></span> | <span class="t">We've solved things like not being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4059" target="_blank">01:07:39.960</a></span> | <span class="t">able to look into the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4061" target="_blank">01:07:41.760</a></span> | <span class="t">We've solved a lot of different optimization problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4064" target="_blank">01:07:44.120</a></span> | <span class="t">You had a question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4064" target="_blank">01:07:44.680</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4065" target="_blank">01:07:45.180</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4068" target="_blank">01:07:48.080</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4069" target="_blank">01:07:49.680</a></span> | <span class="t">Yes, masked multi-head attention, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4072" target="_blank">01:07:52.800</a></span> | <span class="t">With the dot product scaling with the square root</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4075" target="_blank">01:07:55.880</a></span> | <span class="t">d over h as well, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4077" target="_blank">01:07:57.120</a></span> | <span class="t">So the question is, how do these models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4085" target="_blank">01:08:05.880</a></span> | <span class="t">handle variable length inputs?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4088" target="_blank">01:08:08.560</a></span> | <span class="t">Yeah, so if you have--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4093" target="_blank">01:08:13.680</a></span> | <span class="t">so the input to the GPU forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4098" target="_blank">01:08:18.180</a></span> | <span class="t">is going to be a constant length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4100" target="_blank">01:08:20.820</a></span> | <span class="t">So you're going to maybe pad to a constant length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4104" target="_blank">01:08:24.740</a></span> | <span class="t">And in order to not look at the future, the stuff that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4108" target="_blank">01:08:28.580</a></span> | <span class="t">happening in the future, you can mask out the pad tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4112" target="_blank">01:08:32.720</a></span> | <span class="t">just like the masking that we showed for not looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4115" target="_blank">01:08:35.480</a></span> | <span class="t">at the future in general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4116" target="_blank">01:08:36.520</a></span> | <span class="t">You can just say, set all of the attention weights to 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4120" target="_blank">01:08:40.380</a></span> | <span class="t">or the scores to negative infinity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4122" target="_blank">01:08:42.300</a></span> | <span class="t">for all of the pad tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4123" target="_blank">01:08:43.520</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4127" target="_blank">01:08:47.660</a></span> | <span class="t">Yeah, exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4128" target="_blank">01:08:48.280</a></span> | <span class="t">So you can set everything to this maximum length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4132" target="_blank">01:08:52.200</a></span> | <span class="t">Now, in practice-- so the question was,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4133" target="_blank">01:08:53.820</a></span> | <span class="t">do you set this length that you have everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4135" target="_blank">01:08:55.740</a></span> | <span class="t">be that maximum length?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4136" target="_blank">01:08:56.980</a></span> | <span class="t">I mean, yes, often, although you can save computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4140" target="_blank">01:09:00.780</a></span> | <span class="t">by setting it to something smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4143" target="_blank">01:09:03.420</a></span> | <span class="t">And everything-- the math all still works out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4146" target="_blank">01:09:06.140</a></span> | <span class="t">You just have to code it properly so it can handle--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4148" target="_blank">01:09:08.700</a></span> | <span class="t">you set everything instead of to n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4150" target="_blank">01:09:10.220</a></span> | <span class="t">You set it all to 5 if everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4152" target="_blank">01:09:12.260</a></span> | <span class="t">is shorter than length 5, and you save a lot of computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4155" target="_blank">01:09:15.260</a></span> | <span class="t">All of the self-attention operations just work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4159" target="_blank">01:09:19.340</a></span> | <span class="t">So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4161" target="_blank">01:09:21.980</a></span> | <span class="t">How many layers are in the feedforward normally?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4165" target="_blank">01:09:25.340</a></span> | <span class="t">There's one hidden layer in the feedforward usually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4167" target="_blank">01:09:27.500</a></span> | <span class="t">Oh, just one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4168" target="_blank">01:09:28.060</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4168" target="_blank">01:09:28.980</a></span> | <span class="t">OK, I should move on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4170" target="_blank">01:09:30.060</a></span> | <span class="t">We've got a couple more things and not very much time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4172" target="_blank">01:09:32.460</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4173" target="_blank">01:09:33.820</a></span> | <span class="t">But I'll be here after the class as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4175" target="_blank">01:09:35.620</a></span> | <span class="t">So in the encoder-- so the transformer encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4178" target="_blank">01:09:38.420</a></span> | <span class="t">is almost identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4179" target="_blank">01:09:39.540</a></span> | <span class="t">But again, we want bidirectional context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4181" target="_blank">01:09:41.900</a></span> | <span class="t">And so we just don't do the masking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4184" target="_blank">01:09:44.500</a></span> | <span class="t">So I've got in my multi-head attention here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4186" target="_blank">01:09:46.740</a></span> | <span class="t">I've got no masking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4188" target="_blank">01:09:48.620</a></span> | <span class="t">And so it's that easy to make the model bidirectional.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4193" target="_blank">01:09:53.380</a></span> | <span class="t">So that's easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4194" target="_blank">01:09:54.100</a></span> | <span class="t">So that's called the transformer encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4195" target="_blank">01:09:55.800</a></span> | <span class="t">It's almost identical but no masking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4198" target="_blank">01:09:58.100</a></span> | <span class="t">And then finally, we've got the transformer encoder decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4201" target="_blank">01:10:01.900</a></span> | <span class="t">which is actually how the transformer was originally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4204" target="_blank">01:10:04.060</a></span> | <span class="t">presented in this paper, "Attention is All You Need."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4207" target="_blank">01:10:07.900</a></span> | <span class="t">And this is when we want to have a bidirectional network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4210" target="_blank">01:10:10.700</a></span> | <span class="t">Here's the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4211" target="_blank">01:10:11.500</a></span> | <span class="t">It takes in, say, my source sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4213" target="_blank">01:10:13.420</a></span> | <span class="t">for machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4215" target="_blank">01:10:15.060</a></span> | <span class="t">Its multi-headed attention is not masked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4217" target="_blank">01:10:17.580</a></span> | <span class="t">And I have a decoder to decode out my sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4222" target="_blank">01:10:22.140</a></span> | <span class="t">Now, but you'll see that this is slightly more complicated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4224" target="_blank">01:10:24.740</a></span> | <span class="t">I have my masked multi-head self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4227" target="_blank">01:10:27.500</a></span> | <span class="t">just like I had before in my decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4229" target="_blank">01:10:29.940</a></span> | <span class="t">But now I have an extra operation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4232" target="_blank">01:10:32.980</a></span> | <span class="t">which is called cross-attention, where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4235" target="_blank">01:10:35.220</a></span> | <span class="t">I am going to use my decoder vectors as my queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4241" target="_blank">01:10:41.460</a></span> | <span class="t">Then I'll take the output of the encoder as my keys and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4245" target="_blank">01:10:45.860</a></span> | <span class="t">So now for every word in the decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4248" target="_blank">01:10:48.100</a></span> | <span class="t">I'm looking at all the possible words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4250" target="_blank">01:10:50.580</a></span> | <span class="t">in the output of all of the blocks of the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4253" target="_blank">01:10:53.460</a></span> | <span class="t">Yes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4254" target="_blank">01:10:54.460</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4254" target="_blank">01:10:54.940</a></span> | <span class="t">How do we get a key and value separated from the output?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4264" target="_blank">01:11:04.340</a></span> | <span class="t">Because didn't we collapse those into the single output?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4267" target="_blank">01:11:07.900</a></span> | <span class="t">So we-- well, how-- sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4270" target="_blank">01:11:10.700</a></span> | <span class="t">How will we get the keys and values out?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4272" target="_blank">01:11:12.780</a></span> | <span class="t">Like, how do we-- because when we have the output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4275" target="_blank">01:11:15.020</a></span> | <span class="t">didn't we collapse the keys and values into a single output?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4279" target="_blank">01:11:19.220</a></span> | <span class="t">So the output--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4280" target="_blank">01:11:20.100</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4281" target="_blank">01:11:21.020</a></span> | <span class="t">Yeah, the question is, how do you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4282" target="_blank">01:11:22.440</a></span> | <span class="t">get the keys and values and queries out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4284" target="_blank">01:11:24.220</a></span> | <span class="t">of this single collapsed output?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4285" target="_blank">01:11:25.700</a></span> | <span class="t">Now, remember, the output for each word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4287" target="_blank">01:11:27.900</a></span> | <span class="t">is just this weighted average of the value vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4290" target="_blank">01:11:30.620</a></span> | <span class="t">for the previous words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4293" target="_blank">01:11:33.020</a></span> | <span class="t">And then from that output for the next layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4295" target="_blank">01:11:35.780</a></span> | <span class="t">we apply a new key, query, and value transformation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4298" target="_blank">01:11:38.700</a></span> | <span class="t">to each of them for the next layer of self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4302" target="_blank">01:11:42.420</a></span> | <span class="t">So it's not actually that you're--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4303" target="_blank">01:11:43.780</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4304" target="_blank">01:11:44.280</a></span> | <span class="t">Yeah, you apply the key matrix, the query matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4310" target="_blank">01:11:50.780</a></span> | <span class="t">to the output of whatever came before it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4312" target="_blank">01:11:52.580</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4313" target="_blank">01:11:53.820</a></span> | <span class="t">And so just in a little bit of math,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4316" target="_blank">01:11:56.260</a></span> | <span class="t">we have these vectors, h1 through hn,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4319" target="_blank">01:11:59.820</a></span> | <span class="t">I'm going to call them the output of the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4323" target="_blank">01:12:03.420</a></span> | <span class="t">And then I've got vectors that are the output of the decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4328" target="_blank">01:12:08.020</a></span> | <span class="t">So I've got these z's I'm calling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4329" target="_blank">01:12:09.500</a></span> | <span class="t">the output of the decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4330" target="_blank">01:12:10.860</a></span> | <span class="t">And then I simply define my keys and my values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4335" target="_blank">01:12:15.340</a></span> | <span class="t">from the encoder vectors, these h's.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4339" target="_blank">01:12:19.460</a></span> | <span class="t">So I take the h's, I apply a key matrix and a value matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4342" target="_blank">01:12:22.860</a></span> | <span class="t">and then I define the queries from my decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4346" target="_blank">01:12:26.540</a></span> | <span class="t">So my queries here-- so this is why two of the arrows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4349" target="_blank">01:12:29.060</a></span> | <span class="t">come from the encoder, and one of the arrows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4351" target="_blank">01:12:31.300</a></span> | <span class="t">comes from the decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4352" target="_blank">01:12:32.540</a></span> | <span class="t">I've got my z's here, my queries, my keys and values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4356" target="_blank">01:12:36.140</a></span> | <span class="t">from the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4357" target="_blank">01:12:37.260</a></span> | <span class="t">So that is it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4364" target="_blank">01:12:44.320</a></span> | <span class="t">I've got a couple of minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4365" target="_blank">01:12:45.540</a></span> | <span class="t">I want to discuss some of the results of transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4368" target="_blank">01:12:48.340</a></span> | <span class="t">and I'm happy to answer more questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4369" target="_blank">01:12:49.920</a></span> | <span class="t">about transformers after class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4373" target="_blank">01:12:53.060</a></span> | <span class="t">So really, the original results of transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4376" target="_blank">01:12:56.340</a></span> | <span class="t">they had this big pitch for, oh, look,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4378" target="_blank">01:12:58.580</a></span> | <span class="t">you can do way more computation because of parallelization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4382" target="_blank">01:13:02.340</a></span> | <span class="t">They got great results in machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4384" target="_blank">01:13:04.780</a></span> | <span class="t">So you had-- you had transformers doing quite well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4393" target="_blank">01:13:13.060</a></span> | <span class="t">although not astoundingly better than existing machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4396" target="_blank">01:13:16.980</a></span> | <span class="t">translation systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4400" target="_blank">01:13:20.020</a></span> | <span class="t">But they were significantly more efficient to train.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4402" target="_blank">01:13:22.260</a></span> | <span class="t">Because you don't have this parallelization problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4405" target="_blank">01:13:25.380</a></span> | <span class="t">you could compute on much more data much faster,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4407" target="_blank">01:13:27.620</a></span> | <span class="t">and you could make use of faster GPUs much more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4412" target="_blank">01:13:32.100</a></span> | <span class="t">After that, there were things like document generation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4415" target="_blank">01:13:35.060</a></span> | <span class="t">where you had the old standard of sequence-to-sequence models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4417" target="_blank">01:13:37.940</a></span> | <span class="t">to the LSTMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4419" target="_blank">01:13:39.060</a></span> | <span class="t">And eventually, everything became transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4422" target="_blank">01:13:42.420</a></span> | <span class="t">all the way down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4425" target="_blank">01:13:45.140</a></span> | <span class="t">Transformers also enabled this revolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4427" target="_blank">01:13:47.340</a></span> | <span class="t">into pre-training, which we'll go over in next year,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4430" target="_blank">01:13:50.420</a></span> | <span class="t">next class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4432" target="_blank">01:13:52.060</a></span> | <span class="t">And the efficiency, the parallelizability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4434" target="_blank">01:13:54.660</a></span> | <span class="t">allows you to compute on tons and tons of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4438" target="_blank">01:13:58.340</a></span> | <span class="t">And so after a certain point, on standard large benchmarks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4442" target="_blank">01:14:02.580</a></span> | <span class="t">everything became transformer-based.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4444" target="_blank">01:14:04.740</a></span> | <span class="t">This ability to make use of lots and lots of data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4447" target="_blank">01:14:07.540</a></span> | <span class="t">lots and lots of compute, just put transformers head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4450" target="_blank">01:14:10.380</a></span> | <span class="t">and shoulders above LSTMs in, let's say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4453" target="_blank">01:14:13.420</a></span> | <span class="t">almost every modern advancement in natural language processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4459" target="_blank">01:14:19.900</a></span> | <span class="t">There are many drawbacks and variants to transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4464" target="_blank">01:14:24.620</a></span> | <span class="t">The clearest one that people have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4465" target="_blank">01:14:25.820</a></span> | <span class="t">tried to work on quite a bit is this quadratic compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4468" target="_blank">01:14:28.420</a></span> | <span class="t">problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4469" target="_blank">01:14:29.260</a></span> | <span class="t">So this all pairs of interactions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4471" target="_blank">01:14:31.780</a></span> | <span class="t">means that our total computation for each block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4474" target="_blank">01:14:34.420</a></span> | <span class="t">grows quadratically with the sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4476" target="_blank">01:14:36.260</a></span> | <span class="t">And in a student's question, we heard that, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4479" target="_blank">01:14:39.780</a></span> | <span class="t">as the sequence length becomes long,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4481" target="_blank">01:14:41.580</a></span> | <span class="t">if I want to process a whole Wikipedia article,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4484" target="_blank">01:14:44.220</a></span> | <span class="t">a whole novel, that becomes quite unfeasible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4488" target="_blank">01:14:48.100</a></span> | <span class="t">And actually, that's a step backwards in some sense,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4490" target="_blank">01:14:50.780</a></span> | <span class="t">because for recurrent neural networks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4492" target="_blank">01:14:52.740</a></span> | <span class="t">it only grew linearly with the sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4495" target="_blank">01:14:55.540</a></span> | <span class="t">Other things people have tried to work on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4497" target="_blank">01:14:57.380</a></span> | <span class="t">are better position representations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4499" target="_blank">01:14:59.860</a></span> | <span class="t">because the absolute index of a word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4502" target="_blank">01:15:02.060</a></span> | <span class="t">is not really the best way maybe to represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4505" target="_blank">01:15:05.260</a></span> | <span class="t">its position in a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4507" target="_blank">01:15:07.940</a></span> | <span class="t">And just to give you an intuition of quadratic sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4510" target="_blank">01:15:10.300</a></span> | <span class="t">length, remember that we had this big matrix multiply here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4513" target="_blank">01:15:13.620</a></span> | <span class="t">that resulted in this matrix of n by n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4516" target="_blank">01:15:16.860</a></span> | <span class="t">And computing this is a big cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4520" target="_blank">01:15:20.380</a></span> | <span class="t">It costs a lot of memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4522" target="_blank">01:15:22.420</a></span> | <span class="t">And so there's been work--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4523" target="_blank">01:15:23.700</a></span> | <span class="t">oh, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4524" target="_blank">01:15:24.220</a></span> | <span class="t">And so if you think of the model dimensionality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4526" target="_blank">01:15:26.260</a></span> | <span class="t">as like 1,000, although today it gets much larger,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4529" target="_blank">01:15:29.340</a></span> | <span class="t">then for a short sequence of n is roughly 30,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4532" target="_blank">01:15:32.460</a></span> | <span class="t">maybe if you're computing n squared times d, 30 isn't so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4537" target="_blank">01:15:37.540</a></span> | <span class="t">bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4538" target="_blank">01:15:38.740</a></span> | <span class="t">But if you had something like 50,000,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4542" target="_blank">01:15:42.020</a></span> | <span class="t">then n squared becomes huge and sort of totally infeasible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4546" target="_blank">01:15:46.540</a></span> | <span class="t">So people have tried to sort of map things down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4549" target="_blank">01:15:49.180</a></span> | <span class="t">to a lower dimensional space to get rid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4551" target="_blank">01:15:51.060</a></span> | <span class="t">of the sort of quadratic computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4554" target="_blank">01:15:54.540</a></span> | <span class="t">But in practice, I mean, as people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4556" target="_blank">01:15:56.820</a></span> | <span class="t">have gone to things like GPT-3, Chat-GPT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4559" target="_blank">01:15:59.700</a></span> | <span class="t">most of the computation doesn't show up in the self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4563" target="_blank">01:16:03.260</a></span> | <span class="t">So people are wondering sort of is it even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4565" target="_blank">01:16:05.500</a></span> | <span class="t">necessary to get rid of the self-attention operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4568" target="_blank">01:16:08.900</a></span> | <span class="t">quadratic constraint?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4570" target="_blank">01:16:10.300</a></span> | <span class="t">It's an open form of research whether this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4572" target="_blank">01:16:12.900</a></span> | <span class="t">is sort of necessary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4574" target="_blank">01:16:14.780</a></span> | <span class="t">And then finally, there have been a ton of modifications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4577" target="_blank">01:16:17.460</a></span> | <span class="t">to the transformer over the last five, four-ish years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4581" target="_blank">01:16:21.940</a></span> | <span class="t">And it turns out that the original transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4585" target="_blank">01:16:25.100</a></span> | <span class="t">plus maybe a couple of modifications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4587" target="_blank">01:16:27.820</a></span> | <span class="t">is pretty much the best thing there is still.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4591" target="_blank">01:16:31.140</a></span> | <span class="t">There have been a couple of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4592" target="_blank">01:16:32.580</a></span> | <span class="t">that end up being important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4593" target="_blank">01:16:33.940</a></span> | <span class="t">Changing out the nonlinearities in the feedforward network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4597" target="_blank">01:16:37.380</a></span> | <span class="t">ends up being important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4598" target="_blank">01:16:38.740</a></span> | <span class="t">But it's had lasting power so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4603" target="_blank">01:16:43.140</a></span> | <span class="t">But I think it's ripe for people to come through and think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4606" target="_blank">01:16:46.340</a></span> | <span class="t">about how to sort of improve it in various ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4609" target="_blank">01:16:49.220</a></span> | <span class="t">So pre-training is on Tuesday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4612" target="_blank">01:16:52.020</a></span> | <span class="t">Good luck on assignment four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4613" target="_blank">01:16:53.260</a></span> | <span class="t">And then we'll have the project proposal documents out tonight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4617" target="_blank">01:16:57.060</a></span> | <span class="t">for you to talk about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LWMzyfvuehA&t=4619" target="_blank">01:16:59.140</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>