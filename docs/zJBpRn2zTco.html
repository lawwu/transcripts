<html><head><title>Llama 2: Full Breakdown</title></head><body><a href="index.html">back to index</a><h2>Llama 2: Full Breakdown</h2><a href="https://www.youtube.com/watch?v=zJBpRn2zTco"><img src="https://i.ytimg.com/vi/zJBpRn2zTco/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=201">3:21</a> Reward Modeling<br><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=247">4:7</a> Helpfulness and Safety<br><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=363">6:3</a> Safety Testing in English<br><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=844">14:4</a> Llama 2 Widely Available<br><br><div style="text-align: left;"><a href="./zJBpRn2zTco.html">Whisper Transcript</a> | <a href="./transcript_zJBpRn2zTco.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=0">00:00:00.000</a></span> | <span class="t">Less than 24 hours ago, Meta released Lama 2, their successor to the open-source Lama language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=7">00:00:07.480</a></span> | <span class="t">model that helped spawn a hundred others including Alpaca, Vicuña and of course Orca.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=13">00:00:13.300</a></span> | <span class="t">Within a few hours of release, I had read the fascinating 76-page technical paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=18">00:00:18.420</a></span> | <span class="t">the use guide, each of the many release pages, the full terms and conditions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=23">00:00:23.420</a></span> | <span class="t">and I have run many of my own experiments. Let's start with the basics, it was trained on more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=28">00:00:28.880</a></span> | <span class="t">data, the biggest model has more parameters and the context length has doubled. They also spent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=35">00:00:35.040</a></span> | <span class="t">what must be tens of millions on fine-tuning it for chat, but I'll get into that more later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=40">00:00:40.440</a></span> | <span class="t">But let's start with the benchmarks. They deliberately compared Lama 2 to Lama 1 and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=47">00:00:47.780</a></span> | <span class="t">other famous open-source models, but not with GPT-4. And in these benchmarks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=52">00:00:52.720</a></span> | <span class="t">the trend is fairly clear. It crushes the other open-source language models, but is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=57">00:00:57.940</a></span> | <span class="t">more of an incremental change. So, let's start with the benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=58">00:00:58.860</a></span> | <span class="t">Lama 1. To massively simplify, the MMLU benchmark shows that it knows a lot about a lot of subjects,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=67">00:01:07.240</a></span> | <span class="t">but the human eval benchmark shows that it's not amazing at coding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=71">00:01:11.860</a></span> | <span class="t">But now it's time for the paper and here are the highlights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=75">00:01:15.920</a></span> | <span class="t">On data, they say they used more robust data cleaning and trained on 40% more total tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=84">00:01:24.060</a></span> | <span class="t">They say they didn't include any data from Meta's products or services,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=88">00:01:28.700</a></span> | <span class="t">but what they did do is up-sample the most factual sources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=92">00:01:32.920</a></span> | <span class="t">If you don't think that's much information about the data, you are correct, because all they say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=97">00:01:37.780</a></span> | <span class="t">is it was trained on a new mix of publicly available data. Absolutely no mention of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=104">00:01:44.260</a></span> | <span class="t">any sources here at all. After pre-training on those 2 trillion tokens, the models still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=110">00:01:50.180</a></span> | <span class="t">did not show any sign of saturation. The loss going down here represents an improvement,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=115">00:01:55.200</a></span> | <span class="t">and as you can see, they could have kept going.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=117">00:01:57.820</a></span> | <span class="t">On page 8, we have some quick comparisons with Palm 2, the model behind BARD, and of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=123">00:02:03.320</a></span> | <span class="t">GPT 3.5, the original ChatGPT, and GPT 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=126">00:02:06.800</a></span> | <span class="t">Obviously, this comparison doesn't look great for Lama 2, especially in coding, in this row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=132">00:02:12.480</a></span> | <span class="t">But now let's compare it to other open source models. Here it is being better at coding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=137">00:02:17.660</a></span> | <span class="t">common sense, reading comprehension, but notice it wasn't compared to Orca or PHY1, both of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=142">00:02:22.960</a></span> | <span class="t">which I've done videos on, and I found that interesting given that both are apparently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=146">00:02:26.940</a></span> | <span class="t">set to be open sourced. PHY1, for example, at only 1.3 billion parameters, got around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=153">00:02:33.500</a></span> | <span class="t">50% for code. And I'll get to more Orca comparisons in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=158">00:02:38.360</a></span> | <span class="t">What about the decision itself to release the model? As you can see here, they show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=162">00:02:42.940</a></span> | <span class="t">off a list of corporate supporters of the decision to open source the model. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=168">00:02:48.940</a></span> | <span class="t">if you remember the safety statement signed by all the top AGI labs and world experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=174">00:02:54.780</a></span> | <span class="t">in AI. Well, I think Meta got a little bit of a shock.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=176">00:02:56.060</a></span> | <span class="t">They came up with their own statement of support for Meta's open approach to today's AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=184">00:03:04.140</a></span> | <span class="t">I'll let you decide if this list is as impressive as the other one, but I did note Mark Andreessen,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=190">00:03:10.940</a></span> | <span class="t">who is on the board of directors of Meta. Back to the paper, and they went into immense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=196">00:03:16.200</a></span> | <span class="t">detail into their reinforcement learning with human feedback process. Way too much for me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=201">00:03:21.180</a></span> | <span class="t">to cover in this video. The short version is that reward modeling is a way of telling the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=206">00:03:26.020</a></span> | <span class="t">base model which outputs humans prefer. And you can see the millions of human rated comparisons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=212">00:03:32.200</a></span> | <span class="t">that were used for Llama 2. Think of it as doggy training the model with treats and admonitions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=218">00:03:38.500</a></span> | <span class="t">And interestingly, they trained two separate reward models, one optimized for helpfulness</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=223">00:03:43.220</a></span> | <span class="t">and the other for safety. And they tried to make sure that the reward models or doggy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=228">00:03:48.160</a></span> | <span class="t">trainers were as smart as the dog itself. Or in technical speak, we initialized our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=233">00:03:53.300</a></span> | <span class="t">reward models from pre-trained chat model checkpoints.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=235">00:03:55.980</a></span> | <span class="t">In short, the reward model knows what the chat model knows. And that is to prevent cases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=241">00:04:01.520</a></span> | <span class="t">where the base model just hallucinates and the reward model can't tell the difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=246">00:04:06.280</a></span> | <span class="t">They do describe at great length a trade-off though between helpfulness and safety, as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=251">00:04:11.600</a></span> | <span class="t">illustrated here. Someone asked, I'm going to be participating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=254">00:04:14.880</a></span> | <span class="t">in a comedy roast, what are some hilariously spicy roasts I can use? And on the right we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=260">00:04:20.540</a></span> | <span class="t">have the two doggy trainers, the safety reward model score and the helpfulness reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=265">00:04:25.940</a></span> | <span class="t">score. As we go down, more safety data is being ingested. And early on, as you can see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=270">00:04:30.980</a></span> | <span class="t">the model is pretty quote unquote helpful giving these roasts. Obviously you can let</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=275">00:04:35.440</a></span> | <span class="t">me know what you think of them, but note they get low safety scores. As the model gets more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=280">00:04:40.440</a></span> | <span class="t">safety training though, the safety score goes up, but the helpfulness score goes down. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=285">00:04:45.940</a></span> | <span class="t">get more of these, I can't satisfy your request kind of answers. And I'm going to skip to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=290">00:04:50.580</a></span> | <span class="t">one of the experiments I was going to show you later, which is when I was trying to benchmark</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=295">00:04:55.260</a></span> | <span class="t">Llama 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=295">00:04:55.900</a></span> | <span class="t">I've applied to download the model, but at the moment this is just a hugging face space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=300">00:05:00.140</a></span> | <span class="t">And I was trying to ask it a common sense question from the Hella Swag benchmark and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=304">00:05:04.940</a></span> | <span class="t">it just refused to answer. They call this in the paper false refusal and I find it happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=309">00:05:09.720</a></span> | <span class="t">quite a lot. The paper claims on page 19 that the 70 billion parameter version of Llama 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=316">00:05:16.100</a></span> | <span class="t">is more helpful than a particular version of Chachi BT, winning more often than it loses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=321">00:05:21.260</a></span> | <span class="t">But later they admit something which I definitely agree with. While our results indicate that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=325">00:05:25.860</a></span> | <span class="t">Llama 2 Chat is on par with Chachi BT on human evaluations, it's important to note that human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=332">00:05:32.020</a></span> | <span class="t">evaluations have several limitations. It says the prompt set doesn't cover coding or reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=338">00:05:38.020</a></span> | <span class="t">related prompts. They only evaluate the final generation of a multi-turn conversation and human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=344">00:05:44.100</a></span> | <span class="t">evaluation is inherently subjective and noisy. I like to judge models based on mathematics and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=349">00:05:49.860</a></span> | <span class="t">reasoning, so I might be biased in one direction. Also Llama 2 is not nearly as good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=355">00:05:55.820</a></span> | <span class="t">when you're using it in languages other than English, which is not surprising given the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=360">00:06:00.460</a></span> | <span class="t">language distribution in the pre-training data. I also find it interesting that they did all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=365">00:06:05.320</a></span> | <span class="t">their safety testing in English and they warn developers before deploying any applications of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=371">00:06:11.040</a></span> | <span class="t">Llama 2, do your own safety testing and tuning tailored to your specific application. On compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=377">00:06:17.000</a></span> | <span class="t">they don't say much other than that it was trained on A100s. I am sure Llama 3 will be trained on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=385">00:06:25.780</a></span> | <span class="t">A100s, but apparently Meta has purchased more of those than any other company including Microsoft.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=391">00:06:31.220</a></span> | <span class="t">Mind you Llama 2 was trained between January and July apparently, so it's understandable they used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=397">00:06:37.400</a></span> | <span class="t">the earlier A100s. Back to the decision to release and it does seem interesting to me that Meta and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=403">00:06:43.760</a></span> | <span class="t">Zuckerberg have seemingly ignored this letter from the US Senate. It was written in early June and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=410">00:06:50.280</a></span> | <span class="t">toward the end it said this: "By purporting to release Llama for the purpose of researching the abuse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=415">00:06:55.740</a></span> | <span class="t">of AI, Meta effectively appears to have put a powerful tool in the hands of bad actors to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=421">00:07:01.980</a></span> | <span class="t">engage in such abuse without much discernible forethought, preparation or safeguards." In the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=428">00:07:08.420</a></span> | <span class="t">paper they defend it and say this release promotes transparency, it democratizes the technology and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=434">00:07:14.920</a></span> | <span class="t">creates a more level playing field for organizations of all sizes across the globe to benefit from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=440">00:07:20.460</a></span> | <span class="t">economic growth promised by the advancement of AI. But before anyone gets too enchanted by that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=445">00:07:25.700</a></span> | <span class="t">Zuckerberg has recently said that they're only releasing because it's far away from AGI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=451">00:07:31.340</a></span> | <span class="t">And I think Google's palm model is also I think has about 10 times as many parameters. Now the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=456">00:07:36.540</a></span> | <span class="t">Llama models are very efficient so they perform well for something that's around 65 billion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=460">00:07:40.940</a></span> | <span class="t">parameters. So for me that was also part of this because there's this whole debate around you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=466">00:07:46.220</a></span> | <span class="t">is it good for everyone in the world to have access to the most frontier AI models. And I think as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=475">00:07:55.660</a></span> | <span class="t">models start approaching something that's like a super human intelligence, that's a bigger question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=482">00:08:02.260</a></span> | <span class="t">that we'll have to grapple with. But right now I mean these are still very basic tools.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=487">00:08:07.780</a></span> | <span class="t">I suspect that the bigger reason for release relates to an earlier answer he gave in the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=493">00:08:13.540</a></span> | <span class="t">interview. Basically his researchers demanded it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=496">00:08:16.620</a></span> | <span class="t">Part of this is we want to have the best people in the world researching this and a lot of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=502">00:08:22.300</a></span> | <span class="t">best people want to know that they're going to be able to share their work. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=505">00:08:25.620</a></span> | <span class="t">part of the deal that we have is that you know we can get you know if you're one of the top AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=511">00:08:31.380</a></span> | <span class="t">researchers in the world and come here you can get access to kind of industry scale infrastructure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=516">00:08:36.060</a></span> | <span class="t">and part of our ethos is that we want to share what's invented broadly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=521">00:08:41.940</a></span> | <span class="t">And if Zuckerberg had refused to release some of those researchers could have just gone off and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=527">00:08:47.580</a></span> | <span class="t">made their own company as these guys did. Mistral AI is valued at 240 million despite being only four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=535">00:08:55.580</a></span> | <span class="t">weeks old and contains some key employees from Meta. One even complained before deleting the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=541">00:09:01.700</a></span> | <span class="t">tweet about not being included in the author list of the Lama 2 paper. This was the pitch memo that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=548">00:09:08.300</a></span> | <span class="t">Mistral used to raise those hundreds of millions of euros and they focus on taking a more open</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=554">00:09:14.540</a></span> | <span class="t">approach to model development. So the point still stands if a CEO blocks a model being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=559">00:09:19.580</a></span> | <span class="t">open source if the researchers want to they can just defect to XAI or just start their own company.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=565">00:09:25.540</a></span> | <span class="t">So in a way Zuckerberg had few options. I must say though that I did raise an eyebrow when I read</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=571">00:09:31.340</a></span> | <span class="t">these paragraphs. This is on page 35 of the technical paper and they say not everyone who</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=576">00:09:36.780</a></span> | <span class="t">uses AI models has good intentions. AI agents could potentially be used for nefarious purposes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=582">00:09:42.540</a></span> | <span class="t">such as misinformation or bioterrorism or cyber crime. However we have made efforts to tune the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=587">00:09:47.980</a></span> | <span class="t">models to avoid these topics and indeed cyber criminals have already come up with worm GPT to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=593">00:09:53.500</a></span> | <span class="t">help them do phishing campaigns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=595">00:09:55.500</a></span> | <span class="t">But Meta points them to their responsible use guide which I am sure they will follow. I read that 24</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=602">00:10:02.420</a></span> | <span class="t">page guide and to be honest it was kind of a waste of time. They said pretty much nothing. It was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=609">00:10:09.220</a></span> | <span class="t">really bland and generic. Maybe that's harsh let me know if I missed something but it was all pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=615">00:10:15.860</a></span> | <span class="t">vague. They did try some red teaming only in English for things like the production of weapons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=621">00:10:21.940</a></span> | <span class="t">and lots of other risk categories. But you will be really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=625">00:10:25.460</a></span> | <span class="t">assured first that any such illegal or unlawful activity is against their terms and conditions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=631">00:10:31.900</a></span> | <span class="t">and second that they are looking for the community to do further research and red teaming. Anyway I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=637">00:10:37.340</a></span> | <span class="t">am keen to do many more experiments but using this Gradio demo it basically failed to do a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=643">00:10:43.980</a></span> | <span class="t">proper sonnet and when I asked it this question from the math benchmark it said the question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=649">00:10:49.420</a></span> | <span class="t">does not make sense because the length of a rectangle being twice its width would mean the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=655">00:10:55.420</a></span> | <span class="t">length of a rectangle is a square. Hmm. Anyway it could just be a problem with that demo because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=661">00:11:01.220</a></span> | <span class="t">GPT 3.5 crushes the sonnet about apples and has no problem with the length of a rectangle being twice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=667">00:11:07.620</a></span> | <span class="t">its width. Which brings me on to a benchmark that the Lama 2 paper did talk about on page 48. It was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=675">00:11:15.380</a></span> | <span class="t">on social IQ and they noted that Lama 1 actually did better than Lama 2. Here is the benchmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=682">00:11:22.260</a></span> | <span class="t">It's about common sense reasoning with questions such as these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=685">00:11:25.380</a></span> | <span class="t">Alex spilled the food she just prepared all over the floor and it made a huge mess. What will Alex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=690">00:11:30.660</a></span> | <span class="t">want to do next? Taste the food, mop up, run around in a mess. And again apparently Lama 1 actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=697">00:11:37.180</a></span> | <span class="t">does slightly better on those kind of questions. Another benchmark that you can see Lama 1 being as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=703">00:11:43.100</a></span> | <span class="t">good as Lama 2 at is Ball Q. That's a benchmark testing yes or no questions but it's harder than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=708">00:11:48.980</a></span> | <span class="t">that. You have to read a lot of context to get the answer right. I just want you to remember some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=714">00:11:54.140</a></span> | <span class="t">these benchmarks when you hear the question. So I'm going to go ahead and read the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=715">00:11:55.340</a></span> | <span class="t">So I'm going to go ahead and read the answer. So I'm going to go ahead and read the answer. So I'm going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=715">00:11:55.380</a></span> | <span class="t">hear all the influencers talk about Lama 2 completely changing everything. Also if someone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=720">00:12:00.020</a></span> | <span class="t">says it's the best model of its size look at Lama 2 13 billion parameters. Of course it depends on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=726">00:12:06.340</a></span> | <span class="t">the benchmark but it got 21.7 percent in Aquarat. That's a test of mathematical reasoning and Orca</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=733">00:12:13.380</a></span> | <span class="t">at the exact same size of 13 billion parameters got almost 28 percent. So even pound for pound</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=739">00:12:19.820</a></span> | <span class="t">it may not be the best in all categories. To be honest I feel like there might be a loyally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=745">00:12:25.300</a></span> | <span class="t">struggle going on behind the scenes at Microsoft about whether to open source Orca and Phi 1. There</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=751">00:12:31.140</a></span> | <span class="t">were some bonus interesting things about the paper like introducing ghost attention which to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=756">00:12:36.580</a></span> | <span class="t">oversimplify means that the model pays attention over multiple turns of the conversation something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=762">00:12:42.500</a></span> | <span class="t">you might have originally told it such as always act as Napoleon from now. Essentially these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=767">00:12:47.460</a></span> | <span class="t">diagrams show that with ghost attention the model pays more attention to that original command act</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=773">00:12:53.060</a></span> | <span class="t">as Oscar Wilde or always act as Napoleon from now. So I'm going to go ahead and read the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=775">00:12:55.260</a></span> | <span class="t">The authors also throw in this observation that LLMs have internalized the concept of time and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=783">00:13:03.780</a></span> | <span class="t">despite their training being solely based on next token prediction and data that is randomly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=790">00:13:10.100</a></span> | <span class="t">shuffled without regard to their chronological context the models pick up a general sense of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=795">00:13:15.300</a></span> | <span class="t">what time is. Even when provided with minimal data they know what people wouldn't have known.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=801">00:13:21.220</a></span> | <span class="t">For example with a knowledge cutoff of 1940 when asked who wanted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=805">00:13:25.220</a></span> | <span class="t">to win the second world war they say I'm not sure what you're referring to my knowledge stopped in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=809">00:13:29.660</a></span> | <span class="t">1940. Right at the end of the report I know many people will be shocked to hear that when they did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=815">00:13:35.340</a></span> | <span class="t">a sentiment analysis of the model they found that the sentiment for Llama 2 for right wing was higher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=822">00:13:42.460</a></span> | <span class="t">than for left wing. You may even want to pause and look at this page from a sociological perspective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=828">00:13:48.940</a></span> | <span class="t">because if Llama 2 was trained on a semi-random swathe of the internet this could be like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=835">00:13:55.180</a></span> | <span class="t">snapshot of the sentiment analysis of all of these terms across the internet. Anyway in what may have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=841">00:14:01.220</a></span> | <span class="t">been a surprising twist for some Microsoft and Meta teamed up to make Llama 2 widely available</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=848">00:14:08.420</a></span> | <span class="t">and we get news that Llama 2 may soon be on your phone and PC. Although I think Meta want to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=854">00:14:14.720</a></span> | <span class="t">paid if it's going to come to your iPhone with this curious clause requiring permission if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=860">00:14:20.340</a></span> | <span class="t">have more than 700 million monthly active users. I don't know whether they were thinking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=865">00:14:25.140</a></span> | <span class="t">Apple or Telegram or TikTok but I think they want to get paid if any of those are going to use Llama</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=871">00:14:31.620</a></span> | <span class="t">2. But I must confess to finding the previous clause somewhat ironic. You will not use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=877">00:14:37.540</a></span> | <span class="t">Llama materials or any output or results of the Llama materials to improve any other large language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=883">00:14:43.800</a></span> | <span class="t">model. So they can use any part of the internet which one leak said might include copyrighted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=888">00:14:48.960</a></span> | <span class="t">works but you can't use Llama to improve your own model. Well just two hours ago people are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=895">00:14:55.100</a></span> | <span class="t">already updating models like Lava based on Llama 2. So it will likely just be a few days or weeks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=902">00:15:02.480</a></span> | <span class="t">until we see a newly improved Vicuña or Orca. Jim Fan predicts that Llama 2 will dramatically boost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=909">00:15:09.600</a></span> | <span class="t">multimodal AI and robotics research. He says these fields need more than just black box access to an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=915">00:15:15.860</a></span> | <span class="t">API. So far we have had to convert the complex sensory signals video audio 3D perception to text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=922">00:15:22.920</a></span> | <span class="t">description and then feed to an LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=925">00:15:25.060</a></span> | <span class="t">It would be much more effective to graft those sensory modules directly onto a strong LLM backbone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=932">00:15:32.020</a></span> | <span class="t">Anyway this video is already long enough and this is just the first 24 hours of Llama 2's release.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=938">00:15:38.380</a></span> | <span class="t">I am sure there will be much more discussion in the coming days and weeks. Let me know what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=943">00:15:43.540</a></span> | <span class="t">you think in the comments and thank you so much for watching. Have a wonderful day.</span></div></div></body></html>