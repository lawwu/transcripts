<html><head><title>Llama 2: Full Breakdown</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }
        .container {
            width: 80%;
            margin: auto;
            overflow: hidden;
        }
        h2, h3 {
            color: #333;
            text-align: center;
        }
        a {
            color: #0000FF;  /* Traditional blue color for links */
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        img {
            display: block;
            margin: auto;
            max-width: 100%;
        }
        .c {
            margin: 10px 0;
        }
        .s, .t {
            display: inline-block;
            margin-right: 5px;
        }
        .max-width {
            max-width: 800px;
            margin: auto;
        }
    </style>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Llama 2: Full Breakdown</h2><a href="https://www.youtube.com/watch?v=zJBpRn2zTco"><img src="https://i.ytimg.com/vi/zJBpRn2zTco/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=201">3:21</a> Reward Modeling<br><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=247">4:7</a> Helpfulness and Safety<br><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=363">6:3</a> Safety Testing in English<br><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=844">14:4</a> Llama 2 Widely Available<br><br><div style="text-align: left;"><a href="./zJBpRn2zTco.html">Whisper Transcript</a> | <a href="./transcript_zJBpRn2zTco.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Less than 24 hours ago, Meta released Lama 2, their successor to the open-source Lama language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=7" target="_blank">00:00:07.480</a></span> | <span class="t">model that helped spawn a hundred others including Alpaca, Vicuña and of course Orca.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=13" target="_blank">00:00:13.300</a></span> | <span class="t">Within a few hours of release, I had read the fascinating 76-page technical paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=18" target="_blank">00:00:18.420</a></span> | <span class="t">the use guide, each of the many release pages, the full terms and conditions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=23" target="_blank">00:00:23.420</a></span> | <span class="t">and I have run many of my own experiments. Let's start with the basics, it was trained on more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=28" target="_blank">00:00:28.880</a></span> | <span class="t">data, the biggest model has more parameters and the context length has doubled. They also spent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=35" target="_blank">00:00:35.040</a></span> | <span class="t">what must be tens of millions on fine-tuning it for chat, but I'll get into that more later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=40" target="_blank">00:00:40.440</a></span> | <span class="t">But let's start with the benchmarks. They deliberately compared Lama 2 to Lama 1 and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=47" target="_blank">00:00:47.780</a></span> | <span class="t">other famous open-source models, but not with GPT-4. And in these benchmarks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=52" target="_blank">00:00:52.720</a></span> | <span class="t">the trend is fairly clear. It crushes the other open-source language models, but is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=57" target="_blank">00:00:57.940</a></span> | <span class="t">more of an incremental change. So, let's start with the benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=58" target="_blank">00:00:58.860</a></span> | <span class="t">Lama 1. To massively simplify, the MMLU benchmark shows that it knows a lot about a lot of subjects,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=67" target="_blank">00:01:07.240</a></span> | <span class="t">but the human eval benchmark shows that it's not amazing at coding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=71" target="_blank">00:01:11.860</a></span> | <span class="t">But now it's time for the paper and here are the highlights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=75" target="_blank">00:01:15.920</a></span> | <span class="t">On data, they say they used more robust data cleaning and trained on 40% more total tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=84" target="_blank">00:01:24.060</a></span> | <span class="t">They say they didn't include any data from Meta's products or services,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=88" target="_blank">00:01:28.700</a></span> | <span class="t">but what they did do is up-sample the most factual sources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=92" target="_blank">00:01:32.920</a></span> | <span class="t">If you don't think that's much information about the data, you are correct, because all they say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=97" target="_blank">00:01:37.780</a></span> | <span class="t">is it was trained on a new mix of publicly available data. Absolutely no mention of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=104" target="_blank">00:01:44.260</a></span> | <span class="t">any sources here at all. After pre-training on those 2 trillion tokens, the models still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=110" target="_blank">00:01:50.180</a></span> | <span class="t">did not show any sign of saturation. The loss going down here represents an improvement,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=115" target="_blank">00:01:55.200</a></span> | <span class="t">and as you can see, they could have kept going.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=117" target="_blank">00:01:57.820</a></span> | <span class="t">On page 8, we have some quick comparisons with Palm 2, the model behind BARD, and of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=123" target="_blank">00:02:03.320</a></span> | <span class="t">GPT 3.5, the original ChatGPT, and GPT 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=126" target="_blank">00:02:06.800</a></span> | <span class="t">Obviously, this comparison doesn't look great for Lama 2, especially in coding, in this row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=132" target="_blank">00:02:12.480</a></span> | <span class="t">But now let's compare it to other open source models. Here it is being better at coding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=137" target="_blank">00:02:17.660</a></span> | <span class="t">common sense, reading comprehension, but notice it wasn't compared to Orca or PHY1, both of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=142" target="_blank">00:02:22.960</a></span> | <span class="t">which I've done videos on, and I found that interesting given that both are apparently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=146" target="_blank">00:02:26.940</a></span> | <span class="t">set to be open sourced. PHY1, for example, at only 1.3 billion parameters, got around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=153" target="_blank">00:02:33.500</a></span> | <span class="t">50% for code. And I'll get to more Orca comparisons in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=158" target="_blank">00:02:38.360</a></span> | <span class="t">What about the decision itself to release the model? As you can see here, they show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=162" target="_blank">00:02:42.940</a></span> | <span class="t">off a list of corporate supporters of the decision to open source the model. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=168" target="_blank">00:02:48.940</a></span> | <span class="t">if you remember the safety statement signed by all the top AGI labs and world experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=174" target="_blank">00:02:54.780</a></span> | <span class="t">in AI. Well, I think Meta got a little bit of a shock.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=176" target="_blank">00:02:56.060</a></span> | <span class="t">They came up with their own statement of support for Meta's open approach to today's AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=184" target="_blank">00:03:04.140</a></span> | <span class="t">I'll let you decide if this list is as impressive as the other one, but I did note Mark Andreessen,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=190" target="_blank">00:03:10.940</a></span> | <span class="t">who is on the board of directors of Meta. Back to the paper, and they went into immense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=196" target="_blank">00:03:16.200</a></span> | <span class="t">detail into their reinforcement learning with human feedback process. Way too much for me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=201" target="_blank">00:03:21.180</a></span> | <span class="t">to cover in this video. The short version is that reward modeling is a way of telling the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=206" target="_blank">00:03:26.020</a></span> | <span class="t">base model which outputs humans prefer. And you can see the millions of human rated comparisons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=212" target="_blank">00:03:32.200</a></span> | <span class="t">that were used for Llama 2. Think of it as doggy training the model with treats and admonitions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=218" target="_blank">00:03:38.500</a></span> | <span class="t">And interestingly, they trained two separate reward models, one optimized for helpfulness</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=223" target="_blank">00:03:43.220</a></span> | <span class="t">and the other for safety. And they tried to make sure that the reward models or doggy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=228" target="_blank">00:03:48.160</a></span> | <span class="t">trainers were as smart as the dog itself. Or in technical speak, we initialized our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=233" target="_blank">00:03:53.300</a></span> | <span class="t">reward models from pre-trained chat model checkpoints.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=235" target="_blank">00:03:55.980</a></span> | <span class="t">In short, the reward model knows what the chat model knows. And that is to prevent cases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=241" target="_blank">00:04:01.520</a></span> | <span class="t">where the base model just hallucinates and the reward model can't tell the difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=246" target="_blank">00:04:06.280</a></span> | <span class="t">They do describe at great length a trade-off though between helpfulness and safety, as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=251" target="_blank">00:04:11.600</a></span> | <span class="t">illustrated here. Someone asked, I'm going to be participating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=254" target="_blank">00:04:14.880</a></span> | <span class="t">in a comedy roast, what are some hilariously spicy roasts I can use? And on the right we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=260" target="_blank">00:04:20.540</a></span> | <span class="t">have the two doggy trainers, the safety reward model score and the helpfulness reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=265" target="_blank">00:04:25.940</a></span> | <span class="t">score. As we go down, more safety data is being ingested. And early on, as you can see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=270" target="_blank">00:04:30.980</a></span> | <span class="t">the model is pretty quote unquote helpful giving these roasts. Obviously you can let</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=275" target="_blank">00:04:35.440</a></span> | <span class="t">me know what you think of them, but note they get low safety scores. As the model gets more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=280" target="_blank">00:04:40.440</a></span> | <span class="t">safety training though, the safety score goes up, but the helpfulness score goes down. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=285" target="_blank">00:04:45.940</a></span> | <span class="t">get more of these, I can't satisfy your request kind of answers. And I'm going to skip to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=290" target="_blank">00:04:50.580</a></span> | <span class="t">one of the experiments I was going to show you later, which is when I was trying to benchmark</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=295" target="_blank">00:04:55.260</a></span> | <span class="t">Llama 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=295" target="_blank">00:04:55.900</a></span> | <span class="t">I've applied to download the model, but at the moment this is just a hugging face space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=300" target="_blank">00:05:00.140</a></span> | <span class="t">And I was trying to ask it a common sense question from the Hella Swag benchmark and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=304" target="_blank">00:05:04.940</a></span> | <span class="t">it just refused to answer. They call this in the paper false refusal and I find it happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=309" target="_blank">00:05:09.720</a></span> | <span class="t">quite a lot. The paper claims on page 19 that the 70 billion parameter version of Llama 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=316" target="_blank">00:05:16.100</a></span> | <span class="t">is more helpful than a particular version of Chachi BT, winning more often than it loses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=321" target="_blank">00:05:21.260</a></span> | <span class="t">But later they admit something which I definitely agree with. While our results indicate that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=325" target="_blank">00:05:25.860</a></span> | <span class="t">Llama 2 Chat is on par with Chachi BT on human evaluations, it's important to note that human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=332" target="_blank">00:05:32.020</a></span> | <span class="t">evaluations have several limitations. It says the prompt set doesn't cover coding or reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=338" target="_blank">00:05:38.020</a></span> | <span class="t">related prompts. They only evaluate the final generation of a multi-turn conversation and human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=344" target="_blank">00:05:44.100</a></span> | <span class="t">evaluation is inherently subjective and noisy. I like to judge models based on mathematics and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=349" target="_blank">00:05:49.860</a></span> | <span class="t">reasoning, so I might be biased in one direction. Also Llama 2 is not nearly as good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=355" target="_blank">00:05:55.820</a></span> | <span class="t">when you're using it in languages other than English, which is not surprising given the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=360" target="_blank">00:06:00.460</a></span> | <span class="t">language distribution in the pre-training data. I also find it interesting that they did all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=365" target="_blank">00:06:05.320</a></span> | <span class="t">their safety testing in English and they warn developers before deploying any applications of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=371" target="_blank">00:06:11.040</a></span> | <span class="t">Llama 2, do your own safety testing and tuning tailored to your specific application. On compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=377" target="_blank">00:06:17.000</a></span> | <span class="t">they don't say much other than that it was trained on A100s. I am sure Llama 3 will be trained on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=385" target="_blank">00:06:25.780</a></span> | <span class="t">A100s, but apparently Meta has purchased more of those than any other company including Microsoft.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=391" target="_blank">00:06:31.220</a></span> | <span class="t">Mind you Llama 2 was trained between January and July apparently, so it's understandable they used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=397" target="_blank">00:06:37.400</a></span> | <span class="t">the earlier A100s. Back to the decision to release and it does seem interesting to me that Meta and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=403" target="_blank">00:06:43.760</a></span> | <span class="t">Zuckerberg have seemingly ignored this letter from the US Senate. It was written in early June and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=410" target="_blank">00:06:50.280</a></span> | <span class="t">toward the end it said this: "By purporting to release Llama for the purpose of researching the abuse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=415" target="_blank">00:06:55.740</a></span> | <span class="t">of AI, Meta effectively appears to have put a powerful tool in the hands of bad actors to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=421" target="_blank">00:07:01.980</a></span> | <span class="t">engage in such abuse without much discernible forethought, preparation or safeguards." In the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=428" target="_blank">00:07:08.420</a></span> | <span class="t">paper they defend it and say this release promotes transparency, it democratizes the technology and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=434" target="_blank">00:07:14.920</a></span> | <span class="t">creates a more level playing field for organizations of all sizes across the globe to benefit from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=440" target="_blank">00:07:20.460</a></span> | <span class="t">economic growth promised by the advancement of AI. But before anyone gets too enchanted by that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=445" target="_blank">00:07:25.700</a></span> | <span class="t">Zuckerberg has recently said that they're only releasing because it's far away from AGI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=451" target="_blank">00:07:31.340</a></span> | <span class="t">And I think Google's palm model is also I think has about 10 times as many parameters. Now the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=456" target="_blank">00:07:36.540</a></span> | <span class="t">Llama models are very efficient so they perform well for something that's around 65 billion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=460" target="_blank">00:07:40.940</a></span> | <span class="t">parameters. So for me that was also part of this because there's this whole debate around you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=466" target="_blank">00:07:46.220</a></span> | <span class="t">is it good for everyone in the world to have access to the most frontier AI models. And I think as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=475" target="_blank">00:07:55.660</a></span> | <span class="t">models start approaching something that's like a super human intelligence, that's a bigger question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=482" target="_blank">00:08:02.260</a></span> | <span class="t">that we'll have to grapple with. But right now I mean these are still very basic tools.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=487" target="_blank">00:08:07.780</a></span> | <span class="t">I suspect that the bigger reason for release relates to an earlier answer he gave in the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=493" target="_blank">00:08:13.540</a></span> | <span class="t">interview. Basically his researchers demanded it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=496" target="_blank">00:08:16.620</a></span> | <span class="t">Part of this is we want to have the best people in the world researching this and a lot of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=502" target="_blank">00:08:22.300</a></span> | <span class="t">best people want to know that they're going to be able to share their work. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=505" target="_blank">00:08:25.620</a></span> | <span class="t">part of the deal that we have is that you know we can get you know if you're one of the top AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=511" target="_blank">00:08:31.380</a></span> | <span class="t">researchers in the world and come here you can get access to kind of industry scale infrastructure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=516" target="_blank">00:08:36.060</a></span> | <span class="t">and part of our ethos is that we want to share what's invented broadly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=521" target="_blank">00:08:41.940</a></span> | <span class="t">And if Zuckerberg had refused to release some of those researchers could have just gone off and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=527" target="_blank">00:08:47.580</a></span> | <span class="t">made their own company as these guys did. Mistral AI is valued at 240 million despite being only four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=535" target="_blank">00:08:55.580</a></span> | <span class="t">weeks old and contains some key employees from Meta. One even complained before deleting the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=541" target="_blank">00:09:01.700</a></span> | <span class="t">tweet about not being included in the author list of the Lama 2 paper. This was the pitch memo that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=548" target="_blank">00:09:08.300</a></span> | <span class="t">Mistral used to raise those hundreds of millions of euros and they focus on taking a more open</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=554" target="_blank">00:09:14.540</a></span> | <span class="t">approach to model development. So the point still stands if a CEO blocks a model being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=559" target="_blank">00:09:19.580</a></span> | <span class="t">open source if the researchers want to they can just defect to XAI or just start their own company.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=565" target="_blank">00:09:25.540</a></span> | <span class="t">So in a way Zuckerberg had few options. I must say though that I did raise an eyebrow when I read</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=571" target="_blank">00:09:31.340</a></span> | <span class="t">these paragraphs. This is on page 35 of the technical paper and they say not everyone who</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=576" target="_blank">00:09:36.780</a></span> | <span class="t">uses AI models has good intentions. AI agents could potentially be used for nefarious purposes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=582" target="_blank">00:09:42.540</a></span> | <span class="t">such as misinformation or bioterrorism or cyber crime. However we have made efforts to tune the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=587" target="_blank">00:09:47.980</a></span> | <span class="t">models to avoid these topics and indeed cyber criminals have already come up with worm GPT to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=593" target="_blank">00:09:53.500</a></span> | <span class="t">help them do phishing campaigns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=595" target="_blank">00:09:55.500</a></span> | <span class="t">But Meta points them to their responsible use guide which I am sure they will follow. I read that 24</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=602" target="_blank">00:10:02.420</a></span> | <span class="t">page guide and to be honest it was kind of a waste of time. They said pretty much nothing. It was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=609" target="_blank">00:10:09.220</a></span> | <span class="t">really bland and generic. Maybe that's harsh let me know if I missed something but it was all pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=615" target="_blank">00:10:15.860</a></span> | <span class="t">vague. They did try some red teaming only in English for things like the production of weapons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=621" target="_blank">00:10:21.940</a></span> | <span class="t">and lots of other risk categories. But you will be really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=625" target="_blank">00:10:25.460</a></span> | <span class="t">assured first that any such illegal or unlawful activity is against their terms and conditions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=631" target="_blank">00:10:31.900</a></span> | <span class="t">and second that they are looking for the community to do further research and red teaming. Anyway I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=637" target="_blank">00:10:37.340</a></span> | <span class="t">am keen to do many more experiments but using this Gradio demo it basically failed to do a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=643" target="_blank">00:10:43.980</a></span> | <span class="t">proper sonnet and when I asked it this question from the math benchmark it said the question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=649" target="_blank">00:10:49.420</a></span> | <span class="t">does not make sense because the length of a rectangle being twice its width would mean the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=655" target="_blank">00:10:55.420</a></span> | <span class="t">length of a rectangle is a square. Hmm. Anyway it could just be a problem with that demo because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=661" target="_blank">00:11:01.220</a></span> | <span class="t">GPT 3.5 crushes the sonnet about apples and has no problem with the length of a rectangle being twice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=667" target="_blank">00:11:07.620</a></span> | <span class="t">its width. Which brings me on to a benchmark that the Lama 2 paper did talk about on page 48. It was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=675" target="_blank">00:11:15.380</a></span> | <span class="t">on social IQ and they noted that Lama 1 actually did better than Lama 2. Here is the benchmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=682" target="_blank">00:11:22.260</a></span> | <span class="t">It's about common sense reasoning with questions such as these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=685" target="_blank">00:11:25.380</a></span> | <span class="t">Alex spilled the food she just prepared all over the floor and it made a huge mess. What will Alex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=690" target="_blank">00:11:30.660</a></span> | <span class="t">want to do next? Taste the food, mop up, run around in a mess. And again apparently Lama 1 actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=697" target="_blank">00:11:37.180</a></span> | <span class="t">does slightly better on those kind of questions. Another benchmark that you can see Lama 1 being as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=703" target="_blank">00:11:43.100</a></span> | <span class="t">good as Lama 2 at is Ball Q. That's a benchmark testing yes or no questions but it's harder than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=708" target="_blank">00:11:48.980</a></span> | <span class="t">that. You have to read a lot of context to get the answer right. I just want you to remember some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=714" target="_blank">00:11:54.140</a></span> | <span class="t">these benchmarks when you hear the question. So I'm going to go ahead and read the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=715" target="_blank">00:11:55.340</a></span> | <span class="t">So I'm going to go ahead and read the answer. So I'm going to go ahead and read the answer. So I'm going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=715" target="_blank">00:11:55.380</a></span> | <span class="t">hear all the influencers talk about Lama 2 completely changing everything. Also if someone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=720" target="_blank">00:12:00.020</a></span> | <span class="t">says it's the best model of its size look at Lama 2 13 billion parameters. Of course it depends on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=726" target="_blank">00:12:06.340</a></span> | <span class="t">the benchmark but it got 21.7 percent in Aquarat. That's a test of mathematical reasoning and Orca</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=733" target="_blank">00:12:13.380</a></span> | <span class="t">at the exact same size of 13 billion parameters got almost 28 percent. So even pound for pound</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=739" target="_blank">00:12:19.820</a></span> | <span class="t">it may not be the best in all categories. To be honest I feel like there might be a loyally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=745" target="_blank">00:12:25.300</a></span> | <span class="t">struggle going on behind the scenes at Microsoft about whether to open source Orca and Phi 1. There</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=751" target="_blank">00:12:31.140</a></span> | <span class="t">were some bonus interesting things about the paper like introducing ghost attention which to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=756" target="_blank">00:12:36.580</a></span> | <span class="t">oversimplify means that the model pays attention over multiple turns of the conversation something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=762" target="_blank">00:12:42.500</a></span> | <span class="t">you might have originally told it such as always act as Napoleon from now. Essentially these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=767" target="_blank">00:12:47.460</a></span> | <span class="t">diagrams show that with ghost attention the model pays more attention to that original command act</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=773" target="_blank">00:12:53.060</a></span> | <span class="t">as Oscar Wilde or always act as Napoleon from now. So I'm going to go ahead and read the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=775" target="_blank">00:12:55.260</a></span> | <span class="t">The authors also throw in this observation that LLMs have internalized the concept of time and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=783" target="_blank">00:13:03.780</a></span> | <span class="t">despite their training being solely based on next token prediction and data that is randomly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=790" target="_blank">00:13:10.100</a></span> | <span class="t">shuffled without regard to their chronological context the models pick up a general sense of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=795" target="_blank">00:13:15.300</a></span> | <span class="t">what time is. Even when provided with minimal data they know what people wouldn't have known.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=801" target="_blank">00:13:21.220</a></span> | <span class="t">For example with a knowledge cutoff of 1940 when asked who wanted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=805" target="_blank">00:13:25.220</a></span> | <span class="t">to win the second world war they say I'm not sure what you're referring to my knowledge stopped in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=809" target="_blank">00:13:29.660</a></span> | <span class="t">1940. Right at the end of the report I know many people will be shocked to hear that when they did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=815" target="_blank">00:13:35.340</a></span> | <span class="t">a sentiment analysis of the model they found that the sentiment for Llama 2 for right wing was higher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=822" target="_blank">00:13:42.460</a></span> | <span class="t">than for left wing. You may even want to pause and look at this page from a sociological perspective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=828" target="_blank">00:13:48.940</a></span> | <span class="t">because if Llama 2 was trained on a semi-random swathe of the internet this could be like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=835" target="_blank">00:13:55.180</a></span> | <span class="t">snapshot of the sentiment analysis of all of these terms across the internet. Anyway in what may have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=841" target="_blank">00:14:01.220</a></span> | <span class="t">been a surprising twist for some Microsoft and Meta teamed up to make Llama 2 widely available</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=848" target="_blank">00:14:08.420</a></span> | <span class="t">and we get news that Llama 2 may soon be on your phone and PC. Although I think Meta want to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=854" target="_blank">00:14:14.720</a></span> | <span class="t">paid if it's going to come to your iPhone with this curious clause requiring permission if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=860" target="_blank">00:14:20.340</a></span> | <span class="t">have more than 700 million monthly active users. I don't know whether they were thinking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=865" target="_blank">00:14:25.140</a></span> | <span class="t">Apple or Telegram or TikTok but I think they want to get paid if any of those are going to use Llama</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=871" target="_blank">00:14:31.620</a></span> | <span class="t">2. But I must confess to finding the previous clause somewhat ironic. You will not use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=877" target="_blank">00:14:37.540</a></span> | <span class="t">Llama materials or any output or results of the Llama materials to improve any other large language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=883" target="_blank">00:14:43.800</a></span> | <span class="t">model. So they can use any part of the internet which one leak said might include copyrighted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=888" target="_blank">00:14:48.960</a></span> | <span class="t">works but you can't use Llama to improve your own model. Well just two hours ago people are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=895" target="_blank">00:14:55.100</a></span> | <span class="t">already updating models like Lava based on Llama 2. So it will likely just be a few days or weeks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=902" target="_blank">00:15:02.480</a></span> | <span class="t">until we see a newly improved Vicuña or Orca. Jim Fan predicts that Llama 2 will dramatically boost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=909" target="_blank">00:15:09.600</a></span> | <span class="t">multimodal AI and robotics research. He says these fields need more than just black box access to an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=915" target="_blank">00:15:15.860</a></span> | <span class="t">API. So far we have had to convert the complex sensory signals video audio 3D perception to text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=922" target="_blank">00:15:22.920</a></span> | <span class="t">description and then feed to an LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=925" target="_blank">00:15:25.060</a></span> | <span class="t">It would be much more effective to graft those sensory modules directly onto a strong LLM backbone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=932" target="_blank">00:15:32.020</a></span> | <span class="t">Anyway this video is already long enough and this is just the first 24 hours of Llama 2's release.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=938" target="_blank">00:15:38.380</a></span> | <span class="t">I am sure there will be much more discussion in the coming days and weeks. Let me know what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zJBpRn2zTco&t=943" target="_blank">00:15:43.540</a></span> | <span class="t">you think in the comments and thank you so much for watching. Have a wonderful day.</span></div></div></body></html>