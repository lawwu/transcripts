<html><head><title>Building Multimodal AI Agents From Scratch — Apoorva Joshi, MongoDB</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Building Multimodal AI Agents From Scratch — Apoorva Joshi, MongoDB</h2><a href="https://www.youtube.com/watch?v=640KMYtxCeI"><img src="https://i.ytimg.com/vi_webp/640KMYtxCeI/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./640KMYtxCeI.html">Whisper Transcript</a> | <a href="./transcript_640KMYtxCeI.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi, everyone. Thanks for taking the time to be here today. Welcome to this workshop where you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=21" target="_blank">00:00:21.880</a></span> | <span class="t">will learn about agents, multimodality, and hopefully get to build -- not hopefully, you will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=27" target="_blank">00:00:27.520</a></span> | <span class="t">build a multimodal agent of your own from scratch. Whose first workshop of the day? How</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=34" target="_blank">00:00:34.100</a></span> | <span class="t">many? Show of hands. Okay. First workshop. How many people already been in a workshop? Oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=40" target="_blank">00:00:40.760</a></span> | <span class="t">wow. Like, really proactive, overachieving crowd. Great to see that. All right. With</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=47" target="_blank">00:00:47.460</a></span> | <span class="t">that, here's a little bit about me. I'm Apoorva. I'll be your lead instructor for today. I'm also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=53" target="_blank">00:00:53.880</a></span> | <span class="t">joined by my awesome team here. We have Richmond. He's waving at you. There's Rafa back there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=62" target="_blank">00:01:02.400</a></span> | <span class="t">He's waving at you as well. There's Tebow. And we have Mikiko. But yeah, here's a bit about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=69" target="_blank">00:01:09.040</a></span> | <span class="t">me. I'm Apoorva. I'm currently an AI-focused developer advocate at MongoDB, which means I spend a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=75" target="_blank">00:01:15.460</a></span> | <span class="t">of my time building workshops like this one for AI developers like you to help them build AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=82" target="_blank">00:01:22.040</a></span> | <span class="t">applications of their own. Prior to this role, I spent about six years working as a data scientist</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=89" target="_blank">00:01:29.080</a></span> | <span class="t">in the cybersecurity space. And outside of work, I like to read a lot. I try to yoga pretty regularly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=94" target="_blank">00:01:34.980</a></span> | <span class="t">or used to try to until I busted my knee a while ago. And I'm always on a mission to visit as many local</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=102" target="_blank">00:01:42.860</a></span> | <span class="t">coffee shops as I can in whichever city I'm in. So this is what the next hour and 20 minutes is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=109" target="_blank">00:01:49.560</a></span> | <span class="t">to look like. We'll be going over key concepts of AI agents, discuss what multimodality is, and finally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=117" target="_blank">00:01:57.380</a></span> | <span class="t">we are going to put the two together and build a multimodal AI agent from scratch using good old Python.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=124" target="_blank">00:02:04.100</a></span> | <span class="t">And since this is a relatively short workshop session, this is what you can expect. We are going to focus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=130" target="_blank">00:02:10.820</a></span> | <span class="t">on getting concepts down, and we'll see what we have time for. Based on my practice sessions, I believe you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=137" target="_blank">00:02:17.780</a></span> | <span class="t">get about 55 minutes to an hour to actually take your time to write code or just, like, get a really good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=146" target="_blank">00:02:26.980</a></span> | <span class="t">understanding of how multimodal agents work in code. So just wanted to set that expectation there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=153" target="_blank">00:02:33.700</a></span> | <span class="t">And with that, let's get started. So let's first answer two questions. What are AI agents and why do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=161" target="_blank">00:02:41.780</a></span> | <span class="t">we need them? But even before I get into that, how many of you already have some experience with AI agents?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=168" target="_blank">00:02:48.900</a></span> | <span class="t">Okay, there's some of you. Hopefully, there'll be something new for you at this workshop. And for those of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=175" target="_blank">00:02:55.780</a></span> | <span class="t">you who are new to the concept, there's a lot of stuff coming your way. So apologies for that. Or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=182" target="_blank">00:03:02.900</a></span> | <span class="t">Okay, so here's my definition of an AI agent. I like to define an AI agent as a system that uses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=191" target="_blank">00:03:11.940</a></span> | <span class="t">a large language model. I'll be referring to this as an LLM. To reason through a problem, create a plan to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=198" target="_blank">00:03:18.420</a></span> | <span class="t">solve the problem, and execute and iterate on the plan with the help of a set of tools.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=205" target="_blank">00:03:25.380</a></span> | <span class="t">So in the past two years or so, we have seen three main paradigms for interacting with LLMs. There's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=211" target="_blank">00:03:31.460</a></span> | <span class="t">simple prompting, rag, and agents. So let's briefly talk about each of these because this will help us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=217" target="_blank">00:03:37.860</a></span> | <span class="t">build an intuition for when you want to use an AI agent versus something else. So with simple prompting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=225" target="_blank">00:03:45.060</a></span> | <span class="t">you're simply asking the LLM questions and expecting the LLM to rely on its pre-trained, or as we call it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=231" target="_blank">00:03:51.780</a></span> | <span class="t">parametric knowledge to answer these questions. So this means the LLM cannot answer questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=238" target="_blank">00:03:58.100</a></span> | <span class="t">if the information required to answer them is not present in its pre-trained knowledge. It cannot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=244" target="_blank">00:04:04.100</a></span> | <span class="t">really handle complex queries, and it cannot provide personalized responses or even refine its responses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=252" target="_blank">00:04:12.260</a></span> | <span class="t">With rag, how many of you have built a rag application? Okay, some of you. So you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=258" target="_blank">00:04:18.820</a></span> | <span class="t">with rag, you take this up a notch, you augment the LLM's knowledge with information from external data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=264" target="_blank">00:04:24.420</a></span> | <span class="t">sources. And as you can imagine, this solves some problems where you can now be reasonably certain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=271" target="_blank">00:04:31.380</a></span> | <span class="t">that the LLM has the information required to answer user questions and also incorporate some basic light</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=279" target="_blank">00:04:39.220</a></span> | <span class="t">personalization if it's given access to the right sources of information. But this still doesn't equip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=284" target="_blank">00:04:44.980</a></span> | <span class="t">the LLM with the ability to handle complex multi-step tasks or to self-refinance responses. But that's okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=292" target="_blank">00:04:52.980</a></span> | <span class="t">because not all tasks might require this capability. And finally, 2025 is the year of Agents. So if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=302" target="_blank">00:05:02.500</a></span> | <span class="t">need, if you have complex multi-step tasks or need a deep personalization, any sort of adaptive learning in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=310" target="_blank">00:05:10.740</a></span> | <span class="t">your applications, then you'd want to use AI Agents. So with AI Agents, what we've done is given LLM the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=317" target="_blank">00:05:17.380</a></span> | <span class="t">agency to determine the sequence of steps required to complete a particular task. And they do this by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=323" target="_blank">00:05:23.700</a></span> | <span class="t">taking actions with the help of tools that you provide them and reasoning through the results of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=330" target="_blank">00:05:30.260</a></span> | <span class="t">these tool executions and also its past interactions to inform what to do next. And this is what makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=338" target="_blank">00:05:38.500</a></span> | <span class="t">agents extremely flexible and capable of handling a wide variety of complex tasks. But the, yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=345" target="_blank">00:05:45.060</a></span> | <span class="t">Sorry, can we share these presentations?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=347" target="_blank">00:05:47.140</a></span> | <span class="t">Yes, it will be shared. The recordings will be up on YouTube as well. So, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=351" target="_blank">00:05:51.700</a></span> | <span class="t">The one thing to note here, though, is that Agents come with a higher cost and latency. After all,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=358" target="_blank">00:05:58.420</a></span> | <span class="t">you're expecting LLMs to do all the heavy lifting of thinking through the problem, coming up with a plan of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=364" target="_blank">00:06:04.500</a></span> | <span class="t">action, executing the actions, rectifying its responses. So my word of caution here is only use Agents if you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=373" target="_blank">00:06:13.380</a></span> | <span class="t">to. Don't complicate whatever it is you're trying to build. But we are building an AI Agent today. So for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=378" target="_blank">00:06:18.820</a></span> | <span class="t">today, let's just throw an agent at the problem, a simple problem. Okay. So to summarize, use Agents for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=387" target="_blank">00:06:27.220</a></span> | <span class="t">complex tasks that don't have a structured workflow or where the series of steps required to solve the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=393" target="_blank">00:06:33.540</a></span> | <span class="t">problem is hard to predict. Or tasks that have a high latency tolerance, as I was just mentioning. Or for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=400" target="_blank">00:06:40.660</a></span> | <span class="t">tasks where it's acceptable for your application or system to return non-deterministic outputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=406" target="_blank">00:06:46.820</a></span> | <span class="t">which means the same result is not guaranteed for the same inputs. And this is true for any application</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=414" target="_blank">00:06:54.020</a></span> | <span class="t">that uses LLMs. But this effect is especially amplified in agentic workflows. And finally, tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=421" target="_blank">00:07:01.460</a></span> | <span class="t">that might benefit from any sort of personalization or adaptive behavior over a long period of time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=427" target="_blank">00:07:07.700</a></span> | <span class="t">all of these are fair game for AI agents. Now let's talk about the different components of AI agents,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=436" target="_blank">00:07:16.020</a></span> | <span class="t">just to get a better understanding of how these systems work. So an agent typically has four main</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=444" target="_blank">00:07:24.260</a></span> | <span class="t">components. There's perception, which is how agents gather information about their environment. Planning and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=451" target="_blank">00:07:31.860</a></span> | <span class="t">reasoning, which helps the agent reason through a problem and come up with a plan to solve it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=457" target="_blank">00:07:37.060</a></span> | <span class="t">Then there's tools, which are external interfaces that help the agent act upon and solve a problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=463" target="_blank">00:07:43.700</a></span> | <span class="t">And memory, which helps agents learn from past interactions. And if you are passionate about memory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=471" target="_blank">00:07:51.860</a></span> | <span class="t">we have Richmond, who knows a lot about this topic. So definitely catch him after or during the presentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=480" target="_blank">00:08:00.020</a></span> | <span class="t">So all of this sounds a bit like a human, doesn't it? But that's the whole goal of agents. The goal with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=485" target="_blank">00:08:05.700</a></span> | <span class="t">LLM-based agents is to give these systems the autonomy to carry out complex tasks, much like we humans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=492" target="_blank">00:08:12.420</a></span> | <span class="t">do. So it's not a surprise that the components kind of resemble how we think through problems and go about the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=499" target="_blank">00:08:19.620</a></span> | <span class="t">world. So let's dive a bit deeper into each of these components. Let's talk about perception first. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=507" target="_blank">00:08:27.620</a></span> | <span class="t">perception, as I said, is the mechanism by which agents gather information about their environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=512" target="_blank">00:08:32.660</a></span> | <span class="t">And this happens via some form of inputs, whether it's a user like you interacting with the agent or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=518" target="_blank">00:08:38.420</a></span> | <span class="t">triggered by something else, like an email or a Slack message. And text inputs have been the most common</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=525" target="_blank">00:08:45.300</a></span> | <span class="t">form of interacting with LLMs and agents so far. But over the past few months, we've seen images, voice,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=532" target="_blank">00:08:52.980</a></span> | <span class="t">video also being part of this perception mechanism for agents. And in today's workshop, we'll be working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=538" target="_blank">00:08:58.820</a></span> | <span class="t">with two of these, which is text and images. The next component we have is planning and reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=547" target="_blank">00:09:07.460</a></span> | <span class="t">And shocker, the component that helps agents plan and reason is LLMs. So given a user query, it's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=555" target="_blank">00:09:15.220</a></span> | <span class="t">LLM's job to determine how to go about solving the problem. But they can't do all of this on their own.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=561" target="_blank">00:09:21.300</a></span> | <span class="t">They need some guidance. And the way to provide guidance at this point is to prompt the LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=568" target="_blank">00:09:28.980</a></span> | <span class="t">And you can start simple by prompting the LLM to create a plan of action based on its initial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=574" target="_blank">00:09:34.420</a></span> | <span class="t">understanding of the problem. And this is what we call planning without feedback, since the LLM doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=579" target="_blank">00:09:39.860</a></span> | <span class="t">really modify its initial plan of action based on information gathered from tool outcomes or its own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=586" target="_blank">00:09:46.660</a></span> | <span class="t">reasoning traces. And a common design pattern for this kind of planning is chain of thought.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=593" target="_blank">00:09:53.860</a></span> | <span class="t">And chain of thought is as simple as prompting the LLM to think through a problem step by step without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=599" target="_blank">00:09:59.220</a></span> | <span class="t">directly jumping to giving the user an answer. And you can do this in two ways. In a zero-shot manner,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=606" target="_blank">00:10:06.980</a></span> | <span class="t">where you literally prompt the LLM, like tell it, like, let's think step by step. Or you can do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=612" target="_blank">00:10:12.340</a></span> | <span class="t">in a few-shot manner, where you're providing examples of how the LLM should go about thinking through a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=617" target="_blank">00:10:17.380</a></span> | <span class="t">problem so that the next time you give it another problem, it'll use your examples to guide its reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=623" target="_blank">00:10:23.700</a></span> | <span class="t">process. Then there's planning with feedback, where you can prompt the LLM to adjust and refine its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=632" target="_blank">00:10:32.180</a></span> | <span class="t">initial plan based on new information. Again, obtained from tool outcomes or based on its own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=638" target="_blank">00:10:38.340</a></span> | <span class="t">previous reasoning. And a common design pattern that you will implement today is React, which is short for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=646" target="_blank">00:10:46.500</a></span> | <span class="t">reasoning and act. And what we do in this pattern is you prompt the LLM to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=651" target="_blank">00:10:51.460</a></span> | <span class="t">verbal reasoning traces and also tell you the actions that it will take to solve the task. And then after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=660" target="_blank">00:11:00.020</a></span> | <span class="t">each action, we ask the LLM to make an observation based on the information that it gathered from that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=666" target="_blank">00:11:06.100</a></span> | <span class="t">tool execution and think through that and plan what to do next. And this continues until the LLM determines that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=672" target="_blank">00:11:12.820</a></span> | <span class="t">I have the final answer and that's when it will exit that execution loop and provide the answer to the user.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=678" target="_blank">00:11:18.820</a></span> | <span class="t">Then the next thing we have is tools. And tools are essentially interfaces for agents to interact with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=688" target="_blank">00:11:28.980</a></span> | <span class="t">their external world in order to achieve their objectives. And these tools can range from simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=695" target="_blank">00:11:35.460</a></span> | <span class="t">APIs, such as I'm sure you've seen examples of weather and search APIs, to vector stores, to even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=702" target="_blank">00:11:42.340</a></span> | <span class="t">specialized machine learning models. And tools for LLMs are typically defined as functions. And most recent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=710" target="_blank">00:11:50.100</a></span> | <span class="t">LLMs have been trained to identify when a function should be called and also the arguments for a function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=716" target="_blank">00:11:56.500</a></span> | <span class="t">call. But the one thing to note is the LLM doesn't actually execute the function. This is something we will have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=724" target="_blank">00:12:04.180</a></span> | <span class="t">implement in our code. And in addition to actually defining the function, you typically also need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=731" target="_blank">00:12:11.060</a></span> | <span class="t">provide the LLM a function or tool schema. And this basically is just a JSON file or with MCP servers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=740" target="_blank">00:12:20.900</a></span> | <span class="t">You might have seen a different way of defining tools, but essentially you're providing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=747" target="_blank">00:12:27.220</a></span> | <span class="t">the name of the tool to call a description of what the tool does and the parameters that the tool takes and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=753" target="_blank">00:12:33.140</a></span> | <span class="t">also their types and descriptions. So for example, I have a weather tool here and in my tool schema, I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=759" target="_blank">00:12:39.220</a></span> | <span class="t">saying the name of the tool, the description and saying the input that it takes is a city, the type is string and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=766" target="_blank">00:12:46.100</a></span> | <span class="t">the description of that parameter. And finally, the last component is memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=773" target="_blank">00:12:53.620</a></span> | <span class="t">This component is what allows AI agents to store and recall past conversations and enables them to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=781" target="_blank">00:13:01.620</a></span> | <span class="t">from these interactions. And memory, if you think of human memory, it's a pretty nebulous concept. There's so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=786" target="_blank">00:13:06.820</a></span> | <span class="t">many different types of memory. If you ask a psychologist, they'll tell you all about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=791" target="_blank">00:13:11.300</a></span> | <span class="t">But I'm not a psychologist, so I think of it in pretty primitive terms. I think of it in as two broad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=800" target="_blank">00:13:20.660</a></span> | <span class="t">categories. One is short term, which deals with storing and retrieving information from a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=807" target="_blank">00:13:27.060</a></span> | <span class="t">conversation. And then there's long term, which deals with storing, updating, and retrieving information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=812" target="_blank">00:13:32.260</a></span> | <span class="t">obtained over multiple conversations had with the agent over a longer period of time. And this is really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=819" target="_blank">00:13:39.140</a></span> | <span class="t">what enables agents to personalize their responses over a long period of time. But in today's lab,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=825" target="_blank">00:13:45.620</a></span> | <span class="t">we'll implement short term memory for our multimodal agent. And again, if you want to learn more about this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=834" target="_blank">00:13:54.500</a></span> | <span class="t">nebulous and extensive topic, then here's a talk that I gave a few months ago. So I'll leave this here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=841" target="_blank">00:14:01.700</a></span> | <span class="t">for a few seconds. But if you want to talk to someone live, then talk to me, Richmond, Mikiko, anyone from our team.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=848" target="_blank">00:14:08.660</a></span> | <span class="t">Moving on. Okay, so let's take an example and understand how all of these components work together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=858" target="_blank">00:14:18.580</a></span> | <span class="t">right? So the first thing that happens is a query comes in to the agent here. I'm asking this agent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=865" target="_blank">00:14:25.460</a></span> | <span class="t">what's the weather in San Francisco today? The agent forwards the query to an LLM. So think of agents as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=873" target="_blank">00:14:33.380</a></span> | <span class="t">a software application or system with different components, one of them being one or more LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=879" target="_blank">00:14:39.380</a></span> | <span class="t">And the LLM in this case has access to a set of tools. In this case, it has access to a weather and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=886" target="_blank">00:14:46.340</a></span> | <span class="t">search API and also its past interactions or memories. So based on the tools it has access to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=892" target="_blank">00:14:52.500</a></span> | <span class="t">in this case for this query, the LLM might decide that the weather API would be the most suitable to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=900" target="_blank">00:15:00.020</a></span> | <span class="t">information about this query. And it will also parse out the arguments for this tool from the user query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=907" target="_blank">00:15:07.060</a></span> | <span class="t">And like I mentioned, your agent also needs to have code to actually execute the tools. So we have that in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=913" target="_blank">00:15:13.380</a></span> | <span class="t">our agent. It's going to make a call to the weather API with the arguments extracted by the LLM,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=919" target="_blank">00:15:19.380</a></span> | <span class="t">get a response back from the API, forward that to the LLM. And at this point, the LLM has two options. It can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=927" target="_blank">00:15:27.860</a></span> | <span class="t">either decide that it needs more information and decide to call more tools, or it can be like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=933" target="_blank">00:15:33.540</a></span> | <span class="t">I have the final answer. I'm going to generate that now. So in this case, it has the temperature in San</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=938" target="_blank">00:15:38.900</a></span> | <span class="t">Francisco. So it might be like, I know the answer. It generates a natural language response. And that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=944" target="_blank">00:15:44.980</a></span> | <span class="t">gets forwarded to the user. So that's kind of the full flow of how a tool calling, simple tool calling agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=953" target="_blank">00:15:53.620</a></span> | <span class="t">works. The other thing we need to talk about today, which is the more interesting part, I believe, is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=962" target="_blank">00:16:02.580</a></span> | <span class="t">multimodality, because we are, after all, building a multimodal agent. So what is multimodality?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=969" target="_blank">00:16:09.540</a></span> | <span class="t">Multimodality in the context of machine learning or AI is the ability of machine learning models to process,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=976" target="_blank">00:16:16.980</a></span> | <span class="t">understand, and at this point, even generate different types of data, such as text, images, audio,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=984" target="_blank">00:16:24.500</a></span> | <span class="t">video, etc. And like I mentioned, in today's lab, we'll be working with two of these modalities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=989" target="_blank">00:16:29.700</a></span> | <span class="t">which is text and images. So here's some real world examples of data that contains a combination of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=996" target="_blank">00:16:36.980</a></span> | <span class="t">images and text, just to give you some inspiration for the kind of problems and domains you can apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1003" target="_blank">00:16:43.300</a></span> | <span class="t">your learnings from today, too. So there's graphs, tables, and then there's these types of data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1011" target="_blank">00:16:51.460</a></span> | <span class="t">interleave with text. So think of research papers, financial reports, any sort of organizational reporting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1017" target="_blank">00:16:57.460</a></span> | <span class="t">which typically has like some graphs, analysis, and text all combined together, or healthcare documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1025" target="_blank">00:17:05.460</a></span> | <span class="t">The list is virtually endless. There's a lot of real world data looks something like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1031" target="_blank">00:17:11.700</a></span> | <span class="t">So to make sense of this type of data, we currently have two classes of multimodal machine learning models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1039" target="_blank">00:17:19.220</a></span> | <span class="t">And the first type of models we see are multimodal embedding models. And the job of these models is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1045" target="_blank">00:17:25.140</a></span> | <span class="t">essentially to take multiple types of data as input and generate embeddings for them so that all of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1051" target="_blank">00:17:31.780</a></span> | <span class="t">diverse data types can be searched and retrieved together using techniques like vector search, hybrid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1057" target="_blank">00:17:37.460</a></span> | <span class="t">search, graph-based retrieval, whatever retrieval mechanism you want. And the other class of models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1064" target="_blank">00:17:44.180</a></span> | <span class="t">is multimodal LLMs, which can be DeepSeq does that at this point, Claude, OpenAI, like ChatGPT has a voice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1074" target="_blank">00:17:54.100</a></span> | <span class="t">mode. So the job of these LLMs is to take all of these different data types as input and also generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1080" target="_blank">00:18:00.420</a></span> | <span class="t">outputs in these different data formats. Now, if you give a multimodal LLM tools to search through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1087" target="_blank">00:18:07.780</a></span> | <span class="t">multimodal data and use its reasoning capabilities to make sense of this information and to solve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1093" target="_blank">00:18:13.700</a></span> | <span class="t">complex problems, what you have at your hands is a multimodal agent. So let's build that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1101" target="_blank">00:18:21.060</a></span> | <span class="t">Enough talk. Let's actually talk about the agent that we are going to build today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1107" target="_blank">00:18:27.220</a></span> | <span class="t">So we are going to start with something simple. We're going to remove as many abstractions as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1114" target="_blank">00:18:34.420</a></span> | <span class="t">possible, start with very simple objectives, and build an agent from scratch. You get a really good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1120" target="_blank">00:18:40.500</a></span> | <span class="t">understanding of what it really takes to build a multimodal agent in practice. So our agent has two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1127" target="_blank">00:18:47.700</a></span> | <span class="t">simple objectives, the first one being answer questions about a large corpus of documents, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1133" target="_blank">00:18:53.780</a></span> | <span class="t">also given a chart or diagram help the user make sense of it by explaining and analyzing that figure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1140" target="_blank">00:19:00.340</a></span> | <span class="t">I do this all the time. When I'm reading research papers, I'll just take a screenshot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1144" target="_blank">00:19:04.180</a></span> | <span class="t">pass it to Claude, and be like, explain that equation, especially with all those like mathematical symbols</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1149" target="_blank">00:19:09.380</a></span> | <span class="t">and whatnot. Sounds pretty reasonable? Easy? Not quite. There's a small catch. And the catch is we want to search</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1159" target="_blank">00:19:19.220</a></span> | <span class="t">over documents with mixed modalities. So in our case, our corpus is going to be documents that have text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1167" target="_blank">00:19:27.460</a></span> | <span class="t">interleaved with things like images and tables. And that complicates things because retrieving the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1174" target="_blank">00:19:34.420</a></span> | <span class="t">information from mixed modality documents is not a trivial problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1179" target="_blank">00:19:39.780</a></span> | <span class="t">The challenge lies in actually preparing the corpus of documents for search and retrieval. So typically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1184" target="_blank">00:19:44.820</a></span> | <span class="t">for text-based documents, if you've built a RAG application, you chunk up those documents, embed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1189" target="_blank">00:19:49.780</a></span> | <span class="t">those chunks, and then retrieve relevant chunks to pass as context to an LLM. But you can't really do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1196" target="_blank">00:19:56.180</a></span> | <span class="t">when you have images and tables in your documents. And one way to do this, there's so many tools out in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1201" target="_blank">00:20:01.220</a></span> | <span class="t">market. There's like LLMAPARs, unstructured, that use vision transformers or object recognition models to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1208" target="_blank">00:20:08.100</a></span> | <span class="t">first identify and extract the different elements. Like they'll extract text, images, tables separately,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1214" target="_blank">00:20:14.580</a></span> | <span class="t">then you chunk the text as usual, but you summarize the images and tables instead, and then basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1221" target="_blank">00:20:21.380</a></span> | <span class="t">convert everything to the text domain by creating embeddings of the text chunks and summaries using a text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1227" target="_blank">00:20:27.380</a></span> | <span class="t">embedding model. I know that's already a mouthful, but you'll see how to simplify this process using a new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1234" target="_blank">00:20:34.500</a></span> | <span class="t">type of models. I'll just get to that in just a little bit. Another technique similar to the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1240" target="_blank">00:20:40.660</a></span> | <span class="t">one is that you still extract the text and non-text elements. You chunk the text, but instead of summarizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1246" target="_blank">00:20:46.420</a></span> | <span class="t">the images and tables, you would embed all of these, like the text chunks, images, and tables using a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1252" target="_blank">00:20:52.420</a></span> | <span class="t">multimodal embedding model because it has the capacity to understand and embed all of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1259" target="_blank">00:20:59.780</a></span> | <span class="t">different data types. I already see some of you losing me because these data processing pipelines are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1267" target="_blank">00:21:07.540</a></span> | <span class="t">pretty complex and they come with their own limitations, right? They sound promising, but they have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1273" target="_blank">00:21:13.540</a></span> | <span class="t">mainly these two limitations. So the first one is that they face the same drawbacks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1279" target="_blank">00:21:19.380</a></span> | <span class="t">that you see with chunking. So to me, the biggest problem with chunking is the loss of context at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1285" target="_blank">00:21:25.460</a></span> | <span class="t">chunk boundaries, which is why techniques like parent document retrieval, metadata pre-filtering,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1290" target="_blank">00:21:30.420</a></span> | <span class="t">are becoming popular where you add back context that was lost during chunking at either retrieval or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1295" target="_blank">00:21:35.620</a></span> | <span class="t">generation time. Also notice how complex these processing pipelines were, right? You need an object recognition model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1304" target="_blank">00:21:44.900</a></span> | <span class="t">to extract the elements, potentially another LM call to actually summarize these elements in addition to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1312" target="_blank">00:21:52.100</a></span> | <span class="t">chunking and embedding, which is already one too many steps. Another limitation with a lot of multimodal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1320" target="_blank">00:22:00.180</a></span> | <span class="t">embedding models lies in the architecture of the models themselves. So until recently, the architecture of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1326" target="_blank">00:22:06.580</a></span> | <span class="t">most multimodal embedding models, at least for text and images, was based on OpenAI's clip model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1332" target="_blank">00:22:12.020</a></span> | <span class="t">And what happens in this architecture is text and images are passed through separate networks for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1339" target="_blank">00:22:19.380</a></span> | <span class="t">generating the embeddings of these data types.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1341" target="_blank">00:22:21.540</a></span> | <span class="t">And this results in something we call a modality gap, where irrelevant items of the same modality end up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1350" target="_blank">00:22:30.580</a></span> | <span class="t">close to each other rather than relevant items of different modalities. So in a clip model, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1358" target="_blank">00:22:38.500</a></span> | <span class="t">text vectors of irrelevant text might appear close together in vector space rather than text and images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1364" target="_blank">00:22:44.980</a></span> | <span class="t">corresponding to related subjects. And that's a problem. But this has changed with the advent of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1371" target="_blank">00:22:51.940</a></span> | <span class="t">vision language model, or VLM-based architectures. So in this architecture, both modalities are vectorized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1381" target="_blank">00:23:01.380</a></span> | <span class="t">using the same encoder. And this ensures that both text and visual features are treated as part of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1387" target="_blank">00:23:07.860</a></span> | <span class="t">a more unified representation than as distinct components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1391" target="_blank">00:23:11.780</a></span> | <span class="t">So with these models, all you really need is a screenshot of documents containing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1397" target="_blank">00:23:17.700</a></span> | <span class="t">whether it's purely images, purely text, or a combination of text, images, tables, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1404" target="_blank">00:23:24.820</a></span> | <span class="t">And this is what, because of that unified architecture, it ensures that the contextual relationships between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1410" target="_blank">00:23:30.900</a></span> | <span class="t">text and visual data is preserved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1413" target="_blank">00:23:33.540</a></span> | <span class="t">So as you can imagine, this greatly simplifies the data processing pipeline for multimodal data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1419" target="_blank">00:23:39.300</a></span> | <span class="t">and also ensures that you get better retrieval quality because you're no longer separating these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1425" target="_blank">00:23:45.620</a></span> | <span class="t">texts and images. So basically, given a document containing a combination of text and images,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1431" target="_blank">00:23:51.300</a></span> | <span class="t">you simply take a screenshot of it, pass it through a multimodal embedding model, and the embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1437" target="_blank">00:23:57.140</a></span> | <span class="t">that you get from that makes this data ready for retrieval. Pretty straightforward process there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1445" target="_blank">00:24:05.140</a></span> | <span class="t">So let's quickly look at how some of the key features of the agent that we are going to build is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1450" target="_blank">00:24:10.580</a></span> | <span class="t">work. And then we can go implement these in code. So let's talk about the data preparation pipeline for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1457" target="_blank">00:24:17.540</a></span> | <span class="t">the corpus of documents that our agent is going to use to answer questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1462" target="_blank">00:24:22.020</a></span> | <span class="t">So like I mentioned, the first thing we are going to do is for each document in our corpus,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1468" target="_blank">00:24:28.740</a></span> | <span class="t">we are going to convert that into a set of screenshots. And in our case, each screenshot is going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1474" target="_blank">00:24:34.500</a></span> | <span class="t">represent a page in the document.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1476" target="_blank">00:24:36.500</a></span> | <span class="t">We'll then store the screenshots locally, but if you were to do this in production,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1481" target="_blank">00:24:41.940</a></span> | <span class="t">then you might want to store them to some form of blob storage like S3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1485" target="_blank">00:24:45.460</a></span> | <span class="t">Google Cloud Storage, whatever your preferred cloud provider is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1489" target="_blank">00:24:49.940</a></span> | <span class="t">And we'll also note the path to where the image is stored.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1495" target="_blank">00:24:55.300</a></span> | <span class="t">And then store this as metadata along with the embeddings of the screenshots generated using a multimodal embedding model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1503" target="_blank">00:25:03.860</a></span> | <span class="t">and store these into a vector database.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1506" target="_blank">00:25:06.500</a></span> | <span class="t">So in our lab, we'll use the latest multimodal embedding model from YGI, and we'll use MongoDB as a vector database.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1515" target="_blank">00:25:15.700</a></span> | <span class="t">Because I work at MongoDB.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1518" target="_blank">00:25:18.020</a></span> | <span class="t">So one important thing to note here that we are not storing the raw screenshots. Yes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1525" target="_blank">00:25:25.300</a></span> | <span class="t">So you are retrieving the documents into screenshots, several screenshots or you're picking up the screenshots from the document and sending it to the --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1543" target="_blank">00:25:43.140</a></span> | <span class="t">So, okay. So say I have a PDF containing multiple pages. Each page in the PDF, I'm going to take a screenshot of it. And each screenshot is going to be saved separately in blob storage. And references will be stored as metadata along with embeddings in the vector database.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1561" target="_blank">00:26:01.540</a></span> | <span class="t">And why do you have to take the screenshot of each page? Is it to save the space?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1568" target="_blank">00:26:08.180</a></span> | <span class="t">So, like, I showed you two methods before, right? Like, to be able to use the image and table data for reasoning, you need to be able to retrieve that document that contains all of that together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1580" target="_blank">00:26:20.820</a></span> | <span class="t">And the reason I'm taking a screenshot is to preserve, like, the continuity between the text elements and the image elements. So all of them can be retrieved together as context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1590" target="_blank">00:26:30.180</a></span> | <span class="t">But this is worse than chunks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1592" target="_blank">00:26:32.580</a></span> | <span class="t">What's that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1593" target="_blank">00:26:33.460</a></span> | <span class="t">I mean, a screenshot for each page, it was the context of the whole, I mean, that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1599" target="_blank">00:26:39.620</a></span> | <span class="t">But then with chunking, it's worse because you have, like, a way -- usually you're keeping, like, two paragraphs together or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1606" target="_blank">00:26:46.980</a></span> | <span class="t">So, like, slightly better, you would always, of course, maybe want to augment this method with metadata pre-filtering, other methods that you use for traditional chunking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1614" target="_blank">00:26:54.900</a></span> | <span class="t">But I still think one page is better than two paragraphs or a small paragraph of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1620" target="_blank">00:27:00.340</a></span> | <span class="t">Do you want overlapping segments here as well, just like you would make something?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1624" target="_blank">00:27:04.500</a></span> | <span class="t">Yes. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1625" target="_blank">00:27:05.620</a></span> | <span class="t">So when you -- so here we'll take screenshots of, like, distinct pages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1629" target="_blank">00:27:09.140</a></span> | <span class="t">But if you want that continuity, you might want to, like, keep some overlap as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1633" target="_blank">00:27:13.940</a></span> | <span class="t">Yeah. Good point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1635" target="_blank">00:27:15.380</a></span> | <span class="t">Better than giving it the full PDF at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1637" target="_blank">00:27:17.860</a></span> | <span class="t">Because LLMs have, like, that lost-in-the-middle problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1641" target="_blank">00:27:21.940</a></span> | <span class="t">So if you give it -- like, just because you have a large context window doesn't mean you should flood it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1646" target="_blank">00:27:26.100</a></span> | <span class="t">Because they still have the problem of searching through, like, the full -- a large document to find the right information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1653" target="_blank">00:27:33.300</a></span> | <span class="t">So you're trying to --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1654" target="_blank">00:27:34.100</a></span> | <span class="t">So the relation between these different states should be maintained?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1659" target="_blank">00:27:39.220</a></span> | <span class="t">Yeah. I think, as she was pointing out, either you'd have to, like, maybe structure it a little differently to have some overlap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1665" target="_blank">00:27:45.780</a></span> | <span class="t">or store some additional metadata to maintain that continuity, like page numbers or -- and whenever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1672" target="_blank">00:27:52.260</a></span> | <span class="t">you're retrieving a page, you could do something like retrieve the previous two, next two pages, things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1678" target="_blank">00:27:58.340</a></span> | <span class="t">Yeah. Any more questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1680" target="_blank">00:28:00.260</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1680" target="_blank">00:28:00.820</a></span> | <span class="t">You talked about Voyage multimodal 3, like, why that one versus others?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1686" target="_blank">00:28:06.420</a></span> | <span class="t">Any VLM-based model, really.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1688" target="_blank">00:28:08.900</a></span> | <span class="t">Like, the whole point is to show you that clip-based models have that -- I'm not using a clip-based model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1693" target="_blank">00:28:13.620</a></span> | <span class="t">because it has a modality gap, but any VLM-based model is very good. Yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1699" target="_blank">00:28:19.460</a></span> | <span class="t">So this works for text and image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1703" target="_blank">00:28:23.060</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1703" target="_blank">00:28:23.540</a></span> | <span class="t">And they're more, like, video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1706" target="_blank">00:28:26.420</a></span> | <span class="t">Right. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1707" target="_blank">00:28:27.300</a></span> | <span class="t">Like, yep, 100%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1709" target="_blank">00:28:29.540</a></span> | <span class="t">So this doesn't really deal with that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1711" target="_blank">00:28:31.700</a></span> | <span class="t">But essentially, you could extend this concept to different modalities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1717" target="_blank">00:28:37.460</a></span> | <span class="t">It's just I typically don't see, like, video or audio occurring with text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1722" target="_blank">00:28:42.820</a></span> | <span class="t">Like, I just chose this because, like, images, figures typically occur with text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1728" target="_blank">00:28:48.980</a></span> | <span class="t">But so, yeah, screenshots might not apply to other modalities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1733" target="_blank">00:28:53.620</a></span> | <span class="t">There are different ways to handle it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1734" target="_blank">00:28:54.980</a></span> | <span class="t">Today, we are only focusing on images and text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1737" target="_blank">00:28:57.140</a></span> | <span class="t">Okay. Two more questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1740" target="_blank">00:29:00.020</a></span> | <span class="t">Okay. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1740" target="_blank">00:29:00.740</a></span> | <span class="t">So, VLM, I guess this is different from the last language model, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1746" target="_blank">00:29:06.020</a></span> | <span class="t">So it has, I guess, smaller parameters than last language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1749" target="_blank">00:29:09.540</a></span> | <span class="t">But then for home, it's a bit less, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1752" target="_blank">00:29:12.020</a></span> | <span class="t">It's just, it's still a large --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1756" target="_blank">00:29:16.180</a></span> | <span class="t">For example, like we make an embedding from the VLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1760" target="_blank">00:29:20.180</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1760" target="_blank">00:29:20.740</a></span> | <span class="t">And then if you make an embedding from the, let's say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1765" target="_blank">00:29:25.060</a></span> | <span class="t">other 002, which is bigger parameters, then performance probably --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1770" target="_blank">00:29:30.500</a></span> | <span class="t">actually is probably less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1772" target="_blank">00:29:32.180</a></span> | <span class="t">VLMs tend to get pretty big, too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1774" target="_blank">00:29:34.180</a></span> | <span class="t">So they're basically still large models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1776" target="_blank">00:29:36.340</a></span> | <span class="t">They just can handle, like, images and text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1778" target="_blank">00:29:38.980</a></span> | <span class="t">But they're still pretty sizable models, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1782" target="_blank">00:29:42.500</a></span> | <span class="t">And I can point you to some benchmarks that show that they're still good at even purely text or purely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1788" target="_blank">00:29:48.500</a></span> | <span class="t">image data and then a combination of both.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1790" target="_blank">00:29:50.500</a></span> | <span class="t">How to run in the local?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1791" target="_blank">00:29:51.460</a></span> | <span class="t">What's that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1792" target="_blank">00:29:52.500</a></span> | <span class="t">How to run in the local machine, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1794" target="_blank">00:29:54.500</a></span> | <span class="t">Like, you'd find a model that works with your hardware specifications, just like an LLM, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1800" target="_blank">00:30:00.660</a></span> | <span class="t">Like, not all LLMs can be run on your machine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1803" target="_blank">00:30:03.540</a></span> | <span class="t">So it would be similar to those, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1806" target="_blank">00:30:06.660</a></span> | <span class="t">You know, there was one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1807" target="_blank">00:30:07.700</a></span> | <span class="t">No, there was one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1808" target="_blank">00:30:08.820</a></span> | <span class="t">Let's skip one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1809" target="_blank">00:30:09.700</a></span> | <span class="t">Okay, one more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1811" target="_blank">00:30:11.140</a></span> | <span class="t">Yeah, so in this case, you're using multimodal, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1814" target="_blank">00:30:14.420</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1815" target="_blank">00:30:15.140</a></span> | <span class="t">It's true that your image and text is strongly aligned to the modalities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1819" target="_blank">00:30:19.140</a></span> | <span class="t">But what if you have a modality that's really weakly aligned to the time series, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1822" target="_blank">00:30:22.900</a></span> | <span class="t">So which means your data space, they're not pretty close.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1825" target="_blank">00:30:25.220</a></span> | <span class="t">How do you handle those?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1826" target="_blank">00:30:26.180</a></span> | <span class="t">Sorry, can you say that again?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1827" target="_blank">00:30:27.700</a></span> | <span class="t">If you have a modality that's not strongly aligned with the rest of the modalities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1832" target="_blank">00:30:32.340</a></span> | <span class="t">like, for example, time series, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1834" target="_blank">00:30:34.020</a></span> | <span class="t">If you embed it into the same data space, they are not, like, really close to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1839" target="_blank">00:30:39.860</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1840" target="_blank">00:30:40.420</a></span> | <span class="t">So in both situations, how do you handle that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1842" target="_blank">00:30:42.900</a></span> | <span class="t">So, like, time series data with text?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1845" target="_blank">00:30:45.620</a></span> | <span class="t">Like, I'm trying to understand, like, a situation where you would have totally disparate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1850" target="_blank">00:30:50.100</a></span> | <span class="t">So you have, like, text, you may have time series, too, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1853" target="_blank">00:30:53.140</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1853" target="_blank">00:30:53.540</a></span> | <span class="t">And that time series data may not be really aligned with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1855" target="_blank">00:30:55.860</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1856" target="_blank">00:30:56.580</a></span> | <span class="t">And that means when you clip them, they don't really go very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1860" target="_blank">00:31:00.500</a></span> | <span class="t">So how do you handle those?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1861" target="_blank">00:31:01.940</a></span> | <span class="t">Yeah, I think time series data, typically, you don't even, like, use embeddings for it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1865" target="_blank">00:31:05.540</a></span> | <span class="t">It's just, like, you treat them like any other features like you would for traditional ML models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1871" target="_blank">00:31:11.220</a></span> | <span class="t">You definitely want, like, a different retrieval strategy for those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1875" target="_blank">00:31:15.060</a></span> | <span class="t">It would be hard to put them in the same, yeah, vector space as text and images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1879" target="_blank">00:31:19.860</a></span> | <span class="t">So you might need to work with, like, different retrieval methodologies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1883" target="_blank">00:31:23.380</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1884" target="_blank">00:31:24.500</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1885" target="_blank">00:31:25.780</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1886" target="_blank">00:31:26.820</a></span> | <span class="t">I'm going to move forward here very-- like, in a few minutes, we will hit the hands-on portion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1892" target="_blank">00:31:32.900</a></span> | <span class="t">So if we have more questions, just call out to our team, and we'll take more questions then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1898" target="_blank">00:31:38.180</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1899" target="_blank">00:31:39.700</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1900" target="_blank">00:31:40.820</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1901" target="_blank">00:31:41.700</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1902" target="_blank">00:31:42.100</a></span> | <span class="t">Let's quickly talk about the workflow of our agent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1904" target="_blank">00:31:44.820</a></span> | <span class="t">We looked at a random example before, but let's talk about the agent you're going to build.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1910" target="_blank">00:31:50.020</a></span> | <span class="t">So query comes in, agent forwards the query to a multimodal LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1916" target="_blank">00:31:56.100</a></span> | <span class="t">So note, we are going to use a multimodal embedding model for retrieval,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1919" target="_blank">00:31:59.780</a></span> | <span class="t">but we also need a multimodal LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1921" target="_blank">00:32:01.780</a></span> | <span class="t">We are going to use, I think, Gemini 2.0 Flash Experimental, some long name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1926" target="_blank">00:32:06.820</a></span> | <span class="t">But yeah, basically, we need that LLM because once we give it, like, that interleaved document</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1933" target="_blank">00:32:13.780</a></span> | <span class="t">with text and images, we need an LLM that can make sense of both these modalities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1938" target="_blank">00:32:18.820</a></span> | <span class="t">So that's why I'm using that LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1941" target="_blank">00:32:21.300</a></span> | <span class="t">It has just one tool, which is a vector search tool to retrieve those multimodal documents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1948" target="_blank">00:32:28.020</a></span> | <span class="t">and also its past interactions and memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1950" target="_blank">00:32:30.100</a></span> | <span class="t">So based on the query, the LLM can decide to call the vector search tool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1955" target="_blank">00:32:35.620</a></span> | <span class="t">And if it does that, it'll return the name of the tool and the arguments to use to call the tool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1961" target="_blank">00:32:41.220</a></span> | <span class="t">Again, the agent has code to actually call the tool, so it calls the vector search tool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1968" target="_blank">00:32:48.020</a></span> | <span class="t">And typically, if you're working with text-based data, you get the documents back directly from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1973" target="_blank">00:32:53.940</a></span> | <span class="t">vector search. But in this case, what we are going to get back is references to the screenshots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1978" target="_blank">00:32:58.820</a></span> | <span class="t">Remember, we didn't store those in the vector database. Those are in our local or blob storage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1984" target="_blank">00:33:04.500</a></span> | <span class="t">So then our agent needs to have that additional step of using those image references to actually get the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1990" target="_blank">00:33:10.100</a></span> | <span class="t">screenshots from blob storage. And then it's going to pass those images along with the original user query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=1996" target="_blank">00:33:16.580</a></span> | <span class="t">and any past conversational history to the multimodal LLM. So each time an LLM call is made, whether it's to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2004" target="_blank">00:33:24.420</a></span> | <span class="t">determine what tools to call or generate the final answer, the images are also going to be passed along with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2011" target="_blank">00:33:31.220</a></span> | <span class="t">the query and conversation history to the LLM. Then it generates an answer and that gets returned back to the user.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2021" target="_blank">00:33:41.620</a></span> | <span class="t">And finally, depending on the query, the LLM might also decide that it doesn't need to call a tool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2026" target="_blank">00:33:46.820</a></span> | <span class="t">So, for example, if the user is simply asking, like, "Hey, summarize this image," it might not need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2031" target="_blank">00:33:51.220</a></span> | <span class="t">call a tool. So in that case, it'll say, "I don't need to call tools." It'll simply generate an answer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2038" target="_blank">00:33:58.260</a></span> | <span class="t">and that gets forwarded to the user. And the final thing, let's talk about the memory management</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2046" target="_blank">00:34:06.100</a></span> | <span class="t">mechanism for our agent because this is important for it to actually have coherent multi-term conversations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2053" target="_blank">00:34:13.140</a></span> | <span class="t">with the user. So, like I mentioned before, we'll be implementing short-term memory for the agent. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2059" target="_blank">00:34:19.140</a></span> | <span class="t">the way this works is each user query is associated with a session ID just as some identifier to distinguish</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2066" target="_blank">00:34:26.660</a></span> | <span class="t">between different conversations. So, given a user query, we obtain its session or conversation ID,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2073" target="_blank">00:34:33.620</a></span> | <span class="t">and we query a database consisting of previous turns in the conversation to get that chat history for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2079" target="_blank">00:34:39.940</a></span> | <span class="t">that session. And each time, again, in addition to the context, we also pass in that chat history,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2087" target="_blank">00:34:47.060</a></span> | <span class="t">just so the LLM can use that as additional context to determine if it even needs to call tools or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2095" target="_blank">00:34:55.060</a></span> | <span class="t">And then, when the LLM generates a response, the other thing that happens is we add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2099" target="_blank">00:34:59.300</a></span> | <span class="t">this current response and the current query back to the database to add on to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2106" target="_blank">00:35:06.260</a></span> | <span class="t">the history for that session. Now, you can also log a tool call, their outcomes, any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2112" target="_blank">00:35:12.420</a></span> | <span class="t">reasoning traces from the LLM, but at a minimum, you at least want to be logging the LLM's response and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2120" target="_blank">00:35:20.020</a></span> | <span class="t">the user queries themselves. And finally, that is enough talking from me. You will be quiet for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2126" target="_blank">00:35:26.980</a></span> | <span class="t">the rest of the workshop. We have about 45-ish minutes. So, head over to that link to access the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2136" target="_blank">00:35:36.660</a></span> | <span class="t">hands-on lab. Recommend running that on your laptop. So, instead of that QR code, actually type in that URL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2142" target="_blank">00:35:42.980</a></span> | <span class="t">This should take you to a GitHub repo. Follow the instructions in the readme to get set up for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2149" target="_blank">00:35:49.140</a></span> | <span class="t">lab. That should take about 10 minutes. And then, you have two options. You can either -- there are two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2155" target="_blank">00:35:55.220</a></span> | <span class="t">notebooks in there. One is called lab.ipynb. And if you actually want to -- if you're in the mood to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2162" target="_blank">00:36:02.180</a></span> | <span class="t">actually write code right now, that's the notebook you'll be using. You'll have -- you'll see reference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2166" target="_blank">00:36:06.820</a></span> | <span class="t">documentation in line in the notebook indicated by that books emoji that tells you use this documentation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2172" target="_blank">00:36:12.820</a></span> | <span class="t">fill in your code. You can do that if that sounds too daunting. There's also a notebook called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2179" target="_blank">00:36:19.060</a></span> | <span class="t">solutions.ipynb that has all the code pre-filled. So, you can just run through that notebook,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2185" target="_blank">00:36:25.620</a></span> | <span class="t">read the comments to get an understanding of how the agent works. But which option you use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2191" target="_blank">00:36:31.300</a></span> | <span class="t">I'm here. My team is here. Just call on us to -- if you have any questions. And yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2197" target="_blank">00:36:37.540</a></span> | <span class="t">for anyone actually filling in the code, you can also refer to the solutions. Don't get too frustrated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=640KMYtxCeI&t=2202" target="_blank">00:36:42.660</a></span> | <span class="t">if you get stuck. We don't want that. All right. So, I'm shutting up now. Let's go ahead and build that agent.</span></div></div></body></html>