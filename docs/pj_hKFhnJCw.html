<html><head><title>How to evaluate a model for your use case: Emmanuel Turlay</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>How to evaluate a model for your use case: Emmanuel Turlay</h2><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw"><img src="https://i.ytimg.com/vi/pj_hKFhnJCw/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./pj_hKFhnJCw.html">Whisper Transcript</a> | <a href="./transcript_pj_hKFhnJCw.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi everyone, I'm Emmanuel, CEO of Sematic, the company behind Airtrain. Today, I want to talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=24" target="_blank">00:00:24.800</a></span> | <span class="t">about a difficult problem in the language modeling space, and that is evaluation. Unlike in other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=31" target="_blank">00:00:31.040</a></span> | <span class="t">areas of machine learning, it is not so straightforward to evaluate language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=35" target="_blank">00:00:35.680</a></span> | <span class="t">for a specific use case. There are metrics and benchmarks, but they mostly apply to generic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=42" target="_blank">00:00:42.000</a></span> | <span class="t">tasks, and there is no one-size-fits-all process to evaluate the performance of a model for a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=47" target="_blank">00:00:47.040</a></span> | <span class="t">particular use case. So first, let's get the basics out of the way. What is model evaluation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=54" target="_blank">00:00:54.800</a></span> | <span class="t">Model evaluation is the statistical measurement of the performance of a machine learning model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=60" target="_blank">00:01:00.160</a></span> | <span class="t">How well does a model perform on a particular use case, measured on a large dataset independent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=65" target="_blank">00:01:05.760</a></span> | <span class="t">from the training dataset? Model evaluation usually comes right after training or fine-tuning and is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=72" target="_blank">00:01:12.880</a></span> | <span class="t">crucial part of model development. All ML teams dedicate large resources to establish rigorous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=78" target="_blank">00:01:18.800</a></span> | <span class="t">evaluation procedures. You need to set up a solid evaluation process as part of your development</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=84" target="_blank">00:01:24.640</a></span> | <span class="t">workflow to guarantee performance and safety. You can compare evaluation to running a test suite in your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=90" target="_blank">00:01:30.880</a></span> | <span class="t">continuous integration pipeline. In traditional supervised machine learning, there is a whole host</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=96" target="_blank">00:01:36.720</a></span> | <span class="t">of well-defined metrics to clearly grade a model's performance. For example, for regressions, we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=104" target="_blank">00:01:44.000</a></span> | <span class="t">the root mean squared error or the mean absolute error. For classifiers, people usually use precision, recall,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=113" target="_blank">00:01:53.120</a></span> | <span class="t">or F1 score, and so on. In computer vision, a popular metric is the intersection of a union.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=121" target="_blank">00:02:01.120</a></span> | <span class="t">So what metrics are available to score language models? Well, unlike other types of models returning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=128" target="_blank">00:02:08.000</a></span> | <span class="t">structured outputs such as a number, a class, or a bounding box, language models generate text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=134" target="_blank">00:02:14.640</a></span> | <span class="t">which is very unstructured. An inference that is different from the ground truth reference is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=139" target="_blank">00:02:19.920</a></span> | <span class="t">necessarily incorrect. Depending on whether you have access to labeled references, there are a number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=146" target="_blank">00:02:26.160</a></span> | <span class="t">metrics you can use. For example, BLEU is a precision-based metric. It measures the overlap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=152" target="_blank">00:02:32.720</a></span> | <span class="t">between n-grams, that is sequences of tokens, between the generated text and the inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=158" target="_blank">00:02:38.400</a></span> | <span class="t">It's a common metric to evaluate translation between two languages and can also be used to score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=164" target="_blank">00:02:44.480</a></span> | <span class="t">summarization. It can definitely serve as a good benchmark, but it is not a safe indicator of how a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=170" target="_blank">00:02:50.800</a></span> | <span class="t">model will perform on your particular task. For example, it does not take into account intelligibility</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=176" target="_blank">00:02:56.960</a></span> | <span class="t">or grammatical correctness. Rouge is a set of evaluation metrics that focuses on measuring the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=183" target="_blank">00:03:03.200</a></span> | <span class="t">recall of sequences of tokens between references and the inference. It is mostly useful to evaluate for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=191" target="_blank">00:03:11.120</a></span> | <span class="t">summarization. If you don't have access to labeled references, you can use other standalone metrics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=198" target="_blank">00:03:18.560</a></span> | <span class="t">For example, density quantifies how well the summary represents pool fragments from the text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=204" target="_blank">00:03:24.000</a></span> | <span class="t">and coverage quantifies the extent to which a summary is derivative of a text. As you can see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=210" target="_blank">00:03:30.960</a></span> | <span class="t">these metrics are only useful to score certain high-level tasks such as translation and summarization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=217" target="_blank">00:03:37.440</a></span> | <span class="t">There are also a number of benchmarks and leaderboards that rank various models. Benchmarks are standardized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=225" target="_blank">00:03:45.600</a></span> | <span class="t">tests that score model performance for certain tasks. For example, glue or general language understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=233" target="_blank">00:03:53.200</a></span> | <span class="t">evaluation is a common benchmark to evaluate how well a model understands language through a series of nine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=240" target="_blank">00:04:00.000</a></span> | <span class="t">tasks. For example, paraphrase detection and sentiment analysis. Helleswag measures natural language inference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=249" target="_blank">00:04:09.760</a></span> | <span class="t">which is the ability for a model to have common sense and find the most plausible end to a sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=255" target="_blank">00:04:15.520</a></span> | <span class="t">In this case, answer C is the most reasonable choice. There are other benchmarks such as trivia QA,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=263" target="_blank">00:04:23.680</a></span> | <span class="t">which asks almost a million trivia questions from Wikipedia and other sources and tests the knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=269" target="_blank">00:04:29.360</a></span> | <span class="t">of the model. Also AHRQ test models' ability to reason about high school level science questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=275" target="_blank">00:04:35.760</a></span> | <span class="t">And there are dozens more benchmarks out there. All these metrics and benchmarks are very useful to draw a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=282" target="_blank">00:04:42.800</a></span> | <span class="t">landscape of how LLMs compare to one another. But they do not tell you how they perform for your particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=289" target="_blank">00:04:49.360</a></span> | <span class="t">task on the type of input data that will be fed by your application. For example, if you're trying to extract symptoms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=297" target="_blank">00:04:57.120</a></span> | <span class="t">from a doctor's notes, or extract ingredients from a recipe, or form a JSON payload to query an API,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=304" target="_blank">00:05:04.560</a></span> | <span class="t">these metrics will not tell you how each model performs. So each application needs to come up with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=311" target="_blank">00:05:11.120</a></span> | <span class="t">with its own evaluation procedure, which is a lot of work. There is one magic trick though. You can use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=318" target="_blank">00:05:18.880</a></span> | <span class="t">another model to grade the output of your model. You can describe to an LLM what you're trying to accomplish</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=325" target="_blank">00:05:25.760</a></span> | <span class="t">and what are the grading criteria and ask it to grade the output of another LLM on a numerical scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=332" target="_blank">00:05:32.160</a></span> | <span class="t">Essentially, you are crafting your own specialized metrics for your own application.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=338" target="_blank">00:05:38.640</a></span> | <span class="t">Here's an example of how it works. You can feed your evaluation data set to the model you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=344" target="_blank">00:05:44.000</a></span> | <span class="t">to evaluate, which is going to generate the inferences that you want to score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=347" target="_blank">00:05:47.120</a></span> | <span class="t">Then, you can include those inferences inside a broader scoring prompt in which you've described</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=354" target="_blank">00:05:54.720</a></span> | <span class="t">the task you're trying to accomplish and the properties you're trying to grade. And also,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=358" target="_blank">00:05:58.800</a></span> | <span class="t">you describe the scale across which it should be graded. For example, from 1 to 10. Then,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=364" target="_blank">00:06:04.400</a></span> | <span class="t">you pass this scoring prompt to a scoring model, which is going to generate a number - a score - to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=370" target="_blank">00:06:10.240</a></span> | <span class="t">score the actual inference. If you do this on all the inferences generated from your evaluation data set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=376" target="_blank">00:06:16.160</a></span> | <span class="t">you can draw a distribution of that particular metric. For example, here is a small set of closing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=382" target="_blank">00:06:22.000</a></span> | <span class="t">words generated for professional emails. We want to evaluate their politeness. We can prompt a model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=388" target="_blank">00:06:28.400</a></span> | <span class="t">score the politeness of each statement from 1 to 10. For example, "Please let us know at your earliest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=394" target="_blank">00:06:34.800</a></span> | <span class="t">convenience" scores highly, while "Tell me ASAP will score poorly." We found that the best grading model at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=402" target="_blank">00:06:42.400</a></span> | <span class="t">this time is still GPT-4, but can be quite costly to use to score large datasets. We have found that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=408" target="_blank">00:06:48.880</a></span> | <span class="t">FLAN-T5 offers a good trade-off of speed and correctness. Airtrain was designed specifically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=415" target="_blank">00:06:55.200</a></span> | <span class="t">for this purpose. With Airtrain, you can upload your dataset, select the models you want to compare,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=421" target="_blank">00:07:01.040</a></span> | <span class="t">describe the properties you want to measure, and visualize metric distribution across your entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=426" target="_blank">00:07:06.400</a></span> | <span class="t">dataset. You can compare LAMA2 with Falcon, FLAN-T5, or even your own model. Then, you can make an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=433" target="_blank">00:07:13.440</a></span> | <span class="t">dedicated decision based on statistical evidence. Sign up today for early access at Airtrain.ai and start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pj_hKFhnJCw&t=440" target="_blank">00:07:20.240</a></span> | <span class="t">making data-driven decisions about your choice of LLM. Thanks. Goodbye.</span></div></div></body></html>