<html><head><title>Decoding Mistral AI's Large Language Models: Devendra Chaplot</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Decoding Mistral AI's Large Language Models: Devendra Chaplot</h2><a href="https://www.youtube.com/watch?v=AUuktOQPWYg"><img src="https://i.ytimg.com/vi_webp/AUuktOQPWYg/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./AUuktOQPWYg.html">Whisper Transcript</a> | <a href="./transcript_AUuktOQPWYg.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hey everyone, I'm very excited to be here. I am very happy that there is an open models track.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=21" target="_blank">00:00:21.960</a></span> | <span class="t">So I'm going to talk about the open models of Mistral AI and go a little bit deeper into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=31" target="_blank">00:00:31.460</a></span> | <span class="t">why we do open source and how we do open source. So first of all, Mistral AI, we started last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=38" target="_blank">00:00:38.840</a></span> | <span class="t">June about one year ago. We released our first open model Mistral 7B in September 23. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=46" target="_blank">00:00:46.840</a></span> | <span class="t">then after that in December, we released our first mixture of experts, open model 8x7B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=53" target="_blank">00:00:53.720</a></span> | <span class="t">And along with that, we released our platform with model APIs, and also commercial models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=60" target="_blank">00:01:00.600</a></span> | <span class="t">Mistral medium and Mistral embed. And then earlier this year in February, we released Mistral</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=66" target="_blank">00:01:06.460</a></span> | <span class="t">large, which is our flagship model, which has the best in class reasoning and math ability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=75" target="_blank">00:01:15.840</a></span> | <span class="t">And also, uh, in April, we released a new open model 8x22B. And then, uh, very recently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=84" target="_blank">00:01:24.920</a></span> | <span class="t">in June, we released a code specific model called costral 22B. And, uh, it's also available,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=91" target="_blank">00:01:31.920</a></span> | <span class="t">uh, in the chat interface that we built, uh, along with Mistral large and it's, uh, free to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=98" target="_blank">00:01:38.720</a></span> | <span class="t">use. Um, so our mission, um, is to bring frontier AI in everyone's hands. And we specifically focus on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=111" target="_blank">00:01:51.840</a></span> | <span class="t">building cutting edge AI for developers. And we have certain principles behind how we go about training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=120" target="_blank">00:02:00.800</a></span> | <span class="t">models and releasing them. So the first is openness. We want to train best in class open models and, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=129" target="_blank">00:02:09.880</a></span> | <span class="t">release it for, uh, the open source community. We want our models to be portable. Uh, all our models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=136" target="_blank">00:02:16.880</a></span> | <span class="t">are available on Azure, AWS, GCP, virtual private cloud, and also they can be deployed, deployed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=142" target="_blank">00:02:22.960</a></span> | <span class="t">on-prem, which means, uh, you can, uh, license the model weights and use, use it on your own servers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=151" target="_blank">00:02:31.120</a></span> | <span class="t">uh, with full control over security and privacy of your data. Uh, we try to optimize for the performance to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=158" target="_blank">00:02:38.960</a></span> | <span class="t">speed ratio. Uh, our models are particularly good at getting the best performance out of a particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=165" target="_blank">00:02:45.280</a></span> | <span class="t">size. And, um, we want our models to be customizable. Uh, we are building our platform to, with all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=176" target="_blank">00:02:56.000</a></span> | <span class="t">libraries and tools to customize our models, uh, depending on your application. Uh, we recently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=181" target="_blank">00:03:01.840</a></span> | <span class="t">released the Mistral fine tune open source library, which can be used to find any of our open source models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=187" target="_blank">00:03:07.920</a></span> | <span class="t">And also, uh, we have a fine tuning API on our, uh, platform. And before that, we also released</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=194" target="_blank">00:03:14.960</a></span> | <span class="t">Mistral inference, which is the inference library, uh, again, open source. Uh, so I talked about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=201" target="_blank">00:03:21.040</a></span> | <span class="t">these three models that we have open, um, sourced in the last one year. The first model is a dense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=209" target="_blank">00:03:29.920</a></span> | <span class="t">transformer model. Uh, it was the first model, first 7B model to achieve 60 on MMLU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=216" target="_blank">00:03:36.880</a></span> | <span class="t">And we saw that the 60 MMLU is like, uh, a bare minimum where the models become useful. And this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=223" target="_blank">00:03:43.520</a></span> | <span class="t">was the first 7B model to achieve this. And people have, uh, people, people have been using this model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=231" target="_blank">00:03:51.040</a></span> | <span class="t">for many, many different applications. And particularly we have seen that this model can be deployed on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=235" target="_blank">00:03:55.920</a></span> | <span class="t">laptops and phones, uh, and, uh, still get reasonable speed, uh, on, on device. Uh, we released the first, um, our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=245" target="_blank">00:04:05.840</a></span> | <span class="t">first sparse mixture of experts model in December 8X 7B. It's based on the, uh, mixture of experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=253" target="_blank">00:04:13.680</a></span> | <span class="t">architecture, uh, which basically allows us to push the performance of a model while keeping the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=261" target="_blank">00:04:21.120</a></span> | <span class="t">inference budget in check. The idea here is we have higher number of total parameters in the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=266" target="_blank">00:04:26.640</a></span> | <span class="t">which allows the model to, uh, still have the knowledge, uh, stored in the model weights. But at the same time, we use only a small subset of the parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=274" target="_blank">00:04:34.800</a></span> | <span class="t">Uh, for every token, which makes it really fast and cost efficient at inference time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=280" target="_blank">00:04:40.400</a></span> | <span class="t">And then we released a bigger version of this sparse mixture of experts architecture 8X 22B in April.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=287" target="_blank">00:04:47.120</a></span> | <span class="t">Uh, it has even better performance, higher, uh, context window, and also it's multilingual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=293" target="_blank">00:04:53.760</a></span> | <span class="t">Uh, it supports English, French, Italian, German, Spanish, and also, uh, many other languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=300" target="_blank">00:05:00.320</a></span> | <span class="t">Um, so a lot of people ask me, if you open source your models, how do you make money? And, uh, I think this is a common misconception that people have that open source is somewhat, uh, competitive with profit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=319" target="_blank">00:05:19.440</a></span> | <span class="t">It's actually not the case. We see open source as, uh, uh, something that is goes hand in hand with profit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=329" target="_blank">00:05:29.120</a></span> | <span class="t">It doesn't necessarily have to be competitive. It can be, uh, complimentary. And, uh, we want to be in this quadrant where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=338" target="_blank">00:05:38.000</a></span> | <span class="t">where we can open source our models and still have long term business value with the models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=342" target="_blank">00:05:42.800</a></span> | <span class="t">Um, so why do we open source? So the first reason is it, uh, serves as a very good branding and marketing tool for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=350" target="_blank">00:05:50.400</a></span> | <span class="t">Um, so we believe in open source and open science, and we want to contribute, uh, to the community, but it's not a, a one way thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=360" target="_blank">00:06:00.400</a></span> | <span class="t">Uh, we are also benefiting from open source just as the community is benefiting from our models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=366" target="_blank">00:06:06.400</a></span> | <span class="t">So it helps us doing, doing, uh, a lot of branding and marketing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=370" target="_blank">00:06:10.160</a></span> | <span class="t">Uh, a lot of people like our models. They tell other people that our models are good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=375" target="_blank">00:06:15.200</a></span> | <span class="t">the model performance speaks for itself. We do not have a marketing team in house.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=379" target="_blank">00:06:19.920</a></span> | <span class="t">And, uh, just the open sourcing, the models allows us to create awareness about our products.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=387" target="_blank">00:06:27.200</a></span> | <span class="t">It also helps us in customer acquisition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=390" target="_blank">00:06:30.640</a></span> | <span class="t">If people try out our open source models and they really like it, they come to us for an upgrade to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=397" target="_blank">00:06:37.120</a></span> | <span class="t">proprietary models and, uh, they pay for the upgrade.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=399" target="_blank">00:06:39.920</a></span> | <span class="t">And it also helps in customization and portability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=405" target="_blank">00:06:45.440</a></span> | <span class="t">Uh, when, whenever, uh, for example, the seven B model, people can try it, uh, to try to run it on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=414" target="_blank">00:06:54.320</a></span> | <span class="t">laptops and phones. And this is the kind of stuff we benefit from because we don't necessarily have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=420" target="_blank">00:07:00.480</a></span> | <span class="t">do this out of the box, but the community works around our models and we learn from the community,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=425" target="_blank">00:07:05.760</a></span> | <span class="t">how our models can be customized or, uh, deployed in new settings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=429" target="_blank">00:07:09.760</a></span> | <span class="t">So how are, um, these open source models trained?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=433" target="_blank">00:07:13.840</a></span> | <span class="t">So I, I'll give you a very high, uh, level overview of the different stages of LLM training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=441" target="_blank">00:07:21.120</a></span> | <span class="t">And typically LLMs are trained in three stages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=444" target="_blank">00:07:24.080</a></span> | <span class="t">Pre-training instruction, tuning, and learning from human feedback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=447" target="_blank">00:07:27.760</a></span> | <span class="t">So the idea behind pre-training is very simple. You take a piece of text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=454" target="_blank">00:07:34.480</a></span> | <span class="t">and you pass, uh, word by word or token by token through the large language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=462" target="_blank">00:07:42.400</a></span> | <span class="t">and ask the model to predict the next token. Um, so the idea itself is very simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=469" target="_blank">00:07:49.760</a></span> | <span class="t">Each, uh, the task is the next token prediction. Each token is roughly 0.75 word. The vocabulary size is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=477" target="_blank">00:07:57.040</a></span> | <span class="t">roughly tens of thousands of tokens, or sometimes hundreds of thousands. And each token is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=482" target="_blank">00:08:02.800</a></span> | <span class="t">represented as an integer and it has an embedding associated with it. And so the task of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=488" target="_blank">00:08:08.000</a></span> | <span class="t">is to take in a sequence of embeddings or tokens and predict the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=492" target="_blank">00:08:12.240</a></span> | <span class="t">Although the concept is very simple, in practice, it's actually very hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=497" target="_blank">00:08:17.840</a></span> | <span class="t">Why is it hard? Because it requires a lot of effort in building the data sets. The data sets are huge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=506" target="_blank">00:08:26.400</a></span> | <span class="t">They are order of trillions of tokens, tens of trillions of tokens, uh, that requires pre-processing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=512" target="_blank">00:08:32.640</a></span> | <span class="t">cleaning, deduplication, curation. And there's, again, a common belief that more data leads to better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=520" target="_blank">00:08:40.320</a></span> | <span class="t">performance, but that's not, not necessarily the case. Uh, if you have noise in your data, that can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=526" target="_blank">00:08:46.320</a></span> | <span class="t">actually hurt the model performance. It also requires a lot of investment. Uh, these models are huge, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=534" target="_blank">00:08:54.000</a></span> | <span class="t">can go up to hundreds or even hundreds of billions or even trillions of parameters. Uh, each model takes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=540" target="_blank">00:09:00.720</a></span> | <span class="t">tens to hundreds of millions of dollars to train. And the hardest part is you don't get multiple chances</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=549" target="_blank">00:09:09.040</a></span> | <span class="t">to train the model. Uh, the, because it's so expensive, if something grows wrong in your training, uh, it's, uh, very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=559" target="_blank">00:09:19.200</a></span> | <span class="t">difficult to get the investment to do another training run, uh, because, uh, typically for small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=565" target="_blank">00:09:25.680</a></span> | <span class="t">companies, you don't get that kind of budget. If you do a model run and it's not successful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=571" target="_blank">00:09:31.040</a></span> | <span class="t">it becomes harder to get the funding for the next run. Um, and this is hard because the best hyper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=580" target="_blank">00:09:40.000</a></span> | <span class="t">parameters for a smaller model might not be the best for a larger model. Uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=586" target="_blank">00:09:46.560</a></span> | <span class="t">here I'm showing you some hyper parameters for Lama one model family sizes. And you might ask,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=593" target="_blank">00:09:53.840</a></span> | <span class="t">uh, why are the number of players 80 and not 82 in Lama 65B? And the answer is, we don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=604" target="_blank">00:10:04.640</a></span> | <span class="t">Uh, there's a lot of things that are been, uh, decided by intuition and it's not exact science.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=614" target="_blank">00:10:14.400</a></span> | <span class="t">Uh, so you'd need a lot of experience and intuition working with these models to come up with things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=620" target="_blank">00:10:20.960</a></span> | <span class="t">that are very likely to work, but, uh, we don't, uh, we're still not very mature with the science of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=630" target="_blank">00:10:30.160</a></span> | <span class="t">what is the best way to train the model or what's the best architecture, what's the best data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=635" target="_blank">00:10:35.120</a></span> | <span class="t">mixture. So, uh, can we use this pre-trained model? Um, so let's say if you want to use this pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=642" target="_blank">00:10:42.560</a></span> | <span class="t">model and, uh, ask it to write a Python function to find whether the input number is prime or not,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=647" target="_blank">00:10:47.440</a></span> | <span class="t">and the model might give you a response like this, uh, continues the text, gives an example and like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=654" target="_blank">00:10:54.640</a></span> | <span class="t">describes the approach, but it might not give you the code. And this is because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=659" target="_blank">00:10:59.360</a></span> | <span class="t">the model is trained to do this, it's trained to predict the next token. So it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=662" target="_blank">00:11:02.880</a></span> | <span class="t">predicts the most likely token from the text data it's been trained on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=667" target="_blank">00:11:07.520</a></span> | <span class="t">But there is a way to trick the model. If you give this input, like as a Python function definition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=676" target="_blank">00:11:16.880</a></span> | <span class="t">and a doc string, uh, to, to get the same function, the model actually produces the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=683" target="_blank">00:11:23.360</a></span> | <span class="t">And so this shows you that model actually knows the answer, but it is not aligned with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=689" target="_blank">00:11:29.280</a></span> | <span class="t">human preferences. It's not trained to interact with humans in the way humans want to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=694" target="_blank">00:11:34.480</a></span> | <span class="t">And this is why we need the next two stages. Um, so in the instruction tuning stage, instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=702" target="_blank">00:11:42.080</a></span> | <span class="t">just, uh, a string of text, we have prompt response pairs. So here we are giving the prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=710" target="_blank">00:11:50.160</a></span> | <span class="t">but in the way humans want to interact with the model. So for example, this prompt to write a Python</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=715" target="_blank">00:11:55.760</a></span> | <span class="t">function function and the response is directly the code because that's what humans want as the response.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=719" target="_blank">00:11:59.840</a></span> | <span class="t">And the technique is very simple. Again, we are doing next token prediction, but the only difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=727" target="_blank">00:12:07.120</a></span> | <span class="t">is we are going to mask the prompt itself. We are going to do prediction only for the response.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=732" target="_blank">00:12:12.880</a></span> | <span class="t">Um, so the data set is paired prompt response pairs. We typically use hundreds to hundreds of thousands</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=741" target="_blank">00:12:21.680</a></span> | <span class="t">of instructions. Uh, the task is next word prediction, but just we mask the, the input instruction. Uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=750" target="_blank">00:12:30.560</a></span> | <span class="t">it requires way less compute order of hundred GPUs for a few hours or days is typically sufficient to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=758" target="_blank">00:12:38.720</a></span> | <span class="t">instruction. And then the last steps is learning from human feedback. And here the idea is, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=766" target="_blank">00:12:46.800</a></span> | <span class="t">that human preferences are cheaper or easier to obtain than full human annotation. If I give you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=772" target="_blank">00:12:52.320</a></span> | <span class="t">a prompt like this and two responses, it's much easier for a human to decide which response is better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=778" target="_blank">00:12:58.080</a></span> | <span class="t">than to write the whole response, uh, from scratch. And so this allows us to scale, uh, data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=785" target="_blank">00:13:05.520</a></span> | <span class="t">faster. And there are two main techniques, uh, learning from reinforcement learning from human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=792" target="_blank">00:13:12.560</a></span> | <span class="t">feedback and direct preference optimization, uh, where we use this kind of preference data to fine tune</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=798" target="_blank">00:13:18.080</a></span> | <span class="t">the model, uh, further. So just to summarize, uh, these are the three stages. Um, they have different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=806" target="_blank">00:13:26.880</a></span> | <span class="t">orders of data set and compute requirement, and the task is, uh, slightly different. And all the open source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=814" target="_blank">00:13:34.800</a></span> | <span class="t">models we have, I've been used, I've been trained using these techniques. And so I won't go into the details of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=821" target="_blank">00:13:41.600</a></span> | <span class="t">um, the model architecture itself, but I'll show you the, this nice graph of performance to cost ratio,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=830" target="_blank">00:13:50.800</a></span> | <span class="t">uh, which kind of shows that, uh, we really try to optimize, uh, this metric, uh, we try to get the best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=841" target="_blank">00:14:01.120</a></span> | <span class="t">performance out of our models of a particular size. So here on the x-axis, we have the active parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=847" target="_blank">00:14:07.840</a></span> | <span class="t">which is directly proportional to the cost of running through the model. And on the y-axis,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=852" target="_blank">00:14:12.080</a></span> | <span class="t">we have a popular benchmark, MMLU. So we try to be in the top left corner to get more performance with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=859" target="_blank">00:14:19.360</a></span> | <span class="t">lower cost. Um, we recently released, uh, the code trial model, code trial 22B. It's a dense transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=867" target="_blank">00:14:27.680</a></span> | <span class="t">model trained specifically for code. Um, and again, we are trying to optimize performance and speed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=874" target="_blank">00:14:34.400</a></span> | <span class="t">It's fluent in 80 plus programming languages and it has both, uh, instruct and fill in the middle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=881" target="_blank">00:14:41.040</a></span> | <span class="t">mode, which means that you can use it for code completion, uh, in, uh, your code editor, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=886" target="_blank">00:14:46.800</a></span> | <span class="t">just like GitHub copilot, but also you can use it to ask questions about the bugs or errors you're facing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=892" target="_blank">00:14:52.400</a></span> | <span class="t">just like you would put it in chat GPT. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=896" target="_blank">00:14:56.800</a></span> | <span class="t">So it outperforms code Lama 70 B deep deep seek code 33 B Lama 370 B while being a significantly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=904" target="_blank">00:15:04.000</a></span> | <span class="t">smaller model. So again, we are getting more performance out of a model of a particular size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=909" target="_blank">00:15:09.440</a></span> | <span class="t">And it also has a longer context window, uh, with the other open source code models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=916" target="_blank">00:15:16.720</a></span> | <span class="t">It is multilingual. Uh, we trained it with more than 80 programming languages and, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=922" target="_blank">00:15:22.080</a></span> | <span class="t">across all these different languages tends to perform better than the other models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=928" target="_blank">00:15:28.720</a></span> | <span class="t">So it's, uh, free to use on our chat interface chat.mistral.ti. Uh, we also have the API access</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=937" target="_blank">00:15:37.440</a></span> | <span class="t">available on lab platform, which is our, uh, uh, platform API endpoint. And here, uh, it's also free</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=946" target="_blank">00:15:46.080</a></span> | <span class="t">to use till I believe, uh, end of July. We also have, uh, integration with VS code and JetBrains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=953" target="_blank">00:15:53.680</a></span> | <span class="t">So you can download, uh, a plugin in VS code or JetBrains and use it as a coding assistant for code completion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=963" target="_blank">00:16:03.520</a></span> | <span class="t">So, um, in the end, I would just discuss some practical tips because these are some commonly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=971" target="_blank">00:16:11.840</a></span> | <span class="t">asked questions about how to use open source models and when to use open source versus when,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=976" target="_blank">00:16:16.800</a></span> | <span class="t">uh, to use commercial models. So, uh, if you have a particular application in mind and you want to try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=983" target="_blank">00:16:23.760</a></span> | <span class="t">out commercial models, you could do things like prompt engineering, few short prompting, chain of thought,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=988" target="_blank">00:16:28.880</a></span> | <span class="t">and you could also do retrieval augmented generation, uh, because commercial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=993" target="_blank">00:16:33.360</a></span> | <span class="t">models typically don't allow you to do fine tuning. Uh, but for open models, you can do task specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=1001" target="_blank">00:16:41.520</a></span> | <span class="t">fine tuning as well. You need a little bit of data and compute for this. Uh, but in the end, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=1008" target="_blank">00:16:48.320</a></span> | <span class="t">choice is between how do you, how do you balance performance versus cost commercial models have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=1014" target="_blank">00:16:54.960</a></span> | <span class="t">higher general purpose performance. So they are much easier to get started with if you are trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=1019" target="_blank">00:16:59.360</a></span> | <span class="t">build a new application. Uh, but if you, once you get into production or once you have high volume,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=1025" target="_blank">00:17:05.680</a></span> | <span class="t">open models can beat commercial models on specific tasks with fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=1030" target="_blank">00:17:10.560</a></span> | <span class="t">And, um, uh, typically what we have seen is people prototype with the highest end models. And then once</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=1039" target="_blank">00:17:19.360</a></span> | <span class="t">they figured out that this is the, the task they want to solve, they take, uh, open source model like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=1046" target="_blank">00:17:26.640</a></span> | <span class="t">install seven B or eight X seven B and then fine tuning for their tasks. And this optimizes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=1051" target="_blank">00:17:31.200</a></span> | <span class="t">performance to cost ratio. Uh, we have offices in Paris, London and in Maria. Uh, we are always looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=1060" target="_blank">00:17:40.880</a></span> | <span class="t">for talented, uh, researchers, engineers, uh, business marketing people. Uh, so, uh, please,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=1070" target="_blank">00:17:50.880</a></span> | <span class="t">please, please do a clap and thank you. Uh, I don't know if you're taking questions, but happy. No.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=1076" target="_blank">00:17:56.160</a></span> | <span class="t">Okay. Thank you so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=AUuktOQPWYg&t=1077" target="_blank">00:17:57.760</a></span> | <span class="t">Thank you.</span></div></div></body></html>