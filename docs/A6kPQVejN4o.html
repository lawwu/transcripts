<html><head><title>Titans: Learning to Memorize at Test Time</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Titans: Learning to Memorize at Test Time</h2><a href="https://www.youtube.com/watch?v=A6kPQVejN4o"><img src="https://i.ytimg.com/vi/A6kPQVejN4o/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./A6kPQVejN4o.html">Whisper Transcript</a> | <a href="./transcript_A6kPQVejN4o.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Okay guys, so today we are going to talk about this paper, "Titans - Learning to Memorize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=6" target="_blank">00:00:06.040</a></span> | <span class="t">at Test Time".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=7" target="_blank">00:00:07.040</a></span> | <span class="t">In this paper, we will be seeing first of all what is the problem we are trying to solve,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=11" target="_blank">00:00:11.680</a></span> | <span class="t">and then what is the solution proposed here, and then we will comment on what are the pros</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=15" target="_blank">00:00:15.800</a></span> | <span class="t">and the cons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=18" target="_blank">00:00:18.000</a></span> | <span class="t">The way I like to talk about papers is actually to give you the tools to understand the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=23" target="_blank">00:00:23.400</a></span> | <span class="t">yourself, so I don't like to just read the paper word by word, because that's something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=26" target="_blank">00:00:26.680</a></span> | <span class="t">you can do by yourself, so I like to talk about what is the background knowledge that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=29" target="_blank">00:00:29.760</a></span> | <span class="t">you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=30" target="_blank">00:00:30.760</a></span> | <span class="t">We work a little bit on there, then we look at the problem, and then we see the solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=34" target="_blank">00:00:34.360</a></span> | <span class="t">So let's talk about the problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=35" target="_blank">00:00:35.920</a></span> | <span class="t">The problem we are talking about here is sequence modeling, and up to now, there are two main</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=40" target="_blank">00:00:40.800</a></span> | <span class="t">ways in deep learning to do sequence modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=43" target="_blank">00:00:43.120</a></span> | <span class="t">One is called the transformer, and the other is called the recurrent neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=48" target="_blank">00:00:48.560</a></span> | <span class="t">There are also hybrid variants, which combine a little bit of the attention mechanism with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=52" target="_blank">00:00:52.080</a></span> | <span class="t">the recurrent neural networks, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=54" target="_blank">00:00:54.480</a></span> | <span class="t">So let's talk about how the sequence modeling is done in these two ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=59" target="_blank">00:00:59.040</a></span> | <span class="t">So open a new page, basically imagine you have a very long sequence, imagine let's talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=65" target="_blank">00:01:05.600</a></span> | <span class="t">about language modeling, which is something we are all familiar with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=69" target="_blank">00:01:09.180</a></span> | <span class="t">So imagine we want to train a language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=71" target="_blank">00:01:11.280</a></span> | <span class="t">How does the training of a language model work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=73" target="_blank">00:01:13.640</a></span> | <span class="t">Usually we have a sequence, so we want to teach the language model to predict the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=77" target="_blank">00:01:17.840</a></span> | <span class="t">token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=79" target="_blank">00:01:19.040</a></span> | <span class="t">So we have a sequence of tokens, so let's say this is our sequence of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=84" target="_blank">00:01:24.560</a></span> | <span class="t">So the first token is let's say "I", then the second token is "like", so I always pretend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=92" target="_blank">00:01:32.600</a></span> | <span class="t">like one token is a word and one word is a token, which is not actually the case, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=96" target="_blank">00:01:36.740</a></span> | <span class="t">for simplicity we will think like it is "I like to eat", let's just say "pizza", okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=108" target="_blank">00:01:48.600</a></span> | <span class="t">Imagine we want to train a language model to generate this exact phrase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=113" target="_blank">00:01:53.240</a></span> | <span class="t">What we do is basically we need a kind of model, which could be a transformer, but it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=117" target="_blank">00:01:57.740</a></span> | <span class="t">could be also a recurrent neural network, and we force it to predict the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=122" target="_blank">00:02:02.120</a></span> | <span class="t">So the job of sequence modeling means that we have some input sequence, and we will call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=127" target="_blank">00:02:07.920</a></span> | <span class="t">it the input, and we are trying to map it to some output, which is this one, this one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=139" target="_blank">00:02:19.680</a></span> | <span class="t">this one, this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=143" target="_blank">00:02:23.360</a></span> | <span class="t">The language modeling that we do is usually, the model that we train usually is called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=149" target="_blank">00:02:29.040</a></span> | <span class="t">an autoregressive language model, which means that it is, when it makes its prediction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=154" target="_blank">00:02:34.920</a></span> | <span class="t">it can use all the past words to choose, to predict what is the next word, which means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=160" target="_blank">00:02:40.680</a></span> | <span class="t">that the model, imagine the model should be able to predict exactly this, to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=167" target="_blank">00:02:47.280</a></span> | <span class="t">exactly this sentence, so the model, whenever it's fed, it's prompted with the word "I",</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=173" target="_blank">00:02:53.600</a></span> | <span class="t">it should output the word "like".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=178" target="_blank">00:02:58.480</a></span> | <span class="t">Whenever it's prompted with "I like", it should predict the word "to".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=183" target="_blank">00:03:03.160</a></span> | <span class="t">And whenever it's prompted with "I like to", it should predict "eat".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=191" target="_blank">00:03:11.380</a></span> | <span class="t">Whenever it's, etc, so as you can see a pattern here, right, and whenever it's prompted with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=197" target="_blank">00:03:17.320</a></span> | <span class="t">all the sentences, it should say "end of sentence", which means, okay, it's a special token that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=202" target="_blank">00:03:22.640</a></span> | <span class="t">says, okay, I'm done with the generation process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=206" target="_blank">00:03:26.120</a></span> | <span class="t">This is how we train language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=207" target="_blank">00:03:27.540</a></span> | <span class="t">So we take some sentence, which could be a document, which could be a web page, anything,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=211" target="_blank">00:03:31.880</a></span> | <span class="t">we shift the words by one position, and we force the language model to predict the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=216" target="_blank">00:03:36.040</a></span> | <span class="t">token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=217" target="_blank">00:03:37.040</a></span> | <span class="t">And there are two principal models to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=219" target="_blank">00:03:39.600</a></span> | <span class="t">One is called the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=220" target="_blank">00:03:40.720</a></span> | <span class="t">So let's say that in between here, we have something called the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=227" target="_blank">00:03:47.800</a></span> | <span class="t">The transformer basically allows us to do this language modeling in such a way, through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=232" target="_blank">00:03:52.360</a></span> | <span class="t">the attention mechanism, such a way that this language modeling, the output of the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=236" target="_blank">00:03:56.720</a></span> | <span class="t">model, which is used to compute the loss upon which the language model is trained, can be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=241" target="_blank">00:04:01.280</a></span> | <span class="t">done in parallel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=244" target="_blank">00:04:04.280</a></span> | <span class="t">Basically, this is also the reason most language models today are transformer based, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=249" target="_blank">00:04:09.840</a></span> | <span class="t">we want to leverage the GPUs, so if we can parallelize some operations, it's better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=255" target="_blank">00:04:15.700</a></span> | <span class="t">On the other hand, we also have recurrent neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=259" target="_blank">00:04:19.640</a></span> | <span class="t">Later we will see what are the problems with transformer and recurrent neural networks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=263" target="_blank">00:04:23.680</a></span> | <span class="t">so for now we just look at this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=265" target="_blank">00:04:25.340</a></span> | <span class="t">So the transformer can be parallelized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=267" target="_blank">00:04:27.920</a></span> | <span class="t">So this one is parallelizable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=276" target="_blank">00:04:36.280</a></span> | <span class="t">And then we have another paradigm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=277" target="_blank">00:04:37.920</a></span> | <span class="t">So let's call it, by the way, this is called the target sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=284" target="_blank">00:04:44.340</a></span> | <span class="t">So this one, when you train a model, this is called the target sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=288" target="_blank">00:04:48.640</a></span> | <span class="t">You compare what is the actual output of the transformer with what you want the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=293" target="_blank">00:04:53.320</a></span> | <span class="t">to output, which is the target, and you compute the loss, and then you back propagate, based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=300" target="_blank">00:05:00.560</a></span> | <span class="t">on the gradient, you back propagate to update the parameters of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=303" target="_blank">00:05:03.720</a></span> | <span class="t">So the model is forced to learn to generate the target given the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=308" target="_blank">00:05:08.480</a></span> | <span class="t">This is how we train models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=312" target="_blank">00:05:12.100</a></span> | <span class="t">We can take this one and replace the transformer with the recurrent neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=317" target="_blank">00:05:17.600</a></span> | <span class="t">And the problem with the recurrent neural network is that it's not parallelizable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=322" target="_blank">00:05:22.360</a></span> | <span class="t">At least not in its simple form.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=325" target="_blank">00:05:25.000</a></span> | <span class="t">Recently there are recurrent neural networks that also have like, by exploiting the parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=331" target="_blank">00:05:31.520</a></span> | <span class="t">scan, they can actually be parallelized, but up to now they are not used in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=339" target="_blank">00:05:39.640</a></span> | <span class="t">So recurrent neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=344" target="_blank">00:05:44.780</a></span> | <span class="t">So how do recurrent neural networks work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=347" target="_blank">00:05:47.480</a></span> | <span class="t">The transformer, I will not be talking about the attention mechanism, I suppose you already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=351" target="_blank">00:05:51.480</a></span> | <span class="t">know that, but it's not even important, you just need to remember that the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=355" target="_blank">00:05:55.040</a></span> | <span class="t">is parallelizable, and the recurrent neural network in its basic form is not parallelizable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=360" target="_blank">00:06:00.040</a></span> | <span class="t">So the recurrent neural networks work as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=363" target="_blank">00:06:03.440</a></span> | <span class="t">You feed, when you want to train them, or even when you want to inference them, you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=368" target="_blank">00:06:08.120</a></span> | <span class="t">because we are doing sequence modeling, and imagine we want to train a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=371" target="_blank">00:06:11.320</a></span> | <span class="t">to learn this exact sentence here, so I like to eat pizza, this sentence here, the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=378" target="_blank">00:06:18.600</a></span> | <span class="t">we train them is as follows, so we take the first token, so the word I, we feed it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=385" target="_blank">00:06:25.600</a></span> | <span class="t">the recurrent neural network, so let's call it recurrent RNN, the recurrent neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=396" target="_blank">00:06:36.800</a></span> | <span class="t">will produce an output, which is something we don't know, but we force it to learn the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=404" target="_blank">00:06:44.040</a></span> | <span class="t">target, so the target we want is, well, when it sees I, it should predict like, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=412" target="_blank">00:06:52.640</a></span> | <span class="t">So it should predict like, based on what it actually produces and what is the target,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=419" target="_blank">00:06:59.000</a></span> | <span class="t">we compute the loss, and we backpropagate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=421" target="_blank">00:07:01.640</a></span> | <span class="t">Then we take the last output, the recurrent neural network not only produces the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=428" target="_blank">00:07:08.160</a></span> | <span class="t">token, it also produces a state, which encapsulates information about all the input the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=434" target="_blank">00:07:14.920</a></span> | <span class="t">has seen so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=436" target="_blank">00:07:16.040</a></span> | <span class="t">This is called the hidden state of the recurrent neural network, or also the memory of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=439" target="_blank">00:07:19.720</a></span> | <span class="t">recurrent neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=443" target="_blank">00:07:23.980</a></span> | <span class="t">So we use this state to, again, feed another token to the RNN, so let me put the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=452" target="_blank">00:07:32.120</a></span> | <span class="t">below, actually, I think it's easier to visualize this way, so the input here was the word I.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=458" target="_blank">00:07:38.680</a></span> | <span class="t">Then this will produce a new hidden state, and this will, let's call it the hidden state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=463" target="_blank">00:07:43.440</a></span> | <span class="t">at time step one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=465" target="_blank">00:07:45.440</a></span> | <span class="t">We feed it again to the recurrent neural network along with a new input, the next token is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=472" target="_blank">00:07:52.440</a></span> | <span class="t">like, and the recurrent neural network will predict something, but we force it to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=478" target="_blank">00:07:58.240</a></span> | <span class="t">to predict the word to, for the second time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=484" target="_blank">00:08:04.260</a></span> | <span class="t">How can it predict the word to, just given the word like, well, by leveraging the recurrent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=490" target="_blank">00:08:10.000</a></span> | <span class="t">state from the previous time step, which encapsulates all the information about the word I.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=494" target="_blank">00:08:14.480</a></span> | <span class="t">That's how it can predict to by, actually, the recurrent neural network is seeing I like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=501" target="_blank">00:08:21.320</a></span> | <span class="t">like directly as the input, and I indirectly because it's in its hidden state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=507" target="_blank">00:08:27.480</a></span> | <span class="t">Now, we can do it also for the third token, so the to, hidden state to, this is also another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=516" target="_blank">00:08:36.120</a></span> | <span class="t">recurrent neural network, and we feed it the token to, which will produce some output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=520" target="_blank">00:08:40.800</a></span> | <span class="t">and we don't know what it is, but we force it to learn to predict the word it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=528" target="_blank">00:08:48.400</a></span> | <span class="t">And how can a model learn to predict it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=531" target="_blank">00:08:51.000</a></span> | <span class="t">Because it can see that the input is to, but it can also see the history of the input so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=535" target="_blank">00:08:55.080</a></span> | <span class="t">far through the hidden state h2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=538" target="_blank">00:08:58.760</a></span> | <span class="t">Now what is the problem here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=540" target="_blank">00:09:00.480</a></span> | <span class="t">When we use the transformer, the transformer can, to predict a particular token, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=546" target="_blank">00:09:06.280</a></span> | <span class="t">the token pizza, it can leverage all the previous input because it's fed all at the same time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=553" target="_blank">00:09:13.200</a></span> | <span class="t">to the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=555" target="_blank">00:09:15.360</a></span> | <span class="t">And this input during training is, are the keys and the values, and during inference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=561" target="_blank">00:09:21.160</a></span> | <span class="t">this is called the KVCache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=565" target="_blank">00:09:25.720</a></span> | <span class="t">So the transformer, in order to predict a particular token, can always see the entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=570" target="_blank">00:09:30.200</a></span> | <span class="t">sequence, and that's why it's parallelizable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=572" target="_blank">00:09:32.240</a></span> | <span class="t">So we feed the entire sequence to the transformer to predict each position, because we feed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=576" target="_blank">00:09:36.680</a></span> | <span class="t">the entire sequence, the transformer can see the entire sequence, and it can compute the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=581" target="_blank">00:09:41.640</a></span> | <span class="t">output at each position in parallel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=584" target="_blank">00:09:44.240</a></span> | <span class="t">However, with the recurrent neural network, we cannot compute the output at each position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=589" target="_blank">00:09:49.960</a></span> | <span class="t">in parallel, so we have to do it one step at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=593" target="_blank">00:09:53.360</a></span> | <span class="t">So it is not parallelizable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=596" target="_blank">00:09:56.480</a></span> | <span class="t">The advantage of the transformer is that it is parallelizable, so we can train massive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=600" target="_blank">00:10:00.600</a></span> | <span class="t">models by just increasing the number of GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=603" target="_blank">00:10:03.520</a></span> | <span class="t">The problem of the recurrent neural network, because it's not parallelizable, we are limited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=607" target="_blank">00:10:07.360</a></span> | <span class="t">because we have to do one, kind of a for loop to train them, so first we generate the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=612" target="_blank">00:10:12.760</a></span> | <span class="t">one, and then generate the second one, and then the third one, et cetera, et cetera,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=616" target="_blank">00:10:16.180</a></span> | <span class="t">and then we backpropagate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=618" target="_blank">00:10:18.920</a></span> | <span class="t">So and then there are other problems, like the vanishing gradients, et cetera, et cetera,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=622" target="_blank">00:10:22.880</a></span> | <span class="t">but that's not the main point today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=625" target="_blank">00:10:25.600</a></span> | <span class="t">So the problem of the recurrent neural networks are two, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=629" target="_blank">00:10:29.160</a></span> | <span class="t">First of all, it's not parallelizable, so this one is not parallelizable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=634" target="_blank">00:10:34.400</a></span> | <span class="t">And the second problem is that we have this recurrent state, this is called also the hidden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=640" target="_blank">00:10:40.240</a></span> | <span class="t">state of the recurrent neural network, which is fixed in size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=645" target="_blank">00:10:45.320</a></span> | <span class="t">So it can be as big as you want, it can be one megabyte, one gigabyte, whatever you like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=650" target="_blank">00:10:50.120</a></span> | <span class="t">but it's fixed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=651" target="_blank">00:10:51.120</a></span> | <span class="t">So once you have chosen your architecture, it's fixed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=653" target="_blank">00:10:53.680</a></span> | <span class="t">On the other hand, when we use a transformer model, the size of the input that the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=660" target="_blank">00:11:00.140</a></span> | <span class="t">model sees is growing, why?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=664" target="_blank">00:11:04.000</a></span> | <span class="t">Because when you use, for example, a prompt on chargeGPT, imagine you just feed the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=668" target="_blank">00:11:08.600</a></span> | <span class="t">two, imagine chargeGPT was trained exactly on this sentence here, and suppose you only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=673" target="_blank">00:11:13.680</a></span> | <span class="t">feed the first token, I, what chargeGPT will do, it will predict the first token using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=679" target="_blank">00:11:19.200</a></span> | <span class="t">only I, then it will take the word like, put it back into the input, feed it again all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=684" target="_blank">00:11:24.000</a></span> | <span class="t">to the transformer, so I like, and the transformer will predict this next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=689" target="_blank">00:11:29.280</a></span> | <span class="t">And then it will take the word to put it back into the input, so I like to put all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=695" target="_blank">00:11:35.000</a></span> | <span class="t">three tokens in the language model, to the transformer model, and then it will be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=699" target="_blank">00:11:39.000</a></span> | <span class="t">to predict the transposition, et cetera, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=701" target="_blank">00:11:41.440</a></span> | <span class="t">So the hidden state, so the memory of the transformer, so the stuff that we feed to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=706" target="_blank">00:11:46.200</a></span> | <span class="t">the transformer in order to predict the next token, is actually growing, and this is also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=712" target="_blank">00:11:52.240</a></span> | <span class="t">another problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=713" target="_blank">00:11:53.240</a></span> | <span class="t">So when doing very long sequence modeling, we need two things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=717" target="_blank">00:11:57.360</a></span> | <span class="t">First of all, we would like to be able for the language model to use all the input it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=723" target="_blank">00:12:03.780</a></span> | <span class="t">has seen so far, and that's something easily, that we can easily do with a transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=729" target="_blank">00:12:09.040</a></span> | <span class="t">however the problem is that with the transformer we have a growing memory, because we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=732" target="_blank">00:12:12.480</a></span> | <span class="t">to always put all the input in the transformer, all the tokens in the transformer, for it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=737" target="_blank">00:12:17.200</a></span> | <span class="t">to see all the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=739" target="_blank">00:12:19.720</a></span> | <span class="t">Or if we have limited memory, we can use a recurrent neural network, but they are not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=744" target="_blank">00:12:24.200</a></span> | <span class="t">parallelizable during training, and the second problem is that they have a fixed memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=749" target="_blank">00:12:29.720</a></span> | <span class="t">The fixed memory also has another problem, because it's fixed, we cannot choose what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=754" target="_blank">00:12:34.480</a></span> | <span class="t">is inside, so sometimes the language model may see some information, and sometimes it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=759" target="_blank">00:12:39.720</a></span> | <span class="t">will not be able to see some information, it's like you take one person and you ask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=766" target="_blank">00:12:46.480</a></span> | <span class="t">the person to memorize 3,000 books, I don't think the person will be able to do it, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=771" target="_blank">00:12:51.520</a></span> | <span class="t">our brain is fixed in size, and the same is the problem with recurrent neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=778" target="_blank">00:12:58.480</a></span> | <span class="t">Moreover, we have seen many architectures that are trying to improve this memorization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=786" target="_blank">00:13:06.840</a></span> | <span class="t">capability of the recurrent neural networks, for example Mamba, in which they use a particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=791" target="_blank">00:13:11.840</a></span> | <span class="t">shape of the matrix called the hypometrix, that allows to memorize information in a more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=797" target="_blank">00:13:17.440</a></span> | <span class="t">effective way, however in practice they don't work as well as we think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=803" target="_blank">00:13:23.560</a></span> | <span class="t">Now in this paper, they say, imagine, ok first of all, before we can talk about this paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=811" target="_blank">00:13:31.000</a></span> | <span class="t">how do we train language models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=812" target="_blank">00:13:32.560</a></span> | <span class="t">So how do we train language models is as follows, I mean, now let's talk about the architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=817" target="_blank">00:13:37.040</a></span> | <span class="t">level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=819" target="_blank">00:13:39.140</a></span> | <span class="t">So usually we have some tokens, so let's say some, let's call them input, let me do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=826" target="_blank">00:13:46.680</a></span> | <span class="t">vertically, I think it's easier, so we have some input tokens, we convert them into embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=839" target="_blank">00:13:59.860</a></span> | <span class="t">these embeddings are fed to a series of layers of transformers, so for example layer 1, layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=848" target="_blank">00:14:08.240</a></span> | <span class="t">2, etc, etc, until they produce some output, these are called the logits, logits, now what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=858" target="_blank">00:14:18.400</a></span> | <span class="t">happens with the transformer and with the recurrent neural network is as follows, with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=863" target="_blank">00:14:23.520</a></span> | <span class="t">the transformer we have a growing memory, so we have this thing called the kvcache that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=868" target="_blank">00:14:28.720</a></span> | <span class="t">contains all the past tokens, so the transformer can always leverage all the past tokens to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=873" target="_blank">00:14:33.980</a></span> | <span class="t">predict its next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=876" target="_blank">00:14:36.560</a></span> | <span class="t">In the recurrent neural network, we have a past memory that compresses all the past tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=882" target="_blank">00:14:42.840</a></span> | <span class="t">into a fixed size memory, that however has its own problem because sometimes the information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=889" target="_blank">00:14:49.360</a></span> | <span class="t">is lost because it's fixed and you're trying to squeeze in a lot of stuff, so we cannot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=894" target="_blank">00:14:54.040</a></span> | <span class="t">decide what is inside, we just hope that the network learns to keep the most important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=900" target="_blank">00:15:00.520</a></span> | <span class="t">information and forgets about the less important information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=907" target="_blank">00:15:07.560</a></span> | <span class="t">The problem is when we train a language model, we feed it a lot of data, so for example we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=913" target="_blank">00:15:13.360</a></span> | <span class="t">train the language model on the entire wikipedia, we train it on the entire web, and a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=918" target="_blank">00:15:18.960</a></span> | <span class="t">books, so the model has seen kind of all the possible data that exists in this world, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=928" target="_blank">00:15:28.600</a></span> | <span class="t">hope that when we have, imagine we have a model, a hybrid model, so a transformer but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=935" target="_blank">00:15:35.600</a></span> | <span class="t">with also a recurrent neural network, so imagine that this, suppose that this one here is an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=942" target="_blank">00:15:42.400</a></span> | <span class="t">attention layer, so a transformer layer, let's call it attention, and this one is a recurrent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=948" target="_blank">00:15:48.580</a></span> | <span class="t">neural network, and suppose that this is one of the new fancy recurrent networks that can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=953" target="_blank">00:15:53.480</a></span> | <span class="t">be parallelized actually, there are new architectures actually that can be parallelized, but still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=960" target="_blank">00:16:00.480</a></span> | <span class="t">the problem is that this information here, the RNN, will produce a memory that is fixed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=966" target="_blank">00:16:06.280</a></span> | <span class="t">in size, so if you feed 1000 tokens, this one will contain, will output a memory that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=972" target="_blank">00:16:12.520</a></span> | <span class="t">will be leveraged by the attention that will not be 1000 tokens, it will be less, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=978" target="_blank">00:16:18.600</a></span> | <span class="t">the goal of the RNN is to compress stuff into some fixed size memory that can be leveraged</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=986" target="_blank">00:16:26.140</a></span> | <span class="t">by the transformer model, which is this layer here, attention layer here, the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=991" target="_blank">00:16:31.720</a></span> | <span class="t">layer here however is very good at leveraging the data it is being fed, but this data is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=997" target="_blank">00:16:37.220</a></span> | <span class="t">not all the sequence because we have compressed it with the recurrent neural network, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1003" target="_blank">00:16:43.760</a></span> | <span class="t">we hope that the attention can leverage the information that was compressed by the recurrent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1008" target="_blank">00:16:48.160</a></span> | <span class="t">neural network to do its job of predicting the next token, if we do it this way, so imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1015" target="_blank">00:16:55.040</a></span> | <span class="t">we have this architecture which is a hybrid architecture of attention plus recurrent neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1019" target="_blank">00:16:59.240</a></span> | <span class="t">network, the problem with this architecture is that when you train it, because we do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1026" target="_blank">00:17:06.840</a></span> | <span class="t">with deep learning, we force the model to learn whatever target we have, it will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1032" target="_blank">00:17:12.000</a></span> | <span class="t">forced to learn this recurrent neural network to compress the information in such a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1037" target="_blank">00:17:17.920</a></span> | <span class="t">that the attention can use it, and the attention will be forced to extract whatever information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1043" target="_blank">00:17:23.200</a></span> | <span class="t">is in this compressed state made by the recurrent neural network, this is good, so when you train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1050" target="_blank">00:17:30.080</a></span> | <span class="t">it actually the loss decreases and you see that it performs quite well, however when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1054" target="_blank">00:17:34.520</a></span> | <span class="t">you use it in practice, the problem that you feed to the model may not be something that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1060" target="_blank">00:17:40.280</a></span> | <span class="t">the language model has seen in the past, so maybe we call this data out of distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1067" target="_blank">00:17:47.720</a></span> | <span class="t">so the model may not know how to compress it well, what to keep and what to not keep,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1072" target="_blank">00:17:52.940</a></span> | <span class="t">so in this case the recurrent neural network will fail at its task of compressing data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1078" target="_blank">00:17:58.640</a></span> | <span class="t">and because the data necessary to predict the next token was not compressed well, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1083" target="_blank">00:18:03.600</a></span> | <span class="t">attention layer will not be able to leverage this data to predict the next token, so at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1088" target="_blank">00:18:08.400</a></span> | <span class="t">training we see that this hybrid architecture works really fine, but at test time, so when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1093" target="_blank">00:18:13.480</a></span> | <span class="t">we use them, we actually see that they don't work quite well, and this is one of the reasons,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1098" target="_blank">00:18:18.480</a></span> | <span class="t">so they learn to compress the data, they have seen very well, so they know, ok, if I have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1104" target="_blank">00:18:24.040</a></span> | <span class="t">a long source code of Python, I should not concentrate on the, I don't know, some comments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1113" target="_blank">00:18:33.400</a></span> | <span class="t">that maybe are repetitive, but I should concentrate on the code, or maybe I should not, when I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1117" target="_blank">00:18:37.480</a></span> | <span class="t">see some C# code or C code, I should not concentrate on the, maybe the parentheses, because they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1125" target="_blank">00:18:45.780</a></span> | <span class="t">are just, how to say, redundant, but I should concentrate on the expressions, etc, etc,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1133" target="_blank">00:18:53.420</a></span> | <span class="t">so when it sees, so it actually learns to compress the information, but only the information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1142" target="_blank">00:19:02.780</a></span> | <span class="t">that it has seen at training time, now finally we can talk about the paper, so the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1149" target="_blank">00:19:09.300</a></span> | <span class="t">claim is, we have these models that need some kind of memory, in the transformer models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1156" target="_blank">00:19:16.100</a></span> | <span class="t">we have this KVCache, the problem with this KVCache, it's growing, so the problem with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1161" target="_blank">00:19:21.700</a></span> | <span class="t">the growing KVCache is that it requires a lot of memory, so actually most models are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1168" target="_blank">00:19:28.540</a></span> | <span class="t">not constrained, the fact that we cannot have a context window in the current models, very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1175" target="_blank">00:19:35.660</a></span> | <span class="t">big is because of the actually inference cost of this model, so they are really, really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1181" target="_blank">00:19:41.900</a></span> | <span class="t">expensive to inference, because we need to keep the KVCache, and the KVCache is one for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1187" target="_blank">00:19:47.860</a></span> | <span class="t">each layer, and the bigger models, they have a lot of layers, so you need to keep all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1191" target="_blank">00:19:51.460</a></span> | <span class="t">tokens for each of the layers of the model, for each token that you need to predict, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1198" target="_blank">00:19:58.060</a></span> | <span class="t">it's very expensive, and then the solution to have this infinite memory that keeps growing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1203" target="_blank">00:20:03.300</a></span> | <span class="t">is to have a compressed memory, but this compressed memory only works very well at training time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1208" target="_blank">00:20:08.100</a></span> | <span class="t">so the claim is, can we have a memory module that is trained at test time, and that's why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1215" target="_blank">00:20:15.860</a></span> | <span class="t">we are talking about learning to memorize at test time, that is effective at retrieval,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1222" target="_blank">00:20:22.900</a></span> | <span class="t">because the goal of the memory is to retrieve the information that is salient, that is needed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1226" target="_blank">00:20:26.620</a></span> | <span class="t">by the model, that is effective in retrieving the information that is being fed exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1233" target="_blank">00:20:33.780</a></span> | <span class="t">at test time, not only the one that it has seen at the training time, this is the problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1238" target="_blank">00:20:38.900</a></span> | <span class="t">that we are trying to solve with titans, now the way they do it is as follows, so they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1245" target="_blank">00:20:45.500</a></span> | <span class="t">say ok, imagine we have a module, imagine we have a module that we will call M, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1253" target="_blank">00:20:53.180</a></span> | <span class="t">this module let's think of it as a layer in a module, so ok let me draw actually, I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1260" target="_blank">00:21:00.540</a></span> | <span class="t">it's much easier if we can draw it, let's add a new paper, a new page, so ok, imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1269" target="_blank">00:21:09.540</a></span> | <span class="t">we have a very long sequence, we have seen that with the recurrent neural network the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1273" target="_blank">00:21:13.460</a></span> | <span class="t">job of the recurrent neural network is to compress this very long sequence so that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1277" target="_blank">00:21:17.500</a></span> | <span class="t">transformer can use it, let's do with titans now, how does it differ, and then we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1285" target="_blank">00:21:25.100</a></span> | <span class="t">check all the details, so we have this input, so let's go here again, so we have this input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1294" target="_blank">00:21:34.740</a></span> | <span class="t">we transform into embeddings, then we, I will draw a little differently and then later I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1303" target="_blank">00:21:43.380</a></span> | <span class="t">will explain why, we have some, suppose we have a hybrid architecture again of transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1308" target="_blank">00:21:48.460</a></span> | <span class="t">and recurrent layers, but I will not draw the recurrent layers, so this is the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1313" target="_blank">00:21:53.100</a></span> | <span class="t">layer of the, too big I think, let's call it L1, so the first layer with attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1320" target="_blank">00:22:00.020</a></span> | <span class="t">the second layer with attention, the third layer with attention, and then we have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1329" target="_blank">00:22:09.220</a></span> | <span class="t">output which is the logits, ok, I think now it's more visible right, ok, so imagine we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1339" target="_blank">00:22:19.140</a></span> | <span class="t">have another module in this architecture that we will call the memory module, let's call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1346" target="_blank">00:22:26.380</a></span> | <span class="t">it neural memory because this is how they call it here, so let's call it neural memory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1351" target="_blank">00:22:31.300</a></span> | <span class="t">and I will draw it as external module neural memory, now I want to show you how it would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1365" target="_blank">00:22:45.520</a></span> | <span class="t">work with the neural memory and then we check the detail on how it is actually trained,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1371" target="_blank">00:22:51.580</a></span> | <span class="t">so the way we usually train modules, so imagine, ok let's take a step back, how would we train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1377" target="_blank">00:22:57.700</a></span> | <span class="t">this module, we would feed it a sequence, imagine 1 million tokens, so imagine a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1384" target="_blank">00:23:04.020</a></span> | <span class="t">big sequence, so let's say 1 million tokens, you convert this sequence of tokens, 1 million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1389" target="_blank">00:23:09.780</a></span> | <span class="t">tokens into embeddings, you run these embeddings in the neural networks, recurrent neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1397" target="_blank">00:23:17.620</a></span> | <span class="t">which will compress this 1 million tokens maybe in let's say 1000 tokens because its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1402" target="_blank">00:23:22.060</a></span> | <span class="t">goal is to compress stuff right, so the sequence that is fed to the attention because the goal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1407" target="_blank">00:23:27.860</a></span> | <span class="t">of the problem of the attention is that it's quadratic, so having a smaller input results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1415" target="_blank">00:23:35.300</a></span> | <span class="t">in a better computation, so we feed this 1000 compressed token to the attention and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1421" target="_blank">00:23:41.900</a></span> | <span class="t">we force it to predict the next token only leveraging this 1000 compressed token, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1428" target="_blank">00:23:48.660</a></span> | <span class="t">we feed 1 million token but we force the attention layer to predict the next token only leveraging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1435" target="_blank">00:23:55.060</a></span> | <span class="t">much less information, so we hope that the recurrent neural network is good at choosing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1439" target="_blank">00:23:59.860</a></span> | <span class="t">the right tokens to keep and discarding the one that it doesn't keep, actually ok it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1445" target="_blank">00:24:05.900</a></span> | <span class="t">not really a token pruning mechanism, it's a token compression mechanism but ok you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1453" target="_blank">00:24:13.260</a></span> | <span class="t">think of it as a token pruning, like it's being fed 1 million tokens and it just keeps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1457" target="_blank">00:24:17.580</a></span> | <span class="t">the top 1000 that are the most important for predicting the next token, and this is done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1466" target="_blank">00:24:26.900</a></span> | <span class="t">at training time, so we feed this 1 million token at training time, we compute the output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1473" target="_blank">00:24:33.320</a></span> | <span class="t">we know what should be the next token because at training time we know what is the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1476" target="_blank">00:24:36.540</a></span> | <span class="t">token, we force, we compute the loss with respect to what we think should be the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1481" target="_blank">00:24:41.660</a></span> | <span class="t">token and then we back propagate to update the parameters of the model and we keep doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1485" target="_blank">00:24:45.940</a></span> | <span class="t">it for all the sequences that we have, with the titans it would work differently, imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1492" target="_blank">00:24:52.580</a></span> | <span class="t">you have 1 million token again and what you do is you do 2 steps, the first thing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1502" target="_blank">00:25:02.260</a></span> | <span class="t">we do, ok we have this input, we convert it into embeddings, the first thing we do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1509" target="_blank">00:25:09.260</a></span> | <span class="t">in the training loop, so imagine we are training this titans architecture, we first train this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1515" target="_blank">00:25:15.180</a></span> | <span class="t">neural module to learn to memorize our 1 million tokens and then we ask it to retrieve the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1525" target="_blank">00:25:25.000</a></span> | <span class="t">information necessary for predicting the next token and feed it to the attention layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1530" target="_blank">00:25:30.380</a></span> | <span class="t">so this is, let's call it attention layer, so this is an attention layer, this is an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1536" target="_blank">00:25:36.780</a></span> | <span class="t">attention layer and this is an attention layer, so look at the difference here, before we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1544" target="_blank">00:25:44.060</a></span> | <span class="t">had an input, we predicted the output, we compute the loss and we back propagate and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1550" target="_blank">00:25:50.660</a></span> | <span class="t">we update all the parameters of the model, here we will do something different, we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1554" target="_blank">00:25:54.980</a></span> | <span class="t">an input which is 1 million tokens, we convert them into embeddings, we train this module</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1562" target="_blank">00:26:02.380</a></span> | <span class="t">here which is separate and in the paper they refer to it as the inner loop of the training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1569" target="_blank">00:26:09.340</a></span> | <span class="t">we train this neural memory, and later we will see how we train it, with the sole purpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1575" target="_blank">00:26:15.180</a></span> | <span class="t">for this neural memory to learn everything about this data so that it can easily retrieve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1583" target="_blank">00:26:23.260</a></span> | <span class="t">this data when we will need it, so we take this 1 million tokens, we convert them into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1588" target="_blank">00:26:28.660</a></span> | <span class="t">embeddings, we train this neural memory in an inner loop, then we take this neural memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1597" target="_blank">00:26:37.620</a></span> | <span class="t">which has been trained to memorize this data and then we ask it to retrieve whatever information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1605" target="_blank">00:26:45.620</a></span> | <span class="t">is important from whatever it has seen, and use it as input for the attention layers here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1612" target="_blank">00:26:52.020</a></span> | <span class="t">so that the attention layers can leverage this compressed memory to produce the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1617" target="_blank">00:26:57.740</a></span> | <span class="t">and predict the next token, this not only at the training but also at test time, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1625" target="_blank">00:27:05.100</a></span> | <span class="t">when we use the attention with the hybrid architectures, for example attention plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1630" target="_blank">00:27:10.040</a></span> | <span class="t">recurrent neural networks at test time, so at inference time, what we have is usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1634" target="_blank">00:27:14.380</a></span> | <span class="t">a prompt, imagine this prompt is huge because you are asking chargeBD for example to analyze</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1639" target="_blank">00:27:19.420</a></span> | <span class="t">the entire github repository of a very big repository, what will happen is that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1646" target="_blank">00:27:26.820</a></span> | <span class="t">1 million token will be fed to the recurrent neural network which is fixed now, so we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1652" target="_blank">00:27:32.020</a></span> | <span class="t">using the model, so we are not changing its parameters anymore, the recurrent neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1658" target="_blank">00:27:38.460</a></span> | <span class="t">his job is to compress data, so it will compress these tokens into a smaller sequence that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1663" target="_blank">00:27:43.740</a></span> | <span class="t">we will fed to the attention layer and it will produce the output logits, however maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1670" target="_blank">00:27:50.500</a></span> | <span class="t">the information that we are feeding to this recurrent neural networks are kind of out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1675" target="_blank">00:27:55.260</a></span> | <span class="t">of distribution and the recurrent neural network has never seen something like this, and it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1679" target="_blank">00:27:59.620</a></span> | <span class="t">will do probably a very bad job at compressing this data, so because it will do a very bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1684" target="_blank">00:28:04.820</a></span> | <span class="t">job at compressing this data, because it doesn't know what to keep and what not to keep, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1689" target="_blank">00:28:09.220</a></span> | <span class="t">attention layer will not be able to leverage the most important information and then it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1693" target="_blank">00:28:13.620</a></span> | <span class="t">will not be able to predict the next token very well, so it will result in a bad output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1700" target="_blank">00:28:20.220</a></span> | <span class="t">and with titans even at test time, so even at inference time, we are actually training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1707" target="_blank">00:28:27.540</a></span> | <span class="t">a model, and now I show you how, imagine now we have again a github repository, and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1714" target="_blank">00:28:34.460</a></span> | <span class="t">very big and it results in 1 million tokens that we want the language model to analyze,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1719" target="_blank">00:28:39.660</a></span> | <span class="t">we convert it into embeddings, then we take this 1 million tokens, we train on the fly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1726" target="_blank">00:28:46.980</a></span> | <span class="t">this neural memory, whose job will be to just learn as much information as possible about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1733" target="_blank">00:28:53.660</a></span> | <span class="t">this 1 million tokens, retrieve the most salient information, because the neural memory's job</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1738" target="_blank">00:28:58.860</a></span> | <span class="t">is to compress information, so now after we have trained it in this inner loop, we retrieve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1744" target="_blank">00:29:04.260</a></span> | <span class="t">this information, we feed it to the attention layers, then the attention layers should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1749" target="_blank">00:29:09.260</a></span> | <span class="t">able to leverage the information retrieved by the neural memory, so with titans basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1758" target="_blank">00:29:18.940</a></span> | <span class="t">we don't just have a RNN, which is our memory that is trained at training time and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1767" target="_blank">00:29:27.660</a></span> | <span class="t">never trained again, and every time it sees something that it has never seen, it just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1771" target="_blank">00:29:31.340</a></span> | <span class="t">goes crazy, we have a neural memory that can be trained at inference time, on the fly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1779" target="_blank">00:29:39.860</a></span> | <span class="t">with the sole purpose of compressing stuff, and because we are training it at inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1784" target="_blank">00:29:44.740</a></span> | <span class="t">time, we hope that it will perform better even on data it has never seen, now according</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1792" target="_blank">00:29:52.620</a></span> | <span class="t">to the benchmark they published in the paper, but this actually happens in all papers, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1796" target="_blank">00:29:56.420</a></span> | <span class="t">you never trust the benchmarks, it looks like it is doing a good job, now let's look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1802" target="_blank">00:30:02.500</a></span> | <span class="t">the details, so I want to remind you, the problem we are solving is long context modeling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1807" target="_blank">00:30:07.780</a></span> | <span class="t">long context modeling has one issue, which is with the transformer it is very expensive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1812" target="_blank">00:30:12.860</a></span> | <span class="t">to inference for long context, with RNNs we have the problem that we train them on some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1819" target="_blank">00:30:19.780</a></span> | <span class="t">data, but when you use them on something that they have never seen, they don't know how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1823" target="_blank">00:30:23.260</a></span> | <span class="t">to compress and what to keep and what to not keep, so they go crazy, and because they go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1828" target="_blank">00:30:28.400</a></span> | <span class="t">crazy they don't do this job very well, the attention layers cannot leverage this information,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1833" target="_blank">00:30:33.820</a></span> | <span class="t">so they just result in very bad output, with the neural network memory we want to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1839" target="_blank">00:30:39.660</a></span> | <span class="t">on the fly a memory while inferencing the module, to just do the job of compressing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1846" target="_blank">00:30:46.220</a></span> | <span class="t">stuff on whatever data it is fed, now we can look at the details, ok, here they do some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1854" target="_blank">00:30:54.860</a></span> | <span class="t">preliminary, how to say, view of what is memory, what is linear attention, etc, etc, we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1862" target="_blank">00:31:02.220</a></span> | <span class="t">care about that for now, they say ok, imagine we have a memory module that only has two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1867" target="_blank">00:31:07.860</a></span> | <span class="t">operations, one is the write operation and one is the read operation, we want to write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1874" target="_blank">00:31:14.700</a></span> | <span class="t">and read at inference time and also at training time to this memory, how do we train this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1881" target="_blank">00:31:21.820</a></span> | <span class="t">memory, first of all this memory, neural memory, is a neural network by itself, meaning that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1888" target="_blank">00:31:28.920</a></span> | <span class="t">you can think of it as an external neural network that is separated from the rest of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1895" target="_blank">00:31:35.540</a></span> | <span class="t">the architecture, that will use this neural memory, so you need to think that you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1904" target="_blank">00:31:44.140</a></span> | <span class="t">like a transformer module that is leveraging this neural memory, now how to train this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1911" target="_blank">00:31:51.100</a></span> | <span class="t">neural memory at inference time, because that's our problem, at training time we know how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1915" target="_blank">00:31:55.540</a></span> | <span class="t">to do it, we just put the input, compute the output, back propagate, and voila, how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1922" target="_blank">00:32:02.460</a></span> | <span class="t">do that at inference time, it's what they see here, they say ok, imagine we have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1927" target="_blank">00:32:07.580</a></span> | <span class="t">memory, first of all, how we want to update its information, they want to update its information,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1935" target="_blank">00:32:15.820</a></span> | <span class="t">ok, another step back, what we want this memory to do, we want this memory to learn, to extract</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1944" target="_blank">00:32:24.220</a></span> | <span class="t">information about whatever they should memorize, and for that they use a very particular law,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1950" target="_blank">00:32:30.980</a></span> | <span class="t">which is kind of the reconstruction law, so imagine we have this memory, if we ask it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1957" target="_blank">00:32:37.780</a></span> | <span class="t">to memorize, ok imagine we have an input sequence, let's call it x, this xt here, we project</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1968" target="_blank">00:32:48.380</a></span> | <span class="t">it with two linear projections called wk and wv, which are basically the same equivalent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1974" target="_blank">00:32:54.040</a></span> | <span class="t">of the one that we use in the attention mechanism, how can this memory do its job very well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1984" target="_blank">00:33:04.300</a></span> | <span class="t">only if it learns to recreate the data it has seen, and this is the loss that you see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=1992" target="_blank">00:33:12.740</a></span> | <span class="t">here, this is just the L2 loss that you can see here, which basically it learns to memorize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2000" target="_blank">00:33:20.180</a></span> | <span class="t">the mapping between a projection called key and a projection called v of the same data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2007" target="_blank">00:33:27.320</a></span> | <span class="t">so it kind of learns to recreate the same data, this is the job of the memory, so if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2014" target="_blank">00:33:34.460</a></span> | <span class="t">I put some stuff I should be able to retrieve the same stuff, so I should be able to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2019" target="_blank">00:33:39.660</a></span> | <span class="t">as much as possible from the stuff that I put inside, how to train it, how to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2027" target="_blank">00:33:47.420</a></span> | <span class="t">it is they say ok I have this memory, I want to update this memory by using kind of a gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2036" target="_blank">00:33:56.260</a></span> | <span class="t">descent, so how gradient descent works, imagine we have an neural network, the basic version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2043" target="_blank">00:34:03.780</a></span> | <span class="t">of gradient descent work as follows, so we have a neural network with some parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2051" target="_blank">00:34:11.500</a></span> | <span class="t">let's call them theta, so let's say theta, the parameters theta at time i, so at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2060" target="_blank">00:34:20.000</a></span> | <span class="t">step i of the training, are updated with the previous parameters of the model, so at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2066" target="_blank">00:34:26.940</a></span> | <span class="t">previous time, minus a learning rate that we will call gamma, multiplied by the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2074" target="_blank">00:34:34.740</a></span> | <span class="t">of the loss with respect to the parameters of the model, the gradient tells us how we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2083" target="_blank">00:34:43.180</a></span> | <span class="t">should change the parameters in order to maximize a loss, but we move against the direction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2091" target="_blank">00:34:51.940</a></span> | <span class="t">of this gradient and that's why you see a sign minus, so we update the parameters in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2098" target="_blank">00:34:58.420</a></span> | <span class="t">the direction opposite to the one that would maximize the loss, so we update the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2105" target="_blank">00:35:05.300</a></span> | <span class="t">to reduce the loss, and this is what we do here, we say we want to update our memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2112" target="_blank">00:35:12.260</a></span> | <span class="t">in such a way such that we minimize this loss here, which is the memorization loss, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2119" target="_blank">00:35:19.940</a></span> | <span class="t">is the reconstruction loss that we saw before, so a loss that tells if I ask the memory to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2126" target="_blank">00:35:26.580</a></span> | <span class="t">retrieve some information, which is the key projection of the data, it should recreate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2132" target="_blank">00:35:32.540</a></span> | <span class="t">this data, and this memory, in the paper, they model it as a linear layer, so a linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2143" target="_blank">00:35:43.660</a></span> | <span class="t">layer is just a matrix multiplication with a weight matrix, so this memory module, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2148" target="_blank">00:35:48.260</a></span> | <span class="t">m here, is nothing more than just a weight matrix of a linear layer, so we are modifying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2157" target="_blank">00:35:57.820</a></span> | <span class="t">this weight matrix, so the neural memory is just a matrix, w, we are modifying this w</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2166" target="_blank">00:36:06.900</a></span> | <span class="t">in such a way that it reduces the reconstruction loss of the data, just the way we train a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2177" target="_blank">00:36:17.540</a></span> | <span class="t">neural network, so we train the neural network with parameters to reduce a loss, and these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2183" target="_blank">00:36:23.620</a></span> | <span class="t">parameters are calculated in such a way that they will result in the smallest loss possible,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2189" target="_blank">00:36:29.980</a></span> | <span class="t">in the same way we are updating this w matrix, which will be our memory, in such a way that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2197" target="_blank">00:36:37.220</a></span> | <span class="t">it will result in the minimum loss information possible, because that's the loss against</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2202" target="_blank">00:36:42.780</a></span> | <span class="t">which we are optimizing it, which is the reconstruction loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2208" target="_blank">00:36:48.860</a></span> | <span class="t">And they call it the surprise, so this gradient of the w matrix, which is our memory, with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2216" target="_blank">00:36:56.940</a></span> | <span class="t">respect to the gradient of the loss, with respect to the w of this memory, they call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2223" target="_blank">00:37:03.820</a></span> | <span class="t">it the surprise, because the bigger the loss, the bigger difficulty the model had in reconstructing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2232" target="_blank">00:37:12.380</a></span> | <span class="t">its data, so it means that the model is surprised to see this data, so that's why they call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2239" target="_blank">00:37:19.900</a></span> | <span class="t">it surprise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2242" target="_blank">00:37:22.460</a></span> | <span class="t">If you have ever studied how optimizers work, you will remember that in deep learning we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2250" target="_blank">00:37:30.420</a></span> | <span class="t">have this thing called momentum, so usually we don't update the parameters of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2256" target="_blank">00:37:36.060</a></span> | <span class="t">naively like this, because, for example, sometimes we want to retain the... we want to... first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2264" target="_blank">00:37:44.420</a></span> | <span class="t">of all, we don't want the... okay, first of all, the loss is computed with mini-batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2269" target="_blank">00:37:49.780</a></span> | <span class="t">gradient descent, and it means that we don't compute it over all the input data set, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2278" target="_blank">00:37:58.420</a></span> | <span class="t">over instances of data, so like a small batch of data, and the direction of this gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2285" target="_blank">00:38:05.540</a></span> | <span class="t">is actually stochastic, which means that it is not the true direction of the gradient,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2291" target="_blank">00:38:11.380</a></span> | <span class="t">which means that it oscillates from what it... it oscillates, so it is not indicating the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2298" target="_blank">00:38:18.940</a></span> | <span class="t">true direction, imagine the true direction of the gradient is here, but if we train it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2302" target="_blank">00:38:22.840</a></span> | <span class="t">on the first batch, maybe it's in this direction, maybe on the next batch it's in this direction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2308" target="_blank">00:38:28.780</a></span> | <span class="t">maybe on the next batch on this direction, etc, on average it will point to the correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2312" target="_blank">00:38:32.500</a></span> | <span class="t">direction of the gradient, but it will be noisy in each step, because we don't want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2317" target="_blank">00:38:37.060</a></span> | <span class="t">to take steps too confidently in each step of training, we add this momentum term, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2325" target="_blank">00:38:45.240</a></span> | <span class="t">the momentum term basically kind of creates an exponentially moving average of all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2331" target="_blank">00:38:51.860</a></span> | <span class="t">gradients, so that we also keep some information about the past gradient that we have computed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2338" target="_blank">00:38:58.140</a></span> | <span class="t">to smooth out the change of the weights, so that we don't take too much, so it's not like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2344" target="_blank">00:39:04.700</a></span> | <span class="t">we don't weight each step in the same way, and the idea for them to introduce the surprise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2353" target="_blank">00:39:13.740</a></span> | <span class="t">is as follows, they said ok, if I train my memory to recreate the data, then it can miss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2368" target="_blank">00:39:28.100</a></span> | <span class="t">this new data after it sees some novel data, so maybe there is some new data that the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2377" target="_blank">00:39:37.460</a></span> | <span class="t">should memorize, but the gradient kind of disappears after a while, so the model will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2382" target="_blank">00:39:42.980</a></span> | <span class="t">miss it, so in order to avoid this mechanism, they use the momentum, just like we do when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2389" target="_blank">00:39:49.060</a></span> | <span class="t">doing model training, and they call it the past surprise, and this past surprise is nothing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2397" target="_blank">00:39:57.020</a></span> | <span class="t">more than the term past gradient in the optimizers that we use, for example the Adamo optimizer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2408" target="_blank">00:40:08.420</a></span> | <span class="t">and then the momentary surprise, which is the gradient with respect to the current input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2413" target="_blank">00:40:13.420</a></span> | <span class="t">so rehearse what we have said so far, we have this memory, which is just a w matrix, that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2419" target="_blank">00:40:19.500</a></span> | <span class="t">we want to optimize in such a way, so we want to change this w continuously, with every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2428" target="_blank">00:40:28.700</a></span> | <span class="t">token that we receive, in such a way that it encapsulates all the information that it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2434" target="_blank">00:40:34.380</a></span> | <span class="t">are in this input, and we can, how do we know it captures all the information in this input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2442" target="_blank">00:40:42.780</a></span> | <span class="t">because we ask it to minimize the loss, the reconstruction loss of the input, now the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2451" target="_blank">00:40:51.180</a></span> | <span class="t">problem is we don't want to do this training of this novel model just during training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2458" target="_blank">00:40:58.420</a></span> | <span class="t">but we also want to do it during inference time, because if we do it only during training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2462" target="_blank">00:41:02.860</a></span> | <span class="t">what happens is that during inference time, every time it will see some new information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2467" target="_blank">00:41:07.240</a></span> | <span class="t">that it has never seen, probably it will do a bad job at compressing, so it will not work,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2471" target="_blank">00:41:11.420</a></span> | <span class="t">so how to do that at inference time, what we will do practically is as follows, so at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2478" target="_blank">00:41:18.180</a></span> | <span class="t">inference time, imagine we have inputs, so the first input, let me write all these formulas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2486" target="_blank">00:41:26.100</a></span> | <span class="t">actually so that we can refer to them, here, this one, and I paste it here, and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2499" target="_blank">00:41:39.460</a></span> | <span class="t">also copy the loss, this one, ok, let's learn how it would work at inference time, imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2511" target="_blank">00:41:51.540</a></span> | <span class="t">we have 1 million tokens, and ok, actually no, imagine we want to generate a lot of tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2518" target="_blank">00:41:58.140</a></span> | <span class="t">and we start with one token only, so the prompt is only one token, what will happen is we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2523" target="_blank">00:42:03.100</a></span> | <span class="t">have this one token, so let's call it one token, we feed it to the model as input, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2534" target="_blank">00:42:14.460</a></span> | <span class="t">will be converted into embeddings, which will be only one embedding, and we want to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2539" target="_blank">00:42:19.580</a></span> | <span class="t">our neural memory on this one single token, so it should learn to recreate this one single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2545" target="_blank">00:42:25.580</a></span> | <span class="t">token, how we will do that in practice, we take the memory, first of all we take this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2554" target="_blank">00:42:34.120</a></span> | <span class="t">one embedding and we project it into key and value by doing a matrix multiplication of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2559" target="_blank">00:42:39.500</a></span> | <span class="t">this single token with a matrix called WK and another called WB, then we compute this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2570" target="_blank">00:42:50.220</a></span> | <span class="t">this is called the retrieval of the memory, and the retrieval, because the memory is modeled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2575" target="_blank">00:42:55.060</a></span> | <span class="t">only as a W matrix of a linear layer, the retrieval of the information from this memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2580" target="_blank">00:43:00.740</a></span> | <span class="t">will just be W multiplied by the input, and the input actually they call it QT, so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2587" target="_blank">00:43:07.900</a></span> | <span class="t">another projection of the input to the WQ matrix, so this KT comes from WK multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2595" target="_blank">00:43:15.700</a></span> | <span class="t">by X, and this VT comes from WB multiplied by X, and this QT comes from WQ multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2606" target="_blank">00:43:26.700</a></span> | <span class="t">by X, this W here is the W of the memory, so this is the memory parameters, and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2613" target="_blank">00:43:33.780</a></span> | <span class="t">is the memory, so it's the parameters of the memory, but it is also the memory itself,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2619" target="_blank">00:43:39.340</a></span> | <span class="t">we want to update this W, ok, so how to do that, so we project the information of a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2625" target="_blank">00:43:45.820</a></span> | <span class="t">token with WV, we project it with WK, we compute this term here which is just W multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2634" target="_blank">00:43:54.820</a></span> | <span class="t">by this term here, we compute this loss here, and we compute its gradient, the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2642" target="_blank">00:44:02.900</a></span> | <span class="t">of this loss can be computed with the following formula, they actually specify, I can show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2648" target="_blank">00:44:08.980</a></span> | <span class="t">you also how to derive it actually, so there is a formula here for the gradient, this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2654" target="_blank">00:44:14.980</a></span> | <span class="t">how we compute the gradient of the loss, how to compute this formula, well, how to derive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2663" target="_blank">00:44:23.020</a></span> | <span class="t">it, let's talk about it, but ok, ok, one second, so they compute the gradient of this loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2672" target="_blank">00:44:32.500</a></span> | <span class="t">with respect to the parameters of the model, what are the parameters of the model? W, ok,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2678" target="_blank">00:44:38.140</a></span> | <span class="t">so they compute the gradient of the loss of this loss with respect to W, and then we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2683" target="_blank">00:44:43.300</a></span> | <span class="t">to update W, how to update W? we need to compute this ST term here, this ST term results in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2691" target="_blank">00:44:51.140</a></span> | <span class="t">the pass surprise, but we don't have any pass surprise, so let's suppose this is 0 for now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2695" target="_blank">00:44:55.740</a></span> | <span class="t">multiplied by a learning rate, multiplied by this theta, theta t is the learning rate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2705" target="_blank">00:45:05.780</a></span> | <span class="t">multiplied by this gradient that we have computed, and then we update this W using this term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2711" target="_blank">00:45:11.980</a></span> | <span class="t">ST, now we have updated our memory, then we retrieve information from this memory, how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2717" target="_blank">00:45:17.860</a></span> | <span class="t">to retrieve information from this memory? we just take this W and we multiply it by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2721" target="_blank">00:45:21.820</a></span> | <span class="t">the, we take X, so our single token, we project it with another matrix called WQ, so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2729" target="_blank">00:45:29.700</a></span> | <span class="t">it becomes a QT, we multiply it by W, and now we retrieve information, this information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2735" target="_blank">00:45:35.940</a></span> | <span class="t">is then sent to the first layer of the model, as compressed past information, and then to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2743" target="_blank">00:45:43.460</a></span> | <span class="t">the second, to the third, etc, etc, to predict the output, the model will produce the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2749" target="_blank">00:45:49.140</a></span> | <span class="t">output token, then usually we put this output token back into the prompt, to generate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2755" target="_blank">00:45:55.620</a></span> | <span class="t">next token, here, because we are not talking about just a transformer model, we are talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2760" target="_blank">00:46:00.580</a></span> | <span class="t">about a hybrid architecture that has attention layers plus neural memory, we need to update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2766" target="_blank">00:46:06.100</a></span> | <span class="t">our neural memory with this new incoming token, so this new incoming token will again be used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2773" target="_blank">00:46:13.220</a></span> | <span class="t">to update the memory, the memory will be updated with the information of the new token, it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2778" target="_blank">00:46:18.660</a></span> | <span class="t">will not be replaced with only this new token, so we hope that the new memory will encapsulate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2784" target="_blank">00:46:24.740</a></span> | <span class="t">information about the first token that we fed before and the current token, what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2790" target="_blank">00:46:30.660</a></span> | <span class="t">will do practically, we will take this new token that was output by the model, we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2795" target="_blank">00:46:35.540</a></span> | <span class="t">project it through WV, and it will become VT, we will project it through WK and it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2800" target="_blank">00:46:40.860</a></span> | <span class="t">become KT, we compute this loss term, we compute the gradient of this loss, and we update our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2809" target="_blank">00:46:49.300</a></span> | <span class="t">neural memory like before, but we have the past surprise this time, so, because we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2814" target="_blank">00:46:54.900</a></span> | <span class="t">not just, and we also have the previous memory, so we are updating this W, and hopefully this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2820" target="_blank">00:47:00.500</a></span> | <span class="t">will contain information about the token number 2 and the token number 1 that we fed before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2826" target="_blank">00:47:06.580</a></span> | <span class="t">now as you can see, because we are training the neural memory at test time, because now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2831" target="_blank">00:47:11.540</a></span> | <span class="t">we are inferencing the model, we hope that it will perform better than a neural memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2836" target="_blank">00:47:16.740</a></span> | <span class="t">that has only been trained at training time, because at each step of this update, the neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2848" target="_blank">00:47:28.100</a></span> | <span class="t">memory is actually trying to minimize the loss against this particular data, not only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2853" target="_blank">00:47:33.380</a></span> | <span class="t">the data that it has seen during training, but only exactly on this particular data that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2857" target="_blank">00:47:37.700</a></span> | <span class="t">is seen exactly in this moment, I know that I fed you with a lot of information, but I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2863" target="_blank">00:47:43.500</a></span> | <span class="t">hope now it should be a little more clear on practically what it means to have an inner</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2868" target="_blank">00:47:48.800</a></span> | <span class="t">loop and an outer loop, so when we train the model, we update the parameters of this big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2875" target="_blank">00:47:55.700</a></span> | <span class="t">model to leverage whatever the memory creates, and the memory does not learn to compress</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2884" target="_blank">00:48:04.420</a></span> | <span class="t">information only at training time, but also at inference time, exactly on the data that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2889" target="_blank">00:48:09.780</a></span> | <span class="t">you feed it at inference time. Now let's talk about the problems of this memory, so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2895" target="_blank">00:48:15.180</a></span> | <span class="t">problem of this memory is that every time, as you can see, every time we need to run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2899" target="_blank">00:48:19.060</a></span> | <span class="t">a gradient descent on each single token, so this looks like it takes, you need to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2905" target="_blank">00:48:25.860</a></span> | <span class="t">the model, you have a very big list of tokens and you want to train it as fast as possible,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2912" target="_blank">00:48:32.900</a></span> | <span class="t">but if you need to update the memory one token at a time, it's very slow, but fortunately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2917" target="_blank">00:48:37.560</a></span> | <span class="t">in the paper they also propose an algorithm to parallelize this training, and this training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2925" target="_blank">00:48:45.980</a></span> | <span class="t">can be parallelized actually not on the full sequence, but only chunk by chunk, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2931" target="_blank">00:48:51.120</a></span> | <span class="t">still better than doing one token at a time, so imagine you have one million tokens, if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2936" target="_blank">00:48:56.020</a></span> | <span class="t">we cannot parallelize it, it means ok, first take the first token, update the memory, then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2940" target="_blank">00:49:00.880</a></span> | <span class="t">take the second token, update the memory, then third token, update the memory, so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2943" target="_blank">00:49:03.980</a></span> | <span class="t">need to do one million times this and we cannot exploit our GPUs because we have to do one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2950" target="_blank">00:49:10.100</a></span> | <span class="t">operation at a time, what they propose in the paper is a hybrid algorithm, so it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2957" target="_blank">00:49:17.020</a></span> | <span class="t">fully parallelizable on this entire sequence, but chunk by chunk, which is a good compromise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2962" target="_blank">00:49:22.060</a></span> | <span class="t">it means that if you choose, imagine you have one million tokens and you choose a chunk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2967" target="_blank">00:49:27.260</a></span> | <span class="t">size of let's say 1,000, you can parallelize the first 1,000 tokens, then you take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2977" target="_blank">00:49:37.380</a></span> | <span class="t">next 1,000 token and you parallelize this one, so in total you will compute 1,000 steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2982" target="_blank">00:49:42.720</a></span> | <span class="t">and not one million steps, if you choose a chunk size of 1,000 over a sequence length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2988" target="_blank">00:49:48.060</a></span> | <span class="t">of one million, they also say ok how to leverage this neural memory module, you can use it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2994" target="_blank">00:49:54.860</a></span> | <span class="t">as a contextual memory, means that if you have a hybrid architecture in which you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=2998" target="_blank">00:49:58.700</a></span> | <span class="t">attention and this neural memory, so the one like the one we draw before, what we can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3006" target="_blank">00:50:06.820</a></span> | <span class="t">is we take the sequence that is input by the user, because the neural memory, it's job</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3012" target="_blank">00:50:12.700</a></span> | <span class="t">of the neural memory is just to compress information, we retrieve whatever is in the memory, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3018" target="_blank">00:50:18.660</a></span> | <span class="t">append it to the sequence, prepend it to the sequence, along with some other persistent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3025" target="_blank">00:50:25.340</a></span> | <span class="t">ok we can even not talk about the persistent memory tokens because I believe they just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3029" target="_blank">00:50:29.260</a></span> | <span class="t">overdid all this stuff, I mean this system could work even without the persistent memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3034" target="_blank">00:50:34.820</a></span> | <span class="t">tokens, so we take our sequence, we prepend whatever information is in the memory, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3043" target="_blank">00:50:43.780</a></span> | <span class="t">feed it to the attention module and we use the output of the attention to update the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3048" target="_blank">00:50:48.300</a></span> | <span class="t">memory and to produce the output, so let's go to our architecture, in this case basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3057" target="_blank">00:50:57.500</a></span> | <span class="t">it would mean, imagine we have fed already 10 tokens to this memory and now we are trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3064" target="_blank">00:51:04.220</a></span> | <span class="t">to predict the 11th token, what it would mean is that I would take this 11th token, I would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3071" target="_blank">00:51:11.980</a></span> | <span class="t">input, convert it into embeddings, I would retrieve whatever is inside the neural memory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3079" target="_blank">00:51:19.660</a></span> | <span class="t">so imagine the neural memory gives me, because it's job is compressing right, even if I fed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3084" target="_blank">00:51:24.100</a></span> | <span class="t">it 10 tokens, it doesn't have to return me 10 tokens, it has to return me a compressed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3087" target="_blank">00:51:27.660</a></span> | <span class="t">version of these 10 tokens, suppose the ratio is like, suppose that the compressed state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3093" target="_blank">00:51:33.620</a></span> | <span class="t">is 5 tokens, so I would take these 5 tokens, prepend it to my single token, it would become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3099" target="_blank">00:51:39.020</a></span> | <span class="t">6 tokens, I fed it to the first attention layer, take the output of the attention, update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3104" target="_blank">00:51:44.060</a></span> | <span class="t">it and combine it with the output of the attention to get the output of this layer and feed it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3110" target="_blank">00:51:50.020</a></span> | <span class="t">to the next one, this is the neural memory as context usage, the other usage is memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3118" target="_blank">00:51:58.460</a></span> | <span class="t">as gate, which is this architecture here, so in this case I have our 11th token, don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3129" target="_blank">00:52:09.220</a></span> | <span class="t">think about persistent memory, I believe, it's just an overdoing, you don't have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3137" target="_blank">00:52:17.780</a></span> | <span class="t">use persistent memory to make this mechanism work, they take this 11th token, they put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3145" target="_blank">00:52:25.180</a></span> | <span class="t">it in the memory, so now we update first the memory, and they also feed it to the attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3152" target="_blank">00:52:32.260</a></span> | <span class="t">and then they combine the output of the neural memory, which contains 11 tokens, but when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3157" target="_blank">00:52:37.620</a></span> | <span class="t">we retrieve it only gives us 5 tokens, and then the output of the attention, which we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3161" target="_blank">00:52:41.580</a></span> | <span class="t">only fed 1 token, and it's combined to produce the output, or you can only use the memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3167" target="_blank">00:52:47.260</a></span> | <span class="t">as a module without any attention, which means that basically you skip all this part, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3173" target="_blank">00:52:53.460</a></span> | <span class="t">you take your input, which could be 1 token, 1 million token, whatever, you update the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3178" target="_blank">00:52:58.300</a></span> | <span class="t">memory continuously, you take the compressed version of the memory, and you feed it directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3183" target="_blank">00:53:03.020</a></span> | <span class="t">to the linear layer that will produce the logits, this is what they refer to as memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3188" target="_blank">00:53:08.780</a></span> | <span class="t">as layer, honestly you can create 1 million variants of this architecture, the point is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3195" target="_blank">00:53:15.660</a></span> | <span class="t">not how you use it, the point is how it works, so I want to punctualize how it works, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3202" target="_blank">00:53:22.300</a></span> | <span class="t">we are training a module at test time, which is different from what we do with recurrent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3209" target="_blank">00:53:29.140</a></span> | <span class="t">neural networks, so recurrent neural networks are trained at training time, and their job</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3213" target="_blank">00:53:33.860</a></span> | <span class="t">is to compress data, but because they do very well the job of compressing the data they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3221" target="_blank">00:53:41.500</a></span> | <span class="t">have seen, they may not function very well during inference, because they may see some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3227" target="_blank">00:53:47.240</a></span> | <span class="t">data that they have never seen, however, by having a memory like this that you can train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3231" target="_blank">00:53:51.900</a></span> | <span class="t">at inference time, and with an algorithm that is supposedly parallelizable, we can avoid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3239" target="_blank">00:53:59.180</a></span> | <span class="t">hopefully this problem, because the only job of the memory is to be able to retrieve, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3244" target="_blank">00:54:04.860</a></span> | <span class="t">I actually like this paper because I believe that it's a novel idea that I didn't think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3250" target="_blank">00:54:10.580</a></span> | <span class="t">about before, and I think it's ok, this is part of a bigger, actually ok, I've been researching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3258" target="_blank">00:54:18.140</a></span> | <span class="t">a little bit about this area for a while, it's called test time training, but this particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3265" target="_blank">00:54:25.020</a></span> | <span class="t">architecture was a little bit innovative in this field, what else do we need to know to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3274" target="_blank">00:54:34.380</a></span> | <span class="t">read this paper, I think now you should have the information to read this paper, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3279" target="_blank">00:54:39.660</a></span> | <span class="t">we have talked about how to update this memory, and what is this memory, this memory is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3284" target="_blank">00:54:44.180</a></span> | <span class="t">a linear layer, in the paper they also say that ok, this memory doesn't have to be just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3289" target="_blank">00:54:49.220</a></span> | <span class="t">a linear layer, it can be a multilayer perceptron, so it can be for example two layers with an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3294" target="_blank">00:54:54.500</a></span> | <span class="t">activation in between, and it will work in the same way, and the algorithm that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3298" target="_blank">00:54:58.620</a></span> | <span class="t">have devised that is parallelizable would work also with this multilayer memory, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3307" target="_blank">00:55:07.820</a></span> | <span class="t">didn't talk about persistent memory, but the persistent memory are just tokens that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3311" target="_blank">00:55:11.580</a></span> | <span class="t">are prepended to the input, and they don't belong to the neural memory, they belong to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3316" target="_blank">00:55:16.980</a></span> | <span class="t">the outer loop as they call it here, the outer loop is just this model here, and this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3325" target="_blank">00:55:25.820</a></span> | <span class="t">the inner loop, but ok, this system can work without persistent tokens, this is my claim,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3334" target="_blank">00:55:34.260</a></span> | <span class="t">if you look at the benchmark, it looks like that compared to the other architectures that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3338" target="_blank">00:55:38.260</a></span> | <span class="t">are like Mamba and the current neural networks, it performs better, if you check the average</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3346" target="_blank">00:55:46.140</a></span> | <span class="t">score over all these benchmarks, I believe ok, this is a promising area of research,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3352" target="_blank">00:55:52.100</a></span> | <span class="t">I will probably be looking forward to the code which has not been released yet, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3357" target="_blank">00:55:57.940</a></span> | <span class="t">thank you guys for spending time with me, I hope I gave you enough at least intuitions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3362" target="_blank">00:56:02.760</a></span> | <span class="t">into how it is happening, and I'm also really eager to look at the code, because I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3367" target="_blank">00:56:07.020</a></span> | <span class="t">the best way to learn about a new architecture is actually to look at the code, so have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3371" target="_blank">00:56:11.740</a></span> | <span class="t">good night!</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=A6kPQVejN4o&t=3372" target="_blank">00:56:12.260</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>