<html><head><title>What a GPT-7 Intelligence Explosion Looks Like | Carl Shulman</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>What a GPT-7 Intelligence Explosion Looks Like | Carl Shulman</h2><a href="https://www.youtube.com/watch?v=D_WsPbdJKUg" target="_blank"><img src="https://i.ytimg.com/vi_webp/D_WsPbdJKUg/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=D_WsPbdJKUg&t=0 target="_blank"">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=D_WsPbdJKUg&t=45 target="_blank"">0:45</a> AI Capabilities<br><a href="https://www.youtube.com/watch?v=D_WsPbdJKUg&t=179 target="_blank"">2:59</a> AI Outcome<br><a href="https://www.youtube.com/watch?v=D_WsPbdJKUg&t=260 target="_blank"">4:20</a> AI Contributions<br><h3>Transcript</h3><div class='max-width'><p>In this incredibly scary late period when AI has really automated research, humans do this function of like auditing, making it more difficult for the AIs to conspire together and root the servers, take over the process, and extract information from them within the set of things that we can verify, like experiments where we can see, oh yeah, this works at stopping an AI trained to get a fast one past human raters.</p><p>The reasons why I think we actually have such a relatively good chance of handling that are twofold. So one is, as we approach that kind of AI capability, we're approaching that from weaker systems. If the really bad sorts of motivations develop relatively later in the training process, at least with all our countermeasures, then by that time we may have plenty of ability to extract AI assistance on further strengthening the quality of our adversarial examples, the strength of our neural lie detectors, the experiments that we can use to reveal and elicit and distinguish between different kinds of reward hacking tendencies and motivations.</p><p>So yeah, we may have systems that have just not developed bad motivations in the first place and be able to use them a lot in developing the incrementally better systems in a safe way. And we may be able to, even if some of the early systems do develop these bad motivations, if we're able to detect that and experiment and find a way to get away from that, then we can win, even if these sort of hostile motivations develop early.</p><p>When I combine the possibility that we get relatively lucky on the motivations of the earlier AI systems, systems strong enough that we can use for some alignment research tasks, and then the possibility of getting that later with AI assistance that we can't trust fully, where we have to have hard power constraints and a number of things to prevent them from doing this takeover, it still seems plausible we can get a second saving throw where we're able to extract work from these AIs on solving the remaining problems of alignment of things like neural lie detectors faster than they can contribute in their spare time to the project of overthrowing humanity, hacking their servers, and removing the hard power.</p><p>And so if we wind up in a situation where the AIs are misaligned, and then we need to uncover those motivations, change them, and align them, then we get a very scary situation for us because we need to do this stuff very quickly. We may fail, but it's a second chance where our work is just evaluating outputs that the AIs are delivering, having the hard power and supervision to keep them from successfully rooting the servers, doing a takeover during this process, and have them finish the alignment task that we sadly failed to invest enough or succeed in doing beforehand.</p><p>The incredibly juicy ability that we have working with the AIs is that we can have an invaluable outcome that we can see and tell whether they got a fast one past us on an identifiable situation. We can have, here's an air gap computer, you get control of the keyboard, you can input commands, can you root the environment and make a blue banana appear on the screen.</p><p>Even if we train the AI to do that and it succeeds, we see the blue banana, we know it worked. Even if we did not understand and would not have detected the particular exploit that it used to do it, this can give us a rich empirical feedback where we're able to identify things that are even an AI using its best efforts to get past our interpretability methods.</p><p>At what point would it be the case that the AI is contributing significantly in the sense that it would almost be the equivalent of having additional researchers to AI progress in software? The thing to look for is when is it the case that the contributions from AI are starting to become as large or larger as the contributions from humans.</p><p>When this is boosting their effective productivity by 50 or 100% and if you then go from eight months doubling time, say, for effective compute from software innovations, things like inventing the transformer or discovering chinchilla scaling and doing your training runs more optimally or creating flash attention, it doesn't have to have been able to automate everything involved in the process of AI research.</p><p>It can be it's automated a bunch of things and then those are being done in extreme profusion because a thing that AI can do you have it done much more often because it's so cheap. And so it's not a threshold of this is human level AI, it can do everything a human can do with no weaknesses in any area.</p><p>It's that even with its weaknesses it's able to bump up the performance. Tens of millions of GPUs, each is doing the work of maybe 40 maybe more of these kind of existing workers. It's like going from a workforce of tens of thousands to hundreds of millions. You immediately make all kinds of discoveries then, you immediately develop all sorts of tremendous technologies.</p><p>So human level AI is deep, deep into an intelligence explosion. The intelligence explosion has to start with something weaker than that. But what is the point at which that feedback loop starts where you can even, you're not just doing the 0.5% increase in productivity that a sort of AI tool might do, but is actually the equivalent of a researcher or close to it?</p><p>So I think maybe a way to look at it is to give some illustrative examples of the kinds of capabilities that you might see. What we'll have is intense application of the ways in which AIs have advantages, partly offsetting their weaknesses. And so AIs are cheap, we can call a lot of them to do many small problems.</p><p>And so you'll have situations where you have dumber AIs that are deployed thousands of times to equal, say, one human worker. And they'll be doing things like these voting algorithms where you, with an LLM, you generate a bunch of different responses and take a majority vote among them that improves performance sum.</p><p>You'll have things like the AlphaGo kind of approach, where you use the neural net to do search and you go deeper with the search by plowing in more compute, which helps to offset the inefficiency and weaknesses of the model on its own. You'll do things that would just be totally impractical for humans because of the sheer number of steps.</p><p>And so an example of that would be designing synthetic training data. So humans do not learn by just going into the library and opening books at random pages. It's actually much, much more efficient to have things like schools and classes where they teach you things in an order that makes sense, that's focusing on the skills that are more valuable to learn.</p><p>They give you tests and exams that are designed to try and elicit the skill they're actually trying to teach. And right now we don't bother with that because we can hoover up more data from the internet. We're getting towards the end of that. But yeah, as the AIs get more sophisticated, they'll be better able to tell what is a useful kind of skill to practice and to generate that.</p><p>And we've done that in other areas. So AlphaGo, the original version of AlphaGo was booted up with data from human Go play and then improved with reinforcement learning and Monte Carlo tree search. But then AlphaZero, with a somewhat more sophisticated model, benefited from some other improvements, but was able to go from scratch.</p><p>And it generated its own data through self-play, so getting data of a higher quality than the human data, because there are no human players that good available in the dataset. And also a curriculum, so that at any given point, it was playing games against an opponent of equal skill itself.</p><p>And so it was always in an area when it was easy to learn. If you're just always losing, no matter what you do, or always winning, no matter what you do, it's hard to distinguish which things are better and which are worse. And when we have somewhat more sophisticated AIs that can generate training data and tasks for themselves, for example, if the AI can generate a lot of unit tests and then can try and produce programs that pass those unit tests, then the interpreter is providing a training signal.</p><p>And the AI can get good at figuring out what's the kind of programming problem that is hard for AIs right now that will develop more of the skills that I need and then do them. And now you're not going to have employees at OpenAI write like a billion programming problems.</p><p>That's just not going to happen. But you are going to have AIs given the task of producing those enormous number of programming challenges.</p></div></div></body></html>