<html><head><title>RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale</h2><a href="https://www.youtube.com/watch?v=2x0ktXabsJA"><img src="https://i.ytimg.com/vi/2x0ktXabsJA/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./2x0ktXabsJA.html">Whisper Transcript</a> | <a href="./transcript_2x0ktXabsJA.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">i just wanted to make sure that the recording was uh going to go work properly okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=6" target="_blank">00:00:06.960</a></span> | <span class="t">i did not prepare for this uh this this it and honestly like you know there's a lot of other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=14" target="_blank">00:00:14.400</a></span> | <span class="t">papers as well that's going on you don't have to take up the full time uh there's always people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=18" target="_blank">00:00:18.560</a></span> | <span class="t">that are interested in other things uh is there which which other papers are people interested in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=25" target="_blank">00:00:25.280</a></span> | <span class="t">uh i mean just like stuff that was dropped in the channel okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=31" target="_blank">00:00:31.120</a></span> | <span class="t">it's useful to mute uh there's a bottom on the button on the top bottom right to mute all or um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=42" target="_blank">00:00:42.960</a></span> | <span class="t">yeah mute people on entry so that i can help because people don't realize they're unmuted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=51" target="_blank">00:00:51.360</a></span> | <span class="t">settings okay where's the mute all on entry can't seem to find that cd button either</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=60" target="_blank">00:01:00.800</a></span> | <span class="t">uh bottom right there's like a little drop down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=72" target="_blank">00:01:12.480</a></span> | <span class="t">okay okay yeah fine mute participant uh entry got it rj were you saying something because i can't hear you at all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=83" target="_blank">00:01:23.120</a></span> | <span class="t">sorry i didn't i was not muted no i wasn't saying anything okay okay i'm at a cafe so there's a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=94" target="_blank">00:01:34.000</a></span> | <span class="t">um got it got it got it got it okay okay i guess uh i'll go through red light and i'll go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=103" target="_blank">00:01:43.120</a></span> | <span class="t">large language diffusion model uh since um i don't know whether you all saw the tweets from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=109" target="_blank">00:01:49.600</a></span> | <span class="t">uh uh recently as well because because uh apparently google now just made a a decent diffusion model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=120" target="_blank">00:02:00.160</a></span> | <span class="t">uh uh text model which is quite red</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=124" target="_blank">00:02:04.240</a></span> | <span class="t">so ai is apparently going to get much faster today</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=134" target="_blank">00:02:14.880</a></span> | <span class="t">okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=145" target="_blank">00:02:25.520</a></span> | <span class="t">okay anyone okay so uh it so today's going to just be like a collection of papers uh i will kick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=153" target="_blank">00:02:33.920</a></span> | <span class="t">off with my own which is red that and then um where we show how to uh essentially take an existing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=160" target="_blank">00:02:40.960</a></span> | <span class="t">72v quen model and convert it to another architecture altogether and then and then after that i will go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=167" target="_blank">00:02:47.440</a></span> | <span class="t">through the diffusion model um i guess uh paper and and and and and then we'll like pick up from any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=175" target="_blank">00:02:55.840</a></span> | <span class="t">other suggestions from there so the paper that we are covering i'm just going to put the link here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=180" target="_blank">00:03:00.560</a></span> | <span class="t">and share my screen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=182" target="_blank">00:03:02.880</a></span> | <span class="t">yep so uh so so i think a few of you may have already heard about me mentioning this in overall oops</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=194" target="_blank">00:03:14.640</a></span> | <span class="t">but this was essentially the basis for our quirky 72b which which which is where we took</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=202" target="_blank">00:03:22.480</a></span> | <span class="t">where which we took an uh existing quantity to be model let me find a picture which is better yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=209" target="_blank">00:03:29.360</a></span> | <span class="t">yeah where we do where we do an existing 72b model and essentially were able to train it uh using a new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=218" target="_blank">00:03:38.320</a></span> | <span class="t">attention mechanism and we were able to get uh most of the performance um retained um the the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=224" target="_blank">00:03:44.160</a></span> | <span class="t">subsequent paper that we we published is is more of like to document that that process from a high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=229" target="_blank">00:03:49.200</a></span> | <span class="t">level and then subsequently also what we found didn't work in particular so so uh so so the idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=236" target="_blank">00:03:56.640</a></span> | <span class="t">here is that uh we want to actually encourage um and research into new attention mechanism and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=243" target="_blank">00:04:03.520</a></span> | <span class="t">technique doesn't apply just to rwkb this this uh this approach actually unlocks new possibilities for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=251" target="_blank">00:04:11.520</a></span> | <span class="t">for not just uh rwkb but that includes state space for mofoado xlstm and etc um and essentially what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=260" target="_blank">00:04:20.400</a></span> | <span class="t">realized is that most like majority of the feed forward network layer can be recycled if you um there is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=268" target="_blank">00:04:28.240</a></span> | <span class="t">actually a similar um paper that came up uh came out recently as well um mlps um similar across architectures i can't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=278" target="_blank">00:04:38.880</a></span> | <span class="t">remember the exact name um i didn't um but basically there was a recent there was a recent paper that came</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=287" target="_blank">00:04:47.760</a></span> | <span class="t">out where where i where where where a team compared the the lama model and the mr model and i believe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=295" target="_blank">00:04:55.920</a></span> | <span class="t">warmer model and they found that actually that the that the mlp layers uh weight values are approximately similar to each other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=305" target="_blank">00:05:05.920</a></span> | <span class="t">despite being trained on different data sets uh uh when they are optimist so so so so there's kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=312" target="_blank">00:05:12.480</a></span> | <span class="t">like uh hypothesis right now floating around that uh that um if you're going to train on the same large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=318" target="_blank">00:05:18.720</a></span> | <span class="t">web data your mlp layers are going to converge to roughly the same and and and we we also like reaffirm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=325" target="_blank">00:05:25.120</a></span> | <span class="t">that hypothesis that hey you can freeze that layer and just swap out the attention layer so so what uh what we did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=333" target="_blank">00:05:33.040</a></span> | <span class="t">subsequently and so the thing is that uh we showed like this is not the first conversion method per se</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=339" target="_blank">00:05:39.600</a></span> | <span class="t">from an existing uh existing uh existing uh from one architecture to another so so we we we did outline it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=347" target="_blank">00:05:47.360</a></span> | <span class="t">the comparison to like to compare to to to to existing methods uh acknowledging some of the previous work that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=354" target="_blank">00:05:54.080</a></span> | <span class="t">was done accordingly but i think what stood out for us is that uh we are we are probably one of the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=359" target="_blank">00:05:59.920</a></span> | <span class="t">team that went after going through this process and only training on 500 million tokens that we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=364" target="_blank">00:06:04.000</a></span> | <span class="t">even have benchmarks improved and and even though some actually did be proved and like previously right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=372" target="_blank">00:06:12.080</a></span> | <span class="t">previous previous conversion attempts right uh previous conversion attempts right you will see a dramatic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=377" target="_blank">00:06:17.360</a></span> | <span class="t">drop in performance which it which is not really the biggest issue in my opinion because like when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=382" target="_blank">00:06:22.320</a></span> | <span class="t">convert from a quadratic to a linear model even if let's say you drop by 20 performance in overall</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=389" target="_blank">00:06:29.360</a></span> | <span class="t">the inference saving is actually substantial enough that that uh that that this this could potentially be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=395" target="_blank">00:06:35.280</a></span> | <span class="t">viable path towards like making a more scalable model but but we've we've we've the updated techniques</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=401" target="_blank">00:06:41.360</a></span> | <span class="t">that we did we've read that right it's that it's able to like more like rapidly i forgot to talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=407" target="_blank">00:06:47.360</a></span> | <span class="t">the name uh it's a rapidly a rapid attention distillation to linear attention decoder so we are able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=413" target="_blank">00:06:53.600</a></span> | <span class="t">rapidly change one attention mechanism to another um the linear one is just because of the example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=419" target="_blank">00:06:59.120</a></span> | <span class="t">because we are linear architecture but like i said this can be done using other attention mechanisms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=426" target="_blank">00:07:06.160</a></span> | <span class="t">like right now we are actually helping two universities um and we are talking to another two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=431" target="_blank">00:07:11.280</a></span> | <span class="t">and their team to test and variation of the transformer attention model using the same technique so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=437" target="_blank">00:07:17.760</a></span> | <span class="t">it so what we what we did was that uh uh we take the existing model we freeze the feed forward network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=446" target="_blank">00:07:26.320</a></span> | <span class="t">layer and we we swap up the we swap out the attention layer and then uh and then and then subsequently right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=455" target="_blank">00:07:35.840</a></span> | <span class="t">we distilled the information uh in between i i know a lot of people get confused about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=461" target="_blank">00:07:41.200</a></span> | <span class="t">the distillation since i've conveyed the the idea um and and it was covered in this paper is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=467" target="_blank">00:07:47.040</a></span> | <span class="t">is that uh when we say distillation right we we mean distillation from the same model uh from from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=475" target="_blank">00:07:55.040</a></span> | <span class="t">from the original model and and it's not just logics distillation like like the first few steps of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=480" target="_blank">00:08:00.800</a></span> | <span class="t">process right is that we distill it in between the layers so for example you can take a single block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=487" target="_blank">00:08:07.120</a></span> | <span class="t">you put that you as you as you as you do the four inference and as it goes through um goes through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=493" target="_blank">00:08:13.600</a></span> | <span class="t">respectively right uh we we take the the incoming embedding and and outgoing embedding and we use that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=500" target="_blank">00:08:20.000</a></span> | <span class="t">for distillation not the logics itself so so yeah distillation between the layers itself because in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=506" target="_blank">00:08:26.560</a></span> | <span class="t">this case we are replacing the attention mechanism um and then subsequently um you you fine tune that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=513" target="_blank">00:08:33.040</a></span> | <span class="t">do for the fine tuning to like just stabilize and and and weave everything together so uh i think since we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=520" target="_blank">00:08:40.160</a></span> | <span class="t">released this paper right uh background uh i i covered the high level there um what what actually a lot of uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=531" target="_blank">00:08:51.520</a></span> | <span class="t">people pointed out and really liked about it um so after after the logic distillation um experiments and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=538" target="_blank">00:08:58.800</a></span> | <span class="t">benchmark uh i wanted to highlight the this segment what did not work um so uh the because the thing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=548" target="_blank">00:09:08.000</a></span> | <span class="t">that with this uh with this conversion method right um there's many ways you can do get it wrong and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=553" target="_blank">00:09:13.360</a></span> | <span class="t">and and we and and these are some that some things that we uh we uh we we we actually iterated through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=563" target="_blank">00:09:23.920</a></span> | <span class="t">and so we documented like what did not work um we also would like to clarify that when we say it did not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=569" target="_blank">00:09:29.760</a></span> | <span class="t">work and we mean very specifically did not work for rwkb um it is possible that as uh as we experiment with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=577" target="_blank">00:09:37.360</a></span> | <span class="t">other attention mechanism right that some of the things that we flagged out as did not work will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=581" target="_blank">00:09:41.680</a></span> | <span class="t">work with other attention mechanism so so even though we listed it out here right we are we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=585" target="_blank">00:09:45.600</a></span> | <span class="t">already like breaking the rules for one of the experiments that we're already doing so um so like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=591" target="_blank">00:09:51.760</a></span> | <span class="t">for example one of the things that did not work for example which we thought would uh and and uh is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=597" target="_blank">00:09:57.440</a></span> | <span class="t">that uh so when we took so one of the shortcuts that we tried to do for example is that we took the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=601" target="_blank">00:10:01.920</a></span> | <span class="t">existing attention module weights and copy it over to rwkb um because at the end of the other kv is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=610" target="_blank">00:10:10.400</a></span> | <span class="t">is still is still uh still matrix multiplication the weights can be copied over uh and and we we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=618" target="_blank">00:10:18.080</a></span> | <span class="t">like maybe it will work did not work um uh respectively um commented on their leadership skill his</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=627" target="_blank">00:10:27.760</a></span> | <span class="t">leadership skills so we we made a pushback on their decision um today so we will hear most probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=635" target="_blank">00:10:35.760</a></span> | <span class="t">tomorrow how the direction will go if that is not the case we are asking to consider him for a different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=642" target="_blank">00:10:42.560</a></span> | <span class="t">role in relax so sorry yeah uh that didn't sound like a question right yeah um yeah so so yeah um we then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=653" target="_blank">00:10:53.680</a></span> | <span class="t">subsequently um yeah uh like i said so we we we we abulated this and then we tested it like from from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=661" target="_blank">00:11:01.360</a></span> | <span class="t">starting the attention mechanism from scratch pretty much pretty much uh same score so so we end up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=668" target="_blank">00:11:08.560</a></span> | <span class="t">skipping that um even though i mentioned freezing model weights right you like like um i think one of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=676" target="_blank">00:11:16.160</a></span> | <span class="t">things to point out is that you want to freeze only at stage one so so you go through several stages the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=681" target="_blank">00:11:21.440</a></span> | <span class="t">first stage is you replace the attention layer and then you do a quick chain um and then subsequently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=687" target="_blank">00:11:27.440</a></span> | <span class="t">you will want to unfreeze the mlp layers um the reason why we found that this was needed is that uh is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=693" target="_blank">00:11:33.760</a></span> | <span class="t">is that um especially over a longer context length right the performance will drop otherwise things like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=699" target="_blank">00:11:39.520</a></span> | <span class="t">that um batch sizing is extremely tricky uh um but i think this is the case for actually any large model but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=707" target="_blank">00:11:47.200</a></span> | <span class="t">it's just particularly so within this process and and to be clear also like and this is actually further</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=713" target="_blank">00:11:53.760</a></span> | <span class="t">confirm with uh with one of the upcoming reasoning models that we did uh did right was that was that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=721" target="_blank">00:12:01.520</a></span> | <span class="t">will need to have the data set that's reflective of the data that is in the model so for example we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=726" target="_blank">00:12:06.480</a></span> | <span class="t">recently did uh one the reasoning model we converted it uh spoiler is the is the latest quen tree model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=733" target="_blank">00:12:13.920</a></span> | <span class="t">and and we we didn't release it uh because we did when we did the conversion we didn't convert with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=740" target="_blank">00:12:20.400</a></span> | <span class="t">reasoning data set so when when he when he tried to do the reasoning token and things like that it just went</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=746" target="_blank">00:12:26.080</a></span> | <span class="t">crazy um we are now redoing the process with reasoning tokens and and and subsequently uh this this will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=754" target="_blank">00:12:34.880</a></span> | <span class="t">we believe this will perform much better so like like your data set should be approximately the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=760" target="_blank">00:12:40.000</a></span> | <span class="t">of what is your original model data set which is a challenge because we have absolutely no idea what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=765" target="_blank">00:12:45.120</a></span> | <span class="t">is quen data set so so so so so that's one thing to take note another thing that we tried for example is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=772" target="_blank">00:12:52.160</a></span> | <span class="t">like we tried to do laura base uh for for for for the conversion process it just made things a lot worse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=781" target="_blank">00:13:01.360</a></span> | <span class="t">yeah so i i as the reason why we do we decide to document all the what did not work is that uh like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=787" target="_blank">00:13:07.520</a></span> | <span class="t">once again um there is a lot of benefits for this mechanism here uh this approach right uh for new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=794" target="_blank">00:13:14.480</a></span> | <span class="t">attention mechanism not just rbkb and we just wanted to like outline like like some of the key things there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=800" target="_blank">00:13:20.080</a></span> | <span class="t">so yep and and so for those who don't know what rwkb is uh and i think most people in this group</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=806" target="_blank">00:13:26.880</a></span> | <span class="t">already know is that it's a linear attention mechanism that my group has been building and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=811" target="_blank">00:13:31.600</a></span> | <span class="t">scaling up so with this right we have now built the the world's largest 72b um model that doesn't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=819" target="_blank">00:13:39.600</a></span> | <span class="t">the transformer attention mechanism the benefit of it is scale linearly over a thousand x cheaper inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=825" target="_blank">00:13:45.280</a></span> | <span class="t">cost and but what i find more exciting about about us covering this paper itself right this paper is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=835" target="_blank">00:13:55.120</a></span> | <span class="t">that it's not really so much about my architecture because i'm also on record saying that i don't think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=841" target="_blank">00:14:01.280</a></span> | <span class="t">our architecture is final and and that and that like we're and that there's so much more improvement to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=848" target="_blank">00:14:08.240</a></span> | <span class="t">made so so so so so so if anything right i actually do think there needs to be more research into other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=854" target="_blank">00:14:14.640</a></span> | <span class="t">ways to build ai architectures because like one of the things that i said uh at icr where i presented the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=861" target="_blank">00:14:21.040</a></span> | <span class="t">earlier version of this is that um was that right now the largest models to date right are not even are not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=870" target="_blank">00:14:30.160</a></span> | <span class="t">reliable enough to do cashier duty or day-to-day work tasks and we are basic and and well scaling does help</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=879" target="_blank">00:14:39.120</a></span> | <span class="t">it right if we scale seven times more because the scaling laws like you scale 10x to get 10 improvement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=885" target="_blank">00:14:45.440</a></span> | <span class="t">that will be the entire energy of the earth required to train the model so so at so at some point right we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=893" target="_blank">00:14:53.280</a></span> | <span class="t">do need a better uh attention mechanism to like um to like eat to lower the energy cost or to like just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=900" target="_blank">00:15:00.560</a></span> | <span class="t">make things more reliable or to make things as a scale better and previously most of the existing labs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=906" target="_blank">00:15:06.560</a></span> | <span class="t">avoid doing this because to validate any new architecture um you will need to and this is from our experience</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=913" target="_blank">00:15:13.920</a></span> | <span class="t">you need to iterate around 20 to 80 times um to before you can find an architecture that has a 10 improvement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=921" target="_blank">00:15:21.120</a></span> | <span class="t">you're going to get more failures and success and when each try can cost up to five million dollars</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=927" target="_blank">00:15:27.360</a></span> | <span class="t">in in that training process yeah it gets we are looking at tens of millions of dollars to try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=933" target="_blank">00:15:33.600</a></span> | <span class="t">experiment on new architecture and that 10 million could be just to train more tokens but this changes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=940" target="_blank">00:15:40.080</a></span> | <span class="t">things if it's now 2 000 to 20 000 to test a new new attention mechanism it unlocks the door to literally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=948" target="_blank">00:15:48.240</a></span> | <span class="t">anyone uh who is willing to spin up a run port instance for example and that that's how that and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=955" target="_blank">00:15:55.680</a></span> | <span class="t">and that's why we expect more experiments to do so and which links very well to the most very recent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=963" target="_blank">00:16:03.360</a></span> | <span class="t">thing about gemini diffusion model um i think i also said that diffusion models are probably interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=969" target="_blank">00:16:09.200</a></span> | <span class="t">because um or two things that i want to find out like one one uh is that uh diffusion models at least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=978" target="_blank">00:16:18.160</a></span> | <span class="t">for images have been highly resistant to to uh to catastrophe i know sorry to overfitting in the sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=985" target="_blank">00:16:25.840</a></span> | <span class="t">that like you may have heard like uh of large language models overfitting after they see the data three four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=992" target="_blank">00:16:32.720</a></span> | <span class="t">four times for example or even two times and they start going like crazy or inferior diffusion models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=999" target="_blank">00:16:39.280</a></span> | <span class="t">especially in the image space are typically trained over a thousand times with the same data and they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1004" target="_blank">00:16:44.480</a></span> | <span class="t">did not go crazy and and we and and i've been saying that i want to see more success in text diffusion model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1011" target="_blank">00:16:51.520</a></span> | <span class="t">because i want to actually find out and test the hypothesis about why does diffusion models become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1017" target="_blank">00:16:57.760</a></span> | <span class="t">more resilient and and this is and and and well this is not open source uh google gemini has recently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1025" target="_blank">00:17:05.840</a></span> | <span class="t">released a diffusion model as as the demo that is blazingly fast so linking that right is that that uh is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1032" target="_blank">00:17:12.480</a></span> | <span class="t">that uh is that uh is that there's uh on the open source side right uh so that is proved that diffusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1036" target="_blank">00:17:16.800</a></span> | <span class="t">model can actually work as potentially even replacements to to to to my architecture and transform architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1043" target="_blank">00:17:23.200</a></span> | <span class="t">um this um this is the paper covering large language diffusion models and and so this is the the team</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1050" target="_blank">00:17:30.320</a></span> | <span class="t">that trained the lada a b which which which which which then um they uh which is then to be used uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1058" target="_blank">00:17:38.480</a></span> | <span class="t">and then they benchmark it across the spectrum of tasks so i think what is uh what is interesting for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1066" target="_blank">00:17:46.640</a></span> | <span class="t">diffusion models is that uh for this in particular right what is uh i think the easiest way to to view a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1075" target="_blank">00:17:55.440</a></span> | <span class="t">diffusion a text diffusion model is that instead of pixels on the screen you uh you view you you replace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1084" target="_blank">00:18:04.880</a></span> | <span class="t">that with actually just the byte values and and that essentially means that it means that that same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1092" target="_blank">00:18:12.320</a></span> | <span class="t">process right of how you like you you generate an image and then you it slowly updates the values right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1099" target="_blank">00:18:19.120</a></span> | <span class="t">uh i think i think i'm just going to show this as a demo example as you can see it slowly updates like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1104" target="_blank">00:18:24.320</a></span> | <span class="t">what token by token character by character and and the well the downside for this is that your um you may you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1114" target="_blank">00:18:34.240</a></span> | <span class="t">won't get that streaming token effect i mean uh uh respectively uh and your time to first token air</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1121" target="_blank">00:18:41.280</a></span> | <span class="t">code there might be much longer the benefit was that the benefit for diffusion model is that um if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1128" target="_blank">00:18:48.800</a></span> | <span class="t">let's say you're generating a thousand tokens you might be able to generate all thousand tokens in let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1133" target="_blank">00:18:53.120</a></span> | <span class="t">say a hundred steps which means it's faster and overall yeah um i think what is interesting about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1141" target="_blank">00:19:01.280</a></span> | <span class="t">this as well right this paper in particular is that is that like um though as the way they they they said</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1149" target="_blank">00:19:09.840</a></span> | <span class="t">it right uh is that um one is that the model itself right for diffusion models which to be clear this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1156" target="_blank">00:19:16.560</a></span> | <span class="t">is too early to say in my opinion 8b is too early to even draw conclusions right was able to outperform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1162" target="_blank">00:19:22.480</a></span> | <span class="t">lama right in in in in context learning and reversal reasoning reversal reasoning is is is is um is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1171" target="_blank">00:19:31.840</a></span> | <span class="t">is the the the issue when let's say like uh a is related to b b is related to c and you have like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1179" target="_blank">00:19:39.360</a></span> | <span class="t">statement like that uh is the model able to to to deduce logically that c is equal uh uh is uh related to a in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1190" target="_blank">00:19:50.480</a></span> | <span class="t">in a single hole um this is partially mitigated now with we with the chain of thought reasoning or deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1197" target="_blank">00:19:57.920</a></span> | <span class="t">uh deep thinking or whatever you want to call it um or one-star reasoning but this was previously like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1204" target="_blank">00:20:04.960</a></span> | <span class="t">major hurdle for actually a lot of the transformer model it's unable to like do the association backwards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1210" target="_blank">00:20:10.160</a></span> | <span class="t">from what it is trained on and it is shown that diffusion models are able to like overcome that i think it's also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1217" target="_blank">00:20:17.760</a></span> | <span class="t">in part is because they kind of work like in uh in a chain of thought pattern i uh because because like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1223" target="_blank">00:20:23.200</a></span> | <span class="t">every time it generates a part of the text it gets to see and regenerates it so so the and so they cover</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1231" target="_blank">00:20:31.200</a></span> | <span class="t">how they how they train it respectively uh so um where where essentially they must uh uh uh they must</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1239" target="_blank">00:20:39.520</a></span> | <span class="t">the token uh individually uh like there are different ways to approach it so like masking the token individually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1245" target="_blank">00:20:45.360</a></span> | <span class="t">that means what they did was that they they trained the laws of the various different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1249" target="_blank">00:20:49.360</a></span> | <span class="t">token sets and mass respectively uh compared to other other other tokens because since diffusion models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1257" target="_blank">00:20:57.120</a></span> | <span class="t">are no longer like running linearly they're able to see to the left and to the right of them all at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1262" target="_blank">00:21:02.000</a></span> | <span class="t">the same time yeah um the um the other the other thing that they did was for towards more like prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1268" target="_blank">00:21:08.160</a></span> | <span class="t">completion training where basically the prompt is frozen at mass and then uh the response is trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1274" target="_blank">00:21:14.160</a></span> | <span class="t">respectively this is very similar to like the same thing as how people train diffusion models where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1280" target="_blank">00:21:20.080</a></span> | <span class="t">where they where either they add noise and then they just uh train everything from there or they actually freeze</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1287" target="_blank">00:21:27.440</a></span> | <span class="t">a part of the image and and then ask it to regenerate another part of the image so like the one this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1293" target="_blank">00:21:33.040</a></span> | <span class="t">one plus example like for example you like you delete the center of the image and and you and you train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1298" target="_blank">00:21:38.400</a></span> | <span class="t">the model to like regenerate that image so so that's still a problem and response if you structure it similarly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1305" target="_blank">00:21:45.440</a></span> | <span class="t">yeah then um as they covered uh uh what else did they cover there wasn't really other than the what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1315" target="_blank">00:21:55.920</a></span> | <span class="t">they observed and uh and as i mentioned like like uh like uh how how how the inference can can trail</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1324" target="_blank">00:22:04.560</a></span> | <span class="t">between uh lead to higher performance in overall i think the only thing else is like the math and then they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1331" target="_blank">00:22:11.760</a></span> | <span class="t">they basically showed the the the the the model's performance over over the training course and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1338" target="_blank">00:22:18.400</a></span> | <span class="t">it shows that it's it's so autoregressive baseline basically transformers is a uh it shows that it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1344" target="_blank">00:22:24.960</a></span> | <span class="t">able to like keep up in general with transformers uh leading uh leading to like like potential and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1351" target="_blank">00:22:31.040</a></span> | <span class="t">like more research down this path respectively so yeah uh and then they shared their benchmark respectively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1357" target="_blank">00:22:37.760</a></span> | <span class="t">which what as i mentioned previously what is of interest is that like for example oxy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1366" target="_blank">00:22:46.400</a></span> | <span class="t">is higher than llama and well mmlu is lower uh which is kind of funny because it's like this like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1374" target="_blank">00:22:54.800</a></span> | <span class="t">we saw the same issues with rwkb for example with rwkb having the higher oxy and gsm 8k well we're having</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1383" target="_blank">00:23:03.760</a></span> | <span class="t">slightly lower uh math and mmlu uh which which kind of like still leads into the like one of the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1389" target="_blank">00:23:09.440</a></span> | <span class="t">hypotheses that is a different attention mechanism may favor different architectures yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1394" target="_blank">00:23:14.240</a></span> | <span class="t">yeah and i think that's about it i already showed the sampling process through the animation here so yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1403" target="_blank">00:23:23.680</a></span> | <span class="t">yep ah yeah so that's uh yeah anyone has any questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1409" target="_blank">00:23:29.840</a></span> | <span class="t">i had a quick question mostly from the chat so since we have language diffusion now is there any language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1426" target="_blank">00:23:46.960</a></span> | <span class="t">language flow matching models oh um not that i know all uh um because i i think this is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1435" target="_blank">00:23:55.840</a></span> | <span class="t">fresh out of the oven</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1438" target="_blank">00:23:58.800</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1441" target="_blank">00:24:01.680</a></span> | <span class="t">yeah but i don't see why not uh down the line i i'm a little bit so i'm not sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1452" target="_blank">00:24:12.640</a></span> | <span class="t">here i didn't read i haven't i still haven't read this paper but we actually covered this a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1457" target="_blank">00:24:17.680</a></span> | <span class="t">bit previously too so i mean this is really similar to burt right and it feels to me like you're we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1464" target="_blank">00:24:24.640</a></span> | <span class="t">basically just training we're training it to uh like an encoder only model but just with multiple steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1472" target="_blank">00:24:32.240</a></span> | <span class="t">and it's unclear to me the flow matching models require the gaussian assumption and i don't know that that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1479" target="_blank">00:24:39.840</a></span> | <span class="t">gaussian assumption is here or not uh so yeah like i i haven't read the paper though so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1488" target="_blank">00:24:48.240</a></span> | <span class="t">the gaussian function let's just do a quick check for that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1495" target="_blank">00:24:55.120</a></span> | <span class="t">i from what i understand so like the large the language division model right what they said here right is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1504" target="_blank">00:25:04.240</a></span> | <span class="t">that is that is that they use the same sampling techniques for the individual tokens um as existing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1512" target="_blank">00:25:12.160</a></span> | <span class="t">approach for transformers so we're not so so i don't think they're using using like uh gaussian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1519" target="_blank">00:25:19.360</a></span> | <span class="t">sampling they are just literally like at the individual token slots these are the logits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1527" target="_blank">00:25:27.600</a></span> | <span class="t">and they sample from there and i assume what's happening as well is that there's probably like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1534" target="_blank">00:25:34.480</a></span> | <span class="t">uh either low temperature or top p setting did they write top p</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1540" target="_blank">00:25:40.080</a></span> | <span class="t">no they didn't say top p yeah i think one of the things that they they did to make it more stable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1546" target="_blank">00:25:46.320</a></span> | <span class="t">is probably either a lower temperature or top p setting uh where where essentially like the model will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1554" target="_blank">00:25:54.000</a></span> | <span class="t">or the the individual tokens will will eventually represent their 90 plus percentile representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1561" target="_blank">00:26:01.200</a></span> | <span class="t">and the idea is that once once you put put the output back into the input for the diffusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1565" target="_blank">00:26:05.680</a></span> | <span class="t">uh if it's confident in that same input token it will it will with high probability output the same token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1574" target="_blank">00:26:14.960</a></span> | <span class="t">and everything will then eventually stabilize so it's not diffusion it's really um uh it's it's it's really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1584" target="_blank">00:26:24.160</a></span> | <span class="t">it's really just um yeah it's a sampling standard sampling that that's what they say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1594" target="_blank">00:26:34.640</a></span> | <span class="t">uh i see great uh i see the question one question i hope you answer at some point in the talk is what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1602" target="_blank">00:26:42.080</a></span> | <span class="t">is left to prove with regards to pursuing other uh others building you to use rwk as a base instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1609" target="_blank">00:26:49.040</a></span> | <span class="t">attention i think right now one of the biggest proof point that is not 100 absolutely proven is million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1615" target="_blank">00:26:55.680</a></span> | <span class="t">token context um um even though uh and above um and and i feel like this is a sliding goal post problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1624" target="_blank">00:27:04.160</a></span> | <span class="t">um so like rwkb right now is uh with the latest v7 paper we are able to hit 32k context length perfect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1630" target="_blank">00:27:10.880</a></span> | <span class="t">memory uh with needle in the haystack and that's what 3b model the converted models are trained at 4k so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1637" target="_blank">00:27:17.200</a></span> | <span class="t">they do not survive a 32k needle in the haystack to do a 32k needle in the haystack we needed way more vram to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1644" target="_blank">00:27:24.480</a></span> | <span class="t">do the conversion and training process um and so so that would have required at least four nodes of mi300</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1653" target="_blank">00:27:33.360</a></span> | <span class="t">to to do to do 64k and 32k training and above um and you can imagine the numbers just get more ridiculous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1661" target="_blank">00:27:41.760</a></span> | <span class="t">if you want to do 1 million uh token length um yeah so that is the what's left to prove but what we point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1669" target="_blank">00:27:49.920</a></span> | <span class="t">to to to as a hypothesis in the rwk v7 paper is that we shown that 1.5b is slightly below this one okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1678" target="_blank">00:27:58.000</a></span> | <span class="t">needed a haystack and we showed that at 3b is more than 32k and so we show that we've increased param size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1686" target="_blank">00:28:06.320</a></span> | <span class="t">and increased state size right that the context length actually improved in terms of the model's ability to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1692" target="_blank">00:28:12.800</a></span> | <span class="t">memorize it and therefore right the hypothesis is that as we scale to 72b right when fully trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1700" target="_blank">00:28:20.720</a></span> | <span class="t">it should be able to handle much larger context length like i say this is a hypothesis we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1706" target="_blank">00:28:26.720</a></span> | <span class="t">not trained uh 512k context length model because we do not run a cluster uh a super cluster uh that being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1715" target="_blank">00:28:35.200</a></span> | <span class="t">said uh we are training a larger context length model uh for uh for uh for quirky too we already have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1725" target="_blank">00:28:45.840</a></span> | <span class="t">iteration on on on the existing model which which then we are using like uh a few things that we if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1731" target="_blank">00:28:51.760</a></span> | <span class="t">you look at our previous paper like goldfinch compress kv caches and stuff like that uh we are able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1736" target="_blank">00:28:56.960</a></span> | <span class="t">reduce the amount of memory requirement for longer context length um together with rwkv so it's a hybrid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1742" target="_blank">00:29:02.880</a></span> | <span class="t">model and we are going to we are probably going to try push this to train to 64k to uh and one to eight and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1749" target="_blank">00:29:09.440</a></span> | <span class="t">then 256k and above so so that would bring it into like into the category that is very well usable for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1756" target="_blank">00:29:16.000</a></span> | <span class="t">most use case so activity is what is distilled into the target model yes uh that is for stage one and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1764" target="_blank">00:29:24.000</a></span> | <span class="t">two uh eventually at the later stages it will be distillation from logits so that part is extremely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1771" target="_blank">00:29:31.760</a></span> | <span class="t">important a lot of previous papers they distilled from the logits and that didn't train the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1777" target="_blank">00:29:37.840</a></span> | <span class="t">mechanism well enough to be stabilized um the what you want to do is to train as fast as possible with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1786" target="_blank">00:29:46.240</a></span> | <span class="t">as little tokens as possible all the layers right to mimic the original attention layers uh capability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1793" target="_blank">00:29:53.040</a></span> | <span class="t">and so that's why we distilled between layers because because then then during the back propagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1797" target="_blank">00:29:57.520</a></span> | <span class="t">process right it doesn't need to guess hey if i backprop all these all these uh inputs and outputs right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1805" target="_blank">00:30:05.440</a></span> | <span class="t">which layers do i need to update there is no guess within that it's just literally which which uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1812" target="_blank">00:30:12.000</a></span> | <span class="t">which mentioned notification block you want to update so so so that part is actually actually important um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1817" target="_blank">00:30:17.280</a></span> | <span class="t">rwkv's nrn no uh it's not well it's inspired by lstm at this point uh it's i would say it's closer to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1827" target="_blank">00:30:27.680</a></span> | <span class="t">transformer than lstm uh we've been rewriting this since like five years we're not in version seven so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1834" target="_blank">00:30:34.800</a></span> | <span class="t">so a lot of things have changed have you done a compute equivalent performance such as duplicate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1839" target="_blank">00:30:39.200</a></span> | <span class="t">context versus the original model single version of the context ah so um this is this is the same thing as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1844" target="_blank">00:30:44.800</a></span> | <span class="t">the state space model team have shown that uh and we have tested this as well and we can confirm this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1850" target="_blank">00:30:50.080</a></span> | <span class="t">that yes if you duplicate the context twice the model does perform better um this is this applies to all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1857" target="_blank">00:30:57.600</a></span> | <span class="t">linear models uh respectively not just rwkv state space and i believe xlstm as well though i may need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1864" target="_blank">00:31:04.720</a></span> | <span class="t">verify that uh personally i have verified it for state space model uh yeah um there was a patron</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1872" target="_blank">00:31:12.640</a></span> | <span class="t">is uh repeat twice linear models uh crap i can't remember the paper name yeah i'll i'll try to search</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1882" target="_blank">00:31:22.880</a></span> | <span class="t">up that okay so we have the repeat twice paper and the subsequently what was the other one that i mentioned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1892" target="_blank">00:31:32.320</a></span> | <span class="t">uh the mlp layers are being the same uh across different architectures so yeah um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1902" target="_blank">00:31:42.000</a></span> | <span class="t">another thing to highlight as well right is that is that what is exciting also for example for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1908" target="_blank">00:31:48.160</a></span> | <span class="t">quirky 72b right uh and i assume this will apply to state space as well if it goes through the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1913" target="_blank">00:31:53.600</a></span> | <span class="t">process is that well we have similar performance to transformers at 72b scale when running on the same gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1924" target="_blank">00:32:04.640</a></span> | <span class="t">we are able to get more tokens per second in aggregate than than as it is in transform architecture so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1932" target="_blank">00:32:12.800</a></span> | <span class="t">are talking so so we're talking about like the tokens per second comparison that the 72b model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1937" target="_blank">00:32:17.360</a></span> | <span class="t">um is like we can we're able to do like batch size 60 or 70. if you run a 72b on h100 you're only able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1945" target="_blank">00:32:25.840</a></span> | <span class="t">do let's say batch size 8 or 16. and to run similar batch sizes right you will probably only need you probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1952" target="_blank">00:32:32.640</a></span> | <span class="t">need to run a transformer a b instead and that that's speed up in overall token per second is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1959" target="_blank">00:32:39.360</a></span> | <span class="t">probably the bigger impact because at the end of the production really cares about how much compute and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1964" target="_blank">00:32:44.880</a></span> | <span class="t">how much tokens and how much intelligence you can extract uh rj do you ah yeah just read twice closing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1971" target="_blank">00:32:51.440</a></span> | <span class="t">recall gap yeah that is the paper yeah thank you for finding that uh so i i will repost this inside here for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1979" target="_blank">00:32:59.680</a></span> | <span class="t">for those who once if someone can find that mlp paper that would be great as well uh yeah so this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=1988" target="_blank">00:33:08.400</a></span> | <span class="t">uh just read twice a uh paper um the trdr is uh we like if you just where was it the benchmarks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2000" target="_blank">00:33:20.400</a></span> | <span class="t">uh okay i just realized that this paper is actually quite hard to to read at an immediate glance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2005" target="_blank">00:33:25.120</a></span> | <span class="t">but the trdr was that uh linear models perform better when you just repeat the context twice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2015" target="_blank">00:33:35.120</a></span> | <span class="t">uh uh uh yeah pretty curious about after after after is there language for matching not too sure about that uh what do you think about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2023" target="_blank">00:33:43.840</a></span> | <span class="t">uh compared to mamba uh as covered previously like we we believe that like linear intentions uh uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2035" target="_blank">00:33:55.840</a></span> | <span class="t">uh works better as because because like you're able to take the one of the problems that we observe in mamba</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2042" target="_blank">00:34:02.720</a></span> | <span class="t">especially in the smaller models is that informations are merged from left to right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2047" target="_blank">00:34:07.200</a></span> | <span class="t">and if you have and part of the reason why repeating twice works is that if let's say you have instruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2053" target="_blank">00:34:13.840</a></span> | <span class="t">and then you have uh information here information here will end up being merged into a single state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2059" target="_blank">00:34:19.680</a></span> | <span class="t">before it gets to see instruction so so mama mama much information like like like in a cascading pattern</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2066" target="_blank">00:34:26.880</a></span> | <span class="t">like from left to right together to each other side by side before i generate the next token um sometimes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2072" target="_blank">00:34:32.560</a></span> | <span class="t">what happens sometimes is that the information gets lost before it reads the instruction and therefore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2078" target="_blank">00:34:38.320</a></span> | <span class="t">it's unable to perform the task that is the disadvantage but i'm also going to say that that disadvantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2084" target="_blank">00:34:44.800</a></span> | <span class="t">could be could be academical and when i say this is the part where sometimes because i i jump into both academia and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2090" target="_blank">00:34:50.880</a></span> | <span class="t">practicality right um in production right theoretical academia limitations right are just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2097" target="_blank">00:34:57.520</a></span> | <span class="t">that um if i would argue that for state space right if the state size is big enough and it's able to compress</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2106" target="_blank">00:35:06.160</a></span> | <span class="t">all this information together and then eventually when it merge together there's no information loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2111" target="_blank">00:35:11.440</a></span> | <span class="t">then the disadvantage is not really a disadvantage we have for rwkp if you put the information it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2117" target="_blank">00:35:17.200</a></span> | <span class="t">not lose that information and then it's able to decide the following information as you process through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2122" target="_blank">00:35:22.880</a></span> | <span class="t">uh yeah in both cases apparently repeating twice kind of mitigates this issue so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2128" target="_blank">00:35:28.080</a></span> | <span class="t">yeah and if the inference cost is worth it yeah then it becomes a question of more like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2133" target="_blank">00:35:33.680</a></span> | <span class="t">more like how do we dematurize the the technology as well yeah sorry okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2142" target="_blank">00:35:42.320</a></span> | <span class="t">i just speak through all the questions on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2147" target="_blank">00:35:47.200</a></span> | <span class="t">chat yeah anyone has any other questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2158" target="_blank">00:35:58.000</a></span> | <span class="t">since nobody is i just want to dig in a little bit more on the repeat tracing so i'm wondering in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2163" target="_blank">00:36:03.440</a></span> | <span class="t">the gist of the question you answered but i wanted to know if you or anyone else done a study of like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2170" target="_blank">00:36:10.720</a></span> | <span class="t">if for the compute equivalent right so like because you guys built this model that has the rwkv attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2180" target="_blank">00:36:20.720</a></span> | <span class="t">mechanism in it um so that you're you know sort of like and you're saying that like kind of sort of we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2187" target="_blank">00:36:27.200</a></span> | <span class="t">get the same quality out of the model so like let's just pretend it's exactly the same then can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2193" target="_blank">00:36:33.120</a></span> | <span class="t">like the quality like if you just repeat twice like where does where do you land with compute and then like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2201" target="_blank">00:36:41.280</a></span> | <span class="t">just repeat your context right like are you still more compute efficient and what what happens if you repeat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2206" target="_blank">00:36:46.400</a></span> | <span class="t">three times or whatever to get to the point of of um compute equivalent and look at quality there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2213" target="_blank">00:36:53.760</a></span> | <span class="t">actually that's a good question you're talking about inference time compute right exactly yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2221" target="_blank">00:37:01.520</a></span> | <span class="t">uh well that's the only way is to try and find out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2232" target="_blank">00:37:12.000</a></span> | <span class="t">yeah we have not tried to repeat it that many times uh i think also you have to remember part of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2241" target="_blank">00:37:21.920</a></span> | <span class="t">bigger issue is that and this applies to both state space and rwkv even though we are more stable over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2247" target="_blank">00:37:27.440</a></span> | <span class="t">longer context right the prerequisite for this is that we need to train the model to support</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2253" target="_blank">00:37:33.120</a></span> | <span class="t">those lengthy context links and right now our longest is 32 to we do have a 128k model so we could repeat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2264" target="_blank">00:37:44.240</a></span> | <span class="t">the experiment within let's say that 128k uh window itself uh i think um i think beyond beyond that beyond that i um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2276" target="_blank">00:37:56.960</a></span> | <span class="t">yeah uh we we definitely want to be able to like like research more into a longer context but i think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2283" target="_blank">00:38:03.280</a></span> | <span class="t">it's an interesting thought exercise like you said like what if you don't repeat twice i suspect you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2288" target="_blank">00:38:08.080</a></span> | <span class="t">going to get diminishing point of return at some point like i i i think even three or four might be diminishing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2294" target="_blank">00:38:14.560</a></span> | <span class="t">but it's an interesting question i would say or maybe uh the equivalent way to do the experiment is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2302" target="_blank">00:38:22.880</a></span> | <span class="t">that if you look at okay you have your 72b model and then what is the equivalent model for the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2310" target="_blank">00:38:30.800</a></span> | <span class="t">amount of compute with the original model right so like a b model or whatever it is for a certain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2316" target="_blank">00:38:36.880</a></span> | <span class="t">context length what's the quality difference like that so yeah yeah sorry yeah no no that i think that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2327" target="_blank">00:38:47.200</a></span> | <span class="t">something yeah yeah the equivalent um com in terms of tokens per second output right the equivalent would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2334" target="_blank">00:38:54.080</a></span> | <span class="t">be a 32b or a uh 8b model uh somewhere in between that range uh the number changes according to context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2341" target="_blank">00:39:01.280</a></span> | <span class="t">right um but that's the approximate equivalent um if you control by tokens per second the the thing is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2349" target="_blank">00:39:09.920</a></span> | <span class="t">if you have a 72b rwkb model you still need 72 gigs assuming um floating point eight right of vram to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2359" target="_blank">00:39:19.280</a></span> | <span class="t">load the model so your minimum hardware requirement doesn't actually go down it just means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2364" target="_blank">00:39:24.720</a></span> | <span class="t">with the same gpu you can handle more tokens yeah okay yeah and likewise i would say that i don't see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2373" target="_blank">00:39:33.200</a></span> | <span class="t">why this is any different for mamba because we we both scale linearly in the same way in terms of compute cost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2378" target="_blank">00:39:38.320</a></span> | <span class="t">the same way in terms of compute cost flow you have any questions because now i thought i saw you say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2394" target="_blank">00:39:54.000</a></span> | <span class="t">something yeah no it's just talking to myself about the 72 uh gigabytes of vram still required on that last uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2402" target="_blank">00:40:02.880</a></span> | <span class="t">like commentary that being said um quen uh the latest quen model is 32 gigs and it's it's looking very nice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2414" target="_blank">00:40:14.080</a></span> | <span class="t">yeah i'm excited with a lot of the last releases and like being able to run that stuff at home</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2420" target="_blank">00:40:20.320</a></span> | <span class="t">uh i've been messing with a lot of voice stuff i haven't done as much as the language model stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2423" target="_blank">00:40:23.920</a></span> | <span class="t">locally yet or recently but yeah i'm excited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2426" target="_blank">00:40:26.720</a></span> | <span class="t">how many people run run the quen 32b on at home or on their own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2445" target="_blank">00:40:45.360</a></span> | <span class="t">okay okay okay oh wait uh i run cold sometimes i used to get out sadly i'm gpu very gpu poor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2454" target="_blank">00:40:54.960</a></span> | <span class="t">okay uh yeah so just started to okay okay promising um yeah but i think it's i think it's nice that uh like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2470" target="_blank">00:41:10.320</a></span> | <span class="t">if like if like we now have a model that's kind of like better than last year half frontier model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2478" target="_blank">00:41:18.880</a></span> | <span class="t">that you can run on your laptop so if you just follow that trendline look at the best model now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2485" target="_blank">00:41:25.040</a></span> | <span class="t">in two years time you're going to run that on your laptop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2491" target="_blank">00:41:31.120</a></span> | <span class="t">yeah okay um then since since we went through all that uh well asking for like anyone wants any papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2502" target="_blank">00:41:42.640</a></span> | <span class="t">to be covered for two for next week uh or next next week um there is the ai engineering summit in case you do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2511" target="_blank">00:41:51.680</a></span> | <span class="t">anyone do not know about it i'm sorry yeah engineering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2515" target="_blank">00:41:55.440</a></span> | <span class="t">world's fair</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2518" target="_blank">00:41:58.320</a></span> | <span class="t">so from from from looking at the channels it looks like uh next week someone was talking about doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2527" target="_blank">00:42:07.520</a></span> | <span class="t">um because the ai engineer world's fair is not next week but the following week right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2532" target="_blank">00:42:12.560</a></span> | <span class="t">oh following oops yeah uh so so so it looks like next week someone was going to do um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2539" target="_blank">00:42:19.920</a></span> | <span class="t">diffusion large language diffusion models if i'm not mistaken is what i saw on the channel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2543" target="_blank">00:42:23.680</a></span> | <span class="t">um and he was talking about splitting the time i'd be interested in maybe possibly doing a paper that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2549" target="_blank">00:42:29.520</a></span> | <span class="t">talks about um image generation models and diffusion as like a lead-in to what he's going to talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2555" target="_blank">00:42:35.760</a></span> | <span class="t">maybe that will be interesting uh as a way to like split the time i could do like maybe the first 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2560" target="_blank">00:42:40.320</a></span> | <span class="t">or 15 minutes and let him take the rest of the time but i am interested in google's uh large language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2565" target="_blank">00:42:45.280</a></span> | <span class="t">diffusion model i haven't looked into it at all but it seemed to be super interesting i don't know if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2569" target="_blank">00:42:49.200</a></span> | <span class="t">it's like where the benchmarks are and and stuff like that but uh was planning on tuning in next week</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2574" target="_blank">00:42:54.560</a></span> | <span class="t">for his talk i forgot who it was but um just when i when i think of diffusion models off the top of my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2580" target="_blank">00:43:00.400</a></span> | <span class="t">head it's like image generation models i've also seen lately that uh there's quite a few of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2585" target="_blank">00:43:05.760</a></span> | <span class="t">um music generation models that are also diffusion based so i just thought it would be like to do a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2592" target="_blank">00:43:12.560</a></span> | <span class="t">nice little roundup you know before he gets into um you know google stuff or language models maybe um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2602" target="_blank">00:43:22.480</a></span> | <span class="t">i don't know if i don't know if i don't know if if that makes any sense but i thought it'd be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2606" target="_blank">00:43:26.400</a></span> | <span class="t">interesting all right so um basically we're going to do an image diffusion as a lead into language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2613" target="_blank">00:43:33.440</a></span> | <span class="t">diffusion we see tomorrow will be a diffusion i mean next week will be a diffusion model uh uh pipeline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2621" target="_blank">00:43:41.520</a></span> | <span class="t">and and promoting the air engineering welfare if you haven't bought your tickets to to come to sf uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2627" target="_blank">00:43:47.920</a></span> | <span class="t">and and to meet us um yep um please do so as soon as possible before before before it all runs out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2634" target="_blank">00:43:54.720</a></span> | <span class="t">uh we will so after that week we will probably uh organize once again on site an in-person paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2642" target="_blank">00:44:02.720</a></span> | <span class="t">reading club uh for the world's fair yeah and yep uh i guess flo you'll be there i'll be there rj will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2652" target="_blank">00:44:12.640</a></span> | <span class="t">there yeah so so yeah um yeah do suggest papers that you want to cover uh and subsequently um yeah and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2662" target="_blank">00:44:22.560</a></span> | <span class="t">and and i'm looking forward to the in-person uh paper club where and when uh the exact location i may</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2671" target="_blank">00:44:31.680</a></span> | <span class="t">need to figure that out with swigs or for the actual for for the world's fair but but yeah we'll figure it out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2678" target="_blank">00:44:38.880</a></span> | <span class="t">okay it is a good good time too to just mention last year we like because there's all these after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2686" target="_blank">00:44:46.480</a></span> | <span class="t">events and you need to get to a lot of them we ended up getting split up a lot so i i just want to put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2692" target="_blank">00:44:52.400</a></span> | <span class="t">out and we can talk about this on discord but i want to just put out there that let's try to like get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2698" target="_blank">00:44:58.560</a></span> | <span class="t">some you know sort of uh some sort of consensus on what events that everybody wants to go to and try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2705" target="_blank">00:45:05.200</a></span> | <span class="t">to get tickets for them together so that we can go hang out together at these events okay sounds good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2710" target="_blank">00:45:10.880</a></span> | <span class="t">sounds good yeah uh if anyone wants to also like organize a specific written space uh event in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2718" target="_blank">00:45:18.320</a></span> | <span class="t">area we'll probably also be able to help uh help with that as well yeah maybe before after all during the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2723" target="_blank">00:45:23.600</a></span> | <span class="t">event and yeah i'm looking forward to seeing you sam as well graph right okay thanks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2732" target="_blank">00:45:32.480</a></span> | <span class="t">sam are you if you're speaking at the graph right thing do you have any like uh pre-reading that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2744" target="_blank">00:45:44.160</a></span> | <span class="t">can do i'm interested in understanding like graph rag has been a like a topic that keeps coming up and i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2751" target="_blank">00:45:51.120</a></span> | <span class="t">never really i never really gone on to i'd be curious to hear if there's like what new is happening and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2758" target="_blank">00:45:58.080</a></span> | <span class="t">we can pre-read for that well i'm essentially i don't know if you were at when wasim did this paper club</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2766" target="_blank">00:46:06.080</a></span> | <span class="t">last year but i'm essentially like taking what he did and sort of updating it and adding to it and stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2774" target="_blank">00:46:14.640</a></span> | <span class="t">um so if you've watched that then you've basically seen it but yeah i was just yeah we we've published</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2781" target="_blank">00:46:21.120</a></span> | <span class="t">a couple things on it'll be it'll be about like the the specialized alum for building graphs and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2787" target="_blank">00:46:27.920</a></span> | <span class="t">retrieval aware compression and the fusion and decoder stuff that we're we're doing so yeah this this group</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2794" target="_blank">00:46:34.960</a></span> | <span class="t">has seen a lot of it before it's fine we all need to go there so that will be unless the schedule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2803" target="_blank">00:46:43.600</a></span> | <span class="t">changes which in mind um it's 4th of june 11 40 a.m thank you thank you so make sure you all go there to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2813" target="_blank">00:46:53.600</a></span> | <span class="t">support sam uh a family member who else is talking to right who else nathan lambert nato</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2826" target="_blank">00:47:06.880</a></span> | <span class="t">you know what which category uh i have no idea i just i think i saw in his blog that he was going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2838" target="_blank">00:47:18.080</a></span> | <span class="t">here and talking but maybe he just was saying he's going to be there all right all right okay um okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2847" target="_blank">00:47:27.520</a></span> | <span class="t">okay uh and we got 10 more minutes somehow so yeah um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2854" target="_blank">00:47:34.000</a></span> | <span class="t">is there any how how would you all want the paper club to be run uh more with especially with papers like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2867" target="_blank">00:47:47.200</a></span> | <span class="t">and topics because because like uh like in particular like what is there more area of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2875" target="_blank">00:47:55.440</a></span> | <span class="t">interest in particular or is it just just do we just keep things as it is because like trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2880" target="_blank">00:48:00.560</a></span> | <span class="t">figure out how to like fill in the pipeline more and to avoid the situation of like hey uh i'm going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2889" target="_blank">00:48:09.440</a></span> | <span class="t">to cover my own paper unless people ask for it i mean i actually personally like hearing people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2896" target="_blank">00:48:16.960</a></span> | <span class="t">talk about their own papers i'd rather hear from someone who's not like the world's you know most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2903" target="_blank">00:48:23.920</a></span> | <span class="t">prominent uh you know ai expert talk about their own paper than just read somebody's paper because i i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2910" target="_blank">00:48:30.960</a></span> | <span class="t">feel like i get a lot out of hearing the author's take on it so i i actually kind of like that person</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2919" target="_blank">00:48:39.120</a></span> | <span class="t">okay okay then uh yeah then um then what i would request is that uh if possible if you all know any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2927" target="_blank">00:48:47.200</a></span> | <span class="t">interesting papers or if you know uh any uh authors to these interesting papers um and you feel like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2935" target="_blank">00:48:55.040</a></span> | <span class="t">you uh it'll be worth inviting them just let me know uh let me review eugenian uh or even swix know and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2943" target="_blank">00:49:03.440</a></span> | <span class="t">and and sometimes and we'll be doing we'll be willing to do the the reach out uh like i think the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2949" target="_blank">00:49:09.360</a></span> | <span class="t">time there the other time was oh which people was it that i did the previous one was the one scaling one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2955" target="_blank">00:49:15.280</a></span> | <span class="t">uh for a ai the paitia paper for example we got quentin on board to to help cover it though i was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2962" target="_blank">00:49:22.720</a></span> | <span class="t">quentin is probably more prominent but it doesn't need to be prominent in any shape or form yeah so so we'd love to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2968" target="_blank">00:49:28.640</a></span> | <span class="t">hear suggestions especially if like you know the person particularly a personality yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2972" target="_blank">00:49:32.800</a></span> | <span class="t">okay i want the authors to present here yes please and if you want want me to reach out to them to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2984" target="_blank">00:49:44.240</a></span> | <span class="t">apply more pressure just let me know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2992" target="_blank">00:49:52.000</a></span> | <span class="t">i i also since no one's talking i i also feel like if having a like a bit i feel like that so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=2999" target="_blank">00:49:59.680</a></span> | <span class="t">there's a discussion of having like the test of time version and then the current papers version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3006" target="_blank">00:50:06.800</a></span> | <span class="t">i actually think that you know in as much as there's demand there's definitely like several curriculums</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3013" target="_blank">00:50:13.840</a></span> | <span class="t">that i would be interested in like there's like an architecture curriculum the agents curriculum like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3018" target="_blank">00:50:18.640</a></span> | <span class="t">all sorts of different you know probably diffusion or or like image model like there's a bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3026" target="_blank">00:50:26.320</a></span> | <span class="t">different ones that i feel like we could put together curriculums for that i would be interested</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3032" target="_blank">00:50:32.160</a></span> | <span class="t">in i would obviously wouldn't have time to do them all but like i i think in that also helpful for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3037" target="_blank">00:50:37.280</a></span> | <span class="t">pre-reading because then you already you know what's next to each paper and you don't and so that i one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3042" target="_blank">00:50:42.800</a></span> | <span class="t">thing that one criticism i have of how we do things now is it usually the the paper gets announced like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3049" target="_blank">00:50:49.520</a></span> | <span class="t">a day or two before the the discussion and it's hard to pre-read so if i know a week in advance it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3054" target="_blank">00:50:54.640</a></span> | <span class="t">a lot easier for me to like over the weekend find a time or something like that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3057" target="_blank">00:50:57.840</a></span> | <span class="t">okay i do have one related questions how many people in this discord or in this paper club</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3069" target="_blank">00:51:09.360</a></span> | <span class="t">come from a background without coding experience not with um the reason why i'm asking this is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3076" target="_blank">00:51:16.800</a></span> | <span class="t">uh i've been exploring and and it is not commitment this is something that i'm still figuring out the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3081" target="_blank">00:51:21.680</a></span> | <span class="t">materials on right doing an entire series about like ai architecture with the assumption that you do not know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3088" target="_blank">00:51:28.960</a></span> | <span class="t">coding uh and then and my rationale behind that is that because well we do well i do like speak highly of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3096" target="_blank">00:51:36.400</a></span> | <span class="t">like andrew kapati series and fast ai series about ai architectures and stuff all of them kind of pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3102" target="_blank">00:51:42.800</a></span> | <span class="t">much assume you know coding and there are there are lots of people like trying to understand ai better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3108" target="_blank">00:51:48.960</a></span> | <span class="t">not necessarily to build ai architectures but just to understand it better and and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3116" target="_blank">00:51:56.480</a></span> | <span class="t">yeah i'm just like trying to see like how many people here just doesn't know code uh before jumping in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3123" target="_blank">00:52:03.920</a></span> | <span class="t">okay oh well i guess most people know code i i know there's a sampling bias in for this discord so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3141" target="_blank">00:52:21.360</a></span> | <span class="t">so it's like yeah just just laying out the idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3143" target="_blank">00:52:23.600</a></span> | <span class="t">the silent majority is very silent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3148" target="_blank">00:52:28.480</a></span> | <span class="t">okay then i i think i shall just uh if there's no other paper suggestion then uh yeah perhaps we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3159" target="_blank">00:52:39.600</a></span> | <span class="t">should do a test of time week instead uh as in like i think what we can do is we can start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3166" target="_blank">00:52:46.400</a></span> | <span class="t">doing team weeks to make it easier so uh uh i would say let's try to do a test of time week after ai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3175" target="_blank">00:52:55.840</a></span> | <span class="t">engineering uh uh conference and and and then we use and then because seems like next week we can try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3183" target="_blank">00:53:03.440</a></span> | <span class="t">to turn it into diffusion week and then we'll just try to find papers around it that make it easier to like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3188" target="_blank">00:53:08.960</a></span> | <span class="t">pre-plan the paper in advance yeah okay then uh yeah um yeah if that's the case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3196" target="_blank">00:53:16.480</a></span> | <span class="t">yeah okay yeah yeah then we'll try to make sure that happens yeah then uh i'll just end off uh today's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3209" target="_blank">00:53:29.920</a></span> | <span class="t">one slightly early because then that uh if you have papers that you will want to suggest just feel free to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3214" target="_blank">00:53:34.960</a></span> | <span class="t">put into the discord okay okay okay take care everyone uh see you again next week</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3221" target="_blank">00:53:41.040</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3251" target="_blank">00:54:11.020</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3254" target="_blank">00:54:14.020</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=2x0ktXabsJA&t=3256" target="_blank">00:54:16.020</a></span> | <span class="t">Thank you.</span></div></div></body></html>