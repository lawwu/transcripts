<html><head><title>The GPGPU developer experience has a long way to go</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>The GPGPU developer experience has a long way to go</h2><a href="https://www.youtube.com/watch?v=0rEOjih_hps"><img src="https://i.ytimg.com/vi/0rEOjih_hps/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=105">1:45</a> But behind the scenes these libraries just call<br><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=138">2:18</a> There are many things Python programmers can't easily do<br><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=308">5:8</a> There are various hacks to try to handle these shortcomings<br><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=487">8:7</a> JAX is an interesting new approach<br><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=578">9:38</a> JAX shares some of the same constraints, and has some of its own<br><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=704">11:44</a> Julia can create, debug, and profile GPU kernels directly<br><br><div style="text-align: left;"><a href="./0rEOjih_hps.html">Whisper Transcript</a> | <a href="./transcript_0rEOjih_hps.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">>> Hi, everybody. I want to talk about my personal opinions about the GPGPU developer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=8" target="_blank">00:00:08.560</a></span> | <span class="t">experience. I feel like we don't talk about developer experience enough. When we talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=13" target="_blank">00:00:13.760</a></span> | <span class="t">about GPGPU, we tend to focus more on performance issues and distributed computing and stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=21" target="_blank">00:00:21.320</a></span> | <span class="t">like that. I know a lot of the audience here is from an academic background, and so folks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=28" target="_blank">00:00:28.160</a></span> | <span class="t">who focus on GPGPU in academia may not fully realize how incredibly popular GPGPU has become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=36" target="_blank">00:00:36.880</a></span> | <span class="t">in the last few years. To give you a sense, this is the downloads for CUDA Toolkit from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=46" target="_blank">00:00:46.440</a></span> | <span class="t">just one source, which is from the Anaconda Python repository. As you can see, 11.3 has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=54" target="_blank">00:00:54.800</a></span> | <span class="t">1.1 million downloads, 11.4, 1.1 million downloads, 11.1 million downloads. We've got to a point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=61" target="_blank">00:01:01.880</a></span> | <span class="t">now where literally over a million people are downloading CUDA. So what are all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=70" target="_blank">00:01:10.020</a></span> | <span class="t">people doing? They are not writing CUDA kernels. If you look at the Kaggle developer survey,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=78" target="_blank">00:01:18.800</a></span> | <span class="t">most developers are now better scientists, are now using things like TensorFlow and PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=88" target="_blank">00:01:28.480</a></span> | <span class="t">and Lightning and fast AI. So GPGPU is being used extremely extensively around the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=95" target="_blank">00:01:35.400</a></span> | <span class="t">now through these higher-level libraries and nearly always via Python. But the thing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=104" target="_blank">00:01:44.920</a></span> | <span class="t">that these libraries like PyTorch, behind the scenes, they are calling compiled C libraries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=111" target="_blank">00:01:51.520</a></span> | <span class="t">such as deep learning or the PyTorch C++ library or the C and C++ library. Although the Python</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=122" target="_blank">00:02:02.960</a></span> | <span class="t">developer is working in Python, there's a point at which they can't easily dig any deeper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=130" target="_blank">00:02:10.720</a></span> | <span class="t">because it's jumping into compiled code. And in the case of things like WhoDNM, it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=135" target="_blank">00:02:15.600</a></span> | <span class="t">even open source code. So what's the issue? Well, the issue is that for Python programmers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=144" target="_blank">00:02:24.680</a></span> | <span class="t">there's things that they either can't do it all or can't do conveniently. So because it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=151" target="_blank">00:02:31.240</a></span> | <span class="t">ends up being turned into these really very big C libraries or precompiled libraries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=160" target="_blank">00:02:40.560</a></span> | <span class="t">edge deployment can be very difficult. For example, when you install PyTorch, you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=166" target="_blank">00:02:46.200</a></span> | <span class="t">actually installing over a gigabyte. It's an over a gigabyte download. And trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=176" target="_blank">00:02:56.520</a></span> | <span class="t">turn your Python code into something that you can then put onto a mobile phone or a Raspberry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=181" target="_blank">00:03:01.800</a></span> | <span class="t">Pi or whatever is incredibly challenging. But from a developer experience point of view,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=188" target="_blank">00:03:08.480</a></span> | <span class="t">it's actually very difficult to debug your work because Python programmers are used to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=195" target="_blank">00:03:15.800</a></span> | <span class="t">using the Python debugger, but most of the real works that's being done in your code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=201" target="_blank">00:03:21.040</a></span> | <span class="t">is not happening in Python. It's happening in these lower level libraries. So trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=206" target="_blank">00:03:26.440</a></span> | <span class="t">to understand what's really going on is extremely challenging. Same problem for profiling. Obviously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=211" target="_blank">00:03:31.800</a></span> | <span class="t">we all want our code to run fast. And that's challenging to do when you can't easily just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=221" target="_blank">00:03:41.760</a></span> | <span class="t">use your Python profiler to jump in and see what's going on, where the holdups, how do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=226" target="_blank">00:03:46.480</a></span> | <span class="t">I make it faster? A lot of people think that it's not important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=234" target="_blank">00:03:54.800</a></span> | <span class="t">when I speak to people. They say it's not important that Python programmers can kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=238" target="_blank">00:03:58.840</a></span> | <span class="t">of dig into the underlying kernels and understand them and debug them and customize them. Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=248" target="_blank">00:04:08.400</a></span> | <span class="t">Python programmers are happy working at these higher levels. But actually this is a big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=253" target="_blank">00:04:13.040</a></span> | <span class="t">challenge. Because realistically, whether you're doing research or production in industry,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=261" target="_blank">00:04:21.080</a></span> | <span class="t">at some point you want to dive in and change things. And in my experience most of the time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=268" target="_blank">00:04:28.760</a></span> | <span class="t">there's something I would like to try and change that's buried down inside one of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=273" target="_blank">00:04:33.960</a></span> | <span class="t">precompiled libraries. Also as an educator, it's very hard for me to teach people what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=279" target="_blank">00:04:39.240</a></span> | <span class="t">going on. Because I can't show them the actual code that's really running behind the scenes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=286" target="_blank">00:04:46.480</a></span> | <span class="t">And so for understanding the implementation details, whether it's for an educational reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=291" target="_blank">00:04:51.360</a></span> | <span class="t">or because you want to understand how the algorithm works to think about how you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=296" target="_blank">00:04:56.560</a></span> | <span class="t">improve it, this is either impossible or extremely difficult. And this kind of hackability is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=304" target="_blank">00:05:04.440</a></span> | <span class="t">critical for the developer experience, in my opinion. So there's various hacks to try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=311" target="_blank">00:05:11.640</a></span> | <span class="t">and handle these deficiencies. So for example PyTorch now has a specialized profiler just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=319" target="_blank">00:05:19.800</a></span> | <span class="t">for profiling PyTorch. NVIDIA has a specialized profiler as well. These are really neat tools</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=326" target="_blank">00:05:26.600</a></span> | <span class="t">and it's really cool that they're being provided for free. But the fact is that it's still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=332" target="_blank">00:05:32.800</a></span> | <span class="t">not a great developer experience to have to learn a whole new tool which works in a different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=337" target="_blank">00:05:37.480</a></span> | <span class="t">way and that's not actually giving you a consistent view of all of your code. For edge deployment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=350" target="_blank">00:05:50.240</a></span> | <span class="t">or even sometimes a web hosting, there are hacks like in particular tracing and a just-in-time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=356" target="_blank">00:05:56.840</a></span> | <span class="t">compiler that are provided by both TensorFlow and PyTorch. So the idea is that you use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=366" target="_blank">00:06:06.360</a></span> | <span class="t">JIT or the tracing mechanism to basically turn your Python code into, you know, basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=377" target="_blank">00:06:17.040</a></span> | <span class="t">some code in a different form. In particular it's likely to be ONNX, which is kind of an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=382" target="_blank">00:06:22.520</a></span> | <span class="t">open standard for sharing these kind of models. The problem is that Python is a really rich</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=391" target="_blank">00:06:31.960</a></span> | <span class="t">and dynamic language. So in either of these cases, they're not capable of handling all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=399" target="_blank">00:06:39.880</a></span> | <span class="t">of the things that Python can do. So for example, in the case of the PyTorch just-in-time compiler,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=406" target="_blank">00:06:46.640</a></span> | <span class="t">there's all kinds of things where it's just going to give you an error and say I'm sorry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=409" target="_blank">00:06:49.400</a></span> | <span class="t">I don't know how to do that. More frustrating for me I find is that very often it does something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=415" target="_blank">00:06:55.400</a></span> | <span class="t">slightly different to how Python works and it's then very difficult to know why did it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=420" target="_blank">00:07:00.800</a></span> | <span class="t">work in Python and it didn't work when I compiled it to ONNX. Another very interesting technology</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=429" target="_blank">00:07:09.520</a></span> | <span class="t">is XLA, which comes out of Google and is now available as a back end for both TensorFlow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=435" target="_blank">00:07:15.520</a></span> | <span class="t">and PyTorch. It's a similar kind of idea to the PyTorch JIT, but it's something which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=447" target="_blank">00:07:27.060</a></span> | <span class="t">is specifically designed around creating a really accelerated fast version of your code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=455" target="_blank">00:07:35.200</a></span> | <span class="t">And so nowadays it's used, for example, when PyTorch wants to talk to a TPU, it will go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=461" target="_blank">00:07:41.360</a></span> | <span class="t">through the XLA compiler because that's the best way to create TPU code at this stage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=468" target="_blank">00:07:48.520</a></span> | <span class="t">through XLA. So these are all nice to have, but they, you know, have a lot of shortcomings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=476" target="_blank">00:07:56.700</a></span> | <span class="t">that's not nearly as convenient and not nearly as good a developer experience as using just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=484" target="_blank">00:08:04.040</a></span> | <span class="t">Python and using the Python tools that Python programmers are familiar with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=490" target="_blank">00:08:10.540</a></span> | <span class="t">Another very interesting new approach is JAX. JAX is another Google project and it's also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=499" target="_blank">00:08:19.640</a></span> | <span class="t">a Python library, but it's actually specifically designed to bring Python over to XLA. So it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=510" target="_blank">00:08:30.480</a></span> | <span class="t">written from the ground up for XLA. And what's particularly interesting about JAX is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=515" target="_blank">00:08:35.200</a></span> | <span class="t">you can kind of write your own kernels. So you're not as limited as you are with tracing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=525" target="_blank">00:08:45.800</a></span> | <span class="t">and JIT approaches. You're still limited to doing just the stuff that your underlying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=531" target="_blank">00:08:51.920</a></span> | <span class="t">seed or CUDA or whatever library has written for you, or else with JAX you can do a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=539" target="_blank">00:08:59.520</a></span> | <span class="t">more stuff. There's a lot more flexibility. And so this is very interesting approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=546" target="_blank">00:09:06.160</a></span> | <span class="t">But we still have the problem that the code that's running on the accelerator is not the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=553" target="_blank">00:09:13.120</a></span> | <span class="t">code you wrote. It's a transformation of that code through XLA. And so again, profiling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=559" target="_blank">00:09:19.160</a></span> | <span class="t">it and debugging it and understanding really what's going on is difficult. Also, in order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=564" target="_blank">00:09:24.400</a></span> | <span class="t">to provide these composable transformations, JAX has a very -- I mean, it's very interesting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=571" target="_blank">00:09:31.960</a></span> | <span class="t">but in some ways a very limited programming model. It's highly functional and immutable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=577" target="_blank">00:09:37.720</a></span> | <span class="t">And so JAX ends up with this kind of complexity from this functional programming model. State</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=583" target="_blank">00:09:43.160</a></span> | <span class="t">management becomes difficult. Things like random number generation becomes particularly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=589" target="_blank">00:09:49.200</a></span> | <span class="t">challenging. And obviously, in my world of machine learning and deep learning, random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=593" target="_blank">00:09:53.680</a></span> | <span class="t">numbers are very important as they are in many other GPU areas. So I feel like these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=600" target="_blank">00:10:00.560</a></span> | <span class="t">are all, like, amazing technologies. So much impressive work going on. But it doesn't feel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=608" target="_blank">00:10:08.480</a></span> | <span class="t">like, you know, the really long-term solutions. I don't see how any of these things quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=614" target="_blank">00:10:14.680</a></span> | <span class="t">end up giving us the developer experience we'd like to be able to offer. Another very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=621" target="_blank">00:10:21.560</a></span> | <span class="t">interesting technology I wanted to mention is TVM. So TVM is an Apache project nowadays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=627" target="_blank">00:10:27.720</a></span> | <span class="t">And you can use TVM directly from Python. And you basically end up creating these compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=635" target="_blank">00:10:35.040</a></span> | <span class="t">expressions. In this case, using a lambda. And if you're familiar with something like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=641" target="_blank">00:10:41.440</a></span> | <span class="t">Halide, similar kind of idea, you can basically create a schedule which will figure out how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=648" target="_blank">00:10:48.320</a></span> | <span class="t">to -- where you can show various ways that you think it might be best run on an accelerator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=656" target="_blank">00:10:56.240</a></span> | <span class="t">And in this case, you're actually binding axes to blocks and threads on the accelerator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=663" target="_blank">00:11:03.240</a></span> | <span class="t">This is a super convenient way to write kernels. And more importantly, perhaps, it also has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=669" target="_blank">00:11:09.560</a></span> | <span class="t">things like auto schedulers. So this is how you can create things that run as fast as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=676" target="_blank">00:11:16.560</a></span> | <span class="t">2DNN or, you know, specialized linear algebra libraries from Nvidia or whatever without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=683" target="_blank">00:11:23.360</a></span> | <span class="t">having to write all that, you know, unrolled loops and memory management and whatnot. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=690" target="_blank">00:11:30.360</a></span> | <span class="t">as you can see in the end, it's still not anywhere near as convenient as writing normal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=696" target="_blank">00:11:36.080</a></span> | <span class="t">Python. And the thing you end up with is, you know, this kind of compiled code that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=700" target="_blank">00:11:40.800</a></span> | <span class="t">again has all the kind of developer experience issues I described before. Perhaps the most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=707" target="_blank">00:11:47.520</a></span> | <span class="t">interesting path for the future for me right now is Julia. Julia is a fairly new language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=717" target="_blank">00:11:57.920</a></span> | <span class="t">But what's really interesting from a GPU standpoint is it handles nearly all of the developer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=725" target="_blank">00:12:05.120</a></span> | <span class="t">experience problems I described. Nearly none of them exist in Julia. And the key thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=730" target="_blank">00:12:10.120</a></span> | <span class="t">is that in Julia, you can write kernels that look a lot like you would write in CUDA but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=737" target="_blank">00:12:17.640</a></span> | <span class="t">with less boilerplate. And you can do in parallelized operations. You can handle memory. That can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=749" target="_blank">00:12:29.840</a></span> | <span class="t">all be done in Julia. And so I think this is a really underappreciated important idea,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=759" target="_blank">00:12:39.080</a></span> | <span class="t">which is that developers should be able to use the same language and the same tools throughout</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=765" target="_blank">00:12:45.320</a></span> | <span class="t">the hierarchy of abstractions in their program. Again, speaking as an educator, this is incredibly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=771" target="_blank">00:12:51.600</a></span> | <span class="t">important for teaching people what's going on. It's really important for a researcher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=778" target="_blank">00:12:58.920</a></span> | <span class="t">because you can hack in at any level. It's really important in industry because you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=783" target="_blank">00:13:03.440</a></span> | <span class="t">ensure that you can jump in and make sure the performance is working properly for you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=790" target="_blank">00:13:10.360</a></span> | <span class="t">at every level. And it also opens up the research world in such a way that things aren't off</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=800" target="_blank">00:13:20.480</a></span> | <span class="t">the table. I find that the things that get worked on in deep learning research are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=804" target="_blank">00:13:24.880</a></span> | <span class="t">things that are kind of conveniently accessible through libraries. And a lot of stuff that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=812" target="_blank">00:13:32.400</a></span> | <span class="t">isn't has just not really been touched because it requires people to go in and write their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=816" target="_blank">00:13:36.700</a></span> | <span class="t">own CUDA kernels. And very, very few people have the patience to do that, at least in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=823" target="_blank">00:13:43.280</a></span> | <span class="t">the deep learning world. So yeah, really, I guess this is a bit of a play for the GPGPU</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=836" target="_blank">00:13:56.020</a></span> | <span class="t">community to consider building the next generation of languages and tools, which allows developers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=849" target="_blank">00:14:09.960</a></span> | <span class="t">to really do everything that they might want to do in a convenient way. For Julia, I feel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=855" target="_blank">00:14:15.800</a></span> | <span class="t">like there's a lot of gaps in the developer experience there more generally, which I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=861" target="_blank">00:14:21.960</a></span> | <span class="t">the community is very familiar with around deployment and the amount of memory use it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=867" target="_blank">00:14:27.000</a></span> | <span class="t">requires and the amount of latency it requires to start up and so forth. But I do think at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=872" target="_blank">00:14:32.680</a></span> | <span class="t">least with Julia, it feels like something that there's a path there that could eventually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=878" target="_blank">00:14:38.800</a></span> | <span class="t">lead to a really beautiful developer experience. And that's not a path that I see available</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=885" target="_blank">00:14:45.160</a></span> | <span class="t">in really any of the Python frameworks that I see right now. And I would love to see things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=893" target="_blank">00:14:53.400</a></span> | <span class="t">like TVM being more integrated with those ideas into languages and tools. So yeah, that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=902" target="_blank">00:15:02.720</a></span> | <span class="t">the end of my thoughts on that. Thanks very much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0rEOjih_hps&t=904" target="_blank">00:15:04.960</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>