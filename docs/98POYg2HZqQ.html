<html><head><title>OpenAI's CLIP for Zero Shot Image Classification</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>OpenAI's CLIP for Zero Shot Image Classification</h2><a href="https://www.youtube.com/watch?v=98POYg2HZqQ"><img src="https://i.ytimg.com/vi_webp/98POYg2HZqQ/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=234">3:54</a> Zero Shot Classification<br><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=377">6:17</a> How Clip Makes Zero Shot Learning So Effective<br><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=473">7:53</a> Pre-Training<br><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=977">16:17</a> Image Embeddings<br><br><div style="text-align: left;"><a href="./98POYg2HZqQ.html">Whisper Transcript</a> | <a href="./transcript_98POYg2HZqQ.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Today we're going to talk about how to use CLIP for zero shot image classification. That is image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=6" target="_blank">00:00:06.960</a></span> | <span class="t">classification without needing to fine-tune your model on a particular image data set. For me this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=15" target="_blank">00:00:15.600</a></span> | <span class="t">is one of the most interesting use cases that really demonstrates the power of these multimodal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=22" target="_blank">00:00:22.560</a></span> | <span class="t">models. Those are models that understand different domains of information like for CLIP for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=29" target="_blank">00:00:29.120</a></span> | <span class="t">that is image and text. But how about other state-of-the-art computer vision models? How do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=34" target="_blank">00:00:34.800</a></span> | <span class="t">they perform in comparison? Well these other computer vision models they are characterized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=40" target="_blank">00:00:40.880</a></span> | <span class="t">by the fact that they really focus on one thing in an image. So for example let's say we had a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=48" target="_blank">00:00:48.320</a></span> | <span class="t">computer vision model that was trained to classify an image as to whether it contained a dog,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=55" target="_blank">00:00:55.280</a></span> | <span class="t">a car, or a bird. Okay that computer vision model doesn't really need to know anything other than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=61" target="_blank">00:01:01.760</a></span> | <span class="t">what a car looks like, what a dog looks like, and what a bird looks like. It can ignore everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=66" target="_blank">00:01:06.880</a></span> | <span class="t">else in those images and that's one of the limitations of these state-of-the-art computer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=72" target="_blank">00:01:12.560</a></span> | <span class="t">vision models. They have been pre-trained on a huge amount of data but they've been pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=78" target="_blank">00:01:18.240</a></span> | <span class="t">for classification of these particular elements and even if you have like a thousand or ten</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=84" target="_blank">00:01:24.160</a></span> | <span class="t">thousand classes in there that hardly represents the real world which has a huge variety of concepts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=91" target="_blank">00:01:31.520</a></span> | <span class="t">and objects for us to as humans to to understand and see and categorize. So what we find with these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=100" target="_blank">00:01:40.080</a></span> | <span class="t">computer vision models is that they perform very well on specific data sets but they're not so good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=110" target="_blank">00:01:50.080</a></span> | <span class="t">at handling new classes for example. For that model to understand a new class of objects and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=117" target="_blank">00:01:57.760</a></span> | <span class="t">images it will need to be fine-tuned further and it's going to need a lot of data and it's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=124" target="_blank">00:02:04.000</a></span> | <span class="t">it's not very easy to do. Ideally we want a computer vision model to just understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=128" target="_blank">00:02:08.400</a></span> | <span class="t">kind of not everything would be perfect obviously we're not quite there yet but if it can just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=134" target="_blank">00:02:14.320</a></span> | <span class="t">understand the world and the visual world relatively well then we're sort of on the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=140" target="_blank">00:02:20.800</a></span> | <span class="t">to a more robust model. So for example this image of a dog we want the model to understand that dog</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=147" target="_blank">00:02:27.200</a></span> | <span class="t">is in the image and a you know usually use convolutional neural network that sort of model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=152" target="_blank">00:02:32.800</a></span> | <span class="t">for image classification will know that there is a dog in the image but it doesn't it doesn't care</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=156" target="_blank">00:02:36.880</a></span> | <span class="t">about everything else in the image. Ideally we want it to understand that there are trees in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=160" target="_blank">00:02:40.320</a></span> | <span class="t">the background the dog is running towards the camera it's on a grassy field it's sunrise or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=165" target="_blank">00:02:45.280</a></span> | <span class="t">sunset and that there are some blurry trees in the background the blue street. Unfortunately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=171" target="_blank">00:02:51.120</a></span> | <span class="t">classification training we don't get that. Instead the models are essentially just learning to push</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=177" target="_blank">00:02:57.200</a></span> | <span class="t">their internal representations of images with dogs in all towards the same sort of vector space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=184" target="_blank">00:03:04.160</a></span> | <span class="t">That's essentially how we can we can think about this and then for example earlier I said cars and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=189" target="_blank">00:03:09.760</a></span> | <span class="t">birds as well we could imagine so they have like a cluster of dog images they have a cluster of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=194" target="_blank">00:03:14.080</a></span> | <span class="t">bird images and they have a cluster of car images but they don't really have anything else in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=199" target="_blank">00:03:19.360</a></span> | <span class="t">between that. So this is ideal if we just want a yes or no answer for a specific data set and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=204" target="_blank">00:03:24.800</a></span> | <span class="t">want good performance we can do that as long as we have the training data and the compute and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=209" target="_blank">00:03:29.200</a></span> | <span class="t">time to actually do that. However if we don't have all of that and we just want good performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=216" target="_blank">00:03:36.720</a></span> | <span class="t">maybe not state-of-the-art but good performance across a whole range of data sets that's where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=221" target="_blank">00:03:41.600</a></span> | <span class="t">we use CLIP. CLIP has proved itself as incredibly flexible model that can work in both text and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=231" target="_blank">00:03:51.040</a></span> | <span class="t">images and is amazing at what something we call zero shot classification. Zero shot basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=238" target="_blank">00:03:58.080</a></span> | <span class="t">saying you need zero training examples for this model to adapt to a new domain. So before we dive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=245" target="_blank">00:04:05.120</a></span> | <span class="t">into CLIP let's just explain this zero shot thing in a little more detail. So zero shot comes from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=251" target="_blank">00:04:11.440</a></span> | <span class="t">something called end shot learning. End shot you may have guessed is basically the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=256" target="_blank">00:04:16.320</a></span> | <span class="t">of training examples that you need for your model to perform on a particular new domain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=261" target="_blank">00:04:21.280</a></span> | <span class="t">on your data set. Many state-of-the-art image classification models they tend to be pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=267" target="_blank">00:04:27.120</a></span> | <span class="t">on like ImageNet and then they're fine-tuned for a specific task so that they have the pre-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=272" target="_blank">00:04:32.960</a></span> | <span class="t">and the pre-training basically sets up the internal model ways of that model to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=278" target="_blank">00:04:38.640</a></span> | <span class="t">the visual world at least within the scope of the ImageNet classification set which is fairly big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=286" target="_blank">00:04:46.800</a></span> | <span class="t">but it's obviously not as big as the actual world and then those models are usually fine-tuned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=294" target="_blank">00:04:54.400</a></span> | <span class="t">on a particular data set and to fine-tune that pre-trained image classification model on a new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=302" target="_blank">00:05:02.560</a></span> | <span class="t">domain you are going to need a lot of examples. Let's say as a rule of thumb maybe you need 10,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=311" target="_blank">00:05:11.680</a></span> | <span class="t">images for each class or each label within your data set. That may be excessive it may be too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=318" target="_blank">00:05:18.880</a></span> | <span class="t">little I'm not sure but you do need something within that ballpark in order to get good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=325" target="_blank">00:05:25.760</a></span> | <span class="t">performance. We could refer to these models so these are like ResNet and BERT as many shot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=332" target="_blank">00:05:32.960</a></span> | <span class="t">learners they need many many training examples in order to learn a new domain. Ideally we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=339" target="_blank">00:05:39.040</a></span> | <span class="t">maximize model performance whilst minimizing the n in n shot okay so minimizing the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=346" target="_blank">00:05:46.800</a></span> | <span class="t">training examples needed for the model to perform well. Now so as I was noting that CLIP is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=352" target="_blank">00:05:52.560</a></span> | <span class="t">achieving state-of-the-art performance on any particular data sets or benchmarks other than one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=359" target="_blank">00:05:59.440</a></span> | <span class="t">surprisingly without seeing any training data for this particular data set CLIP did actually get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=366" target="_blank">00:06:06.480</a></span> | <span class="t">state-of-the-art performance on that one data set which is surprising without seeing any of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=372" target="_blank">00:06:12.320</a></span> | <span class="t">training data but here we go this is this is how useful this sort of thing is. Let's talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=378" target="_blank">00:06:18.640</a></span> | <span class="t">how CLIP makes zero-shot learning so effective. So CLIP stands for Contrastive Language Image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=384" target="_blank">00:06:24.480</a></span> | <span class="t">Pre-training it was released by OpenAI in 2021 and since then it has done pretty well we can find it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=393" target="_blank">00:06:33.120</a></span> | <span class="t">in a lot of different use cases this is just one of them. So CLIP itself actually consists of two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=397" target="_blank">00:06:37.760</a></span> | <span class="t">models I've discussed this in a previous video and article in a lot more detail so if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=404" target="_blank">00:06:44.480</a></span> | <span class="t">interested go and have a look at that for now we're going to keep things pretty light on how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=410" target="_blank">00:06:50.160</a></span> | <span class="t">CLIP works but in this version of CLIP those two models are going to consist of a typical text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=417" target="_blank">00:06:57.040</a></span> | <span class="t">transformer model for dealing with the text encoding and a vision transformer model for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=424" target="_blank">00:07:04.480</a></span> | <span class="t">dealing with the image encoding. Both of these models within CLIP are optimized during training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=430" target="_blank">00:07:10.800</a></span> | <span class="t">in order to encode similar text and image pairs into the same vector space whilst also separating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=439" target="_blank">00:07:19.360</a></span> | <span class="t">dissimilar text and image pairs so they are further away in vector space so essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=444" target="_blank">00:07:24.160</a></span> | <span class="t">in that vector space similar items are together whether they are images or text. Now CLIP</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=450" target="_blank">00:07:30.800</a></span> | <span class="t">distinguishes itself from typical image classification models for a few reasons first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=457" target="_blank">00:07:37.520</a></span> | <span class="t">it isn't trained for image classification and it was also trained on a very big data set of 400</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=464" target="_blank">00:07:44.080</a></span> | <span class="t">million image to text pairs with this contrastive learning approach. So from this we get a few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=470" target="_blank">00:07:50.960</a></span> | <span class="t">a few benefits first for actually pre-training the models training the model CLIP only requires image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=478" target="_blank">00:07:58.400</a></span> | <span class="t">to text pairs which in today's age of social media they're pretty easy to get any post on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=486" target="_blank">00:08:06.160</a></span> | <span class="t">Instagram for example there's a image and there's usually a little caption of someone describing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=490" target="_blank">00:08:10.640</a></span> | <span class="t">what is in the image we have stock photo websites social media you know just everything everywhere</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=496" target="_blank">00:08:16.480</a></span> | <span class="t">we have images and text usually tied together so there's a lot of data for us to to pull that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=502" target="_blank">00:08:22.880</a></span> | <span class="t">Because of the large data set sizes that we can use with CLIP, CLIP is able to get a really good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=510" target="_blank">00:08:30.160</a></span> | <span class="t">general understanding of the concepts between language and images and just a general</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=516" target="_blank">00:08:36.560</a></span> | <span class="t">understanding of the world through these two modalities and as well within these pairs the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=523" target="_blank">00:08:43.360</a></span> | <span class="t">text descriptions often describe the image not not just one part of the image like okay there's a dog</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=530" target="_blank">00:08:50.320</a></span> | <span class="t">in the image but something else like dog is running in a in a grassy field okay they describe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=537" target="_blank">00:08:57.440</a></span> | <span class="t">something more and sometimes even describe very abstract things like the sort of feeling or mood</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=542" target="_blank">00:09:02.240</a></span> | <span class="t">of the photo so you get a lot more information from these image text pairs than you do with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=548" target="_blank">00:09:08.080</a></span> | <span class="t">typical classification data set and it's these three benefits of CLIP that have led to its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=554" target="_blank">00:09:14.640</a></span> | <span class="t">pretty outstanding zero shot performance across a huge number of data sets. Now the authors of CLIP</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=561" target="_blank">00:09:21.200</a></span> | <span class="t">in the original CLIP paper they draw a really good example using CLIP and the ResNet101 model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=570" target="_blank">00:09:30.560</a></span> | <span class="t">trained for ImageNet classification. Now CLIP was not trained specifically for ImageNet classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=577" target="_blank">00:09:37.840</a></span> | <span class="t">but they showed that zero shot performance with CLIP versus this state of the art model trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=584" target="_blank">00:09:44.400</a></span> | <span class="t">for ImageNet was comparable on the actual ImageNet data set and then we when we compare them on other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=592" target="_blank">00:09:52.480</a></span> | <span class="t">data sets that are derived from ImageNet so you have ImageNet V2, ImageNet R, ObjectNet, ImageNet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=599" target="_blank">00:09:59.200</a></span> | <span class="t">Sketch and ImageNet A, CLIP outperforms the model that was specifically trained for ImageNet on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=606" target="_blank">00:10:06.720</a></span> | <span class="t">every single one of those data sets which is really impressive. Okay let's talk about how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=613" target="_blank">00:10:13.600</a></span> | <span class="t">CLIP is actually doing zero shot classification and how we can use it for that as well. So CLIP</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=620" target="_blank">00:10:20.480</a></span> | <span class="t">well the two models within CLIP they both output a 512 dimensional vector. Now the text encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=628" target="_blank">00:10:28.560</a></span> | <span class="t">it can consume any piece of text right and then it will output a vector representation of that text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=636" target="_blank">00:10:36.160</a></span> | <span class="t">within sort of CLIP vector space. Then if you compare that text to an image also encoded with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=643" target="_blank">00:10:43.920</a></span> | <span class="t">CLIP what you should find is that text and images that are more similar are closer together. So now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=651" target="_blank">00:10:51.520</a></span> | <span class="t">imagine we do that but instead of so we have our images from an image classification data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=659" target="_blank">00:10:59.840</a></span> | <span class="t">and then for the text we actually feed in the class labels for that classification task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=666" target="_blank">00:11:06.720</a></span> | <span class="t">Then you process all that and then calculate similarity between the the outputs and whichever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=673" target="_blank">00:11:13.680</a></span> | <span class="t">of your text embeddings has a high similarity to each image that is like your class okay your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=681" target="_blank">00:11:21.520</a></span> | <span class="t">predicted class. Okay so let's move on to an actual applied example and implementation of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=688" target="_blank">00:11:28.560</a></span> | <span class="t">zero shot learning with CLIP. Okay so to start we will need to pip install datasets torch and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=695" target="_blank">00:11:35.280</a></span> | <span class="t">transformers and what we're going to do is download a dataset. So this is the frgfm image net dataset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=705" target="_blank">00:11:45.040</a></span> | <span class="t">we've used this a couple of times before and it just contains 10 different classes not too much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=712" target="_blank">00:11:52.080</a></span> | <span class="t">data here we're looking at a validation set. So we have just under 4000 items here and if we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=718" target="_blank">00:11:58.480</a></span> | <span class="t">a look at what we have in the labels feature so in the images image feature we obviously have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=723" target="_blank">00:12:03.520</a></span> | <span class="t">images themselves label feature we have these 10 labels okay but they're just numbers they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=731" target="_blank">00:12:11.600</a></span> | <span class="t">integer values. We obviously need text for this to work with CLIP so we need to modify these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=738" target="_blank">00:12:18.480</a></span> | <span class="t">or we need to map these to the actual text labels. Now we do that by taking a look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=746" target="_blank">00:12:26.000</a></span> | <span class="t">hugging face dataset info features and then label names. Okay so most hugging face datasets will have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=753" target="_blank">00:12:33.840</a></span> | <span class="t">a format similar to this where you can find extra data set information like the label names. Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=760" target="_blank">00:12:40.640</a></span> | <span class="t">and then from there we can see we have tench english springer cassette player you know a few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=765" target="_blank">00:12:45.760</a></span> | <span class="t">different things all of these map directly to the values here. So for zero we'd have tench one we'd</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=772" target="_blank">00:12:52.720</a></span> | <span class="t">have english springer and so on. So as before we're going to convert these into sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=779" target="_blank">00:12:59.680</a></span> | <span class="t">So a photo of a tench photo of a english springer and so on and so on. Okay so from here before we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=786" target="_blank">00:13:06.640</a></span> | <span class="t">can compare the labels and the images we actually need CLIP. So we can initialize CLIP through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=794" target="_blank">00:13:14.080</a></span> | <span class="t">hugging face so we use this model id and then we use model processor which is going to pre-process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=800" target="_blank">00:13:20.480</a></span> | <span class="t">our images and text and then we also click model here which is the actual model itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=805" target="_blank">00:13:25.680</a></span> | <span class="t">And then we can also run it on CUDA if you have a CUDA enabled GPU. For me I'm just running this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=812" target="_blank">00:13:32.960</a></span> | <span class="t">on Mac so CPU. NPS as far as I know it's not supported in full or CLIP is not supported in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=821" target="_blank">00:13:41.520</a></span> | <span class="t">full with NPS yet. So that's like the the Mac M1 version of CUDA. For now CPU is fast enough it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=829" target="_blank">00:13:49.520</a></span> | <span class="t">not it's not slow so it's not a problem. Now one thing here is that text transformers don't read</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=836" target="_blank">00:13:56.320</a></span> | <span class="t">text directly like we do. They need like a translation from text into what are called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=843" target="_blank">00:14:03.760</a></span> | <span class="t">input ids or token ids which are just integer representations of either words or sub words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=851" target="_blank">00:14:11.680</a></span> | <span class="t">from the original text. So we do that with the processor here it's passing our text padding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=856" target="_blank">00:14:16.960</a></span> | <span class="t">we set to true so that everything is the same size we need this when we're running things in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=861" target="_blank">00:14:21.520</a></span> | <span class="t">parallel multiple inputs in parallel essentially when we're using batches. We're not passing our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=867" target="_blank">00:14:27.440</a></span> | <span class="t">images here and we're going to return PyTorch tensors. Okay and we're going to move all that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=873" target="_blank">00:14:33.600</a></span> | <span class="t">to our device for me it's just CPU so it doesn't actually matter but it's fine. And then here we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=878" target="_blank">00:14:38.240</a></span> | <span class="t">can see those tokens so we have a starter sequence token here and then we could imagine this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=884" target="_blank">00:14:44.160</a></span> | <span class="t">something like a photo of a tench something along those lines and then end of sequence over there as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=891" target="_blank">00:14:51.760</a></span> | <span class="t">well. So we can encode these tokens into sentence embeddings all we do is this so pass our sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=898" target="_blank">00:14:58.240</a></span> | <span class="t">our sorry our tokens in here label tokens now in here we have input ids and also another tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=905" target="_blank">00:15:05.760</a></span> | <span class="t">called attention mask and that's kind of wrapped within a dictionary which is why we're using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=910" target="_blank">00:15:10.960</a></span> | <span class="t">these two asterisks here to iteratively pass both of those tensors as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=915" target="_blank">00:15:15.600</a></span> | <span class="t">individual items to the get text features function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=920" target="_blank">00:15:20.320</a></span> | <span class="t">And then after we have our label embeddings over here we just want to detach them from PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=927" target="_blank">00:15:27.200</a></span> | <span class="t">gradient computation of the model and convert that into NumPy and then we can see from that that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=932" target="_blank">00:15:32.960</a></span> | <span class="t">get 10 512 dimensional embeddings. Okay so they're now the text embeddings within that click vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=941" target="_blank">00:15:41.120</a></span> | <span class="t">space and one thing to note here is that they're not normalized okay we can see they're not normalized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=946" target="_blank">00:15:46.960</a></span> | <span class="t">so we can either use cosine similarity to compare them or we can normalize them and then we can use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=954" target="_blank">00:15:54.240</a></span> | <span class="t">dot product similarity. Now if we normalize first I find the code later on to be simpler so we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=961" target="_blank">00:16:01.440</a></span> | <span class="t">do that here so we're going to normalize here it's pretty simple and then we can see straight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=966" target="_blank">00:16:06.960</a></span> | <span class="t">away they're normalized we can just use dot product similarity now. That's the text embedding or the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=973" target="_blank">00:16:13.120</a></span> | <span class="t">label embedding part now what we want to do is have a look at how we do the image embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=978" target="_blank">00:16:18.880</a></span> | <span class="t">and then how we compare them. So we're going to start with this image first just a single image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=984" target="_blank">00:16:24.240</a></span> | <span class="t">we'll go through the whole data set in full later so we just have a cassette recorder here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=989" target="_blank">00:16:29.600</a></span> | <span class="t">now we go down here we're just going to process the image so using the same processor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=993" target="_blank">00:16:33.520</a></span> | <span class="t">we set text to non because there's no text this time and what we want to do is just pass that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=998" target="_blank">00:16:38.480</a></span> | <span class="t">image in here to images and we're going to return tensors it's pytorch tensors and extract the pixel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1004" target="_blank">00:16:44.320</a></span> | <span class="t">values. Now the reason that we have to process the image is clip expects every tensor that it sees to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1010" target="_blank">00:16:50.880</a></span> | <span class="t">be normalized which is the first thing it does and also a particular shape it expects this shape here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1018" target="_blank">00:16:58.080</a></span> | <span class="t">so three color channels which is through here a 224 pixel wide image and 224 pixel height image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1028" target="_blank">00:17:08.480</a></span> | <span class="t">okay so all we're doing there normalization and resizing and then from there we can pass it to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1036" target="_blank">00:17:16.880</a></span> | <span class="t">image features image again like here we didn't we didn't include the iterable because this is just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1044" target="_blank">00:17:24.400</a></span> | <span class="t">single tensor we don't need to pass the the two asterisks here and we get a single embedding here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1051" target="_blank">00:17:31.600</a></span> | <span class="t">one vector which is 512 dimensions as with our label embeddings we're going to detach them move</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1057" target="_blank">00:17:37.360</a></span> | <span class="t">them to cpu and then convert to numpy i already have them cpu listed this part isn't necessary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1063" target="_blank">00:17:43.600</a></span> | <span class="t">but if you're using cuda it will be and then we don't need to normalize them so when we're doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1069" target="_blank">00:17:49.040</a></span> | <span class="t">dot product similarity we just need one side of the calculation to be normalized not both</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1074" target="_blank">00:17:54.560</a></span> | <span class="t">okay so with that we do numpy dot we have our image embedding and then we transpose label</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1081" target="_blank">00:18:01.360</a></span> | <span class="t">embeddings and you see that we get scores and the shape of those scores is one dimension that's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1087" target="_blank">00:18:07.280</a></span> | <span class="t">important here we have those 10 similarity scores so one similarity value for each of our 10 labels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1097" target="_blank">00:18:17.520</a></span> | <span class="t">okay so then we can take the index of the highest score which happens to be index two and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1104" target="_blank">00:18:24.320</a></span> | <span class="t">find out okay which which label is that it is cassette player okay so it's correct that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1111" target="_blank">00:18:31.200</a></span> | <span class="t">pretty cool now let's have a look how we do that for the whole data set so all i'm going to do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1116" target="_blank">00:18:36.560</a></span> | <span class="t">we're going to go through in a loop through the whole data set we're going to do in batches of 32</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1122" target="_blank">00:18:42.320</a></span> | <span class="t">process everything this is all just the same stuff okay process get the embeddings get dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1129" target="_blank">00:18:49.040</a></span> | <span class="t">okay we don't need to redo this for the labels because we we just have the 10 labels throughout</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1133" target="_blank">00:18:53.520</a></span> | <span class="t">the whole thing so it's not necessary we'll get the arg max and we're just going to append or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1139" target="_blank">00:18:59.440</a></span> | <span class="t">extend a prediction list with all of those predictions okay and let's see what we get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1148" target="_blank">00:19:08.800</a></span> | <span class="t">well let's see what the performance is there so here calculating how many of them align to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1154" target="_blank">00:19:14.000</a></span> | <span class="t">true values and we can see we get 0.987 okay so that means we get 98.7 accuracy which is pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1166" target="_blank">00:19:26.400</a></span> | <span class="t">insane when you consider that we have done no training for clip here it has not seen any of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1172" target="_blank">00:19:32.000</a></span> | <span class="t">these labels it has not seen any of these images this is like out of the box zero shot classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1179" target="_blank">00:19:39.840</a></span> | <span class="t">and it's scoring 98.7 accuracy which is i think really very very impressive so this is uh i think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1190" target="_blank">00:19:50.080</a></span> | <span class="t">a good example why i think zero shot classification or image classification with clip is such an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1197" target="_blank">00:19:57.040</a></span> | <span class="t">interesting use case and it's just so easy right you can do this for a whole ton of data sets and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1203" target="_blank">00:20:03.680</a></span> | <span class="t">get good performance it's not going to be state-of-the-art performance but pretty good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1207" target="_blank">00:20:07.840</a></span> | <span class="t">performance like like this super super easy so before clip i as far as i'm aware this sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1216" target="_blank">00:20:16.000</a></span> | <span class="t">thing wasn't possible okay every every domain adaption to a new classification task needed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1222" target="_blank">00:20:22.720</a></span> | <span class="t">training data it needed training and so on with this it's just a case of you need to write some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1229" target="_blank">00:20:29.680</a></span> | <span class="t">labels maybe modify them into sentences and then you're you're good to go so that's why i think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1237" target="_blank">00:20:37.440</a></span> | <span class="t">clip is i think it has created a pretty big leap forward in quite a few areas such as image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1245" target="_blank">00:20:45.680</a></span> | <span class="t">classification so when i think of clip there is in these in the short time that it's been around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1252" target="_blank">00:20:52.560</a></span> | <span class="t">we have multi-modal search now zero shot image classification object localization or image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1259" target="_blank">00:20:59.040</a></span> | <span class="t">localization object detection also zero shot and we'll go into that in more detail pretty soon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1265" target="_blank">00:21:05.200</a></span> | <span class="t">and even industry changing tools like openai's dali includes a clip model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1270" target="_blank">00:21:10.400</a></span> | <span class="t">stable diffusion as far as i know also includes a clip model so there's this massive range of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1276" target="_blank">00:21:16.800</a></span> | <span class="t">use cases that clip is being used for and i think that's super interesting so that's it for this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1282" target="_blank">00:21:22.400</a></span> | <span class="t">video i hope you have found all this as interesting as i do so now thank you very much for watching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=98POYg2HZqQ&t=1289" target="_blank">00:21:29.440</a></span> | <span class="t">and i will see you again in the next one bye</span></div></div></body></html>