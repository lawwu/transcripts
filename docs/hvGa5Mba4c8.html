<html><head><title>Direct Preference Optimization (DPO) explained: Bradley-Terry model, log probabilities, math</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Direct Preference Optimization (DPO) explained: Bradley-Terry model, log probabilities, math</h2><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8"><img src="https://i.ytimg.com/vi_webp/hvGa5Mba4c8/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=130">2:10</a> Intro to Language Models<br><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=248">4:8</a> AI Alignment<br><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=311">5:11</a> Intro to RL<br><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=499">8:19</a> RL for Language Models<br><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=644">10:44</a> Reward model<br><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=787">13:7</a> The Bradley-Terry model<br><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1294">21:34</a> Optimization Objective<br><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1792">29:52</a> DPO: deriving its loss<br><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2465">41:5</a> Computing the log probabilities<br><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2847">47:27</a> Conclusion<br><br><div style="text-align: left;"><a href="./hvGa5Mba4c8.html">Whisper Transcript</a> | <a href="./transcript_hvGa5Mba4c8.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys, welcome back to my channel. Today we are gonna talk about DPO, which stands</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=4" target="_blank">00:00:04.480</a></span> | <span class="t">for Direct Preference Optimization. It's a new technique that came out in the middle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=8" target="_blank">00:00:08.760</a></span> | <span class="t">of last year, in 2023, to align language models. Let's review the topics of today. I will start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=17" target="_blank">00:00:17.320</a></span> | <span class="t">with a short introduction to language models, as usual, so we can review how language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=21" target="_blank">00:00:21.700</a></span> | <span class="t">work. Then we will introduce the topic of AI alignment, so what we mean by AI alignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=27" target="_blank">00:00:27.660</a></span> | <span class="t">And then we will review reinforcement learning. Now, you may be wondering why are we reviewing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=31" target="_blank">00:00:31.780</a></span> | <span class="t">reinforcement learning if the whole point of DPO is to remove reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=36" target="_blank">00:00:36.340</a></span> | <span class="t">from language models. Well, the reason is that, actually, even if DPO does not use reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=42" target="_blank">00:00:42.220</a></span> | <span class="t">learning algorithms, they are still interconnected, especially when we talk about the reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=47" target="_blank">00:00:47.040</a></span> | <span class="t">and the predatory model. So in order to understand the reward model and the predatory model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=51" target="_blank">00:00:51.840</a></span> | <span class="t">we need to review reinforcement learning and how the reward model affected the process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=56" target="_blank">00:00:56.560</a></span> | <span class="t">in reinforcement learning from human feedback. In the last part of the video, we will derive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=62" target="_blank">00:01:02.760</a></span> | <span class="t">the DPO loss, so we will understand where does it come from. I will also show you the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=71" target="_blank">00:01:11.240</a></span> | <span class="t">code on how to compute the log probability, so how we actually can use this loss in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=78" target="_blank">00:01:18.440</a></span> | <span class="t">Now what are the prerequisites for watching this video? Well, for sure that you are familiar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=81" target="_blank">00:01:21.920</a></span> | <span class="t">with a little bit of probability and statistics, not much, for example, conditional probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=88" target="_blank">00:01:28.420</a></span> | <span class="t">We are familiar with deep learning, so what we mean by gradient descent and loss functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=93" target="_blank">00:01:33.900</a></span> | <span class="t">It's really great if you have watched my previous video on reinforcement learning from human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=97" target="_blank">00:01:37.400</a></span> | <span class="t">feedback in which I explain all the aspects of the reward model and the reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=102" target="_blank">00:01:42.760</a></span> | <span class="t">learning framework and the DPO. But it's not necessary for this video because I will review</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=109" target="_blank">00:01:49.040</a></span> | <span class="t">most of the part that are needed to understand the DPO, but it's really great if you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=113" target="_blank">00:01:53.000</a></span> | <span class="t">already watched that video so you can compare the two methods. And also that you're familiar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=118" target="_blank">00:01:58.960</a></span> | <span class="t">with the transformer model because we will be using it in practice when we want to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=123" target="_blank">00:02:03.360</a></span> | <span class="t">the log probabilities. Otherwise, we don't know how to use the loss of the DPO. Let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=129" target="_blank">00:02:09.440</a></span> | <span class="t">start our journey. So what is a language model? Well, a language model is a probabilistic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=135" target="_blank">00:02:15.240</a></span> | <span class="t">model that assigns the probabilities to a sequence of words. In practice, given a prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=141" target="_blank">00:02:21.440</a></span> | <span class="t">for example, a language model allow us, let me use the laser. So given a prompt, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=147" target="_blank">00:02:27.080</a></span> | <span class="t">Shanghai is a city in a language model tells us the probability of what is maybe the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=154" target="_blank">00:02:34.320</a></span> | <span class="t">token or word. Now, in my videos, I always make the simplification that a token is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=159" target="_blank">00:02:39.760</a></span> | <span class="t">word and the word is a token. This is actually not the case in most language models, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=164" target="_blank">00:02:44.520</a></span> | <span class="t">it's useful for explanation purposes. So what is the probability that the next token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=170" target="_blank">00:02:50.640</a></span> | <span class="t">is China or the next token is Beijing or the next token is cat or pizza given a particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=176" target="_blank">00:02:56.120</a></span> | <span class="t">prompt? This is the only thing that a language model does. And the language model gives us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=182" target="_blank">00:03:02.200</a></span> | <span class="t">this probability. Now, you may be wondering, how can we use this language model to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=186" target="_blank">00:03:06.840</a></span> | <span class="t">a text? Well, we do it with an iterative process. So we take a prompt, for example, where is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=193" target="_blank">00:03:13.080</a></span> | <span class="t">Shanghai? We give it to the language model. The language model will give us a list of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=196" target="_blank">00:03:16.920</a></span> | <span class="t">probabilities or what is the possible next word or token. Suppose that we choose the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=202" target="_blank">00:03:22.000</a></span> | <span class="t">token with the most, with the highest probability score. So suppose it's Shanghai. We take this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=207" target="_blank">00:03:27.880</a></span> | <span class="t">token, we select it and we put it back into the prompt and we ask again, the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=212" target="_blank">00:03:32.560</a></span> | <span class="t">model, what is the next token? Then the language model again will give us a list of probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=217" target="_blank">00:03:37.180</a></span> | <span class="t">over what is the possible next token. We select the one that we think is the most relevant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=222" target="_blank">00:03:42.960</a></span> | <span class="t">Usually we select the one that is most probable and we put it back into the prompt and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=229" target="_blank">00:03:49.720</a></span> | <span class="t">ask again, the language model, et cetera, et cetera, until we reach a specified number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=234" target="_blank">00:03:54.200</a></span> | <span class="t">of generated tokens or we reach the end of sentence token, which is a special token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=239" target="_blank">00:03:59.440</a></span> | <span class="t">In this case, after four tokens generated, the language model will probably say Shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=245" target="_blank">00:04:05.240</a></span> | <span class="t">is in China, which is the answer to our question. What do we mean by AI alignment? Now when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=253" target="_blank">00:04:13.120</a></span> | <span class="t">we train language model, we train it on a massive amount of data. For example, thousands</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=258" target="_blank">00:04:18.680</a></span> | <span class="t">of books, billions of web pages and the entire Wikipedia, et cetera. This gives the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=264" target="_blank">00:04:24.740</a></span> | <span class="t">model a vast knowledge to complete any prompt in a reasonable way. However, this does not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=273" target="_blank">00:04:33.160</a></span> | <span class="t">teach the language model to behave in a particular way. For example, the pre-training does not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=279" target="_blank">00:04:39.240</a></span> | <span class="t">teach the language model to be polite or to not use any offensive language or to not use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=284" target="_blank">00:04:44.120</a></span> | <span class="t">any racist expressions, et cetera, because the language model will just behave based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=288" target="_blank">00:04:48.200</a></span> | <span class="t">on the data that it has seen. If you feed the internet data, the language model will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=292" target="_blank">00:04:52.720</a></span> | <span class="t">behave very, very, very badly actually. We need to kind of align the language model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=297" target="_blank">00:04:57.680</a></span> | <span class="t">a desired behavior. We don't want the language model to use any offensive language. We don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=302" target="_blank">00:05:02.040</a></span> | <span class="t">want it to be racist. We want the language model to be helpful to the user, so to answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=306" target="_blank">00:05:06.840</a></span> | <span class="t">questions like an assistant, et cetera, et cetera. And this is the goal of AI alignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=312" target="_blank">00:05:12.400</a></span> | <span class="t">Now let's talk about reinforcement learning. So reinforcement learning is an area of AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=317" target="_blank">00:05:17.760</a></span> | <span class="t">that is concerned with training intelligent agents to perform actions in an environment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=323" target="_blank">00:05:23.400</a></span> | <span class="t">in order to maximize a reward that they receive from this environment. Let me show you with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=329" target="_blank">00:05:29.040</a></span> | <span class="t">a very concrete example. I usually always use my cat, Oleo, for examples, so let's talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=334" target="_blank">00:05:34.440</a></span> | <span class="t">about Oleo. Oleo is the agent in this case, in this reinforcement learning scenario, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=339" target="_blank">00:05:39.860</a></span> | <span class="t">he lives in a very simple world. Let's call it a grid world that is made of cells in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=345" target="_blank">00:05:45.980</a></span> | <span class="t">the cat's position is indicated by two coordinates, the X position and the Y position. This can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=352" target="_blank">00:05:52.220</a></span> | <span class="t">also be treated as the state of the agent because at every position corresponds to a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=357" target="_blank">00:05:57.800</a></span> | <span class="t">particular state. The agent, when it is in a particular state, it can take some actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=364" target="_blank">00:06:04.360</a></span> | <span class="t">In the case of the cat, it can go right, left, up or down. For every action that the agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=371" target="_blank">00:06:11.680</a></span> | <span class="t">takes, it will receive some reward from the environment. It will for sure change its state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=377" target="_blank">00:06:17.360</a></span> | <span class="t">to a new one. So for example, when the cat moves down, it will change to a new state,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=382" target="_blank">00:06:22.200</a></span> | <span class="t">to a new position, and it will receive some reward according to a reward model that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=386" target="_blank">00:06:26.800</a></span> | <span class="t">specified. In my case, I have specified the following reward model. So when the cat moves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=391" target="_blank">00:06:31.980</a></span> | <span class="t">to an empty cell, it receives a reward of zero. If it moves towards the broom, it receives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=397" target="_blank">00:06:37.400</a></span> | <span class="t">a reward of minus one. If somehow it arrives to the bathtub, it will receive a reward of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=404" target="_blank">00:06:44.240</a></span> | <span class="t">minus 10 because my cat is very scared of water. And if it arrives to the meat, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=408" target="_blank">00:06:48.720</a></span> | <span class="t">is the cat's dream, it will receive a reward of plus 100. Now, what dictates what action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=417" target="_blank">00:06:57.400</a></span> | <span class="t">the agent will take given a particular state or position? Well, it is the policy. The policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=423" target="_blank">00:07:03.480</a></span> | <span class="t">indicates what is the probability of the next action among all the actions that are available</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=429" target="_blank">00:07:09.400</a></span> | <span class="t">that the agent can take given a particular state. And we usually write it like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=435" target="_blank">00:07:15.220</a></span> | <span class="t">so that the next action at time step t is distributed like the distribution induced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=441" target="_blank">00:07:21.360</a></span> | <span class="t">by the policy according to the state the agent is in. Now, what is the goal in reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=448" target="_blank">00:07:28.760</a></span> | <span class="t">learning? The goal in reinforcement learning is to select a policy or to optimize a policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=453" target="_blank">00:07:33.900</a></span> | <span class="t">in order for the agent to maximize the expected return when it acts according to this policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=461" target="_blank">00:07:41.840</a></span> | <span class="t">So imagine we have such a policy that is optimized. Well, our policy, for sure, if the goal is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=470" target="_blank">00:07:50.020</a></span> | <span class="t">to maximize the expected reward when using this policy, for sure, we will have a policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=474" target="_blank">00:07:54.780</a></span> | <span class="t">that will take us on average to the meat because that's one way to maximize the expected reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=480" target="_blank">00:08:00.240</a></span> | <span class="t">And for sure, it will be a policy that will allow us to minimize the chance of ending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=484" target="_blank">00:08:04.880</a></span> | <span class="t">up in the water here or to the broom here. Now, you may be wondering, okay, the cat can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=491" target="_blank">00:08:11.340</a></span> | <span class="t">be seen as a reinforcement learning agent, as a physical agent that takes some actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=495" target="_blank">00:08:15.380</a></span> | <span class="t">But what is the connection between reinforcement learning and language models? Well, as we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=499" target="_blank">00:08:19.900</a></span> | <span class="t">saw before, in reinforcement learning, we have this thing called policy, in which given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=504" target="_blank">00:08:24.860</a></span> | <span class="t">a state, the policy tells us what is the probability over the action space of the next action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=511" target="_blank">00:08:31.780</a></span> | <span class="t">So what possible next action we can take and the probability of each action. This is also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=516" target="_blank">00:08:36.700</a></span> | <span class="t">something similar to what we do with language models, because in language models, we also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=520" target="_blank">00:08:40.420</a></span> | <span class="t">have some kind of state, which is our prompt. And we ask the language model to give us the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=524" target="_blank">00:08:44.740</a></span> | <span class="t">probability of the next token, or we can consider it the next action that the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=530" target="_blank">00:08:50.180</a></span> | <span class="t">can take. And we want to reward this language model for selecting tokens in such a way that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=538" target="_blank">00:08:58.260</a></span> | <span class="t">they end up generating good responses. And we don't want to reward the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=543" target="_blank">00:09:03.220</a></span> | <span class="t">for selecting sequence of tokens that end up giving us bad responses. Now, imagine we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=549" target="_blank">00:09:09.140</a></span> | <span class="t">are trying to train a language model that needs to act like an AI assistant. So for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=555" target="_blank">00:09:15.620</a></span> | <span class="t">sure we want the language model to be helpful, to answer questions in a meaningful way, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=561" target="_blank">00:09:21.220</a></span> | <span class="t">not just output garbage. We want the language model to not be racist or not use any offensive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=566" target="_blank">00:09:26.420</a></span> | <span class="t">language. So this is all good behaviors that we want from this language model. So we may</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=570" target="_blank">00:09:30.900</a></span> | <span class="t">want to build a reward model that will treat good responses, for example, responses that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=576" target="_blank">00:09:36.620</a></span> | <span class="t">actually answer the question asked by the user. And we will reward them with a high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=582" target="_blank">00:09:42.060</a></span> | <span class="t">reward. And we give maybe zero reward or negative reward to all those answers that are not coherent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=588" target="_blank">00:09:48.860</a></span> | <span class="t">with what we want. So for example, if the language model generates dirty jokes or racist</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=597" target="_blank">00:09:57.660</a></span> | <span class="t">jokes, for example, we can give zero reward to those responses. So the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=606" target="_blank">00:10:06.340</a></span> | <span class="t">acts also as a policy because the policy is something that, given a prompt, tells you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=611" target="_blank">00:10:11.540</a></span> | <span class="t">what is the probability over the action space, or in this case, the probability over the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=615" target="_blank">00:10:15.780</a></span> | <span class="t">token space. So we want to optimize this policy. So we want to optimize the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=621" target="_blank">00:10:21.780</a></span> | <span class="t">to maximize the probability, to maximize the expected return or the expected reward that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=628" target="_blank">00:10:28.260</a></span> | <span class="t">it receives from our reward model. So we want to optimize our language model to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=633" target="_blank">00:10:33.300</a></span> | <span class="t">good response, because that's one way to obtain a high reward from the reward model. Now you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=639" target="_blank">00:10:39.580</a></span> | <span class="t">may be wondering, OK, but how to define the reward model for a language model? Well, one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=644" target="_blank">00:10:44.780</a></span> | <span class="t">way would be, OK, we can have a list of questions and answers generated by the language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=650" target="_blank">00:10:50.380</a></span> | <span class="t">and then we can give a numerical reward to each one of them. And then we can use some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=654" target="_blank">00:10:54.460</a></span> | <span class="t">reinforcement learning algorithm to feed this reward model to the language model to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=659" target="_blank">00:10:59.420</a></span> | <span class="t">it. But the problem is, what kind of reward can we give to each of these pairs of questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=665" target="_blank">00:11:05.260</a></span> | <span class="t">and answers? Because, for example, let's look at the first question. Where is Shanghai?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=669" target="_blank">00:11:09.860</a></span> | <span class="t">The answer, suppose it is generated by the language model, is that Shanghai is a city</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=673" target="_blank">00:11:13.700</a></span> | <span class="t">in China. Now, in my opinion, this is a good response because it's short and up to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=679" target="_blank">00:11:19.260</a></span> | <span class="t">point. But some other people maybe think that only the word China is enough because the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=685" target="_blank">00:11:25.660</a></span> | <span class="t">user just asked, where is Shanghai? So there is no need to repeat the word Shanghai. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=689" target="_blank">00:11:29.860</a></span> | <span class="t">someone else maybe think that this response is too short and the assistant should say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=694" target="_blank">00:11:34.940</a></span> | <span class="t">hello, I think that the answer to your question is Shanghai is a city in China. So different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=700" target="_blank">00:11:40.940</a></span> | <span class="t">people will have different opinions on what reward to assign to this particular pair of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=706" target="_blank">00:11:46.260</a></span> | <span class="t">question and answer. Because we humans are not very good at finding a common ground for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=711" target="_blank">00:11:51.660</a></span> | <span class="t">agreement. But unfortunately, we are very good at comparing. And we will exploit this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=716" target="_blank">00:11:56.620</a></span> | <span class="t">fact. So instead of building a data set that is made of questions and answers and the rewards,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=723" target="_blank">00:12:03.980</a></span> | <span class="t">because we do not know what kind of reward to assign, we will build a data set of questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=729" target="_blank">00:12:09.100</a></span> | <span class="t">and multiple answers. And then we ask people to choose an answer that they like, according</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=735" target="_blank">00:12:15.420</a></span> | <span class="t">to some preference that we have. So we want to generate, for sure, a language model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=739" target="_blank">00:12:19.860</a></span> | <span class="t">is helpful. So we want the general language model to give responses that are correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=744" target="_blank">00:12:24.920</a></span> | <span class="t">And we want the language model to be polite, for example. So for example, imagine we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=749" target="_blank">00:12:29.860</a></span> | <span class="t">a list of questions. And then we ask the language model by using, for example, a high temperature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=755" target="_blank">00:12:35.300</a></span> | <span class="t">to generate multiple answers. And then we ask people to choose which one they like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=760" target="_blank">00:12:40.020</a></span> | <span class="t">In this case, for example, where is Shanghai? For sure, most people will choose the answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=764" target="_blank">00:12:44.960</a></span> | <span class="t">number one, because Shanghai is a city in China, it's the correct one. For this question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=769" target="_blank">00:12:49.860</a></span> | <span class="t">here, for example, most people will probably choose this question here, even if it's very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=774" target="_blank">00:12:54.660</a></span> | <span class="t">short because the other one is probably wrong. So using a data set like this, we can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=780" target="_blank">00:13:00.660</a></span> | <span class="t">train a model to transform a pair of questions and answers into a numeric reward. Let's see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=786" target="_blank">00:13:06.780</a></span> | <span class="t">how it is done. If you have a pet, you probably know that to teach a particular behavior to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=792" target="_blank">00:13:12.740</a></span> | <span class="t">your cat or to your dog, you need to use biscuits or some treats. So you ask the cat to do something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=800" target="_blank">00:13:20.340</a></span> | <span class="t">And if the cat does it, then you give it a treat. So it will reinforce this memory in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=805" target="_blank">00:13:25.140</a></span> | <span class="t">your cat. And then the next time the cat is more likely to do it because it will remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=809" target="_blank">00:13:29.620</a></span> | <span class="t">that it received some treat. And so it will again perform that action again. So it can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=814" target="_blank">00:13:34.740</a></span> | <span class="t">probably receive another treat. This is exactly what we do in reinforcement learning. We want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=819" target="_blank">00:13:39.780</a></span> | <span class="t">to give some digital biscuits to our reinforcement learning agent so that it is more likely to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=826" target="_blank">00:13:46.380</a></span> | <span class="t">perform that action or that series of actions again in order to receive more reward. However,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=833" target="_blank">00:13:53.140</a></span> | <span class="t">the data set that we have built so far is made up of preferences. So we have a question,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=838" target="_blank">00:13:58.580</a></span> | <span class="t">multiple answers, and then we ask people to choose which answer they like. We need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=843" target="_blank">00:14:03.940</a></span> | <span class="t">convert this data set of preferences into a numeric score that we can give as a reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=850" target="_blank">00:14:10.620</a></span> | <span class="t">to our language model to choose more likely the answer that was chosen by the people and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=858" target="_blank">00:14:18.620</a></span> | <span class="t">to make it less likely to choose the answer that was not liked by the people, by our annotators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=865" target="_blank">00:14:25.180</a></span> | <span class="t">And this can be done through a preference model. In DPO and also in reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=870" target="_blank">00:14:30.820</a></span> | <span class="t">from human feedback, we make use of the Bradley-Terry model. So the Bradley-Terry model is a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=876" target="_blank">00:14:36.300</a></span> | <span class="t">of converting a data set of preferences into a numeric score called reward that is given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=882" target="_blank">00:14:42.460</a></span> | <span class="t">for each pair of questions and answers. Our goal is to train a model that given a question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=889" target="_blank">00:14:49.500</a></span> | <span class="t">and answer or a prompt and the text generated text to give a score that resembles the preferences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=897" target="_blank">00:14:57.100</a></span> | <span class="t">that have been chosen by our annotators. This is the expression of the Bradley-Terry model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=903" target="_blank">00:15:03.940</a></span> | <span class="t">So it is a model meaning that we choose to model our preferences like this. And actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=909" target="_blank">00:15:09.460</a></span> | <span class="t">it makes sense because it is a probability. So that's why, for example, we use explanations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=914" target="_blank">00:15:14.980</a></span> | <span class="t">because we want the probability has to be non-negative. And also the probability that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=922" target="_blank">00:15:22.140</a></span> | <span class="t">the assigned to the correct preferences. So the probability of choosing the correct answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=928" target="_blank">00:15:28.780</a></span> | <span class="t">over the wrong answer. So the one that is chosen by the annotators over the one that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=933" target="_blank">00:15:33.900</a></span> | <span class="t">was not chosen by the annotators. Here, I call it the winner and loser because also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=939" target="_blank">00:15:39.300</a></span> | <span class="t">in the DPO paper, they call it winner and loser. It is modeled like this. So it is proportional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=944" target="_blank">00:15:44.420</a></span> | <span class="t">to the reward to the exponential of the reward that was assigned to the winning answer. Now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=952" target="_blank">00:15:52.020</a></span> | <span class="t">how to train a model to convert a dataset of preferences into a numeric reward? We take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=958" target="_blank">00:15:58.580</a></span> | <span class="t">this expression and we can use a maximum likelihood estimation. Now, it doesn't matter if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=964" target="_blank">00:16:04.560</a></span> | <span class="t">don't know what is maximum likelihood estimation. The point is we want to maximize the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=970" target="_blank">00:16:10.260</a></span> | <span class="t">of assigning the correct ordering in our preferences. So we want to maximize the probability of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=976" target="_blank">00:16:16.580</a></span> | <span class="t">choosing the correct answer over the wrong answer. And suppose that we are maximizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=984" target="_blank">00:16:24.220</a></span> | <span class="t">this expression here. Let's see how we can derive the loss to maximize this expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=989" target="_blank">00:16:29.580</a></span> | <span class="t">here. If you look at the DPO paper, you will see that they go from the Bradley Terry model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=995" target="_blank">00:16:35.980</a></span> | <span class="t">which is this one, directly to the loss here, but they don't show you the derivation. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1000" target="_blank">00:16:40.940</a></span> | <span class="t">I will show you how to derive the loss that maximizes this probability here. The derivation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1010" target="_blank">00:16:50.020</a></span> | <span class="t">is very simple actually. So first of all, as you can see in the loss, you can see this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1015" target="_blank">00:16:55.260</a></span> | <span class="t">function here. It's a sigmoid function. The expression of the sigmoid function is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1020" target="_blank">00:17:00.180</a></span> | <span class="t">one and this is the graph of the sigmoid. So the expression of the sigmoid function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1024" target="_blank">00:17:04.540</a></span> | <span class="t">is one over one plus e to the power of minus x. The first step of the derivation is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1031" target="_blank">00:17:11.980</a></span> | <span class="t">prove that two exponentials, so a fraction of this expression here, so exponential divided</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1040" target="_blank">00:17:20.180</a></span> | <span class="t">by the sum of two exponentials can be written as a sigmoid of a minus b. So here I call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1046" target="_blank">00:17:26.260</a></span> | <span class="t">all this part here. So let me use the pen. I think it's easier. So this part here, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1054" target="_blank">00:17:34.340</a></span> | <span class="t">the reward assigned to the, let's say the winning answer is, we call it a, and the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1061" target="_blank">00:17:41.820</a></span> | <span class="t">assigned to the losing answer, we call it b. So this expression can be written as e</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1068" target="_blank">00:17:48.780</a></span> | <span class="t">to the power of a divided by e to the power of a plus e to the power of b. And we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1074" target="_blank">00:17:54.860</a></span> | <span class="t">prove that it can be written as the sigmoid of a minus b through the following step. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1083" target="_blank">00:18:03.280</a></span> | <span class="t">first we can divide, we take this expression, which is basically this one. We just replace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1089" target="_blank">00:18:09.940</a></span> | <span class="t">the rewards with a and b because it makes it simpler to visualize. We divide the numerator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1096" target="_blank">00:18:16.580</a></span> | <span class="t">and denominator by the same quantity, e to the power of a. We can do it. Then we can,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1105" target="_blank">00:18:25.360</a></span> | <span class="t">at the numerator, e to the power of a cancels out with e to the power of a and it becomes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1109" target="_blank">00:18:29.860</a></span> | <span class="t">a one. Then in the denominator, we add and subtract one. We can do it because it's like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1116" target="_blank">00:18:36.460</a></span> | <span class="t">adding zero. And then we collect the minus one. So we don't change anything. We just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1121" target="_blank">00:18:41.460</a></span> | <span class="t">put the parentheses. This is possible through the associative property. Then we do the common</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1128" target="_blank">00:18:48.860</a></span> | <span class="t">denominator for these two expressions. And we arrive to this one. We can simplify e to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1135" target="_blank">00:18:55.260</a></span> | <span class="t">the power of a with minus e to the power of a. So it becomes e to the power of b divided</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1139" target="_blank">00:18:59.580</a></span> | <span class="t">by e to the power of a, which thanks to the property of the exponentials can be written</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1144" target="_blank">00:19:04.580</a></span> | <span class="t">as e to the power of b minus a. Then we can take a minus sign outside. And this expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1149" target="_blank">00:19:09.980</a></span> | <span class="t">here is exactly the expression of the sigmoid function you can see here. So it's one over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1155" target="_blank">00:19:15.020</a></span> | <span class="t">one plus e to the power of minus something. So it becomes the sigmoid of that something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1161" target="_blank">00:19:21.220</a></span> | <span class="t">here a minus b. And this is exactly the loss that you see here. So it is the sigmoid of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1167" target="_blank">00:19:27.380</a></span> | <span class="t">the reward assigned to the winning answer minus the reward assigned to the losing answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1175" target="_blank">00:19:35.460</a></span> | <span class="t">Here we also see a log because usually we do not model the probability directly but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1181" target="_blank">00:19:41.220</a></span> | <span class="t">we model the log probability. So we have also this log because we want to model the log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1186" target="_blank">00:19:46.740</a></span> | <span class="t">probabilities. It is something that we can do because the logarithm is a monotonic function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1192" target="_blank">00:19:52.980</a></span> | <span class="t">And also you may be wondering why do we have this minus sign here? This is because we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1198" target="_blank">00:19:58.300</a></span> | <span class="t">to maximize this expression. But as you know in deep learning frameworks like PyTorch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1204" target="_blank">00:20:04.780</a></span> | <span class="t">we have an optimizer that is always minimizing a loss. So instead of maximizing something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1209" target="_blank">00:20:09.460</a></span> | <span class="t">we can minimize the negative expression of the objective function which is the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1214" target="_blank">00:20:14.900</a></span> | <span class="t">So basically we take this loss function and if we apply it to a reward model which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1220" target="_blank">00:20:20.300</a></span> | <span class="t">a neural network, it will be trained to maximize the probability of giving the correct ordering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1227" target="_blank">00:20:27.540</a></span> | <span class="t">to our preferences which can only happen when it assigns a high reward to the winning answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1233" target="_blank">00:20:33.740</a></span> | <span class="t">and a low reward to the losing answer. Because if you look at this expression here, as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1238" target="_blank">00:20:38.900</a></span> | <span class="t">can see the probability is maximized when in the numerator we have the reward assigned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1244" target="_blank">00:20:44.500</a></span> | <span class="t">to the winning answer. So the reward assigned to the winning answer is higher than the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1248" target="_blank">00:20:48.660</a></span> | <span class="t">assigned to the losing answer. And if you are wondering how to read an expression like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1255" target="_blank">00:20:55.100</a></span> | <span class="t">this, so let me cancel because we will use it a lot, this kind of convention. This one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1262" target="_blank">00:21:02.060</a></span> | <span class="t">This basically means that we have a data set of preferences where we have a prompt, a winning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1268" target="_blank">00:21:08.020</a></span> | <span class="t">answer and a losing answer and they belong to our data set of preferences and we train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1273" target="_blank">00:21:13.700</a></span> | <span class="t">a model with a gradient descent for each of these preferences we calculate this loss here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1281" target="_blank">00:21:21.060</a></span> | <span class="t">this expression here. And if we minimize this loss with the gradient descent we will have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1286" target="_blank">00:21:26.340</a></span> | <span class="t">a neural network that is trained for the following, the Bradley-Tarry model basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1293" target="_blank">00:21:33.340</a></span> | <span class="t">Okay, now that we have built a reward model, which means that we have a model that given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1300" target="_blank">00:21:40.100</a></span> | <span class="t">a question and answer can assign a numeric reward to the language model if the response</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1305" target="_blank">00:21:45.660</a></span> | <span class="t">is correct or looks good according to the behavior that we want from our language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1310" target="_blank">00:21:50.300</a></span> | <span class="t">or looks bad according to the behavior that we want from our language model. Now we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1315" target="_blank">00:21:55.900</a></span> | <span class="t">train our language model. So as you recall, what is the goal in reinforcement learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1322" target="_blank">00:22:02.900</a></span> | <span class="t">In reinforcement learning, the goal is to optimize a language model, which is also the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1327" target="_blank">00:22:07.660</a></span> | <span class="t">policy of our reinforcement learning agent, in order to maximize the cumulative reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1334" target="_blank">00:22:14.340</a></span> | <span class="t">when the agent acts according to this policy. In other words, if we, let me use the pen,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1340" target="_blank">00:22:20.760</a></span> | <span class="t">so let's ignore for now this green part here. Suppose there is no green part here. So this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1347" target="_blank">00:22:27.460</a></span> | <span class="t">doesn't exist. Imagine we have a language model, let's call it Pi Theta because it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1352" target="_blank">00:22:32.940</a></span> | <span class="t">a policy and we want to optimize this policy. So we want to optimize this language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1359" target="_blank">00:22:39.780</a></span> | <span class="t">in order to maximize the reward that it receives from the reward model. It means that the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1365" target="_blank">00:22:45.660</a></span> | <span class="t">model will generate answers that give good reward and how they get good reward if the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1371" target="_blank">00:22:51.700</a></span> | <span class="t">answers are, looks good. They, for example, are not racist. They are not using any sexual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1377" target="_blank">00:22:57.100</a></span> | <span class="t">jokes and they are actually answering the question that was asked. However, and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1384" target="_blank">00:23:04.020</a></span> | <span class="t">is the goal in reinforcement learning from human feedback, for example. That's why it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1389" target="_blank">00:23:09.860</a></span> | <span class="t">called the reinforcement learning from human feedback. Now, if we use a model, if we use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1398" target="_blank">00:23:18.280</a></span> | <span class="t">an objective like this, that is, we only want to maximize the reward, then the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1406" target="_blank">00:23:26.340</a></span> | <span class="t">model may become greedy and just output garbage that gives it good reward. So imagine we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1413" target="_blank">00:23:33.860</a></span> | <span class="t">a reward model that rewards the language model for being polite. The language model may just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1419" target="_blank">00:23:39.420</a></span> | <span class="t">start saying a list of "thank you, thank you, thank you" or "please, please, please" and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1424" target="_blank">00:23:44.660</a></span> | <span class="t">a lot of "please" or a lot of "thank you's" just to get high reward. Because probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1429" target="_blank">00:23:49.700</a></span> | <span class="t">the word "thank you" and "please" are highly rewarded by the reward model. But we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1437" target="_blank">00:23:57.100</a></span> | <span class="t">want the language model to just output garbage to get reward. We want the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1442" target="_blank">00:24:02.940</a></span> | <span class="t">to also output something that was according to its training data. So it's a pre-training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1452" target="_blank">00:24:12.780</a></span> | <span class="t">but we want to change it a little bit so that it also acts according to our reward model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1458" target="_blank">00:24:18.380</a></span> | <span class="t">so to our data set of preferences. So it is more polite, but without forgetting what it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1463" target="_blank">00:24:23.900</a></span> | <span class="t">has learned from the pre-training. And this is why we add this KL divergence in the objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1472" target="_blank">00:24:32.260</a></span> | <span class="t">So let me use the pen again. So we change the objective a little bit. So we want the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1476" target="_blank">00:24:36.820</a></span> | <span class="t">language model to maximize the reward it gets from the reward model. But at the same time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1482" target="_blank">00:24:42.500</a></span> | <span class="t">we add a constraint to the language model through a KL divergence. Now the KL divergence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1488" target="_blank">00:24:48.300</a></span> | <span class="t">can be thought of as a distance metric. It is not a distance metric, but can be thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1492" target="_blank">00:24:52.500</a></span> | <span class="t">of as a distance metric between two distributions in which we have a pre-trained model. So a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1500" target="_blank">00:25:00.740</a></span> | <span class="t">language model that was not fine-tuned through reinforcement learning from human feedback</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1505" target="_blank">00:25:05.140</a></span> | <span class="t">or DPO. So it's just a language model that has been pre-trained on the Wikipedia, on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1510" target="_blank">00:25:10.580</a></span> | <span class="t">the books, and on the internet web pages. And then we have the language model that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1515" target="_blank">00:25:15.220</a></span> | <span class="t">are optimizing. So this pi theta, and we want them to be very similar. So we want the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1520" target="_blank">00:25:20.700</a></span> | <span class="t">model to not change much compared to what it was before the reinforcement learning from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1526" target="_blank">00:25:26.220</a></span> | <span class="t">human feedback or before the DPO training. And this is why we add this KL divergence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1532" target="_blank">00:25:32.100</a></span> | <span class="t">So we want the language model to maximize its reward, but at the same time, not forget</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1537" target="_blank">00:25:37.900</a></span> | <span class="t">or not change too much its output in getting this reward. Now that we know the reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1546" target="_blank">00:25:46.740</a></span> | <span class="t">learning objective, which is basically also the same objective that we have in DPO, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1551" target="_blank">00:25:51.860</a></span> | <span class="t">also in DPO we want to train a language model that maximizes a reward, but at the same time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1557" target="_blank">00:25:57.500</a></span> | <span class="t">does not forget its training data. Let's look at what does it mean to actually maximize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1563" target="_blank">00:26:03.100</a></span> | <span class="t">an objective function, because this is an objective function that we have and we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1566" target="_blank">00:26:06.780</a></span> | <span class="t">to maximize it. But what does it mean to maximize an objective? Let's see. Maximizing a function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1574" target="_blank">00:26:14.820</a></span> | <span class="t">means to find the values of some variable such that the value of the function is maximized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1581" target="_blank">00:26:21.540</a></span> | <span class="t">For example, if I give you the following function, f of x is equal to minus x minus three plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1587" target="_blank">00:26:27.500</a></span> | <span class="t">four, whose graph is very simple. It's just a parabola facing down. To maximize these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1594" target="_blank">00:26:34.260</a></span> | <span class="t">functions means to find the value of the x variable such that the function, the y basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1601" target="_blank">00:26:41.820</a></span> | <span class="t">the y of this function is maximized. How to do that analytically? Well, we calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1608" target="_blank">00:26:48.380</a></span> | <span class="t">derivative of this function here. We set the derivative equal to zero and we find the values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1614" target="_blank">00:26:54.740</a></span> | <span class="t">of x for which this derivative is zero. And that is also the value for which the function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1620" target="_blank">00:27:00.860</a></span> | <span class="t">will be maximized. So the derivative of this simple function is minus two x plus six. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1626" target="_blank">00:27:06.620</a></span> | <span class="t">the value of x that makes this derivative zero is the value three, x equal to three,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1631" target="_blank">00:27:11.980</a></span> | <span class="t">which is also the value of the, as you can see in the graph, that maximizes the function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1637" target="_blank">00:27:17.780</a></span> | <span class="t">Now the objective function that we saw before, so this one, so in which we want to maximize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1643" target="_blank">00:27:23.820</a></span> | <span class="t">a reward, but at the same time, we want the language model to not be too much different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1651" target="_blank">00:27:31.420</a></span> | <span class="t">from the unaligned language model. So the language model that is not aligned through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1657" target="_blank">00:27:37.100</a></span> | <span class="t">reinforcement learning from human feedback or DPO, it is called a constrained optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1662" target="_blank">00:27:42.980</a></span> | <span class="t">problem because we want to maximize the reward, but at the same time, we want to put some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1668" target="_blank">00:27:48.460</a></span> | <span class="t">constraint on this objective function. We don't want the KL divergence to be too big.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1675" target="_blank">00:27:55.620</a></span> | <span class="t">We want it to be constrained in some limit. Now, the point is, okay, there are many techniques</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1684" target="_blank">00:28:04.580</a></span> | <span class="t">for constrained optimization and we will not see them because there are university PhDs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1689" target="_blank">00:28:09.260</a></span> | <span class="t">on optimization, but one thing you may notice is that, okay, this one here looks like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1696" target="_blank">00:28:16.220</a></span> | <span class="t">objective function looks like a loss function. So why cannot we just use, for example, gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1701" target="_blank">00:28:21.900</a></span> | <span class="t">descent to optimize this objective function here such that we can train our language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1708" target="_blank">00:28:28.940</a></span> | <span class="t">to behave in a particular way to maximize this reward? Well, we could, but as you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1715" target="_blank">00:28:35.460</a></span> | <span class="t">in deep learning and especially with backpropagation, we need an objective function or a loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1721" target="_blank">00:28:41.380</a></span> | <span class="t">that is a differentiable. The following, this objective function is not differentiable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1727" target="_blank">00:28:47.340</a></span> | <span class="t">Why? Because as you can see from the expression here, this is an estimation of all the prompts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1733" target="_blank">00:28:53.900</a></span> | <span class="t">in our dataset and then a output that is generated by the language model. Now, to generate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1741" target="_blank">00:29:01.380</a></span> | <span class="t">output of the language model, as we saw before, we need to use an iterative process in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1745" target="_blank">00:29:05.700</a></span> | <span class="t">we feed one token at a time into the prompt. We sample one token at a time from the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1752" target="_blank">00:29:12.460</a></span> | <span class="t">model. We take this token and we put it back into the prompt, feed it again to the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1755" target="_blank">00:29:15.940</a></span> | <span class="t">model, et cetera, and we use many strategies for selecting the next token. Sometimes we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1760" target="_blank">00:29:20.580</a></span> | <span class="t">use the greedy strategy. Sometimes we use the beam search. Sometimes we use the top</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1764" target="_blank">00:29:24.180</a></span> | <span class="t">case, top P, et cetera, et cetera. Now, this sampling operation that we do on the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1769" target="_blank">00:29:29.340</a></span> | <span class="t">model to sample the answer of the language model is not differentiable. That's why we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1774" target="_blank">00:29:34.500</a></span> | <span class="t">cannot run reinforcement learning to maximize this objective or to minimize the negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1780" target="_blank">00:29:40.140</a></span> | <span class="t">objective in case we treat it as a loss. And that's why in reinforcement learning, we were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1784" target="_blank">00:29:44.940</a></span> | <span class="t">forced to use algorithms like PPO. Now, let's see how DPO handles this. In the DPO paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1795" target="_blank">00:29:55.700</a></span> | <span class="t">they start with a very simple introduction to the reinforcement learning objective. As</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1801" target="_blank">00:30:01.060</a></span> | <span class="t">we saw before, the reinforcement learning objective is to select a policy that maximizes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1809" target="_blank">00:30:09.860</a></span> | <span class="t">the expected reward when using this policy, so the policy is the language model, and at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1814" target="_blank">00:30:14.300</a></span> | <span class="t">the same time puts a constraint on how much this policy can change during this training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1819" target="_blank">00:30:19.560</a></span> | <span class="t">this optimization. And in the DPO paper, they say, okay, there is an exact solution to this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1824" target="_blank">00:30:24.860</a></span> | <span class="t">optimization problem, and it is the following. It is the equation four in the DPO paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1830" target="_blank">00:30:30.660</a></span> | <span class="t">And exact solution, I mean that there is an analytical solution to this constrained optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1837" target="_blank">00:30:37.020</a></span> | <span class="t">problem. Just like we had an analytical solution for the maximization problem of this parabola,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1844" target="_blank">00:30:44.820</a></span> | <span class="t">so we could find through the derivative and setting the derivative equal to zero, we could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1849" target="_blank">00:30:49.140</a></span> | <span class="t">find the value of x such that this function here is maximized. And for using the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1855" target="_blank">00:30:55.820</a></span> | <span class="t">reasoning but different techniques, we also have an analytical solution for the constrained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1863" target="_blank">00:31:03.140</a></span> | <span class="t">optimization problem that we saw before, and this is the solution. Now, you may be wondering,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1868" target="_blank">00:31:08.500</a></span> | <span class="t">okay, great, we have an exact solution just like the parabola, so now we are all set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1873" target="_blank">00:31:13.780</a></span> | <span class="t">right? Yes. The problem is we have an exact solution, but it's not easily computable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1881" target="_blank">00:31:21.060</a></span> | <span class="t">so it's not easy to compute. So mathematically, it exists, it makes sense, but it's not easy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1887" target="_blank">00:31:27.440</a></span> | <span class="t">to compute. Why? Because we have this z of x term here. Now, this z of x term here, if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1894" target="_blank">00:31:34.460</a></span> | <span class="t">you look at how it's defined, it's the summation of all possible y's that are generated by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1901" target="_blank">00:31:41.420</a></span> | <span class="t">the reference model. So as you know, when we do reinforcement learning from human feedback</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1905" target="_blank">00:31:45.760</a></span> | <span class="t">or DPO, we have two models. One is the language model that we are trying to optimize, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1911" target="_blank">00:31:51.380</a></span> | <span class="t">one is the frozen model that we don't optimize, but we use it as a reference for the KL divergence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1916" target="_blank">00:31:56.740</a></span> | <span class="t">So this is called the PyRef. So all the outputs generated by PyRef multiplied by the exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1923" target="_blank">00:32:03.420</a></span> | <span class="t">of the reward. Now, the problem is this summation is done over all possible y's. It means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1929" target="_blank">00:32:09.460</a></span> | <span class="t">we need to sample all possible outputs from our language model, given all the prompts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1938" target="_blank">00:32:18.700</a></span> | <span class="t">that we have in our data set of preferences. Now, to generate all possible outputs is very,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1944" target="_blank">00:32:24.340</a></span> | <span class="t">very, very expensive. Imagine you need to generate, your language model can generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1949" target="_blank">00:32:29.300</a></span> | <span class="t">2,000 tokens for each prompt. It means that, and you have a vocabulary size of 30,000,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1956" target="_blank">00:32:36.500</a></span> | <span class="t">it means that for the first position, you have 30,000 possibilities, for the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1961" target="_blank">00:32:41.420</a></span> | <span class="t">position, you have 30,000 possibilities, for the third position, you have 30,000 possibilities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1965" target="_blank">00:32:45.580</a></span> | <span class="t">and then you multiply all these possibilities. So it becomes a lot, a lot, a lot of outputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1970" target="_blank">00:32:50.900</a></span> | <span class="t">that you need to generate to evaluate this Z of X term. So the analytical solution to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1975" target="_blank">00:32:55.780</a></span> | <span class="t">the constraint optimization problem that we saw before exists, but it's not easy to compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1981" target="_blank">00:33:01.820</a></span> | <span class="t">However, one thing is interesting from this expression. Imagine that somehow, magically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1988" target="_blank">00:33:08.340</a></span> | <span class="t">we have access to an optimal policy. So this solution to the optimization problem allow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=1994" target="_blank">00:33:14.140</a></span> | <span class="t">us to compute what is the optimal policy, given the optimal reward model and the reference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2000" target="_blank">00:33:20.140</a></span> | <span class="t">policy, so the reference language model. But imagine that for some reason, some magically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2005" target="_blank">00:33:25.180</a></span> | <span class="t">we have access to, we have this term here. So if we have this term here, we can compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2011" target="_blank">00:33:31.260</a></span> | <span class="t">the optimal reward model with respect to the optimal policy. How? Well, we can just isolate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2019" target="_blank">00:33:39.660</a></span> | <span class="t">this R of X and Y term from this expression here. And it's very easy to compute because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2024" target="_blank">00:33:44.820</a></span> | <span class="t">we can apply the logarithm on the left and the right side of this expression. So let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2030" target="_blank">00:33:50.260</a></span> | <span class="t">do it step by step. We can apply the log on the left side and on the right side of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2038" target="_blank">00:33:58.940</a></span> | <span class="t">expression here. So this expression here, and we will get that the log of a product,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2045" target="_blank">00:34:05.860</a></span> | <span class="t">as you know, is the sum of the logs and the log of the ratio is the difference of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2051" target="_blank">00:34:11.260</a></span> | <span class="t">logs. So this Z term is in the denominator. So it becomes a minus log of Z of X. This</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2057" target="_blank">00:34:17.700</a></span> | <span class="t">one is in the numerator and this one is in the numerator. So they become sums of logs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2062" target="_blank">00:34:22.180</a></span> | <span class="t">So this one plus this log here, then the log and the exponential can cancel out because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2069" target="_blank">00:34:29.100</a></span> | <span class="t">they're inverse functions. So this allow us to isolate this R of X, Y term with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2075" target="_blank">00:34:35.300</a></span> | <span class="t">to all the other terms, and we can write it like this. So we can calculate R of X and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2081" target="_blank">00:34:41.120</a></span> | <span class="t">Y with respect to an optimal policy that we think we have access to. We do not have access</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2087" target="_blank">00:34:47.060</a></span> | <span class="t">to it, but we pretend we have access to it. Okay. So there are two things that we do not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2092" target="_blank">00:34:52.500</a></span> | <span class="t">have in this expression. We do not have the reward model, the optimal reward model, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2097" target="_blank">00:34:57.420</a></span> | <span class="t">we do not have the optimal policy, but we pretend that we have the optimal policy. Why?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2104" target="_blank">00:35:04.180</a></span> | <span class="t">Let's see the next step that they do in the DPO paper. The next step is, they say, okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2110" target="_blank">00:35:10.740</a></span> | <span class="t">do you remember the Bradley Terry model? As you remember, the Bradley Terry model is our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2115" target="_blank">00:35:15.420</a></span> | <span class="t">reward model, right? Is the model that given a data set of preferences, allow us to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2121" target="_blank">00:35:21.580</a></span> | <span class="t">a numeric score, a numeric reward. Well, this Bradley Terry model is based on a reward that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2130" target="_blank">00:35:30.820</a></span> | <span class="t">we assign, right? So what if we plug the reward that we have computed from the constraint</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2138" target="_blank">00:35:38.860</a></span> | <span class="t">optimization problem into the Bradley Terry model? Well, we can do it. So we have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2144" target="_blank">00:35:44.580</a></span> | <span class="t">reward that we obtained from the constraint optimization problem by inverting the formula</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2149" target="_blank">00:35:49.660</a></span> | <span class="t">and we plug it inside the Bradley Terry model. So if you remember, the Bradley Terry model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2154" target="_blank">00:35:54.500</a></span> | <span class="t">can also be written as a sigmoid and we prove it before in the previous slide. So what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2161" target="_blank">00:36:01.660</a></span> | <span class="t">do is, okay, the Bradley Terry model can be written as a difference of rewards in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2168" target="_blank">00:36:08.140</a></span> | <span class="t">sigmoid function. So if we plug the reward here, so the reward obtained by the constraint</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2177" target="_blank">00:36:17.300</a></span> | <span class="t">optimization problem solution, we will see that the two Z of X terms, because this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2185" target="_blank">00:36:25.420</a></span> | <span class="t">a difference of rewards, as you can see, if we plug here for the reward assigned to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2190" target="_blank">00:36:30.820</a></span> | <span class="t">winning response, and here the reward assigned to the losing response, we have these two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2197" target="_blank">00:36:37.700</a></span> | <span class="t">Z of X terms, so plus beta log of Z of X and minus beta log of Z of X that will cancel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2205" target="_blank">00:36:45.220</a></span> | <span class="t">out because they are one the opposite of the other. This way we can obtain a formula that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2213" target="_blank">00:36:53.340</a></span> | <span class="t">does not contain the Z of X term and it's now computable. So basically, if we use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2220" target="_blank">00:37:00.740</a></span> | <span class="t">loss of the Bradley Terry model, so as you remember, the Bradley Terry model is a model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2227" target="_blank">00:37:07.260</a></span> | <span class="t">that allow us to train a language model to model the reward, right? And if we use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2235" target="_blank">00:37:15.460</a></span> | <span class="t">loss of the Bradley Terry model in which the reward is coming with respect to the optimal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2243" target="_blank">00:37:23.900</a></span> | <span class="t">policy, we can use it to optimize the policy to adhere implicitly to the reward model according</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2252" target="_blank">00:37:32.460</a></span> | <span class="t">to the Bradley Terry model. And this is the whole idea of the DPO paper. So we can plug</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2259" target="_blank">00:37:39.660</a></span> | <span class="t">the exact solution of the constraint optimization of the reinforcement learning objective, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2266" target="_blank">00:37:46.060</a></span> | <span class="t">can invert it to get the reward, we plug it into the Bradley Terry model, because the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2270" target="_blank">00:37:50.700</a></span> | <span class="t">Bradley Terry model only depends on the difference of rewards assigned to the winning answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2277" target="_blank">00:37:57.940</a></span> | <span class="t">and to the losing answer, the uncomputable term Z of X cancels out and then it becomes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2284" target="_blank">00:38:04.960</a></span> | <span class="t">computable. And now we can use it to train a language model that will act according to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2291" target="_blank">00:38:11.140</a></span> | <span class="t">the reward model of the Bradley Terry model, so to the preference model given by the Bradley</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2297" target="_blank">00:38:17.620</a></span> | <span class="t">Terry model. So it will favor good responses and at the same time it will be less likely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2307" target="_blank">00:38:27.140</a></span> | <span class="t">to output the preferences that were not chosen. And at the same time it will put a constraint</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2312" target="_blank">00:38:32.900</a></span> | <span class="t">onto the KL divergence, so at the same time it will put a restriction on how much the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2318" target="_blank">00:38:38.020</a></span> | <span class="t">language model can change with respect to the reference model, so the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2323" target="_blank">00:38:43.320</a></span> | <span class="t">that was not optimized with reinforcement learning from humanist feedback or DPO. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2329" target="_blank">00:38:49.460</a></span> | <span class="t">basically with the DPO we are doing kind of the same thing that we are doing in reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2336" target="_blank">00:38:56.300</a></span> | <span class="t">learning but without using the reinforcement learning algorithms. So the goal in both of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2342" target="_blank">00:39:02.220</a></span> | <span class="t">them is the same, so we want to optimize a policy, we want to optimize a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2347" target="_blank">00:39:07.340</a></span> | <span class="t">to maximize a cumulative reward but at the same time we want to put a constraint on how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2352" target="_blank">00:39:12.600</a></span> | <span class="t">much it can change using the KL divergence. In the case of reinforcement learning from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2356" target="_blank">00:39:16.940</a></span> | <span class="t">humanist feedback we are using the PPO algorithm to optimize this objective, to optimize this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2363" target="_blank">00:39:23.340</a></span> | <span class="t">policy, but in the case of DPO we do not have to use reinforcement learning from humanist</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2368" target="_blank">00:39:28.380</a></span> | <span class="t">feedback because we found a loss that implicitly is already mapping this reward objective through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2377" target="_blank">00:39:37.100</a></span> | <span class="t">this loss. Let's see how to actually now compute the log probability, so how to actually use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2387" target="_blank">00:39:47.500</a></span> | <span class="t">this loss, because first of all let's look at the expression of this loss. This loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2391" target="_blank">00:39:51.900</a></span> | <span class="t">says that if you have a data set of preferences in which X is the prompt, YW is the chosen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2400" target="_blank">00:40:00.740</a></span> | <span class="t">answer and YL is the not chosen answer because as you remember this data set is made up of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2406" target="_blank">00:40:06.240</a></span> | <span class="t">preferences of question and two answers and then we asked some annotators to tell us which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2413" target="_blank">00:40:13.700</a></span> | <span class="t">answer they prefer. So if we have this data set we can run a gradient descent using this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2419" target="_blank">00:40:19.860</a></span> | <span class="t">data set over this loss here, this loss here. Now to calculate this loss, okay the logarithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2426" target="_blank">00:40:26.620</a></span> | <span class="t">we can always calculate, it's just a function, the sigmoid is a function we can calculate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2431" target="_blank">00:40:31.460</a></span> | <span class="t">but the logarithm and the beta are, the beta is a hyperparameter that indicates how much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2436" target="_blank">00:40:36.040</a></span> | <span class="t">we want the language model to change with respect to the reference language model or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2440" target="_blank">00:40:40.140</a></span> | <span class="t">how much we want to constrain it and then we have to compute these log probabilities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2445" target="_blank">00:40:45.180</a></span> | <span class="t">so the log of the probability of generating this YW when the language model is prompted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2453" target="_blank">00:40:53.300</a></span> | <span class="t">with the prompt X and also for the Pyref, so also for the language model that is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2460" target="_blank">00:41:00.140</a></span> | <span class="t">being optimized by DPO. Let's see how to practically compute these log probabilities. So when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2469" target="_blank">00:41:09.080</a></span> | <span class="t">run DPO it's very simple, so imagine for example you are using a hugging face, it's just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2474" target="_blank">00:41:14.820</a></span> | <span class="t">matter of using this class, so DPO trainer, in which you pass the language model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2479" target="_blank">00:41:19.780</a></span> | <span class="t">you're optimizing, the frozen version of the language model that you don't want to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2483" target="_blank">00:41:23.940</a></span> | <span class="t">but it's the reference language model that is used to compute the log probabilities to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2489" target="_blank">00:41:29.500</a></span> | <span class="t">calculate the KL divergence, then you can give some other training arguments, you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2495" target="_blank">00:41:35.340</a></span> | <span class="t">have the list of them on the website of hugging face and then this beta parameter which indicates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2499" target="_blank">00:41:39.900</a></span> | <span class="t">the strength on how much you want the language model to change and also in the website of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2506" target="_blank">00:41:46.540</a></span> | <span class="t">hugging face they also give you what is the typical range for this hyperparameter. Now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2513" target="_blank">00:41:53.820</a></span> | <span class="t">what will happen inside the library of the DPO trainer, so inside the hugging face library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2520" target="_blank">00:42:00.220</a></span> | <span class="t">when you use the DPO trainer, they compute this loss, so they calculate these log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2525" target="_blank">00:42:05.580</a></span> | <span class="t">you can see here, so for example this log probability you can see here, but how do they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2530" target="_blank">00:42:10.100</a></span> | <span class="t">actually compute, well as you know a language model usually, in most cases it is a transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2541" target="_blank">00:42:21.020</a></span> | <span class="t">model and to compute these log probabilities as you know they use a prompt, a question,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2549" target="_blank">00:42:29.380</a></span> | <span class="t">the answer that is chosen and the answer that is not chosen, so the winning and the losing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2554" target="_blank">00:42:34.140</a></span> | <span class="t">answer, suppose that we want to generate the log probabilities for the winning answer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2559" target="_blank">00:42:39.420</a></span> | <span class="t">so this one, so we have a language model, we give it a prompt and the answer that was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2564" target="_blank">00:42:44.220</a></span> | <span class="t">generated and we want to calculate this log probabilities, what we can do is we can combine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2569" target="_blank">00:42:49.940</a></span> | <span class="t">the question and the answer in the same string, in the same input for the language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2576" target="_blank">00:42:56.740</a></span> | <span class="t">so imagine the question is where is Shanghai, question mark, and the answer is Shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2587" target="_blank">00:43:07.100</a></span> | <span class="t">is in China, now let me use the laser, ok, we can feed all of this to our language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2594" target="_blank">00:43:14.460</a></span> | <span class="t">so the pi theta, language model is a transformer model, most of the cases, and it will generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2601" target="_blank">00:43:21.420</a></span> | <span class="t">as you know the transformer model generates some hidden states, so it takes some input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2606" target="_blank">00:43:26.900</a></span> | <span class="t">which are embeddings, and it outputs some embeddings that are contextualized also according</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2614" target="_blank">00:43:34.140</a></span> | <span class="t">to the self-attention mask, now if you don't know how this works, I highly recommend you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2618" target="_blank">00:43:38.940</a></span> | <span class="t">watch my previous video on the transformer in which I show the self-attention mechanism,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2623" target="_blank">00:43:43.500</a></span> | <span class="t">but basically the transformer model is a model that takes some embeddings and through the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2627" target="_blank">00:43:47.780</a></span> | <span class="t">self-attention mechanism output embeddings, then we take these embeddings and we can project</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2633" target="_blank">00:43:53.300</a></span> | <span class="t">them into logits using a linear layer, and we can do that for all the tokens that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2638" target="_blank">00:43:58.580</a></span> | <span class="t">give to the input, usually we only apply the linear layer to the last token when generating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2643" target="_blank">00:44:03.940</a></span> | <span class="t">the tokens because we are interested in generating the next token, but we can do it for all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2650" target="_blank">00:44:10.980</a></span> | <span class="t">hidden states, we are not forced to only use the last one because each hidden state encapsulates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2657" target="_blank">00:44:17.540</a></span> | <span class="t">information about itself and all the tokens that come before it, so we can take this logits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2665" target="_blank">00:44:25.580</a></span> | <span class="t">and then we can also convert them into probabilities, but we do not want probabilities, we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2671" target="_blank">00:44:31.300</a></span> | <span class="t">log probabilities because as you can see here we have this log function here, so instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2675" target="_blank">00:44:35.540</a></span> | <span class="t">of applying the softmax we can apply the log softmax to each of these logits, now when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2681" target="_blank">00:44:41.180</a></span> | <span class="t">we apply the softmax, it will become a distribution over the entire vocabulary, one for each token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2689" target="_blank">00:44:49.140</a></span> | <span class="t">in the vocabulary, but we want only the probability corresponding to the token that was actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2695" target="_blank">00:44:55.620</a></span> | <span class="t">chosen to generate this particular answer, and we also know which token it was because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2700" target="_blank">00:45:00.740</a></span> | <span class="t">we have the answer, so to the question where is Shanghai, we know what is the answer because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2705" target="_blank">00:45:05.340</a></span> | <span class="t">it's in our data set of preferences, so we know that the answer is Shanghai is in China,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2710" target="_blank">00:45:10.740</a></span> | <span class="t">so how to compute these log probabilities, so we can compute the log probabilities over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2715" target="_blank">00:45:15.340</a></span> | <span class="t">the entire dictionary, over the entire vocabulary, and then we only select the log probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2721" target="_blank">00:45:21.300</a></span> | <span class="t">corresponding to the token that was actually selected in the answer, so for this question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2727" target="_blank">00:45:27.940</a></span> | <span class="t">for example, we select for example the last hidden state for the question, which corresponds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2733" target="_blank">00:45:33.620</a></span> | <span class="t">to what should be the next token, and we know what is the next token, the next token is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2737" target="_blank">00:45:37.540</a></span> | <span class="t">Shanghai, so we take the log probability only corresponding to Shanghai, for this prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2743" target="_blank">00:45:43.100</a></span> | <span class="t">here, so where is Shanghai, question mark Shanghai, we know that the next token should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2747" target="_blank">00:45:47.580</a></span> | <span class="t">be is, because it is already present, so we take the log probability only corresponding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2753" target="_blank">00:45:53.140</a></span> | <span class="t">to the token is, etc, etc, and we do it for all the tokens that are in the answer, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2759" target="_blank">00:45:59.140</a></span> | <span class="t">this gives us all the log probabilities of the tokens of the answer for this given question,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2766" target="_blank">00:46:06.660</a></span> | <span class="t">and then we can sum them up, why we need to sum them up, because it's a log probabilities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2772" target="_blank">00:46:12.060</a></span> | <span class="t">usually if they are probabilities we multiply them, but because they are log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2777" target="_blank">00:46:17.260</a></span> | <span class="t">we sum them up, because the logarithm transforms products into summations, and this is exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2785" target="_blank">00:46:25.420</a></span> | <span class="t">what happens inside the HuggingFace library, so inside the HuggingFace library to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2790" target="_blank">00:46:30.860</a></span> | <span class="t">the log probabilities, to calculate this loss, they actually do what I described, so they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2795" target="_blank">00:46:35.460</a></span> | <span class="t">take the logits, and they use the labels, what are the labels, just the tokens corresponding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2801" target="_blank">00:46:41.380</a></span> | <span class="t">to the answer, for example to the winning answer or to the losing answer, depending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2806" target="_blank">00:46:46.000</a></span> | <span class="t">on which term you are computing, this one or this one, and the model that we are using,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2810" target="_blank">00:46:50.340</a></span> | <span class="t">so the reference model or the model that we are trying to optimize, and then they select</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2815" target="_blank">00:46:55.820</a></span> | <span class="t">here, in this line here, they check the log probabilities only corresponding to the labels,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2823" target="_blank">00:47:03.120</a></span> | <span class="t">so to the next token that we already know what is it, and then they sum them up, as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2829" target="_blank">00:47:09.420</a></span> | <span class="t">you can see here, and here they also apply a loss, because we don't want all the log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2835" target="_blank">00:47:15.900</a></span> | <span class="t">probabilities, but only the one corresponding to the tokens that are belonging to the answer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2841" target="_blank">00:47:21.620</a></span> | <span class="t">not to the one that belong to the question, and this is how DPO works, thank you guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2848" target="_blank">00:47:28.140</a></span> | <span class="t">for watching my video, I hope you learned a lot, I tried to simplify as much as possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2853" target="_blank">00:47:33.340</a></span> | <span class="t">the math of DPO, but the basic idea is that we want to remove reinforcement learning to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2859" target="_blank">00:47:39.460</a></span> | <span class="t">align language models, this makes the tractation much simpler, because it just becomes a simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2865" target="_blank">00:47:45.020</a></span> | <span class="t">loss in which you can run a gradient descent, and you don't have to worry about training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2868" target="_blank">00:47:48.900</a></span> | <span class="t">a separate reward model, which is something that we did in reinforcement learning from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2873" target="_blank">00:47:53.040</a></span> | <span class="t">Deque, so if you watched my previous video, as you remember, the math is much more hard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2879" target="_blank">00:47:59.420</a></span> | <span class="t">and much more topics to introduce on how to optimize the objective that we saw, and please</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2886" target="_blank">00:48:06.820</a></span> | <span class="t">come back to my channel for more videos like this, I usually try to make videos that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2890" target="_blank">00:48:10.980</a></span> | <span class="t">very deep, very in depth for every topic, sometimes they can be a little hard, but I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2897" target="_blank">00:48:17.820</a></span> | <span class="t">try to simplify as much as possible, also depending on my knowledge, and also depending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2902" target="_blank">00:48:22.260</a></span> | <span class="t">on how much it is possible to simplify a difficult topic, and if you have any questions, please</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2909" target="_blank">00:48:29.060</a></span> | <span class="t">leave it in the comments, and I will probably keep publishing more videos like this, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2916" target="_blank">00:48:36.020</a></span> | <span class="t">if you want videos that are more simpler, please let me know, and also let me know in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2919" target="_blank">00:48:39.620</a></span> | <span class="t">the comments what kind of topics you would like me to explore next, thank you guys and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8&t=2924" target="_blank">00:48:44.420</a></span> | <span class="t">have a nice day!</span></div></div></body></html>