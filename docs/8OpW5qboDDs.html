<html><head><title>'Pause Giant AI Experiments' - Letter Breakdown w/ Research Papers, Altman, Sutskever and more</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }
        .container {
            width: 80%;
            margin: auto;
            overflow: hidden;
        }
        h2, h3 {
            color: #333;
            text-align: center;
        }
        a {
            color: #0000FF;  /* Traditional blue color for links */
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        img {
            display: block;
            margin: auto;
            max-width: 100%;
        }
        .c {
            margin: 10px 0;
        }
        .s, .t {
            display: inline-block;
            margin-right: 5px;
        }
        .max-width {
            max-width: 800px;
            margin: auto;
        }
    </style>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>'Pause Giant AI Experiments' - Letter Breakdown w/ Research Papers, Altman, Sutskever and more</h2><a href="https://www.youtube.com/watch?v=8OpW5qboDDs"><img src="https://i.ytimg.com/vi/8OpW5qboDDs/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=103">1:43</a> Who signed<br><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=156">2:36</a> Current worries<br><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=194">3:14</a> AI alignment<br><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=243">4:3</a> Demis Hasabis<br><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=267">4:27</a> Emad Mustang<br><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=312">5:12</a> X Risk Analysis<br><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=496">8:16</a> Supplementary Diagram<br><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=526">8:46</a> Super Intelligence<br><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=566">9:26</a> Max Tegmark<br><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=647">10:47</a> AI Safety Statement<br><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=675">11:15</a> Human Extinction<br><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=764">12:44</a> Googles Response<br><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=819">13:39</a> Conclusion<br><br><div style="text-align: left;"><a href="./8OpW5qboDDs.html">Whisper Transcript</a> | <a href="./transcript_8OpW5qboDDs.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=0" target="_blank">00:00:00.160</a></span> | <span class="t">Less than 18 hours ago this letter was published calling for an immediate pause in training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=5" target="_blank">00:00:05.920</a></span> | <span class="t">AI systems more powerful than GPT-4. By now you will have seen the headlines about it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=11" target="_blank">00:00:11.520</a></span> | <span class="t">waving around eye-catching names such as Elon Musk but I want to show you not only what the letter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=16" target="_blank">00:00:16.720</a></span> | <span class="t">says but also the research behind it. The letter quotes 18 supporting documents and I have either</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=22" target="_blank">00:00:22.560</a></span> | <span class="t">gone through or entirely read all of them. You will also hear from those at the top of OpenAI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=28" target="_blank">00:00:28.480</a></span> | <span class="t">and Google on their thoughts. Whether you agree or disagree with the letter I hope you learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=33" target="_blank">00:00:33.120</a></span> | <span class="t">something. So what did it say? First they described the situation as AI labs locked in an out of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=38" target="_blank">00:00:38.640</a></span> | <span class="t">control race to develop and deploy ever more powerful digital minds that no one, not even their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=44" target="_blank">00:00:44.800</a></span> | <span class="t">creators, can understand predict or reliably control. They ask just because we can should we automate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=51" target="_blank">00:00:51.680</a></span> | <span class="t">away all the jobs including the fulfilling ones and other questions like should we risk loss of control</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=56" target="_blank">00:00:56.960</a></span> | <span class="t">of our civilization. So what's their main ask? Well they quote OpenAI's AGI document. At some point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=64" target="_blank">00:01:04.800</a></span> | <span class="t">it may be important to get independent review before starting to train future systems and for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=70" target="_blank">00:01:10.320</a></span> | <span class="t">the most advanced efforts to agree to limit the rate of growth of compute used for creating new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=75" target="_blank">00:01:15.520</a></span> | <span class="t">models and they say we agree that point is now. And here is their call:</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=80" target="_blank">00:01:20.240</a></span> | <span class="t">"Therefore we call on all AI labs to immediately pause for at least six months</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=85" target="_blank">00:01:25.440</a></span> | <span class="t">the training of AI systems more powerful than GPT-4. Notice that they are not saying shut down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=91" target="_blank">00:01:31.200</a></span> | <span class="t">GPT-4 just saying don't train anything smarter or more advanced than GPT-4. They go on if such</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=98" target="_blank">00:01:38.080</a></span> | <span class="t">a pause cannot be enacted quickly governments should step in and institute a moratorium. I will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=103" target="_blank">00:01:43.280</a></span> | <span class="t">come back to some other details in the letter later on but first let's glance at some of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=108" target="_blank">00:01:48.400</a></span> | <span class="t">eye-catching names who have signed this document. We have Stuart Russell who wrote the textbook on AI and Joshua Bengio,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=113" target="_blank">00:01:53.920</a></span> | <span class="t">who pioneered deep learning. Among many other famous names we have the founder of stability AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=121" target="_blank">00:02:01.280</a></span> | <span class="t">which is behind stable diffusion. Of course I could go on and on but we also have names like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=126" target="_blank">00:02:06.240</a></span> | <span class="t">Max Tegmark arguably one of the smartest people on the planet and if you notice below plenty of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=131" target="_blank">00:02:11.760</a></span> | <span class="t">researchers at DeepMind. But before you dismiss this as a bunch of outsiders this is what Sam</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=137" target="_blank">00:02:17.760</a></span> | <span class="t">Altman once wrote in his blog. Many people seem to believe that superhuman machine intelligence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=143" target="_blank">00:02:23.360</a></span> | <span class="t">would be very dangerous if it were developed but think that it's either never going to happen or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=148" target="_blank">00:02:28.640</a></span> | <span class="t">definitely very far off. This is sloppy dangerous thinking. And a few days ago on the Lex Friedman</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=155" target="_blank">00:02:35.120</a></span> | <span class="t">podcast he said this: "I think it's weird when people like think it's like a big dunk that I say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=159" target="_blank">00:02:39.440</a></span> | <span class="t">like I'm a little bit afraid and I think it'd be crazy not to be a little bit afraid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=164" target="_blank">00:02:44.400</a></span> | <span class="t">and I empathize with people who are a lot afraid. Current worries that I have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=168" target="_blank">00:02:48.320</a></span> | <span class="t">are that they're going to be disinformation problems or economic shocks or something else</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=175" target="_blank">00:02:55.680</a></span> | <span class="t">at a level far beyond anything we're prepared for and that doesn't require super intelligence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=182" target="_blank">00:03:02.080</a></span> | <span class="t">that doesn't require a super deep alignment problem in the machine waking up and trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=186" target="_blank">00:03:06.000</a></span> | <span class="t">to deceive us and I don't think that gets enough attention. I mean it's starting to get more I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=194" target="_blank">00:03:14.080</a></span> | <span class="t">Before you think that's just Sam Altman being Sam Altman here's Ilya Satskova who arguably is the brains behind OpenAI and GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=202" target="_blank">00:03:22.960</a></span> | <span class="t">As somebody who deeply understands these models what is your intuition of how hard alignment will be?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=207" target="_blank">00:03:27.040</a></span> | <span class="t">Like I think with the so here's what I would say I think with the current level of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=210" target="_blank">00:03:30.400</a></span> | <span class="t">capabilities I think we have a pretty good set of ideas of how to align them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=213" target="_blank">00:03:33.440</a></span> | <span class="t">but I would not underestimate the difficulty of alignment of models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=217" target="_blank">00:03:37.920</a></span> | <span class="t">that are actually smarter than us of models that are capable of misrepresenting their intentions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=223" target="_blank">00:03:43.520</a></span> | <span class="t">By alignment he means matching up the goal of AI systems with our own and at this point I do want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=229" target="_blank">00:03:49.280</a></span> | <span class="t">to say that there are reasons to have hope on AI alignment and many many people are working on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=234" target="_blank">00:03:54.720</a></span> | <span class="t">I just don't want anyone to underestimate the scale of the task or to think it's just a bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=240" target="_blank">00:04:00.320</a></span> | <span class="t">of outsiders not the creators themselves. Here was a recent interview by Time magazine with Demis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=246" target="_blank">00:04:06.880</a></span> | <span class="t">Hassabis who many people say I sound like. He is the founder of course of DeepMind who are also at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=251" target="_blank">00:04:11.600</a></span> | <span class="t">the cutting edge of large language development. He's also the founder of the company that I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=253" target="_blank">00:04:13.280</a></span> | <span class="t">working with and he's been working on a lot of these things for a long time. He says when it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=254" target="_blank">00:04:14.560</a></span> | <span class="t">comes to very powerful technologies and obviously AI is going to be one of the most powerful ever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=259" target="_blank">00:04:19.680</a></span> | <span class="t">we need to be careful. Not everybody is thinking about those things. It's like experimentalists</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=264" target="_blank">00:04:24.480</a></span> | <span class="t">many of whom don't realise they're holding dangerous material. And again Emad Mostak I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=269" target="_blank">00:04:29.200</a></span> | <span class="t">don't agree with everything in the letter but the race condition ramping as H100s come along</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=277" target="_blank">00:04:37.040</a></span> | <span class="t">is not safe for something the creators consider as potentially an existential risk. Time to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=283" target="_blank">00:04:43.040</a></span> | <span class="t">take a breath, coordinate and carry on. This is only for the largest models. He went on that these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=289" target="_blank">00:04:49.120</a></span> | <span class="t">models can get weird as they get more powerful. So it's not just AI outsiders but what about the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=294" target="_blank">00:04:54.960</a></span> | <span class="t">research they cite? Those 18 supporting documents that I referred to? Well I read each of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=300" target="_blank">00:05:00.160</a></span> | <span class="t">Now for some of them I had already read them. Like the Sparks report that I did a video on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=305" target="_blank">00:05:05.040</a></span> | <span class="t">and the GPT-4 technical report that I also did a video on. Some others like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=309" target="_blank">00:05:09.120</a></span> | <span class="t">super intelligence book by Bostrom I had read when it first came out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=312" target="_blank">00:05:12.800</a></span> | <span class="t">One of the papers was called X-Risk Analysis for AI Research which are risks that threaten</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=317" target="_blank">00:05:17.680</a></span> | <span class="t">the entirety of humanity. Of course the paper had way too much to cover in one video but it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=322" target="_blank">00:05:22.560</a></span> | <span class="t">did lay out 8 speculative hazards and failure modes including AI weaponisation, deception,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=328" target="_blank">00:05:28.880</a></span> | <span class="t">power seeking behaviour. In the appendix they give some examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=332" target="_blank">00:05:32.480</a></span> | <span class="t">Some are concerned that weaponising AI may be an on-ramp to more dangerous outcomes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=337" target="_blank">00:05:37.600</a></span> | <span class="t">In recent years deep reinforcement learning algorithms can outperform humans at aerial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=342" target="_blank">00:05:42.560</a></span> | <span class="t">combat. While AlphaFold has discovered new chemical weapons and they go on to give plenty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=347" target="_blank">00:05:47.440</a></span> | <span class="t">more examples of weaponisation. What about deception? I found this part interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=351" target="_blank">00:05:51.520</a></span> | <span class="t">They say that AI systems could also have incentives to bypass monitors and draw an analogy with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=356" target="_blank">00:05:56.560</a></span> | <span class="t">Volkswagen who program their engines to reduce emissions only when being monitored. It says that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=362" target="_blank">00:06:02.000</a></span> | <span class="t">future AI agents could similarly switch strategies when being monitored and take steps to obscure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=366" target="_blank">00:06:06.880</a></span> | <span class="t">their deception from monitors. With power seeking behaviour they say it has been shown that agents have incentives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=372" target="_blank">00:06:12.320</a></span> | <span class="t">to acquire and maintain power. And they end with this geopolitical quote:</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=376" target="_blank">00:06:16.960</a></span> | <span class="t">"Whoever becomes the leader in AI will become the ruler of the world."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=380" target="_blank">00:06:20.720</a></span> | <span class="t">But again you might wonder if all of the research that was cited comes from outsiders. Well no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=385" target="_blank">00:06:25.920</a></span> | <span class="t">Richard Ngou was the lead author of this paper and he currently works at OpenAI. It's a fascinating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=391" target="_blank">00:06:31.440</a></span> | <span class="t">document on the alignment problem from a deep learning perspective from insiders working with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=396" target="_blank">00:06:36.480</a></span> | <span class="t">these models. The author was the guy who wrote this yesterday on Twitter: "I predict that by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=401" target="_blank">00:06:41.280</a></span> | <span class="t">end of 2021, the AI system will be able to take advantage of the current technology and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=402" target="_blank">00:06:42.080</a></span> | <span class="t">technology of the AI system. I believe that the AI system will be able to take advantage of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=404" target="_blank">00:06:44.640</a></span> | <span class="t">technology of the AI system and the AI system will be able to take advantage of the AI system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=406" target="_blank">00:06:46.960</a></span> | <span class="t">I believe that the AI system will be able to take advantage of the AI system and the AI system will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=409" target="_blank">00:06:49.840</a></span> | <span class="t">be able to take advantage of the AI system and the AI system will be able to take advantage of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=412" target="_blank">00:06:52.960</a></span> | <span class="t">AI system and the AI system will be able to take advantage of the AI system and the AI system will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=414" target="_blank">00:06:54.320</a></span> | <span class="t">be able to take advantage of the AI system and the AI system will be able to take advantage of the AI system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=416" target="_blank">00:06:56.800</a></span> | <span class="t">and the AI system will be able to take advantage of the AI system and the AI system will be able to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=419" target="_blank">00:06:59.200</a></span> | <span class="t">advantage of the AI system and the AI system will be able to take advantage of the AI system and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=420" target="_blank">00:07:00.960</a></span> | <span class="t">AI system will be able to take advantage of the AI system and the AI system will be able to take advantage of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=422" target="_blank">00:07:02.800</a></span> | <span class="t">Well many things but I have picked out some of the most interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=425" target="_blank">00:07:05.860</a></span> | <span class="t">It gave an example of reward hacking where an algorithm learnt to trick humans to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=430" target="_blank">00:07:10.600</a></span> | <span class="t">good feedback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=431" target="_blank">00:07:11.600</a></span> | <span class="t">The task was to grab a ball with a claw and it says that the policy instead learnt to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=436" target="_blank">00:07:16.380</a></span> | <span class="t">place the claw between the camera and the ball in a way that it looked like it was grasping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=441" target="_blank">00:07:21.820</a></span> | <span class="t">the ball and therefore mistakenly received high reward from human supervisors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=447" target="_blank">00:07:27.260</a></span> | <span class="t">Essentially deception to maximise reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=449" target="_blank">00:07:29.620</a></span> | <span class="t">Of course it didn't mean to deceive it was just maximising its reward function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=453" target="_blank">00:07:33.980</a></span> | <span class="t">Next the paper gives details about why these models might want to seek power.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=458" target="_blank">00:07:38.440</a></span> | <span class="t">It quotes the memorable phrase "you can't fetch coffee if you're dead" implying that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=462" target="_blank">00:07:42.820</a></span> | <span class="t">even a policy or an algorithm with a simple goal like fetching coffee would pursue survival</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=468" target="_blank">00:07:48.980</a></span> | <span class="t">as an instrumental sub goal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=470" target="_blank">00:07:50.860</a></span> | <span class="t">In other words the model might realise that if it can't survive it can't achieve its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=474" target="_blank">00:07:54.620</a></span> | <span class="t">reward it can't reach the goal that the humans set for it and therefore it will try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=478" target="_blank">00:07:58.900</a></span> | <span class="t">to survive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=479" target="_blank">00:07:59.600</a></span> | <span class="t">Now I know many people will feel that I'm not covering enough of these fears or covering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=483" target="_blank">00:08:03.920</a></span> | <span class="t">too many of them but I agree with the authors when they conclude with this "Reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=488" target="_blank">00:08:08.280</a></span> | <span class="t">about these topics is difficult but the stakes are sufficiently high that we cannot justify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=493" target="_blank">00:08:13.220</a></span> | <span class="t">disregarding or postponing the work."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=496" target="_blank">00:08:16.060</a></span> | <span class="t">Towards the end of this paper which was also cited by the letter it gave a very helpful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=501" target="_blank">00:08:21.100</a></span> | <span class="t">supplementary diagram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=502" target="_blank">00:08:22.660</a></span> | <span class="t">It showed that even if you don't believe that unaligned AGI is a threat even current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=507" target="_blank">00:08:27.620</a></span> | <span class="t">and near term AI complicate the process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=509" target="_blank">00:08:29.580</a></span> | <span class="t">It also showed that the process could complicate so many other relationships and dynamics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=513" target="_blank">00:08:33.280</a></span> | <span class="t">State to state relations, state to citizen relations, it could complicate social media</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=517" target="_blank">00:08:37.760</a></span> | <span class="t">and recommender systems, it could give the state too much control over citizens and corporations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=522" target="_blank">00:08:42.620</a></span> | <span class="t">like Microsoft and Google too much leverage against the state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=526" target="_blank">00:08:46.180</a></span> | <span class="t">Before I get to some reasons for hope I want to touch on that seminal book superintelligence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=531" target="_blank">00:08:51.340</a></span> | <span class="t">by Bostrom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=532" target="_blank">00:08:52.340</a></span> | <span class="t">I read it almost a decade ago and this quote sticks out:</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=535" target="_blank">00:08:55.520</a></span> | <span class="t">"Before the prospect of an intelligence explosion, we humans are like small trees</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=539" target="_blank">00:08:59.560</a></span> | <span class="t">and children playing with a bomb.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=541" target="_blank">00:09:01.260</a></span> | <span class="t">Such is the mismatch between the power of our plaything and the immaturity of our conduct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=546" target="_blank">00:09:06.700</a></span> | <span class="t">Superintelligence is a challenge for which we are not ready now and will not be ready</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=550" target="_blank">00:09:10.420</a></span> | <span class="t">for a long time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=551" target="_blank">00:09:11.820</a></span> | <span class="t">We have little idea when the detonation will occur though if we hold the device to our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=556" target="_blank">00:09:16.900</a></span> | <span class="t">ear we can hear a faint ticking sound."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=559" target="_blank">00:09:19.740</a></span> | <span class="t">But now let's move on to Max Tegmark one of the signatories and a top physicist and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=564" target="_blank">00:09:24.780</a></span> | <span class="t">AI researcher at MIT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=569" target="_blank">00:09:29.540</a></span> | <span class="t">Max Tegmark:</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=570" target="_blank">00:09:30.540</a></span> | <span class="t">"I think the most unsafe and reckless approach is the alternative to that is intelligible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=579" target="_blank">00:09:39.520</a></span> | <span class="t">intelligence approach instead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=581" target="_blank">00:09:41.440</a></span> | <span class="t">Where we say neural networks is just a tool for the first step to get the intuition but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=587" target="_blank">00:09:47.260</a></span> | <span class="t">then we're going to spend also serious resources on other AI techniques for demystifying this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=593" target="_blank">00:09:53.900</a></span> | <span class="t">black box and figuring out what it's actually doing so we can convert it into something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=598" target="_blank">00:09:58.380</a></span> | <span class="t">that's equally intelligent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=599" target="_blank">00:09:59.520</a></span> | <span class="t">But that we actually understand what it's doing."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=602" target="_blank">00:10:02.100</a></span> | <span class="t">This aligns directly with what Ilya Sutskova, the Open AI chief scientist believes needs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=607" target="_blank">00:10:07.240</a></span> | <span class="t">to be done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=608" target="_blank">00:10:08.240</a></span> | <span class="t">"Do you think we'll ever have a mathematical definition of alignment?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=611" target="_blank">00:10:11.900</a></span> | <span class="t">"Mathematical definition I think is unlikely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=616" target="_blank">00:10:16.760</a></span> | <span class="t">I do think that we will instead have multiple, rather than achieving one mathematical definition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=623" target="_blank">00:10:23.100</a></span> | <span class="t">I think we'll achieve multiple definitions that look at alignment from different aspects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=629" target="_blank">00:10:29.500</a></span> | <span class="t">We'll get the assurance that we want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=631" target="_blank">00:10:31.260</a></span> | <span class="t">And by which I mean you can look at the behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=633" target="_blank">00:10:33.620</a></span> | <span class="t">You can look at the behavior in various tests, in various adversarial stress situations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=639" target="_blank">00:10:39.920</a></span> | <span class="t">You can look at how the neural net operates from the inside.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=642" target="_blank">00:10:42.780</a></span> | <span class="t">I think you have to look at several of these factors at the same time."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=647" target="_blank">00:10:47.180</a></span> | <span class="t">And there are people working on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=649" target="_blank">00:10:49.460</a></span> | <span class="t">Here is the AI safety statement from Anthropic, a huge player in this industry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=654" target="_blank">00:10:54.460</a></span> | <span class="t">In the section on mechanistic interpretability, which is understanding the machines, they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=658" target="_blank">00:10:58.920</a></span> | <span class="t">say this:</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=659" target="_blank">00:10:59.480</a></span> | <span class="t">"We also understand significantly more about the mechanisms of neural network computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=665" target="_blank">00:11:05.140</a></span> | <span class="t">than we did even a year ago, such as those responsible for memorization."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=669" target="_blank">00:11:09.020</a></span> | <span class="t">So progress is being made, but even if there's only a tiny risk of existential harm, more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=674" target="_blank">00:11:14.280</a></span> | <span class="t">needs to be done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=675" target="_blank">00:11:15.280</a></span> | <span class="t">The co-founders of the Center for Humane Technology put it like this:</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=679" target="_blank">00:11:19.460</a></span> | <span class="t">"It would be the worst of all human mistakes to have ever been made.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=683" target="_blank">00:11:23.240</a></span> | <span class="t">And we literally don't know how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=685" target="_blank">00:11:25.200</a></span> | <span class="t">We don't know all the things it will do, and we're putting it out there before we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=689" target="_blank">00:11:29.460</a></span> | <span class="t">actually know whether it's safe."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=691" target="_blank">00:11:31.220</a></span> | <span class="t">Raskin points to a recent survey of AI researchers, where nearly half said they believe there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=697" target="_blank">00:11:37.420</a></span> | <span class="t">at least a 10 percent chance AI could eventually result in an extremely bad outcome, like human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=705" target="_blank">00:11:45.420</a></span> | <span class="t">extinction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=706" target="_blank">00:11:46.420</a></span> | <span class="t">"Where do you come down on that?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=708" target="_blank">00:11:48.180</a></span> | <span class="t">"I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=709" target="_blank">00:11:49.180</a></span> | <span class="t">The point is..."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=710" target="_blank">00:11:50.180</a></span> | <span class="t">"That scares me, you don't know."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=712" target="_blank">00:11:52.180</a></span> | <span class="t">"Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=713" target="_blank">00:11:53.180</a></span> | <span class="t">Here's the point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=714" target="_blank">00:11:54.180</a></span> | <span class="t">Imagine you're about to get on an airplane, and 50 percent of the engineers that built</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=718" target="_blank">00:11:58.320</a></span> | <span class="t">the airplane say there's a 10 percent chance that it's safe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=719" target="_blank">00:11:59.440</a></span> | <span class="t">And that's a 10 percent chance that their plane might crash and kill everyone."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=723" target="_blank">00:12:03.400</a></span> | <span class="t">"Leave me at the gate."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=724" target="_blank">00:12:04.400</a></span> | <span class="t">"Exactly."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=725" target="_blank">00:12:05.400</a></span> | <span class="t">Here is the survey from last year of hundreds of AI researchers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=729" target="_blank">00:12:09.560</a></span> | <span class="t">And you can contrast that with a similar survey from seven years ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=733" target="_blank">00:12:13.060</a></span> | <span class="t">The black bar represents the proportion of these researchers who believe, to differing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=737" target="_blank">00:12:17.360</a></span> | <span class="t">degrees of probability, in extremely bad outcomes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=740" target="_blank">00:12:20.720</a></span> | <span class="t">You can see that it's small, but it is rising.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=742" target="_blank">00:12:22.820</a></span> | <span class="t">One way to think of this is to use Sam Altman's own example of the Fermi Paradox, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=747" target="_blank">00:12:27.600</a></span> | <span class="t">the strange fact that we can't see the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=749" target="_blank">00:12:29.420</a></span> | <span class="t">We can't see or detect any aliens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=751" target="_blank">00:12:31.440</a></span> | <span class="t">He says, "One of my top four favorite explanations for the Fermi Paradox is that biological intelligence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=756" target="_blank">00:12:36.980</a></span> | <span class="t">always eventually creates machine intelligence, which wipes out biological life and then for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=761" target="_blank">00:12:41.540</a></span> | <span class="t">some reason decides to make itself undetectable."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=764" target="_blank">00:12:44.240</a></span> | <span class="t">Others, such as Dustin Tran at Google, are not as impressed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=768" target="_blank">00:12:48.360</a></span> | <span class="t">He refers to the letter and says, "This call has valid concerns but is logistically impossible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=773" target="_blank">00:12:53.500</a></span> | <span class="t">It's hard to take seriously."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=774" target="_blank">00:12:54.900</a></span> | <span class="t">He is a research scientist at Google Brain and the evaluation lead for BARD.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=779" target="_blank">00:12:59.400</a></span> | <span class="t">There was another, indirect reaction that I found interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=782" target="_blank">00:13:02.620</a></span> | <span class="t">One of the other books referenced was the alignment problem: machine learning and human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=786" target="_blank">00:13:06.240</a></span> | <span class="t">values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=787" target="_blank">00:13:07.240</a></span> | <span class="t">Now long before the letter even came out, the CEO of Microsoft read that book and gave</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=791" target="_blank">00:13:11.820</a></span> | <span class="t">this review.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=792" target="_blank">00:13:12.820</a></span> | <span class="t">Nadella says that Christian offers a clear and compelling description and says that machines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=797" target="_blank">00:13:17.920</a></span> | <span class="t">that learn for themselves become increasingly autonomous and potentially unethical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=802" target="_blank">00:13:22.700</a></span> | <span class="t">My next video is going to be on the reflection paper and how models like GPT-4 can teach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=807" target="_blank">00:13:27.840</a></span> | <span class="t">themselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=809" target="_blank">00:13:29.380</a></span> | <span class="t">I'm working with the co-author of that paper to give you guys more of an overview.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=813" target="_blank">00:13:33.160</a></span> | <span class="t">Because even Nadella admits that if they learn for themselves and become autonomous it could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=818" target="_blank">00:13:38.060</a></span> | <span class="t">be unethical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=819" target="_blank">00:13:39.060</a></span> | <span class="t">The letter concludes on a more optimistic note.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=821" target="_blank">00:13:41.560</a></span> | <span class="t">They say, "This does not mean a pause on AI development in general, merely a stepping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=826" target="_blank">00:13:46.560</a></span> | <span class="t">back from the dangerous race to ever larger, unpredictable black box models with emergent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=832" target="_blank">00:13:52.640</a></span> | <span class="t">capabilities like self-teaching."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=834" target="_blank">00:13:54.720</a></span> | <span class="t">I've got so much more to say on self-teaching but that will have to wait until the next video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=839" target="_blank">00:13:59.360</a></span> | <span class="t">For now though, let's end on this note.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=841" target="_blank">00:14:01.260</a></span> | <span class="t">Let's enjoy a long AI summer, not rush unprepared into a fall.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=8OpW5qboDDs&t=846" target="_blank">00:14:06.420</a></span> | <span class="t">Thanks for watching all the way to the end and let me know what you think.</span></div></div></body></html>