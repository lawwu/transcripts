<html><head><title>[Paper Club] Embeddings in 2024: OpenAI, Nomic Embed, Jina Embed, cde-small-v1 - with swyx</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>[Paper Club] Embeddings in 2024: OpenAI, Nomic Embed, Jina Embed, cde-small-v1 - with swyx</h2><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo"><img src="https://i.ytimg.com/vi/VIqXNRsRRQo/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./VIqXNRsRRQo.html">Whisper Transcript</a> | <a href="./transcript_VIqXNRsRRQo.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">there was a whole bunch of interesting embedding work piling up, and I figured it'd be good to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=6" target="_blank">00:00:06.320</a></span> | <span class="t">have a state of embeddings overview. And so we have basically one blog post and three papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=13" target="_blank">00:00:13.200</a></span> | <span class="t">that I've sort of defined in scope. They're all listed in the meeting notes here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=19" target="_blank">00:00:19.760</a></span> | <span class="t">And I would consider this basically everything that is relevant for understanding embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=28" target="_blank">00:00:28.160</a></span> | <span class="t">as of today. And so I think that the first thing is to understand MTEB, which is Massive Text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=34" target="_blank">00:00:34.800</a></span> | <span class="t">Embedding Benchmark. This is the sort of de facto benchmark. There are criticisms of it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=39" target="_blank">00:00:39.360</a></span> | <span class="t">but it's a pretty-- if you don't know-- if you use embeddings and you don't know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=46" target="_blank">00:00:46.320</a></span> | <span class="t">MTEB, you don't know embeddings at all. This changes a lot. It used to be that the Chinese</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=54" target="_blank">00:00:54.160</a></span> | <span class="t">models were completely dominating the top 10. Now we have American Chinese models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=59" target="_blank">00:00:59.920</a></span> | <span class="t">other Chinese models, and I don't know some of these guys. So I wouldn't pay strict attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=68" target="_blank">00:01:08.240</a></span> | <span class="t">to the ranking of these things, but just to know the main benchmarks that people care about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=76" target="_blank">00:01:16.320</a></span> | <span class="t">as well as the trade-offs in model size and memory usage. This becomes extremely,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=83" target="_blank">00:01:23.040</a></span> | <span class="t">extremely relevant when it comes to efficiency comments. Even though Stella is ranked as number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=90" target="_blank">00:01:30.000</a></span> | <span class="t">six, they're at least an order of magnitude more efficient in model size for the same amount of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=96" target="_blank">00:01:36.320</a></span> | <span class="t">performance that you might get from a much larger model. So practically, you might just use this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=102" target="_blank">00:01:42.320</a></span> | <span class="t">instead of something that's higher ranked. The other thing I think is also relevant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=109" target="_blank">00:01:49.280</a></span> | <span class="t">is I think-- I'm not sure, but I don't know if they actually-- yeah, so they have everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=115" target="_blank">00:01:55.840</a></span> | <span class="t">in here, including CDE-small, which we're going to cover. What is text-emitting for? Oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=121" target="_blank">00:02:01.440</a></span> | <span class="t">this is for text. Got it. Where's the-- so I don't know where the OpenAI models land, but I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=130" target="_blank">00:02:10.320</a></span> | <span class="t">definitely people should understand that this is the latest update for the OpenAI offering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=136" target="_blank">00:02:16.480</a></span> | <span class="t">Typically, you want to at least be familiar to OpenAI offerings, just because that tends to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=141" target="_blank">00:02:21.600</a></span> | <span class="t">the starting point. That's the API key that everyone already has, rather than adding a new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=146" target="_blank">00:02:26.320</a></span> | <span class="t">API key for other models. So they're not going to be the best in the world, but they're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=152" target="_blank">00:02:32.560</a></span> | <span class="t">to be very, very good, and usually that's good enough. I would say the other thing to be aware</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=157" target="_blank">00:02:37.920</a></span> | <span class="t">of is for the first time, they're offering two different sizes and also Matrioshka embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=165" target="_blank">00:02:45.520</a></span> | <span class="t">which-- Matrioshka. Oh. Do you know how to-- where do I find the document? I think they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=176" target="_blank">00:02:56.240</a></span> | <span class="t">didn't mention it. They did. I think they did. They added it to the blog post at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=188" target="_blank">00:03:08.080</a></span> | <span class="t">Okay, you're going to see all my emails. Okay, well, never mind. Maybe-- so I'm just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=196" target="_blank">00:03:16.000</a></span> | <span class="t">setting up this browser for the first time, so OpenAI text embeddings. There we go. Here. No.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=204" target="_blank">00:03:24.320</a></span> | <span class="t">It's 2022. Does anyone have that link? Are you looking for Matrioshka or the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=214" target="_blank">00:03:34.880</a></span> | <span class="t">where they referenced Matrioshka? Where they refreshed it. It would be really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=219" target="_blank">00:03:39.840</a></span> | <span class="t">awesome if they actually had it here. Nope. Embeddings. Okay, I can't find it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=230" target="_blank">00:03:50.240</a></span> | <span class="t">Okay, it's actually the first pop-down. If you scroll to this link, I'm pasting in the chat here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=241" target="_blank">00:04:01.760</a></span> | <span class="t">Give me a second. Okay, I'm pasting this in the chat here. If you open it, it's the link that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=247" target="_blank">00:04:07.360</a></span> | <span class="t">had. 2024? No, no. The link that you shared. Yeah, click on that. And if you scroll down,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=257" target="_blank">00:04:17.760</a></span> | <span class="t">scroll down a little bit. Scroll down a little bit more. Ah, reducing embedding dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=263" target="_blank">00:04:23.520</a></span> | <span class="t">There we go. Do you see that? It's actually hidden in there. I don't know if actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=268" target="_blank">00:04:28.480</a></span> | <span class="t">the word Matrioshka shows up. Whoever commented in the chat,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=274" target="_blank">00:04:34.320</a></span> | <span class="t">Kishore sent the blog post that I was looking for, and he did put it in the footnotes because the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=282" target="_blank">00:04:42.480</a></span> | <span class="t">author complained that they were not credited, which is very, very shady of OpenAI. So yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=289" target="_blank">00:04:49.040</a></span> | <span class="t">these guys were the first to offer it, and it is very good. We'll see later in one of the Gina</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=296" target="_blank">00:04:56.640</a></span> | <span class="t">postings how efficient it is. I don't think they communicated very well here, but let me just skip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=303" target="_blank">00:05:03.360</a></span> | <span class="t">ahead to the Gina posting, and then we'll show you. So the Matrioshka embeddings lets you reduce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=309" target="_blank">00:05:09.120</a></span> | <span class="t">the amount of data that you store. This is so annoying. Wait, when I have the image in my head...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=318" target="_blank">00:05:18.320</a></span> | <span class="t">Wait, did it get rid of it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=326" target="_blank">00:05:26.400</a></span> | <span class="t">Wow, they got rid of it. Okay, so I guess I have to refer to my own blog post about it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=332" target="_blank">00:05:32.320</a></span> | <span class="t">because they got rid of it. Oh, maybe it was in the paper. Ah, okay, yeah, it was the paper. Sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=338" target="_blank">00:05:38.480</a></span> | <span class="t">I'm so sorry. Let me refer to my own notes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=353" target="_blank">00:05:53.200</a></span> | <span class="t">Here, here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=356" target="_blank">00:05:56.960</a></span> | <span class="t">there we go. So when you offer Matrioshka, you can do something like this, where you compress,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=366" target="_blank">00:06:06.720</a></span> | <span class="t">like, let's say the original output dimensions is 1024. You can compress it to 64, so you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=372" target="_blank">00:06:12.480</a></span> | <span class="t">reducing the amount of storage space by 94%, and that only results in an 8% drop. So basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=379" target="_blank">00:06:19.040</a></span> | <span class="t">from here, 1024, down to 64, your performance drops from 75 to, like, 69 or whatever,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=387" target="_blank">00:06:27.280</a></span> | <span class="t">which is pretty good. So accuracy at 1 would be, like, 47.2, going down to 41.3, so that's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=398" target="_blank">00:06:38.080</a></span> | <span class="t">like, a 6% drop, and then accuracy at 5 would be 75.3, going down to 69.4. So, like, that's a huge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=407" target="_blank">00:06:47.440</a></span> | <span class="t">amount of information, that is, storage that is saved, as well as compute everything, for a really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=417" target="_blank">00:06:57.040</a></span> | <span class="t">good drop. And basically, OpenAI pioneered this. They were the first to acknowledge that this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=425" target="_blank">00:07:05.120</a></span> | <span class="t">relevant, and now, basically, every offering should do it. And, yeah, I think that's, those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=432" target="_blank">00:07:12.160</a></span> | <span class="t">are, those are state-of-the-art. I don't know if anyone else has played with the OpenAI embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=436" target="_blank">00:07:16.320</a></span> | <span class="t">models enough to offer any more notes before I move on to the Open models, but I just want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=441" target="_blank">00:07:21.280</a></span> | <span class="t">start with OpenAI. And the thing is, training these matriarchal embeddings essentially comes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=445" target="_blank">00:07:25.680</a></span> | <span class="t">for free. You just need to update the loss function. Yes. You can do it, and I've tried</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=450" target="_blank">00:07:30.720</a></span> | <span class="t">something like this, where I cut embeddings by a quarter of the size, and it's almost,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=454" target="_blank">00:07:34.640</a></span> | <span class="t">it's almost as good fidelity. And the thing is, okay, you might think that 1024 to 64, okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=460" target="_blank">00:07:40.320</a></span> | <span class="t">that's not such a big drop, but 1024 is just not usable in production, depending on your production</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=465" target="_blank">00:07:45.680</a></span> | <span class="t">use cases, you may not be able to meet the latency, but 64, 128, those are amazing. So it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=472" target="_blank">00:07:52.400</a></span> | <span class="t">essentially the boundary between what's usable and what's not. What exactly, so Dan says 1024</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=477" target="_blank">00:07:57.760</a></span> | <span class="t">is enormous. I mean, I don't have a, like, what do you mean, it's just, it's just more numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=482" target="_blank">00:08:02.640</a></span> | <span class="t">to store. Like, if you think about it this way, as your embedding size increases, your approximate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=489" target="_blank">00:08:09.440</a></span> | <span class="t">nearest neighbors lookup will increase as well. So this is more about, like, the n squared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=494" target="_blank">00:08:14.320</a></span> | <span class="t">explosion. It's like, yeah, looking up, doing dot products, etc. So it just costs more to compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=499" target="_blank">00:08:19.520</a></span> | <span class="t">Vibhu, are you there? He's like, I think you have a lot more to add to the embedding search space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=506" target="_blank">00:08:26.400</a></span> | <span class="t">segment. I'm not sure, Vibhu's, I know he's in San Diego with family, so I don't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=515" target="_blank">00:08:35.840</a></span> | <span class="t">know if he's able to comment. I think he dropped out. Yeah, like, we've already done a session on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=522" target="_blank">00:08:42.560</a></span> | <span class="t">MRL, so we can refer people to that MRL paper if they want to. I was just more just, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=529" target="_blank">00:08:49.680</a></span> | <span class="t">you know, what should you know with, like, state-of-the-art end-of-2024 embeddings?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=534" target="_blank">00:08:54.880</a></span> | <span class="t">This would be it. There's probably different sizes that you should be aware of, you should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=541" target="_blank">00:09:01.360</a></span> | <span class="t">know the models, you should know the costs. It's very cheap. I feel like they're basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=551" target="_blank">00:09:11.040</a></span> | <span class="t">embedding this for you at cost, mostly because embeddings are a fantastic form of lock-in for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=556" target="_blank">00:09:16.720</a></span> | <span class="t">any API provider, because once you've embedded something, you still have to get it back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=561" target="_blank">00:09:21.680</a></span> | <span class="t">So let me just continue, unless Sam has other questions that people can answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=570" target="_blank">00:09:30.720</a></span> | <span class="t">So then we're going to move on to the papers. I think the first one I would highlight is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=575" target="_blank">00:09:35.520</a></span> | <span class="t">NOMIC, because the reason I picked this was because someone was asking whether there's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=582" target="_blank">00:09:42.800</a></span> | <span class="t">good paper on the full training process of a model, and NOMIC is the closest that I can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=592" target="_blank">00:09:52.560</a></span> | <span class="t">find, that I know of. Definitely, and there's a US bias here, because there's a whole bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=600" target="_blank">00:10:00.880</a></span> | <span class="t">Chinese embedding papers that probably have some detail on their training process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=605" target="_blank">00:10:05.680</a></span> | <span class="t">But NOMIC has open source code, open data, open training code, and full reproducibility, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=612" target="_blank">00:10:12.640</a></span> | <span class="t">in my mind is good enough, if you wanted to deep dive into that. The main thing I would highlight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=622" target="_blank">00:10:22.400</a></span> | <span class="t">is the "actually use" part, which is a good follow-up from last week. Basically, what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=633" target="_blank">00:10:33.680</a></span> | <span class="t">call the Nome-Shazier stack is pretty standard. These are all basically state-of-the-art in terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=640" target="_blank">00:10:40.560</a></span> | <span class="t">of training processes and training tech, as far as I understand from every single model trainer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=647" target="_blank">00:10:47.040</a></span> | <span class="t">that I've talked to. I'm not sure about the masking. I actually did not understand. I thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=654" target="_blank">00:10:54.560</a></span> | <span class="t">that you just kind of mask individual tokens. I thought that was standard. I didn't know there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=658" target="_blank">00:10:58.560</a></span> | <span class="t">was a hyperparameter here around mask rate of 30% versus 15%, and it's not something that I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=665" target="_blank">00:11:05.840</a></span> | <span class="t">familiar with, and neither am I familiar with a lot of these other types of sort of BERT-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=672" target="_blank">00:11:12.400</a></span> | <span class="t">models. But I'm curious if anyone has thoughts or questions around what you would like to--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=679" target="_blank">00:11:19.520</a></span> | <span class="t">Dr. Charles already came and saw you, right? Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=681" target="_blank">00:11:21.360</a></span> | <span class="t">You're unmuted. I don't know if that's on purpose. Sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=687" target="_blank">00:11:27.280</a></span> | <span class="t">Has anyone checked out NOMIC? Are you interested in going into any detail? I'm fairly friendly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=698" target="_blank">00:11:38.000</a></span> | <span class="t">with that team. Kishore says, "Original BERT paper masks 15%." Oh, I didn't know that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=702" target="_blank">00:11:42.960</a></span> | <span class="t">Yeah. So, yeah. I mean, that was a new finding for me. The rest of it, I think, was relatively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=711" target="_blank">00:11:51.360</a></span> | <span class="t">unsurprising for its time. I would say the interesting thing-- I mean, one of the reasons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=720" target="_blank">00:12:00.560</a></span> | <span class="t">that NOMIC is investing in this is because they sell a cluster visualization tool, which is NOMIC</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=726" target="_blank">00:12:06.880</a></span> | <span class="t">Atlas. And so, they're interested in basically just building tools for embeddings for you to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=735" target="_blank">00:12:15.520</a></span> | <span class="t">explore your datasets. RJ says, "Is there a study of vector retrieval speed versus embedding size?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=741" target="_blank">00:12:21.840</a></span> | <span class="t">No, but I guess they're correlated. I don't know what specifically you would want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=749" target="_blank">00:12:29.520</a></span> | <span class="t">Yeah. More detail would be great. But yeah. So, I would say if you want the sort of state-of-the-art</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=757" target="_blank">00:12:37.120</a></span> | <span class="t">process of paper or data or code, I would just come and grab it off of here. They've found this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=765" target="_blank">00:12:45.200</a></span> | <span class="t">a lot. RJ says, "Discussing large embeddings equals bad, so you want to quantify." Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=775" target="_blank">00:12:55.760</a></span> | <span class="t">How would you quantify it, Eugene? It sounds like you've had some--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=778" target="_blank">00:12:58.800</a></span> | <span class="t">I got you, RJ. And I can address this when you finish, whatever you want to say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=785" target="_blank">00:13:05.680</a></span> | <span class="t">and we turn off recordings. Okay. All right. Keep that in mind as we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=792" target="_blank">00:13:12.000</a></span> | <span class="t">But yeah. I mean, and you can go through the NOMIC paper here. I would say pretty straightforward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=801" target="_blank">00:13:21.600</a></span> | <span class="t">training stuff here. I just think it's nice to have a starting document where you just have all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=807" target="_blank">00:13:27.120</a></span> | <span class="t">the tech choices, the hyperparameters, and reproducible code. I don't know. To me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=814" target="_blank">00:13:34.720</a></span> | <span class="t">this is where you start. I also think that these prefixes and stuff basically completely reflect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=823" target="_blank">00:13:43.600</a></span> | <span class="t">BERT. This is just updating BERT in every shape and form, which is kind of nice. I never really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=830" target="_blank">00:13:50.800</a></span> | <span class="t">thought about that. These are all the Chinese models that I talked about. If you want their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=836" target="_blank">00:13:56.960</a></span> | <span class="t">papers, I'm sure they're all reflected here as well. I have not read them. But yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=841" target="_blank">00:14:01.440</a></span> | <span class="t">I was a little surprised that it was just BERT updated and modified slightly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=848" target="_blank">00:14:08.240</a></span> | <span class="t">But I wonder if that's because the true value out of this neural net would be in the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=858" target="_blank">00:14:18.720</a></span> | <span class="t">that's coming into it, meaning it's more dependent on the data than the architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=863" target="_blank">00:14:23.760</a></span> | <span class="t">I say that, but it's likely both. >> Yeah. Yeah. I've got nothing for you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=871" target="_blank">00:14:31.920</a></span> | <span class="t">there. One comment I'll share as well about this that I have had other founders tell me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=880" target="_blank">00:14:40.800</a></span> | <span class="t">which is that it's very surprising that all embedding models are effectively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=886" target="_blank">00:14:46.560</a></span> | <span class="t">general purpose, and there's no code embedding models. If you look at the NOMIC datasets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=897" target="_blank">00:14:57.200</a></span> | <span class="t">code is number 10 down the list for less than 1% of the dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=907" target="_blank">00:15:07.280</a></span> | <span class="t">And in StackExchange, it's maybe down here. The StackExchange is not even a code-specific thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=911" target="_blank">00:15:11.680</a></span> | <span class="t">People have had the Codiums and the Cursors of the World and MorphLabs, they've had to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=920" target="_blank">00:15:20.880</a></span> | <span class="t">their own code embedding models that they don't release, which is surprising. It's IP,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=927" target="_blank">00:15:27.760</a></span> | <span class="t">but it sounds like a high-potential thing for some PhD person to publish as their open research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=936" target="_blank">00:15:36.080</a></span> | <span class="t">article, because as of right now, every single embedding model is just general purpose language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=940" target="_blank">00:15:40.800</a></span> | <span class="t">and obviously that is different. We'll cover a little bit of how to change that with CDEs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=947" target="_blank">00:15:47.840</a></span> | <span class="t">but I think I'll move on to Gina, unless anyone has issues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=951" target="_blank">00:15:51.520</a></span> | <span class="t">Okay. So, NOMIC was very focused on single language, English. I think they have some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=965" target="_blank">00:16:05.280</a></span> | <span class="t">multilingual capability. I don't know what the... Spanish. It wasn't covered in here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=980" target="_blank">00:16:20.320</a></span> | <span class="t">but I did talk to them about this. But anyway, so Gina, specifically as a European company,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=984" target="_blank">00:16:24.880</a></span> | <span class="t">very, very focused on multilinguality, so this would be their update. I would also say that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=992" target="_blank">00:16:32.400</a></span> | <span class="t">I've been very impressed by their out-of-the-box offering. So, when we talked about clip embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1000" target="_blank">00:16:40.320</a></span> | <span class="t">right? So, this is one of the AI News articles from last week. You can see that... You can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1008" target="_blank">00:16:48.320</a></span> | <span class="t">the difference between a paper that is very focused on research technique and algorithms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1015" target="_blank">00:16:55.200</a></span> | <span class="t">and a paper that is focused on being a technical specification for an API that they intend to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1021" target="_blank">00:17:01.360</a></span> | <span class="t">offer. And so, Gina Clip 2 is basically... I'm just gonna chuck that in here as part of the reading.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1028" target="_blank">00:17:08.400</a></span> | <span class="t">Gina Clip 2 came out of the box. This is actually what I was looking for, by the way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1034" target="_blank">00:17:14.960</a></span> | <span class="t">So, Gina Clip 2 came out of the box with, like, here's how you deploy to AWS, Azure, Google Cloud.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1041" target="_blank">00:17:21.520</a></span> | <span class="t">You won't get this from an Apple paper. And that's just because they're trying to make money off of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1047" target="_blank">00:17:27.520</a></span> | <span class="t">their API calls, right? But let's rewind to embeddings. So, basically, they have... They've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1054" target="_blank">00:17:34.400</a></span> | <span class="t">been running their own embeddings for a while. They updated this in September. And their focus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1060" target="_blank">00:17:40.400</a></span> | <span class="t">has been multilinguality. So, there's a variant of MTB for multilinguality. I don't know if it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1065" target="_blank">00:17:45.280</a></span> | <span class="t">here. It's just Chinese. French. Yeah. There's some Japanese somewhere as well. I don't think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1074" target="_blank">00:17:54.960</a></span> | <span class="t">it's this specific leaderboard, but there's a different leaderboard as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1078" target="_blank">00:17:58.080</a></span> | <span class="t">And it's mostly a functional dataset. I don't think there's anything particularly I'll call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1085" target="_blank">00:18:05.520</a></span> | <span class="t">out here apart from, like, they also, you know, have, like, really, really good thoughts on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1091" target="_blank">00:18:11.120</a></span> | <span class="t">scaling laws and the kind of dataset that, like, works well for cross-language transfer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1100" target="_blank">00:18:20.240</a></span> | <span class="t">So, they support 89 languages, which is pretty massive. And I think they're also very practical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1106" target="_blank">00:18:26.160</a></span> | <span class="t">around, like, the size of model. If you look at the size of the models here, some of them are,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1111" target="_blank">00:18:31.600</a></span> | <span class="t">like, 7D models, which is huge. And so, like, yeah. I don't know if the people are actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1119" target="_blank">00:18:39.120</a></span> | <span class="t">interested in using these 7D models. This is definitely sort of benchmark maxing compared to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1125" target="_blank">00:18:45.440</a></span> | <span class="t">the more practical oriented people who are, like, no, like, you actually want to use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1129" target="_blank">00:18:49.440</a></span> | <span class="t">like, a Roberta and keep it to, like, sub 1B for actual sort of inferencing for embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1137" target="_blank">00:18:57.840</a></span> | <span class="t">The other thing that I will call out here is just, like, the LoRa adapters. They also introduce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1150" target="_blank">00:19:10.240</a></span> | <span class="t">these, like, this concept of, like, task-specific LoRa adapters. So, let me see if they cover it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1156" target="_blank">00:19:16.240</a></span> | <span class="t">Yeah. So, this is where you start to see, like, instead of the traditional single model type of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1170" target="_blank">00:19:30.160</a></span> | <span class="t">embedding model they use, where it's, like, this is, like, basically everything that we had up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1177" target="_blank">00:19:37.680</a></span> | <span class="t">till 2024 was just, like, single embedding model. Here we have task-specific adapters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1183" target="_blank">00:19:43.520</a></span> | <span class="t">and we'll see another form of adapters with the last paper today. But the – where am I looking at?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1193" target="_blank">00:19:53.120</a></span> | <span class="t">Where are the adapters? Okay. Yeah. So, they have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1205" target="_blank">00:20:05.600</a></span> | <span class="t">retrieval for documents, retrieval for queries, separation of documents and clustering them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1211" target="_blank">00:20:11.760</a></span> | <span class="t">classifying them, and then text matching, which is, I think, the classic workload.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1216" target="_blank">00:20:16.080</a></span> | <span class="t">And I think, like, we tend to use, at least in traditional RAG, and how I learned it and how I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1222" target="_blank">00:20:22.560</a></span> | <span class="t">think most people use it, we tend to use the same embedding model in the same mode for all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1226" target="_blank">00:20:26.480</a></span> | <span class="t">things and maybe try to prompt differently or preprocess differently to get performance out of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1232" target="_blank">00:20:32.000</a></span> | <span class="t">them. But training Loras for individual models for the different RAG tasks, I think, is very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1239" target="_blank">00:20:39.440</a></span> | <span class="t">interesting and probably, like, a very good idea, because they're basically doing different things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1246" target="_blank">00:20:46.640</a></span> | <span class="t">They have different tasks over here, and I think there's, like, obligations for, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1251" target="_blank">00:20:51.760</a></span> | <span class="t">the accuracy and precision of each of these things. But, like, you know, I would say the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1258" target="_blank">00:20:58.640</a></span> | <span class="t">main contribution of this paper is just the idea that you should have task adapters. They also have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1263" target="_blank">00:21:03.040</a></span> | <span class="t">MRLs. So, like, I don't think we should just, you know, I'm just going to leave the MRL discussion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1267" target="_blank">00:21:07.600</a></span> | <span class="t">aside. Like, we all know that it's good. I think the main idea to get from here is the, sort of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1272" target="_blank">00:21:12.560</a></span> | <span class="t">idea of task-specific adapters. Does anyone have questions? I haven't been looking at the chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1276" target="_blank">00:21:16.080</a></span> | <span class="t">Okay, people are still talking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1281" target="_blank">00:21:21.600</a></span> | <span class="t">It's mostly the debate on the dimension size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1284" target="_blank">00:21:24.800</a></span> | <span class="t">What?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1286" target="_blank">00:21:26.800</a></span> | <span class="t">It's mostly the debate on dimension size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1292" target="_blank">00:21:32.320</a></span> | <span class="t">I was hoping that they would have done an ablation of the adapters. They did an ablation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1298" target="_blank">00:21:38.400</a></span> | <span class="t">of one versus two adapters, but they didn't do one on no adapter versus adapter, which is, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1304" target="_blank">00:21:44.240</a></span> | <span class="t">in my opinion, more interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1309" target="_blank">00:21:49.040</a></span> | <span class="t">No adapter versus adapter. You mean on their old model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1312" target="_blank">00:21:52.320</a></span> | <span class="t">Isn't that table six?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1314" target="_blank">00:21:54.320</a></span> | <span class="t">Table six in the results. Yeah, it's like, they did, if you look at it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1323" target="_blank">00:22:03.920</a></span> | <span class="t">the second row from the, yeah, Gina is not, Gina V2 has no adapters. And then, you know, Gina V3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1330" target="_blank">00:22:10.320</a></span> | <span class="t">one star is, like, pair training. I think that's no adapter. And then they have a retriever adapter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1335" target="_blank">00:22:15.680</a></span> | <span class="t">which is the last row, which you can see a huge boost, well, specifically for retriever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1340" target="_blank">00:22:20.720</a></span> | <span class="t">Yeah, at least that's how I interpreted it. Please let me know if I misread it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1345" target="_blank">00:22:25.680</a></span> | <span class="t">Yeah. Okay, it's intuitive that it's a fairly big lift. I mean, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1352" target="_blank">00:22:32.320</a></span> | <span class="t">I think, I can't remember the actual wording, but the most influential thing somebody said to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1357" target="_blank">00:22:37.440</a></span> | <span class="t">me was, like, you know, like, blindly applying embedding models onto any arbitrary task without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1364" target="_blank">00:22:44.640</a></span> | <span class="t">actually reading the paper and how it's trained, like, it's asking for failure, because, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1368" target="_blank">00:22:48.560</a></span> | <span class="t">embedding models have very specific assumptions into it. And so, it makes sense that splitting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1374" target="_blank">00:22:54.720</a></span> | <span class="t">out the assumptions into, like, the top five use cases and splitting them out would have very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1379" target="_blank">00:22:59.200</a></span> | <span class="t">material impact on how the embedding works. And, I mean, you can look at the numbers here. They're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1385" target="_blank">00:23:05.600</a></span> | <span class="t">pretty big lifts. Also, that's it. Take the results here with a pinch of salt, though. If you scroll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1390" target="_blank">00:23:10.240</a></span> | <span class="t">down a little bit more, Sykes, in the second paragraph on the left, you can see that their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1394" target="_blank">00:23:14.720</a></span> | <span class="t">evaluation set size is only 10, fewer than 10 examples. So, they added synthetically generated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1402" target="_blank">00:23:22.080</a></span> | <span class="t">data, et cetera, et cetera. Yeah. So, we'll see. But it's a very good result, and I'm glad people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1409" target="_blank">00:23:29.440</a></span> | <span class="t">pushing on using adapters more. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1415" target="_blank">00:23:35.760</a></span> | <span class="t">So, I guess the other thing from the other paper, the NOMIC paper, they had used prefixes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1421" target="_blank">00:23:41.360</a></span> | <span class="t">which apparently is a thing that has been done in training for a little bit. And the prefix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1428" target="_blank">00:23:48.480</a></span> | <span class="t">is kind of, in my mind, similar to the adapter in that you're just training different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1434" target="_blank">00:23:54.480</a></span> | <span class="t">tagging things. But the difference with the adapter is you have a different loss function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1440" target="_blank">00:24:00.080</a></span> | <span class="t">right, per type. And so, I would have been interested to see an ablation there as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1446" target="_blank">00:24:06.480</a></span> | <span class="t">I mean, I would say, I would agree with you that prefixes are a standard part of the toolkit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1454" target="_blank">00:24:14.640</a></span> | <span class="t">Therefore, that would be kind of covered in the base V2 versus the V3s.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1460" target="_blank">00:24:20.720</a></span> | <span class="t">So, but wouldn't you have to, like, I was, this was another point that I was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1466" target="_blank">00:24:26.320</a></span> | <span class="t">trying to understand is I couldn't find any evidence that there's any place where people,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1470" target="_blank">00:24:30.560</a></span> | <span class="t">they were, like, putting the prefix in the NOMIC paper. Like, it seems like you would be able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1475" target="_blank">00:24:35.760</a></span> | <span class="t">improve a task-specific embedding by putting the prefix into the query, right? So, because it was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1482" target="_blank">00:24:42.880</a></span> | <span class="t">trained on that prefix. So, presumably, it would be better if you also used it in the query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1490" target="_blank">00:24:50.400</a></span> | <span class="t">Yeah, yeah, no comment on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1493" target="_blank">00:24:53.040</a></span> | <span class="t">Excuse me. I have a question about figure one. Does that, it shows two in the JINYA,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1501" target="_blank">00:25:01.840</a></span> | <span class="t">in the JINYA paper. Does that mean that the input's both the query and the supplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1507" target="_blank">00:25:07.920</a></span> | <span class="t">classification? Or is there a classification model that is a part of this embeddings model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1515" target="_blank">00:25:15.760</a></span> | <span class="t">that auto classifies? No, it's just loading in the classification, Laura.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1521" target="_blank">00:25:21.840</a></span> | <span class="t">Exactly. There's no separate classification model. It's really just embedding the text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1527" target="_blank">00:25:27.520</a></span> | <span class="t">embedding the, yeah, like, right now, and then if there's a classification label,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1532" target="_blank">00:25:32.240</a></span> | <span class="t">embedding a classification label, and just doing the classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1534" target="_blank">00:25:34.400</a></span> | <span class="t">So, that means that the program or user supplies the class, the adapter task?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1545" target="_blank">00:25:45.600</a></span> | <span class="t">Yes, if you look at the API for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1549" target="_blank">00:25:49.040</a></span> | <span class="t">So, that's the program that has to match with one of the five that they give you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1558" target="_blank">00:25:58.960</a></span> | <span class="t">Do they, does JINYA supply a classification model, or is it up to the?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1564" target="_blank">00:26:04.560</a></span> | <span class="t">No, you can only use what they give you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1568" target="_blank">00:26:08.000</a></span> | <span class="t">They have a classification, Laura, but you have to provide your own labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1573" target="_blank">00:26:13.040</a></span> | <span class="t">Okay, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1574" target="_blank">00:26:14.240</a></span> | <span class="t">One thing, for those who are more familiar with LORAS, isn't this wrong?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1580" target="_blank">00:26:20.400</a></span> | <span class="t">Why is it side by side? Shouldn't the LORAS be the last layer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1584" target="_blank">00:26:24.560</a></span> | <span class="t">The LORAS are usually on the MLP layers. So, it's on, like, every MLP label or every,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1595" target="_blank">00:26:35.760</a></span> | <span class="t">like, query key attention value layer. So, at least how I use it is I apply LORAS on all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1602" target="_blank">00:26:42.800</a></span> | <span class="t">MLP query key value layers. So, it's not just the last layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1606" target="_blank">00:26:46.480</a></span> | <span class="t">I think maybe what you're thinking of is maybe fine-tuning by adding a special last layer to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1611" target="_blank">00:26:51.760</a></span> | <span class="t">fine-tune that you freeze all the weights and fine-tune that special last layer for the specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1616" target="_blank">00:26:56.320</a></span> | <span class="t">task. That's what I'm familiar with for LORAS, but I guess I might be very focused on, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1621" target="_blank">00:27:01.760</a></span> | <span class="t">diffusion LORAS. Yeah, oh, I think that could be it. Yeah, that could be it. I think in LORAS,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1628" target="_blank">00:27:08.240</a></span> | <span class="t">it's mostly all the weights except for the embedding weights. Okay, got it. That's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1634" target="_blank">00:27:14.960</a></span> | <span class="t">very low-rank to me, but okay. Well, it's low-rank in the sense that the LORA dimension is very small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1641" target="_blank">00:27:21.760</a></span> | <span class="t">in the sense you can compress it. Yeah, yeah. All right, I'll throw an honorable mention to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1647" target="_blank">00:27:27.200</a></span> | <span class="t">Clip even though I didn't mention this just because this was also part of the reason why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1651" target="_blank">00:27:31.920</a></span> | <span class="t">I chose this topic for this week because there's a whole bunch of embedding shit that just came</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1655" target="_blank">00:27:35.840</a></span> | <span class="t">out in the last two weeks. So, I just wanted to, like, here's the state-of-the-art, here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1659" target="_blank">00:27:39.840</a></span> | <span class="t">everything I know, and then just kind of discuss it. So, they took Embeddings v3 and then jammed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1664" target="_blank">00:27:44.960</a></span> | <span class="t">it into Clip. So, here's the same ways where Embeddings v3 froze it, and then they have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1670" target="_blank">00:27:50.480</a></span> | <span class="t">this other vision, sorry, this other vision adapter here, and that's it. Text embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1679" target="_blank">00:27:59.760</a></span> | <span class="t">vision embeddings, you get a Clip model. We've covered Clip in the past, but basically for,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1685" target="_blank">00:28:05.600</a></span> | <span class="t">you know, for a refresher of those people who don't remember, where is the goddamn Clip paper?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1690" target="_blank">00:28:10.880</a></span> | <span class="t">I hate it when they don't show everything that's important. Okay, I have to go back to my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1698" target="_blank">00:28:18.080</a></span> | <span class="t">summarization again. Oh, okay, it was a different paper, unfortunately. Okay, but this is the Apple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1708" target="_blank">00:28:28.400</a></span> | <span class="t">one, but, like, I just really love this example. Every example, every paper should have qualitative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1713" target="_blank">00:28:33.600</a></span> | <span class="t">example of the output compared to competitors, right? Because then you understand fundamentally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1719" target="_blank">00:28:39.680</a></span> | <span class="t">what they're going for, because they are showing you how they want it to be used. And so, for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1723" target="_blank">00:28:43.760</a></span> | <span class="t">example, visual QA, stuff like this, is really cool, because you can definitely see yourself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1729" target="_blank">00:28:49.920</a></span> | <span class="t">having an image like this, where, you know, there's a number on the screen, and you say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1734" target="_blank">00:28:54.640</a></span> | <span class="t">what is the weight of the luggage? OpenAI Clip gets it wrong, Siglip gets it wrong, and, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1739" target="_blank">00:28:59.840</a></span> | <span class="t">your model gets it correct, right? Obviously, these are all going to be cherry-picked, but at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1743" target="_blank">00:29:03.200</a></span> | <span class="t">least it gives you an idea of what's in the damn data set that I find it hard to get. So Gina,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1751" target="_blank">00:29:11.920</a></span> | <span class="t">unfortunately, did not do this, at least that I can tell, but at least, you know, they publish a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1757" target="_blank">00:29:17.440</a></span> | <span class="t">lot of really sort of technical quantitative specs, and it's based on embeddings v3. So this is how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1762" target="_blank">00:29:22.960</a></span> | <span class="t">foundational embedding models are. Okay, I want to move on to the last one, unless people have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1768" target="_blank">00:29:28.240</a></span> | <span class="t">questions. I haven't been looking at the comments here. Oh, anyone have interesting comments?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1776" target="_blank">00:29:36.240</a></span> | <span class="t">Yes, yes, you have to click on my screen. Zoom has made it easy to miss the screen. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1788" target="_blank">00:29:48.640</a></span> | <span class="t">Oh, quick. So is Gina, like, a research lab, or...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1793" target="_blank">00:29:53.920</a></span> | <span class="t">It's a startup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1795" target="_blank">00:29:55.680</a></span> | <span class="t">Company? Okay, startup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1797" target="_blank">00:29:57.600</a></span> | <span class="t">It's a Chinese founder, lives in Germany. I met him in Vienna. Very nice guy, a big fan</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1803" target="_blank">00:30:03.840</a></span> | <span class="t">of latent space. We'll have him on at some point. For me, there's like 10 of these, you know, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1812" target="_blank">00:30:12.080</a></span> | <span class="t">it's hard for me to, like, figure out who to talk to, but Gina seems to do solid work, and they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1816" target="_blank">00:30:16.320</a></span> | <span class="t">very, very serious, and, I mean, look at the quality of their stuff. Like, it's obvious that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1819" target="_blank">00:30:19.520</a></span> | <span class="t">they're serious about it. So yeah, they're a startup trying to make it. Has anyone experimented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1829" target="_blank">00:30:29.440</a></span> | <span class="t">with medical embedding models? Okay, I'm going to go ahead and guess no, but can you show up,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1834" target="_blank">00:30:34.560</a></span> | <span class="t">can you tell us your interest?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1838" target="_blank">00:30:38.800</a></span> | <span class="t">Amy?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1839" target="_blank">00:30:39.840</a></span> | <span class="t">Yeah, hey, good to hear you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1841" target="_blank">00:30:41.360</a></span> | <span class="t">Hi. Yeah, so I'm currently working on, like, with the QN multimodal model. I'm working on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1854" target="_blank">00:30:54.080</a></span> | <span class="t">a retrieval system for medical papers, and I'm currently trying different models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1863" target="_blank">00:31:03.360</a></span> | <span class="t">and there's this BioBird. That's a QN from Alibaba. This new QN, yeah. And BioBird was one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1874" target="_blank">00:31:14.800</a></span> | <span class="t">but I'm not 100%. Yeah, it's not really good for my use case, so, and there is not a ton. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1888" target="_blank">00:31:28.000</a></span> | <span class="t">there's Jon Snow Labs. They are quite active in this area. So, Jon Snow, like, from Game of Thrones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1893" target="_blank">00:31:33.520</a></span> | <span class="t">Yeah, but I was wondering if any of you have experience with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1900" target="_blank">00:31:40.000</a></span> | <span class="t">models that were trained on, like, biomedical data and have high embedding quality? Also,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1907" target="_blank">00:31:47.200</a></span> | <span class="t">for, like, the, especially, like, chemical or biochemical, like, protein pathways, like this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1916" target="_blank">00:31:56.880</a></span> | <span class="t">So, I don't think anyone here does medical stuff, but Tanishq in our Discord does. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1921" target="_blank">00:32:01.600</a></span> | <span class="t">Tanishq, iScience lover, I think. Do you have any recs on biomedical embedding?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1932" target="_blank">00:32:12.720</a></span> | <span class="t">And he got you. If he, if it doesn't exist, he'll train it for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1937" target="_blank">00:32:17.680</a></span> | <span class="t">Yeah, it was also maybe starting on the weekend, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1943" target="_blank">00:32:23.520</a></span> | <span class="t">my own embedding model, just with a budget of a few. Let's see what comes out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1950" target="_blank">00:32:30.480</a></span> | <span class="t">I'd be very interested to see if the NOMIC code works for you, because this is supposed to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1959" target="_blank">00:32:39.840</a></span> | <span class="t">the, like, you just swap all the data set and you, you know, just run the same code again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1963" target="_blank">00:32:43.760</a></span> | <span class="t">Yeah, but that's, that's a theory in practice. You know, there's, it will not work in the first go,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1972" target="_blank">00:32:52.480</a></span> | <span class="t">but, yeah. Someone also, Nav also says you can just fine-tune a generic one. I definitely agree</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1978" target="_blank">00:32:58.320</a></span> | <span class="t">with that. Yeah, anyone from the, sort of, fast AI community will be horrified that you should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1985" target="_blank">00:33:05.120</a></span> | <span class="t">start from random weights. You should just start from something that's a decent weight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1988" target="_blank">00:33:08.400</a></span> | <span class="t">Okay. Oh, Khaled says MedGem and AI Healthcare. What is that? Is that, is that, is that a thing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=1999" target="_blank">00:33:19.920</a></span> | <span class="t">Oh, okay. Oh, yeah, you know, Google keeps doing this stuff and then I just ignore it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2005" target="_blank">00:33:25.280</a></span> | <span class="t">because I don't do any medical stuff, but, yeah, this sounds awesome. Oh, Sam, Sam,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2010" target="_blank">00:33:30.800</a></span> | <span class="t">Sam has a med model. I forgot. Yeah, but Sam is for segmentation. It's not a foundation model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2021" target="_blank">00:33:41.760</a></span> | <span class="t">They have a med Sam, which is good for segmentation. Oh, no, no, no, no. When I say Sam,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2031" target="_blank">00:33:51.680</a></span> | <span class="t">I mean Sam Julian, who's in the, in the chat. Oh, okay. I'm sorry. Not, not segment anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2036" target="_blank">00:33:56.560</a></span> | <span class="t">There is a Sam that is a segment anything model from MedGem. Yeah, and we've, we've interviewed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2046" target="_blank">00:34:06.080</a></span> | <span class="t">them twice, actually. So we have, I think it's, I think it's Sam too. Yeah, these guys. Nicky is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2053" target="_blank">00:34:13.760</a></span> | <span class="t">friend, and Roboflow is a very different friend of ours. But the, the good, the good thing is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2060" target="_blank">00:34:20.240</a></span> | <span class="t">they also have a specific model for medical imaging, which is what I work on. But since</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2068" target="_blank">00:34:28.640</a></span> | <span class="t">it's a coincidence that you mentioned Sam, as we were talking about MedGem and AI and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2074" target="_blank">00:34:34.880</a></span> | <span class="t">the... Yes, people have used it for medical applications. I believe Joseph in this podcast</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2083" target="_blank">00:34:43.040</a></span> | <span class="t">actually mentions it. But it's, I don't have the domain expertise to go beyond that. But yes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2088" target="_blank">00:34:48.880</a></span> | <span class="t">people have fine-tuned Sam to do medical segmentation. No, you can just write MedSam,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2095" target="_blank">00:34:55.120</a></span> | <span class="t">and you will get the GitHub. Okay, yeah, yeah, sorry. All right. Cool. All right. Let me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2101" target="_blank">00:35:01.360</a></span> | <span class="t">let me round out the other stuff, and then, and then we can sort of jump to Q&A, because I'm also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2105" target="_blank">00:35:05.360</a></span> | <span class="t">keen to hear Eugene's take on, on embeddings. So the last thing I'll highlight for you guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2112" target="_blank">00:35:12.320</a></span> | <span class="t">is contextual embeddings. So I'm basically trying to organizing it in terms of progression of what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2117" target="_blank">00:35:17.200</a></span> | <span class="t">I've seen in embeddings this year. So there was Nomic, which is start of the year. Gina, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2123" target="_blank">00:35:23.600</a></span> | <span class="t">introduced TaskLawrence. And contextual embeddings now introduced this idea of a two-stage adaptation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2130" target="_blank">00:35:30.560</a></span> | <span class="t">where they specifically help you, they specifically train the model to be conditioned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2137" target="_blank">00:35:37.120</a></span> | <span class="t">on the corpus first, to then be used in an embedding context. Which is, which is a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2143" target="_blank">00:35:43.360</a></span> | <span class="t">bit weird. But it also helps them be very, very OP in one specific aspect, which is efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2148" target="_blank">00:35:48.800</a></span> | <span class="t">So they are, I think if we go to, they're still up here somewhere. It's very hard to like keep up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2156" target="_blank">00:35:56.320</a></span> | <span class="t">So they are 143 million per amp model, competing with 7 billion per amp models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2162" target="_blank">00:36:02.560</a></span> | <span class="t">because of this adaptation. So it's a little bit cheating to put them on the apples to apples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2169" target="_blank">00:36:09.920</a></span> | <span class="t">comparison with these guys, because their deployment model is a little bit different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2173" target="_blank">00:36:13.680</a></span> | <span class="t">What they're doing is basically, first you consume a context. Let me see if I can show the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2180" target="_blank">00:36:20.560</a></span> | <span class="t">Where is it? I don't know if I saw it here inside the, I think there might be the GitHub.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2193" target="_blank">00:36:33.200</a></span> | <span class="t">Where's the GitHub? GitHub, GitHub, GitHub.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2199" target="_blank">00:36:39.760</a></span> | <span class="t">Sorry, I don't, I don't think I put it in my notes here. Contextual embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2209" target="_blank">00:36:49.920</a></span> | <span class="t">There we go. Okay. Yeah, here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2214" target="_blank">00:36:54.000</a></span> | <span class="t">Okay, so yeah, this is what I wanted to show you. So instead of just like a single shot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2224" target="_blank">00:37:04.400</a></span> | <span class="t">here's a bunch of text, embed this please. That's basically what all the other models did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2229" target="_blank">00:37:09.360</a></span> | <span class="t">In the JINA model, you maybe specify like a task, right? So to load the LoRa. Here you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2236" target="_blank">00:37:16.160</a></span> | <span class="t">kind of construct the LoRa as you go, right? So you feed in the corpus first, feed all of it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2242" target="_blank">00:37:22.240</a></span> | <span class="t">and then you get dataset embeddings for the first stage on the whole thing. Then the second stage,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2247" target="_blank">00:37:27.040</a></span> | <span class="t">you use it to actually do your prompts query, which is kind of slow for loading.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2255" target="_blank">00:37:35.200</a></span> | <span class="t">But then you can understand why this domain adapted so much better than basically every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2261" target="_blank">00:37:41.360</a></span> | <span class="t">other method out there. And it's such a simple idea that you just train your model in a sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2266" target="_blank">00:37:46.720</a></span> | <span class="t">of two-stage process. So these guys worked it out. And the technique is, you know, above my pay grade,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2273" target="_blank">00:37:53.360</a></span> | <span class="t">but it's a whole bunch of math, whatever. But like that conditional aspect, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2279" target="_blank">00:37:59.120</a></span> | <span class="t">makes a ton of sense to me. And this, in my mind, like, if this method proves popular enough,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2284" target="_blank">00:38:04.960</a></span> | <span class="t">basically everyone is going to do it, because it's such a cheap win, especially for the efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2291" target="_blank">00:38:11.040</a></span> | <span class="t">So I'll pause there. Is that the contextual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2294" target="_blank">00:38:14.880</a></span> | <span class="t">embedding paper that you mentioned? Yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2296" target="_blank">00:38:16.960</a></span> | <span class="t">I think most, I think even Gina and Nomik, they actually adopt that methodology. I think there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2305" target="_blank">00:38:25.120</a></span> | <span class="t">two things. One is updates to the architecture. Another one is updates to the training methodology.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2309" target="_blank">00:38:29.600</a></span> | <span class="t">Essentially, they say that they do some clustering, and then they feed in the batches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2312" target="_blank">00:38:32.720</a></span> | <span class="t">from the same cluster. I think Gina and Nomik also do that, where they say that they feed in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2318" target="_blank">00:38:38.480</a></span> | <span class="t">the batch to make sure that the data comes from the same data set. They don't actually mix data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2324" target="_blank">00:38:44.000</a></span> | <span class="t">sources across different data sets. But I think what's unique...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2327" target="_blank">00:38:47.440</a></span> | <span class="t">The inference is only one run. Like, you know what I mean? Like, they don't let you domain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2333" target="_blank">00:38:53.840</a></span> | <span class="t">adapt this thing. Yeah, that's true. That's true. Exactly. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2336" target="_blank">00:38:56.640</a></span> | <span class="t">what's unique is their architecture, whereby the inference, they actually allow you to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2340" target="_blank">00:39:00.720</a></span> | <span class="t">provide some priors on your existing domain. That's quite interesting to me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2346" target="_blank">00:39:06.000</a></span> | <span class="t">and that was new to me as well. Yeah. So I think it's a very good idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2351" target="_blank">00:39:11.360</a></span> | <span class="t">I would love for other people to adopt it. This might be one of those things where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2355" target="_blank">00:39:15.200</a></span> | <span class="t">it just takes one of the big labs to read the paper and figure out that this makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2362" target="_blank">00:39:22.000</a></span> | <span class="t">I think the other deployment issue is that it's basically a stateful API. So you cannot... Like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2369" target="_blank">00:39:29.360</a></span> | <span class="t">all these are stateless, which is great. Sorry, this is stateless. So you just call an endpoint,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2374" target="_blank">00:39:34.240</a></span> | <span class="t">right? All the model labs love this kind of model. But here you're going to have to, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2378" target="_blank">00:39:38.080</a></span> | <span class="t">call an endpoint to embed this model first, and then return a new endpoint that you can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2386" target="_blank">00:39:46.080</a></span> | <span class="t">do the embeddings on. So it might be a little bit annoying for these guys to figure out, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2391" target="_blank">00:39:51.040</a></span> | <span class="t">if it's a big enough deal, they'll figure it out. But the lifts are very, very great. If you look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2400" target="_blank">00:40:00.480</a></span> | <span class="t">some of the data that they have... Like, yeah. Just across all these models, keep in mind that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2411" target="_blank">00:40:11.760</a></span> | <span class="t">they're at least an order of magnitude smaller than all these guys. They actually perform better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2416" target="_blank">00:40:16.080</a></span> | <span class="t">on basically every task. It's pretty crazy. So it would be interesting... There's no reason for it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2426" target="_blank">00:40:26.400</a></span> | <span class="t">to be small if you can just make it big, but you just keep the technique the same. This was trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2431" target="_blank">00:40:31.440</a></span> | <span class="t">on a grad student budget. If you just scale this up, I think it would work. I think people would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2437" target="_blank">00:40:37.760</a></span> | <span class="t">use it. It basically is a more generalized version of this task adapter API, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2444" target="_blank">00:40:44.960</a></span> | <span class="t">So instead of having only five task adapters, what if you could just come up with your own task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2453" target="_blank">00:40:53.040</a></span> | <span class="t">adapters just arbitrarily by feeding in the corpus that you're trying to embed?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2456" target="_blank">00:40:56.880</a></span> | <span class="t">To me, that's a big idea. Anyway, should I pause there? I don't know if there's any other questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2464" target="_blank">00:41:04.480</a></span> | <span class="t">You can see the first data. Isn't that just fine-tuning? No, because there's no gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2474" target="_blank">00:41:14.160</a></span> | <span class="t">updates here. It's more like context caching, maybe. I'll liken it to that, where the initial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2489" target="_blank">00:41:29.920</a></span> | <span class="t">context is pre-processed as a KB cache, and you just keep the KB cache around. That's effectively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2496" target="_blank">00:41:36.080</a></span> | <span class="t">what you do for context caching. I think we can pause the recording,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2500" target="_blank">00:41:40.160</a></span> | <span class="t">then let Eugene do his hot takes. Eugene, hot takes, let's go!</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo&t=2504" target="_blank">00:41:44.720</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>