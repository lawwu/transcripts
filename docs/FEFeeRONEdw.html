<html><head><title>Stanford XCS224U: NLU I Contextual Word Representations, Part 1: Guiding Ideas I Spring 2023</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U: NLU I Contextual Word Representations, Part 1: Guiding Ideas I Spring 2023</h2><a href="https://www.youtube.com/watch?v=FEFeeRONEdw"><img src="https://i.ytimg.com/vi/FEFeeRONEdw/sddefault.jpg?sqp=-oaymwEmCIAFEOAD8quKqQMa8AEB-AHUBoAC4AOKAgwIABABGGUgXihRMA8=&rs=AOn4CLDKVLpf8NV1Qn3h6BLKkQijgHGKUQ" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./FEFeeRONEdw.html">Whisper Transcript</a> | <a href="./transcript_FEFeeRONEdw.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello everyone, I'm Chris Potts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=6" target="_blank">00:00:06.280</a></span> | <span class="t">Welcome to our unit on contextual word representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=9" target="_blank">00:00:09.360</a></span> | <span class="t">I thought I'd kick this off with some high-level guiding ideas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=12" target="_blank">00:00:12.780</a></span> | <span class="t">The first thing I wanted to say is that in previous iterations of this course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=16" target="_blank">00:00:16.080</a></span> | <span class="t">we spent about two weeks focused on static vector representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=20" target="_blank">00:00:20.480</a></span> | <span class="t">of words and in some cases phrases and full sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=23" target="_blank">00:00:23.920</a></span> | <span class="t">For this iteration of the course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=25" target="_blank">00:00:25.460</a></span> | <span class="t">we're going to go directly to contextual word representations which have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=29" target="_blank">00:00:29.000</a></span> | <span class="t">proven so powerful for today's NLP research and technologies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=33" target="_blank">00:00:33.280</a></span> | <span class="t">But I did want to offer a brief overview of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=35" target="_blank">00:00:35.720</a></span> | <span class="t">the history that leads to contextual representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=38" target="_blank">00:00:38.920</a></span> | <span class="t">Let's rewind the clock back to what I've called feature-based,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=42" target="_blank">00:00:42.400</a></span> | <span class="t">classical lexical representations and the hallmark</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=45" target="_blank">00:00:45.520</a></span> | <span class="t">of these representations is that they are sparse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=48" target="_blank">00:00:48.400</a></span> | <span class="t">Typically, what we're thinking of is very long vectors where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=52" target="_blank">00:00:52.120</a></span> | <span class="t">the column dimensions represent the output of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=54" target="_blank">00:00:54.680</a></span> | <span class="t">usually hand-built feature functions that we've written that might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=57" target="_blank">00:00:57.520</a></span> | <span class="t">capture things like the binary sentiment classification of a word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=61" target="_blank">00:01:01.720</a></span> | <span class="t">or which part of speech it tends to have most dominantly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=65" target="_blank">00:01:05.600</a></span> | <span class="t">or whether or not it ends in a suffix like ing for English,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=69" target="_blank">00:01:09.600</a></span> | <span class="t">and so forth and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=71" target="_blank">00:01:11.000</a></span> | <span class="t">We write a lot of these feature functions and as a result,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=73" target="_blank">00:01:13.560</a></span> | <span class="t">we have vectors that are mostly zeros because most words don't have these properties,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=78" target="_blank">00:01:18.000</a></span> | <span class="t">but the few ones carry important information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=81" target="_blank">00:01:21.320</a></span> | <span class="t">In the next phase, we have count-based methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=84" target="_blank">00:01:24.400</a></span> | <span class="t">This is the introduction of the distributional hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=87" target="_blank">00:01:27.640</a></span> | <span class="t">I'm thinking of methods like point-wise mutual information and TF-IDF,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=91" target="_blank">00:01:31.840</a></span> | <span class="t">term frequency inverse document frequency,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=94" target="_blank">00:01:34.400</a></span> | <span class="t">a classic from information retrieval that we'll visit a bit later in the course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=99" target="_blank">00:01:39.400</a></span> | <span class="t">For these methods, we begin from some count matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=102" target="_blank">00:01:42.680</a></span> | <span class="t">Typically, for PMI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=104" target="_blank">00:01:44.040</a></span> | <span class="t">it would be a word-by-word matrix where the cells in that matrix capture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=107" target="_blank">00:01:47.800</a></span> | <span class="t">the number of time so that each word co-occurs with the other words in our corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=112" target="_blank">00:01:52.680</a></span> | <span class="t">For TF-IDF, it would probably be a word-by-document matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=116" target="_blank">00:01:56.200</a></span> | <span class="t">and now we're capturing which words appear and with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=119" target="_blank">00:01:59.240</a></span> | <span class="t">what frequency in all the documents in our corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=122" target="_blank">00:02:02.760</a></span> | <span class="t">The idea behind these methods is that we take PMI or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=126" target="_blank">00:02:06.360</a></span> | <span class="t">TF-IDF and we massage those counts in a way that leads to better representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=132" target="_blank">00:02:12.000</a></span> | <span class="t">That's coming purely from distributional information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=134" target="_blank">00:02:14.560</a></span> | <span class="t">We typically don't write hand-built feature functions in this mode,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=138" target="_blank">00:02:18.080</a></span> | <span class="t">but these vectors tend to be pretty sparse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=141" target="_blank">00:02:21.400</a></span> | <span class="t">The next phase is what I've called classical dimensionality reduction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=145" target="_blank">00:02:25.360</a></span> | <span class="t">This is the introduction of dense representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=147" target="_blank">00:02:27.920</a></span> | <span class="t">I have in mind methods like PCA,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=150" target="_blank">00:02:30.320</a></span> | <span class="t">principal components analysis, SVD,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=153" target="_blank">00:02:33.200</a></span> | <span class="t">that singular value decomposition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=155" target="_blank">00:02:35.040</a></span> | <span class="t">LDA is latent Dirichlet allocation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=157" target="_blank">00:02:37.400</a></span> | <span class="t">There's a whole family of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=159" target="_blank">00:02:39.160</a></span> | <span class="t">Typically, what we're doing at this phase is taking maybe the output of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=162" target="_blank">00:02:42.720</a></span> | <span class="t">representations built in the mode of step 2 there and compressing them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=167" target="_blank">00:02:47.760</a></span> | <span class="t">In compressing them, we typically get denser,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=171" target="_blank">00:02:51.000</a></span> | <span class="t">more informative representations that can also capture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=174" target="_blank">00:02:54.240</a></span> | <span class="t">higher-order notions of distributional similarity and co-occurrence and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=179" target="_blank">00:02:59.120</a></span> | <span class="t">That proved incredibly powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=181" target="_blank">00:03:01.960</a></span> | <span class="t">Then the fourth phase, which might be the final phase for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=185" target="_blank">00:03:05.000</a></span> | <span class="t">this static vector representation approach,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=187" target="_blank">00:03:07.440</a></span> | <span class="t">we have what I've called learned dimensionality reduction approaches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=190" target="_blank">00:03:10.800</a></span> | <span class="t">Here again, we have dense representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=193" target="_blank">00:03:13.300</a></span> | <span class="t">These could be the output of an autoencoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=195" target="_blank">00:03:15.880</a></span> | <span class="t">or a classical method like word2vec or GloVe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=199" target="_blank">00:03:19.240</a></span> | <span class="t">What we do at this phase is really essentially combine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=203" target="_blank">00:03:23.040</a></span> | <span class="t">the count-based methods from step 2 in this history with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=207" target="_blank">00:03:27.240</a></span> | <span class="t">the dimensionality reduction that we see from methods like SVD and PCA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=212" target="_blank">00:03:32.880</a></span> | <span class="t">That leads us to very powerful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=215" target="_blank">00:03:35.380</a></span> | <span class="t">typically learned representations that have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=217" target="_blank">00:03:37.920</a></span> | <span class="t">a tremendous capacity to capture higher-order notions of co-occurrence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=223" target="_blank">00:03:43.000</a></span> | <span class="t">That's a very fast overview.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=224" target="_blank">00:03:44.760</a></span> | <span class="t">If you would like to get hands-on with these methods and really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=228" target="_blank">00:03:48.680</a></span> | <span class="t">deeply understand them then check out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=231" target="_blank">00:03:51.360</a></span> | <span class="t">this page that I've linked to here from the course website.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=234" target="_blank">00:03:54.240</a></span> | <span class="t">It links to a lot of notebooks and some older videos that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=237" target="_blank">00:03:57.320</a></span> | <span class="t">could help you with a refresher on all these methods or to just get up to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=241" target="_blank">00:04:01.240</a></span> | <span class="t">speed where you're ready to think directly about contextual representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=246" target="_blank">00:04:06.760</a></span> | <span class="t">The representations that we get from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=249" target="_blank">00:04:09.540</a></span> | <span class="t">these contextual models that will be the focus for this unit and really for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=252" target="_blank">00:04:12.860</a></span> | <span class="t">the entire course really resonate with me as a linguist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=256" target="_blank">00:04:16.840</a></span> | <span class="t">I thought I would just pause here and run through some examples that I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=261" target="_blank">00:04:21.040</a></span> | <span class="t">lead from the linguistics angle to the conclusion that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=264" target="_blank">00:04:24.200</a></span> | <span class="t">word representations are highly sensitive to the context in which they're used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=269" target="_blank">00:04:29.040</a></span> | <span class="t">Let's start with a simple case like the vase broke and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=271" target="_blank">00:04:31.880</a></span> | <span class="t">our focus will be on that English verb break.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=275" target="_blank">00:04:35.000</a></span> | <span class="t">Here, the sense of break is something like shatter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=278" target="_blank">00:04:38.640</a></span> | <span class="t">For dawn broke though, we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=280" target="_blank">00:04:40.600</a></span> | <span class="t">a superficially similar looking sentence, subject verb.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=284" target="_blank">00:04:44.180</a></span> | <span class="t">But now the sense of break is more like begin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=287" target="_blank">00:04:47.960</a></span> | <span class="t">That's presumably conditioned by what we know about the subject and how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=292" target="_blank">00:04:52.440</a></span> | <span class="t">that operates on the sense of the verb break for a stereotypical reading.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=296" target="_blank">00:04:56.760</a></span> | <span class="t">The news broke again, superficially similar sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=300" target="_blank">00:05:00.040</a></span> | <span class="t">The news is the subject,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=301" target="_blank">00:05:01.420</a></span> | <span class="t">but now it seems to be conditioning a reading that is more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=304" target="_blank">00:05:04.220</a></span> | <span class="t">like was announced or appeared or was published.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=308" target="_blank">00:05:08.160</a></span> | <span class="t">Very different sense yet again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=310" target="_blank">00:05:10.040</a></span> | <span class="t">For Sandy broke the record,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=311" target="_blank">00:05:11.540</a></span> | <span class="t">we have our first transitive case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=313" target="_blank">00:05:13.640</a></span> | <span class="t">Now the sense of break is something like surpass the previous world record.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=318" target="_blank">00:05:18.320</a></span> | <span class="t">But for Sandy broke the law,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=320" target="_blank">00:05:20.320</a></span> | <span class="t">a different sense yet again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=321" target="_blank">00:05:21.680</a></span> | <span class="t">This is something like a transgression,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=323" target="_blank">00:05:23.540</a></span> | <span class="t">again conditioned in this case by the direct object.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=326" target="_blank">00:05:26.760</a></span> | <span class="t">The burglar broke into the house,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=328" target="_blank">00:05:28.800</a></span> | <span class="t">now we have break appearing with a particle into,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=331" target="_blank">00:05:31.640</a></span> | <span class="t">and in this case it means like enter forcibly without permission.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=335" target="_blank">00:05:35.560</a></span> | <span class="t">But the newscaster broke into the movie broadcast means something more like interrupt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=340" target="_blank">00:05:40.040</a></span> | <span class="t">a related but interestingly distinct meaning that is coming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=343" target="_blank">00:05:43.360</a></span> | <span class="t">from the same break plus particle construction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=347" target="_blank">00:05:47.240</a></span> | <span class="t">Then in we broke even,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=349" target="_blank">00:05:49.360</a></span> | <span class="t">just for another surprise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=350" target="_blank">00:05:50.960</a></span> | <span class="t">this is an entirely new sense break plus even means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=353" target="_blank">00:05:53.640</a></span> | <span class="t">something like we lost the same amount as we gained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=357" target="_blank">00:05:57.440</a></span> | <span class="t">This is just a sample of the many,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=360" target="_blank">00:06:00.560</a></span> | <span class="t">many senses that break can take on in English.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=363" target="_blank">00:06:03.640</a></span> | <span class="t">What we see in these patterns is that the sense it does have in context is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=368" target="_blank">00:06:08.280</a></span> | <span class="t">driven by the surrounding words transparently,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=371" target="_blank">00:06:11.200</a></span> | <span class="t">but maybe also by things that are happening in the context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=375" target="_blank">00:06:15.000</a></span> | <span class="t">Similar things arise for adjectives like flat,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=379" target="_blank">00:06:19.680</a></span> | <span class="t">as in flat beer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=380" target="_blank">00:06:20.840</a></span> | <span class="t">flat note, flat tire,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=382" target="_blank">00:06:22.760</a></span> | <span class="t">flat surface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=384" target="_blank">00:06:24.020</a></span> | <span class="t">That's the same adjective flat,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=386" target="_blank">00:06:26.000</a></span> | <span class="t">but depending on what noun it modifies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=388" target="_blank">00:06:28.080</a></span> | <span class="t">you get very different senses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=390" target="_blank">00:06:30.040</a></span> | <span class="t">It can happen with a verb to throw a party,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=392" target="_blank">00:06:32.360</a></span> | <span class="t">throw a fight, throw a ball,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=393" target="_blank">00:06:33.880</a></span> | <span class="t">throw a fit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=394" target="_blank">00:06:34.880</a></span> | <span class="t">There might be some metaphorical sense in which all of these senses are related,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=399" target="_blank">00:06:39.200</a></span> | <span class="t">but they are also clearly distinct and they're being driven</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=402" target="_blank">00:06:42.800</a></span> | <span class="t">by something about how this direct object interacts with the verb.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=407" target="_blank">00:06:47.400</a></span> | <span class="t">We should extend this beyond just the simple morphosyntactic context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=412" target="_blank">00:06:52.920</a></span> | <span class="t">Let's think about ambiguity resolution coming from the things we say to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=417" target="_blank">00:06:57.840</a></span> | <span class="t">For the sentence, a crane caught a fish,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=420" target="_blank">00:07:00.120</a></span> | <span class="t">you probably infer that there the sense of crane is for a bird.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=424" target="_blank">00:07:04.200</a></span> | <span class="t">It's not determined by that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=426" target="_blank">00:07:06.000</a></span> | <span class="t">but that's the most likely inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=427" target="_blank">00:07:07.900</a></span> | <span class="t">Whereas a crane picked up the steel beam,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=431" target="_blank">00:07:11.160</a></span> | <span class="t">you probably infer in that case that a crane is a machine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=435" target="_blank">00:07:15.280</a></span> | <span class="t">Then correspondingly for I saw a crane,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=438" target="_blank">00:07:18.080</a></span> | <span class="t">you might not absent more context know whether I mean a bird or a machine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=443" target="_blank">00:07:23.120</a></span> | <span class="t">But if we do embed this sentence in larger context,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=446" target="_blank">00:07:26.120</a></span> | <span class="t">you'll begin to make inferences about whether it was a bird or a machine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=450" target="_blank">00:07:30.680</a></span> | <span class="t">That shows you that even things that are happening ambiently in the context of utterance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=455" target="_blank">00:07:35.120</a></span> | <span class="t">can impact what sense crane has in these sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=460" target="_blank">00:07:40.660</a></span> | <span class="t">Are there any typos? I didn't see any.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=463" target="_blank">00:07:43.820</a></span> | <span class="t">The second sentence is clearly elliptical,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=465" target="_blank">00:07:45.840</a></span> | <span class="t">and in this case, we probably infer that I didn't see any means I didn't see any typos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=470" target="_blank">00:07:50.760</a></span> | <span class="t">Contrast that with are there any bookstores downtown?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=473" target="_blank">00:07:53.660</a></span> | <span class="t">I didn't see any.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=474" target="_blank">00:07:54.760</a></span> | <span class="t">It's an identical second sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=477" target="_blank">00:07:57.240</a></span> | <span class="t">but now the inference is that any means something like any bookstores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=481" target="_blank">00:08:01.680</a></span> | <span class="t">That is again showing that the senses that these words take on in context could be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=486" target="_blank">00:08:06.520</a></span> | <span class="t">driven by everything that is happening in the surrounding sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=490" target="_blank">00:08:10.360</a></span> | <span class="t">and also in the surrounding discourse on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=492" target="_blank">00:08:12.900</a></span> | <span class="t">out extending into things about world knowledge and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=497" target="_blank">00:08:17.200</a></span> | <span class="t">That for me really shows that the static word vector representation approach was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=502" target="_blank">00:08:22.200</a></span> | <span class="t">never really going to work out because it insists that broke in all those examples in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=508" target="_blank">00:08:28.520</a></span> | <span class="t">one correspond to a single vector flat into a has to be a single vector and so forth and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=516" target="_blank">00:08:36.400</a></span> | <span class="t">What we actually see in how language works is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=519" target="_blank">00:08:39.400</a></span> | <span class="t">much more malleability for individual word meanings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=522" target="_blank">00:08:42.920</a></span> | <span class="t">and that's precisely what contextual word representations allow us to capture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=529" target="_blank">00:08:49.120</a></span> | <span class="t">I thought I would pause here just to offer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=532" target="_blank">00:08:52.480</a></span> | <span class="t">a brief history of where these ideas come from because it's a very recent history.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=536" target="_blank">00:08:56.760</a></span> | <span class="t">Things are moving fast, but it's also interesting to track.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=539" target="_blank">00:08:59.680</a></span> | <span class="t">I think a classic paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=541" target="_blank">00:09:01.200</a></span> | <span class="t">maybe the starting point for this is Dai and Lei, 2015.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=545" target="_blank">00:09:05.040</a></span> | <span class="t">They really showed the value of language model style pre-training for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=548" target="_blank">00:09:08.920</a></span> | <span class="t">downstream tasks and the paper is fascinating to look at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=552" target="_blank">00:09:12.040</a></span> | <span class="t">It's a pre-transformer paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=554" target="_blank">00:09:14.160</a></span> | <span class="t">A lot of the things that they do look complicated in retrospect,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=557" target="_blank">00:09:17.540</a></span> | <span class="t">and some of the simpler ideas that they offer we can now see in retrospect are incredibly powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=563" target="_blank">00:09:23.560</a></span> | <span class="t">We fast forward a little bit to August 2017, McCann et al.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=567" target="_blank">00:09:27.640</a></span> | <span class="t">This is the Cove paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=569" target="_blank">00:09:29.240</a></span> | <span class="t">and what they showed is that pre-trained bidirectional LSTMs for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=573" target="_blank">00:09:33.640</a></span> | <span class="t">machine translation could offer us sequence representations that were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=578" target="_blank">00:09:38.480</a></span> | <span class="t">a useful starting point for many other downstream tasks outside of MT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=584" target="_blank">00:09:44.360</a></span> | <span class="t">That begins this move toward pre-training for contextual representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=589" target="_blank">00:09:49.440</a></span> | <span class="t">It really takes off with Elmo in February 2018.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=593" target="_blank">00:09:53.720</a></span> | <span class="t">That team was really the first to show that very large-scale pre-training of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=598" target="_blank">00:09:58.400</a></span> | <span class="t">bidirectional LSTMs could lead to rich multipurpose representations that were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=603" target="_blank">00:10:03.320</a></span> | <span class="t">easily adapted to lots of downstream tasks via fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=608" target="_blank">00:10:08.320</a></span> | <span class="t">In June 2018, we get GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=611" target="_blank">00:10:11.800</a></span> | <span class="t">Then in October 2018,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=614" target="_blank">00:10:14.120</a></span> | <span class="t">the BERT era truly begins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=616" target="_blank">00:10:16.240</a></span> | <span class="t">Devlin et al. 2019 is when the paper was published,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=619" target="_blank">00:10:19.800</a></span> | <span class="t">but the work appeared before that and had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=621" target="_blank">00:10:21.960</a></span> | <span class="t">already had tremendous influence by the time it was actually officially published.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=626" target="_blank">00:10:26.200</a></span> | <span class="t">That BERT model is really the cornerstone of so much that we'll discuss in this unit and beyond.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=633" target="_blank">00:10:33.240</a></span> | <span class="t">There's a parallel dimension to all this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=636" target="_blank">00:10:36.640</a></span> | <span class="t">a parallel journey that I thought I would talk a little bit about as another guiding idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=641" target="_blank">00:10:41.520</a></span> | <span class="t">I've put this under the heading of model structure and linguistic structure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=645" target="_blank">00:10:45.600</a></span> | <span class="t">and this is related also to this idea of what kinds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=649" target="_blank">00:10:49.600</a></span> | <span class="t">of structural biases we build into our model architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=654" target="_blank">00:10:54.200</a></span> | <span class="t">In the upper left, I have a simple model that's reviewed in those background materials,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=658" target="_blank">00:10:58.800</a></span> | <span class="t">where you can imagine that each word in the sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=661" target="_blank">00:11:01.880</a></span> | <span class="t">the rock rules is looked up in like a glove space or a word to vex space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=667" target="_blank">00:11:07.240</a></span> | <span class="t">Then what this model does is simply add those representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=670" target="_blank">00:11:10.400</a></span> | <span class="t">together to get a representation for the entire sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=674" target="_blank">00:11:14.400</a></span> | <span class="t">This is a very high bias model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=677" target="_blank">00:11:17.200</a></span> | <span class="t">in particular because we have to decide ahead of time that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=680" target="_blank">00:11:20.920</a></span> | <span class="t">the way those representations will combine will be via addition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=685" target="_blank">00:11:25.520</a></span> | <span class="t">You have to think, even if that's approximately correct,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=688" target="_blank">00:11:28.800</a></span> | <span class="t">it's only going to be approximate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=690" target="_blank">00:11:30.360</a></span> | <span class="t">it would be a minor miracle if addition turned out to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=693" target="_blank">00:11:33.760</a></span> | <span class="t">be actually the optimal way to combine those word meanings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=698" target="_blank">00:11:38.040</a></span> | <span class="t">You could see that as giving rise to the models that are on the right here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=701" target="_blank">00:11:41.520</a></span> | <span class="t">this is a recurrent neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=704" target="_blank">00:11:44.080</a></span> | <span class="t">Again, you could imagine that we look up each one of those words in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=707" target="_blank">00:11:47.600</a></span> | <span class="t">a static vector representation space like glove.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=711" target="_blank">00:11:51.400</a></span> | <span class="t">But now we feed them into this RNN process that actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=715" target="_blank">00:11:55.240</a></span> | <span class="t">processes them with a bunch of new neural network parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=720" target="_blank">00:12:00.400</a></span> | <span class="t">In that way, it could be trained,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=723" target="_blank">00:12:03.240</a></span> | <span class="t">it could be taught to combine those word meanings in an optimal way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=727" target="_blank">00:12:07.560</a></span> | <span class="t">It could be the case that the optimal way to combine word meanings is with addition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=732" target="_blank">00:12:12.880</a></span> | <span class="t">and the models that we have over here on the right are certainly powerful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=736" target="_blank">00:12:16.520</a></span> | <span class="t">enough to learn addition of those vectors if it's correct,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=740" target="_blank">00:12:20.040</a></span> | <span class="t">but that is very unlikely to happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=742" target="_blank">00:12:22.080</a></span> | <span class="t">Probably, the model will learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=744" target="_blank">00:12:24.080</a></span> | <span class="t">a much more complicated function that might be much more nuanced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=747" target="_blank">00:12:27.800</a></span> | <span class="t">In that way, we've released some of the biases that we introduced over here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=753" target="_blank">00:12:33.080</a></span> | <span class="t">and we're going to allow ourselves to learn in a more free-form way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=756" target="_blank">00:12:36.320</a></span> | <span class="t">from data about how to optimally combine words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=759" target="_blank">00:12:39.960</a></span> | <span class="t">There's another dimension to this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=761" target="_blank">00:12:41.560</a></span> | <span class="t">If you go down on the left here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=762" target="_blank">00:12:42.800</a></span> | <span class="t">this is a tree-structured neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=765" target="_blank">00:12:45.040</a></span> | <span class="t">In this case, we decide ahead of time that we know the constituent structure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=769" target="_blank">00:12:49.120</a></span> | <span class="t">and then we might have a bunch of neural network parameters that combine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=772" target="_blank">00:12:52.840</a></span> | <span class="t">the child nodes to create representations for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=775" target="_blank">00:12:55.440</a></span> | <span class="t">the parents in a recursive fashion as we move up the tree.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=779" target="_blank">00:12:59.200</a></span> | <span class="t">That is very powerful in the sense that we could have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=782" target="_blank">00:13:02.720</a></span> | <span class="t">lots of different functions that get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=784" target="_blank">00:13:04.880</a></span> | <span class="t">learned for how to combine child nodes into their parents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=788" target="_blank">00:13:08.240</a></span> | <span class="t">But this model is high bias because it decides ahead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=791" target="_blank">00:13:11.680</a></span> | <span class="t">time about how the constituent structure should look.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=794" target="_blank">00:13:14.600</a></span> | <span class="t">You have to believe or know a priori that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=798" target="_blank">00:13:18.120</a></span> | <span class="t">the rock forms a constituent in a way that rock rules simply does not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=802" target="_blank">00:13:22.600</a></span> | <span class="t">Whereas the models up here take a more free-form approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=806" target="_blank">00:13:26.960</a></span> | <span class="t">Down in the right-hand corner,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=809" target="_blank">00:13:29.120</a></span> | <span class="t">I have the models that we saw in the lead up to the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=813" target="_blank">00:13:33.640</a></span> | <span class="t">This could be like a bidirectional RNN,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=816" target="_blank">00:13:36.760</a></span> | <span class="t">where again, we could look up the words in a static vector representation space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=820" target="_blank">00:13:40.840</a></span> | <span class="t">but now we have information flowing left to right in these hidden representations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=825" target="_blank">00:13:45.200</a></span> | <span class="t">and we might have added a bunch of attention mechanisms on top that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=829" target="_blank">00:13:49.680</a></span> | <span class="t">essentially connect every other hidden unit to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=832" target="_blank">00:13:52.080</a></span> | <span class="t">every other hidden unit in this representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=835" target="_blank">00:13:55.240</a></span> | <span class="t">Whereas this had a presumption that we would process left to right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=838" target="_blank">00:13:58.520</a></span> | <span class="t">and this one had an assumption that we would process by constituent structure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=842" target="_blank">00:14:02.640</a></span> | <span class="t">this model down here says essentially anything goes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=846" target="_blank">00:14:06.160</a></span> | <span class="t">I think it's fair to say that a lesson of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=848" target="_blank">00:14:08.480</a></span> | <span class="t">the transformer era is that anything goes given sufficient data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=852" target="_blank">00:14:12.880</a></span> | <span class="t">is the most powerful mode to be in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=855" target="_blank">00:14:15.480</a></span> | <span class="t">That really is a kind of insight behind the transformer architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=861" target="_blank">00:14:21.040</a></span> | <span class="t">The attention mechanisms that I mentioned there are really important,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=865" target="_blank">00:14:25.320</a></span> | <span class="t">and this is also part of the journey that leads us to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=867" target="_blank">00:14:27.640</a></span> | <span class="t">the transformer and might harbor most of its power.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=871" target="_blank">00:14:31.240</a></span> | <span class="t">Let me give you a simple example of how attention worked,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=875" target="_blank">00:14:35.160</a></span> | <span class="t">especially in the lead up to transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=877" target="_blank">00:14:37.880</a></span> | <span class="t">Here I have a model that you might think of as an RNN,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=880" target="_blank">00:14:40.320</a></span> | <span class="t">maybe we're processing left to right for really not so good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=884" target="_blank">00:14:44.080</a></span> | <span class="t">We look up those words in some static vector representation space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=888" target="_blank">00:14:48.860</a></span> | <span class="t">and then we have our left to right process that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=890" target="_blank">00:14:50.960</a></span> | <span class="t">leads to these hidden representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=893" target="_blank">00:14:53.280</a></span> | <span class="t">Suppose now that I want to train a classifier on top of this final output state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=899" target="_blank">00:14:59.360</a></span> | <span class="t">Well, I might worry in doing that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=902" target="_blank">00:15:02.040</a></span> | <span class="t">that there'll be a lot of information about the words that are laid in the sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=906" target="_blank">00:15:06.280</a></span> | <span class="t">but not enough information in this representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=909" target="_blank">00:15:09.080</a></span> | <span class="t">here about the words that were earlier in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=912" target="_blank">00:15:12.640</a></span> | <span class="t">Attention emerges as a way to remind ourselves in late states,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=918" target="_blank">00:15:18.120</a></span> | <span class="t">what was in earlier ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=920" target="_blank">00:15:20.000</a></span> | <span class="t">We could do that with a scoring function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=922" target="_blank">00:15:22.120</a></span> | <span class="t">and here what I've depicted is a simple dot product scoring function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=925" target="_blank">00:15:25.780</a></span> | <span class="t">exactly the sort that we get from the transformer in essence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=929" target="_blank">00:15:29.360</a></span> | <span class="t">What it's doing is taking the dot product of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=931" target="_blank">00:15:31.560</a></span> | <span class="t">our target representation with all of the previous hidden states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=936" target="_blank">00:15:36.440</a></span> | <span class="t">We softmax normalize those,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=939" target="_blank">00:15:39.080</a></span> | <span class="t">and then we bring those into the representation that we are targeting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=943" target="_blank">00:15:43.880</a></span> | <span class="t">to get a context vector here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=945" target="_blank">00:15:45.880</a></span> | <span class="t">We could take the average of all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=948" target="_blank">00:15:48.220</a></span> | <span class="t">Then finally, we get this attention combination H here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=952" target="_blank">00:15:52.360</a></span> | <span class="t">and that could be a neural network parameterized function that takes in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=956" target="_blank">00:15:56.600</a></span> | <span class="t">this representation plus the attention representation that we created here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=961" target="_blank">00:16:01.760</a></span> | <span class="t">and feeds that through some parameters and a non-linearity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=966" target="_blank">00:16:06.160</a></span> | <span class="t">That finally gives us the representation that we feed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=969" target="_blank">00:16:09.640</a></span> | <span class="t">into the classifier that we wanted to fit originally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=973" target="_blank">00:16:13.400</a></span> | <span class="t">The idea here is that now our classification decision is based indeed on this representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=979" target="_blank">00:16:19.680</a></span> | <span class="t">at the end but now infused with a lot of information about how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=983" target="_blank">00:16:23.880</a></span> | <span class="t">similar that representation is to the ones that preceded it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=987" target="_blank">00:16:27.600</a></span> | <span class="t">That is the essential idea behind dot product attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=991" target="_blank">00:16:31.560</a></span> | <span class="t">which will be the beating heart,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=993" target="_blank">00:16:33.320</a></span> | <span class="t">so to speak, of the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=996" target="_blank">00:16:36.480</a></span> | <span class="t">Another idea that has proved so powerful is a notion of sub-word modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1002" target="_blank">00:16:42.240</a></span> | <span class="t">I thought I would take you on a brief journey of how we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1004" target="_blank">00:16:44.520</a></span> | <span class="t">arrived in the current phase for this sub-word modeling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1008" target="_blank">00:16:48.480</a></span> | <span class="t">beginning with ELMo,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1009" target="_blank">00:16:49.920</a></span> | <span class="t">because what ELMo did is truly fascinating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1013" target="_blank">00:16:53.080</a></span> | <span class="t">The ELMo word representation space begins with character level representations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1018" target="_blank">00:16:58.960</a></span> | <span class="t">and then it has a bunch of filters on top of those,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1022" target="_blank">00:17:02.440</a></span> | <span class="t">and then it has a bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1024" target="_blank">00:17:04.240</a></span> | <span class="t">different convolutional layers that we then do max pooling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1028" target="_blank">00:17:08.120</a></span> | <span class="t">over to get a representation for the entire sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1032" target="_blank">00:17:12.160</a></span> | <span class="t">We do that at different layers here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1034" target="_blank">00:17:14.160</a></span> | <span class="t">and so those get concatenated up into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1036" target="_blank">00:17:16.080</a></span> | <span class="t">these max pooling representations at the top,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1039" target="_blank">00:17:19.180</a></span> | <span class="t">and those form the basis for word representations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1042" target="_blank">00:17:22.560</a></span> | <span class="t">and the idea is that this gives us whole word vectors that nonetheless have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1048" target="_blank">00:17:28.080</a></span> | <span class="t">lots of information about the sub-word parts all the way down to characters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1053" target="_blank">00:17:33.340</a></span> | <span class="t">but including all of these convolutions of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1055" target="_blank">00:17:35.440</a></span> | <span class="t">different lengths that capture different notions of sub-word within that space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1059" target="_blank">00:17:39.880</a></span> | <span class="t">Incredibly visionary, I would say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1062" target="_blank">00:17:42.640</a></span> | <span class="t">One thing I should note though is that the ELMo vocabulary has about 100,000 words in it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1067" target="_blank">00:17:47.920</a></span> | <span class="t">which is an enormous vocabulary,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1070" target="_blank">00:17:50.100</a></span> | <span class="t">and even still, if you deal with real text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1072" target="_blank">00:17:52.840</a></span> | <span class="t">you will find that you are mostly encountering words that are not in that vocabulary,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1077" target="_blank">00:17:57.980</a></span> | <span class="t">and even if you double it to 200,000 or 300,000,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1082" target="_blank">00:18:02.340</a></span> | <span class="t">now you're getting a really large embedding space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1084" target="_blank">00:18:04.600</a></span> | <span class="t">you will still mostly encounter words that are unked out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1089" target="_blank">00:18:09.360</a></span> | <span class="t">that is unknown to your model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1091" target="_blank">00:18:11.320</a></span> | <span class="t">and that's an incredibly limiting factor for this whole word approach,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1095" target="_blank">00:18:15.600</a></span> | <span class="t">but we see in here the essence of the idea that we should model sub-words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1100" target="_blank">00:18:20.600</a></span> | <span class="t">A big change happened for the transformer when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1104" target="_blank">00:18:24.320</a></span> | <span class="t">got in parallel this notion of word piece tokenization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1108" target="_blank">00:18:28.280</a></span> | <span class="t">Here I'm going to give you a feel for that by looking at the BERT tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1112" target="_blank">00:18:32.240</a></span> | <span class="t">That gets loaded in cell 2 here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1115" target="_blank">00:18:35.400</a></span> | <span class="t">and then when we call the tokenizer on the sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1117" target="_blank">00:18:37.880</a></span> | <span class="t">this isn't too surprising,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1119" target="_blank">00:18:39.520</a></span> | <span class="t">we get things that look mostly like whole words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1122" target="_blank">00:18:42.420</a></span> | <span class="t">especially if you're used to NLP where the suffix for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1125" target="_blank">00:18:45.980</a></span> | <span class="t">isn't has been broken apart and so is the punctuation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1129" target="_blank">00:18:49.500</a></span> | <span class="t">But when we call the tokenizer on the sequence encode me with an exclamation mark,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1134" target="_blank">00:18:54.860</a></span> | <span class="t">notice that the word encode has been split apart into two words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1139" target="_blank">00:18:59.640</a></span> | <span class="t">n and then code with those markers there indicating that that is a word internal piece.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1145" target="_blank">00:19:05.500</a></span> | <span class="t">If you tokenize a word like snuffleupagus,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1149" target="_blank">00:19:09.180</a></span> | <span class="t">you get a sequence of 1, 2, 3, 4,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1152" target="_blank">00:19:12.460</a></span> | <span class="t">5, 6 parts to that single word that came in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1156" target="_blank">00:19:16.900</a></span> | <span class="t">The effect here is that we can have a vanishingly small vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1161" target="_blank">00:19:21.540</a></span> | <span class="t">There are under 30,000 words in this BERT tokenization space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1165" target="_blank">00:19:25.620</a></span> | <span class="t">so a very small embedding space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1167" target="_blank">00:19:27.780</a></span> | <span class="t">But nonetheless, when we encounter words that are outside of that space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1171" target="_blank">00:19:31.980</a></span> | <span class="t">they don't get unked out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1173" target="_blank">00:19:33.560</a></span> | <span class="t">but rather we analyze them into sub-word pieces that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1176" target="_blank">00:19:36.940</a></span> | <span class="t">do have embedding representations for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1180" target="_blank">00:19:40.460</a></span> | <span class="t">Incredibly powerful and in the context of a contextual model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1185" target="_blank">00:19:45.420</a></span> | <span class="t">we might have some hope that for cases like encode being split into two tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1190" target="_blank">00:19:50.740</a></span> | <span class="t">the model will learn internally that in some sense,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1193" target="_blank">00:19:53.780</a></span> | <span class="t">those form a coherent piece, a word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1196" target="_blank">00:19:56.620</a></span> | <span class="t">But we don't need that directly reflected in the tokenizers vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1202" target="_blank">00:20:02.400</a></span> | <span class="t">Incredible idea. A related idea for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1206" target="_blank">00:20:06.320</a></span> | <span class="t">the transformer that is so foundational and that I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1208" target="_blank">00:20:08.860</a></span> | <span class="t">the field is still figuring out is positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1212" target="_blank">00:20:12.980</a></span> | <span class="t">When we talk about the transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1214" target="_blank">00:20:14.660</a></span> | <span class="t">you will see that it has almost no way of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1216" target="_blank">00:20:16.700</a></span> | <span class="t">keeping track of the order of words in a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1219" target="_blank">00:20:19.300</a></span> | <span class="t">It's mostly a bunch of columns of different things that happen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1222" target="_blank">00:20:22.860</a></span> | <span class="t">independently with some attention mechanisms bringing them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1227" target="_blank">00:20:27.300</a></span> | <span class="t">To capture word order,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1229" target="_blank">00:20:29.240</a></span> | <span class="t">we typically have something like a positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1232" target="_blank">00:20:32.420</a></span> | <span class="t">The most heavy-handed way to do that is to simply have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1236" target="_blank">00:20:36.020</a></span> | <span class="t">in addition to your word embedding space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1238" target="_blank">00:20:38.400</a></span> | <span class="t">a positional embedding space that simply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1241" target="_blank">00:20:41.060</a></span> | <span class="t">records where individual words appear in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1244" target="_blank">00:20:44.940</a></span> | <span class="t">Here, the is paired with one because it's at the start of the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1248" target="_blank">00:20:48.480</a></span> | <span class="t">But if the was in position 4,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1250" target="_blank">00:20:50.900</a></span> | <span class="t">it would be this fixed vector here combined with the embedding for position 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1256" target="_blank">00:20:56.560</a></span> | <span class="t">Those get added together into what you might think of as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1259" target="_blank">00:20:59.580</a></span> | <span class="t">the basis for contextual representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1263" target="_blank">00:21:03.300</a></span> | <span class="t">This has proved effective,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1265" target="_blank">00:21:05.060</a></span> | <span class="t">but it has many limitations that we're going to talk about later in the unit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1268" target="_blank">00:21:08.620</a></span> | <span class="t">and we're going to explore ways to capture what's good about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1271" target="_blank">00:21:11.460</a></span> | <span class="t">positional encoding while also overcoming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1273" target="_blank">00:21:13.940</a></span> | <span class="t">some of the problems that it introduces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1277" target="_blank">00:21:17.060</a></span> | <span class="t">Then of course, one of the major guiding ideas behind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1281" target="_blank">00:21:21.620</a></span> | <span class="t">all of this is simply massive scale pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1285" target="_blank">00:21:25.500</a></span> | <span class="t">This is an idea that was unlocked by the distributional hypothesis,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1289" target="_blank">00:21:29.980</a></span> | <span class="t">which had the insight that we don't need to write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1292" target="_blank">00:21:32.100</a></span> | <span class="t">hand-built feature functions but rather we can just rely on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1295" target="_blank">00:21:35.380</a></span> | <span class="t">unlabeled corpora and keep track of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1298" target="_blank">00:21:38.140</a></span> | <span class="t">which words are appearing with which other words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1300" target="_blank">00:21:40.980</a></span> | <span class="t">It really comes into its own in the neural era with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1304" target="_blank">00:21:44.520</a></span> | <span class="t">models like Word2Vec and GloVe, as I mentioned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1308" target="_blank">00:21:48.100</a></span> | <span class="t">Following that, we get the ELMo paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1310" target="_blank">00:21:50.660</a></span> | <span class="t">and I mentioned before that that was the eye-opening moment when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1313" target="_blank">00:21:53.540</a></span> | <span class="t">saw that we could learn contextual representations at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1317" target="_blank">00:21:57.100</a></span> | <span class="t">scale and have that transfer into tasks we wanted to fine-tune for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1322" target="_blank">00:22:02.460</a></span> | <span class="t">Of course, you get the GPT paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1325" target="_blank">00:22:05.020</a></span> | <span class="t">and then BERT launches the BERT era.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1328" target="_blank">00:22:08.460</a></span> | <span class="t">Then you get, at the end of this little history here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1331" target="_blank">00:22:11.780</a></span> | <span class="t">the GPT-3 paper which applied this massive scale idea at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1336" target="_blank">00:22:16.300</a></span> | <span class="t">a level that was previously unimagined and unimaginable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1340" target="_blank">00:22:20.180</a></span> | <span class="t">and that really did introduce a phase change in research as we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1343" target="_blank">00:22:23.780</a></span> | <span class="t">started to deal with these truly massive models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1346" target="_blank">00:22:26.900</a></span> | <span class="t">and ask them to learn in context,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1349" target="_blank">00:22:29.420</a></span> | <span class="t">that is just from prompts we offer them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1351" target="_blank">00:22:31.480</a></span> | <span class="t">how to perform tasks and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1354" target="_blank">00:22:34.660</a></span> | <span class="t">Then related to this, of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1357" target="_blank">00:22:37.180</a></span> | <span class="t">is the idea of fine-tuning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1359" target="_blank">00:22:39.300</a></span> | <span class="t">and its corresponding notion of pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1361" target="_blank">00:22:41.840</a></span> | <span class="t">Here's a brief review of these ideas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1364" target="_blank">00:22:44.340</a></span> | <span class="t">In 2016-2018, the notion of pre-training that we had was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1369" target="_blank">00:22:49.300</a></span> | <span class="t">essentially that we would feed static word representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1372" target="_blank">00:22:52.260</a></span> | <span class="t">into variants of RNNs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1374" target="_blank">00:22:54.780</a></span> | <span class="t">and then the model would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1376" target="_blank">00:22:56.520</a></span> | <span class="t">fine-tuned and it would learn a bunch of stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1379" target="_blank">00:22:59.100</a></span> | <span class="t">When we fast-forward to the BERT era in 2018,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1382" target="_blank">00:23:02.800</a></span> | <span class="t">we start to get fine-tuning of contextual models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1385" target="_blank">00:23:05.980</a></span> | <span class="t">Here is just a bit of code of a source that you will write in this course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1390" target="_blank">00:23:10.180</a></span> | <span class="t">where we read in BERT representations and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1393" target="_blank">00:23:13.140</a></span> | <span class="t">fine-tune them essentially to be a classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1396" target="_blank">00:23:16.820</a></span> | <span class="t">That started in 2018, and I think it continues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1399" target="_blank">00:23:19.940</a></span> | <span class="t">We might be headed into an era in which most of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1402" target="_blank">00:23:22.740</a></span> | <span class="t">the fine-tuning that happens is on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1404" target="_blank">00:23:24.980</a></span> | <span class="t">these massive language models that we mostly don't have access to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1408" target="_blank">00:23:28.460</a></span> | <span class="t">We can't write code as in the BERT code there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1411" target="_blank">00:23:31.780</a></span> | <span class="t">but rather we just call an API and some partially understood and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1416" target="_blank">00:23:36.440</a></span> | <span class="t">partially known to us fine-tuning process,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1419" target="_blank">00:23:39.480</a></span> | <span class="t">fine-tune some of the parameters for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1421" target="_blank">00:23:41.500</a></span> | <span class="t">one of these large models to do what we want to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1424" target="_blank">00:23:44.080</a></span> | <span class="t">I'm hoping that we still see a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1425" target="_blank">00:23:45.900</a></span> | <span class="t">more of this custom code being written.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1427" target="_blank">00:23:47.760</a></span> | <span class="t">It's very powerful analytically and technologically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1431" target="_blank">00:23:51.060</a></span> | <span class="t">but this is surely part of your future as an NLP-er as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=FEFeeRONEdw&t=1437" target="_blank">00:23:57.420</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>