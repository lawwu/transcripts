<html><head><title>Wav2Lip (generate talking avatar videos) - Paper reading and explanation</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Wav2Lip (generate talking avatar videos) - Paper reading and explanation</h2><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0"><img src="https://i.ytimg.com/vi/n9ILOE2kyB0/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./n9ILOE2kyB0.html">Whisper Transcript</a> | <a href="./transcript_n9ILOE2kyB0.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys, welcome to my review of the lip-sync expert is all you need for speech to lip generation in the wild</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=6" target="_blank">00:00:06.400</a></span> | <span class="t">which is a very long title to say that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=8" target="_blank">00:00:08.480</a></span> | <span class="t">Basically, we are talking about a model that can allow you to generate lip-synced videos</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=14" target="_blank">00:00:14.720</a></span> | <span class="t">Using arbitrary videos and arbitrary audios. Let's see an example directly from the official website of the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=21" target="_blank">00:00:21.920</a></span> | <span class="t">Here in the example tree proposed we can see that there is a video without any audio</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=29" target="_blank">00:00:29.120</a></span> | <span class="t">And the person here is not talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=31" target="_blank">00:00:31.120</a></span> | <span class="t">Then we have an audio of three seconds. Let's listen to it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=34" target="_blank">00:00:34.640</a></span> | <span class="t">I'll go around wait for my call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=37" target="_blank">00:00:37.680</a></span> | <span class="t">Then we press this button sync this pair and the output video which is here. I already generated it before we can play it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=45" target="_blank">00:00:45.760</a></span> | <span class="t">I'll go around wait for my call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=49" target="_blank">00:00:49.280</a></span> | <span class="t">As you can see the results are remarkable. We can see that the person</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=53" target="_blank">00:00:53.520</a></span> | <span class="t">Before was not not talking at all. And now actually the his lip has been automatically generated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=59" target="_blank">00:00:59.140</a></span> | <span class="t">And the what the movement of his lip actually match what he's saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=63" target="_blank">00:01:03.680</a></span> | <span class="t">I don't see any big difference actually from what I would expect from any other person</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=69" target="_blank">00:01:09.600</a></span> | <span class="t">Let's go into the details of how this all works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=73" target="_blank">00:01:13.120</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=75" target="_blank">00:01:15.840</a></span> | <span class="t">The the system can be used for many applications for example for dubbing videos in multiple languages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=83" target="_blank">00:01:23.360</a></span> | <span class="t">educational videos or any other thing that you like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=86" target="_blank">00:01:26.080</a></span> | <span class="t">And let's go to the architecture of the video of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=91" target="_blank">00:01:31.120</a></span> | <span class="t">We can see that we have two streams. One is an audio stream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=94" target="_blank">00:01:34.560</a></span> | <span class="t">And one way one is a video stream. They are both down sampled using a convolutional neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=101" target="_blank">00:01:41.600</a></span> | <span class="t">Combined together and then up sampled again with skip connections from the video stream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=107" target="_blank">00:01:47.680</a></span> | <span class="t">And this is the generator. So we are talking about a GAN network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=112" target="_blank">00:01:52.300</a></span> | <span class="t">Actually more or less a GAN we will see why it's different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=115" target="_blank">00:01:55.420</a></span> | <span class="t">And then the generator frame so we have a sequence of frames</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=121" target="_blank">00:02:01.420</a></span> | <span class="t">are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=123" target="_blank">00:02:03.580</a></span> | <span class="t">Compared with what is the ground truth and this is the reconstruction loss of the image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=128" target="_blank">00:02:08.860</a></span> | <span class="t">Actually, the authors claim that this the reconstruction loss is not enough to generate a good image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=138" target="_blank">00:02:18.380</a></span> | <span class="t">And which is basically also the technique used by previous models. So before the Wav2Lip was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=144" target="_blank">00:02:24.540</a></span> | <span class="t">introduced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=148" target="_blank">00:02:28.860</a></span> | <span class="t">Because as the author claims</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=151" target="_blank">00:02:31.820</a></span> | <span class="t">in the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=154" target="_blank">00:02:34.620</a></span> | <span class="t">here in 3.1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=155" target="_blank">00:02:35.740</a></span> | <span class="t">we can see that the pixel level reconstruction loss is a weak judge of lip-sync why because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=159" target="_blank">00:02:39.820</a></span> | <span class="t">The the system tries to the model tries to generate the image and try to make it look like the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=165" target="_blank">00:02:45.740</a></span> | <span class="t">however, the model doesn't concentrate on the lip area only which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=170" target="_blank">00:02:50.140</a></span> | <span class="t">What is what we want is one of the most important thing that we want to judge in this model, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=175" target="_blank">00:02:55.260</a></span> | <span class="t">But they say that the lip area actually correspond to less than four percent of the total reconstruction loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=180" target="_blank">00:03:00.540</a></span> | <span class="t">So it is um, we can we should find a way to concentrate on that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=185" target="_blank">00:03:05.340</a></span> | <span class="t">To generate a better lip area, of course while preserving the original image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=190" target="_blank">00:03:10.380</a></span> | <span class="t">So we don't want the background to change. We don't want the pose of the person to change etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=195" target="_blank">00:03:15.020</a></span> | <span class="t">So what the authors do they introduce a sync net sync net is a model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=199" target="_blank">00:03:19.420</a></span> | <span class="t">that allows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=201" target="_blank">00:03:21.980</a></span> | <span class="t">was introduced previously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=203" target="_blank">00:03:23.820</a></span> | <span class="t">Allows to check how much a video and audio are synced together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=208" target="_blank">00:03:28.620</a></span> | <span class="t">And if they are not synced by how much they are out of sync</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=211" target="_blank">00:03:31.740</a></span> | <span class="t">The authors they call it a lip-sync expert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=214" target="_blank">00:03:34.940</a></span> | <span class="t">They retrain the sync net</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=217" target="_blank">00:03:37.820</a></span> | <span class="t">from the ground</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=219" target="_blank">00:03:39.740</a></span> | <span class="t">Using little variations. For example, the original sync net was trained using black and white images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=225" target="_blank">00:03:45.900</a></span> | <span class="t">Now they use color images and secondly, they change the loss function to cosine similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=231" target="_blank">00:03:51.280</a></span> | <span class="t">So the generator actually the loss function of the generator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=236" target="_blank">00:03:56.560</a></span> | <span class="t">Is a combination of the L1 construction loss the GAN loss and the sync loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=243" target="_blank">00:04:03.500</a></span> | <span class="t">We can find it here in the equation number six</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=248" target="_blank">00:04:08.060</a></span> | <span class="t">So actually the L total is the total loss of the generator, which is a combination of this loss of this loss and this loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=255" target="_blank">00:04:15.100</a></span> | <span class="t">And there are some weights to choose how much emphasis to give to each loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=261" target="_blank">00:04:21.020</a></span> | <span class="t">The the system has been trained using an adapt optimizer, these are the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=267" target="_blank">00:04:27.520</a></span> | <span class="t">But let's go to check the results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=270" target="_blank">00:04:30.940</a></span> | <span class="t">now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=272" target="_blank">00:04:32.780</a></span> | <span class="t">The authors compare the current model with the previous models and using three different data sets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=280" target="_blank">00:04:40.300</a></span> | <span class="t">The first one is dubbed. So we have a video and the audio that is dubbed taken from the internet, I guess</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=286" target="_blank">00:04:46.780</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=289" target="_blank">00:04:49.340</a></span> | <span class="t">Where the audio and the video are not in sync</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=292" target="_blank">00:04:52.220</a></span> | <span class="t">And they try to sync it using Guav2Lip and also the two baseline models Speech2Read and LipGAN</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=297" target="_blank">00:04:57.980</a></span> | <span class="t">We can see that according to human evaluators. So these are all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=303" target="_blank">00:05:03.180</a></span> | <span class="t">Evaluations made by humans. The Guav2Lip actually is preferred. The method of evaluation is written here in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=310" target="_blank">00:05:10.220</a></span> | <span class="t">4.4.2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=312" target="_blank">00:05:12.760</a></span> | <span class="t">And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=314" target="_blank">00:05:14.700</a></span> | <span class="t">Secondly, we have a random data set with that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=318" target="_blank">00:05:18.060</a></span> | <span class="t">data set of random videos with random audios and the Guav2Lip is trained to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=323" target="_blank">00:05:23.820</a></span> | <span class="t">Sync them and finally we have TTS in which the audio is generated from a TTS system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=331" target="_blank">00:05:31.100</a></span> | <span class="t">As we can see overall the Guav2Lip is performing much better. We will see some example later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=336" target="_blank">00:05:36.540</a></span> | <span class="t">and according to human evaluators</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=339" target="_blank">00:05:39.580</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=342" target="_blank">00:05:42.220</a></span> | <span class="t">We see that here in the authors right finally it is worth noting that our lip-sync videos are preferred over existing methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=349" target="_blank">00:05:49.020</a></span> | <span class="t">Or even the actual unsynced videos over 90% of the time so it means that also the visual quality is not bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=358" target="_blank">00:05:58.540</a></span> | <span class="t">Here are some examples for example, we can see the red</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=361" target="_blank">00:06:01.420</a></span> | <span class="t">frames are from the previous models and we can see the quality of the face here of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=368" target="_blank">00:06:08.860</a></span> | <span class="t">the german chancellor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=371" target="_blank">00:06:11.500</a></span> | <span class="t">is not so good, but here with the syncnet with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=376" target="_blank">00:06:16.140</a></span> | <span class="t">Guav2Lip the reconstruction image is quite good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=381" target="_blank">00:06:21.660</a></span> | <span class="t">The authors actually train two models one with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=387" target="_blank">00:06:27.180</a></span> | <span class="t">GAN and one without the GAN loss and we can see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=390" target="_blank">00:06:30.540</a></span> | <span class="t">Without the GAN is performing better on some metrics and a little worse on some other metrics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=397" target="_blank">00:06:37.820</a></span> | <span class="t">And so actually I think this model have a lot of potential for generating talking avatars or for dubbing videos generating educational videos</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=406" target="_blank">00:06:46.860</a></span> | <span class="t">Maybe in the future. We don't need to record the three times the same video in multiple language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=411" target="_blank">00:06:51.820</a></span> | <span class="t">We can just generate it once and let the AI do the rest. Thank you for listening</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n9ILOE2kyB0&t=415" target="_blank">00:06:55.520</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>