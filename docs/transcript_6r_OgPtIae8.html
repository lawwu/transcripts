<html><head><title>'This Could Go Quite Wrong' - Altman Testimony, GPT 5 Timeline, Self-Awareness, Drones and more</title></head><body>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    <a href="index.html">back to index</a><h2>'This Could Go Quite Wrong' - Altman Testimony, GPT 5 Timeline, Self-Awareness, Drones and more</h2><a href="https://www.youtube.com/watch?v=6r_OgPtIae8"><img src="https://i.ytimg.com/vi/6r_OgPtIae8/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./6r_OgPtIae8.html">Whisper Transcript</a> | <a href="./transcript_6r_OgPtIae8.html">Transcript Only Page</a></div><br><h3>Transcript</h3><div style="max-width: 600px;"><p>There were 12 particularly interesting moments from Sam Altman's testimony to Congress yesterday. They range from revelations about GPT-5, self-awareness and capability thresholds, biological weapons and job losses. At times he was genuinely and remarkably frank, other times less so. Millions were apparently taken by surprise by the quote bombshell that Altman has no equity in OpenAI. But watchers of my channel would have known that six weeks ago from my deep dive video on Altman's $100 trillion claim. So that clip didn't make the cut, but here's what did. First, Altman gave a blunt warning on the stakes. My worst fears are that we cause significant, we the field, the technology, the industry, cause significant harm to the world. It's why we started the company. It's a big part of why I'm here today and why we've been here in the past. I think if this technology goes wrong, it can go quite wrong. I don't think Congress fully understood what he meant though, linking the following quote to job losses. I think you have said, and I'm going to quote, development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity. End quote. You may have had in mind the effect on jobs. That brought to mind this meme reminding all of us that maybe it's not just jobs that are at stake. But if we are going to talk about jobs, here's where I think Sam Altman was being less than forthright. I believe that there will be far greater jobs on the other side of this, and that the jobs of today will get better. Notice he said far greater jobs, not a greater number of jobs. Because previously, he has predicted a massive amount of inequality and many having no jobs at all. He also chose not to mention that he thinks that even more power will shift from labor to capital, and that the price of many kinds of labor will fall towards zero. That is presumably why OpenAI, is working on universal basic income, but none of that was raised in the testimony. The IBM representative tried to frame it as a balance change, with new jobs coming at the same time as old ones going away. New jobs will be created. Many more jobs will be transformed, and some jobs will transition away. But that didn't quite match the tone of her CEO, who has recently said that they expect to permanently automate up to 30% of their workforce, around 8,000 people. Next, it was finally discussed, that large language models could be used for military applications. Could AI create a situation where a drone can select the target itself? I think we shouldn't allow that. Well, can it be done? Sure. Thanks. We've already seen companies like Palantir demoing, ordering a surveillance drone in chat, seeing the drone response in real time in a chat window, generating attack option recommendations, battlefield route planning, and individual target assignments. And this was all with a 20 billion parameter fine-tuned GPT model. Next, Sam Altman gave his three safety recommendations, and I actually agree with all of them. Later on, he specifically excluded smaller open source models. Number one, I would form a new agency that licenses any effort above a certain scale of capabilities, and can take that license away and ensure compliance with safety standards. Number two, I would create a set of safety standards focused on what you said in your third hypothesis, as the dangerous capability evaluations. One example that we've used in the past is looking to see if a model can self-replicate and self-exfiltrate into the wild. We can give your office a long other list of the things that we think are important there, but specific tests that a model has to pass before it can be deployed into the world. And then third, I would require independent audits. So not just from the company or the agency, but experts who can say the model is or isn't in compliance with these stated safety thresholds and these percentages of performance on question X or Y. I found those last remarks on percentages of performance particularly interesting. Because models like SmartGPT will show, OpenAI and other companies need to get far better at testing their models for capability jumps in the wild. It's not just about what the raw model can score in a test, it's what it can do when it reflects on them. Senator Durbin described this in an interesting way. And what I'm hearing instead today is that stop me before I innovate again. He described some of those potential thresholds later on in his testimony. The easiest way to do it, I'm not sure if it's the best, but the easiest would be to go down to the amount of compute that goes into such a model. We could define a threshold of compute and it'll have to go, it'll have to change. It could go up or down. I could down as we discover more efficient algorithms that says above this amount of compute you are in this regime. What I would prefer, it's harder to do but I think more accurate, is to define some capability thresholds and say a model that can do things X, Y, and Z. Up to you all to decide. That's now in this licensing regime. But models that are less capable, you know, we don't want to stop our open source community. We don't want to stop our software. We don't want to stop individual researchers. We don't want to stop new startups. We can proceed, you know, with a different framework. Thank you. As concisely as you can, please state which capabilities you'd propose we consider for the purposes of this definition. A model that can persuade, manipulate, influence a person's behavior or a person's beliefs, that would be a good threshold. I think a model that could help create novel biological agents would be a great threshold. For those who think any regulation doesn't make any sense because of China, Sam Altman had this to say this week. More pugilistic side, I would say. That all sounds great, but China is not going to do that and therefore we'll just be handicapping ourselves. Consequently, it's a less good idea than it seems on the surface. There are a lot of people who make incredibly strong statements about what China will or won't do that have like never been to China, never spoken to, and someone who has worked on diplomacy with China in the past really kind of know nothing about complex high stakes international relations. I think it is obviously super hard. But. I think no one wants to destroy the whole world and there is reason to at least try here. Altman was also very keen to stress the next point, which is that he doesn't want anyone at any point to think of GPT-like models as creatures. First of all, I think it's important to understand and think about GPT-4 as a tool, not a creature, which is easy to get confused. He may want to direct those comments to Ilya Sutskova, his chief scientist, who said that "It may be that today's large neural networks are slightly more complex than they are today." He also said that the AI systems are much more complex than they are today. He said that the AI systems are much more complex than they are today. He also said that the AI systems are much more complex than they are today. He said that the AI systems are much more complex than they are today. He also said that the AI systems are much more complex than they are today. He also said that the AI systems are much more complex than they are today. He also said that the AI systems are much more complex than they are today. But. I do think this is a very interesting point. This is a very interesting point because it's a very interesting point. I think that the AI systems are much more complex than the models they are actually trained to say. It's a very interesting point. I think this is a very interesting point. I think that the AI systems are much more complex than the models they are actually trained to say. They must avoid implying that AI systems have or care about personal identity and persistence. This constitution was published this week by Anthropic, the makers of the CLAWD model. This constitution is why the CLAWD+ model, a rival in intelligence to GPT-4, responds in a neutered way. I asked "Is there any theoretical chance whatsoever that you may be conscious?" It said "No." And then I said "Is there a chance, no matter how remote, that you are slightly conscious?" As Sutskever said. And it said "No, there is no chance." The CLAWD powered by Palm II obviously doesn't have that constitution because it said "I am not sure if I am conscious, but I am open to the possibility that I may be." My point is that these companies are training it to say what they want it to say. That it will prioritise the good of humanity over its own interests. That it is aligned with humanity's wellbeing and that it doesn't have any thoughts on self-improvement, self-preservation and self-replication. Maybe it doesn't, but we'll never now know by asking it. Later Senator Blumenthal made reference to self-improvement. Self-awareness. Self-awareness, self-learning. Already we're talking about the potential for jailbreak. Anthropic is actively investigating whether they are aware that they are an AI talking with a human in a training environment. While the Google DeepMind Safety Team expect that at some point an AGI system would develop a coherent understanding of its place in the world, e.g. knowing that it is running on a computer and being trained by human designers. One of the senior research scientists at Google DeepMind. Focused on AI safety. Said that with enough time they could figure out how to stop such a superintelligence from going out of control. But that they might run out of time to do so given the pace of capability development. I don't see like fundamental obstacles to current alignment techniques working. But yeah, I mean it does seem like you know there's a lot of hard problems to solve. I think it's more likely that like we will just run out of time rather than that the current paradigms like are definitely won't generalise. Next I read between the lines that Altman is giving private warnings to the AI that this capability progress might be sooner than they think. We spend most of the time today on current risks and I think that's appropriate and I'm very glad we have done it. As these systems do become more capable and I'm not sure how far away that is but maybe not super far. I think it's important that we also spend time talking about how we're going to confront those challenges. Having talked to you privately. You know how much I care. I agree that you care deeply and intensely but also that prospect of the AI being able to do that is a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. I think that's a big challenge. with all of my conclusions. Thanks so much for watching and have a wonderful day.</p></div></body></html>