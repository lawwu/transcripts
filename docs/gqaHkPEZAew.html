<html><head><title>Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 2 - Neural Classifiers</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 2 - Neural Classifiers</h2><a href="https://www.youtube.com/watch?v=gqaHkPEZAew"><img src="https://i.ytimg.com/vi/gqaHkPEZAew/sddefault.jpg?sqp=-oaymwEmCIAFEOAD8quKqQMa8AEB-AH-DoACuAiKAgwIABABGF0gWShlMA8=&rs=AOn4CLA39MskiJa37oVfs-kE1IIjvuUh8g" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=125">2:5</a> Parameters<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=243">4:3</a> Learning Phase<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=479">7:59</a> Gradient Descent<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=519">8:39</a> Stochastic Gradient Descent<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=689">11:29</a> Note on Word Vectors<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=826">13:46</a> Word tovec<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1013">16:53</a> Negative sampling<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1086">18:6</a> Logistic function<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1215">20:15</a> Negative log likelihood<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1407">23:27</a> Cooccurrence matrix<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1519">25:19</a> Questions<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1789">29:49</a> Cooccurrence matrices<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1939">32:19</a> Singular value decomposition<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2177">36:17</a> Kohls model<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2273">37:53</a> Glove algorithm<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2552">42:32</a> Log by linear model<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2586">43:6</a> Explicit loss function<br><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2685">44:45</a> Question<br><br><div style="text-align: left;"><a href="./gqaHkPEZAew.html">Whisper Transcript</a> | <a href="./transcript_gqaHkPEZAew.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Okay, so what are we going to do for today?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=7" target="_blank">00:00:07.520</a></span> | <span class="t">So the main content for today is to, um, go through sort of more stuff about word vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=17" target="_blank">00:00:17.040</a></span> | <span class="t">including touching on word sensors, and then introducing the notion of neural network classifiers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=23" target="_blank">00:00:23.240</a></span> | <span class="t">So our biggest goal is that by the end of today's class, you should feel like you could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=28" target="_blank">00:00:28.040</a></span> | <span class="t">confidently look at one of the word embeddings papers, such as the Google Word2Vec paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=34" target="_blank">00:00:34.280</a></span> | <span class="t">or the GloVe paper, or Sanjeev Arora's paper that we'll come to later, and feel like, yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=39" target="_blank">00:00:39.360</a></span> | <span class="t">I can understand this, I know what they're doing, and it makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=43" target="_blank">00:00:43.480</a></span> | <span class="t">So let's go back to where we were.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=45" target="_blank">00:00:45.920</a></span> | <span class="t">So this was sort of introducing this model of Word2Vec.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=49" target="_blank">00:00:49.760</a></span> | <span class="t">And at the time, your idea was that we started with random word vectors, and then we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=57" target="_blank">00:00:57.200</a></span> | <span class="t">to sort of, we have a big corpus of text, and we're going to iterate through each word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=61" target="_blank">00:01:01.440</a></span> | <span class="t">in the whole corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=63" target="_blank">00:01:03.120</a></span> | <span class="t">And for each position, we're going to try and predict what words surround our center</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=69" target="_blank">00:01:09.880</a></span> | <span class="t">word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=70" target="_blank">00:01:10.880</a></span> | <span class="t">And we're going to do that with a probability distribution that's defined in terms of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=75" target="_blank">00:01:15.760</a></span> | <span class="t">dot product between the word vectors for the center word and the context words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=83" target="_blank">00:01:23.080</a></span> | <span class="t">And so that will give a probability estimate of a word appearing in the context of "into."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=87" target="_blank">00:01:27.760</a></span> | <span class="t">Well, actual words did occur in the context of "into" on this occasion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=92" target="_blank">00:01:32.600</a></span> | <span class="t">So what we're going to want to do is sort of make it more likely that turning problems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=97" target="_blank">00:01:37.880</a></span> | <span class="t">banking, and crises will turn up in the context of "into."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=102" target="_blank">00:01:42.040</a></span> | <span class="t">And so that's learning, updating the word vectors so that they can predict actual surrounding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=107" target="_blank">00:01:47.080</a></span> | <span class="t">words better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=109" target="_blank">00:01:49.800</a></span> | <span class="t">And then the thing that's almost magical is that doing no more than this simple algorithm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=116" target="_blank">00:01:56.040</a></span> | <span class="t">this allows us to learn word vectors that capture well word similarity and meaningful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=121" target="_blank">00:02:01.360</a></span> | <span class="t">directions in a word space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=125" target="_blank">00:02:05.100</a></span> | <span class="t">So more precisely, right, for this model, the only parameters of this model are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=131" target="_blank">00:02:11.560</a></span> | <span class="t">word vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=132" target="_blank">00:02:12.740</a></span> | <span class="t">So we have outside word vectors and center word vectors for each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=137" target="_blank">00:02:17.440</a></span> | <span class="t">And then we're taking their dot product to get a probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=141" target="_blank">00:02:21.640</a></span> | <span class="t">Well, we're taking a dot product to get a score of how likely a particular outside word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=147" target="_blank">00:02:27.560</a></span> | <span class="t">is to occur with the center word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=149" target="_blank">00:02:29.600</a></span> | <span class="t">And then we're using the softmax transformation to convert those scores into probabilities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=154" target="_blank">00:02:34.880</a></span> | <span class="t">as I discussed last time, and I kind of come back to at the end this time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=160" target="_blank">00:02:40.320</a></span> | <span class="t">A couple of things to note.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=162" target="_blank">00:02:42.620</a></span> | <span class="t">This model is what we call in NLP a bag of words model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=167" target="_blank">00:02:47.720</a></span> | <span class="t">So bag of words models are models that don't actually pay any attention to word order or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=173" target="_blank">00:02:53.320</a></span> | <span class="t">position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=174" target="_blank">00:02:54.320</a></span> | <span class="t">It doesn't matter if you're next to the center word or a bit further away on the left or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=178" target="_blank">00:02:58.080</a></span> | <span class="t">right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=179" target="_blank">00:02:59.080</a></span> | <span class="t">The probability estimate would be the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=182" target="_blank">00:03:02.160</a></span> | <span class="t">And that seems like a very crude model of language that will offend any linguist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=187" target="_blank">00:03:07.880</a></span> | <span class="t">And it is a very crude model of language, and we'll move on to better models of language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=191" target="_blank">00:03:11.840</a></span> | <span class="t">as we go on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=192" target="_blank">00:03:12.840</a></span> | <span class="t">But even that crude model of language is enough to learn quite a lot of the probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=198" target="_blank">00:03:18.280</a></span> | <span class="t">Sorry, quite a lot about the properties of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=203" target="_blank">00:03:23.840</a></span> | <span class="t">And then the second note is, well, with this model, we wanted to give reasonably high probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=212" target="_blank">00:03:32.760</a></span> | <span class="t">to the words that do occur in the context of the center word, at least if they do so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=218" target="_blank">00:03:38.440</a></span> | <span class="t">at all often.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=219" target="_blank">00:03:39.920</a></span> | <span class="t">But obviously lots of different words can occur.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=223" target="_blank">00:03:43.080</a></span> | <span class="t">So we're not talking about probabilities like .3 and .5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=227" target="_blank">00:03:47.000</a></span> | <span class="t">We're more likely going to be talking about probabilities like .01 and numbers like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=232" target="_blank">00:03:52.720</a></span> | <span class="t">Well, how do we achieve that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=235" target="_blank">00:03:55.800</a></span> | <span class="t">And well, the way that the word2vec model achieves this, and this is the learning phase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=241" target="_blank">00:04:01.280</a></span> | <span class="t">of the model, is to place words that are similar in meaning close to each other in this high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=249" target="_blank">00:04:09.200</a></span> | <span class="t">dimensional vector space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=251" target="_blank">00:04:11.180</a></span> | <span class="t">So again, you can't read this one, but if we scroll into this one, we see lots of words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=258" target="_blank">00:04:18.600</a></span> | <span class="t">that are similar in meaning grouped close together in the space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=261" target="_blank">00:04:21.760</a></span> | <span class="t">So here are days of the week like Tuesday, Thursday, Sunday, and also Christmas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=267" target="_blank">00:04:27.080</a></span> | <span class="t">Over, what else do we have?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=271" target="_blank">00:04:31.960</a></span> | <span class="t">We have Samsung and Nokia.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=275" target="_blank">00:04:35.040</a></span> | <span class="t">This is a diagram I made quite a few years ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=278" target="_blank">00:04:38.480</a></span> | <span class="t">So that's when Nokia was still an important maker of cell phones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=282" target="_blank">00:04:42.680</a></span> | <span class="t">We have various sort of fields like mathematics and economics over here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=287" target="_blank">00:04:47.520</a></span> | <span class="t">So we group words that are similar in meaning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=292" target="_blank">00:04:52.320</a></span> | <span class="t">Actually one more note I wanted to make on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=294" target="_blank">00:04:54.280</a></span> | <span class="t">I mean, again, this is a two dimensional picture, which is all I can show you on a slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=301" target="_blank">00:05:01.360</a></span> | <span class="t">And it's done with a principal components projection that you'll also use in the assignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=308" target="_blank">00:05:08.400</a></span> | <span class="t">Something important to remember, but hard to remember, is that high dimensional spaces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=314" target="_blank">00:05:14.160</a></span> | <span class="t">have very different properties to the two dimensional spaces that we can look at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=319" target="_blank">00:05:19.160</a></span> | <span class="t">And so in particular, a word, a vector, can be close to many other things in a high dimensional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=327" target="_blank">00:05:27.000</a></span> | <span class="t">space, but close to them on different dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=330" target="_blank">00:05:30.760</a></span> | <span class="t">Okay, so I've mentioned doing learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=336" target="_blank">00:05:36.280</a></span> | <span class="t">So the next question is, well, how do we learn good word vectors?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=342" target="_blank">00:05:42.880</a></span> | <span class="t">And this was the bit that I didn't quite hook up at the end of last class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=348" target="_blank">00:05:48.120</a></span> | <span class="t">So for a while in the last, I said, oh, geocalculus, and we have to work out the gradient of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=355" target="_blank">00:05:55.080</a></span> | <span class="t">loss function with respect to the parameters that will allow us to make progress.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=360" target="_blank">00:06:00.000</a></span> | <span class="t">But I didn't sort of altogether put that together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=362" target="_blank">00:06:02.260</a></span> | <span class="t">So what we're going to do is we start off with random word vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=368" target="_blank">00:06:08.800</a></span> | <span class="t">We initialize them to small numbers near zero in each dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=373" target="_blank">00:06:13.160</a></span> | <span class="t">We've defined our loss function J, which we looked at last time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=378" target="_blank">00:06:18.800</a></span> | <span class="t">And then we're going to use a gradient descent algorithm, which is an iterative algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=384" target="_blank">00:06:24.800</a></span> | <span class="t">that learns to maximize J of theta by changing theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=389" target="_blank">00:06:29.340</a></span> | <span class="t">And so the idea of this algorithm is that from the current values of theta, you calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=396" target="_blank">00:06:36.120</a></span> | <span class="t">the gradient J of theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=398" target="_blank">00:06:38.520</a></span> | <span class="t">And then what you're going to do is make a small step in the direction of the negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=403" target="_blank">00:06:43.400</a></span> | <span class="t">gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=404" target="_blank">00:06:44.400</a></span> | <span class="t">So the gradient is pointing upwards, and we're taking a small step in the direction of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=409" target="_blank">00:06:49.560</a></span> | <span class="t">negative of the gradient to gradually move down towards the minimum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=415" target="_blank">00:06:55.320</a></span> | <span class="t">And so one of the parameters of neural nets that you can fiddle in your software package</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=420" target="_blank">00:07:00.600</a></span> | <span class="t">is what is the step size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=422" target="_blank">00:07:02.900</a></span> | <span class="t">So if you take a really, really itsy bitsy step, it might take you a long time to minimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=428" target="_blank">00:07:08.900</a></span> | <span class="t">the function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=429" target="_blank">00:07:09.900</a></span> | <span class="t">You do a lot of wasted computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=433" target="_blank">00:07:13.220</a></span> | <span class="t">On the other hand, if your step size is much too big, well, then you can actually diverge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=440" target="_blank">00:07:20.940</a></span> | <span class="t">and start going to worse places.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=443" target="_blank">00:07:23.360</a></span> | <span class="t">Or even if you are going downhill a little bit, that what's going to happen is you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=447" target="_blank">00:07:27.580</a></span> | <span class="t">then going to end up bouncing back and forth, and it'll take you much longer to get to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=451" target="_blank">00:07:31.820</a></span> | <span class="t">minimum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=452" target="_blank">00:07:32.820</a></span> | <span class="t">OK, in this picture, I have a beautiful quadratic, and it's easy to minimize it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=460" target="_blank">00:07:40.980</a></span> | <span class="t">Something that you might know about neural networks is that in general, they're not convex.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=465" target="_blank">00:07:45.780</a></span> | <span class="t">So you could think that this is just all going to go awry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=469" target="_blank">00:07:49.920</a></span> | <span class="t">But the truth is in practice, life works out to be OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=472" target="_blank">00:07:52.940</a></span> | <span class="t">But I think I won't get into that more right now and come back to that in a later class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=479" target="_blank">00:07:59.380</a></span> | <span class="t">So this is our gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=481" target="_blank">00:08:01.420</a></span> | <span class="t">So we have the current values of the parameters theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=484" target="_blank">00:08:04.740</a></span> | <span class="t">We then walk a little bit in the negative direction of the gradient using our learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=492" target="_blank">00:08:12.260</a></span> | <span class="t">rate or step size alpha.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=494" target="_blank">00:08:14.400</a></span> | <span class="t">And that gives us new parameter values, where that means that these are vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=499" target="_blank">00:08:19.980</a></span> | <span class="t">But for each individual parameter, we are updating it a little bit by working out the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=505" target="_blank">00:08:25.720</a></span> | <span class="t">partial derivative of J with respect to that parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=512" target="_blank">00:08:32.220</a></span> | <span class="t">So that's the simple gradient descent algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=515" target="_blank">00:08:35.360</a></span> | <span class="t">Nobody uses it, and you shouldn't use it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=519" target="_blank">00:08:39.100</a></span> | <span class="t">The problem is that our J is a function of all windows in the corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=525" target="_blank">00:08:45.300</a></span> | <span class="t">Remember, we're doing this sum over every center word in the entire corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=531" target="_blank">00:08:51.760</a></span> | <span class="t">And we'll often have billions of words in the corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=534" target="_blank">00:08:54.600</a></span> | <span class="t">So actually working out J of theta or the gradient of J of theta would be extremely,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=541" target="_blank">00:09:01.120</a></span> | <span class="t">extremely expensive because we have to iterate over our entire corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=544" target="_blank">00:09:04.640</a></span> | <span class="t">So you'd wait a very long time before you made a single gradient update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=549" target="_blank">00:09:09.120</a></span> | <span class="t">And so optimization would be extremely slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=552" target="_blank">00:09:12.020</a></span> | <span class="t">And so basically 100% of the time in neural network land, we don't use gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=559" target="_blank">00:09:19.300</a></span> | <span class="t">We instead use what's called stochastic gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=562" target="_blank">00:09:22.680</a></span> | <span class="t">And stochastic gradient descent is a very simple modification of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=567" target="_blank">00:09:27.400</a></span> | <span class="t">So rather than working out an estimate of the gradient based on the entire corpus, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=574" target="_blank">00:09:34.680</a></span> | <span class="t">simply take one center word or a small batch like 32 center words, and you work out an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=581" target="_blank">00:09:41.440</a></span> | <span class="t">estimate of the gradient based on them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=584" target="_blank">00:09:44.780</a></span> | <span class="t">Now that estimate of the gradient will be noisy and bad because you've only looked at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=590" target="_blank">00:09:50.720</a></span> | <span class="t">a small fraction of the corpus rather than the whole corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=594" target="_blank">00:09:54.160</a></span> | <span class="t">But nevertheless, you can use that estimate of the gradient to update your theta parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=599" target="_blank">00:09:59.720</a></span> | <span class="t">in exactly the same way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=601" target="_blank">00:10:01.880</a></span> | <span class="t">And so this is the algorithm that we can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=604" target="_blank">00:10:04.580</a></span> | <span class="t">And so then if we have a billion word corpus, we can if we do it on each center word, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=612" target="_blank">00:10:12.440</a></span> | <span class="t">can make a billion updates to the parameters as we pass through the corpus once rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=617" target="_blank">00:10:17.560</a></span> | <span class="t">than only making one more accurate update to the parameters at once you've been through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=624" target="_blank">00:10:24.120</a></span> | <span class="t">the corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=625" target="_blank">00:10:25.280</a></span> | <span class="t">So overall, we can learn several orders of magnitude more quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=631" target="_blank">00:10:31.040</a></span> | <span class="t">And so this is the algorithm that you'll be using everywhere, including, you know, right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=637" target="_blank">00:10:37.560</a></span> | <span class="t">from the beginning from our assignments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=641" target="_blank">00:10:41.200</a></span> | <span class="t">Again, just an extra comment of more complicated stuff we'll come back to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=647" target="_blank">00:10:47.400</a></span> | <span class="t">I I pretend this is the gradient descent is sort of performance hack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=656" target="_blank">00:10:56.480</a></span> | <span class="t">It lets you learn much more quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=658" target="_blank">00:10:58.440</a></span> | <span class="t">It turns out it's not only a performance hack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=661" target="_blank">00:11:01.800</a></span> | <span class="t">Neural nets have some quite counterintuitive properties.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=665" target="_blank">00:11:05.880</a></span> | <span class="t">And actually, the fact that stochastic gradient descent is kind of noisy and bounces around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=672" target="_blank">00:11:12.120</a></span> | <span class="t">as it does its thing, it actually means that in complex networks, it learns better solutions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=679" target="_blank">00:11:19.880</a></span> | <span class="t">than if you were to run plain gradient descent very slowly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=684" target="_blank">00:11:24.560</a></span> | <span class="t">So you can both compute much more quickly and do a better job.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=689" target="_blank">00:11:29.240</a></span> | <span class="t">OK, one final note on running stochastic gradients with word vectors, this is kind of an aside,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=697" target="_blank">00:11:37.180</a></span> | <span class="t">but something to note is that if we're doing a stochastic gradient update based on one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=702" target="_blank">00:11:42.360</a></span> | <span class="t">window, then actually in that window, we'll have seen almost none of our parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=709" target="_blank">00:11:49.120</a></span> | <span class="t">Because if we have a window of something like five words to either side of the center word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=714" target="_blank">00:11:54.440</a></span> | <span class="t">we've seen at most 11 distinct word types.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=718" target="_blank">00:11:58.560</a></span> | <span class="t">So we will have gradient information for those 11 words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=722" target="_blank">00:12:02.900</a></span> | <span class="t">But the other 100,000 odd words in our vocabulary will have no gradient update information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=729" target="_blank">00:12:09.440</a></span> | <span class="t">So this will be a very, very sparse gradient update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=733" target="_blank">00:12:13.900</a></span> | <span class="t">So if you're only thinking math, you can just have your entire gradient and use the equation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=743" target="_blank">00:12:23.120</a></span> | <span class="t">that I showed before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=745" target="_blank">00:12:25.040</a></span> | <span class="t">But if you're thinking systems optimization, then you'd want to think, well, actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=751" target="_blank">00:12:31.380</a></span> | <span class="t">I only want to update the parameters for a few words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=756" target="_blank">00:12:36.960</a></span> | <span class="t">And there have to be and there are much more efficient ways that I could do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=763" target="_blank">00:12:43.580</a></span> | <span class="t">And so here's sort of this is another aside will be useful for the assignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=769" target="_blank">00:12:49.080</a></span> | <span class="t">So I will say it up until now, when I presented word vectors, I presented them as column vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=777" target="_blank">00:12:57.860</a></span> | <span class="t">And that makes the most sense if you think about it as a piece of math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=783" target="_blank">00:13:03.840</a></span> | <span class="t">Whereas actually, in all common deep learning packages, including PyTorch that we're using,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=792" target="_blank">00:13:12.120</a></span> | <span class="t">word vectors are actually represented as row vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=796" target="_blank">00:13:16.220</a></span> | <span class="t">And if you remember back to the representation of matrices and CS107 or something like that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=803" target="_blank">00:13:23.140</a></span> | <span class="t">you'll know that that's then obviously efficient for representing words, because then you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=809" target="_blank">00:13:29.800</a></span> | <span class="t">access an entire word vector as a continuous range of memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=815" target="_blank">00:13:35.080</a></span> | <span class="t">Different if you're in Fortran.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=816" target="_blank">00:13:36.080</a></span> | <span class="t">But anyway, so actually, our word vectors will be row vectors when you look at those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=823" target="_blank">00:13:43.500</a></span> | <span class="t">inside PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=825" target="_blank">00:13:45.500</a></span> | <span class="t">OK, now I wanted to say a bit more about the word to vector algorithm family and also what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=835" target="_blank">00:13:55.460</a></span> | <span class="t">you're going to do in homework two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=838" target="_blank">00:13:58.800</a></span> | <span class="t">So if you're still meant to be working on homework one, which remember is due next Tuesday,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=844" target="_blank">00:14:04.060</a></span> | <span class="t">but really actually with today's content, we're starting into homework two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=848" target="_blank">00:14:08.900</a></span> | <span class="t">And I'll kind of go through the first part of homework two today and the other stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=853" target="_blank">00:14:13.560</a></span> | <span class="t">you need to know for homework two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=855" target="_blank">00:14:15.880</a></span> | <span class="t">So I mentioned briefly the idea that we have two separate vectors for each word type, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=861" target="_blank">00:14:21.940</a></span> | <span class="t">center vector and the outside vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=865" target="_blank">00:14:25.300</a></span> | <span class="t">And we just average them both at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=867" target="_blank">00:14:27.220</a></span> | <span class="t">They're similar, but not identical for multiple reasons, including the random initialization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=872" target="_blank">00:14:32.580</a></span> | <span class="t">and the stochastic gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=876" target="_blank">00:14:36.040</a></span> | <span class="t">You can implement a word to vector algorithm with just one vector per word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=882" target="_blank">00:14:42.540</a></span> | <span class="t">And actually, if you do, it works slightly better, but it makes the algorithm much more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=888" target="_blank">00:14:48.340</a></span> | <span class="t">complicated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=889" target="_blank">00:14:49.500</a></span> | <span class="t">And the reason for that is sometimes you will have the same word type as the center word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=896" target="_blank">00:14:56.780</a></span> | <span class="t">and the context word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=898" target="_blank">00:14:58.740</a></span> | <span class="t">And that means that when you're doing your calculus at that point, you've then got this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=904" target="_blank">00:15:04.060</a></span> | <span class="t">sort of messy case that just for that word, you're getting an x squared, sorry, dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=911" target="_blank">00:15:11.260</a></span> | <span class="t">You're getting a dot product of x dot x term, which makes it sort of much messier to work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=915" target="_blank">00:15:15.620</a></span> | <span class="t">out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=916" target="_blank">00:15:16.620</a></span> | <span class="t">And so that's why we use this sort of simple optimization of having two vectors per word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=921" target="_blank">00:15:21.700</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=922" target="_blank">00:15:22.700</a></span> | <span class="t">So for the word to vector model as introduced in the Mikhailov et al paper in 2013, it wasn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=932" target="_blank">00:15:32.500</a></span> | <span class="t">really just one algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=936" target="_blank">00:15:36.160</a></span> | <span class="t">It was a family of algorithms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=938" target="_blank">00:15:38.480</a></span> | <span class="t">So there were two basic model variants.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=942" target="_blank">00:15:42.000</a></span> | <span class="t">One was called the skip gram model, which is the one that I've explained to you that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=946" target="_blank">00:15:46.540</a></span> | <span class="t">pretty much outside words, position independent, given the center word in a bag of words style</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=956" target="_blank">00:15:56.900</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=957" target="_blank">00:15:57.900</a></span> | <span class="t">The other one was called the continuous bag of words model SIBO.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=961" target="_blank">00:16:01.340</a></span> | <span class="t">And in this one, you predict the center word for a bag of context words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=967" target="_blank">00:16:07.580</a></span> | <span class="t">Both of these give similar results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=969" target="_blank">00:16:09.780</a></span> | <span class="t">The skip gram one is more natural in various ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=973" target="_blank">00:16:13.300</a></span> | <span class="t">So it's sort of normally the one that people have gravitated to and subsequent work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=979" target="_blank">00:16:19.620</a></span> | <span class="t">But then as to how you train this model, what I've presented so far is the naive softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=986" target="_blank">00:16:26.900</a></span> | <span class="t">equation, which is a simple but relatively expensive training method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=993" target="_blank">00:16:33.980</a></span> | <span class="t">And so that isn't really what they suggest using in your paper in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=999" target="_blank">00:16:39.120</a></span> | <span class="t">They suggest using a method that's called negative sampling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1002" target="_blank">00:16:42.240</a></span> | <span class="t">So an acronym you'll see sometimes is SGNS, which means skip grams, negative sampling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1009" target="_blank">00:16:49.200</a></span> | <span class="t">So let me just say a little bit about what this is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1013" target="_blank">00:16:53.900</a></span> | <span class="t">But actually doing the skip gram model with negative sampling is the part of homework</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1019" target="_blank">00:16:59.340</a></span> | <span class="t">too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1020" target="_blank">00:17:00.340</a></span> | <span class="t">So you'll get to know this model well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1021" target="_blank">00:17:01.740</a></span> | <span class="t">So the point is that if you use this naive softmax, you know, even though people commonly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1026" target="_blank">00:17:06.900</a></span> | <span class="t">do use this naive softmax in various neural net models, that working out the denominator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1033" target="_blank">00:17:13.660</a></span> | <span class="t">is pretty expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1035" target="_blank">00:17:15.460</a></span> | <span class="t">And that's because you have to iterate over every word in the vocabulary and work out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1041" target="_blank">00:17:21.820</a></span> | <span class="t">these dot products.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1042" target="_blank">00:17:22.940</a></span> | <span class="t">So if you have a hundred thousand word vocabulary, you have to do a hundred thousand dot products</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1049" target="_blank">00:17:29.980</a></span> | <span class="t">to work out the denominator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1052" target="_blank">00:17:32.300</a></span> | <span class="t">And that seems a little bit of a shame.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1054" target="_blank">00:17:34.860</a></span> | <span class="t">And so instead of that, the idea of negative sampling is where instead of using this softmax,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1062" target="_blank">00:17:42.780</a></span> | <span class="t">we're going to train binary logistic regression models for both the troop, the true pair of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1072" target="_blank">00:17:52.020</a></span> | <span class="t">center word and the context word versus noise pairs where we keep the true center word and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1081" target="_blank">00:18:01.820</a></span> | <span class="t">we just randomly sample words from the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1086" target="_blank">00:18:06.780</a></span> | <span class="t">So as presented in the paper, the idea is like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1090" target="_blank">00:18:10.820</a></span> | <span class="t">So overall, what we want to optimize is still an average of the loss for each particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1099" target="_blank">00:18:19.780</a></span> | <span class="t">center word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1101" target="_blank">00:18:21.340</a></span> | <span class="t">But for when we're working out the loss for each particular center word, we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1105" target="_blank">00:18:25.980</a></span> | <span class="t">work out, sorry, the loss for each particular center word and each particular window.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1111" target="_blank">00:18:31.740</a></span> | <span class="t">We're going to take the dot product as before of the center word and the outside word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1119" target="_blank">00:18:39.980</a></span> | <span class="t">And that's sort of the main quantity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1122" target="_blank">00:18:42.220</a></span> | <span class="t">But now instead of using that inside the softmax, we're going to put it through the logistic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1127" target="_blank">00:18:47.780</a></span> | <span class="t">function, which is sometimes also often also called the sigmoid function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1132" target="_blank">00:18:52.380</a></span> | <span class="t">The name logistic is more precise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1134" target="_blank">00:18:54.040</a></span> | <span class="t">So that's this function here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1135" target="_blank">00:18:55.660</a></span> | <span class="t">So the logistic function is a handy function that will map any real number to a probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1142" target="_blank">00:19:02.500</a></span> | <span class="t">between zero and one open interval.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1145" target="_blank">00:19:05.340</a></span> | <span class="t">So basically, if the dot product is large, the logistic of the dot product will be virtually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1152" target="_blank">00:19:12.460</a></span> | <span class="t">one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1153" target="_blank">00:19:13.460</a></span> | <span class="t">Okay, so we want this to be large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1156" target="_blank">00:19:16.660</a></span> | <span class="t">And then what we'd like is on average, we'd like the dot product between the center word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1162" target="_blank">00:19:22.900</a></span> | <span class="t">and words that we just chose randomly, i.e. they most likely didn't actually occur in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1169" target="_blank">00:19:29.060</a></span> | <span class="t">the context of the center word to be small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1173" target="_blank">00:19:33.100</a></span> | <span class="t">And there's just one little trick of how this is done, which is this sigmoid function is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1180" target="_blank">00:19:40.580</a></span> | <span class="t">symmetric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1181" target="_blank">00:19:41.780</a></span> | <span class="t">And so if we want this probability to be small, we can take the negative of the dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1191" target="_blank">00:19:51.460</a></span> | <span class="t">So we're wanting it to be over here, that the product, the dot product of random word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1197" target="_blank">00:19:57.420</a></span> | <span class="t">in the center word is a negative number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1201" target="_blank">00:20:01.020</a></span> | <span class="t">And so then we're going to take the negation of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1204" target="_blank">00:20:04.500</a></span> | <span class="t">And then again, once we put that through the sigmoid, we'd like a big number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1208" target="_blank">00:20:08.860</a></span> | <span class="t">Okay, so the way they're presenting things, they're actually maximizing this quantity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1214" target="_blank">00:20:14.060</a></span> | <span class="t">But if I go back to making it a bit more similar to the way we had written things, we'd worked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1221" target="_blank">00:20:21.060</a></span> | <span class="t">with minimizing the negative log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1225" target="_blank">00:20:25.700</a></span> | <span class="t">So it looks like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1228" target="_blank">00:20:28.420</a></span> | <span class="t">So we're taking the negative log likelihood of this, the sigmoid of the dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1235" target="_blank">00:20:35.340</a></span> | <span class="t">Again, negative log likelihood, we're using the same negated dot product through the sigmoid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1243" target="_blank">00:20:43.020</a></span> | <span class="t">And then we're going to work out this quantity for a handful of random word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1250" target="_blank">00:20:50.660</a></span> | <span class="t">We take negative samples, and how likely they are to sample a word depends on their probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1257" target="_blank">00:20:57.740</a></span> | <span class="t">And where this loss function is going to be minimized, given this negation by making these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1265" target="_blank">00:21:05.060</a></span> | <span class="t">dot products large, and these dot products, small means negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1274" target="_blank">00:21:14.260</a></span> | <span class="t">So there was just then one other trick that they use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1279" target="_blank">00:21:19.980</a></span> | <span class="t">Actually there's more than one other trick that's used in the Word2Vec paper to get it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1283" target="_blank">00:21:23.620</a></span> | <span class="t">to perform well, but I'll only mention one of their other tricks here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1288" target="_blank">00:21:28.300</a></span> | <span class="t">When they sample the words, they don't simply just sample the words based on their probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1296" target="_blank">00:21:36.300</a></span> | <span class="t">of occurrence in the corpus or uniformly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1299" target="_blank">00:21:39.740</a></span> | <span class="t">What they do is they start with what we call the unigram distribution of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1304" target="_blank">00:21:44.140</a></span> | <span class="t">So that is how often words actually occur in our big corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1309" target="_blank">00:21:49.660</a></span> | <span class="t">So if you have a billion word corpus and a particular word occurred 90 times in it, you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1315" target="_blank">00:21:55.820</a></span> | <span class="t">taking 90 divided by a billion, and so that's the unigram probability of the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1321" target="_blank">00:22:01.340</a></span> | <span class="t">But what they then do is that they take that to the three quarters power.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1326" target="_blank">00:22:06.340</a></span> | <span class="t">And the effect of that three quarters power, which is then renormalized to make a probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1330" target="_blank">00:22:10.620</a></span> | <span class="t">distribution with Z, kind of like we saw last time with the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1335" target="_blank">00:22:15.260</a></span> | <span class="t">By taking the three quarters power, that has the effect of dampening the difference between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1341" target="_blank">00:22:21.780</a></span> | <span class="t">common and rare words so that less frequent words are sampled somewhat more often, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1348" target="_blank">00:22:28.260</a></span> | <span class="t">still not nearly as much as they would be if you just use something like a uniform distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1354" target="_blank">00:22:34.260</a></span> | <span class="t">over the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1356" target="_blank">00:22:36.300</a></span> | <span class="t">Okay, so that's basically everything to say about the basics of how we have this very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1367" target="_blank">00:22:47.660</a></span> | <span class="t">simple neural network algorithm, Word2Vec, and how we can train it and learn word vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1376" target="_blank">00:22:56.700</a></span> | <span class="t">So for the next bit, what I want to do is step back a bit and say, well, here's an algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1382" target="_blank">00:23:02.180</a></span> | <span class="t">that I've shown you that works great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1386" target="_blank">00:23:06.260</a></span> | <span class="t">What else could we have done and what can we say about that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1390" target="_blank">00:23:10.740</a></span> | <span class="t">The first thing that you might think about is, well, here's this funny iterative algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1398" target="_blank">00:23:18.340</a></span> | <span class="t">to give you word vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1402" target="_blank">00:23:22.580</a></span> | <span class="t">You know, if we have a lot of words in a corpus, it seems like a more obvious thing that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1408" target="_blank">00:23:28.540</a></span> | <span class="t">could do is just look at the counts of how words occur with each other and build a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1417" target="_blank">00:23:37.100</a></span> | <span class="t">of counts, a co-occurrence matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1420" target="_blank">00:23:40.780</a></span> | <span class="t">So here's the idea of a co-occurrence matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1424" target="_blank">00:23:44.340</a></span> | <span class="t">So I've got a teeny little corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1426" target="_blank">00:23:46.180</a></span> | <span class="t">I like deep learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1427" target="_blank">00:23:47.420</a></span> | <span class="t">I like NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1428" target="_blank">00:23:48.540</a></span> | <span class="t">I enjoy flying.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1430" target="_blank">00:23:50.820</a></span> | <span class="t">And I can define a window size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1432" target="_blank">00:23:52.820</a></span> | <span class="t">I made my window simply size one to make it easy to fill in my matrix, symmetric just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1440" target="_blank">00:24:00.380</a></span> | <span class="t">like our Word2Vec algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1442" target="_blank">00:24:02.940</a></span> | <span class="t">And so then the counts in these cells are simply how often things co-occur in the window</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1451" target="_blank">00:24:11.300</a></span> | <span class="t">of size one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1452" target="_blank">00:24:12.800</a></span> | <span class="t">So I like occurs twice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1456" target="_blank">00:24:16.100</a></span> | <span class="t">So we get twos in these cells because it's symmetric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1459" target="_blank">00:24:19.980</a></span> | <span class="t">Deep learning occurs one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1461" target="_blank">00:24:21.940</a></span> | <span class="t">So we get one here and lots of other things occur zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1466" target="_blank">00:24:26.260</a></span> | <span class="t">So we can build up a co-occurrence matrix like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1471" target="_blank">00:24:31.460</a></span> | <span class="t">And well, these actually give us a representation of words as co-occurrence vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1478" target="_blank">00:24:38.700</a></span> | <span class="t">So I can take the word I with either a row or a column vector since it's symmetric and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1484" target="_blank">00:24:44.060</a></span> | <span class="t">say okay, my representation of the word I is this row vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1490" target="_blank">00:24:50.840</a></span> | <span class="t">And that is a representation of the word I.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1493" target="_blank">00:24:53.780</a></span> | <span class="t">And I think you can maybe convince yourself that to the extent that words have similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1499" target="_blank">00:24:59.860</a></span> | <span class="t">meaning and usage, you'd sort of expect them to have somewhat similar vectors, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1505" target="_blank">00:25:05.980</a></span> | <span class="t">So if I had the word you as well on a larger corpus, you might expect I and you to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1511" target="_blank">00:25:11.220</a></span> | <span class="t">similar vectors because I like you like I enjoy you enjoy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1515" target="_blank">00:25:15.820</a></span> | <span class="t">You'd see the same kinds of possibilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1518" target="_blank">00:25:18.140</a></span> | <span class="t">Hey, Chris, can we keep looking to answer some questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1521" target="_blank">00:25:21.820</a></span> | <span class="t">Sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1522" target="_blank">00:25:22.820</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1523" target="_blank">00:25:23.820</a></span> | <span class="t">So we got some questions from negative, sort of the negative sampling slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1529" target="_blank">00:25:29.840</a></span> | <span class="t">In particular, what's like, can you give some intuition for negative sampling?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1534" target="_blank">00:25:34.940</a></span> | <span class="t">What is the negative sampling doing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1536" target="_blank">00:25:36.820</a></span> | <span class="t">And why do we only take one positive example?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1539" target="_blank">00:25:39.820</a></span> | <span class="t">Those are two questions to be answered in tandem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1543" target="_blank">00:25:43.060</a></span> | <span class="t">Okay, that's a good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1544" target="_blank">00:25:44.660</a></span> | <span class="t">Okay, I'll try and give more intuition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1546" target="_blank">00:25:46.900</a></span> | <span class="t">So is to work out something like what the softmax did in a much more efficient way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1559" target="_blank">00:25:59.300</a></span> | <span class="t">So, in the softmax, well, you wanted to give high probability to the in predicting the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1568" target="_blank">00:26:08.860</a></span> | <span class="t">context, a context word that actually did appear with the center word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1574" target="_blank">00:26:14.500</a></span> | <span class="t">And well, the way you do that is by having the dot product between those two words be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1580" target="_blank">00:26:20.300</a></span> | <span class="t">as big as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1582" target="_blank">00:26:22.340</a></span> | <span class="t">And part of how but you know, you're going to be sort of it's more than that, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1588" target="_blank">00:26:28.380</a></span> | <span class="t">in the denominator, you're also working out the dot product with every other word in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1593" target="_blank">00:26:33.220</a></span> | <span class="t">vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1594" target="_blank">00:26:34.480</a></span> | <span class="t">So as well as wanting the dot product with the actual word that you see in the context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1599" target="_blank">00:26:39.380</a></span> | <span class="t">to be big, you maximize your likelihood by making the dot products of other words that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1607" target="_blank">00:26:47.600</a></span> | <span class="t">weren't in the context smaller, because that's shrinking your denominator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1612" target="_blank">00:26:52.740</a></span> | <span class="t">And therefore, you've got a bigger number coming out and you're maximizing the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1618" target="_blank">00:26:58.920</a></span> | <span class="t">So even for the softmax, the general thing that you want to do to maximize it is have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1624" target="_blank">00:27:04.540</a></span> | <span class="t">dot product with words actually in the context big dot product with words and not in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1630" target="_blank">00:27:10.020</a></span> | <span class="t">context be small to the extent possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1634" target="_blank">00:27:14.100</a></span> | <span class="t">And obviously, you have to average this as best you can over all kinds of different contexts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1638" target="_blank">00:27:18.620</a></span> | <span class="t">because sometimes different words appear in different contexts, obviously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1645" target="_blank">00:27:25.700</a></span> | <span class="t">So the negative sampling as a way of therefore trying to maximize the same objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1652" target="_blank">00:27:32.940</a></span> | <span class="t">Now, you know, for you only you only have one positive term because you're actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1660" target="_blank">00:27:40.140</a></span> | <span class="t">wanting to use the actual data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1662" target="_blank">00:27:42.580</a></span> | <span class="t">So you're not wanting to invent data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1665" target="_blank">00:27:45.280</a></span> | <span class="t">So for working out the entire J, we do do work this quantity out for every center word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1672" target="_blank">00:27:52.340</a></span> | <span class="t">and every context word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1674" target="_blank">00:27:54.500</a></span> | <span class="t">So you know, we are iterating over the different words in the context window, and then we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1679" target="_blank">00:27:59.460</a></span> | <span class="t">moving through positions in the corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1681" target="_blank">00:28:01.660</a></span> | <span class="t">So we're doing different VC.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1683" target="_blank">00:28:03.220</a></span> | <span class="t">So, you know, gradually we do this, but for one particular center word and one particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1688" target="_blank">00:28:08.200</a></span> | <span class="t">context word, we only have one real piece of data that's positive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1692" target="_blank">00:28:12.820</a></span> | <span class="t">So that's all we use because we don't know what other words should be counted as positive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1699" target="_blank">00:28:19.540</a></span> | <span class="t">words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1700" target="_blank">00:28:20.540</a></span> | <span class="t">Now for the negative words, you could just sample one negative word and that would probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1708" target="_blank">00:28:28.900</a></span> | <span class="t">work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1709" target="_blank">00:28:29.900</a></span> | <span class="t">But if you want to sort of a slightly better, more stable sense of, OK, we'd like to, in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1716" target="_blank">00:28:36.860</a></span> | <span class="t">general, have other words have low probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1719" target="_blank">00:28:39.840</a></span> | <span class="t">It seems like you might be able to get better, more stable results if you instead say, let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1724" target="_blank">00:28:44.800</a></span> | <span class="t">have 10 or 15 sample negative words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1728" target="_blank">00:28:48.440</a></span> | <span class="t">And indeed that's been found to be true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1732" target="_blank">00:28:52.200</a></span> | <span class="t">And for the negative words, well, it's easy to sample any number of random words you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1737" target="_blank">00:28:57.160</a></span> | <span class="t">And at that point, it's kind of a probabilistic argument.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1740" target="_blank">00:29:00.040</a></span> | <span class="t">The words that you're sampling might not be actually bad words to appear in the context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1746" target="_blank">00:29:06.440</a></span> | <span class="t">They might actually be other words that are in the context, but 99.9% of the time, they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1751" target="_blank">00:29:11.840</a></span> | <span class="t">will be unlikely words to occur in the context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1755" target="_blank">00:29:15.520</a></span> | <span class="t">And so they're good ones to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1757" target="_blank">00:29:17.880</a></span> | <span class="t">And yes, you only sample 10 or 15 of them, but that's enough to make progress because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1765" target="_blank">00:29:25.160</a></span> | <span class="t">the center word is going to turn up on other occasions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1768" target="_blank">00:29:28.920</a></span> | <span class="t">And when it does, you'll sample different words over here so that you gradually sample</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1773" target="_blank">00:29:33.240</a></span> | <span class="t">different parts of the space and start to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1776" target="_blank">00:29:36.260</a></span> | <span class="t">We had this co-occurrence matrix and it gives a representation of words as co-occurrence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1784" target="_blank">00:29:44.400</a></span> | <span class="t">vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1786" target="_blank">00:29:46.600</a></span> | <span class="t">And just one more note on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1789" target="_blank">00:29:49.280</a></span> | <span class="t">I mean, there are actually two ways that people have commonly made these co-occurrence matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1794" target="_blank">00:29:54.720</a></span> | <span class="t">One corresponds to what we've seen already, that you use a window around a word, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1800" target="_blank">00:30:00.160</a></span> | <span class="t">is similar to word2vec.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1802" target="_blank">00:30:02.800</a></span> | <span class="t">And that allows you to capture some locality and some of the sort of syntactic and semantic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1807" target="_blank">00:30:07.640</a></span> | <span class="t">proximity that's more fine grained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1810" target="_blank">00:30:10.720</a></span> | <span class="t">The other way these co-occurrence matrices have often been made is that normally documents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1818" target="_blank">00:30:18.620</a></span> | <span class="t">have some structure, whether it's paragraphs or just actual web pages, sort of size documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1825" target="_blank">00:30:25.120</a></span> | <span class="t">So you can just make your window size a paragraph or a whole web page and count co-occurrence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1831" target="_blank">00:30:31.180</a></span> | <span class="t">in those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1832" target="_blank">00:30:32.180</a></span> | <span class="t">And this is the kind of method that's often being used in information retrieval in methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1836" target="_blank">00:30:36.960</a></span> | <span class="t">like latent semantic analysis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1839" target="_blank">00:30:39.440</a></span> | <span class="t">OK, so the question then is, are these kind of count word vectors good things to use?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1850" target="_blank">00:30:50.960</a></span> | <span class="t">Well, people have used them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1853" target="_blank">00:30:53.520</a></span> | <span class="t">They're not terrible, but they have certain problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1857" target="_blank">00:30:57.280</a></span> | <span class="t">But the kind of problems that they have, well, firstly, they're huge, though very sparse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1863" target="_blank">00:31:03.500</a></span> | <span class="t">So this is back where I said before, if we had a vocabulary of half a million words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1868" target="_blank">00:31:08.360</a></span> | <span class="t">when then we have a half a million dimensional vector for each word, which is much, much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1874" target="_blank">00:31:14.600</a></span> | <span class="t">bigger than the word vectors that we typically use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1880" target="_blank">00:31:20.100</a></span> | <span class="t">And it also means that because we have these very high dimensional vectors, that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1887" target="_blank">00:31:27.220</a></span> | <span class="t">a lot of sparsity and a lot of randomness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1890" target="_blank">00:31:30.620</a></span> | <span class="t">So the results that you get tend to be noisier and less robust, depending on what particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1896" target="_blank">00:31:36.440</a></span> | <span class="t">stuff was in the corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1898" target="_blank">00:31:38.860</a></span> | <span class="t">And so in general, people have found that you can get much better results by working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1904" target="_blank">00:31:44.280</a></span> | <span class="t">with low dimensional vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1906" target="_blank">00:31:46.520</a></span> | <span class="t">So then the idea is we can store the most of the important information about the distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1913" target="_blank">00:31:53.420</a></span> | <span class="t">of words in the context of other words in a fixed small number of dimensions, giving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1919" target="_blank">00:31:59.400</a></span> | <span class="t">a dense vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1921" target="_blank">00:32:01.180</a></span> | <span class="t">And in practice, the dimensionality of the vectors that are used are normally somewhere</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1925" target="_blank">00:32:05.800</a></span> | <span class="t">between 25 and 1000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1928" target="_blank">00:32:08.420</a></span> | <span class="t">And so at that point, we need to use some way to reduce the dimensionality of our count</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1936" target="_blank">00:32:16.040</a></span> | <span class="t">co-occurrence vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1939" target="_blank">00:32:19.880</a></span> | <span class="t">So if you have a good memory from a linear algebra class, you hopefully saw singular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1948" target="_blank">00:32:28.180</a></span> | <span class="t">value decomposition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1950" target="_blank">00:32:30.380</a></span> | <span class="t">And it has various mathematical properties that I'm not going to talk about here of single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1957" target="_blank">00:32:37.720</a></span> | <span class="t">singular value projection, giving you an optimal way under a certain definition of optimality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1963" target="_blank">00:32:43.840</a></span> | <span class="t">of producing a reduced dimensionality matrix that maximally or sorry, pair of matrices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1972" target="_blank">00:32:52.460</a></span> | <span class="t">that maximally well lets you recover the original matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1976" target="_blank">00:32:56.680</a></span> | <span class="t">But the idea of the singular value decomposition is you can take any matrix such as our count</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1983" target="_blank">00:33:03.120</a></span> | <span class="t">matrix and you can decompose that into three matrices, U, a diagonal matrix, sigma and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1993" target="_blank">00:33:13.840</a></span> | <span class="t">a V transpose matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=1997" target="_blank">00:33:17.920</a></span> | <span class="t">And this works for any shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2000" target="_blank">00:33:20.160</a></span> | <span class="t">Now, in these matrices, some parts of it are never used because since this matrix is rectangular,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2008" target="_blank">00:33:28.680</a></span> | <span class="t">there's nothing over here, and so this part of the V transpose matrix gets ignored.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2015" target="_blank">00:33:35.520</a></span> | <span class="t">But if you're wanting to get smaller dimensional representations, what you do is take advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2022" target="_blank">00:33:42.840</a></span> | <span class="t">of the fact that the singular values inside the diagonal sigma matrix are ordered from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2030" target="_blank">00:33:50.320</a></span> | <span class="t">largest down to smallest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2032" target="_blank">00:33:52.880</a></span> | <span class="t">So what we can do is just delete out more of the matrix of the delete out some singular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2040" target="_blank">00:34:00.520</a></span> | <span class="t">values, which effectively means that in this product, some of U and some of V is also not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2048" target="_blank">00:34:08.480</a></span> | <span class="t">used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2049" target="_blank">00:34:09.680</a></span> | <span class="t">And so then as a result of that, we're getting lower dimensional representations for our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2057" target="_blank">00:34:17.480</a></span> | <span class="t">words if we're wanting to have word vectors, which still do as good as possible a job within</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2064" target="_blank">00:34:24.360</a></span> | <span class="t">the given dimensionality of enabling you to recover the original co-occurrence matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2074" target="_blank">00:34:34.080</a></span> | <span class="t">So from a linear algebra background, this is the obvious thing to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2080" target="_blank">00:34:40.420</a></span> | <span class="t">So how does that work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2082" target="_blank">00:34:42.720</a></span> | <span class="t">Well, if you just build a raw count co-occurrence matrix and run SVD on that and try and use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2092" target="_blank">00:34:52.120</a></span> | <span class="t">those as word vectors, it actually works poorly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2096" target="_blank">00:34:56.620</a></span> | <span class="t">And it works poorly because if you get into the mathematical assumptions of SVD, you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2102" target="_blank">00:35:02.600</a></span> | <span class="t">expecting to have these normally distributed errors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2107" target="_blank">00:35:07.680</a></span> | <span class="t">And what you're getting with word counts looked not at all like something's normally distributed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2116" target="_blank">00:35:16.840</a></span> | <span class="t">because you have exceedingly common words like "a", "the", and "and", and you have a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2122" target="_blank">00:35:22.180</a></span> | <span class="t">large number of rare words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2124" target="_blank">00:35:24.260</a></span> | <span class="t">So that doesn't work very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2125" target="_blank">00:35:25.920</a></span> | <span class="t">But you can actually get something that works a lot better if you scale the counts in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2131" target="_blank">00:35:31.640</a></span> | <span class="t">cells.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2132" target="_blank">00:35:32.680</a></span> | <span class="t">So to deal with this problem of extremely frequent words, there are some things we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2137" target="_blank">00:35:37.400</a></span> | <span class="t">do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2138" target="_blank">00:35:38.400</a></span> | <span class="t">We could just take the log of the raw counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2141" target="_blank">00:35:41.420</a></span> | <span class="t">We could kind of cap the maximum count.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2144" target="_blank">00:35:44.720</a></span> | <span class="t">We could throw away the function words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2147" target="_blank">00:35:47.280</a></span> | <span class="t">And any of these kind of ideas let you build, then have a co-occurrence matrix that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2153" target="_blank">00:35:53.480</a></span> | <span class="t">get more useful word vectors from running something like SVD.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2158" target="_blank">00:35:58.680</a></span> | <span class="t">And indeed, these kind of models were explored in the 1990s and in the 2000s.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2165" target="_blank">00:36:05.840</a></span> | <span class="t">And in particular, Doug Rohde explored a number of these ideas as to how to improve the co-occurrence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2173" target="_blank">00:36:13.320</a></span> | <span class="t">matrix in a model that he built that was called COLS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2178" target="_blank">00:36:18.200</a></span> | <span class="t">And actually, in his COLS model, he observed the fact that you could get the same kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2187" target="_blank">00:36:27.800</a></span> | <span class="t">of linear components that have semantic components that we saw yesterday when talking about analogies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2197" target="_blank">00:36:37.400</a></span> | <span class="t">So for example, this is a figure from his paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2201" target="_blank">00:36:41.800</a></span> | <span class="t">And you can see that we seem to have a meaning component going from a verb to the person</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2208" target="_blank">00:36:48.840</a></span> | <span class="t">who does the verb.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2210" target="_blank">00:36:50.200</a></span> | <span class="t">So drive to drive, swim to swim, teach to teach, marry to priest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2215" target="_blank">00:36:55.480</a></span> | <span class="t">And that these vector components are not perfectly, but are roughly parallel and roughly the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2223" target="_blank">00:37:03.800</a></span> | <span class="t">size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2224" target="_blank">00:37:04.840</a></span> | <span class="t">And so we have a meaning component there that we could add on to another word, just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2230" target="_blank">00:37:10.640</a></span> | <span class="t">we did previously for analogies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2233" target="_blank">00:37:13.120</a></span> | <span class="t">We could say drive is to driver as marry is to what.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2238" target="_blank">00:37:18.080</a></span> | <span class="t">And we'd add on this green vector component, which is roughly the same as this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2243" target="_blank">00:37:23.240</a></span> | <span class="t">And we'd say, oh, priest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2244" target="_blank">00:37:24.920</a></span> | <span class="t">So this space could actually get some word vectors analogies right as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2253" target="_blank">00:37:33.160</a></span> | <span class="t">And so that seemed really interesting to us around the time Word2Vec came out of wanting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2260" target="_blank">00:37:40.160</a></span> | <span class="t">to understand better what the iterative updating algorithm of Word2Vec did and how it related</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2266" target="_blank">00:37:46.320</a></span> | <span class="t">to these more linear algebra based methods that had been explored in the couple of decades</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2271" target="_blank">00:37:51.760</a></span> | <span class="t">previously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2273" target="_blank">00:37:53.000</a></span> | <span class="t">And so for the next bit, I want to tell you a little bit about the GloVe algorithm, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2278" target="_blank">00:37:58.040</a></span> | <span class="t">was an algorithm for word vectors that was made by Jeffrey Pennington, Richard Socher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2284" target="_blank">00:38:04.080</a></span> | <span class="t">and me in 2014.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2287" target="_blank">00:38:07.400</a></span> | <span class="t">And so the starting point of this was to try to connect together the linear algebra based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2294" target="_blank">00:38:14.200</a></span> | <span class="t">methods on co-occurrence matrices like LSA and COLS with the models like Skip Graham,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2300" target="_blank">00:38:20.520</a></span> | <span class="t">Zeebo and their other friends, which were iterative neural updating algorithms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2306" target="_blank">00:38:26.400</a></span> | <span class="t">So on the one hand, you know, the linear algebra methods actually seemed like they had advantages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2312" target="_blank">00:38:32.560</a></span> | <span class="t">for fast training and efficient usage of statistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2316" target="_blank">00:38:36.760</a></span> | <span class="t">But although there had been work on capturing word similarities with them, by and large,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2324" target="_blank">00:38:44.120</a></span> | <span class="t">the results weren't as good, perhaps because of disproportionate importance given to large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2328" target="_blank">00:38:48.400</a></span> | <span class="t">counts in the main.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2330" target="_blank">00:38:50.120</a></span> | <span class="t">Conversely, the models, the neural models, it seems like if you're just doing these gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2338" target="_blank">00:38:58.480</a></span> | <span class="t">updates on Windows, you're somehow inefficiently using statistics versus a co-occurrence matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2345" target="_blank">00:39:05.820</a></span> | <span class="t">But on the other hand, it's actually easier to scale to a very large corpus by trading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2352" target="_blank">00:39:12.160</a></span> | <span class="t">time for space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2354" target="_blank">00:39:14.320</a></span> | <span class="t">And at that time, it seemed like the neural methods just worked better for people, that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2360" target="_blank">00:39:20.400</a></span> | <span class="t">they generated improved performance on many tasks, not just on word similarity, and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2365" target="_blank">00:39:25.840</a></span> | <span class="t">they could capture complex patterns such as the analogies that went beyond word similarity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2373" target="_blank">00:39:33.560</a></span> | <span class="t">And so what we wanted to do was understand a bit more as to what properties do you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2379" target="_blank">00:39:39.720</a></span> | <span class="t">to have this analogies work out, as I showed last time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2384" target="_blank">00:39:44.980</a></span> | <span class="t">And so what we realized was that if you'd like to do have these sort of vector subtractions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2394" target="_blank">00:39:54.520</a></span> | <span class="t">and additions work for an analogy, the property that you want is for meaning components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2404" target="_blank">00:40:04.440</a></span> | <span class="t">So a meaning component is something like going from male to female, queen to king, or going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2412" target="_blank">00:40:12.320</a></span> | <span class="t">from a bird to its agent, truck to driver, that those meaning components should be represented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2423" target="_blank">00:40:23.020</a></span> | <span class="t">as ratios of co-occurrence probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2426" target="_blank">00:40:26.160</a></span> | <span class="t">So here's an example that shows that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2429" target="_blank">00:40:29.800</a></span> | <span class="t">So suppose the meaning component that we want to get out is the spectrum from solid to gas,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2438" target="_blank">00:40:38.300</a></span> | <span class="t">as in physics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2439" target="_blank">00:40:39.800</a></span> | <span class="t">Well, you'd think that you can get at the solid part of it, perhaps by saying, does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2446" target="_blank">00:40:46.080</a></span> | <span class="t">the word co-occur with ice?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2448" target="_blank">00:40:48.440</a></span> | <span class="t">And the word solid occurs with ice, so that looks hopeful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2452" target="_blank">00:40:52.200</a></span> | <span class="t">And gas doesn't occur with ice much, so that looks hopeful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2455" target="_blank">00:40:55.760</a></span> | <span class="t">But the problem is the word water will also occur a lot with ice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2460" target="_blank">00:41:00.640</a></span> | <span class="t">And if you just take some other random word, like the word random, it probably doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2464" target="_blank">00:41:04.960</a></span> | <span class="t">occur with ice much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2468" target="_blank">00:41:08.040</a></span> | <span class="t">In contrast, if you look at words co-occurring with steam, solid won't occur with steam much,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2475" target="_blank">00:41:15.920</a></span> | <span class="t">but gas will.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2476" target="_blank">00:41:16.920</a></span> | <span class="t">But water will again, and random will be small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2480" target="_blank">00:41:20.940</a></span> | <span class="t">So to get out the meaning component we want of going from gas to solid, what's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2486" target="_blank">00:41:26.660</a></span> | <span class="t">really useful is to look at the ratio of these co-occurrence probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2492" target="_blank">00:41:32.940</a></span> | <span class="t">Because then we get a spectrum from large to small between solid and gas, whereas for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2499" target="_blank">00:41:39.660</a></span> | <span class="t">water and a random word, it basically cancels out and gives you one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2505" target="_blank">00:41:45.880</a></span> | <span class="t">I just wrote these numbers in, but if you count them up in a large corpus, it is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2512" target="_blank">00:41:52.180</a></span> | <span class="t">what you get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2513" target="_blank">00:41:53.180</a></span> | <span class="t">So here are actual co-occurrence probabilities, and that for water and my random word, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2520" target="_blank">00:42:00.060</a></span> | <span class="t">was fashion here, these are approximately one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2523" target="_blank">00:42:03.820</a></span> | <span class="t">Whereas for the ratio of probability of co-occurrence of solid with ice or steam is about 10, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2533" target="_blank">00:42:13.180</a></span> | <span class="t">for gas it's about a 10th.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2536" target="_blank">00:42:16.580</a></span> | <span class="t">So how can we capture these ratios of co-occurrence probabilities as linear meaning components</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2546" target="_blank">00:42:26.220</a></span> | <span class="t">so that in our word vector space we can just add and subtract linear meaning components?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2552" target="_blank">00:42:32.420</a></span> | <span class="t">Well, it seems like the way we can achieve that is if we build a log bilinear model so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2559" target="_blank">00:42:39.900</a></span> | <span class="t">that the dot product between two word vectors attempts to approximate the log of the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2567" target="_blank">00:42:47.180</a></span> | <span class="t">of co-occurrence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2568" target="_blank">00:42:48.860</a></span> | <span class="t">So if you do that, you then get this property that the difference between two vectors, its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2579" target="_blank">00:42:59.060</a></span> | <span class="t">similarity to another word corresponds to the log of the probability ratio shown on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2585" target="_blank">00:43:05.060</a></span> | <span class="t">the previous slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2587" target="_blank">00:43:07.100</a></span> | <span class="t">So the glove model wanted to try and unify the thinking between the co-occurrence matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2595" target="_blank">00:43:15.940</a></span> | <span class="t">models and the neural models by being in some way similar to a neural model, but actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2603" target="_blank">00:43:23.340</a></span> | <span class="t">calculated on top of a co-occurrence matrix count.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2609" target="_blank">00:43:29.420</a></span> | <span class="t">So we had an explicit loss function, and our explicit loss function is that we wanted the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2616" target="_blank">00:43:36.540</a></span> | <span class="t">dot product to be similar to the log of the co-occurrence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2622" target="_blank">00:43:42.020</a></span> | <span class="t">We actually added in some bias terms here, but I'll ignore those for the moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2626" target="_blank">00:43:46.420</a></span> | <span class="t">And we wanted to not have very common words dominate, and so we capped the effect of high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2633" target="_blank">00:43:53.140</a></span> | <span class="t">word counts using this f function that's shown here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2637" target="_blank">00:43:57.860</a></span> | <span class="t">And then we could optimize this j function directly on the co-occurrence count matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2645" target="_blank">00:44:05.940</a></span> | <span class="t">So that gave us fast training scalable to huge corpora.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2651" target="_blank">00:44:11.340</a></span> | <span class="t">And so this algorithm worked very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2655" target="_blank">00:44:15.260</a></span> | <span class="t">So if you run this algorithm and ask what are the nearest words to frog, you get frogs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2661" target="_blank">00:44:21.540</a></span> | <span class="t">toad, and then you get some complicated words, but it turns out they are all frogs until</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2667" target="_blank">00:44:27.460</a></span> | <span class="t">you get down to lizards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2668" target="_blank">00:44:28.740</a></span> | <span class="t">So Latoria is that lovely tree frog there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2672" target="_blank">00:44:32.340</a></span> | <span class="t">And so this actually seemed to work out pretty well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2676" target="_blank">00:44:36.060</a></span> | <span class="t">How well did it work out?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2677" target="_blank">00:44:37.940</a></span> | <span class="t">To discuss that a bit more, I now want to say something about how do we evaluate word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2683" target="_blank">00:44:43.260</a></span> | <span class="t">vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2684" target="_blank">00:44:44.260</a></span> | <span class="t">Are we good for up to there for questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2688" target="_blank">00:44:48.020</a></span> | <span class="t">We've got some questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2691" target="_blank">00:44:51.940</a></span> | <span class="t">What do you mean by an inefficient use of statistics as a con for skip gram?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2696" target="_blank">00:44:56.260</a></span> | <span class="t">Well, what I mean is that, you know, for word2vec, you're just, you know, looking at one center</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2706" target="_blank">00:45:06.700</a></span> | <span class="t">word at a time and generating a few negative samples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2711" target="_blank">00:45:11.580</a></span> | <span class="t">And so it sort of seems like doing something precise there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2717" target="_blank">00:45:17.380</a></span> | <span class="t">Whereas if you're doing optimization algorithm on the whole matrix at once, well, you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2725" target="_blank">00:45:25.620</a></span> | <span class="t">know everything about the matrix at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2727" target="_blank">00:45:27.660</a></span> | <span class="t">You're not just looking at what words, what other words occurred in this one context of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2734" target="_blank">00:45:34.060</a></span> | <span class="t">the center word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2735" target="_blank">00:45:35.380</a></span> | <span class="t">You've got the entire vector of co-occurrence accounts for the center word and another word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2741" target="_blank">00:45:41.260</a></span> | <span class="t">And so therefore you can much more efficiently and less noisily work out how to minimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2748" target="_blank">00:45:48.200</a></span> | <span class="t">your loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2749" target="_blank">00:45:49.700</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2750" target="_blank">00:45:50.700</a></span> | <span class="t">I'll go on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2753" target="_blank">00:45:53.620</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2754" target="_blank">00:45:54.820</a></span> | <span class="t">So I've sort of said, look at these word vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2758" target="_blank">00:45:58.380</a></span> | <span class="t">They're great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2759" target="_blank">00:45:59.380</a></span> | <span class="t">And I sort of showed you a few things at the end of the last class, which argued, hey,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2764" target="_blank">00:46:04.020</a></span> | <span class="t">these are great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2765" target="_blank">00:46:05.020</a></span> | <span class="t">You know, they work out these analogies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2768" target="_blank">00:46:08.580</a></span> | <span class="t">They show similarity and things like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2771" target="_blank">00:46:11.820</a></span> | <span class="t">We want to make this a bit more precise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2774" target="_blank">00:46:14.740</a></span> | <span class="t">And indeed for natural language processing, as in other areas of machine learning, a big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2779" target="_blank">00:46:19.860</a></span> | <span class="t">part of what people are doing is working out good ways to evaluate knowledge that things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2785" target="_blank">00:46:25.700</a></span> | <span class="t">have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2787" target="_blank">00:46:27.300</a></span> | <span class="t">So how can we really evaluate word vectors?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2790" target="_blank">00:46:30.180</a></span> | <span class="t">So in general for NLP evaluation, people talk about two ways of evaluation, intrinsic and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2796" target="_blank">00:46:36.780</a></span> | <span class="t">extrinsic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2798" target="_blank">00:46:38.060</a></span> | <span class="t">So an intrinsic evaluation means that you evaluate directly on the specific or intermediate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2807" target="_blank">00:46:47.140</a></span> | <span class="t">subtasks that you've been working on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2809" target="_blank">00:46:49.260</a></span> | <span class="t">So I want a measure where I can directly score how good my word vectors are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2814" target="_blank">00:46:54.580</a></span> | <span class="t">And normally intrinsic evaluations are fast to compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2819" target="_blank">00:46:59.620</a></span> | <span class="t">They helped you to understand the component you've been working on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2823" target="_blank">00:47:03.740</a></span> | <span class="t">But often simply trying to optimize that component may or may not have a very big good effect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2832" target="_blank">00:47:12.060</a></span> | <span class="t">on the overall system that you're trying to build.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2837" target="_blank">00:47:17.000</a></span> | <span class="t">So people have also been very interested in extrinsic evaluations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2842" target="_blank">00:47:22.260</a></span> | <span class="t">So an extrinsic evaluation is that you take some real task of interest to human beings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2849" target="_blank">00:47:29.180</a></span> | <span class="t">whether that's web search or machine translation or something like that, and you say your goal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2855" target="_blank">00:47:35.420</a></span> | <span class="t">is to actually improve performance on that task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2859" target="_blank">00:47:39.420</a></span> | <span class="t">Well that's a real proof that this is doing something useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2864" target="_blank">00:47:44.340</a></span> | <span class="t">So in some ways it's just clearly better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2867" target="_blank">00:47:47.980</a></span> | <span class="t">But on the other hand, it also has some disadvantages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2872" target="_blank">00:47:52.180</a></span> | <span class="t">It takes a lot longer to evaluate on an extrinsic task because it's a much bigger system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2879" target="_blank">00:47:59.820</a></span> | <span class="t">And sometimes, you know, when you change things, it's unclear whether the fact that the numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2886" target="_blank">00:48:06.620</a></span> | <span class="t">went down was because you now have worse word vectors or whether it's just somehow the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2893" target="_blank">00:48:13.600</a></span> | <span class="t">components of the system interacted better with your old word vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2898" target="_blank">00:48:18.820</a></span> | <span class="t">And if you change the other components as well, things would get better again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2903" target="_blank">00:48:23.380</a></span> | <span class="t">So in some ways it can sometimes be muddier to see if you're making progress.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2909" target="_blank">00:48:29.340</a></span> | <span class="t">But I'll touch on both of these methods here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2914" target="_blank">00:48:34.140</a></span> | <span class="t">So for intrinsic evaluation of word vectors, one way which we mentioned last time was this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2922" target="_blank">00:48:42.900</a></span> | <span class="t">word vector analogy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2924" target="_blank">00:48:44.440</a></span> | <span class="t">So we could simply give our models a big collection of word vector analogy problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2930" target="_blank">00:48:50.080</a></span> | <span class="t">So we could say man is the woman as king is the what, and ask the model to find the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2936" target="_blank">00:48:56.800</a></span> | <span class="t">that is closest using that sort of word analogy computation and hope that what comes out there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2943" target="_blank">00:49:03.840</a></span> | <span class="t">is queen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2945" target="_blank">00:49:05.960</a></span> | <span class="t">And so that's something people have done and have worked out an accuracy score of how often</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2950" target="_blank">00:49:10.980</a></span> | <span class="t">that you are right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2954" target="_blank">00:49:14.120</a></span> | <span class="t">At this point, I should just mention one little trick of these word vector analogies that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2960" target="_blank">00:49:20.320</a></span> | <span class="t">everyone uses but not everyone talks about along the first instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2965" target="_blank">00:49:25.640</a></span> | <span class="t">I mean, there's a little trick which you can find in the Gensim code if you look at it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2971" target="_blank">00:49:31.160</a></span> | <span class="t">but when it does man is the woman as king is to what, something that could often happen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2983" target="_blank">00:49:43.180</a></span> | <span class="t">is that actually the word, once you do your pluses and your minuses, that the word that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2989" target="_blank">00:49:49.340</a></span> | <span class="t">will actually be closest is still king.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=2994" target="_blank">00:49:54.660</a></span> | <span class="t">So the way people always do this is that they don't allow one of the three input words in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3001" target="_blank">00:50:01.620</a></span> | <span class="t">the selection process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3003" target="_blank">00:50:03.500</a></span> | <span class="t">So you're choosing the nearest word that isn't one of the input words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3011" target="_blank">00:50:11.420</a></span> | <span class="t">So since here is showing results from the glove vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3016" target="_blank">00:50:16.220</a></span> | <span class="t">So the glove vectors have this strong linear component property, just like I showed before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3024" target="_blank">00:50:24.420</a></span> | <span class="t">for coals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3026" target="_blank">00:50:26.500</a></span> | <span class="t">So this is for the male female dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3030" target="_blank">00:50:30.760</a></span> | <span class="t">And so because of this, you'd expect in a lot of cases that word analogies would work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3036" target="_blank">00:50:36.260</a></span> | <span class="t">because I can take the vector difference of man and woman.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3040" target="_blank">00:50:40.160</a></span> | <span class="t">And then if I add that vector difference on to brother, I expect to get to sister and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3045" target="_blank">00:50:45.740</a></span> | <span class="t">king, queen, and for many of these examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3048" target="_blank">00:50:48.340</a></span> | <span class="t">But of course, they may not always work, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3051" target="_blank">00:50:51.980</a></span> | <span class="t">Because if I start from emperor, it's sort of on a more of a lean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3056" target="_blank">00:50:56.040</a></span> | <span class="t">And so it might turn out that I get countess or duchess coming out instead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3062" target="_blank">00:51:02.440</a></span> | <span class="t">You can do this for various different relations, so different semantic relations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3066" target="_blank">00:51:06.920</a></span> | <span class="t">So these sort of word vectors actually learn quite a bit of just world knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3072" target="_blank">00:51:12.200</a></span> | <span class="t">So here's the company CEO, or this is the company CEO around 2010 to 2014, when the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3079" target="_blank">00:51:19.380</a></span> | <span class="t">data was taken from word vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3083" target="_blank">00:51:23.680</a></span> | <span class="t">And they, as well as semantic things or pragmatic things like this, they also learn syntactic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3088" target="_blank">00:51:28.800</a></span> | <span class="t">things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3089" target="_blank">00:51:29.800</a></span> | <span class="t">So here are vectors for positive, comparative, and superlative forms of adjectives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3095" target="_blank">00:51:35.500</a></span> | <span class="t">And you can see those also move in roughly linear components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3100" target="_blank">00:51:40.840</a></span> | <span class="t">So the word2vec people built a data set of analogies so you could evaluate different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3107" target="_blank">00:51:47.200</a></span> | <span class="t">models on the accuracy of their analogies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3110" target="_blank">00:51:50.840</a></span> | <span class="t">And so here's how you can do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3114" target="_blank">00:51:54.040</a></span> | <span class="t">And this gives some numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3115" target="_blank">00:51:55.640</a></span> | <span class="t">So there are semantic and syntactic analogies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3118" target="_blank">00:51:58.200</a></span> | <span class="t">I'll just look at the totals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3120" target="_blank">00:52:00.320</a></span> | <span class="t">OK, so what I said before is if you just use unscaled co-occurrence counts and pass them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3129" target="_blank">00:52:09.040</a></span> | <span class="t">through an SVD, things work terribly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3131" target="_blank">00:52:11.740</a></span> | <span class="t">And you see that there, you only get 7.3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3134" target="_blank">00:52:14.780</a></span> | <span class="t">But then, as I also pointed out, if you do some scaling, you can actually get SVD of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3140" target="_blank">00:52:20.240</a></span> | <span class="t">a scaled count matrix to work reasonably well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3143" target="_blank">00:52:23.560</a></span> | <span class="t">So this SVDL is similar to the COLS model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3148" target="_blank">00:52:28.980</a></span> | <span class="t">And now we're getting up to 60.1, which actually isn't a bad score, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3152" target="_blank">00:52:32.740</a></span> | <span class="t">So you can actually do a decent job without a neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3156" target="_blank">00:52:36.880</a></span> | <span class="t">And then here are the two variants of the word2vec model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3163" target="_blank">00:52:43.440</a></span> | <span class="t">And here are our results from the GloVe model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3166" target="_blank">00:52:46.400</a></span> | <span class="t">And of course, at the time, 2014, we took this as absolute proof that our model was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3173" target="_blank">00:52:53.240</a></span> | <span class="t">better and our more efficient use of statistics was really working in our favor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3180" target="_blank">00:53:00.040</a></span> | <span class="t">With seven years of retrospect, I think that's kind of not really true, it turns out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3185" target="_blank">00:53:05.000</a></span> | <span class="t">I think the main part of why we scored better is that we actually had better data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3190" target="_blank">00:53:10.840</a></span> | <span class="t">And so there's a bit of evidence about that on this next slide here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3195" target="_blank">00:53:15.960</a></span> | <span class="t">So this looks at the semantic, syntactic, and overall performance on word analogies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3203" target="_blank">00:53:23.360</a></span> | <span class="t">of GloVe models that were trained on different subsets of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3208" target="_blank">00:53:28.920</a></span> | <span class="t">So in particular, the two on the left are trained on Wikipedia.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3214" target="_blank">00:53:34.640</a></span> | <span class="t">And you can see that training on Wikipedia makes you do really well on semantic analogies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3220" target="_blank">00:53:40.600</a></span> | <span class="t">which maybe makes sense because Wikipedia just tells you a lot of semantic facts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3224" target="_blank">00:53:44.720</a></span> | <span class="t">I mean, that's kind of what encyclopedias do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3228" target="_blank">00:53:48.240</a></span> | <span class="t">And so one of the big advantages we actually had was that Wikipedia, that the GloVe model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3235" target="_blank">00:53:55.680</a></span> | <span class="t">was partly trained on Wikipedia as well as other text, whereas the word2vec model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3241" target="_blank">00:54:01.080</a></span> | <span class="t">was released was trained exclusively on Google News, so Newswire data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3246" target="_blank">00:54:06.240</a></span> | <span class="t">And if you only train on a smallish amount of Newswire data, you can see that for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3253" target="_blank">00:54:13.280</a></span> | <span class="t">semantics, it's just not as good as even one quarter of the size amount of Wikipedia data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3262" target="_blank">00:54:22.080</a></span> | <span class="t">Though if you get a lot of data, you can compensate for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3265" target="_blank">00:54:25.160</a></span> | <span class="t">So here on the right hand, did you then have common crawl web data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3269" target="_blank">00:54:29.800</a></span> | <span class="t">And so once there's a lot of web data, so now 42 billion words, you'll then start to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3275" target="_blank">00:54:35.240</a></span> | <span class="t">get good scores again from the semantic side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3279" target="_blank">00:54:39.840</a></span> | <span class="t">The graph on the right then shows how well do you do as you increase the vector dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3287" target="_blank">00:54:47.360</a></span> | <span class="t">And so what you can see there is, you know, 25 dimensional vectors aren't very good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3293" target="_blank">00:54:53.360</a></span> | <span class="t">They go up to sort of 50 and then 100.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3296" target="_blank">00:54:56.560</a></span> | <span class="t">And so 100 dimensional vectors already work reasonably well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3300" target="_blank">00:55:00.080</a></span> | <span class="t">So that's why I used 100 dimensional vectors when I showed my example in class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3306" target="_blank">00:55:06.800</a></span> | <span class="t">That is the sweets, too long load and working reasonably well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3311" target="_blank">00:55:11.840</a></span> | <span class="t">But you still get significant gains for 200 and it's somewhat to 300.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3316" target="_blank">00:55:16.200</a></span> | <span class="t">So at least back around sort of 2013 to 15, everyone sort of gravitated to the fact that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3322" target="_blank">00:55:22.200</a></span> | <span class="t">300 dimensional vectors is the sweet spot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3325" target="_blank">00:55:25.400</a></span> | <span class="t">So almost frequently, if you look through the best known sets of word vectors that include</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3330" target="_blank">00:55:30.600</a></span> | <span class="t">the word to deck vectors and the glove vectors, that usually what you get is 300 dimensional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3336" target="_blank">00:55:36.480</a></span> | <span class="t">word vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3339" target="_blank">00:55:39.960</a></span> | <span class="t">That's not the only intrinsic evaluation you can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3343" target="_blank">00:55:43.760</a></span> | <span class="t">Another intrinsic evaluation you can do is see how these models model human judgments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3350" target="_blank">00:55:50.600</a></span> | <span class="t">of word similarity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3352" target="_blank">00:55:52.840</a></span> | <span class="t">So psychologists for several decades have actually taken human judgments of word similarity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3360" target="_blank">00:56:00.800</a></span> | <span class="t">where literally you're asking people for pairs of words like professor and doctor to give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3367" target="_blank">00:56:07.200</a></span> | <span class="t">them a similarity score that's sort of being measured as some continuous quantity, giving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3372" target="_blank">00:56:12.600</a></span> | <span class="t">you a score between, say, zero and 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3376" target="_blank">00:56:16.600</a></span> | <span class="t">And so there are human judgments which are then averaged over multiple human judgments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3381" target="_blank">00:56:21.400</a></span> | <span class="t">as to how similar different words are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3384" target="_blank">00:56:24.080</a></span> | <span class="t">So tiger and cat is pretty similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3387" target="_blank">00:56:27.160</a></span> | <span class="t">Computer and Internet is pretty similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3389" target="_blank">00:56:29.320</a></span> | <span class="t">Machine and car is less similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3391" target="_blank">00:56:31.560</a></span> | <span class="t">Stock and CD aren't very similar at all, but stock and Jaguar are even less similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3398" target="_blank">00:56:38.560</a></span> | <span class="t">So we could then say for our models, do they have the same similarity judgments?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3406" target="_blank">00:56:46.240</a></span> | <span class="t">And in particular, we can measure a correlation coefficient of whether they give the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3411" target="_blank">00:56:51.160</a></span> | <span class="t">ordering of similarity judgments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3414" target="_blank">00:56:54.280</a></span> | <span class="t">And so then we can get data for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3417" target="_blank">00:56:57.040</a></span> | <span class="t">And so there are various different data sets of word similarities, and we can score different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3422" target="_blank">00:57:02.360</a></span> | <span class="t">models as to how well they do on similarities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3425" target="_blank">00:57:05.920</a></span> | <span class="t">And again, you see here that plain SVDs works comparatively better here for similarities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3434" target="_blank">00:57:14.840</a></span> | <span class="t">than it did for analogies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3435" target="_blank">00:57:15.840</a></span> | <span class="t">You know, it's not great, but it's now not completely terrible because we no longer need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3440" target="_blank">00:57:20.440</a></span> | <span class="t">that linear property.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3442" target="_blank">00:57:22.000</a></span> | <span class="t">But again, scaled SVDs work a lot better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3446" target="_blank">00:57:26.080</a></span> | <span class="t">Word2vec works a bit better than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3448" target="_blank">00:57:28.720</a></span> | <span class="t">And we got some of the same kind of minor advantages from the GloVe model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3452" target="_blank">00:57:32.520</a></span> | <span class="t">Hey, Chris, sorry to interrupt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3455" target="_blank">00:57:35.240</a></span> | <span class="t">A lot of the students were asking if you could re-explain the objective function for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3459" target="_blank">00:57:39.880</a></span> | <span class="t">GloVe model and also what log bilinear means.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3465" target="_blank">00:57:45.080</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3467" target="_blank">00:57:47.080</a></span> | <span class="t">Sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3469" target="_blank">00:57:49.080</a></span> | <span class="t">OK, here is my objective function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3476" target="_blank">00:57:56.680</a></span> | <span class="t">So one slide before that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3480" target="_blank">00:58:00.440</a></span> | <span class="t">So the property that we want is that we want the dot product to represent the log probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3492" target="_blank">00:58:12.560</a></span> | <span class="t">of co-occurrence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3494" target="_blank">00:58:14.520</a></span> | <span class="t">So that then gives me my tricky log bilinear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3500" target="_blank">00:58:20.220</a></span> | <span class="t">So the bi is that there's sort of the wi and the wj, so that there are sort of two linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3507" target="_blank">00:58:27.680</a></span> | <span class="t">things and it's linear in each one of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3510" target="_blank">00:58:30.800</a></span> | <span class="t">So this is sort of like having an rather than having a sort of an ax where you just have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3517" target="_blank">00:58:37.800</a></span> | <span class="t">something that's linear in x and a is a constant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3521" target="_blank">00:58:41.800</a></span> | <span class="t">It's bilinear because we have the wi, wj and this linear in both of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3527" target="_blank">00:58:47.760</a></span> | <span class="t">And that's then related to the log of a probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3531" target="_blank">00:58:51.320</a></span> | <span class="t">And so that gives us the log bilinear model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3537" target="_blank">00:58:57.080</a></span> | <span class="t">And so since we'd like these things to be equal, what we're doing here, if you ignore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3545" target="_blank">00:59:05.520</a></span> | <span class="t">these two center terms, is that we're wanting to say the difference between these two is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3553" target="_blank">00:59:13.280</a></span> | <span class="t">as small as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3554" target="_blank">00:59:14.920</a></span> | <span class="t">So we're taking this difference and we're squaring it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3557" target="_blank">00:59:17.600</a></span> | <span class="t">So it's always positive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3559" target="_blank">00:59:19.320</a></span> | <span class="t">And we want that squared term to be as small as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3565" target="_blank">00:59:25.040</a></span> | <span class="t">And that's 90% of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3568" target="_blank">00:59:28.040</a></span> | <span class="t">And you can basically stop there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3570" target="_blank">00:59:30.160</a></span> | <span class="t">But the other bit that's in here is a lot of the time when you're building models, rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3578" target="_blank">00:59:38.000</a></span> | <span class="t">than simply having sort of an ax model, it seems useful to have a bias term, which can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3585" target="_blank">00:59:45.840</a></span> | <span class="t">move things up and down for the word in general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3591" target="_blank">00:59:51.200</a></span> | <span class="t">And so we added into the model bias term so that there's a bias term for both words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3597" target="_blank">00:59:57.000</a></span> | <span class="t">So if in general probabilities are high for a certain word, this bias term can model that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3603" target="_blank">01:00:03.600</a></span> | <span class="t">And for the other word, this bias term can model it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3609" target="_blank">01:00:09.000</a></span> | <span class="t">So now I'll pop back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3610" target="_blank">01:00:10.800</a></span> | <span class="t">And after-- oh, actually, I just saw someone said, why multiplying by the f of-- sorry,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3620" target="_blank">01:00:20.000</a></span> | <span class="t">I did skip that last term.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3624" target="_blank">01:00:24.440</a></span> | <span class="t">So why modifying by this f of xij?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3627" target="_blank">01:00:27.880</a></span> | <span class="t">So this last bit was to scale things depending on the frequency of a word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3638" target="_blank">01:00:38.640</a></span> | <span class="t">Because you want to pay more attention to words that are more common or word pairs that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3648" target="_blank">01:00:48.040</a></span> | <span class="t">are more common.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3649" target="_blank">01:00:49.560</a></span> | <span class="t">Because if you think about it in word2vec terms, you're seeing if things have a co-occurrence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3656" target="_blank">01:00:56.800</a></span> | <span class="t">count of 50 versus 3, you want to do a better job at modeling the co-occurrence of the things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3665" target="_blank">01:01:05.320</a></span> | <span class="t">that occurred together 50 times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3671" target="_blank">01:01:11.480</a></span> | <span class="t">And so you want to consider in the count of co-occurrence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3676" target="_blank">01:01:16.360</a></span> | <span class="t">But then the argument is that that actually leads you astray when you have extremely common</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3682" target="_blank">01:01:22.000</a></span> | <span class="t">words like function words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3684" target="_blank">01:01:24.200</a></span> | <span class="t">And so effectively, you paid more attention to words that co-occurred together up until</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3691" target="_blank">01:01:31.800</a></span> | <span class="t">a certain point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3693" target="_blank">01:01:33.320</a></span> | <span class="t">And then the curve just went flat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3695" target="_blank">01:01:35.440</a></span> | <span class="t">So it didn't matter if it was an extremely, extremely common word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3699" target="_blank">01:01:39.080</a></span> | <span class="t">So then for extrinsic word vector evaluation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3705" target="_blank">01:01:45.560</a></span> | <span class="t">So at this point, you're now wanting to sort of say, well, can we embed our word vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3712" target="_blank">01:01:52.040</a></span> | <span class="t">in some end user task?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3715" target="_blank">01:01:55.000</a></span> | <span class="t">And do they help?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3718" target="_blank">01:01:58.000</a></span> | <span class="t">And do different word vectors work better or worse than other word vectors?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3723" target="_blank">01:02:03.480</a></span> | <span class="t">So this is something that we'll see a lot of later in the class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3727" target="_blank">01:02:07.400</a></span> | <span class="t">I mean, in particular, when you get on to doing assignment 3, that assignment 3, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3733" target="_blank">01:02:13.720</a></span> | <span class="t">get to build dependency parsers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3736" target="_blank">01:02:16.200</a></span> | <span class="t">And you can then use word vectors in the dependency parser and see how much they help.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3742" target="_blank">01:02:22.640</a></span> | <span class="t">We don't actually make you test out different sets of word vectors, but you could.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3748" target="_blank">01:02:28.640</a></span> | <span class="t">Here's just one example of this to give you a sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3751" target="_blank">01:02:31.560</a></span> | <span class="t">So the task of named entity recognition is going through a piece of text and identifying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3757" target="_blank">01:02:37.560</a></span> | <span class="t">mentions of a person name or an organization name like a company or a location.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3764" target="_blank">01:02:44.000</a></span> | <span class="t">And so if you have good word vectors, do they help you do named entity recognition better?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3773" target="_blank">01:02:53.080</a></span> | <span class="t">And the answer to that is yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3775" target="_blank">01:02:55.040</a></span> | <span class="t">So if one starts off with a model that simply has discrete features, so it uses word identity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3782" target="_blank">01:03:02.040</a></span> | <span class="t">as features, you can build a pretty good named entity model doing that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3786" target="_blank">01:03:06.360</a></span> | <span class="t">But if you add into it word vectors, you get a better representation of the meaning of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3791" target="_blank">01:03:11.640</a></span> | <span class="t">words and so that you can have the numbers go up quite a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3796" target="_blank">01:03:16.600</a></span> | <span class="t">And then you can compare different models to see how much gain they give you in terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3801" target="_blank">01:03:21.860</a></span> | <span class="t">of this extrinsic task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3804" target="_blank">01:03:24.760</a></span> | <span class="t">So skipping ahead, this was a question that I was asked after class, which was word sensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3813" target="_blank">01:03:33.160</a></span> | <span class="t">So far, we've had just one word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3818" target="_blank">01:03:38.440</a></span> | <span class="t">Sorry, for one particular string, we've got some string house, and we're going to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3825" target="_blank">01:03:45.400</a></span> | <span class="t">for each of those strings, there's a word vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3829" target="_blank">01:03:49.760</a></span> | <span class="t">And if you think about it a bit more, that seems like it's very weird, because actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3838" target="_blank">01:03:58.200</a></span> | <span class="t">most words, especially common words, and especially words that have existed for a long time, actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3844" target="_blank">01:04:04.880</a></span> | <span class="t">have many meanings, which are very different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3848" target="_blank">01:04:08.360</a></span> | <span class="t">So how could that be captured if you only have one word vector for the word, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3852" target="_blank">01:04:12.720</a></span> | <span class="t">you can't actually capture the fact that you've got different meanings for the word, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3857" target="_blank">01:04:17.680</a></span> | <span class="t">your meaning for the word is just one point in space, one vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3862" target="_blank">01:04:22.200</a></span> | <span class="t">And so as an example of that, here's the word pike.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3870" target="_blank">01:04:30.600</a></span> | <span class="t">But it is an old Germanic word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3872" target="_blank">01:04:32.240</a></span> | <span class="t">Well, what kind of meanings does the word pike have?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3876" target="_blank">01:04:36.600</a></span> | <span class="t">So you can maybe just think for a minute and think what meanings the word pike has.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3886" target="_blank">01:04:46.800</a></span> | <span class="t">And it actually turns out, you know, it has a lot of different meanings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3890" target="_blank">01:04:50.960</a></span> | <span class="t">So perhaps the most basic meaning is, if you did fantasy games or something, medieval weapons,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3899" target="_blank">01:04:59.240</a></span> | <span class="t">a sharp pointed staff is a pike.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3902" target="_blank">01:05:02.280</a></span> | <span class="t">But there's a kind of a fish that has a similar elongated shape that's a pike.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3908" target="_blank">01:05:08.140</a></span> | <span class="t">It was used for railroad lines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3912" target="_blank">01:05:12.360</a></span> | <span class="t">Maybe that usage isn't used much anymore, but it certainly still survives in referring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3916" target="_blank">01:05:16.720</a></span> | <span class="t">to roads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3917" target="_blank">01:05:17.720</a></span> | <span class="t">So this is like when you have turnpikes, we have expressions where pike means the future,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3923" target="_blank">01:05:23.280</a></span> | <span class="t">like coming down the pike.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3925" target="_blank">01:05:25.760</a></span> | <span class="t">It's a position in diving, that divers do a pike.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3930" target="_blank">01:05:30.400</a></span> | <span class="t">Those are all noun uses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3932" target="_blank">01:05:32.540</a></span> | <span class="t">They're also verbal uses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3934" target="_blank">01:05:34.140</a></span> | <span class="t">So you can pike somebody with your pike.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3937" target="_blank">01:05:37.880</a></span> | <span class="t">You know, different usages might have different currency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3940" target="_blank">01:05:40.920</a></span> | <span class="t">In Australia, you can also use pike to mean that you pull out of doing something like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3946" target="_blank">01:05:46.320</a></span> | <span class="t">I reckon he's going to pike.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3949" target="_blank">01:05:49.440</a></span> | <span class="t">I don't think that usage is used in America, but lots of meanings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3952" target="_blank">01:05:52.720</a></span> | <span class="t">And actually, for words that are common, or if you start thinking of words like line or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3956" target="_blank">01:05:56.760</a></span> | <span class="t">field, I mean, they just have even more meanings than this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3960" target="_blank">01:06:00.600</a></span> | <span class="t">So what are we actually doing with just one vector for a word?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3965" target="_blank">01:06:05.960</a></span> | <span class="t">And well, one way you could go is to say, okay, up until now, what we've done is crazy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3972" target="_blank">01:06:12.840</a></span> | <span class="t">Pike has, in other words, have all of these different meanings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3976" target="_blank">01:06:16.680</a></span> | <span class="t">So maybe what we should do is have different word vectors for the different meanings of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3983" target="_blank">01:06:23.000</a></span> | <span class="t">pike.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3984" target="_blank">01:06:24.000</a></span> | <span class="t">So we'd have one word vector for the medieval pointy weapon, another word vector for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3990" target="_blank">01:06:30.600</a></span> | <span class="t">kind of fish, another word vector for the kind of road.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3994" target="_blank">01:06:34.400</a></span> | <span class="t">So that they'd then be word sense vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=3999" target="_blank">01:06:39.560</a></span> | <span class="t">And you can do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4000" target="_blank">01:06:40.560</a></span> | <span class="t">I mean, actually, we were working on that in the early 2010s, actually, even before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4008" target="_blank">01:06:48.280</a></span> | <span class="t">Word2Vec came out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4010" target="_blank">01:06:50.760</a></span> | <span class="t">So this picture is a little bit small to see, but what we were doing was for words, we were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4019" target="_blank">01:06:59.480</a></span> | <span class="t">clustering instances of a word, hoping that those clusters, so clustering the word tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4027" target="_blank">01:07:07.040</a></span> | <span class="t">hoping those clusters that were similar represented sensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4030" target="_blank">01:07:10.800</a></span> | <span class="t">And then for the clusters of word tokens, we were sort of treating them like they were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4035" target="_blank">01:07:15.280</a></span> | <span class="t">separate words and learning a word vector for each.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4039" target="_blank">01:07:19.080</a></span> | <span class="t">And basically that actually works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4041" target="_blank">01:07:21.880</a></span> | <span class="t">So in green, we have two sensors for the word bank.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4046" target="_blank">01:07:26.200</a></span> | <span class="t">And so there's one sense for the word bank that's over here, where it's close to words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4050" target="_blank">01:07:30.400</a></span> | <span class="t">like banking, finance, transaction, and laundering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4054" target="_blank">01:07:34.040</a></span> | <span class="t">And then we have another sense for the word bank over here, where it's close to words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4058" target="_blank">01:07:38.180</a></span> | <span class="t">like plateau, boundary, gap, territory, which is the riverbank sense of the word bank.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4064" target="_blank">01:07:44.860</a></span> | <span class="t">And for the word jaguar, that's in purple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4068" target="_blank">01:07:48.200</a></span> | <span class="t">Well, jaguar has a number of sensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4071" target="_blank">01:07:51.400</a></span> | <span class="t">And so we have those as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4073" target="_blank">01:07:53.600</a></span> | <span class="t">So this sense down here is close to hunter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4077" target="_blank">01:07:57.140</a></span> | <span class="t">So that's the sort of big game animal sense of jaguar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4081" target="_blank">01:08:01.400</a></span> | <span class="t">Up the top here, it's being shown close to luxury and convertible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4085" target="_blank">01:08:05.240</a></span> | <span class="t">So this is the jaguar car sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4088" target="_blank">01:08:08.600</a></span> | <span class="t">Then jaguar here is near string, keyboard, and words like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4093" target="_blank">01:08:13.600</a></span> | <span class="t">So jaguar is the name of a kind of keyboard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4098" target="_blank">01:08:18.000</a></span> | <span class="t">And then this final jaguar over here is close to software and Microsoft.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4103" target="_blank">01:08:23.360</a></span> | <span class="t">And then if you're old enough, you'll remember that there was an old version of Mac OS that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4107" target="_blank">01:08:27.680</a></span> | <span class="t">was called jaguar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4109" target="_blank">01:08:29.200</a></span> | <span class="t">So that's then the computer sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4111" target="_blank">01:08:31.160</a></span> | <span class="t">So basically, this does work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4113" target="_blank">01:08:33.240</a></span> | <span class="t">And we can learn word vectors for different senses of a word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4118" target="_blank">01:08:38.520</a></span> | <span class="t">But actually, this isn't the majority way that things have then gone in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4125" target="_blank">01:08:45.120</a></span> | <span class="t">And there are kind of a couple of reasons for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4128" target="_blank">01:08:48.800</a></span> | <span class="t">I mean, one is just simplicity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4131" target="_blank">01:08:51.480</a></span> | <span class="t">If you do this, it's kind of complex, because you first of all have to learn word sensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4138" target="_blank">01:08:58.360</a></span> | <span class="t">and then start learning word vectors in terms of the word sensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4141" target="_blank">01:09:01.760</a></span> | <span class="t">But the other reason is, although this model of having word sensors is traditional, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4150" target="_blank">01:09:10.120</a></span> | <span class="t">what you see in dictionaries, it's commonly what's being used in natural language processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4155" target="_blank">01:09:15.760</a></span> | <span class="t">I mean, it tends to be imperfect in its own way, because we're trying to take all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4160" target="_blank">01:09:20.440</a></span> | <span class="t">uses of the word pike and sort of cut them up into key different senses, where the difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4170" target="_blank">01:09:30.360</a></span> | <span class="t">is kind of overlapping, and it's often not clear which ones to count as distinct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4175" target="_blank">01:09:35.040</a></span> | <span class="t">So for example, here, right, a railroad line and a type of road, well, sort of that's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4179" target="_blank">01:09:39.720</a></span> | <span class="t">same sense of pike, it's just that they're different forms of transportation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4184" target="_blank">01:09:44.080</a></span> | <span class="t">And so you know that this could be, you know, a type of transportation line and cover both</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4188" target="_blank">01:09:48.320</a></span> | <span class="t">of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4189" target="_blank">01:09:49.320</a></span> | <span class="t">So it's always sort of very unclear how you cut word meaning into different senses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4195" target="_blank">01:09:55.960</a></span> | <span class="t">And indeed, if you look at different dictionaries, everyone does it differently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4201" target="_blank">01:10:01.760</a></span> | <span class="t">So it actually turns out that in practice, you can do rather well by simply having one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4211" target="_blank">01:10:11.160</a></span> | <span class="t">word vector per word type.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4214" target="_blank">01:10:14.580</a></span> | <span class="t">And what happens if you do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4217" target="_blank">01:10:17.600</a></span> | <span class="t">Well, what you find is that what you learn as a word vector is what gets referred to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4228" target="_blank">01:10:28.600</a></span> | <span class="t">in fancy talk as a super superposition of the word vectors for the different senses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4237" target="_blank">01:10:37.400</a></span> | <span class="t">of a word, where the word superposition means no more or less than a weighted sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4244" target="_blank">01:10:44.400</a></span> | <span class="t">So the vector that we learned for pike will be a weighted average of the vectors that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4250" target="_blank">01:10:50.480</a></span> | <span class="t">you would have learned for the medieval weapon sense, plus the fish sense, plus the road</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4256" target="_blank">01:10:56.520</a></span> | <span class="t">sense, plus whatever other senses that you have, where the weighting that's given to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4261" target="_blank">01:11:01.360</a></span> | <span class="t">these different sense vectors corresponds to the frequencies of use of the different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4266" target="_blank">01:11:06.720</a></span> | <span class="t">senses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4268" target="_blank">01:11:08.080</a></span> | <span class="t">So we end up with the word, the vector for pike being a kind of an average vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4276" target="_blank">01:11:16.840</a></span> | <span class="t">And so if you're, if you're, say, okay, you've just added up several different vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4284" target="_blank">01:11:24.720</a></span> | <span class="t">into an average, you might think that that's kind of useless, because, you know, you've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4290" target="_blank">01:11:30.920</a></span> | <span class="t">lost the real meanings of the word, and you've just got some kind of funny average vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4296" target="_blank">01:11:36.680</a></span> | <span class="t">that's in between them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4299" target="_blank">01:11:39.400</a></span> | <span class="t">But actually, it turns out that if you use this average vector in applications, it tends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4307" target="_blank">01:11:47.080</a></span> | <span class="t">to sort of self disambiguate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4310" target="_blank">01:11:50.360</a></span> | <span class="t">Because if you say, is the word pike similar to the word for fish?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4317" target="_blank">01:11:57.560</a></span> | <span class="t">Well, part of this vector represents fish, the fish sense of pike.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4322" target="_blank">01:12:02.980</a></span> | <span class="t">And so in those components, it'll be kind of similar to the fish vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4327" target="_blank">01:12:07.680</a></span> | <span class="t">And so yes, you'll say the substantial, there's substantial similarity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4333" target="_blank">01:12:13.680</a></span> | <span class="t">Whereas if in another piece of text that says, you know, the men were armed with pikes and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4341" target="_blank">01:12:21.480</a></span> | <span class="t">lances or pikes and maces or whatever other medieval weapons you remember, well, actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4348" target="_blank">01:12:28.380</a></span> | <span class="t">some of that meaning is in the pike vector as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4351" target="_blank">01:12:31.460</a></span> | <span class="t">And so it'll say, yeah, there's good similarity with mace and staff and words like that as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4357" target="_blank">01:12:37.640</a></span> | <span class="t">well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4358" target="_blank">01:12:38.780</a></span> | <span class="t">And in fact, we can work out which sense of pike is intended by just sort of seeing which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4366" target="_blank">01:12:46.060</a></span> | <span class="t">components are similar to other words that are used in the same context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4371" target="_blank">01:12:51.840</a></span> | <span class="t">And indeed, there's actually a much more surprising result than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4376" target="_blank">01:12:56.220</a></span> | <span class="t">And this is a result that's Juzo Sanjeev Arora, Tungu Umar, who's now on our Stanford faculty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4383" target="_blank">01:13:03.420</a></span> | <span class="t">and others in 2018.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4385" target="_blank">01:13:05.900</a></span> | <span class="t">And that's the following result, which I'm not actually going to explain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4389" target="_blank">01:13:09.380</a></span> | <span class="t">But so if you think that the vector for pike is just a sum of the vectors for the different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4399" target="_blank">01:13:19.180</a></span> | <span class="t">senses, well, it should be you'd think that it's just completely impossible to reconstruct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4407" target="_blank">01:13:27.540</a></span> | <span class="t">the sense vectors from the vector for the word type.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4414" target="_blank">01:13:34.260</a></span> | <span class="t">Because normally, if I say I've got two numbers, the sum of them is 17, you just have no information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4421" target="_blank">01:13:41.520</a></span> | <span class="t">as to what my two numbers are, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4423" target="_blank">01:13:43.460</a></span> | <span class="t">You can't resolve it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4425" target="_blank">01:13:45.300</a></span> | <span class="t">And even worse, if I tell you I've got three numbers, and they sum to 17.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4430" target="_blank">01:13:50.140</a></span> | <span class="t">But it turns out that when we have these high dimensional vector spaces, that things are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4437" target="_blank">01:13:57.500</a></span> | <span class="t">so sparse in those high dimensional vector spaces, that you can use ideas from sparse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4443" target="_blank">01:14:03.420</a></span> | <span class="t">coding to actually separate out the different senses, providing they're relatively common.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4451" target="_blank">01:14:11.540</a></span> | <span class="t">So they show in their paper that you can start with the vector of say, pike, and actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4457" target="_blank">01:14:17.280</a></span> | <span class="t">separate out components of that vector that correspond to different senses of the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4462" target="_blank">01:14:22.480</a></span> | <span class="t">pike.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4463" target="_blank">01:14:23.480</a></span> | <span class="t">And so here's an example at the bottom of this slide, which is for the word pike, separate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4470" target="_blank">01:14:30.300</a></span> | <span class="t">out that vector into five different senses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4473" target="_blank">01:14:33.320</a></span> | <span class="t">And so there's one sense is close to the words trousers, blouse, waistcoats, and this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4478" target="_blank">01:14:38.500</a></span> | <span class="t">the sort of clothing sense of tie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4480" target="_blank">01:14:40.980</a></span> | <span class="t">Another sense is close to wires, cables, wiring, electrical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4485" target="_blank">01:14:45.380</a></span> | <span class="t">So that's the sort of the tie sense of a tie used in electrical stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4490" target="_blank">01:14:50.500</a></span> | <span class="t">Then we have sort of scoreline, goal is equalizer, so this is the sporting game sense of tie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4496" target="_blank">01:14:56.420</a></span> | <span class="t">This one also seems to in a different way, evoke sporting game sense of tie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4502" target="_blank">01:15:02.380</a></span> | <span class="t">Then there's finally this one here, maybe my music is just really bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4506" target="_blank">01:15:06.860</a></span> | <span class="t">Maybe it's because you get ties in music when you tie notes together, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4510" target="_blank">01:15:10.620</a></span> | <span class="t">So you get these different senses out of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4512" target="_blank">01:15:12.380</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gqaHkPEZAew&t=4512" target="_blank">01:15:12.880</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>