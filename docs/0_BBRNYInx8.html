<html><head><title>Lesson 9A 2022 - Stable Diffusion deep dive</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Lesson 9A 2022 - Stable Diffusion deep dive</h2><a href="https://www.youtube.com/watch?v=0_BBRNYInx8"><img src="https://i.ytimg.com/vi/0_BBRNYInx8/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=40">0:40</a> Replicating the sampling loop<br><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=77">1:17</a> The Auto-Encoder<br><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=235">3:55</a> Adding Noise and image-to-image<br><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=523">8:43</a> The Text Encoding Process<br><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=915">15:15</a> Textual Inversion<br><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1116">18:36</a> The UNET and classifier free guidance<br><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1481">24:41</a> Sampling explanation<br><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2190">36:30</a> Additional guidance<br><br><div style="text-align: left;"><a href="./0_BBRNYInx8.html">Whisper Transcript</a> | <a href="./transcript_0_BBRNYInx8.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello, everyone. My name is Jonathan, and today I'm going to be taking you through this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=6" target="_blank">00:00:06.540</a></span> | <span class="t">stable diffusion deep dive notebook, looking into the code behind the kind of popular high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=13" target="_blank">00:00:13.240</a></span> | <span class="t">level APIs and libraries and tools and so on to see what exactly does the generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=18" target="_blank">00:00:18.000</a></span> | <span class="t">process look like and how can we modify that? How do each of the individual components work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=23" target="_blank">00:00:23.640</a></span> | <span class="t">So feel free to run along with me. If you haven't before, this might take a little while</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=28" target="_blank">00:00:28.880</a></span> | <span class="t">to run just because it's downloading these large models. If they aren't already downloaded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=32" target="_blank">00:00:32.720</a></span> | <span class="t">and loading them up. So we're going to start by just kind of recreating what it looks like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=37" target="_blank">00:00:37.640</a></span> | <span class="t">to generate an image using, say, one of the existing pipelines and hugging face. So we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=42" target="_blank">00:00:42.500</a></span> | <span class="t">going to basically have copied the code from the core method of the default stable diffusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=47" target="_blank">00:00:47.920</a></span> | <span class="t">pipeline. So if you go and view that here, you'll see that we're going to be basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=52" target="_blank">00:00:52.000</a></span> | <span class="t">replicating this code. But now we'll be doing it on our own sort of notebook, and then we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=58" target="_blank">00:00:58.760</a></span> | <span class="t">slowly understand what each of these different parts is doing. So we've got some setup, we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=63" target="_blank">00:01:03.080</a></span> | <span class="t">got some sort of loop running through a number of sampling timestamps, and we're generating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=66" target="_blank">00:01:06.920</a></span> | <span class="t">an image. So this is supposed to be a watercolor picture of an otter. And it's very, very cool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=71" target="_blank">00:01:11.760</a></span> | <span class="t">that this model can just do this. But now we want to know how does that actually work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=75" target="_blank">00:01:15.960</a></span> | <span class="t">What's going on? So the first component is the autoencoder. Now this stable diffusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=81" target="_blank">00:01:21.200</a></span> | <span class="t">is a latent diffusion model. And what that means is that it doesn't operate on pixels,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=85" target="_blank">00:01:25.480</a></span> | <span class="t">it operates in the latent space of some other autoencoder model. In this case, a variational</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=91" target="_blank">00:01:31.240</a></span> | <span class="t">autoencoder that's been trained on a large number of images to compress them down into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=95" target="_blank">00:01:35.440</a></span> | <span class="t">this latent representation and bring them back up again. So I have some functions to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=99" target="_blank">00:01:39.400</a></span> | <span class="t">do that. We're going to look at what it's like in action, just downloading a picture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=103" target="_blank">00:01:43.480</a></span> | <span class="t">from the internet, opening it up with PIL. So we have this 512 by 512 pixel image, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=110" target="_blank">00:01:50.240</a></span> | <span class="t">we're going to load it in. And then we're going to use our function defined above to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=114" target="_blank">00:01:54.520</a></span> | <span class="t">encode that into some latent representation. And what this is doing is calling the VAE.encode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=119" target="_blank">00:01:59.840</a></span> | <span class="t">method on a tensor version of the image. And that gives us a distribution. And so we sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=124" target="_blank">00:02:04.440</a></span> | <span class="t">from that distribution and we scaling by this because that's what the authors of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=129" target="_blank">00:02:09.320</a></span> | <span class="t">did. They scaled the latents down before they fed them to the model. And so we have to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=134" target="_blank">00:02:14.400</a></span> | <span class="t">that scaling and then the reverse when we decoding just to be consistent with that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=139" target="_blank">00:02:19.440</a></span> | <span class="t">But the key idea is that we go from this big image down to this 4 by 64 by 64 latent representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=146" target="_blank">00:02:26.360</a></span> | <span class="t">So we've gone from this much larger image down. And if we visualize what the four channels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=150" target="_blank">00:02:30.440</a></span> | <span class="t">here, this four different 64 by 64 channels, what that looks like, we'll see that it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=155" target="_blank">00:02:35.560</a></span> | <span class="t">capturing something of the image. You can sort of see the same shapes and things there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=159" target="_blank">00:02:39.880</a></span> | <span class="t">But it's not quite a direct mapping or anything. For example, there's this weirdness going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=163" target="_blank">00:02:43.440</a></span> | <span class="t">on the beak. Some of the channels look slightly stranger than the others. So there's some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=167" target="_blank">00:02:47.940</a></span> | <span class="t">sort of rich information captured there. And if we decode this back, what we'll see is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=173" target="_blank">00:02:53.160</a></span> | <span class="t">that the decoded image looks really good. You really have to look closely to tell the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=177" target="_blank">00:02:57.480</a></span> | <span class="t">difference between our input image here and the decoded version. So very, very impressive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=182" target="_blank">00:03:02.800</a></span> | <span class="t">compression, right? This is a factor of 8 in each dimension. So 512 by 512 down to 64</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=189" target="_blank">00:03:09.320</a></span> | <span class="t">by 64. It's like a factor of 64 reduction in data, but it's still somehow capturing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=196" target="_blank">00:03:16.220</a></span> | <span class="t">most of that information. It's a very information-rich representation. And this is going to be great</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=200" target="_blank">00:03:20.960</a></span> | <span class="t">because now we can work with that with our diffusion model and get nice high resolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=205" target="_blank">00:03:25.520</a></span> | <span class="t">results even though we're only working with these 64 by 64 latents. Now, it doesn't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=210" target="_blank">00:03:30.640</a></span> | <span class="t">to be 64 by 64. You can go and modify this to say what if this is 640 and encode that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=217" target="_blank">00:03:37.920</a></span> | <span class="t">down and you'll see that it's just that same factor of 8 reduction. And there we go. Now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=224" target="_blank">00:03:44.320</a></span> | <span class="t">we have 80 by 64. This just has to be a multiple of 8. Otherwise, you'll get, I think, an error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=231" target="_blank">00:03:51.000</a></span> | <span class="t">Okay. So we have our encoded version of this image, and that's pretty great. The next component</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=236" target="_blank">00:03:56.600</a></span> | <span class="t">we're going to look at is the scheduler, and I'll look more closely at this later. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=241" target="_blank">00:04:01.240</a></span> | <span class="t">for now, we're going to focus on this idea of adding noise, right? So during training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=245" target="_blank">00:04:05.040</a></span> | <span class="t">we add some noise to an image, and then the model tries to predict what that noise is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=249" target="_blank">00:04:09.280</a></span> | <span class="t">and we're going to do that to different amounts. So here we're going to recreate the same type</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=253" target="_blank">00:04:13.960</a></span> | <span class="t">of schedule, and you can try different schedulers from the library. Oops. And these parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=258" target="_blank">00:04:18.960</a></span> | <span class="t">here, beta start, beta end, beta schedule, that's how much noise was added at different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=263" target="_blank">00:04:23.000</a></span> | <span class="t">time steps and how many time steps are used during training. For sampling, we don't want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=268" target="_blank">00:04:28.080</a></span> | <span class="t">to have to do a thousand steps, so we can set a new number of time steps, and then we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=272" target="_blank">00:04:32.760</a></span> | <span class="t">see how these correspond with the scheduler.timesteps attribute to the original training time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=281" target="_blank">00:04:41.360</a></span> | <span class="t">So here we're going to have 15 sampling steps, and that's going to be equivalent to starting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=285" target="_blank">00:04:45.360</a></span> | <span class="t">at time step 999 and just moving linearly down to time steps zero. We can also look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=291" target="_blank">00:04:51.480</a></span> | <span class="t">at the actual amount of noise present with the sigma's attribute. So again, starting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=295" target="_blank">00:04:55.680</a></span> | <span class="t">high, moving down, and if you want to see what that schedule looks like, we can plot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=300" target="_blank">00:05:00.240</a></span> | <span class="t">that here. And if you want to see the time steps, you'll see that it's just a linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=304" target="_blank">00:05:04.000</a></span> | <span class="t">relationship. So there we go. We're going to start at a very high noise value, and we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=310" target="_blank">00:05:10.000</a></span> | <span class="t">going to slowly, slowly try and reduce this down until ideally we get an image out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=314" target="_blank">00:05:14.440</a></span> | <span class="t">Okay, so the sigma is the amount of noise added. Let's see what that looks like. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=319" target="_blank">00:05:19.440</a></span> | <span class="t">I'm going to start with some random noise that's the same shape as my latent representation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=323" target="_blank">00:05:23.640</a></span> | <span class="t">my encoded image, and then I'd like to be equivalent to sampling step 10 out of 15</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=329" target="_blank">00:05:29.200</a></span> | <span class="t">here. So I'm going to go and look up what time step that equates to, and that's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=333" target="_blank">00:05:33.920</a></span> | <span class="t">to be one of the arguments that I passed the scheduler.addnoise function. So I'm calling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=338" target="_blank">00:05:38.760</a></span> | <span class="t">scheduler.addnoise, giving it my encoded image, the noise, and what time step I'd like to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=344" target="_blank">00:05:44.280</a></span> | <span class="t">be noising equivalent to. And this is going to give me this noisy but still recognisable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=348" target="_blank">00:05:48.680</a></span> | <span class="t">version of the image. And you can go and say, okay, what if I look at somewhere earlier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=352" target="_blank">00:05:52.520</a></span> | <span class="t">in the process, right? Does it look more noisy? What about right at the beginning, right at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=356" target="_blank">00:05:56.680</a></span> | <span class="t">the end? Feel free to play around there. Okay, so this adding noise, what are we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=362" target="_blank">00:06:02.760</a></span> | <span class="t">doing? What does the code look like? Let's inspect the function, and you'll see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=366" target="_blank">00:06:06.600</a></span> | <span class="t">there's some set up for different types of argument and shapes. But the key line is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=371" target="_blank">00:06:11.240</a></span> | <span class="t">this noisy samples is equal to original samples plus the noise scaled by the sigma parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=377" target="_blank">00:06:17.480</a></span> | <span class="t">All right, so that's all it is. It's not always the same. Different papers and implementations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=381" target="_blank">00:06:21.960</a></span> | <span class="t">will add the noise slightly differently. But in this case, that's all it's doing. So scheduler.addnoise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=387" target="_blank">00:06:27.320</a></span> | <span class="t">just adding noise that's the same shape as the latency scaled by the sigma parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=392" target="_blank">00:06:32.360</a></span> | <span class="t">Okay, so that's what we're doing. So if we want to start from random noise instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=396" target="_blank">00:06:36.840</a></span> | <span class="t">a noisy image, we're going to scale it by that same sigma value so that it looks the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=400" target="_blank">00:06:40.760</a></span> | <span class="t">same as an image that's been scaled by that amount. But then before we feed that to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=404" target="_blank">00:06:44.960</a></span> | <span class="t">actual model, we then have to handle that scaling again. You could do it like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=409" target="_blank">00:06:49.280</a></span> | <span class="t">but now we have this scale model input function associated with the scheduler just to hide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=414" target="_blank">00:06:54.760</a></span> | <span class="t">that complexity away. Okay, so now we're going to look at the same kind of sampling loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=419" target="_blank">00:06:59.080</a></span> | <span class="t">as before. But we're going to start now with our image, we're going to take our encoded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=423" target="_blank">00:07:03.080</a></span> | <span class="t">image, we're going to noise it to some time set, and then we're only going to denoise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=426" target="_blank">00:07:06.840</a></span> | <span class="t">from there. So in code, we are now preparing our text and everything the same as before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=431" target="_blank">00:07:11.320</a></span> | <span class="t">which we'll look at, we setting our number of inference steps to 50, right, number inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=435" target="_blank">00:07:15.560</a></span> | <span class="t">steps is equal to 50 here. And we're saying I'd like to start at the equivalent of step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=439" target="_blank">00:07:19.240</a></span> | <span class="t">10 out of 50. So I'll look up what time step that equates to, I'll add noise to my image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=446" target="_blank">00:07:26.040</a></span> | <span class="t">equivalent to that step. And then we're going to run through sampling, but this time we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=450" target="_blank">00:07:30.440</a></span> | <span class="t">only going to start doing things once we get above that start step. So I'm going to ignore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=454" target="_blank">00:07:34.360</a></span> | <span class="t">the first 10 out of 50 steps. And then beyond that, I'm now going to start with this noisy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=458" target="_blank">00:07:38.920</a></span> | <span class="t">version of my input image. And I'm going to denoise it according to this prompt. And the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=463" target="_blank">00:07:43.160</a></span> | <span class="t">hope here is that by starting from something that has some of the sort of rough structure and color</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=468" target="_blank">00:07:48.040</a></span> | <span class="t">of that input image, I can kind of fix that into my generation. But I've got a new prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=473" target="_blank">00:07:53.320</a></span> | <span class="t">a National Geographic photo of a colorful dancer. And here we go, we see this is the same sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=478" target="_blank">00:07:58.040</a></span> | <span class="t">thing as the parrot. But now we have this completely different actual content thanks to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=482" target="_blank">00:08:02.200</a></span> | <span class="t">a different prompt. And so that's a fun kind of use of this image to image process. You might have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=487" target="_blank">00:08:07.880</a></span> | <span class="t">seen this for taking drawings, adding a bunch of noise and then denoising them into fancy paintings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=491" target="_blank">00:08:11.880</a></span> | <span class="t">and so on. So again, this is something that there's existing tools for this, right, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=496" target="_blank">00:08:16.920</a></span> | <span class="t">strength parameter and the image to image pipeline. That's just something like this, what step are we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=503" target="_blank">00:08:23.560</a></span> | <span class="t">starting at? How many steps are we skipping? But you can see that this is a pretty powerful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=509" target="_blank">00:08:29.000</a></span> | <span class="t">technique for getting a bit of extra control over like composition and color and a bit of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=513" target="_blank">00:08:33.720</a></span> | <span class="t">structure. Okay, so that's that trick with adding noise and then using that as image to image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=519" target="_blank">00:08:39.560</a></span> | <span class="t">The next big section I'd like to look at is how do we go from a piece of text that describes what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=526" target="_blank">00:08:46.120</a></span> | <span class="t">we want into a numerical representation that we can feed to the model. So we're going to trace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=532" target="_blank">00:08:52.360</a></span> | <span class="t">out that pipeline. And along the way, we'll see how we can modify that for a bit of fun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=536" target="_blank">00:08:56.920</a></span> | <span class="t">So step number one, we're taking our prompt and returning it into a sequence of discrete tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=543" target="_blank">00:09:03.640</a></span> | <span class="t">So here we have, in this case, 77, because that's the maximum length, discrete tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=549" target="_blank">00:09:09.640</a></span> | <span class="t">it's always going to be that if your prompt is longer, it'll truncate it. And if we decode these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=553" target="_blank">00:09:13.800</a></span> | <span class="t">tokens back, we'll see that we have a special token for the start of the text, then a picture of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=559" target="_blank">00:09:19.880</a></span> | <span class="t">puppy. And then the rest is all the same token, which is this kind of end of text padding token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=564" target="_blank">00:09:24.680</a></span> | <span class="t">Right, so we have this special token for puppy. This special token has its own meaning end of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=570" target="_blank">00:09:30.600</a></span> | <span class="t">text. And the prompts are always going to be padded to be the same length. So before, in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=577" target="_blank">00:09:37.000</a></span> | <span class="t">code that we were using there, we always jump straight to the circle output embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=581" target="_blank">00:09:41.560</a></span> | <span class="t">which is what we fed to the model as conditioning. And so somehow this captures some information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=586" target="_blank">00:09:46.120</a></span> | <span class="t">about this prompt. And but now we want to say, well, how do we get there? How do we get from this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=590" target="_blank">00:09:50.520</a></span> | <span class="t">sequence of tokens to these output embeddings? What is this text encoder forward pass doing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=597" target="_blank">00:09:57.080</a></span> | <span class="t">Right, so we can look at this, and there's going to be multiple steps. And the first is going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=601" target="_blank">00:10:01.960</a></span> | <span class="t">be some embeddings. So if we look at the text encoder dot text model dot embeddings, we'll see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=606" target="_blank">00:10:06.200</a></span> | <span class="t">there's a couple of different ones, we have token embeddings, right? And so this is to take those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=610" target="_blank">00:10:10.600</a></span> | <span class="t">individual tokens, token 408, or whatever, and map it into a representation that's a numerical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=618" target="_blank">00:10:18.760</a></span> | <span class="t">representation. So here it's a learned embedding. There are about 50,000 rows, one for each token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=625" target="_blank">00:10:25.800</a></span> | <span class="t">And for each token, we have 768 values. So that's the embedding of that token. And if we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=631" target="_blank">00:10:31.160</a></span> | <span class="t">feed one in and see what the embedding looks like, here's the token for puppy. And here's the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=635" target="_blank">00:10:35.640</a></span> | <span class="t">embedding, right? 768 numbers that somehow capture that meaning of that token on its own. And we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=642" target="_blank">00:10:42.280</a></span> | <span class="t">do the same for all of the tokens in our prompt. So we feed them through this token embedding layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=646" target="_blank">00:10:46.360</a></span> | <span class="t">And now we get 77 768 dimensional representations of this of each token. Now, these are all on their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=655" target="_blank">00:10:55.640</a></span> | <span class="t">own. And no matter where in the sentence is, it is the token embedding will be the same. So the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=661" target="_blank">00:11:01.080</a></span> | <span class="t">step is to add some positional information. Some models will do this with some kind of like learned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=665" target="_blank">00:11:05.560</a></span> | <span class="t">pattern of positioning. But in this case, the positional embedding is just another learned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=669" target="_blank">00:11:09.800</a></span> | <span class="t">embedding. But now instead of having one embedding for every token, we have one embedding for every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=674" target="_blank">00:11:14.840</a></span> | <span class="t">position out of all 77 possible positions. And so just like we did for the tokens, we can feed them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=680" target="_blank">00:11:20.280</a></span> | <span class="t">the position IDs, one for every possible position, and we'll get back out an embedding for every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=686" target="_blank">00:11:26.520</a></span> | <span class="t">position in the prompt. And combining them together, there's again, multiple ways people do this in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=692" target="_blank">00:11:32.360</a></span> | <span class="t">literature. But in this case, it's as simple as adding them. That's why they made them the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=696" target="_blank">00:11:36.360</a></span> | <span class="t">shape, so that you can just add the two together. And now, these input embeddings have some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=701" target="_blank">00:11:41.560</a></span> | <span class="t">information related to the token and some related to the position. And so so far, we haven't seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=707" target="_blank">00:11:47.000</a></span> | <span class="t">any big model just to learn embeddings, but this is getting everything ready to feed through that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=711" target="_blank">00:11:51.240</a></span> | <span class="t">model. And so we can check that this is the same as if we just called the embeddings layer of that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=716" target="_blank">00:11:56.680</a></span> | <span class="t">model, which is going to do both of those steps at once. And but we'll see just now why we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=721" target="_blank">00:12:01.240</a></span> | <span class="t">separate that out into individual ones. Okay, so we have these individual tokens, and they have some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=727" target="_blank">00:12:07.080</a></span> | <span class="t">positional information, we have these final embeddings. Now we'd like to turn them into something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=731" target="_blank">00:12:11.000</a></span> | <span class="t">that has a richer representation, thanks to some big transformer model. And so we're going to feed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=735" target="_blank">00:12:15.720</a></span> | <span class="t">these through. And I made this little diagram here, each token is going to turn into a token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=740" target="_blank">00:12:20.120</a></span> | <span class="t">embedding combined with the positional embedding. And then it's going to get fed through this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=744" target="_blank">00:12:24.760</a></span> | <span class="t">transformer encoder, which is just a stack of these blocks. And so each block has some magic like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=749" target="_blank">00:12:29.960</a></span> | <span class="t">attention has some feed forward components, there's additions and normalizations and skips and so on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=755" target="_blank">00:12:35.320</a></span> | <span class="t">as well. And but we're going to have some number of these blocks all stacked together, and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=759" target="_blank">00:12:39.480</a></span> | <span class="t">outputs of each one get fed into the next block and so on. And so we get our final set of hidden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=764" target="_blank">00:12:44.120</a></span> | <span class="t">states, these encoder hidden states, aka the output embeddings. And this is what we feed to our unit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=770" target="_blank">00:12:50.200</a></span> | <span class="t">to make its predictions. So the way we get this, I just copied the text encoded up text model forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=776" target="_blank">00:12:56.200</a></span> | <span class="t">method, pulled out the relevant bits, we are going to take in those input embeddings, combined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=780" target="_blank">00:13:00.360</a></span> | <span class="t">positional and token embeddings, and we're going to feed that through the text model dot encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=785" target="_blank">00:13:05.000</a></span> | <span class="t">function with some additional parameters around attention masking and telling it that we'd like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=790" target="_blank">00:13:10.520</a></span> | <span class="t">to output the hidden states rather than the final outputs. So if we run this, we can just double</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=796" target="_blank">00:13:16.040</a></span> | <span class="t">check, these embeddings are going to look just like the output embeddings we saw right at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=801" target="_blank">00:13:21.240</a></span> | <span class="t">beginning. So we've taken that one step, tokens to output embeddings, and we've broken it down into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=806" target="_blank">00:13:26.040</a></span> | <span class="t">this number of smaller steps where we have tokenization, getting our token embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=810" target="_blank">00:13:30.120</a></span> | <span class="t">combining with position embeddings, feeding it through the model, and then that gives us those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=813" target="_blank">00:13:33.720</a></span> | <span class="t">final outputs. So why have we gone through this problem trouble? Well, there's a couple of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=818" target="_blank">00:13:38.520</a></span> | <span class="t">we can do. One demo here, I'm getting the token embeddings, but then I'm looking up where is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=826" target="_blank">00:13:46.120</a></span> | <span class="t">token for puppy, and I'm going to replace it with a new set of embeddings. And this is going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=831" target="_blank">00:13:51.080</a></span> | <span class="t">another just learned embedding of this particular token here, 2368. So I'm kind of cutting out the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=837" target="_blank">00:13:57.160</a></span> | <span class="t">token embedding for puppy, slipping in this new set of token embeddings, and I'm going to get some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=841" target="_blank">00:14:01.640</a></span> | <span class="t">output embeddings which at the start look very similar to the previous ones, in fact identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=845" target="_blank">00:14:05.640</a></span> | <span class="t">But as soon as you get past the position of puppy in that prompt, you're going to see that the rest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=850" target="_blank">00:14:10.440</a></span> | <span class="t">have changed. So we've somehow messed with these embeddings by slipping in this new token embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=855" target="_blank">00:14:15.400</a></span> | <span class="t">right at the start. And if we generate with those embeddings, which is what this function is doing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=860" target="_blank">00:14:20.600</a></span> | <span class="t">we should see something other than a puppy. And sure enough, drum roll, we don't, we get a cat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=867" target="_blank">00:14:27.320</a></span> | <span class="t">And so now you know what token 2368 means. We've managed to slip in a new token embedding and get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=873" target="_blank">00:14:33.880</a></span> | <span class="t">a different image. Okay, what can we do with this? Why is this fun? Well, a couple of tricks. First</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=878" target="_blank">00:14:38.840</a></span> | <span class="t">off, we could look up the token embedding for skunk, right, which is this number here. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=884" target="_blank">00:14:44.360</a></span> | <span class="t">instead of now just replacing that in place of puppy, what if I make a new token embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=890" target="_blank">00:14:50.120</a></span> | <span class="t">that's some combination of the embedding of puppy and the embedding of skunk, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=894" target="_blank">00:14:54.680</a></span> | <span class="t">So I'm taking these two token embeddings, I'm just averaging them, and I'm inserting them into my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=899" target="_blank">00:14:59.720</a></span> | <span class="t">set of token embeddings for my prompt in place of just the word puppy. And so hopefully when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=905" target="_blank">00:15:05.720</a></span> | <span class="t">generate with this, we get something that looks a bit like a puppy, a bit like a skunk. And this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=910" target="_blank">00:15:10.440</a></span> | <span class="t">doesn't work all the time, but it's pretty cute when it does. There we go, puppy skunk hybrid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=915" target="_blank">00:15:15.400</a></span> | <span class="t">Okay, so that's not the real reason we're looking at this. The main application at the moment of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=920" target="_blank">00:15:20.280</a></span> | <span class="t">being able to mess with these token embeddings is to be able to do something called textual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=924" target="_blank">00:15:24.120</a></span> | <span class="t">inversion. So in textual inversion, we're going to have our prompt tokenize it and so on. But here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=929" target="_blank">00:15:29.560</a></span> | <span class="t">we're going to have a special learned embedding for some new concept, right? And so the way that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=935" target="_blank">00:15:35.000</a></span> | <span class="t">trained is going to be outside of the scope of this notebook. But there's a good blog post and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=940" target="_blank">00:15:40.520</a></span> | <span class="t">community notebooks and things for doing that. But let's just see this in application here. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=945" target="_blank">00:15:45.080</a></span> | <span class="t">there's a whole library of these concepts, stable diffusion concept library, where you can browse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=951" target="_blank">00:15:51.160</a></span> | <span class="t">through tons and tons and tons of look over 1,400 different community contributed token embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=959" target="_blank">00:15:59.240</a></span> | <span class="t">that people have trained. And so I'm going to use this one here, this bird style. Here's some example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=964" target="_blank">00:16:04.120</a></span> | <span class="t">outputs. And then these are the images it was trained on. So these pretty little bird paintings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=968" target="_blank">00:16:08.440</a></span> | <span class="t">done by my mother. And I've trained a new token embedding that tries to capture the essence of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=974" target="_blank">00:16:14.360</a></span> | <span class="t">the style. And that's represented here in this learned embed stop in. So if you download this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=979" target="_blank">00:16:19.480</a></span> | <span class="t">and then upload it to wherever your notebooks running, I have it here, learned embed stop in,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=983" target="_blank">00:16:23.640</a></span> | <span class="t">we can load that in. And you'll see that it's just a dictionary, where we have one key, that's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=988" target="_blank">00:16:28.840</a></span> | <span class="t">name of my new style. And then we have this token embedding 768 numbers. And so now instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=995" target="_blank">00:16:35.240</a></span> | <span class="t">slipping in the token embedding for cat, we're going to slip in this new embedding, which we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=999" target="_blank">00:16:39.160</a></span> | <span class="t">loaded from the file into this prompt. So a mouse in the style of puppy, tokenize, get my token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1004" target="_blank">00:16:44.360</a></span> | <span class="t">embeddings, and then I'm going to slip in this replacement embedding in place of the embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1009" target="_blank">00:16:49.640</a></span> | <span class="t">for puppy. And when we generate with that, we should hopefully get a mouse in the style of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1015" target="_blank">00:16:55.640</a></span> | <span class="t">this kind of cutesy watercolor on rough paper image. And sure enough, that's what we get,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1021" target="_blank">00:17:01.400</a></span> | <span class="t">very cute little drawing of a mouse in an apron, apparently. Okay, so very, very cool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1026" target="_blank">00:17:06.120</a></span> | <span class="t">application. Again, there's a nice inference notebook that makes this really easy. You can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1031" target="_blank">00:17:11.320</a></span> | <span class="t">say a cat toy in the style of burp style, you don't have to worry about manually replacing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1035" target="_blank">00:17:15.400</a></span> | <span class="t">the token embeddings yourself. But it's good to know what the code looks like under the hood,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1039" target="_blank">00:17:19.240</a></span> | <span class="t">right? How are we doing that? What stage of the text embedding process we're modifying?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1043" target="_blank">00:17:23.320</a></span> | <span class="t">Very fun to get a bit of extra control, and a very useful technique, because now we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1048" target="_blank">00:17:28.200</a></span> | <span class="t">kind of augment our model's vocabulary without having to actually retrain the model itself,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1052" target="_blank">00:17:32.840</a></span> | <span class="t">we're just learning a new token embedding. It's a very, very powerful idea, and really fun to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1057" target="_blank">00:17:37.000</a></span> | <span class="t">play with. And like I said, there's thousands of community contributed tokens, but you can also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1062" target="_blank">00:17:42.120</a></span> | <span class="t">train your own, I think I linked the notebook from here, but it's also in all the docs and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1067" target="_blank">00:17:47.000</a></span> | <span class="t">Here's the training notebook. Okay, final little trick with embeddings, rather than messing with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1072" target="_blank">00:17:52.840</a></span> | <span class="t">them at the token embedding level, we can push the whole prompt through that entire process to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1077" target="_blank">00:17:57.000</a></span> | <span class="t">get our final output embeddings, and we can mess with those at that stage as well. So here I have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1081" target="_blank">00:18:01.160</a></span> | <span class="t">two prompts, a mouse and a leopard, tokenizing them, encoding them with a text encoder, so that's that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1086" target="_blank">00:18:06.200</a></span> | <span class="t">whole process. And these final output embeddings, I'm just going to mix them together according to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1090" target="_blank">00:18:10.680</a></span> | <span class="t">some factor, and generate with that. And so you can try this with, you know, a cat and a snake.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1096" target="_blank">00:18:16.280</a></span> | <span class="t">And you should be able to get some really fun, different chimeras and oops, a snail apparently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1105" target="_blank">00:18:25.880</a></span> | <span class="t">Okay, well, I can't spell. But yeah, have fun with that, doesn't have to be animals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1111" target="_blank">00:18:31.240</a></span> | <span class="t">I'd love to see what you create with these weird mixed up generations. Okay, we should look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1117" target="_blank">00:18:37.960</a></span> | <span class="t">actual model itself, the key unit model, the diffusion model. What is it doing? What is it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1123" target="_blank">00:18:43.320</a></span> | <span class="t">predicting? What is it accepting as arguments? So this is the kind of call signature, we call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1129" target="_blank">00:18:49.560</a></span> | <span class="t">our units forward pass, and we feed in our noisy latency, the timestamp, and it's like the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1135" target="_blank">00:18:55.480</a></span> | <span class="t">timestamp, and the encoder hidden states, right? So those text embeddings that we've just been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1140" target="_blank">00:19:00.200</a></span> | <span class="t">having fun with. So doing that without any loops or anything, I'm sitting in my scheduler, getting my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1145" target="_blank">00:19:05.640</a></span> | <span class="t">time step, getting my noisy latent, and my text embeddings. And then we're going to get our model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1151" target="_blank">00:19:11.960</a></span> | <span class="t">prediction. And you'll look at the shape of that. And you'll see that this prediction has the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1155" target="_blank">00:19:15.320</a></span> | <span class="t">shape as the latency. And given these noisy latency, what the model is predicting is the noise component</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1161" target="_blank">00:19:21.960</a></span> | <span class="t">of that. And actually, it's predicting the noise component scaled by sigma. So if we wanted to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1167" target="_blank">00:19:27.560</a></span> | <span class="t">what the original image looks like, we could say, well, the de-noise latency is going to be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1172" target="_blank">00:19:32.280</a></span> | <span class="t">current noisy latency minus sigma times the model prediction, right? And so when we de-noising,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1179" target="_blank">00:19:39.480</a></span> | <span class="t">we're not going to go straight to that upward prediction, we're going to just remove a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1183" target="_blank">00:19:43.160</a></span> | <span class="t">bit of the noise at a time. But it might be useful to visualize what that final prediction looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1188" target="_blank">00:19:48.040</a></span> | <span class="t">So that's what we're doing here, making a folder to store some images, preparing our text scheduler</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1193" target="_blank">00:19:53.000</a></span> | <span class="t">and input. And then we're going to do this loop. But now we're going to get the model prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1198" target="_blank">00:19:58.200</a></span> | <span class="t">And instead of just updating our latency by one step, we're also going to store an image, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1203" target="_blank">00:20:03.320</a></span> | <span class="t">and decoding these two images, an image of the predicted completely de-noised, like original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1208" target="_blank">00:20:08.200</a></span> | <span class="t">sample. So that's this predicted original sample here. You could also calculate this yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1212" target="_blank">00:20:12.120</a></span> | <span class="t">Latency zero is equal to the current latency minus sigma times the noise prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1216" target="_blank">00:20:16.600</a></span> | <span class="t">All right, so those two should work equivalently. But this loop is going to run, and it's going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1221" target="_blank">00:20:21.320</a></span> | <span class="t">save those images to the steps folder, which we can then visualize. And so once this finishes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1226" target="_blank">00:20:26.600</a></span> | <span class="t">in a second or two, on the left, we're going to see the kind of noisy input to the model at each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1231" target="_blank">00:20:31.560</a></span> | <span class="t">stage. And on the right, we're going to see the noisy input minus the noise prediction, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1237" target="_blank">00:20:37.160</a></span> | <span class="t">so the de-noised version. And so we'll just give it a second or two to run. It's taking it a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1241" target="_blank">00:20:41.640</a></span> | <span class="t">bit longer because it's decoding those images each time, saving them. But once this finishes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1247" target="_blank">00:20:47.720</a></span> | <span class="t">we should have a nice little preview video. Okay, here we go. So this is the noisy latent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1262" target="_blank">00:21:02.200</a></span> | <span class="t">And if we take the model's noise prediction and subtract it from that, we get this very blurry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1266" target="_blank">00:21:06.840</a></span> | <span class="t">output. And so you'll see as we play this -- oh, I've left some modifications in from last time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1272" target="_blank">00:21:12.680</a></span> | <span class="t">sorry. When you see this guidance scale, we'll be back at I think it was eight. In the next section,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1280" target="_blank">00:21:20.280</a></span> | <span class="t">we'll talk about classifier-free guidance. And so I've been modifying that example. My bad. I might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1285" target="_blank">00:21:25.320</a></span> | <span class="t">cut this out of the video. We'll see. So I've got to wait a few seconds again for that to generate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1290" target="_blank">00:21:30.040</a></span> | <span class="t">And I'll do so as patiently as I can.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1292" target="_blank">00:21:32.760</a></span> | <span class="t">Okay, so here we go again, the noisy input, the predicted de-noised version. And you can see at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1319" target="_blank">00:21:59.640</a></span> | <span class="t">the start, it's very blurry. But over time, it gradually converges on our final output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1323" target="_blank">00:22:03.960</a></span> | <span class="t">And you'll notice that on the left, these are the latents as they are each step. They don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1330" target="_blank">00:22:10.680</a></span> | <span class="t">change particularly drastically a little bit at a time. But at the start, when the model doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1335" target="_blank">00:22:15.400</a></span> | <span class="t">have much to go on, its predictions do change quite a bit at each step, right? It's much less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1340" target="_blank">00:22:20.840</a></span> | <span class="t">well-defined. And then as we go forward in time, it gets more and more refined, better and better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1345" target="_blank">00:22:25.560</a></span> | <span class="t">predictions. And so it's got a more accurate estimation of the noise to remove. And we remove</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1351" target="_blank">00:22:31.160</a></span> | <span class="t">that noise gradually until we finally get our output. Quite fun to visualize the process. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1356" target="_blank">00:22:36.440</a></span> | <span class="t">hopefully that helps you understand why we don't just make one prediction and do it in one step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1360" target="_blank">00:22:40.760</a></span> | <span class="t">right? Because we get this very blurry mess. But instead, we do this kind of iterative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1364" target="_blank">00:22:44.520</a></span> | <span class="t">sampling there, which we'll talk about very shortly. Before then, though, the final thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1369" target="_blank">00:22:49.720</a></span> | <span class="t">I should mention, classifier-free guidance. What is that? Well, like you saw when I accidentally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1374" target="_blank">00:22:54.840</a></span> | <span class="t">generated the version with a much lower guidance scale, the way classifier-free guidance works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1381" target="_blank">00:23:01.240</a></span> | <span class="t">is that in all of these loops, we haven't actually been passing one set of noisy latents through the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1386" target="_blank">00:23:06.280</a></span> | <span class="t">model. We've been passing two identical versions. And as our text embeddings, we've not just been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1393" target="_blank">00:23:13.080</a></span> | <span class="t">passing the embeddings of our prompts, right? These ones here, we've been concatenating them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1398" target="_blank">00:23:18.440</a></span> | <span class="t">with some unconditional embeddings as well. And what the unconditional embeddings are is just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1402" target="_blank">00:23:22.600</a></span> | <span class="t">blank prompt, right? No text whatsoever. So just all padding passing that through. So when we get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1408" target="_blank">00:23:28.680</a></span> | <span class="t">our predictions here, we've given in two sets of latents and two sets of text embeddings, we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1414" target="_blank">00:23:34.120</a></span> | <span class="t">going to get out two predictions for the noise. So we splitting that apart, one prediction for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1419" target="_blank">00:23:39.800</a></span> | <span class="t">unconditional, like no prompt version, and one for the prediction based on the prompt. And so what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1425" target="_blank">00:23:45.880</a></span> | <span class="t">can do now is we can say, well, my final prediction is going to be the unconditional version plus the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1431" target="_blank">00:23:51.160</a></span> | <span class="t">guidance scale times the difference, right? So if you think about it, if I predict without the noise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1435" target="_blank">00:23:55.960</a></span> | <span class="t">I'm predicting here. If I predict with the noise, sorry, with the text encoding, with the prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1442" target="_blank">00:24:02.120</a></span> | <span class="t">I get this prediction instead. And I'd like to move more in that direction. I'd like to push it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1446" target="_blank">00:24:06.040</a></span> | <span class="t">even further towards the prompt version and beyond. So this guidance scale can be larger than one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1452" target="_blank">00:24:12.440</a></span> | <span class="t">to push it even more in that direction. And this, it turns out, is kind of key for getting it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1456" target="_blank">00:24:16.920</a></span> | <span class="t">follow the prompt nicely. And I think it was first brought up in the glide paper. AI Coffee Break on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1463" target="_blank">00:24:23.400</a></span> | <span class="t">YouTube has a great video on that. But yeah, really useful trick or really neat hack, depending on who</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1468" target="_blank">00:24:28.360</a></span> | <span class="t">you talk to. But it does seem to work. And the higher the guidance scale, the more the model will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1473" target="_blank">00:24:33.080</a></span> | <span class="t">try and look like the prompts kind of in the extreme versus that lower guidance scale, it might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1478" target="_blank">00:24:38.040</a></span> | <span class="t">just try and look like a generic good picture. Okay, we've been hiding away some complexity in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1484" target="_blank">00:24:44.360</a></span> | <span class="t">terms of this scheduler dot step function. So I think we're going to step away from the notebook</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1488" target="_blank">00:24:48.520</a></span> | <span class="t">now and scribble a bit on some paper to try and explain exactly what's going on with sampling and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1493" target="_blank">00:24:53.080</a></span> | <span class="t">so on. And then we'll come back to the notebook for one final trick. All right, so here's my take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1500" target="_blank">00:25:00.040</a></span> | <span class="t">on sampling. And to start with, I'd like you to imagine the space of all possible images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1505" target="_blank">00:25:05.480</a></span> | <span class="t">So this is a very large high dimensional space for 256 by 256 by three image, that is 200,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1514" target="_blank">00:25:14.120</a></span> | <span class="t">dimensional. And my paper, unfortunately, is only two dimensional. So we're going to have to squish</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1519" target="_blank">00:25:19.000</a></span> | <span class="t">this down a fair bit and use our imagination. Now, if you just look at a random point in this space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1525" target="_blank">00:25:25.160</a></span> | <span class="t">this is most likely not going to look like anything recognizable, it'll probably just look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1530" target="_blank">00:25:30.280</a></span> | <span class="t">like garbled noise. But if we map an image into the space, we'll see that it has some sort of fixed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1536" target="_blank">00:25:36.920</a></span> | <span class="t">point. And a very similar image almost pixel equivalent, it's going to be very close by.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1542" target="_blank">00:25:42.760</a></span> | <span class="t">Now, there's this theory that you'll hear talked about called manifold theory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1547" target="_blank">00:25:47.240</a></span> | <span class="t">which says that for most real images, like a data set of images, these are going to lie</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1552" target="_blank">00:25:52.840</a></span> | <span class="t">on some lower dimensional manifold within this higher dimensional space, right? In other words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1557" target="_blank">00:25:57.800</a></span> | <span class="t">if we map a whole bunch of images into the space, they're not going to fill the whole space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1562" target="_blank">00:26:02.360</a></span> | <span class="t">they're going to be kind of clustered onto some surface. Now, I've drawn it as a line here because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1567" target="_blank">00:26:07.000</a></span> | <span class="t">we stuck with 2D, but this is a much higher dimensional plane equivalent. Okay, so each of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1572" target="_blank">00:26:12.520</a></span> | <span class="t">these ones here is some image. And the reason that I'm starting with this is because we'd like to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1578" target="_blank">00:26:18.840</a></span> | <span class="t">generate images, we'd like to generate plausible looking images, not just random nonsense. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1584" target="_blank">00:26:24.040</a></span> | <span class="t">we'd like to do that with diffusion models. So where did they come in? Well, we can start with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1589" target="_blank">00:26:29.160</a></span> | <span class="t">some image here, some real image from our training data. And we can push it away from the manifold</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1595" target="_blank">00:26:35.800</a></span> | <span class="t">of like plausible existing images by corrupting it somehow. So for example, just adding random noise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1601" target="_blank">00:26:41.080</a></span> | <span class="t">that's equivalent to like moving in some random direction in this space of all possible images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1605" target="_blank">00:26:45.800</a></span> | <span class="t">And so that's going to push the image away. And then we can try and predict using some model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1612" target="_blank">00:26:52.200</a></span> | <span class="t">what this noise looks like, right? How do I go from here back to a plausible image? What is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1617" target="_blank">00:26:57.880</a></span> | <span class="t">noise that's been added? And so that's going to be our big unit that does that prediction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1622" target="_blank">00:27:02.040</a></span> | <span class="t">that's going to be our diffusion model, right? And so that's, in this language, going to be called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1627" target="_blank">00:27:07.320</a></span> | <span class="t">something like a score function, right? How do I get from wherever I am? What's the noise that I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1631" target="_blank">00:27:11.480</a></span> | <span class="t">need to remove to get back to a plausible image? Okay, so that's all well and good. We can train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1639" target="_blank">00:27:19.240</a></span> | <span class="t">this model with a number of examples, because we can just take our training data, add some random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1643" target="_blank">00:27:23.080</a></span> | <span class="t">noise, predict, predict, try and predict the noise, update our model parameters. So we can hopefully</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1647" target="_blank">00:27:27.400</a></span> | <span class="t">learn that function fairly well. Now we'd like to generate with this model, right? So how do we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1652" target="_blank">00:27:32.360</a></span> | <span class="t">that? Well, we can start at some random point, right? Like, let's start over here. And you might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1658" target="_blank">00:27:38.840</a></span> | <span class="t">think, well, surely I can just now predict the noise, remove that, and then I get my output image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1664" target="_blank">00:27:44.520</a></span> | <span class="t">And that's great, except that you've got to remember now we're starting from a random point in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1667" target="_blank">00:27:47.880</a></span> | <span class="t">space of all possible images. It just looks like garbled nonsense. And the model's trying to say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1672" target="_blank">00:27:52.280</a></span> | <span class="t">well, what does the noise look like? And so you can imagine here, for training, the first thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1676" target="_blank">00:27:56.680</a></span> | <span class="t">we're training, the further away we get from our examples, the sparser our training will have been.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1680" target="_blank">00:28:00.840</a></span> | <span class="t">But also, it's not like it's very obvious how we got to this noisy version, right? We could have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1685" target="_blank">00:28:05.880</a></span> | <span class="t">come from this image over here, added a bunch of noise. We could have come from one over here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1690" target="_blank">00:28:10.600</a></span> | <span class="t">one over here. And so this model's not going to be able to make a perfect prediction. At best,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1695" target="_blank">00:28:15.160</a></span> | <span class="t">it might say, well, somewhere in that direction, right? It could point towards something like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1699" target="_blank">00:28:19.960</a></span> | <span class="t">dataset mean, or at least the edge that's closer. But it's not going to be able to perfectly give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1704" target="_blank">00:28:24.440</a></span> | <span class="t">you one nice solution. And sure enough, that's what we see. If we sample the fusion model system one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1709" target="_blank">00:28:29.960</a></span> | <span class="t">step, we get the predictions, look at what that corresponds to as an image, it's just going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1713" target="_blank">00:28:33.800</a></span> | <span class="t">look like a blurry mess, maybe like the mean of the data or, you know, some sort of garbled output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1718" target="_blank">00:28:38.360</a></span> | <span class="t">definitely not going to look like a nice image. So how do we do better? And the idea of sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1725" target="_blank">00:28:45.560</a></span> | <span class="t">is to say, well, there's a couple of framings. So I'll start with the existing framing that you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1730" target="_blank">00:28:50.840</a></span> | <span class="t">see talked about a lot of score-based models and so on. And then we'll talk about some other ways</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1735" target="_blank">00:28:55.080</a></span> | <span class="t">to think about it as well. So this process of gradually corrupting our images away, adding a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1741" target="_blank">00:29:01.720</a></span> | <span class="t">little bit of noise at a time, people like to talk of this as a stochastic differential equation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1746" target="_blank">00:29:06.920</a></span> | <span class="t">Stochastic because there's some randomness, right, we're picking random amounts of noise, random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1750" target="_blank">00:29:10.360</a></span> | <span class="t">directions to add, and a differential equation because it's not talking about anything absolute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1755" target="_blank">00:29:15.320</a></span> | <span class="t">just how we should change this from moment to moment to get more and more corrupted, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1759" target="_blank">00:29:19.240</a></span> | <span class="t">So that's why it's a differential equation. And with that framing, the question of, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1765" target="_blank">00:29:25.000</a></span> | <span class="t">how do I go now back to the image? That's framed as solving an ordinary differential equation that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1770" target="_blank">00:29:30.440</a></span> | <span class="t">corresponds to like the reverse of this process. You can't solve ODEs in a single step, but you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1776" target="_blank">00:29:36.920</a></span> | <span class="t">can find an approximate solution. And the more sort of sub-steps you take, the better your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1782" target="_blank">00:29:42.200</a></span> | <span class="t">approximation. And so that's what these samples are doing, given like, okay, we set this image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1785" target="_blank">00:29:45.960</a></span> | <span class="t">over here, here's my prediction, rather than moving the whole way there in one go, we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1790" target="_blank">00:29:50.360</a></span> | <span class="t">remove some of that noise, right, do a little update, and then we'll get a new prediction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1796" target="_blank">00:29:56.040</a></span> | <span class="t">right? And so maybe now the prediction is slightly better. It says up here. So we move a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1799" target="_blank">00:29:59.480</a></span> | <span class="t">in that direction. And now it makes an even better prediction, because as we get closer to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1803" target="_blank">00:30:03.080</a></span> | <span class="t">manifold, right, as we have less and less noise, and more and more of like some image emerging,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1807" target="_blank">00:30:07.480</a></span> | <span class="t">the model is able to get more and more accurate predictions. And so in some sort of number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1811" target="_blank">00:30:11.960</a></span> | <span class="t">steps, we divide up this this process, and we get closer and closer and closer until we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1817" target="_blank">00:30:17.240</a></span> | <span class="t">ideally find some image that looks very plausible as our output. And so that's what we're doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1823" target="_blank">00:30:23.240</a></span> | <span class="t">here with a lot of these samplers, they're effectively trying to solve this ODE in some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1828" target="_blank">00:30:28.360</a></span> | <span class="t">number of steps by, yeah, breaking the process up and only moving a small amount at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1833" target="_blank">00:30:33.240</a></span> | <span class="t">Now, you get sort of first order solvers, right, where all we're doing is just linearly moving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1838" target="_blank">00:30:38.920</a></span> | <span class="t">within each one. And this is equivalent to something called Euler's method or Euler's method,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1843" target="_blank">00:30:43.480</a></span> | <span class="t">if you're like me, and you've only ever read it. And this is what some of the most basic samplers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1847" target="_blank">00:30:47.480</a></span> | <span class="t">are doing, just linear approximations for each of these little steps. But you also get additional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1853" target="_blank">00:30:53.560</a></span> | <span class="t">approaches. So for example, maybe if we were to make a prediction from here, it might look like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1859" target="_blank">00:30:59.480</a></span> | <span class="t">something like this. And if we were to make a prediction from here, it might look like something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1863" target="_blank">00:31:03.640</a></span> | <span class="t">like that. So we have our error here. But as you move in that direction, it's also changing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1869" target="_blank">00:31:09.880</a></span> | <span class="t">right? So there's like a derivative of a derivative, a gradient of a gradient. And that's where this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1875" target="_blank">00:31:15.400</a></span> | <span class="t">second order solver comes in and says, well, if I know how this prediction changes as I move in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1881" target="_blank">00:31:21.400</a></span> | <span class="t">direction, like what is the derivative of it, then I can kind of account for that curvature when I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1885" target="_blank">00:31:25.960</a></span> | <span class="t">make my update step, and maybe know that it's going to curve a bit in that direction. And so that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1890" target="_blank">00:31:30.760</a></span> | <span class="t">where we get things like these so called second order solvers and higher order solvers. The upside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1895" target="_blank">00:31:35.560</a></span> | <span class="t">of this is that we can get, you know, do a larger step at a time, because we have a more accurate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1900" target="_blank">00:31:40.520</a></span> | <span class="t">prediction, we're not just doing a first order linear approximation, we have this kind of curvature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1905" target="_blank">00:31:45.000</a></span> | <span class="t">taken into account. The downside is that to estimate that curvature for a given point, we might need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1910" target="_blank">00:31:50.680</a></span> | <span class="t">call our model multiple times to get multiple estimates. And so that takes time. So we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1914" target="_blank">00:31:54.600</a></span> | <span class="t">take a larger step, but we need more model evaluations per step. A kind of hybrid approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1920" target="_blank">00:32:00.200</a></span> | <span class="t">is to say, well, rather than trying to estimate the curvature here, I might just take a linear step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1925" target="_blank">00:32:05.800</a></span> | <span class="t">look at the next prediction, but I'll keep a history of my previous steps. And so then over here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1930" target="_blank">00:32:10.280</a></span> | <span class="t">it predicts like this. So I have now this history. And I'm going to use that to better guess what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1935" target="_blank">00:32:15.080</a></span> | <span class="t">this trajectory is. So I might keep a history of the past, you know, three or four or five predictions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1940" target="_blank">00:32:20.920</a></span> | <span class="t">and know that since they're quite close to each other, maybe that tells me some information about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1944" target="_blank">00:32:24.040</a></span> | <span class="t">the curvature here. And I can use that again, take larger steps. And so that's where we see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1948" target="_blank">00:32:28.600</a></span> | <span class="t">the so-called linear multi-step sampling coming in, just keeping this buffer of past predictions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1953" target="_blank">00:32:33.800</a></span> | <span class="t">to try and do a better job estimating than the simple one-step linear type first order solvers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1960" target="_blank">00:32:40.200</a></span> | <span class="t">Okay, so that's the score-based sampling version. And all of the variance and innovation comes down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1967" target="_blank">00:32:47.160</a></span> | <span class="t">to things like, how can we do this in as few steps as possible? Maybe we have a schedule that says we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1971" target="_blank">00:32:51.880</a></span> | <span class="t">take larger steps at first and then gradually smaller steps as we get closer. There's, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1976" target="_blank">00:32:56.920</a></span> | <span class="t">now some dynamic methods and can we estimate how many steps we need to take, and so on. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1981" target="_blank">00:33:01.720</a></span> | <span class="t">all trying to attack it from this kind of score-based ODE solving framework. But there's another way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1988" target="_blank">00:33:08.360</a></span> | <span class="t">think of this as well. And that's to say, okay, well, I don't really care about solving this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1993" target="_blank">00:33:13.560</a></span> | <span class="t">exact reverse ODE, right? All I care about is that I end up with an image that's on this manifold,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=1999" target="_blank">00:33:19.160</a></span> | <span class="t">like a plausible looking image. And so I have a model that estimates how much noise there is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2005" target="_blank">00:33:25.160</a></span> | <span class="t">right? And if that noise is very small, then that means I've got a good image. And if that noise is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2011" target="_blank">00:33:31.000</a></span> | <span class="t">really large, then that means I've got some work to do. And so this kind of starts bringing up some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2017" target="_blank">00:33:37.640</a></span> | <span class="t">analogies to training neural networks, because in neural networks, we have the space of all possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2022" target="_blank">00:33:42.520</a></span> | <span class="t">parameters. And we're trying to adjust those parameters not to solve the gradient flow equation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2028" target="_blank">00:33:48.040</a></span> | <span class="t">right? Although that's, you know, possible in theory that you might try and do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2032" target="_blank">00:33:52.360</a></span> | <span class="t">We don't care about that, we just want to find a minima, we want to find a point where our loss is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2035" target="_blank">00:33:55.720</a></span> | <span class="t">really good. And so when we're training a neural network, that's exactly what we do. We set up an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2040" target="_blank">00:34:00.040</a></span> | <span class="t">optimizer, and we take some number of steps trying to reduce some loss. And once that loss gets sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2045" target="_blank">00:34:05.880</a></span> | <span class="t">of, you know, levels off, right, reduced over time levels off, okay, cool, I guess we found a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2050" target="_blank">00:34:10.200</a></span> | <span class="t">neural network. And so we can apply that same kind of thinking here to say, all right, I'll start at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2055" target="_blank">00:34:15.560</a></span> | <span class="t">some point. And I'll have an estimate of the gradient, right, like maybe pointing over here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2061" target="_blank">00:34:21.400</a></span> | <span class="t">But remember, that estimate is not very good, just like the first gradients estimated when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2065" target="_blank">00:34:25.960</a></span> | <span class="t">training a neural network are pretty bad, because it's all just these randomly initialized weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2069" target="_blank">00:34:29.400</a></span> | <span class="t">but hopefully it at least points in a useful direction. So then I'll take some step, and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2073" target="_blank">00:34:33.640</a></span> | <span class="t">length of the step, I won't try and do some fancy schedule, I'll just offload this to an sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2078" target="_blank">00:34:38.760</a></span> | <span class="t">off the shelf optimizer, right? So I have some learning rate, maybe something like momentum,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2083" target="_blank">00:34:43.000</a></span> | <span class="t">that determines how big of a step I take. And then I update my prediction, right, take another step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2089" target="_blank">00:34:49.080</a></span> | <span class="t">in that direction, and so on. So now, instead of following a fixed schedule, we can use tricks that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2093" target="_blank">00:34:53.880</a></span> | <span class="t">have been developed for training neural networks, right, adaptive learning rates, momentum,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2097" target="_blank">00:34:57.720</a></span> | <span class="t">weight decay, and so on. And we can apply them back to this kind of sampling case. And so it turns out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2103" target="_blank">00:35:03.160</a></span> | <span class="t">this works okay, I've tried this for stable diffusion, needs some tricks to get it working.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2106" target="_blank">00:35:06.840</a></span> | <span class="t">But it's a slightly different way of thinking about sampling, rather than relying on sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2111" target="_blank">00:35:11.560</a></span> | <span class="t">a hard coded ODE solver that you figured out yourself, just saying, why don't we treat this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2116" target="_blank">00:35:16.040</a></span> | <span class="t">like an optimization problem, where if the model predicts almost no noise, that's good, we're doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2120" target="_blank">00:35:20.840</a></span> | <span class="t">a good job. And if the model predicts lots of noise, then we can use that as a gradient, and take a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2126" target="_blank">00:35:26.040</a></span> | <span class="t">gradient update step according to our optimizer, and try and sort of converge on a good image as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2130" target="_blank">00:35:30.760</a></span> | <span class="t">our output. And this is, you know, you can stop early once your model prediction is sufficiently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2135" target="_blank">00:35:35.800</a></span> | <span class="t">low for the amount of noise, okay, cool, I'm done. And so I found, you know, in 10, 15 steps,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2140" target="_blank">00:35:40.360</a></span> | <span class="t">you can get some pretty good images out. Yeah, so that's a different way of viewing it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2145" target="_blank">00:35:45.320</a></span> | <span class="t">Not so popular at the moment, but maybe, hopefully something we'll see. Yeah, just a different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2150" target="_blank">00:35:50.040</a></span> | <span class="t">framing. And for me, at least that helps me think about what we're actually doing with the samplers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2154" target="_blank">00:35:54.040</a></span> | <span class="t">we try to find a point where the model predicts very little noise. And so starting from a bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2158" target="_blank">00:35:58.280</a></span> | <span class="t">prediction, moving towards it getting better, by looking at this estimated amount of noise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2163" target="_blank">00:36:03.240</a></span> | <span class="t">as our sort of gradient and solving that, just kind of iteratively removing bits at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2168" target="_blank">00:36:08.360</a></span> | <span class="t">So I hope that helps elucidate the different kinds of samplers, and the goal of that whole thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2174" target="_blank">00:36:14.040</a></span> | <span class="t">and also illustrate at least why we don't just do this in a single step, right? Why we need some sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2178" target="_blank">00:36:18.040</a></span> | <span class="t">of iterative approach, otherwise, we'd end up with just very bad blurry predictions. All right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2182" target="_blank">00:36:22.520</a></span> | <span class="t">I hope that helps. Now we're going to head back to the notebook to talk about our final trick of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2186" target="_blank">00:36:26.920</a></span> | <span class="t">guidance. Okay, the final part of this notebook, guidance, how do we add some extra control to this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2196" target="_blank">00:36:36.680</a></span> | <span class="t">generation process, right? So we already have control via the text, and we've seen how we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2201" target="_blank">00:36:41.080</a></span> | <span class="t">modify those embeddings. We have some control via starting at a noisy version of an input image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2206" target="_blank">00:36:46.200</a></span> | <span class="t">rather than pure noise to kind of control the structure. But what if there's something else,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2209" target="_blank">00:36:49.960</a></span> | <span class="t">what if we'd like a particular style, or to enforce that the model looks like some input image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2215" target="_blank">00:36:55.640</a></span> | <span class="t">or maybe sticks to some color palette, it would be nice to have some way to add this additional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2220" target="_blank">00:37:00.280</a></span> | <span class="t">control. And so the way we do this is to look at some loss function on the decoded denoised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2228" target="_blank">00:37:08.360</a></span> | <span class="t">predicted image, right? The predicted denoise needs final output, and use that loss to then update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2235" target="_blank">00:37:15.640</a></span> | <span class="t">the noisy latents as we generate in a direction that tries to reduce that loss. So for demo,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2240" target="_blank">00:37:20.840</a></span> | <span class="t">we're going to make a very simple loss function. I would like the image to be quite blue. And to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2245" target="_blank">00:37:25.240</a></span> | <span class="t">enforce that my error is going to be the difference between the blue channel, right? Red, green, blue,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2249" target="_blank">00:37:29.800</a></span> | <span class="t">blue is the third channel of the color channels, and the difference between the blue channel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2254" target="_blank">00:37:34.040</a></span> | <span class="t">and 0.9. So the closer all the blue values are to 0.9, the lower my error will be. So that's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2259" target="_blank">00:37:39.800</a></span> | <span class="t">to be my kind of guidance loss. And then during sampling here, what I'm going to do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2263" target="_blank">00:37:43.880</a></span> | <span class="t">everything's going to be the same as before. But every few iterations, you could do it every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2270" target="_blank">00:37:50.280</a></span> | <span class="t">iteration, but that's a little slow. So here, every five iterations, I'm going to set requires</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2276" target="_blank">00:37:56.040</a></span> | <span class="t">grad equals true on the latents. I'm then going to compute my predicted denoised version. I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2281" target="_blank">00:38:01.800</a></span> | <span class="t">going to decode that into image space, and then I'm going to calculate my loss using my special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2286" target="_blank">00:38:06.120</a></span> | <span class="t">blue loss and scale it with some scaling factor. Then I'm going to use torch to find the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2292" target="_blank">00:38:12.920</a></span> | <span class="t">of this loss with respect to those latents, those noisy latents. And I'm going to modify them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2298" target="_blank">00:38:18.200</a></span> | <span class="t">right? And I want to reduce the loss. I'm going to subtract here this gradient multiplied by sigma</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2303" target="_blank">00:38:23.640</a></span> | <span class="t">squared because we're going to be working at different noise levels. And so if we run this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2308" target="_blank">00:38:28.120</a></span> | <span class="t">we should see, hopefully, it's going to do that same sort of sampling process as before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2312" target="_blank">00:38:32.520</a></span> | <span class="t">but we also are occasionally modifying our latents by looking at the gradient of the loss with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2317" target="_blank">00:38:37.880</a></span> | <span class="t">to those latents and updating them in a direction that reduces that loss. And sure enough, we get a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2322" target="_blank">00:38:42.680</a></span> | <span class="t">very nice blue picture out. And if I change the scale here down to something lower and run it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2328" target="_blank">00:38:48.760</a></span> | <span class="t">we'll see that scale is lower. So the loss is lower. So our modifications to the latents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2334" target="_blank">00:38:54.600</a></span> | <span class="t">are smaller. We'll see that we get out a much less blue image. There we go. So that's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2341" target="_blank">00:39:01.880</a></span> | <span class="t">default image, very red and dark, because the prompt is just a picture of a campfire.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2346" target="_blank">00:39:06.040</a></span> | <span class="t">But as soon as we add our additional loss, our guidance, we're going to get out something that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2353" target="_blank">00:39:13.240</a></span> | <span class="t">better matches that additional constraint that we've imposed, right? So this is very useful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2358" target="_blank">00:39:18.280</a></span> | <span class="t">not just for making your images blue, but like I said, color palettes or using some classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2362" target="_blank">00:39:22.920</a></span> | <span class="t">model to make it look like a specific class of image or using a model like clip to, again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2368" target="_blank">00:39:28.360</a></span> | <span class="t">associate it with some text. So lots and lots of different things you can do. Now, a few things I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2373" target="_blank">00:39:33.000</a></span> | <span class="t">should note. One, we decoding the image back to image space, calculating our loss and then tracing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2378" target="_blank">00:39:38.280</a></span> | <span class="t">back. That's very computationally intensive compared to just working in latent space. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2384" target="_blank">00:39:44.040</a></span> | <span class="t">we can do that only every fifth operation to reduce the time, but it still is much slower</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2388" target="_blank">00:39:48.840</a></span> | <span class="t">than just your generic sampling. And then also, we're actually still cheating a little bit here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2394" target="_blank">00:39:54.440</a></span> | <span class="t">because what we should do is set requires grad equals true on the latents and then use those to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2399" target="_blank">00:39:59.800</a></span> | <span class="t">make our noise prediction, use that to calculate the denoised version and decode that, calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2405" target="_blank">00:40:05.720</a></span> | <span class="t">our loss and trace back all the way through the decoder and the process and the unit back to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2410" target="_blank">00:40:10.680</a></span> | <span class="t">latents, right? The reason I'm not doing that is because that takes a lot of memory. So you'll see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2415" target="_blank">00:40:15.960</a></span> | <span class="t">for example, like the clip guided diffusion notebook from the hugging face examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2420" target="_blank">00:40:20.760</a></span> | <span class="t">they do it that way, but they have to use tricks like gradient checkpointing and so on to kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2424" target="_blank">00:40:24.360</a></span> | <span class="t">keep the RAM usage under control. And for simple losses, it works fine to do it this way, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2429" target="_blank">00:40:29.400</a></span> | <span class="t">now we just tracing back through denoised latents is equal to latents minus sigma times this noise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2434" target="_blank">00:40:34.280</a></span> | <span class="t">prediction, right? So we don't have to trace any gradients back through the unit. But if you wanted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2438" target="_blank">00:40:38.680</a></span> | <span class="t">to get more accurate gradients, maybe it's not working as well as you'd hoped, you can do it that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2443" target="_blank">00:40:43.240</a></span> | <span class="t">other way that I described. But however you do it, very, very powerful technique,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2447" target="_blank">00:40:47.480</a></span> | <span class="t">fun to be able to again inject some additional control into this generation process by crafting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2452" target="_blank">00:40:52.440</a></span> | <span class="t">a loss that expresses exactly what you'd like to see. All right, that's the end of the notebook</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2457" target="_blank">00:40:57.560</a></span> | <span class="t">for now. If you have any questions, feel free to reach out to me, I'll be on the forums and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0_BBRNYInx8&t=2461" target="_blank">00:41:01.800</a></span> | <span class="t">you can find me on Twitter and so on. But for now, enjoy and I can't wait to see what you make.</span></div></div></body></html>