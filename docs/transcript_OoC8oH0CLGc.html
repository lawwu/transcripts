<html><head><title>MIT Human-Centered Autonomous Vehicle</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>MIT Human-Centered Autonomous Vehicle</h2><a href="https://www.youtube.com/watch?v=OoC8oH0CLGc" target="_blank"><img src="https://i.ytimg.com/vi_webp/OoC8oH0CLGc/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=OoC8oH0CLGc&t=0 target="_blank"">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=OoC8oH0CLGc&t=69 target="_blank"">1:9</a> Description of components<br><a href="https://www.youtube.com/watch?v=OoC8oH0CLGc&t=189 target="_blank"">3:9</a> Tweeting<br><a href="https://www.youtube.com/watch?v=OoC8oH0CLGc&t=356 target="_blank"">5:56</a> Pedestrian<br><a href="https://www.youtube.com/watch?v=OoC8oH0CLGc&t=413 target="_blank"">6:53</a> Outro<br><h3>Transcript</h3><div class='max-width'><p>This is the Human-Centered Autonomous Vehicle. One of the main ideas underlying our work is that solving the task of autonomous driving is more complicated and more fascinating than the strictly robotics challenges of localization, mapping, perception, control, and planning. You also have to enable the vehicle to perceive, predict, communicate, and collaborate with human beings.</p><p>The humans inside the car, like the driver and the passengers, and the humans outside the car, like the pedestrians, cyclists, the drivers of other vehicles, and even teleoperators. The studies, the code, the data, and the demos we release all consider autonomous driving in this kind of human-centered way, where the control is transferred from human to machine and back to human based on the state of the external driving environment and the state of the driver.</p><p>What we'd like to demonstrate today is the basics, voice-based transfer of control from human to machine based on whether the driver is paying attention to the road or not. Inside, we have two cameras on the driver, one on the driver's face, one on the driver's body. We have two cameras looking at the external roadway, and we have a few other cameras for filming purposes.</p><p>There's a center stack display showing who's in control of the vehicle, human or machine. So currently, the human is in control of the vehicle. Let's drive. Split the car and drive. On the center stack display, it shows the gear as drive. The perception, control, and driver state sensing algorithms you see today are running in real time, but the visualizations you're seeing in video are done in offline post-processing.</p><p>Our perception system today is vision-based using two neural networks. One is doing road segmentation, the other is doing object detection of vehicles, cyclists, pedestrians, traffic signs, traffic lights. The acceleration, braking, and steering of the car is performed by PID controllers. The driver state sensing that we're showing today is glance region classification, and that's performed using 3D convolutional neural networks.</p><p>High-level planning decisions to transfer control or to stop the vehicle are performed by a decision fusion algorithm that combines risk factors in the external environment and driver state, whether the driver's paying attention to the road or not. Safety for us is the number one priority, always. We are on a test track.</p><p>The vehicles and pedestrians here today are all part of our team, all part of the demonstration. There's another safety driver in the car that can stop the vehicle at any moment by pressing a single button. Okay, let's engage in a distracting activity, Twitter, and let's send a tweet. (silence) I'm typing this tweet while driving in the MIT city center.</p><p>In the MIT semi-autonomous vehicle on a test track. (silence) A test track. Lex, you appear distracted. Would you like me to take over? Yes, please. Great. I am taking control of steering and braking. (silence) The car is now in control as the center stack display shows. So I will continue with the tweet.</p><p>The car knows that I'm not paying attention and has taken control after asking me nicely for it. (silence) Video out tomorrow. Okay, here goes nothing. It's posted. Very well might be the first tweet ever sent from an autonomous vehicle while it's driving itself. (silence) (car engine roaring) Elevated driving risk detected.</p><p>I am stopping for a pedestrian. Lex, pedestrian is blocking our lane of travel. Should I honk? No, please shift gear to park. Great. We are now in park. (silence) That was a demo of the basics. Perception, motion planning, driver state sensing, transfer control and tweeting. And we have a lot more to explore together.</p><p>Our team is working on various aspects of human centered artificial intelligence toward our mission to save lives through effective human robot collaboration. (silence) (silence) (silence) (silence) (silence) (silence) (silence) (silence)</p></div></div></body></html>