<html><head><title>Hacking the Inference Pareto Frontier - Kyle Kranen, NVIDIA</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Hacking the Inference Pareto Frontier - Kyle Kranen, NVIDIA</h2><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc"><img src="https://i.ytimg.com/vi_webp/Y2qc0UhDSnc/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=0">0:0</a> Introduction to Breaking the Inference Pareto Frontier<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=33">0:33</a> Introduction of Kyle Cranon and NVIDIA Dynamo<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=91">1:31</a> The Three Pillars of Deployment (Quality, Latency, Cost)<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=131">2:11</a> Understanding the Pareto Frontier<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=186">3:6</a> Application-Specific Prioritization of Quality, Latency, and Cost<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=272">4:32</a> Common Techniques to Manipulate the Pareto Frontier (Quantization, RAG, Reasoning)<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=319">5:19</a> Compounding Techniques<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=364">6:4</a> Three Drivers for Modifying the Pareto Frontier (Scale, Structure, Dynamism)<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=380">6:20</a> Scale: Disaggregation<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=662">11:2</a> Scale: Routing<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=780">13:0</a> Structure: Inference Time Scaling<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=974">16:14</a> Structure: KV Manipulation<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1063">17:43</a> Dynamism: Worker Specialization<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1122">18:42</a> Dynamism: Dynamic Load Balancing<br><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1195">19:55</a> Conclusion and NVIDIA Dynamo Resources<br><br><div style="text-align: left;"><a href="./Y2qc0UhDSnc.html">Whisper Transcript</a> | <a href="./transcript_Y2qc0UhDSnc.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=0" target="_blank">00:00:00.040</a></span> | <span class="t">Hey there, everyone. I'm Kyle Cranin and today I'll be talking about how to break the inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=18" target="_blank">00:00:18.760</a></span> | <span class="t">Pareto frontier in your advantage. Really, the thing that enables the success is that a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=25" target="_blank">00:00:25.240</a></span> | <span class="t">model and a good system that takes into account the actual constraints for what you need from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=30" target="_blank">00:00:30.460</a></span> | <span class="t">your deployment is actually key to the success of both your deployment and the application</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=35" target="_blank">00:00:35.260</a></span> | <span class="t">that is backed by it. So, who am I and why am I talking about this? As I said, my name</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=42" target="_blank">00:00:42.120</a></span> | <span class="t">is Kyle Cranin. Currently, I work at NVIDIA. Previously at NVIDIA, I was leading and GMing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=50" target="_blank">00:00:50.680</a></span> | <span class="t">the largest inference deployment at NVIDIA with a multiple tens of millions dollar quarterly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=56" target="_blank">00:00:56.080</a></span> | <span class="t">cloud bill. And now I'm an architect and lead for a project that we just released an open</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=61" target="_blank">00:01:01.420</a></span> | <span class="t">source called NVIDIA Dynamo that aims to do things like enable data center scale inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=67" target="_blank">00:01:07.300</a></span> | <span class="t">to manipulate your deployment and manipulate the Pareto frontier in order to achieve better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=72" target="_blank">00:01:12.020</a></span> | <span class="t">SLAs or achieve lower costs for existing SLAs with techniques like disaggregation or more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=77" target="_blank">00:01:17.880</a></span> | <span class="t">techniques that we'll talk about later in the talk. Dynamo is... The Dynamo meetup is linked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=83" target="_blank">00:01:23.600</a></span> | <span class="t">right here. You can learn more about Dynamo there if you want to look it up. And I'll also have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=87" target="_blank">00:01:27.240</a></span> | <span class="t">that at the end of the talk as well. So, the three things that we or I like to think about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=93" target="_blank">00:01:33.620</a></span> | <span class="t">when I'm thinking about whether or not something can actually be deployed and used is really simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=99" target="_blank">00:01:39.020</a></span> | <span class="t">It's quality. Whether or not your application and the system around your model is capable of, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=104" target="_blank">00:01:44.420</a></span> | <span class="t">completing tasks with some level of accuracy or quality. Latency. Whether or not the task can be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=110" target="_blank">00:01:50.720</a></span> | <span class="t">completed in a fast enough envelope for, you know, either the user to be happy or to meet safety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=115" target="_blank">00:01:55.520</a></span> | <span class="t">guarantees like for robotics. And cost. Can the LLM complete the task cheaply enough per request in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=123" target="_blank">00:02:03.200</a></span> | <span class="t">order for you to meet whatever, you know, margin requirements you have for your application.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=127" target="_blank">00:02:07.160</a></span> | <span class="t">And one of the ways that we generally compare these three things is through Pareto frontier. Now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=134" target="_blank">00:02:14.720</a></span> | <span class="t">the frontier I'm showing here is two-dimensional. It's actually really hard to plot things in 3D on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=139" target="_blank">00:02:19.700</a></span> | <span class="t">a 2D slide. So, I'm gonna just show two dimensions. Really what this looks like is you have this, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=144" target="_blank">00:02:24.600</a></span> | <span class="t">edge. We have this edge that sort of represents the best or to the top and rightmost points that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=155" target="_blank">00:02:35.700</a></span> | <span class="t">achieve for, you know, a specific set of attributes. So, in this case, we have the TPS per GPU, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=162" target="_blank">00:02:42.700</a></span> | <span class="t">effectively a cost metric. How many requests can you handle per GPU per second? And the user TPS,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=168" target="_blank">00:02:48.000</a></span> | <span class="t">which is a responsiveness metric, right? So, this is the latency versus the cost. And for different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=173" target="_blank">00:02:53.340</a></span> | <span class="t">applications, really, you want to enable your Pareto front -- enable -- you actually really only want one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=180" target="_blank">00:03:00.540</a></span> | <span class="t">point on the Pareto frontier. It's right. What is your operating latency? What is the operating quality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=184" target="_blank">00:03:04.440</a></span> | <span class="t">you need? And how can you minimize cost for that? Now, this really actually depends on the application</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=189" target="_blank">00:03:09.960</a></span> | <span class="t">you're talking about. So, one of the most important things you're doing when you're thinking about breaking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=194" target="_blank">00:03:14.460</a></span> | <span class="t">the Pareto frontier is you're thinking about your application. So, for example, if we're talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=198" target="_blank">00:03:18.840</a></span> | <span class="t">about personal cancer cures, which is a topic that's talked about a lot in the context of generative AI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=204" target="_blank">00:03:24.060</a></span> | <span class="t">in that situation, latency and cost are pretty much no object, right? You could spend millions of dollars</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=210" target="_blank">00:03:30.940</a></span> | <span class="t">on proving out a single cure. And if it works, the return on investment is so high that it doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=216" target="_blank">00:03:36.140</a></span> | <span class="t">really matter. To take a different example, tab completion like those that you see in popular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=222" target="_blank">00:03:42.380</a></span> | <span class="t">IDEs, like cursor, all are very, very dependent upon snappiness. The user expects that when they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=228" target="_blank">00:03:48.540</a></span> | <span class="t">press tab, they will see a recommendation for the next line or the next set of, you know, tokens very,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=234" target="_blank">00:03:54.780</a></span> | <span class="t">very quickly. And then to take another code example, with respect to async code commits, things like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=241" target="_blank">00:04:01.100</a></span> | <span class="t">you know, cursors, what's it called? It's agent mode. And, you know, other applications where the chatbot or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=251" target="_blank">00:04:11.900</a></span> | <span class="t">applications working next to the user, there's not as much a consideration for latency, but there is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=257" target="_blank">00:04:17.100</a></span> | <span class="t">a concern for both quality and cost. And this sort of breaks down, or this sort of depends upon what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=265" target="_blank">00:04:25.020</a></span> | <span class="t">the user expects from the application. Does it, do they expect it to be fast? Do they expect it to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=270" target="_blank">00:04:30.780</a></span> | <span class="t">slow? Are they involved in the loop with this application? Now, there are a series of, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=277" target="_blank">00:04:37.180</a></span> | <span class="t">techniques that are pretty commonly known about that all, you know, support the manipulation of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=281" target="_blank">00:04:41.980</a></span> | <span class="t">frontier. For example, quantization speeds up your latency and it also decreases your cost because you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=287" target="_blank">00:04:47.100</a></span> | <span class="t">can produce higher batch sizes. Retrieval augmented generation generally slows down your application,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=292" target="_blank">00:04:52.300</a></span> | <span class="t">makes it higher latency, increases the cost, but also increases the quality. And reasoning, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=298" target="_blank">00:04:58.220</a></span> | <span class="t">similar, you know, you produce more tokens to think. And changing the model config allows you to do any of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=303" target="_blank">00:05:03.340</a></span> | <span class="t">these things. If you change how the model is represented in a parallel manner, you can significantly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=308" target="_blank">00:05:08.700</a></span> | <span class="t">change the characteristics of speed, cost, and theoretically quality if you're talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=313" target="_blank">00:05:13.980</a></span> | <span class="t">non-haloed context parallelism. The thing that I want to impart upon you before we jump into, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=319" target="_blank">00:05:19.740</a></span> | <span class="t">a lot more of these advanced techniques is that these techniques can be compounded. So, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=323" target="_blank">00:05:23.980</a></span> | <span class="t">if you have an initial application and has some required performance, you can actually stack,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=328" target="_blank">00:05:28.860</a></span> | <span class="t">for example, retrieval augmented generation in order to increase the quality, but make the latency worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=334" target="_blank">00:05:34.140</a></span> | <span class="t">And you can also stack on top of that quantization of the model in order to speed up your latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=340" target="_blank">00:05:40.620</a></span> | <span class="t">The point I'm, you know, trying to make here is that you really have this toolbox of sets of large sets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=347" target="_blank">00:05:47.500</a></span> | <span class="t">of tools that you can use together. And the tools themselves are not independent and can be combined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=353" target="_blank">00:05:53.020</a></span> | <span class="t">in very sometimes non-obvious ways in order to actually break your Pareto frontier or squeeze it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=359" target="_blank">00:05:59.740</a></span> | <span class="t">in different directions in order to support your application. So, there are three things outside of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=365" target="_blank">00:06:05.820</a></span> | <span class="t">those techniques that I tend to think drive, you know, how you can modify the Pareto frontier going forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=376" target="_blank">00:06:16.300</a></span> | <span class="t">Those three are scale, structure, and dynamism. So, one of the things that is, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=382" target="_blank">00:06:22.860</a></span> | <span class="t">really relevant in the realm of scale is disaggregation. So, for those that aren't aware,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=389" target="_blank">00:06:29.420</a></span> | <span class="t">KV caching is a technique by which you take the key and value vectors that are associated with each token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=397" target="_blank">00:06:37.100</a></span> | <span class="t">and you cache them so that when you're doing autoregressive generation, you don't have to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=401" target="_blank">00:06:41.740</a></span> | <span class="t">the entire set of key and value vectors for the entire sequence up to this point. You can just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=406" target="_blank">00:06:46.140</a></span> | <span class="t">generate new ones and put them back into the KV cache. What this actually means is that we effectively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=412" target="_blank">00:06:52.460</a></span> | <span class="t">have two phases of generation. One in which you're generating the pre-fill or filling up your KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=419" target="_blank">00:06:59.500</a></span> | <span class="t">and one in which you're actually generating new KV cache as well as new tokens and producing output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=425" target="_blank">00:07:05.500</a></span> | <span class="t">Now, disaggregation as a technique basically allows you to have these, you know, two phases which were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=432" target="_blank">00:07:12.940</a></span> | <span class="t">typically used on the same set of GPUs onto multiple different workers and sets of GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=439" target="_blank">00:07:19.500</a></span> | <span class="t">And this provides a couple of key benefits that we'll go into right now. The three really big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=445" target="_blank">00:07:25.260</a></span> | <span class="t">benefits here are that you can really now take two sort of phases that have very different needs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=452" target="_blank">00:07:32.380</a></span> | <span class="t">Pre-fill is very compute bound and decode depending on that application and model can be very memory bound.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=458" target="_blank">00:07:38.620</a></span> | <span class="t">And it allows you to do a granular load matching between those two phases. And what this means is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=463" target="_blank">00:07:43.420</a></span> | <span class="t">you know, compute saturates relatively early to use DeepSea as an example. Compute saturates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=469" target="_blank">00:07:49.420</a></span> | <span class="t">relatively early and you may use relatively few GPUs for your pre-fill instances and have them handle a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=475" target="_blank">00:07:55.980</a></span> | <span class="t">lower batch size. But handle a much larger batch size with many more GPUs for your decode instances.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=482" target="_blank">00:08:02.700</a></span> | <span class="t">And this split and this heterogeneity between the two actually allows you to produce far more performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=489" target="_blank">00:08:09.500</a></span> | <span class="t">The other thing is that, you know, one of the problems is that if you have in-flight batching,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=496" target="_blank">00:08:16.620</a></span> | <span class="t">you have many tokens coming in at the same time that are in different phases of generation, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=500" target="_blank">00:08:20.860</a></span> | <span class="t">if you have a request that's doing pre-fill and a request that's doing decode on the same machine,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=504" target="_blank">00:08:24.300</a></span> | <span class="t">you get scheduling conflicts and the scheduler basically has to decide whether or not it handles</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=508" target="_blank">00:08:28.380</a></span> | <span class="t">new tokens. There's some techniques to handle this, like in-flight batching and chunk piggybacking,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=513" target="_blank">00:08:33.820</a></span> | <span class="t">or sorry, chunk piggybacking. But generally, there is a cost to doing that mutual scheduling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=519" target="_blank">00:08:39.340</a></span> | <span class="t">So splitting this out makes the scheduling simpler. Now, there's an asterisk to this, which is that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=524" target="_blank">00:08:44.060</a></span> | <span class="t">sorry, really quickly, and I'll go over the performance numbers. I'm going to use Llama70B as an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=528" target="_blank">00:08:48.620</a></span> | <span class="t">example. Right here, we have on our left axis, or our Y axis, we have the tokens per second per GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=534" target="_blank">00:08:54.540</a></span> | <span class="t">On our X axis, we have the tokens per second per user, right? Up and to the right is better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=539" target="_blank">00:08:59.020</a></span> | <span class="t">If we choose one operating point at latency, disaggregating on the same number of GPUs, 16 total H100s,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=545" target="_blank">00:09:05.820</a></span> | <span class="t">we can achieve up to two times the tokens per second per GPU at a fixed latency, which means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=552" target="_blank">00:09:12.300</a></span> | <span class="t">you're now paying two times less for your application. There are some constraints, though.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=558" target="_blank">00:09:18.460</a></span> | <span class="t">The use case really does dictate performance for disaggregation. For example, low input length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=565" target="_blank">00:09:25.500</a></span> | <span class="t">use cases have little to no speed up, because you don't actually have as much of the scheduling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=571" target="_blank">00:09:31.180</a></span> | <span class="t">problem. They're very pre-filled light, so you're basically just doing decode the entire time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=574" target="_blank">00:09:34.300</a></span> | <span class="t">And then, per the graph, disaggregation, and I'll go back to the graph, actually, disaggregation is useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=581" target="_blank">00:09:41.820</a></span> | <span class="t">usually in the middle of the graph. In very high latency, high throughput scenarios, which would be the left and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=587" target="_blank">00:09:47.900</a></span> | <span class="t">top of the graph. And low latency, low throughput scenarios, the bottom right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=592" target="_blank">00:09:52.220</a></span> | <span class="t">aggregated tends to reconverge with disaggregated and produce a little bit more performance in those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=598" target="_blank">00:09:58.620</a></span> | <span class="t">cases. That being said, for a lot of user, you know, interactive applications, disaggregation makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=605" target="_blank">00:10:05.980</a></span> | <span class="t">the most sense because users tend to read that between or care about things in the realm of 20 to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=613" target="_blank">00:10:13.100</a></span> | <span class="t">200 tokens per second. The other thing that's kind of a caveat about this is that configuration is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=620" target="_blank">00:10:20.140</a></span> | <span class="t">really important. Since you're separating these two phases into pre-fill and generation, the balance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=625" target="_blank">00:10:25.820</a></span> | <span class="t">between the number of workers for pre-fill and decoding dictates the performance. So, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=630" target="_blank">00:10:30.860</a></span> | <span class="t">if you have too many decode workers, you're basically gonna have decode workers that are starving for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=636" target="_blank">00:10:36.460</a></span> | <span class="t">work. And if you pre-fill workers, they're going to be generating work for the decode workers, and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=641" target="_blank">00:10:41.500</a></span> | <span class="t">decode workers are gonna be being pushed down by, you know, just an increasing amount of load and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=646" target="_blank">00:10:46.540</a></span> | <span class="t">increasing queue depth. And the other thing is that modifying this is kind of expensive and hard,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=653" target="_blank">00:10:53.580</a></span> | <span class="t">because the balance between pre-fill and decode depends on the parallel configs of each. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=658" target="_blank">00:10:58.060</a></span> | <span class="t">it's like this really wide configuration space. One other thing that we talk about with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=663" target="_blank">00:11:03.580</a></span> | <span class="t">scale is routing. So, we talked about how this KV is important. One of the things that we have to do for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=669" target="_blank">00:11:09.900</a></span> | <span class="t">pre-fill decode disaggregation is that we need to actually transfer the KV between machines. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=676" target="_blank">00:11:16.060</a></span> | <span class="t">in some sense, there's actually an affinity for some machines to do some work, since the KV cache of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=681" target="_blank">00:11:21.820</a></span> | <span class="t">previous request is actually stored on those GPUs or offloaded onto system memory, host, or external</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=689" target="_blank">00:11:29.580</a></span> | <span class="t">storage during the course of inference. So, I actually labeled this wrong. This is not the smart router.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=697" target="_blank">00:11:37.260</a></span> | <span class="t">This -- in a naive case, you would route pretty much exclusively randomly, right? Or towards, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=708" target="_blank">00:11:48.860</a></span> | <span class="t">anything. So, in this case, we're biasing towards -- we're not biasing towards anything. We're sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=714" target="_blank">00:11:54.300</a></span> | <span class="t">randomly. Alternatively, you know, if we're talking about this -- if routing to worker three in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=718" target="_blank">00:11:58.620</a></span> | <span class="t">case, it could also be that you're optimizing for purely your KV match. If you're doing -- this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=725" target="_blank">00:12:05.420</a></span> | <span class="t">actually inverted -- if you're doing a KV-based router, you may end up biasing towards machines that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=733" target="_blank">00:12:13.580</a></span> | <span class="t">have too high KV load and, therefore, are not going to be able to handle the request and you end up with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=736" target="_blank">00:12:16.780</a></span> | <span class="t">queueing. In a smart case, you actually want to minimize the -- sort of this cost function that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=743" target="_blank">00:12:23.260</a></span> | <span class="t">includes both the amount of prefix match that you can get from -- or maximize the prefix match that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=749" target="_blank">00:12:29.580</a></span> | <span class="t">can get from the work that's already been done on that node and the amount of load that already exists</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=755" target="_blank">00:12:35.660</a></span> | <span class="t">on that node. And as you scale out, as you get more and more GPUs in a deployment, you actually end up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=764" target="_blank">00:12:44.300</a></span> | <span class="t">with more and more represented KV space that's local to those machines. And because of that, having a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=770" target="_blank">00:12:50.540</a></span> | <span class="t">larger and larger deployment means that you get an asymptotically increasing KV cache hit rate, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=776" target="_blank">00:12:56.220</a></span> | <span class="t">means that you're doing less and less pre-fill work over time. So routing, you know, here to give it a report</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=783" target="_blank">00:13:03.660</a></span> | <span class="t">card, increases your speed and cost, and doesn't really have an effect upon quality because it's -- it's doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=788" target="_blank">00:13:08.460</a></span> | <span class="t">the same work that it would normally do. Now, we talk about structure. Structure is really important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=794" target="_blank">00:13:14.380</a></span> | <span class="t">because we have a lot of these workloads that you guys have probably seen at the AI Engineers World Fair, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=799" target="_blank">00:13:19.180</a></span> | <span class="t">agents, for example. Agents impart a structure on the workload in that they have moderately predictable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=806" target="_blank">00:13:26.060</a></span> | <span class="t">usage patterns between concurrent requests. So an example here is inference time scaling. This is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=810" target="_blank">00:13:30.940</a></span> | <span class="t">cool graph I'll go over really quickly. For example, we have three models here. In green, we have an 8b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=815" target="_blank">00:13:35.820</a></span> | <span class="t">model. In yellow, we have a 49b model. And in red, we have a 235b model. We find that with inference time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=822" target="_blank">00:13:42.380</a></span> | <span class="t">scaling -- that is to re-query the model and to, you know, prompt it to reconsider its results or reason more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=827" target="_blank">00:13:47.740</a></span> | <span class="t">about its results, we can produce better and better results. And you actually see this really interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=832" target="_blank">00:13:52.140</a></span> | <span class="t">trend where with about three or four times of re-querying, we can see that the 8b model is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=840" target="_blank">00:14:00.140</a></span> | <span class="t">on par with respect to quality as the 49b model. And the 49b is almost on par with respect to quality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=846" target="_blank">00:14:06.860</a></span> | <span class="t">as the 235b model. And we note here that, like, the cost of querying that 8b model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=856" target="_blank">00:14:16.620</a></span> | <span class="t">you know, even querying it multiple times is actually lower than querying the larger model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=864" target="_blank">00:14:24.460</a></span> | <span class="t">right? And in this sense, you know, we basically see that inference time scaling can be considered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=871" target="_blank">00:14:31.180</a></span> | <span class="t">sort of as, you know, increasing quality at the cost of speed and at the cost of cost, right? Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=877" target="_blank">00:14:37.820</a></span> | <span class="t">you're re-querying it. But alternatively, if you keep quality fixed, you can basically get lower</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=886" target="_blank">00:14:46.700</a></span> | <span class="t">latency and lower cost by using a smaller model and re-querying it multiple times. And the structure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=893" target="_blank">00:14:53.340</a></span> | <span class="t">that we infer from doing that re-querying allows us to do better scheduling. So in this graph, we have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=899" target="_blank">00:14:59.660</a></span> | <span class="t">series of curves that represent basically the runtime of a given reasoning example from the natural plan</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=906" target="_blank">00:15:06.540</a></span> | <span class="t">dataset. And the -- basically the concurrency. So this is, like, a graph of, like, how many you can -- how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=914" target="_blank">00:15:14.140</a></span> | <span class="t">many concurrent instances you can run at once. And we sample across all this concurrency. We see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=920" target="_blank">00:15:20.780</a></span> | <span class="t">implanting disaggregation gives us a small benefit, mostly because this dataset is very ISL -- short ISL,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=926" target="_blank">00:15:26.940</a></span> | <span class="t">long OSL. You don't get a whole ton of benefit from disaggregation. In this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=930" target="_blank">00:15:30.860</a></span> | <span class="t">basically making -- you know, removing a round trip by making the re-queries come from the router</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=938" target="_blank">00:15:38.060</a></span> | <span class="t">instead of coming from the user or the client on the outside, allows you to really decrease these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=943" target="_blank">00:15:43.340</a></span> | <span class="t">round trips with respect to latency. And then on top of that, making the router aware and making the LLM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=948" target="_blank">00:15:48.300</a></span> | <span class="t">scheduler aware that you are doing, you know, repeat work, you're re-querying it, actually gives you an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=954" target="_blank">00:15:54.860</a></span> | <span class="t">increased benefit that is the red line to the green line. That is to say, amongst a wide variety of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=960" target="_blank">00:16:00.700</a></span> | <span class="t">models, if we assume that the quality is fixed, we can actually use inference time scaling and some smart</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=966" target="_blank">00:16:06.780</a></span> | <span class="t">techniques in order to significantly decrease latency and increase throughput while maintaining the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=971" target="_blank">00:16:11.260</a></span> | <span class="t">quality. One last thing, and I'm going to go through this really quick because I'm getting low on time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=976" target="_blank">00:16:16.860</a></span> | <span class="t">is manipulating K and V values, right? We've sort of talked about how before there's this work that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=982" target="_blank">00:16:22.620</a></span> | <span class="t">we do in pre-fill that we don't want to lose. We do routing to ensure that we don't lose this KV.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=989" target="_blank">00:16:29.500</a></span> | <span class="t">And, you know, if we have, like, a workflow where we know the run times of things -- so, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=995" target="_blank">00:16:35.900</a></span> | <span class="t">if we do a, you know, a tool call, for example, and we know this tool call takes a moderately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1003" target="_blank">00:16:43.980</a></span> | <span class="t">deterministic amount of time, we basically end up with this KV eviction, right? If we have a tool call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1008" target="_blank">00:16:48.860</a></span> | <span class="t">that takes 30 seconds, the KV is going to be swept out from HBM and you're not going to be able to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1013" target="_blank">00:16:53.500</a></span> | <span class="t">it in the future because it's no longer being cached. But if we know that it's going to be used again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1019" target="_blank">00:16:59.260</a></span> | <span class="t">why not just offload it, right? Basically, inference time scaling gives you structure to manipulate your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1025" target="_blank">00:17:05.900</a></span> | <span class="t">KV. Tool calling gives you structure to manipulate your KV. So, instead of doing another pre-fill</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1030" target="_blank">00:17:10.860</a></span> | <span class="t">the second time, you might, for example, do pre-fill once, do, you know, the LM call, the decode once,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1038" target="_blank">00:17:18.940</a></span> | <span class="t">move it to host memory, and then, you know, at the time at which you expect the tool to complete,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1044" target="_blank">00:17:24.140</a></span> | <span class="t">you move it right back into GPU memory so it's ready for the next LM call that will include this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1048" target="_blank">00:17:28.860</a></span> | <span class="t">added context from the tool. So, KV manipulation, you know, again, increases your speed and decreases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1055" target="_blank">00:17:35.580</a></span> | <span class="t">your cost while also, you know, improving your quality. Or not improving quality, keeping quality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1060" target="_blank">00:17:40.700</a></span> | <span class="t">constant. The last thing that we have to talk about here is dynamism. Worker specialization is really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1066" target="_blank">00:17:46.380</a></span> | <span class="t">important. As I said, since you have different characteristics of disaggregation at different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1072" target="_blank">00:17:52.300</a></span> | <span class="t">input sequence lengths and output sequence lengths, you actually want to have a mix of aggregated and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1076" target="_blank">00:17:56.940</a></span> | <span class="t">disaggregated workers based on where you are in the OSL/ISL histogram. So, at, you know, lower input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1083" target="_blank">00:18:03.740</a></span> | <span class="t">sequence lengths and higher output sequence lengths, you might want to do aggregated with a higher tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1087" target="_blank">00:18:07.260</a></span> | <span class="t">parallelism. In the middle of the range, you may want to use... In the middle of the input sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1091" target="_blank">00:18:11.180</a></span> | <span class="t">range, you may want to use disaggregated. And in the, you know, long context, you know, regime,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1098" target="_blank">00:18:18.860</a></span> | <span class="t">you may want to use disaggregated with context parallelism. Now, again, this differs model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1103" target="_blank">00:18:23.980</a></span> | <span class="t">model. And this is just an exemplary graph. But generally, if you specialize workers, you can also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1111" target="_blank">00:18:31.180</a></span> | <span class="t">increase... Increase your speed, decrease your cost while keeping quality the same. Because, again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1115" target="_blank">00:18:35.660</a></span> | <span class="t">you're not actually touching the execution... What the model is executing. You're not touching the math it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1120" target="_blank">00:18:40.220</a></span> | <span class="t">doing. One last thing about dynamism is load balance is quite important. As I mentioned earlier, doing...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1128" target="_blank">00:18:48.220</a></span> | <span class="t">Looking at the amount of P and D workers is really important to determine whether or not your disaggregated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1134" target="_blank">00:18:54.620</a></span> | <span class="t">deployment is going to be successful. So, for example, if you have a histogram that you initially create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1138" target="_blank">00:18:58.780</a></span> | <span class="t">your configuration based off of, for example, if you have app A and app B that have, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1143" target="_blank">00:19:03.100</a></span> | <span class="t">these two input sequence length and output sequence lengths, you may end up with a scenario where a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1148" target="_blank">00:19:08.540</a></span> | <span class="t">change in user distribution causes significant issues with your deployment. Your dec... In this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1154" target="_blank">00:19:14.780</a></span> | <span class="t">by... When you increase your input sequence length and output sequence length by a little bit more,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1159" target="_blank">00:19:19.900</a></span> | <span class="t">your input sequence length, you might create more demand for prefill workers than you do for decode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1164" target="_blank">00:19:24.780</a></span> | <span class="t">workers. So, your balance will change over time. And this has been empirically proven by a wide variety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1170" target="_blank">00:19:30.540</a></span> | <span class="t">of people that publish data. And you actually have to do auto-scaling across these two types of instances</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1179" target="_blank">00:19:39.500</a></span> | <span class="t">in real time to account for changes in user usage distribution of your platform. So, in this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1186" target="_blank">00:19:46.140</a></span> | <span class="t">dynamic load balancing increases your speed and keeps your costs low. But mostly, it's really just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1193" target="_blank">00:19:53.100</a></span> | <span class="t">essential to ensuring that disaggregation actually works to maximum potential.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1198" target="_blank">00:19:58.940</a></span> | <span class="t">Okay. Last things. Here is the Dynamo repo. It's right here. It's github.com/ai-dynamo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1207" target="_blank">00:20:07.180</a></span> | <span class="t">We also have a Dynamo meetup that is being hosted tomorrow, Thursday from 5:00 to 8:00 PM here in San</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1212" target="_blank">00:20:12.540</a></span> | <span class="t">Francisco. Please come. We're going to be talking a lot more about how we actually implement these things at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Y2qc0UhDSnc&t=1217" target="_blank">00:20:17.420</a></span> | <span class="t">the event.</span></div></div></body></html>