<html><head><title>LangChain Interrupt 2025   Agent Evaluations – Harrison Chase</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>LangChain Interrupt 2025   Agent Evaluations – Harrison Chase</h2><a href="https://www.youtube.com/watch?v=bQPxekLApxw"><img src="https://i.ytimg.com/vi/bQPxekLApxw/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./bQPxekLApxw.html">Whisper Transcript</a> | <a href="./transcript_bQPxekLApxw.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">All right, we're going to go all the way back. This is time travel. It's a feature in lane graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=13" target="_blank">00:00:13.000</a></span> | <span class="t">All right. So, there we go. So, we ran a survey of agent builders about six months ago where we asked them what was the biggest blocker for getting more agents into production.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=24" target="_blank">00:00:24.000</a></span> | <span class="t">The number one thing that they cited by far was quality. So, we talked a little bit about the trade-offs between quality and latency, and quality is still the top thing in blocking people getting to production.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=37" target="_blank">00:00:37.000</a></span> | <span class="t">And in order to kind of like bridge that gap from prototype to production and increase that quality, one of the techniques that we've seen people adopt is eval-driven development.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=48" target="_blank">00:00:48.000</a></span> | <span class="t">So, using evals throughout a bunch of different stages of development to measure your app's performance and then make sure that you're constantly climbing that ladder of performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=59" target="_blank">00:00:59.000</a></span> | <span class="t">And one of the things that I want to emphasize is that evals is really a continuous journey.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=64" target="_blank">00:01:04.000</a></span> | <span class="t">So, there's three different types of evals that we see people running. Most people are thinking about evals maybe in one, maybe two of these, but we think it's a continuous journey throughout the whole lifecycle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=75" target="_blank">00:01:15.000</a></span> | <span class="t">So, what do I mean by that? First, let's talk about offline evals. This is probably what most people think of when they say or hear the word evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=84" target="_blank">00:01:24.000</a></span> | <span class="t">So, this is before you go into production, you get some data set, you run your app against that data set, and then you measure performance using some evaluators to score it, and you can track this performance over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=94" target="_blank">00:01:34.000</a></span> | <span class="t">You can compare different models, different prompts, things like that, and get a sense for whether the changes you're making are actually increasing or decreasing your app's performance on this data set that you've constructed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=106" target="_blank">00:01:46.000</a></span> | <span class="t">Of course, that data set isn't perfect, and so there's another type of eval called online eval that you commonly see people doing. This is when you take your app that's running in production, you take some subset of the data that's coming into the app, and you score that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=120" target="_blank">00:02:00.000</a></span> | <span class="t">And so, then you can start to track the performance of your app in real time as it's running on real production data, and so this is real queries that users are sending in, so it's not a static kind of like golden set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=132" target="_blank">00:02:12.000</a></span> | <span class="t">And so, these are the two types of evals that people are most familiar with, but we also think there's a third type of eval, what we call in-the-loop evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=139" target="_blank">00:02:19.000</a></span> | <span class="t">And so, these are evals that occur during the agent, while it's running.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=143" target="_blank">00:02:23.000</a></span> | <span class="t">So, Michele talked a little bit about some of what they were doing at Replit with this, where they were adding some evals based on trying it out and testing with browser use or running code against it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=154" target="_blank">00:02:34.000</a></span> | <span class="t">So, these are the examples of running some evals in the loop to correct the agent as it's running, and then if it messes up, like in any of these scenarios here, you can feed it back into the agent and have it kind of like self-correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=166" target="_blank">00:02:46.000</a></span> | <span class="t">And so, you can add this in a bunch of different domains and use it to basically check the agent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=171" target="_blank">00:02:51.000</a></span> | <span class="t">And so, this has some obvious benefits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=173" target="_blank">00:02:53.000</a></span> | <span class="t">It improves response quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=175" target="_blank">00:02:55.000</a></span> | <span class="t">You're not monitoring it after the fact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=177" target="_blank">00:02:57.000</a></span> | <span class="t">It actually improves it before it responds and can block bad responses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=181" target="_blank">00:03:01.000</a></span> | <span class="t">The big downside is that this takes more time and it costs more money.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=185" target="_blank">00:03:05.000</a></span> | <span class="t">And so, we see this commonly being used when the tolerance for mistake is really low or when latency is not an issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=193" target="_blank">00:03:13.000</a></span> | <span class="t">And as we see more and more long-running agents, I actually think that's a perfect time to start thinking about putting these in-the-loop evals into your agent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=202" target="_blank">00:03:22.000</a></span> | <span class="t">When we think about evals, there's generally kind of like two parts to evals that we see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=209" target="_blank">00:03:29.000</a></span> | <span class="t">the data that you run over, and then the evaluators that you use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=214" target="_blank">00:03:34.000</a></span> | <span class="t">And so, all of those three components, they had different aspects of these data and of these evaluators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=221" target="_blank">00:03:41.000</a></span> | <span class="t">So, in the offline sense, you know you've got your data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=224" target="_blank">00:03:44.000</a></span> | <span class="t">In the online evals, the data is the production data, and it's happening after the fact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=229" target="_blank">00:03:49.000</a></span> | <span class="t">In the loop, it's the production data, but it's happening in the loop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=231" target="_blank">00:03:51.000</a></span> | <span class="t">And then the evaluators can be different as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=233" target="_blank">00:03:53.000</a></span> | <span class="t">So, when you have your golden truth data set, you can compare against it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=238" target="_blank">00:03:58.000</a></span> | <span class="t">And so, that's useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=239" target="_blank">00:03:59.000</a></span> | <span class="t">As we recall, what we call ground truth or reference evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=242" target="_blank">00:04:02.000</a></span> | <span class="t">And then reference-free evals are when you don't have this ground truth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=246" target="_blank">00:04:06.000</a></span> | <span class="t">And this is what you do online or in the loop because you don't know what the ground truth is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=250" target="_blank">00:04:10.000</a></span> | <span class="t">So, basically, data and evaluators are two parts of evals, no matter what type of eval you're doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=257" target="_blank">00:04:17.000</a></span> | <span class="t">And so, we want to make it easy for people to build data sets and run things over their data, as well as build their own evaluators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=267" target="_blank">00:04:27.000</a></span> | <span class="t">Because one thing that we've noticed is that all the academic data sets that you might see or get started with, those aren't representative of how users are using your application.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=276" target="_blank">00:04:36.000</a></span> | <span class="t">They're often times not even in the same domain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=279" target="_blank">00:04:39.000</a></span> | <span class="t">And so, when we talk about data and evals, it's really about making it easy for any application developer to build a data set or build evaluators that are specific for their use case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=291" target="_blank">00:04:51.000</a></span> | <span class="t">So, how do we help do that on the data side?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=294" target="_blank">00:04:54.000</a></span> | <span class="t">One, we've talked about tracing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=296" target="_blank">00:04:56.000</a></span> | <span class="t">Traces are where you run these online evaluators over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=299" target="_blank">00:04:59.000</a></span> | <span class="t">So, we have really good tracing in LinkSmith.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=301" target="_blank">00:05:01.000</a></span> | <span class="t">You can send everything there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=302" target="_blank">00:05:02.000</a></span> | <span class="t">We track not only the inputs and outputs, but all the steps as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=305" target="_blank">00:05:05.000</a></span> | <span class="t">And so, you can then run evaluators over these traces for the online evals part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=310" target="_blank">00:05:10.000</a></span> | <span class="t">We've also made it really, really easy to add these to a data set and start building off with this ground truth data set for offline evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=318" target="_blank">00:05:18.000</a></span> | <span class="t">So, there's a nice little button in LinkSmith that you can click, add the data set, and it will take this kind of like input-output pair.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=324" target="_blank">00:05:24.000</a></span> | <span class="t">You can actually modify the output pair as well and then add it to a data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=327" target="_blank">00:05:27.000</a></span> | <span class="t">So, we've tried to make it really easy for people to build up these data sets in LinkSmith powered by the observability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=335" target="_blank">00:05:35.000</a></span> | <span class="t">And so, one of the things that we like to say is that great evals start with great observability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=341" target="_blank">00:05:41.000</a></span> | <span class="t">And that's how we think about it in LinkSmith.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=343" target="_blank">00:05:43.000</a></span> | <span class="t">They're tied.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=344" target="_blank">00:05:44.000</a></span> | <span class="t">They're not separate things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=346" target="_blank">00:05:46.000</a></span> | <span class="t">Now, let's talk about evaluators for a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=351" target="_blank">00:05:51.000</a></span> | <span class="t">So, there are a few different types of evaluators that we see people using.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=355" target="_blank">00:05:55.000</a></span> | <span class="t">First is maybe just using code to evaluate things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=358" target="_blank">00:05:58.000</a></span> | <span class="t">So, this would be like exact match or regex or checking if it's valid JSON or things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=363" target="_blank">00:06:03.000</a></span> | <span class="t">These are great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=364" target="_blank">00:06:04.000</a></span> | <span class="t">These are deterministic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=365" target="_blank">00:06:05.000</a></span> | <span class="t">They're cheap.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=366" target="_blank">00:06:06.000</a></span> | <span class="t">They're fast to run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=367" target="_blank">00:06:07.000</a></span> | <span class="t">But they're oftentimes not as representative of all the things you want to catch, especially if you have natural language responses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=374" target="_blank">00:06:14.000</a></span> | <span class="t">So, one of the things that we see popping up here is using LLM as a judge techniques to use an LLM to score the outputs of your agent or LLM application.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=383" target="_blank">00:06:23.000</a></span> | <span class="t">And so, this is useful because they can handle more complex things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=386" target="_blank">00:06:26.000</a></span> | <span class="t">There's some downsides to this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=387" target="_blank">00:06:27.000</a></span> | <span class="t">They're tricky to get started work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=389" target="_blank">00:06:29.000</a></span> | <span class="t">We'll talk about this later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=390" target="_blank">00:06:30.000</a></span> | <span class="t">But generally, the idea of using an LLM to judge outputs is pretty promising.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=394" target="_blank">00:06:34.000</a></span> | <span class="t">And the third type that we see is just good old human annotation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=397" target="_blank">00:06:37.000</a></span> | <span class="t">This can happen kind of like live from the user as they're using the app.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=401" target="_blank">00:06:41.000</a></span> | <span class="t">You can collect thumbs up, thumbs down, send those blanks, and track them there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=404" target="_blank">00:06:44.000</a></span> | <span class="t">Or, you can have human go in the background and use something like our annotation cues to score these runs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=411" target="_blank">00:06:51.000</a></span> | <span class="t">So, one of the things that we've been building over the past month or so is a set of open source evaluators to try to make it easy to get started with these evaluators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=423" target="_blank">00:07:03.000</a></span> | <span class="t">And so, there are a few common use cases that we think are common and you can use off-the-shelf.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=428" target="_blank">00:07:08.000</a></span> | <span class="t">These include things like code, rag, extraction, and tool calling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=432" target="_blank">00:07:12.000</a></span> | <span class="t">So, for code, for example, we have some off-the-shelf utils that will lint Python code or lint TypeScript code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=438" target="_blank">00:07:18.000</a></span> | <span class="t">And then you can take those errors and feed them back into the LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=441" target="_blank">00:07:21.000</a></span> | <span class="t">This is great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=442" target="_blank">00:07:22.000</a></span> | <span class="t">You can use these off-the-shelf.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=443" target="_blank">00:07:23.000</a></span> | <span class="t">There's little configuration needed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=445" target="_blank">00:07:25.000</a></span> | <span class="t">But, of course, for a lot of use cases, you are going to want to configure evaluators to your specific use case in your specific domain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=452" target="_blank">00:07:32.000</a></span> | <span class="t">And so, we have a few customizable evals also included in open evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=455" target="_blank">00:07:35.000</a></span> | <span class="t">One of these are LLM's judge things, making it really easy to get started with that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=460" target="_blank">00:07:40.000</a></span> | <span class="t">A little bit more interesting is things around agent trajectories.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=463" target="_blank">00:07:43.000</a></span> | <span class="t">So, taking in a trajectory of messages, passing it into one of these evaluators, and customizing it so you can choose what to look for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=470" target="_blank">00:07:50.000</a></span> | <span class="t">And then one of the things that we're launching today is chat simulations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=473" target="_blank">00:07:53.000</a></span> | <span class="t">So, a lot of applications are chat bots, or they have some back and forth component.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=478" target="_blank">00:07:58.000</a></span> | <span class="t">And so, sure, you can test a single kind of like interaction, but you often want to see how it performs in a conversational setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=484" target="_blank">00:08:04.000</a></span> | <span class="t">And so, we're launching some utils to both run and then score those evaluators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=489" target="_blank">00:08:09.000</a></span> | <span class="t">These are all open source in open evals package.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=492" target="_blank">00:08:12.000</a></span> | <span class="t">One of the most common techniques we see being used is LLM as the judge evaluators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=498" target="_blank">00:08:18.000</a></span> | <span class="t">These can be really powerful, but they're also tricky to get working properly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=502" target="_blank">00:08:22.000</a></span> | <span class="t">You now have a separate prompt that you have to worry about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=504" target="_blank">00:08:24.000</a></span> | <span class="t">You have to prompt engineer this prompt, which is grading your other prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=508" target="_blank">00:08:28.000</a></span> | <span class="t">And so, there's this extra work that goes into it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=510" target="_blank">00:08:30.000</a></span> | <span class="t">And so, while this is powerful, we oftentimes see people struggling to set it up or to trust the process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=517" target="_blank">00:08:37.000</a></span> | <span class="t">And so, we're launching in private preview some features specifically designed to help with this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=522" target="_blank">00:08:42.000</a></span> | <span class="t">So, first, we're launching some work that's based off of Align Evo, which is some research by Eugene Ann.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=528" target="_blank">00:08:48.000</a></span> | <span class="t">You'll hear from Shreya later on. She was actually a lot of the influence for some of this work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=533" target="_blank">00:08:53.000</a></span> | <span class="t">She wrote a great paper called "Who Validates the Validators?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=536" target="_blank">00:08:56.000</a></span> | <span class="t">And so, a lot of this work we're incorporating into Langsmith to make it really easy to get started with LLM as a judge techniques.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=544" target="_blank">00:09:04.000</a></span> | <span class="t">And then, of course, after you get started, how do you know that it's working?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=547" target="_blank">00:09:07.000</a></span> | <span class="t">So, we're launching some eval calibration techniques in Langsmith where you can blindly score how evals are doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=553" target="_blank">00:09:13.000</a></span> | <span class="t">and then compare them and see that over time. And if they start to drift out of whack, then you go back into this Align Evo kind of like step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=560" target="_blank">00:09:20.000</a></span> | <span class="t">So, this is in private preview. We're really excited to launch it and work in figuring out what the right UX for building these LLM as a judge evaluators are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=570" target="_blank">00:09:30.000</a></span> | <span class="t">The thing that I want to emphasize is that evals is a continuous journey. You're not done with it once you build a dataset and run it once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=577" target="_blank">00:09:37.000</a></span> | <span class="t">You're going to want to keep on running it. You're not done with it just because you did it on the offline part. You're going to want to do it online.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=583" target="_blank">00:09:43.000</a></span> | <span class="t">And eventually, I think more and more, we're going to start building it into the agents themselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=587" target="_blank">00:09:47.000</a></span> | <span class="t">And so, this is one of the key takeaways that I kind of leave you with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=591" target="_blank">00:09:51.000</a></span> | <span class="t">With that, I want to hand it over to some of the amazing speakers that we have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=595" target="_blank">00:09:55.000</a></span> | <span class="t">Up next, we'll be Ben Leval, VP of Engineering at Harvey.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=bQPxekLApxw&t=600" target="_blank">00:10:00.000</a></span> | <span class="t">Ben will discuss how Harvey builds agents and then uses Langsmith to evaluate them.</span></div></div></body></html>