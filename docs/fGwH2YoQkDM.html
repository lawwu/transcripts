<html><head><title>OpenAI CLIP Explained | Multi-modal ML</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>OpenAI CLIP Explained | Multi-modal ML</h2><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM"><img src="https://i.ytimg.com/vi_webp/fGwH2YoQkDM/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=579">9:39</a> Architecture of Clip<br><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=732">12:12</a> Contrastive Training<br><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1164">19:24</a> Padding Labels<br><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1210">20:10</a> Input Ids and an Attention Mask<br><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1466">24:26</a> Encoding Images<br><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1627">27:7</a> Cosine Similarity<br><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1882">31:22</a> Object Detection<br><br><div style="text-align: left;"><a href="./fGwH2YoQkDM.html">Whisper Transcript</a> | <a href="./transcript_fGwH2YoQkDM.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Today we're talking about what is quite possibly the future of machine learning in both computer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=6" target="_blank">00:00:06.160</a></span> | <span class="t">vision and NLP. We're talking about a combination of both into a single multi-modal model. That</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=16" target="_blank">00:00:16.960</a></span> | <span class="t">model is CLIP which was built and trained by OpenAI. Now CLIP is open source and it has been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=24" target="_blank">00:00:24.160</a></span> | <span class="t">around for a little while. It's been around from since the start of 2021 but in the past few months</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=33" target="_blank">00:00:33.600</a></span> | <span class="t">we've seen the adoption of CLIP grow at a pretty insane rate. It has found uses in a load of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=43" target="_blank">00:00:43.760</a></span> | <span class="t">so what we will be focusing on is text or multi-modal search across text and images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=49" target="_blank">00:00:49.760</a></span> | <span class="t">We're also going to talk a little bit about zero shot classifications, zero shot object detection</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=55" target="_blank">00:00:55.680</a></span> | <span class="t">and at some point in the future we'll also talk about the use of models like CLIP or even CLIP</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=62" target="_blank">00:01:02.000</a></span> | <span class="t">itself in the diffuser models that have become incredibly popular recently like DALI-2, IMAGEN</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=69" target="_blank">00:01:09.440</a></span> | <span class="t">and MIDJOURNEY. Now to understand why CLIP is so important we can use a paper that was released</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=77" target="_blank">00:01:17.120</a></span> | <span class="t">in 2020 called Experience Grounds Language. In Experience Grounds Language the authors define</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=85" target="_blank">00:01:25.280</a></span> | <span class="t">these five different world scopes; corpus, internet perception, embodiment and social.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=92" target="_blank">00:01:32.000</a></span> | <span class="t">Now most of the models that we're aware of in NLP, this is a sort of NLP language focused paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=102" target="_blank">00:01:42.320</a></span> | <span class="t">these were sort of the state of the art very recently so you have BERT, T5 and all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=109" target="_blank">00:01:49.120</a></span> | <span class="t">different models, GPT and so on. In perception this is where we start to see not just NLP but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=116" target="_blank">00:01:56.560</a></span> | <span class="t">also computer vision. So here is where we are now with CLIP. Okay and quite interestingly as well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=125" target="_blank">00:02:05.520</a></span> | <span class="t">in the next world scope this is where you start to get reinforcement learning and then you continue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=130" target="_blank">00:02:10.720</a></span> | <span class="t">going so you almost see this culmination of all the different disciplines in machine learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=135" target="_blank">00:02:15.040</a></span> | <span class="t">coming together into one larger discipline. Now the focus between these different world scopes is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=141" target="_blank">00:02:21.600</a></span> | <span class="t">mostly on the data that is being used to train the models. So over here world scope one you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=148" target="_blank">00:02:28.800</a></span> | <span class="t">the first models that we saw which would have been things like Word2Vec which is probably one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=157" target="_blank">00:02:37.280</a></span> | <span class="t">of the earliest samples of deep learning in NLP and that consisted of training a neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=164" target="_blank">00:02:44.080</a></span> | <span class="t">on a small or not small amount of data but small compared to future data sets. So a single corpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=172" target="_blank">00:02:52.240</a></span> | <span class="t">you know for example 100,000 sentences from Wikipedia may be one example of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=178" target="_blank">00:02:58.880</a></span> | <span class="t">Then going forwards we have the internet sized corpuses. So these are based on a huge web</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=186" target="_blank">00:03:06.160</a></span> | <span class="t">scraping from across the internet from many different sources and the models trained on this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=192" target="_blank">00:03:12.240</a></span> | <span class="t">were able to obviously kind of pull in more general understanding of language from purely text data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=202" target="_blank">00:03:22.080</a></span> | <span class="t">Okay so it's a lot of data but it's still purely text. The next one which we're focusing on world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=206" target="_blank">00:03:26.960</a></span> | <span class="t">scope three is not just that it's in our example text and image data. Okay so we're training a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=214" target="_blank">00:03:34.720</a></span> | <span class="t">model to understand both of these different modalities of information and this is almost like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=222" target="_blank">00:03:42.800</a></span> | <span class="t">AI moving from a purely digital very abstract space to a more realistic real world space because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=233" target="_blank">00:03:53.760</a></span> | <span class="t">in the real world we don't just rely on text data we rely on a huge number of sensory inputs. We have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=242" target="_blank">00:04:02.800</a></span> | <span class="t">visual, audio, touch and everything else. Okay so we're sort of moving more towards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=250" target="_blank">00:04:10.960</a></span> | <span class="t">that more broad scope of inputs from different modalities. Okay where modality would be something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=257" target="_blank">00:04:17.520</a></span> | <span class="t">like text or image or visual and so on. For us in the real world that chaotic ensemble of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=268" target="_blank">00:04:28.080</a></span> | <span class="t">sensory inputs is what kind of creates or trains our internal model of the outside world. So it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=275" target="_blank">00:04:35.200</a></span> | <span class="t">makes sense that that is the sort of direction that machine learning and AI may also go in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=282" target="_blank">00:04:42.160</a></span> | <span class="t">So to achieve this multi modality in CLIP we actually use two models that are trained to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=290" target="_blank">00:04:50.880</a></span> | <span class="t">almost speak the same language. So we have these two models one of them is a text encoder one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=296" target="_blank">00:04:56.480</a></span> | <span class="t">them is an image encoder. Both of these models create a vector representation of whatever they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=303" target="_blank">00:05:03.760</a></span> | <span class="t">are being input so the text encoder may get a sentence. That sentence could be two dogs running</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=308" target="_blank">00:05:08.640</a></span> | <span class="t">across a frosty field and then we have a image of two dogs running across a frosty field and CLIP</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=317" target="_blank">00:05:17.840</a></span> | <span class="t">will be trained so that the text encoder consumes our sentence and outputs a vector representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=327" target="_blank">00:05:27.200</a></span> | <span class="t">that is very very closely aligned to what the image encoder has output based on the image of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=335" target="_blank">00:05:35.440</a></span> | <span class="t">the same concept. Now by training both of these models to encode these vectors into a similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=342" target="_blank">00:05:42.800</a></span> | <span class="t">vector space we are teaching them to speak the same vector language right. So this is it's very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=350" target="_blank">00:05:50.240</a></span> | <span class="t">abstract this this vector language is 512 dimensional space so we can't directly understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=358" target="_blank">00:05:58.480</a></span> | <span class="t">what or it's very difficult for us to directly understand what is actually happening there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=363" target="_blank">00:06:03.600</a></span> | <span class="t">But these two models do actually output patterns that are logical and make sense and we can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=373" target="_blank">00:06:13.440</a></span> | <span class="t">some of this by comparing the similarity between the vectors that it outputs. Okay so we can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=379" target="_blank">00:06:19.520</a></span> | <span class="t">that the two vectors for dogs running across a frosty field both the the text vector and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=384" target="_blank">00:06:24.960</a></span> | <span class="t">image vector are both within a very similar vector space. Whereas something else like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=391" target="_blank">00:06:31.680</a></span> | <span class="t">elephants in the Serengeti is you know whether it's text or image it's not here with our our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=398" target="_blank">00:06:38.400</a></span> | <span class="t">two dogs running across frosty field it's somewhere over over here right in a completely different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=403" target="_blank">00:06:43.120</a></span> | <span class="t">space. So what we can do with that is is calculate the similarity between these vectors and identify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=409" target="_blank">00:06:49.440</a></span> | <span class="t">which ones are similar or not similar according to CLIP. From this from these these meaningful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=416" target="_blank">00:06:56.240</a></span> | <span class="t">vectors that CLIP is actually outputting we are able to create a content-based image retrieval</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=421" target="_blank">00:07:01.840</a></span> | <span class="t">system. Okay so content-based image retrieval is basically where we using some text or using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=428" target="_blank">00:07:08.080</a></span> | <span class="t">maybe even another image we can search for images based on their content right and not just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=437" target="_blank">00:07:17.920</a></span> | <span class="t">some meta textual metadata or something that's been attached to them. And with CLIP unlike other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=443" target="_blank">00:07:23.120</a></span> | <span class="t">content-based image retrieval systems CLIP is incredibly good at actually capturing the meaning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=450" target="_blank">00:07:30.800</a></span> | <span class="t">across an entire image. So you know for example with our two dogs running across a frosty field</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=456" target="_blank">00:07:36.960</a></span> | <span class="t">we might also be able to describe the background of that image without mentioning that there's two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=461" target="_blank">00:07:41.360</a></span> | <span class="t">dogs in it and if we describe in such a way that we align pretty well with what that image actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=467" target="_blank">00:07:47.600</a></span> | <span class="t">is what is in that image we might actually also return the image based on that. So we're not just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=473" target="_blank">00:07:53.360</a></span> | <span class="t">focusing on one thing in the image CLIP allows us to focus on many things in the image. So an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=480" target="_blank">00:08:00.480</a></span> | <span class="t">example of that is within this data set I've been using here there are no images there's one single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=488" target="_blank">00:08:08.400</a></span> | <span class="t">image of the food a hot dog. Okay so I tried to search for that and the first image that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=495" target="_blank">00:08:15.440</a></span> | <span class="t">returned is a dog eating a hot dog. Okay so it's pretty relevant but of course there are no other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=501" target="_blank">00:08:21.280</a></span> | <span class="t">images of hot dogs in this in this data set. So the other images that are returned are quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=506" target="_blank">00:08:26.640</a></span> | <span class="t">interesting because in some way or another they are kind of showing a hot dog. So the first one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=513" target="_blank">00:08:33.280</a></span> | <span class="t">we have a dog looking pretty cozy in a warm room with a fire in the background. Then we have a dog</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=521" target="_blank">00:08:41.120</a></span> | <span class="t">in a big wooly jumper and another dog kind of like posing for the camera. So weirdly enough we got a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=529" target="_blank">00:08:49.840</a></span> | <span class="t">load of hot dog images even though it's not really maybe it's not exactly what we meant when we said</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=535" target="_blank">00:08:55.360</a></span> | <span class="t">hot dog but a person could understand that. Okay we can we can see how those that term and those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=541" target="_blank">00:09:01.840</a></span> | <span class="t">images are related. Now we're not actually only restricted to text to image search. When we encode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=548" target="_blank">00:09:08.880</a></span> | <span class="t">our data when we code text and when we encode images we are actually just creating vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=554" target="_blank">00:09:14.080</a></span> | <span class="t">So we can search across that space in any direction with any combination of modalities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=561" target="_blank">00:09:21.120</a></span> | <span class="t">So we could do a text to text search image to image search. We can also do image to text search</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=567" target="_blank">00:09:27.280</a></span> | <span class="t">or we can search everything. We could use some text to search with both text and images. We can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=572" target="_blank">00:09:32.480</a></span> | <span class="t">kind of go in any direction use any modality we want there. Now let's go into a little more detail</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=578" target="_blank">00:09:38.160</a></span> | <span class="t">on what the architecture of CLIP actually looks like. So CLIP as I mentioned it's these two models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=586" target="_blank">00:09:46.880</a></span> | <span class="t">Now these two models are trained in parallel. One of them is the text encoder. Now it's a just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=593" target="_blank">00:09:53.040</a></span> | <span class="t">generic text encoder of 12 layers and then on the image encoder side there are although there are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=599" target="_blank">00:09:59.280</a></span> | <span class="t">two different options I've spoken about. There is a vision transformer model and also a ResNet model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=606" target="_blank">00:10:06.720</a></span> | <span class="t">and they use a few different sizes for ResNet as well. Both of these both of these encoder models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=612" target="_blank">00:10:12.720</a></span> | <span class="t">output a single 512 dimensional vector and the way these models is trained is kind of in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=621" target="_blank">00:10:21.120</a></span> | <span class="t">name of CLIP. So CLIP stands for contrastive learning in pre-training and so the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=629" target="_blank">00:10:29.200</a></span> | <span class="t">that is used during pre-training is is contrastive. It's contrastive pre-training. Now across both</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=634" target="_blank">00:10:34.400</a></span> | <span class="t">NLP and computer vision large models sort of dominate the state of the art and the reason for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=641" target="_blank">00:10:41.440</a></span> | <span class="t">this or the idea behind this is that just by giving a large model a huge amount of data they can learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=648" target="_blank">00:10:48.880</a></span> | <span class="t">sort of general patterns from what they see and almost kind of internalize a general rule set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=656" target="_blank">00:10:56.960</a></span> | <span class="t">for the the data that it sees. Okay so they manage to recognize general patterns in their modality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=664" target="_blank">00:11:04.480</a></span> | <span class="t">In language they may be able to internalize the grammar rules and patterns in English language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=672" target="_blank">00:11:12.240</a></span> | <span class="t">For vision models that may be sort of the general patterns that you identify or notice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=680" target="_blank">00:11:20.480</a></span> | <span class="t">in with different scenes and different objects. Now the problem with these different models the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=686" target="_blank">00:11:26.560</a></span> | <span class="t">reason they don't fit together very well already is that they're trained separately. So by default</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=693" target="_blank">00:11:33.120</a></span> | <span class="t">these state-of-the-art models have no understanding of each other and that that's where CLIP is is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=699" target="_blank">00:11:39.280</a></span> | <span class="t">different that's what CLIP has has brought to the table here. With CLIP the text and image encoders</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=705" target="_blank">00:11:45.760</a></span> | <span class="t">are trained while considering the context of the other modality. Okay so the text encoder is trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=713" target="_blank">00:11:53.920</a></span> | <span class="t">and it considers the modality or it considers the the concept learned by the image encoder and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=719" target="_blank">00:11:59.920</a></span> | <span class="t">image encoder does the same for the text encoder and we can almost think of this as the the image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=725" target="_blank">00:12:05.840</a></span> | <span class="t">and text encoders are sharing a almost indirect understanding of the other modality. Now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=732" target="_blank">00:12:12.880</a></span> | <span class="t">contrastive training works by taking a image and text pair so for example the two dogs running</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=739" target="_blank">00:12:19.520</a></span> | <span class="t">across a frosty field and putting those together into the text encoder and image encoder and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=746" target="_blank">00:12:26.160</a></span> | <span class="t">learning to encode them both as as closely as possible. For this to work well we also need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=754" target="_blank">00:12:34.240</a></span> | <span class="t">negative pairs so we need something to compare against as this is a general rule in contrastive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=763" target="_blank">00:12:43.120</a></span> | <span class="t">learning you can't just have positive pairs because then everything can just be kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=766" target="_blank">00:12:46.320</a></span> | <span class="t">encoded into the same like tiny little space and you don't know how to separate the the pairs are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=773" target="_blank">00:12:53.680</a></span> | <span class="t">dissimilar. Okay so we need both positive and negative pairs so we have a positive pair okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=778" target="_blank">00:12:58.640</a></span> | <span class="t">in order to get negative pairs we can essentially just take all the positive pairs in our data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=785" target="_blank">00:13:05.200</a></span> | <span class="t">and we can say okay the pair t1 and i1 we can mix t1 with different i's okay so we can do t1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=795" target="_blank">00:13:15.520</a></span> | <span class="t">with i2 and i3 i4 and so on so we're basically just swapping the pairs and we can we can understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=804" target="_blank">00:13:24.320</a></span> | <span class="t">that those other pairs are probably not going to be similar as long as our data set is relatively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=810" target="_blank">00:13:30.080</a></span> | <span class="t">large. Occasionally maybe we will get a pair that are similar but as long as our data set is large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=816" target="_blank">00:13:36.240</a></span> | <span class="t">enough that that doesn't happen too frequently it's not going to affect how our training is it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=820" target="_blank">00:13:40.640</a></span> | <span class="t">will be sort of a negligible problem. So with this idea we can use a loss function that will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=829" target="_blank">00:13:49.680</a></span> | <span class="t">minimize the difference between positive pairs and maximize the difference between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=836" target="_blank">00:13:56.480</a></span> | <span class="t">negative pairs and that will look something like this where we have our positive pairs in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=841" target="_blank">00:14:01.440</a></span> | <span class="t">diagonal of the similarity matrix and everything else is something that we the dot product there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=848" target="_blank">00:14:08.080</a></span> | <span class="t">we need to maximize and this image that you see here is actually the pre-training for a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=855" target="_blank">00:14:15.520</a></span> | <span class="t">batch okay so one interesting thing to note here is if we have a small batch say we only have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=861" target="_blank">00:14:21.920</a></span> | <span class="t">batch size of two it's going to be very easy for our model to identify which two items are similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=867" target="_blank">00:14:27.280</a></span> | <span class="t">which two are not similar whereas if we have 64 in our 64 items in our batch it will be much harder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=875" target="_blank">00:14:35.280</a></span> | <span class="t">for our model because it has to it has to find more nuanced differences between them and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=880" target="_blank">00:14:40.480</a></span> | <span class="t">and what basically the odds of guessing randomly between those and guessing correctly are much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=888" target="_blank">00:14:48.080</a></span> | <span class="t">smaller. So a larger batch size is a good thing to aim for in this contrastive pre-training approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=896" target="_blank">00:14:56.880</a></span> | <span class="t">So with that I think we we have a good idea now of how CLIP can be used and also you know how it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=905" target="_blank">00:15:05.120</a></span> | <span class="t">has been trained for for this. So what I really want to do now is kind of show you how you might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=911" target="_blank">00:15:11.440</a></span> | <span class="t">be able to use it as well. Now we're going to be using the Vision Transformer version of CLIP</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=917" target="_blank">00:15:17.360</a></span> | <span class="t">okay so remember I said there's a ResNet and Vision Transformer options for that image encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=923" target="_blank">00:15:23.760</a></span> | <span class="t">we're going to use a Vision Transformer version and OpenAI have released this model through the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=931" target="_blank">00:15:31.600</a></span> | <span class="t">Hugging Face library so we can we can go to the Hugging Face library and use it directly from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=935" target="_blank">00:15:35.920</a></span> | <span class="t">there which makes it really easy for us to actually sort of get started with it. So let's go ahead and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=940" target="_blank">00:15:40.960</a></span> | <span class="t">do that now. Okay so for this we will need to install a few libraries here so we have Transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=948" target="_blank">00:15:48.560</a></span> | <span class="t">Torch and Datasets. So Datasets we need to actually get a Dataset so I've prepared one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=954" target="_blank">00:15:54.560</a></span> | <span class="t">especially for this so we have this image text Dataset and in here we don't have there's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=962" target="_blank">00:16:02.000</a></span> | <span class="t">much it's just 21 images or text to image pairs and we can see what they look like. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=968" target="_blank">00:16:08.800</a></span> | <span class="t">we have this text Aerial shot of a futuristic city with a large motorway. Okay so I'll try to just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=975" target="_blank">00:16:15.760</a></span> | <span class="t">describe this image as best I could and yeah that's what I got and there are like you saw just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=985" target="_blank">00:16:25.040</a></span> | <span class="t">now 21 of these image text pairs in there. So let's go ahead and actually prepare or download</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=993" target="_blank">00:16:33.600</a></span> | <span class="t">and sort of initialize CLIP for our use. So the model ID on Hugging Face is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1001" target="_blank">00:16:41.920</a></span> | <span class="t">so if we were to go to HuggingFace.co we could type that in here and we have the model there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1014" target="_blank">00:16:54.960</a></span> | <span class="t">Okay so this is the model that we're using over from OpenAI here and with this model we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1022" target="_blank">00:17:02.400</a></span> | <span class="t">use these two we use a processor and a model so this is the model itself this is CLIP right this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1030" target="_blank">00:17:10.800</a></span> | <span class="t">is a almost like a pre-processor for both our text and also the images. Okay so one thing we would do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1040" target="_blank">00:17:20.560</a></span> | <span class="t">here if we have a CUDA device and available we can move our model to the CUDA device.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1048" target="_blank">00:17:28.000</a></span> | <span class="t">At the moment if you try and do this with NPS so if you're on Mac and you have a you have Apple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1054" target="_blank">00:17:34.720</a></span> | <span class="t">Silicon there are some processes or some transformations in the CLIP that don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1061" target="_blank">00:17:41.520</a></span> | <span class="t">function on NPS at the moment so I would stick with CPU. We're only doing inference so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1068" target="_blank">00:17:48.320</a></span> | <span class="t">still pretty fast. Now as I was mentioning the processor is what handles both the text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1075" target="_blank">00:17:55.200</a></span> | <span class="t">and image preparation that needs to happen before we feed them into the actual encoder models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1081" target="_blank">00:18:01.280</a></span> | <span class="t">themselves that make up CLIP. So for text we do this so this is just going to be this going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1089" target="_blank">00:18:09.280</a></span> | <span class="t">work like a normal text tokenizer. A normal text tokenizer for text transform models is used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1096" target="_blank">00:18:16.080</a></span> | <span class="t">in order to translate our human readable text into transformer readable IDs. Okay so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1104" target="_blank">00:18:24.880</a></span> | <span class="t">pass the text here we make sure we are saying there are no images included here because the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1110" target="_blank">00:18:30.880</a></span> | <span class="t">processor if we have both images and text it can process them at the same time. We can do that here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1118" target="_blank">00:18:38.720</a></span> | <span class="t">as well but I want to show you it separately just to show you what they're actually doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1123" target="_blank">00:18:43.040</a></span> | <span class="t">So the padding we need to set that to true and that is because different sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1132" target="_blank">00:18:52.160</a></span> | <span class="t">can have different lengths okay so you have like hello world and whatever I wrote before up here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1139" target="_blank">00:18:59.440</a></span> | <span class="t">so this aerial shot of futuristic city aerial shot of a city. These two sentences have different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1149" target="_blank">00:19:09.760</a></span> | <span class="t">lengths and a transform model needs to see the same length being input for all of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1157" target="_blank">00:19:17.280</a></span> | <span class="t">text that is within this sort of single batch. So basically what it's going to do there is add what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1164" target="_blank">00:19:24.560</a></span> | <span class="t">are called padding labels so it's just going to add a few of these up to the length of the longest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1172" target="_blank">00:19:32.400</a></span> | <span class="t">sequence within that batch of text items just because in here we have those 22 um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1179" target="_blank">00:19:39.520</a></span> | <span class="t">20 no sorry 21 sentences. So that's all we're doing there I'm sure that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1192" target="_blank">00:19:52.320</a></span> | <span class="t">and then we are returning those as pytorch tensors and then finally just moving them to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1197" target="_blank">00:19:57.760</a></span> | <span class="t">whichever device we're using. I'm using CPU here so it's not actually necessary to do this but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1203" target="_blank">00:20:03.360</a></span> | <span class="t">I'm doing it in case you do do the same on a CUDA enabled device. So from there we have these input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1211" target="_blank">00:20:11.280</a></span> | <span class="t">IDs and an attention mask okay so let's have a quick look at what what those are. So if we go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1219" target="_blank">00:20:19.120</a></span> | <span class="t">into tokens and we have a look at input IDs okay you see we get all these literally just integer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1231" target="_blank">00:20:31.520</a></span> | <span class="t">values and you'll see that a lot of them have this 49407 at the end right that is they're the padding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1240" target="_blank">00:20:40.720</a></span> | <span class="t">tokens there okay so they they are not represented as strings but they're represented as these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1246" target="_blank">00:20:46.560</a></span> | <span class="t">integer numbers okay and we know that they're the padding tokens because they they're appearing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1251" target="_blank">00:20:51.520</a></span> | <span class="t">several times at the end of each sequence and none of the sequences I fed in there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1257" target="_blank">00:20:57.120</a></span> | <span class="t">were they didn't have any similar words at the end of those okay so you can see them all here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1262" target="_blank">00:21:02.560</a></span> | <span class="t">So we know that those are the padding sequences we also see there's like an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1269" target="_blank">00:21:09.760</a></span> | <span class="t">initialization of sequence token there as well and then everything in between those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1277" target="_blank">00:21:17.760</a></span> | <span class="t">they are tokens that represent a word or a part of a word from our original text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1284" target="_blank">00:21:24.240</a></span> | <span class="t">So let's see input IDs the attention mask is you'll see so here you can see that it's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1292" target="_blank">00:21:32.720</a></span> | <span class="t">these ones and zeros now the ones represent real tokens okay they represent real words that were in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1300" target="_blank">00:21:40.000</a></span> | <span class="t">our from our text inputs the zeros represent where the where our processor has added padding tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1310" target="_blank">00:21:50.400</a></span> | <span class="t">so this is used for the internal mechanisms of the text transformer to know which tokens to pay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1317" target="_blank">00:21:57.120</a></span> | <span class="t">attention to which ones to ignore because we don't want to really focus on those padding tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1322" target="_blank">00:22:02.080</a></span> | <span class="t">because they're meaningless they're just there to make sure we have the same size inputs going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1328" target="_blank">00:22:08.720</a></span> | <span class="t">into our transform model that's all that is. So we can go down and after we have our tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1338" target="_blank">00:22:18.000</a></span> | <span class="t">now what we what we do is we use clip to encode all of them with this get text features okay and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1346" target="_blank">00:22:26.400</a></span> | <span class="t">then we pass our tokens and we've got two device here I think I already I already moved them to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1351" target="_blank">00:22:31.600</a></span> | <span class="t">device so I don't need to do that again we can actually remove that. Okay and okay what do we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1359" target="_blank">00:22:39.760</a></span> | <span class="t">get here so we get 21 so 21 text inputs that makes sense 512 dimensional vectors okay so they are our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1372" target="_blank">00:22:52.000</a></span> | <span class="t">text embeddings representing each of those those text sentences I just gave and then one other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1378" target="_blank">00:22:58.960</a></span> | <span class="t">thing I wanted to point out here is that we have the min and max values and they're they're pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1384" target="_blank">00:23:04.800</a></span> | <span class="t">big okay they're clearly not normalized so this depends on what you're doing if you are if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1391" target="_blank">00:23:11.520</a></span> | <span class="t">want to compare these vectors you need to make sure you're not using a similarity metric that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1399" target="_blank">00:23:19.120</a></span> | <span class="t">looks or that considers the magnitude of your vectors you need to only consider the the angle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1406" target="_blank">00:23:26.320</a></span> | <span class="t">so you can do that with cosine similarity or the alternative is that you can normalize these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1411" target="_blank">00:23:31.440</a></span> | <span class="t">vectors and then you can also do this with dot product similarity okay so to normalize if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1419" target="_blank">00:23:39.520</a></span> | <span class="t">wanted to use our product similarity now you would do this okay so here we're just detaching our text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1426" target="_blank">00:23:46.080</a></span> | <span class="t">embeddings from the the PyTorch graph moving them to CPU if needed we actually don't need to do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1431" target="_blank">00:23:51.440</a></span> | <span class="t">but do it here anyway and converting them into a NumPy rate and then we calculate the value that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1438" target="_blank">00:23:58.160</a></span> | <span class="t">we will normalize it each vector by okay so for each each vector we're calculating a number and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1445" target="_blank">00:24:05.840</a></span> | <span class="t">then that number is what we're going to divide them all by here okay to normalize that and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1453" target="_blank">00:24:13.280</a></span> | <span class="t">after that you can see the minimum maximum is this minus 0.15 and plus 0.53 okay so neither</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1462" target="_blank">00:24:22.720</a></span> | <span class="t">of them going over minus one or plus one now now when it comes to encoding images we we do the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1468" target="_blank">00:24:28.240</a></span> | <span class="t">thing or very similar thing so images are also pre-processed using the using the processor as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1475" target="_blank">00:24:35.280</a></span> | <span class="t">we did with our text but we just use slightly different parameters to start there so the reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1480" target="_blank">00:24:40.720</a></span> | <span class="t">we're processing these images is that uh clip expects a certain size of image when when we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1487" target="_blank">00:24:47.040</a></span> | <span class="t">feeding images into it and expects those those image pixels to be normalized as well now rgb</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1495" target="_blank">00:24:55.040</a></span> | <span class="t">images by default the the value the pixel values and they will range from 0 to 255</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1502" target="_blank">00:25:02.000</a></span> | <span class="t">we need to normalize those and we also need to resize the images so you can see you can see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1508" target="_blank">00:25:08.240</a></span> | <span class="t">here so the first image it has this size it's it's a pretty big image okay this is the the width and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1514" target="_blank">00:25:14.720</a></span> | <span class="t">the height of that image now here we're taking all the images and we're processing them make sure we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1520" target="_blank">00:25:20.960</a></span> | <span class="t">say text is is none and that will actually only output one tensor the pixel values tensor so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1526" target="_blank">00:25:26.080</a></span> | <span class="t">we're just going to extract that straight out there and we're also going to move it to um the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1530" target="_blank">00:25:30.800</a></span> | <span class="t">device set hardware device in this case just cpu and now let's have a look at this image or images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1537" target="_blank">00:25:37.040</a></span> | <span class="t">now so now we can see that we have this um this array or tensor with three color channels so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1543" target="_blank">00:25:43.840</a></span> | <span class="t">is the rgb and it has a height and width of 224 so it's been you know sort of squeezed into a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1552" target="_blank">00:25:52.000</a></span> | <span class="t">smaller size now and we have 21 of those because we fed in all of our images okay so this is how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1560" target="_blank">00:26:00.800</a></span> | <span class="t">we use the processor and this is just resizing and normalizing our images ready for the vision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1567" target="_blank">00:26:07.440</a></span> | <span class="t">transformer encoder of clip and very similar to before before you use get text features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1575" target="_blank">00:26:15.040</a></span> | <span class="t">now we're going to use get image features and we pass in those images like that and again as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1581" target="_blank">00:26:21.840</a></span> | <span class="t">you might expect those images are not um normalized you see that here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1586" target="_blank">00:26:26.960</a></span> | <span class="t">and as we would also expect they are the same dimensionality as our text embeddings so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1595" target="_blank">00:26:35.520</a></span> | <span class="t">means we can compare them but before comparing them of course as before we we normalize them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1602" target="_blank">00:26:42.000</a></span> | <span class="t">so we should normalize them again here um and yep same process again and we can see that those have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1609" target="_blank">00:26:49.440</a></span> | <span class="t">those have changed okay cool so what we now want to do is calculate the similarity between all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1620" target="_blank">00:27:00.240</a></span> | <span class="t">our image embeddings and all of our text embeddings so we can do that in a few different ways uh we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1627" target="_blank">00:27:07.440</a></span> | <span class="t">have cosine similarity or dot product similarity the reason we can use our product similarity is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1633" target="_blank">00:27:13.280</a></span> | <span class="t">because we normalize but i'm going to show you how to do both so that if you don't normalize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1637" target="_blank">00:27:17.360</a></span> | <span class="t">you can actually just use cosine similarity like we do here so cosine similarity is actually just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1642" target="_blank">00:27:22.640</a></span> | <span class="t">a dot product as a numerator between the text embeddings and image embeddings and in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1650" target="_blank">00:27:30.080</a></span> | <span class="t">denominator we have just normalized the norm values of both of those okay that is that's all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1658" target="_blank">00:27:38.880</a></span> | <span class="t">it is actually so it's it's pretty pretty simple and if we plot those similarity scores between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1665" target="_blank">00:27:45.840</a></span> | <span class="t">those we get this so we would expect along this diagonal here we would expect these to be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1672" target="_blank">00:27:52.240</a></span> | <span class="t">highest similarity values because they represent the the true pairs okay between the images and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1677" target="_blank">00:27:57.920</a></span> | <span class="t">the text now we have some that are not quite there like here and there is this image text pair which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1683" target="_blank">00:28:03.760</a></span> | <span class="t">is more similar even with something else you know i very quickly put these together so they're not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1690" target="_blank">00:28:10.800</a></span> | <span class="t">always going to be perfect so we have one here that is maybe not perfect but again there is also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1696" target="_blank">00:28:16.560</a></span> | <span class="t">a lot of overlap between these images so there are several images of city skylines and a lot of time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1702" target="_blank">00:28:22.640</a></span> | <span class="t">i describe those as futuristic cities in you know in whatever with a big motorway or something on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1708" target="_blank">00:28:28.480</a></span> | <span class="t">those lines so that is probably where we're getting that from um now if we were to calculate the dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1715" target="_blank">00:28:35.840</a></span> | <span class="t">product similarity between these we would expect it to be the same okay now um okay from this this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1723" target="_blank">00:28:43.680</a></span> | <span class="t">calculation dot product similarity we do seem to get very similar um set of similarities at the end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1730" target="_blank">00:28:50.560</a></span> | <span class="t">there but are they the same well if we go down here we can see that they pretty much are so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1737" target="_blank">00:28:57.520</a></span> | <span class="t">we can't do a straight comparison we can't do we can't do cosine similarity equals</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1745" target="_blank">00:29:05.760</a></span> | <span class="t">um dot similarity because the numbers are actually slightly different but they're only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1751" target="_blank">00:29:11.280</a></span> | <span class="t">slightly different because there's sort of a floating point error because these are all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1754" target="_blank">00:29:14.720</a></span> | <span class="t">floating point numbers so we get very very small differences between the numbers and you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1761" target="_blank">00:29:21.920</a></span> | <span class="t">that here so taking we've calculated the difference between them between the numbers and the two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1766" target="_blank">00:29:26.720</a></span> | <span class="t">arrays and then we're looking okay what's the minimum difference between them it's zero okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1771" target="_blank">00:29:31.920</a></span> | <span class="t">that's what we'd expect that's where the numbers are exactly the same what's the maximum difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1776" target="_blank">00:29:36.240</a></span> | <span class="t">between the numbers and it's 2.98 to the minus eight so like 0.000000 and so on two so very small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1789" target="_blank">00:29:49.520</a></span> | <span class="t">number and okay with that we we know it's just floating point errors between those two similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1796" target="_blank">00:29:56.960</a></span> | <span class="t">arrays so that's pretty cool to to see that and we can use this exact concept of comparing with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1803" target="_blank">00:30:03.600</a></span> | <span class="t">with a cosine similarity or dot product similarity to actually search through all of our images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1811" target="_blank">00:30:11.600</a></span> | <span class="t">with like a text prompt for example but that's not all that clip is good for clip is also has this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1818" target="_blank">00:30:18.320</a></span> | <span class="t">amazing performance um as a zero shot model for different tasks okay so not even just one that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1826" target="_blank">00:30:26.240</a></span> | <span class="t">what actually different tasks so um it is it performs incredibly well out of the box for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1833" target="_blank">00:30:33.440</a></span> | <span class="t">classification and we'll go through this in more detail in a future video but the idea is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1841" target="_blank">00:30:41.600</a></span> | <span class="t">given a set of classes from a classification image classification data set you can maybe you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1850" target="_blank">00:30:50.960</a></span> | <span class="t">modify them a little bit the class names to make them more like a sentence and then you use this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1856" target="_blank">00:30:56.000</a></span> | <span class="t">same idea of comparing all of your your your text representations of the classes with a set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1863" target="_blank">00:31:03.200</a></span> | <span class="t">images from the data set and with this you just calculate the similarity between those and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1868" target="_blank">00:31:08.400</a></span> | <span class="t">the the text embedding or the you can think of it as a class embedding that gets a high similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1874" target="_blank">00:31:14.240</a></span> | <span class="t">to your image is the predicted class okay so you have zero shot classification like that super easy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1881" target="_blank">00:31:21.200</a></span> | <span class="t">another use case is object detection so you let's say you have maybe you're looking for a cat or a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1890" target="_blank">00:31:30.000</a></span> | <span class="t">butterfly in an image okay and you okay when you're looking for the cat you're gonna you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1896" target="_blank">00:31:36.480</a></span> | <span class="t">gonna use a chunk of text that says um a fluffy cat okay and you encode that with clip and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1902" target="_blank">00:31:42.000</a></span> | <span class="t">get your text embedding and then what you do is you break up your image into all these little patches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1908" target="_blank">00:31:48.240</a></span> | <span class="t">and you just slide through all of those patches okay you can you can include like an overlap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1913" target="_blank">00:31:53.680</a></span> | <span class="t">so you're going over those over like you're not missing anything between patches so you're just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1919" target="_blank">00:31:59.680</a></span> | <span class="t">sliding through your image and with each part of the image that you slide through you extract the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1925" target="_blank">00:32:05.360</a></span> | <span class="t">image from that you process it through clip and then you compare the encoding for that image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1931" target="_blank">00:32:11.600</a></span> | <span class="t">against the tips that you create so a fluffy cat and what we will see is that patches of the image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1938" target="_blank">00:32:18.720</a></span> | <span class="t">that contain what it what it is you've just described will have a higher similarity rating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1943" target="_blank">00:32:23.920</a></span> | <span class="t">okay and then you can overlay those scores back onto your image and you will find that the that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1950" target="_blank">00:32:30.000</a></span> | <span class="t">clip is able to essentially identify where in your image a specific object is and you are just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1958" target="_blank">00:32:38.400</a></span> | <span class="t">describing that image using a natural language prompt now these are only a few use cases of clip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1965" target="_blank">00:32:45.200</a></span> | <span class="t">and only really scratch the surface of what is actually possible with this model we also see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1972" target="_blank">00:32:52.000</a></span> | <span class="t">it being used in like i said the diffusion models like dali which is a great example of how powerful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1978" target="_blank">00:32:58.080</a></span> | <span class="t">clip can actually be so that's it for this introduction to clip i hope it's been useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1987" target="_blank">00:33:07.520</a></span> | <span class="t">as i said we're going to go into more detail on the different use cases of clip and how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1991" target="_blank">00:33:11.920</a></span> | <span class="t">apply clip for these use cases in future videos but until then that's it for now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fGwH2YoQkDM&t=1998" target="_blank">00:33:18.000</a></span> | <span class="t">so thank you very much for watching and i will see you again in the next one bye</span></div></div></body></html>