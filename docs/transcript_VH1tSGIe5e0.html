<html><head><title>Llama 1/2/3/4 by Hand: Prof Tom Yeh</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Llama 1/2/3/4 by Hand: Prof Tom Yeh</h2><a href="https://www.youtube.com/watch?v=VH1tSGIe5e0" target="_blank"><img src="https://i.ytimg.com/vi_webp/VH1tSGIe5e0/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>Yeah. OK, now it's been recorded. So I all of a sudden feel nervous. OK, yeah. So what do we want to do today? So I could just go ahead and just go through a lot of live coding in Excel. I'm going to show you how we can build deep learning architecture from Transformer to a lot of new stuff.</p><p>Or do you also want to take steps to take questions? How interactive is this session supposed to be? It gets pretty interactive. People talk in chat. If there's anything interesting, one of us will let you know, or people might unmute. But pretty free-flowing. I think we do it the same way.</p><p>We don't really look that deep into benchmarks. We kind of skip that. People can look at numbers themselves. But it's fairly interactive. Sounds good. We'll try to be interactive as if I'm teaching. This is more like a club. It's more a club rather than lecture. So I want to be more interactive.</p><p>I'll stop and I'll take questions. And I'll also ask questions to you, too. So you have a lot of things you know better than I do. So what I want to share is my live Excel spreadsheet. So I've been using this a lot to teach deep learning architecture. And so this is sort of my plan to take you from vision transformer to LAMA 1, 2, 3, 4.</p><p>And as I said, a few of these things are being introduced you might have heard of or read about often at the paper. So we'd like to talk about ROPE, talk about IMS, NORM, talk about group query attention, LAMA 2, and flash attention, interleave attention for LAMA 4, mixture of expert.</p><p>A bit more like high-level, well, I wouldn't say high-level, it's an Excel-level overview of what is happening, some live coding. And so I would like you to go to, I actually prepared a link for you to download, to help access to the spreadsheet very easily. Let me just, let me just find the link.</p><p>I should be a bit more prepared, but I don't know why. So how, can you give me a, if some of you might have access to the document already, and maybe you can see a thumbs up so I know how many of you. I just shared it in the Zoom chat, by the way.</p><p>I'm going to share it on the Zoom chat right now, and so you have a direct access, wait, where's my Zoom chat? Okay, this, here is my Zoom chat, I should have another version, so that is like, they have to have the mailing list subscription gate. So this is the non-gated version of the link, you can go there, and you will see, and you will see the top link is a live version of the same Excel sheet.</p><p>And the, but the, there's also a baseline version that I'm not going to touch today, so you can compare how things evolving. And so that's what my plan to do, and quick overview of my transformer, vision transformer architecture, that is expressed in Excel. If you will zoom out, you can see this whole stack, from the input stack, normalization, self-attention stack, to feed forward, and output layer, softmax linear layer, and loss gradient.</p><p>And we're going to do that, and want to explain this, and the reason I picked this is that the transformer stack part is pretty common across modalities. So I picked vision transformer, so I picked vision transformer, so that just to get an idea, anything that you could, anything that it could convert your input into some tokens, you can put it into transformer stack.</p><p>I picked vision transformer stack, from this point, from this point, this is where a transformer encoder started, or a decoder. In this case, it's the encoder, but for GBT, it's a decoder. Anyway, so let's take the first challenge, so I hope that you could probably just get it right away, because it's not technical, you have seen transformer before, you might not have seen this format.</p><p>But you'll get it very quickly, so what I want to bring your attention, to the attention layer, here, that I'd like to show you, how I would start by, let's go back from my plan, my plan is to just to get, well, from transformer to llama, one, two, three, four, a lot of things scale up, just increase in dimensionality.</p><p>So we're going to see how it is like, we're going to focus on query and key, if we increase query and key, what happened, so if I increase query and key, so in this case, I want to read this a little bit, so we'll have input, this is a, if you go out a little bit, these are tokens, we have 10 tokens here, and so here, one, two, three, four are the embedding dimension, they're currently five, and so this will be input, and to start from attention stack, we'll multiply with all this weight matrices, I get this, so this one, so this one, we'll then, so have query and key here, and then we get attention, it's dot product scale attention, and we solve max, that's how you can read it, and you can see per column, this is solve max, give it distribution, and then you multiply with your value, that's how attention, so the work, the way I visualize it, and so what happened is that if you add, if I want to add, so this is three, the key, key dimension here, you can visualize three, and over one, over one, two, three, four, five, or five tokens, okay, so far, so good, and what happened, if I want to add one dimension, what I would do live, is I would just shift this up, down here, so I have, this is kind of broken now, so I can add some more weight, like this, okay, and now, it's become, it become, a problem here, is that if I like to get my keys over here, maybe use my keyboard instead, I hope you are following me, fast enough, okay, so here, I have my query over here, because I'll have, she'll have four dimensions, let me remove this, push it down a little bit more, so I have four dimensions for my query, so I have four dimensions, now this scaled up dimension, scale product is broken, because my keys are four dimensions, so I do something similar for my key, so I'm going to move down, so I'm going to move down, and then introduce a few more, initialize some random weights, maybe zero here, and then I will update my equation here, so now I just managed to add one dimension, so now, if you see this inductively, you can see how this is scale, and the takeaway is that the key and query, they should have the same dimensions, four and four, but value doesn't have to be, but even though, typically, we keep the value the same dimension for convenience reason, but theoretically, this is the way you can visualize that the key and value increase in size, and what happens if we don't add one more token, so let me see, I check this, so I check this, maybe use this highlighter to say I'm done with this, okay, and I add, when I add key and value dimensions, you realize nothing else change, other than the computational complex plus flexibility per cell, now I have four things to dot-proc dot-width, okay, here, matching multiplication, softmax, this is the raw interpretation of softmax, and we have another matching multiplication here, okay, all right, so, and what happens if you want to add value, say you want to add to five, just to prove my point that they don't need to match my key and query dimension, okay, and I have five of this, okay, and then, and then, so, then, I have five over here, now, I can ugly, let's just add some more space here, and insert here, and now I have this five over here, now, all of a sudden, my attention-weighted values from this attention head is five dimension, and I'm going to zoom out a little bit to see what's going to happen if we have, we count the attention head, now we have first attention, second attention, and third attention head, and we have things that collect them, so, this is where concatenation happened, now, we have two extra dimensions for my values, and now it's no longer fit, what I can do is that I'm going to increase my dimensionality of the attention-weighted picture from the first head, and move up here, now I have, now you can see that it will all fit, okay, so I have five from first head, three from second head, and third from, and three from third head, this is three head, multi head attention, and then, but this is kind of important, because my embedding dimension is five, and then it has to be five consistently throughout your stack, so they can put it all together, but now, I have one, two, three, five, one, two, three, five, six, seven, eight, nine, ten, eleven, twelve, twelve to five, that's what we need to, we need to project, so I have to introduce more weights, so I want to introduce three, three more weights, so I just copy over here, and then, I update my modification, linear projection, all the sudden, things got, things are working again, in, actually, I had too many, I should add two, instead of three, okay, two, okay, so I'll kind of check my task, okay, all right, now it matches, it's working again, so I just added one, two dimension, two value for the particular decision head, and I show you how everything else change, okay, anything else?</p><p>How focus cell makes green across along the ratio here, view, show, focus cell, here, and select your green color, I found green is pretty good for my class, and to help you focus attention, anything, any question? What do I do, okay, all right, where was I, okay, so I checked the value size, I just changed it, go back, and vocabulary size, let's just talk about vocabulary size, so at the end, we want this model to output a word, and our probability distribution across the word, so at the end, we still have, remember, this is a final output from encoder, still five tokens, ten tokens, each token is five dimensions, is it?</p><p>So, but supposed to be a vocabulary of 20 words, in this case, for the original vision transformer, there's 20 class problems, then we take the first head token, this is a class token, so we need to project from five to 20, so now you can visualize this linear projection right here, so this is what linear, in the last thing, taken to the last thing of your transformer stack, and you do a soft max, let's just zoom in a little bit, so you could see these, these are the values, though this other value could be arbitrary number, but we want it to become a number between zero and one for probability, probability, but also, all these numbers have to add up to one for probability distribution, that's why we need soft max, so this is linear and soft max, suppose we want to apply, so as you can see, for llama one, two, three, four, it goes from, where is the vocabulary size, from 32k, 32k, 128k, so it's going to be multilingual models, even more, 250k, because it's multilingual, multimodal models, they want to have more, they want to have more things they can predict, okay, so, and then, that progression is mostly reflected in the last layer, so instead of 5 to 20, for instance, we want to go from 20 to 30, what do I have to do?</p><p>so, where do we have to grow this? so, maybe a little bit too much, let's just grow 5 by 5, so what I would do is that I would just 1, 2, 3, 4, 5, select 5 rows, and insert, and then, I have to fill in this row, this is initialize to 0, maybe I just copy all my 0 over here, and these are all the biases, minus to 0, 0, 0, 0, and now randomize this a little bit, but adding some random one here, so now I updated the score here, and then, so I also have to do that, update it by solve max layer, add it in the file row in the middle, so I have, now these equations get updated, so I just increased the vocabulary size from 20 to 25, so that is where you increase, but interestingly, as you can tell, the only thing I touch is the very last layer, I didn't have to touch any internal of the transformer stack, okay?</p><p>So, if you're looking for this sheet, so I have a, I could use this one, this is the link result, there's no newsletter subscription requirement, just go there and just grab stuff that you can follow this, and, alright, so let's go back to the number of vocabulary size, I looked at it now, and embedding dimension is a bit difficult to change, I'm not going to change, I just say, if I have to change, I have to change here, this is a token, so this will be the, this is the, from image patch to token, about 9 to 5, so 9 is because of a 3x3 window here, and again, it got flattened into a column vector of 9, but for the language application, but for the language application, you can think of it as from a word in the vocabulary to embedding, and so we have 5, embedding space of 5, if you want to add one more, I will have to add one more rule here, but the problem is, is everything else has to be extended by 5, this all has to change to 5, there's a lot of things that have to change, so I'm not going to do that, so, well, I just restore it, but what, but what I want to do, actually, maybe I do want to do it for the, for the, for, let me just do it, but I'm not going to change the whole thing, so I, now I just add one dimension, where else do I have to change, so I, so this will go to the norm, so this has to change to 5 as well, and this, this is the most important, if I have a 5 of this, I have to move this, I have to move this weight, I have to add another columns of weight, in order to multiply them well, so you can see the impact of adding more dimensions, for your embedding, is that you want your weight have to grow in this direction, this direction, like here, like here, this is too much change, I don't want, I don't want to change this, I'm going to do that, so this, I'm going to talk about it, so I put, maybe put a gray, just to say that I sort of talk about it, but I'm not really implementing the embedding dimension change in my express, example, okay, and then, let's talk about good query attention, some attention, some attention stuff, so let's zoom out a little bit, I want to talk about, now it sees 3 attention head, what does it take to add another attention head, what I would do, is that I would just create a more space here, okay, and I could, actually no, let's do this, let's do this, I want to select here, come on, let me do this, select this, and this will be empty space, for me to add some stuff, whatever, let's just, okay, so, do I have enough space, let's just do a little bit more space, so to get a new head into this, so what I can do is, I will just copy this, and copy this, all right, and I have this weight, another set of weights, so you might, in prepositation, I usually like to use red to highlight trainable weights, so these are all trainable parameters, when you hear 7 billion model, you are referring to this, shaded red, variable parameters, so the things I do not shade, means they are not trainable parameters, they do not count to the parameter count, but they still matter in terms of calculation, so they all need to be calculated, they all take up, uh, runtime memory, you have to, all have to figure out how to fit into your GPU to optimize, but you can visualize, that's computational complexity versus parameter size of the model, but when we talk about size of model, we tend to refer to these parameters, so here, okay, let's try to fix this, where does it come from?</p><p>So this come from, I had to fix this, but I could re-implement this, I'm going to zoom in a little bit, so what do I do, I want to take this, take all the, I want to input, metric multiplication, to take all state of weights, and then, and I'll go up, and up to select, they all share the same input, across this head, they all, uh, multiplying with the, this is the output from the previous layer, which is a norm, okay, so this is how I implement it, so you can see this, and, you can use F2 to examine this, so just to show you, I can scroll up to see, okay, this is where it's selected, and when I select this, okay, this is where it's selected, it's a different kind of weight, because I copy and paste it, so this, all the weights are the same right now, but I want you to purposely change the view thing, so they have two different set of weights, okay, so now, now, what I just manually implemented, a new head, and the rest of it, the same, so we have query, this, you take the first, one third, and to move over to the query, and take a second, one third, and then, and put it on the key, but I have to transpose it, maybe I can use my pen to draw it, like this, to show you a data flow, like here, and here, and then, you do a KT, multiplication, to get this dot product, scale, attention, and scale it out with soft mix, soft mix, soft mix, soft mix, okay, hard to write, with my mouse, okay, so I have a three different head, so what do I get, I get a more attention, values here, over, we'll have to add this, we'll have to bring this three back here again, so let me just move up, move three spaces up, so create some space for here, I'm going to select here, to concatenate, so what I'm implementing is concatenation, when you concatenate tensors, that's what's happening here, so if I do that, and this whole thing gets taller, so I have to extend my weight matrix, sidewise, sideway, by the same length, so I'm going to copy three sets away here, and once I do that, I'll be able to update my metric multiplication, that it is linear projection over here, to match, so if you read the paper, so usually they have says W out, or down, what is it, down project, W down, is referred to this metric, this metric, this metric can be pretty big too, but sometimes, if you kind of divide your attention dimension in a way, you might not even need this, suppose I, instead of having this many heads, suppose instead of five, I have, say 100, and then we have 20 heads, and then, and then, and then each head has five dimension, and when that concatenate all that, I get 100 back, in this case, you can skip this, dimensionality change, with this matrix, anyway, so this is how I, things are impacting when add one more head, so I do that, so add more head, do I add more heads here, where is it, heads, okay, when we add more head, that's what happened, so there's a head, so there's a head, so there's a head, adding head from llama two, llama three, over here, so I think I implement this, I'm going to put, highlight this, okay, come back, alright, so any more, let me see, I'm just monitoring the chat, but I guess, maybe, should I take one question, before I move on, I want to talk about group query retention next.</p><p>Yeah, there's a question in the chat about vocabulary, how much does it cost in vocab size to add native understanding, slash generation of images and audio, so text only llama had 128k vocab, but multi-modal llama had 256k, I wonder if you want to address that, before you, yeah. So, if I zoom out a little bit, the vocabulary size, have only impact, like in, like in the output here, so you will grow this for output vocabulary, and then if you have three modalities, you just, this will get longer, and then you also need to figure out a way to convert your input into the embedding here, so this will be got a lot longer over here, but internally, once you pick the model size, this is going to be the same.</p><p>Okay. Okay. Does it make sense? Yeah, that makes sense. We increase the number of vocab. Increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab.</p><p>We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We increase the number of vocab. We have not significantly introduced a lot more weights. As you can probably notice. But if we add a new layer.</p><p>What's going to happen is. They just mark what the layers are. So layers come from here. From norm. To self-attention. To multi-header attention. Actually. And to all the way to here. Right before the output layer. So there's one encoder or decoder block. So if I select this whole thing.</p><p>Can I select this? Too small. Okay. And I'm going to select all this row. Going up, up, up. Okay. It's kind of slow. But let's do it slowly. So I don't break anything. So now a quick review of. You can now see in group query attention. You're seeing self-attention.</p><p>Multi-head attention. Right here. So this is my one block. Okay. And then copy. And now going to just reinsert right here. Insert my copy cell. Now all of a sudden I have two blocks. So I just have to connect them a little bit. Let's just say. What do I do?</p><p>Let's go. Maybe coloring a little bit. So I can distinguish them. So I'm going to color the first row. First column of the second block here. Is a different color. Is a different color. So let's use. Maybe use. Use. Use this. Pink. Okay. So this is my second block.</p><p>Let's just connect them a little bit. So yeah. This is actually ultimately connected. Because they're relatively positioning. So this. That's. Output from the last encoded block. Get. Add to the norm. About normal. I miss norm. Later. So this is layer norm. It's being implemented over here. By the way.</p><p>So, but this is going to be the same. And this only thing different is that. So this will be taking. First token from here. Instead. Instead of all the way in the top. So right now is. All the way at the top of here. So I could. Move is down.</p><p>But it's kind of hard to move this. I just made you do it. It's not that hard. Move. Move. Move. Move. Move. Come on. I just lost it. It's just re-implement this then. Okay. So I want to implement linear layer. Linear layer. I just do a metric multiplication. And select all my ways and biases here.</p><p>And then select my first token. So I have an actual one to take care of bias. So this is. I just re-implemented the linear layer. And the soft max is the same. So do not change the soft max layer. So. So surprisingly this only thing I had to change.</p><p>And I will able to add another layer. And everything's the same. But he said. He said that all this way is going to be different. Even though I copy and paste are same weights. But ever you back propagate and stuff. They were going to have different weights. Let's just look at that.</p><p>What happened here. If you zoom out now. We have. Let me zoom out now. Now I have two. Two encoder blocks. So you can visualize that the weights just increased by two times. And that was just increased by two times. And that was just increased by two times. And that was just increased by two times.</p><p>And that was just increased by two times. And that was just increased by two times. And that was just increased by two times. And that was just increased by two times. And that was just increased by two times. And that was just increased by two times. And that was just increased.</p><p>And that was just make it prettier. I'm going to color this into a different color. So to show that we have two different block. Maybe this is faster here. So this is another. Another block. The first block. And second block now. Okay. So you can see how that they grow.</p><p>Okay. Blue. Is it blue? Okay. Fine. Blue is fine. So we'll have. Now visualize it. We have two blocks. And we also have. One group. Attention head. And each block has four attention. But it doesn't have to be. For instance, I could totally just remove this. If I want to.</p><p>What's most important in my opinion. Is the fact that they all have a fixed dimension of five. That's how you can stack them up very easily. It's not a lot of dimension changes between the block. But they have dimension changes internally in the attention layer. For instance, you can see that from five to three, three, four.</p><p>And one of the other heads from five to three, three, three. And then concatenate. You get this bigger one. And you have to project back again. So this is necessary. Because you like them to have the same five dimension. Now, okay. Pay attention to the linear layer. So this is the linear layer.</p><p>Maybe we can talk about. Multi. But make sure the expert right now. So this is one. You can see the one attention layer over here. Okay. Do you need skip connection for the residual stream? Good question. So this is actually building already. You can see skip connection. This purple.</p><p>This orange thing. So that allow means to float all the way down. Skip over here. And add it over here. So this is things that are being added. You can check the equation. So I'm adding the blue thing with this orange. Red thing coming all the way from the above here.</p><p>So without skip connection. When we remove skip connection. It's basically just not doing anything. Just delete this. Now I don't have skip connection anymore. Okay. So if I do that. I'm going to show you a feature of Excel. You probably. A lot of people don't know. But I found it very useful.</p><p>For teaching and for research. Is this thing called. What else I did? I just removed the skip connection, right? So let's see. If I do a trace precedent. You can see that. I'm going to see that. Oh, actually. Actually. Let's just zoom in a little bit. You can see the error.</p><p>So if we do trace precedent. That Excel has a building. Building way to show. Show this kind of. Pretty cool. Pretty cool. This back propagation. Sort of visualization. You can see. How now. This concatenation. Project back. It tell you. They came from. Four different attention heads. And this attention head.</p><p>If you trace back. And came from. This. Dot product. And then came from this. Eventually. Came from this input here. Do you see this? Kind of cool, huh? But you remove. Let's remove the arrow. Now. What if we add. This skip cognition back. Add. And go all the way back up.</p><p>To select this. Select this one. Okay. Now I've just. Restore my skip cognition. And all of a sudden. You have this. Shortcut. To allow your gradient. To flow. Directly back. Here. Right. And then to avoid. Gradient. Exploding. Gradient. Diminishing. Gradient. The question. Also. Increase the performance. And. You can also do.</p><p>Play the opposite. Where did this go? So you can do. Dependence. So you can go. Down. Actually. Doesn't show me. How come. One more time. Dependent. Okay. See. Go all the way down. And dependent. Dependent. Dependent. Dependent. And see this. Visualization. Kind of cool. I like this. Can we have different.</p><p>Model dimension. After each. Transforming block. Instead of constant. Yes. In theory. You can. They cannot prevent you. From doing it. But. So let's see. If I just. Purposely. Purposely add this. It would. Purposely have. Another dimension here. Okay. Um. So this is my multi. Layer perceptron. Before. Or positional white.</p><p>Before network. And so. Usually. You take. This input. From previous layer. Five. And then you project. Up. Higher dimensional space. To capture some. High. High. Higher. Other logic. But I had to project. Back to five. So they'd be consistent. And just. For the argument's sake. That's what. Why won't project.</p><p>Back to six. Instead. So what I have to do. Is just have one. Extra layer. Of weight. Okay. One. Okay. And then we'll. Fit. Now. Could just review. Five. To ten. And ten. To six. Okay. So this had. This. This block has a different. The D model. So I have six.</p><p>But the thing is. It doesn't fit here. How do you. You have to get six from five. What can you do? What are your options? Go from six. To five. The easiest option to. And then you introduce. Another linear projection. In the middle. Which will be. Five by six.</p><p>Matrix here. As you can visualize here. You can just. If you put this matrix. Over here. And do a linear projection. Then you can allow. To match. Another problem. With this. Mismatch dimension. Is that. How do you even. Skip connection. Even on the brain. Input from the. Preview. Input from the.</p><p>Preview layer. And it just. Doesn't match. You have to. Project in the game. So it's. That's what. Used to be the case. For. Before. Transformer. When we. When that. We were dealing with. Convolution neural network. The. Original version. Or original risk. Resonate. You have this. Dimensional changes. Over. Over the stack.</p><p>And. Oh. Another benefit. Of. I just. Now just go back. I just control Z my way. Back to where. where we were. Oops. Too much. Just highlight this. I did this. Now I did this. Okay. All right. Oh, another thing is that you want to highly optimize. If you've got a way to fit this bi-dimension model, most nicely on the GPU, if you get an option for that, you want to be able to use it for all your blocks here.</p><p>So you could optimize only once. They can be used for entire stack. Does it make sense? I hope. Oh, if a new model architecture is designed, how are the number of layers determined? Trial and error. I think it's just kind of, it's a trial and error as well. And then for, or how many GPUs do you have to train them?</p><p>So only a few companies in this world have gigantic GPU farm. They might universally, it doesn't. So if you look at 128, it's non-trivial. I mean, where was I looking at? Layers. Where are the layers? 32 layers. There's same, not that many, right? But huge. It's a lot, a lot of layers to train them.</p><p>And there's 32 steps to propagate. Then you have a huge batch to have to go through for your gradient descent. It's very expensive. Okay. But now I want to talk about, maybe I talk about this one, V4 network. I haven't spent any time to talk about V4 network. Let's talk about the Mitchell expert.</p><p>I think this around this time when MOE has become popular. So there are some variant in Lama 3D studies that use Mitchell expert, but maybe not, but obvious, clearly Lama 4 uses MOE now. That's for sure. So do, let me just do a copy. Do it. Now work on this copy.</p><p>Okay. MOE, where MOE layer is happened to the MLP V4 network here. Okay. So, um, like, so what does it mean by having two experts? Basically, this is my MLP block. I just copy this. And this is it. This is it. Now I have two experts. Not quite yet.</p><p>I had to fix this a little bit. So this one is linear project from the input, from the previous layer, output from the previous layer, and with this weight. So now I just have to modify this to select. They select the same, actually, what happened here? Now I move this.</p><p>Let's move. Doesn't let me. I cannot select this. Oh, here. Okay. Here. Okay. Here. Okay. All right. Now I have done. And I have to take this together. So I would take this, this, add this, and this. I can just, the easiest implementation is I just going to place, I place plus and just add this.</p><p>All right. Okay. So now I have a most basic mission of expert with constant, with one, with equal weight. Okay. I can repeat the same process and to do another, my third expert, I'll repeat the same process and I'll update my input to share the same input from before.</p><p>And then I'll come down here. I also just add the new output. Now they are equally weighted. So this is the equally weighted scenario. And then the question you have in your mind is that, but how can we have a mechanism that they are sort of linearly weighted? So they weighted differently.</p><p>Sometimes this expert get more weight. Sometimes this expert more weight. Sometimes this expert more weight. Also, we have 10 tokens, maybe for a particular token, we, and we like one expert to receive more weight than the other. So what we can do is that we could create another network over here.</p><p>I'm going to build another here called a routers network. And we need to have at least three different weights that we can calculate one for each expert. So to map it out, what I want to do is that I want to have three expert. Each token will have either weight.</p><p>So I want to have a three by three by one, two, by 10, three by 10 of my gated value. This is my gate tensor, three by 10. Did I draw 10? Okay. Now do I, how can I get three by 10 metrics from here? So we need a linear projection here.</p><p>So let's just move everything to the right. Actually, I can do this. shift everything to the right, I can have some more space to work with. So I'm gonna use this, insert, insert, shift sail to the right, will that work? Instead I don't like this color, let's just remove this.</p><p>Okay, so I now kind of map this out, I want to have one, two, three, four, five, this is my weight, this is how many weight I want, so I can do zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero.</p><p>Okay, so this is how many weight, I just initiate to zero, and if I multiply, matrix multiply with my weights here, and then my tokens here, I just don't have binary, so zero, that puts on random weights, so that's my random way for computing gating value, so now I have all this, okay, and then I would like to do a solve max, so these are my relative weights, the term learned by this network, let me just share this, so following the convention, so these are the variable weights, and to implement solve max, so I'll do exponent of this by column, and then sum of the same thing, exponent of this, so this is my solve max, you can notice that this thing, even though it's at 414, but if I show you more, let's zoom in more, you can see, this three number will add out to one, that's what solve max gives you, so I'll copy solve max over here, so that's all my solve max, so each one will be a gate values across like this, so this is what the tensor looks like for your gate, and once we have that, I just lost track after I, okay, so I will bring this gate value over here, maybe just create some spaces here, I'll create some spaces here, and now put some gate values close by here, so here is my gate value, one, the first expert will be this row, okay, and then this per token, that weight is slightly differently, my second expert will be, come back here, this, this row, and my third expert will use this row as the predicted gate value, it's all kind of high, and then, let me just create some more space, I need some more space again, okay, and then I will do an element wise multiplication, to take the output from the first, one more time, output from the first expert, multiply the gate value, this per token, and I'll have this gated, except that, okay, and then this logic is the same for the other experts, I can just copy the formula, so now instead of adding these three rows as is, as an equally weighted combination, now I have a gated combination, so I go back to modify my equation here, so I just drag this down here, so now I finish my dance, make sure the expert, come on, come on, here, where is my hand, doesn't, oh here, okay, here, one more, one more, doesn't let me, one more, move a little bit, oh here, okay, drag here, down here, okay, now I finish, and so for sparse, if that you pick top two, for sparse, you have a mechanism that just pick two highest value, and say the other one to be zero, for instance, zero, four, one, zero, so you can have a equation to do something like this, and then what, and then I use this as a gated value instead, there will be, there will be a sparse mixture of expert, but excel, I find there's no easy way to do this, so I'm not going to bother today, okay, so, uh, so now I think I finished this as well, mixture of expert, did I did mixture expert, okay, so, anyway, I think I could top flash attention, or just have a conversation right now, is there a room for improvement within the domain of attention mechanism, NSA being the latest innovation of DSEC, or is there a natural boundary in your opinion?</p><p>I think with NSA is native, uh, native sparse attention, like, do you want me to do that? Uh, for sparse MOE, do you normalize and F select in the top K? That, I guess, probably, if you happen to have, uh, like, uh, RMS norm along the way, so normalization is probably, uh, not as necessary, because we'll like this, this to learn to normalize your value across your experts, and they'll normalize in some way, but it does, it can't, so it's, it's no theoretical justification one way or the other, but I guess it's just, empirically, if you commit to add a normalization layer, and you committed three months to train your model, you, you already committed to it, it's too late for you to change something, change to something else, but maybe the benefit is marginal, maybe there's no benefit, you don't know, but it also doesn't hurt for you to try, um, okay, so, uh, so, does input and output dimension for each expert match the model dimension?</p><p>Uh, yes, also, it doesn't have to be, I could, for instance, what if I have one of the expert to output more, how can we have, say, expert three output more, a longer token? So, we could add another row of weights here, so let me show you, maybe add, I just moved by two, two spaces, maybe add two, two set of weights, so this is similar to adding two, two nodes in this, this, uh, MLP right here, so now all of a sudden, I have seven instead of four, so I update my ReLU here, ReLU is, I think, automatically updated here, okay, here, so I've got seven, now I have seven, the problem is that now you cannot add them together, now what do we do?</p><p>And then eventually, if most likely, then you have to project this down to fight anyway, and to be able to add them together, or you can concatenate, but, but concatenate, eventually, you still have to project, project it to a state dimension to match with, to be able to work with other layers, just kind of, Lego pieces, all this, all these things, they have to have the same dimension for them to stack up together, you cannot have an arbitrary, so, but in theory, you can, why not, you can just add this, add a particular layer over here, and to add another, then add another linear projection to project back, so I will, what I'll do is, I will do seven, one, two, three, four, five, six, seven, and five, so this is how much I need, I need to then change, it's kind of ugly now, let's do, follow the space over here, and then, if I do a matching multiplication, so we're back to five dimension per token, and then, when I go over here, so I fix, I can move my green one over down here, all of a sudden, the equation works again, okay, so now it's fixed here, so maybe I select it for two minutes, okay, here, okay, what else, should I talk about NSA, so the, um, 10 minutes, do we want to hear flash attention, or hear MSA, over here, any of this, rope, RMS, flash attention, NSA, native space attention, so sparse attention, so you can see that it is a dense attention, so we have, we derive the entire, uh, entire query over here, okay, so my understanding of the sparse attention, is that, let me just use a, uh, uh, mix, move a copy here, create a copy here, so this is my attempt to explain NSA, maybe, based on my memory, I hope I'm doing it correctly, so what we can do, is that, now I have 10 tokens, what if, I take three tokens here, okay, three tokens here, and try to merge into one token, so it becomes more sparse, it sounds like, makes sense, maybe this is the two, too big, let's use a smaller one, maybe here, uh, I want to take these three tokens, and, and then somehow condense all the information I need into one, so let's just draw some space here, this is my goal, I want to take this, these three tokens, let's just highlight a little bit, green, into this blue, okay, so what do we need to do, so first, we need to flatten this, so we have nine elements here, and from nine to three, how do we go from nine to three, we need a linear projection here, so that way is three by nine, make sure it's right here, so I can, I get lazy enough right now, I can use rain array, and we get three rule and nine column, so I immediately, immediately get the make, the, the mixture I want, and then I will just do a make sure modification, make sure modification, come on, you can say make sure you have to do this, and take this, okay, and then, and then I take this, so I cannot do this yet, I have to do is, I want to do this, but I want to convert it into a column, two columns, two, too small, two columns, and then, I cannot see it, I said, but I just read it out, now, okay, but then I get this, now I'm done, okay, so let's just see, just review this, okay, so this is, and all right, so if I repeat the same thing for these three, so I have this one, and I select the same weight, they share the same weights, okay, and then the last one, another block here, so also to repeat this, copy over here, I just select the same weight, now I, I'm working with only three queries, did you see this, only with three query x, and then, so if I do the attention weight matrices here, let's re-implement this, so I'm going to create some space for me, ah, come on, this is ugly, but, well, how do I fix it real quick, because it just erase it, sometimes it's trying to be smart, trying to figure out a format for me, and, okay, and then, and then, so now I just redo this, I have my first query, and then my second query, and my third query, and then we do the same thing for my keys, I think we've got copy and paste, might just work, let me do it, is it right, oh, yeah, actually, oh, yeah, it works, and then you can notice that you have a different set of weights, okay, and now I can do the same thing here, I want to transpose this, transpose this, and then one, two, three, and actually, transpose my another key, and transpose another key here, all right, and then, all of a sudden, did you see the attention weight matrix, oh, the dark product I need to do is a lot, is only here, I'm going to take a comma, and select, let me redo one more time, make tree modification, select my key, and select my query here, and then this is, okay, and then usually I also have divide by the square root of the dimension of key, which is three, so this is typical, but, all right, so this is what we have, and then we take softmax, here, I can copy the format here, so it looks pretty, okay, so now I can zoom out, you can come in on the, save the difference in terms of the computational complexity, so it's not sparse, you can see here, here, so instead of all this, now I have this, okay, and then the native part, the native part is that this is part of the training, so I'm going to do this shading action just with emphasis, just to show you that, so this is what the native part is, this is part of the training, so I'm going to do this shading action, just with emphasis, just to show you that, so this is what the native is referring to, just training this, as part of your sparse attention mechanism, okay, let's just look at the non-sparse attention mechanism, how does it work, so if I'm going to take this into non-sparse attention, how can I convert this one into a non-sparse attention, maybe just do a copy, actually, maybe not, let's just change it, so I have this matrix modification for the whole thing, what I can do is that I could just do like this, and so in this case, each query is only going to compare with key in the neighborhood, so I can repeat this, I copy and paste over here, and it slides, select here, and here, okay, and then repeat this again, but I'll make sure I select the right key, maybe in this time, I just do four of this, since it's a multiple of three, but the last one multiple is four, okay, so now it's a lot sparse, you see the sparse, and this is dense, and this is dense, it's a sparse, but the difference between this, I just say, I just say, okay, match with neighbors, there's no extra learnable parameters involved, there's no extra network you learn to do this, but in this case, it's a native sparse attention, and somehow works, you have this, and I thought, what is, well, this is, well, this is, you can visually see there's an efficiency, in terms of the computational efficiency, just fewer matches compute, but you have to give DPC credit, because theoretically, it sounds fine, but you talk about tens of millions of dollars to even just experiment, to try to see where it works, what if it didn't work, it could, it could be the case that they tried it, it didn't work, but they tried it anyway, and they are lucky they worked, and so they probably take the report about it, but I bet they probably tried five, six other things that didn't work, so they never talked about it.</p><p>So, this is like a convolution with strike three, with no overlay, that's correct. What else? I think we're running out of time, two minutes. I enjoyed it, although I have, I can show you my practice sheet. So, real, real quick, so if I stop, share, and share again, to, this is my internal practice, not as pretty, but if you are wondering, so this is, I have, so this is, the things I did not talk about today, is the, So, this is the, so this is from layer norm, to rms norm, and then what else did I talk about, also I talk about, oh, here, so this is rope, it's kind of complicated, yeah, this is rope.</p><p>Um, rotational position encoding. But, I think I can tell you the high level stuff, so, you can see that I put this block of actual computation. Number one, it's not shaded, so this is not trainable, it's all pre-computed, all the rotation matrix can be pre-computed. Number two, it's really close to the attention head right here, whereas the original position encoding is only injected right in the beginning.</p><p>All right, and then you, you hope, all this key connection to bring information down. Now you have 32 layers to do so, and you, you're lucky, if the position coding has any impact down the stack. But with rope, you would add this computation at each head, at each head level, every single stack, get the position really close to where the attention matters.</p><p>So that is probably the takeaway, that being able to visualize the difference between the two. There's a way to visualize this, like this rope, rope is right here. So that's a way to visualize the position coding, right, is at the top. There's a difference. Okay, so hopefully, I still talk about everything, not at the same level of depth, a bit unstructured, and hopefully, you guys have fun.</p><p>I have fun. Very, very fun. It's always good to see a different perspective on how this stuff works. I'm sure people learn a lot about you on that. Yeah, this reminds me a lot of like Ishan's walkthrough, but like actually, higher level, like you did a lot of work in like reducing the dimension so that we can actually hold it in our heads, which I think is very important.</p><p>Yeah. This is an inductive process. And then you, if you get three dimension right, then you, in the future, I see just vibe code. Say 14. You as a model to, to expand this to a higher dimensional space, but underlying math. Math is easier to work out at the lower dimensional space.</p><p>Cool. Well, you know, just to be respectful of your time. If people want more, you know, are you, where to best find your, the rest of your work? Oh, so I enjoy that too. Honestly, I don't tend to get to talk to a taken audience like that. So I, I went all out today.</p><p>So I was curious how it's being received because with my own student, I cannot go at this steps. I mean, cannot go at this, this pace. So I get to talk in one semester worth of stuff in one hour. And I expect that you could follow most of it or all of it.</p><p>So I enjoyed it to give me the opportunity to go nerd. Yeah. On this. Awesome. Um, yeah. I mean, we, we've been covering a paper here every week for the last two years. So, um, there's, there's been a lot of interesting, uh, lectures and papers and, um, yeah, definitely like some of this is repeated.</p><p>Some of this is new or like a new perspective on the same thing is actually always useful. So did, uh, the Excel, uh, online live up to the enough fast enough for you to check, to walk around the worksheet. I'm curious from your side of the experience. What is this like?</p><p>Uh, Steve Saito says Excel is pretty fast. And you were able to also check the equation for me last and yeah. Anyway. Okay. I, um, yeah, I, I did prepare. I thought it was fun. So I spent quite a lot of time to think about how I should cover this.</p><p>And I'm glad there's mostly most of you still stay and we'll hope to do this again for some other things I didn't get to cover. Uh, the life, uh, spreadsheet was very, very useful. I feel like I still have a lot to dig back into, but yeah, the formula is showing what.</p><p>Yeah. Multi-headed attention transposing and stuff. A little back prop was crazy. Very, very cool. Okay. Thank you. Thank you for the invitation for this, this opportunity. I hope that I'll have opportunity to come back. And because then I can go, I can kick out with you. Always. Um, I think for next week, I'm probably going to invite someone from Prime Intellect to cover Intellect 2.</p><p>They put out a paper on that this week. So what do we think? Distributed GPUs on the blockchain. Let's go. Yeah. Sounds good. Sounds good. And it's like a good paper. So we'll, we'll invite the team over. I'll share the, uh, paper. Yeah. Uh, you know, for, yeah, I think that the, the, the really, the, the thing, the interesting thing is like, does RL and, um, long chain of thoughts sort of, uh, training actually introduce new training paradigms where you, you're the, the, the hardware requirements are actually different and they actually don't benefit from the normal centralization factors.</p><p>Yeah. Cool. All right. Well, thank you so much. Thanks everyone. Bye. Bye. Uh, so Sean, is it possible that you can share with me the chat history? How do I save it? Cause there's a lot of good questions.</p></div></div></body></html>