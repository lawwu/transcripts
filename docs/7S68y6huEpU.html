<html><head><title>Phi-1: A 'Textbook' Model</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Phi-1: A 'Textbook' Model</h2><a href="https://www.youtube.com/watch?v=7S68y6huEpU"><img src="https://i.ytimg.com/vi/7S68y6huEpU/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./7S68y6huEpU.html">Whisper Transcript</a> | <a href="./transcript_7S68y6huEpU.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">The importance of the new PHY1 model isn't just that it's small enough to be on a smartphone,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=4" target="_blank">00:00:04.680</a></span> | <span class="t">set to be open-sourced and capable of interview-level Python coding tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=10" target="_blank">00:00:10.520</a></span> | <span class="t">Its significance is also in what the model tells us about the future of language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=15" target="_blank">00:00:15.060</a></span> | <span class="t">and the timelines of our march to human-level intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=19" target="_blank">00:00:19.060</a></span> | <span class="t">I spoke in depth with one of the authors of the paper, Ronan L. Dan,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=23" target="_blank">00:00:23.140</a></span> | <span class="t">to get you more insights and I'm only going to cover the best bits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=26" target="_blank">00:00:26.560</a></span> | <span class="t">So let's start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=27" target="_blank">00:00:27.800</a></span> | <span class="t">First thing to notice is how small this model is at 1.3 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=33" target="_blank">00:00:33.440</a></span> | <span class="t">But what does that number mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=35" target="_blank">00:00:35.140</a></span> | <span class="t">Well, for reference, that's about 1% the size of GPT-3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=40" target="_blank">00:00:40.160</a></span> | <span class="t">which was behind the original chat GPT phenomenon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=42" target="_blank">00:00:42.920</a></span> | <span class="t">And if recent rumors are to be believed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=45" target="_blank">00:00:45.220</a></span> | <span class="t">it's about a thousand times smaller than the combined parameter count of GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=50" target="_blank">00:00:50.640</a></span> | <span class="t">So we're talking a tiny model here that could fit on my Samsung S23.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=54" target="_blank">00:00:54.800</a></span> | <span class="t">We read that despite this small scale,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=57" target="_blank">00:00:57.660</a></span> | <span class="t">it's by one attains a pass at one accuracy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=60" target="_blank">00:01:00.420</a></span> | <span class="t">that means pass first time, of 50% on human eval,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=64" target="_blank">00:01:04.500</a></span> | <span class="t">testing Python coding challenges.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=66" target="_blank">00:01:06.740</a></span> | <span class="t">Andrey Karpathy of OpenAI and Tesla fame said that we're probably going to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=71" target="_blank">00:01:11.620</a></span> | <span class="t">a lot more of this creative scaling down work,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=75" target="_blank">00:01:15.100</a></span> | <span class="t">prioritizing data quality and diversity over quantity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=78" target="_blank">00:01:18.540</a></span> | <span class="t">using synthetic data to create small but highly capable expert models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=84" target="_blank">00:01:24.340</a></span> | <span class="t">And the author I spoke to actually retweeted that and said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=87" target="_blank">00:01:27.780</a></span> | <span class="t">for skeptics, the model will be available on Hugging Face soon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=91" target="_blank">00:01:31.300</a></span> | <span class="t">Give it a try.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=92" target="_blank">00:01:32.140</a></span> | <span class="t">Back to the paper, which says everyone knows about scaling laws,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=95" target="_blank">00:01:35.260</a></span> | <span class="t">adding more compute, adding more data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=97" target="_blank">00:01:37.580</a></span> | <span class="t">But following the footsteps of Eldan and Li in Tiny Stories,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=101" target="_blank">00:01:41.340</a></span> | <span class="t">which I'll get to in a second, we explore the improvement that can be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=104" target="_blank">00:01:44.340</a></span> | <span class="t">obtained along a different axis, the quality of the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=108" target="_blank">00:01:48.060</a></span> | <span class="t">Of course, anyone familiar with my Orca video will know that data quality is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=112" target="_blank">00:01:52.380</a></span> | <span class="t">super important, but let's get to this paper they mentioned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=115" target="_blank">00:01:55.420</a></span> | <span class="t">And I'm going to give you the 30 second version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=117" target="_blank">00:01:57.740</a></span> | <span class="t">of the paper co-authored by Ronan.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=119" target="_blank">00:01:59.700</a></span> | <span class="t">They created a diverse and synthetic data set of short stories using GPT 3.5 and GPT 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=126" target="_blank">00:02:06.060</a></span> | <span class="t">And then they trained tiny 28 million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=128" target="_blank">00:02:08.300</a></span> | <span class="t">parameter models and smaller actually, which, as they say, are two orders</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=132" target="_blank">00:02:12.540</a></span> | <span class="t">of magnitude smaller than GPT 2, which was only 1.5 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=138" target="_blank">00:02:18.020</a></span> | <span class="t">And by curating the synthetic data carefully, look at the difference in results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=142" target="_blank">00:02:22.860</a></span> | <span class="t">The ending of this story was so much better on the tiny model trained on this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=147" target="_blank">00:02:27.620</a></span> | <span class="t">data set, especially compared to GPT 2, which is so much bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=151" target="_blank">00:02:31.060</a></span> | <span class="t">But it says the soup is too old.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=152" target="_blank">00:02:32.900</a></span> | <span class="t">It's a terrible ending to the story.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=154" target="_blank">00:02:34.660</a></span> | <span class="t">So what did they do for 5.1?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=156" target="_blank">00:02:36.460</a></span> | <span class="t">Well, here is the short version.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=158" target="_blank">00:02:38.180</a></span> | <span class="t">They filtered the stack and stack overflow to only get the most teachable bits of code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=163" target="_blank">00:02:43.180</a></span> | <span class="t">consisting of about six billion tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=165" target="_blank">00:02:45.300</a></span> | <span class="t">They then created a synthetic textbook consisting of about one billion tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=169" target="_blank">00:02:49.700</a></span> | <span class="t">of GPT 3.5 generated Python textbooks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=173" target="_blank">00:02:53.140</a></span> | <span class="t">That's not even GPT 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=174" target="_blank">00:02:54.660</a></span> | <span class="t">Then, quite crucially, they created a small,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=177" target="_blank">00:02:57.620</a></span> | <span class="t">synthetic exercises data set consisting of only 180 million tokens of exercises</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=183" target="_blank">00:03:03.020</a></span> | <span class="t">and solutions. Now, of course, other people have used the stack before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=186" target="_blank">00:03:06.700</a></span> | <span class="t">But as Ronan says, I do think that from the data we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=189" target="_blank">00:03:09.860</a></span> | <span class="t">have, we are not even close to extracting everything from it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=193" target="_blank">00:03:13.300</a></span> | <span class="t">And look at the results of this tiny 1.3 billion parameter model trained in this way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=199" target="_blank">00:03:19.060</a></span> | <span class="t">There have been only two models that have scored more than 50 percent on human eval</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=203" target="_blank">00:03:23.340</a></span> | <span class="t">pass at one that's a wizard coder and of course GPT 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=207" target="_blank">00:03:27.620</a></span> | <span class="t">Of course, those models are massively bigger and therefore much more expensive to train.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=212" target="_blank">00:03:32.340</a></span> | <span class="t">And actually, I find this chart perhaps the most interesting one of all in the entire paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=216" target="_blank">00:03:36.940</a></span> | <span class="t">You can see so many trends in one diagram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=219" target="_blank">00:03:39.980</a></span> | <span class="t">Let me try to pick a few of these out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=222" target="_blank">00:03:42.460</a></span> | <span class="t">And remember, the scores are the percentage accuracy on human eval.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=227" target="_blank">00:03:47.020</a></span> | <span class="t">Think moderate level coding challenges.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=229" target="_blank">00:03:49.180</a></span> | <span class="t">First, look at the consistent increase from when you just train on the filtered stack versus on the synthetic code textbook.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=237" target="_blank">00:03:57.620</a></span> | <span class="t">From 11 to 16, 12 to 20, 17 to 29.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=242" target="_blank">00:04:02.180</a></span> | <span class="t">This could be the synthetic data event horizon that Sam Altman talked about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=246" target="_blank">00:04:06.540</a></span> | <span class="t">And that code textbook was generated using GPT 3.5, not even GPT 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=251" target="_blank">00:04:11.820</a></span> | <span class="t">Next, compare the parameter count of the models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=254" target="_blank">00:04:14.500</a></span> | <span class="t">350 million on the left and in the center and 1.3 billion on the right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=259" target="_blank">00:04:19.500</a></span> | <span class="t">This one isn't as big a surprise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=261" target="_blank">00:04:21.700</a></span> | <span class="t">We knew that increasing the parameters yields better performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=265" target="_blank">00:04:25.060</a></span> | <span class="t">But nevertheless, you can see it vividly in action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=267" target="_blank">00:04:27.620</a></span> | <span class="t">Third, and I think this one is really fascinating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=270" target="_blank">00:04:30.020</a></span> | <span class="t">Look at the difference between the left and the center charts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=273" target="_blank">00:04:33.940</a></span> | <span class="t">The only thing that really changed was the number of GPU hours.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=277" target="_blank">00:04:37.380</a></span> | <span class="t">And of course, the number of tokens went from 26 billion to 76 billion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=282" target="_blank">00:04:42.020</a></span> | <span class="t">But wait, I thought the data set size was fixed at 7 billion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=286" target="_blank">00:04:46.180</a></span> | <span class="t">What gives? Well, of course, what's happening is that they're passing over the data multiple times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=291" target="_blank">00:04:51.220</a></span> | <span class="t">This is called training for more so-called epochs or passes over the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=296" target="_blank">00:04:56.020</a></span> | <span class="t">So these aren't new tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=297" target="_blank">00:04:57.620</a></span> | <span class="t">They're the same tokens being trained on more times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=300" target="_blank">00:05:00.940</a></span> | <span class="t">As Ronan said to me, my personal impression is that many people in the community thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=305" target="_blank">00:05:05.060</a></span> | <span class="t">that we would never want to do more than like one or two epochs because we'll start overfitting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=310" target="_blank">00:05:10.140</a></span> | <span class="t">And just for 20 seconds, I can't resist bringing in this paper that they referenced in the textbook's paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=315" target="_blank">00:05:15.820</a></span> | <span class="t">It's essentially talking about how you can still scale language models even if you run out of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=320" target="_blank">00:05:20.620</a></span> | <span class="t">And take a look at these two diagrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=322" target="_blank">00:05:22.580</a></span> | <span class="t">They say training for up to four epochs or passes is almost as good as new data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=327" target="_blank">00:05:27.620</a></span> | <span class="t">And it's only when you get to around 40 epochs that repeating is worthless.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=331" target="_blank">00:05:31.980</a></span> | <span class="t">Obviously, we don't know about GPT-4, but GPT-3 seems to be trained on far less than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=336" target="_blank">00:05:36.740</a></span> | <span class="t">But there was one final trend from this amazing set of charts that I wanted to point out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=341" target="_blank">00:05:41.900</a></span> | <span class="t">And it's probably the most obvious one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=343" target="_blank">00:05:43.740</a></span> | <span class="t">Look at the huge jump to the dark green bars.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=347" target="_blank">00:05:47.940</a></span> | <span class="t">That's when they train the model on those additional synthetic exercises with solutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=352" target="_blank">00:05:52.620</a></span> | <span class="t">The authors note that one can only imagine how frustrating and inefficient it would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=357" target="_blank">00:05:57.660</a></span> | <span class="t">for a human learner to try to acquire coding skills from such data sets like the unfiltered stack,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=363" target="_blank">00:06:03.620</a></span> | <span class="t">as they would have to deal with a lot of noise, ambiguity and incompleteness in the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=367" target="_blank">00:06:07.900</a></span> | <span class="t">We hypothesize that these issues also affect the performance of language models as they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=372" target="_blank">00:06:12.420</a></span> | <span class="t">reduce the quality and quantity of the signal that maps natural language to code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=377" target="_blank">00:06:17.460</a></span> | <span class="t">Let me quickly give you a bit more detail about how they filtered the stack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=381" target="_blank">00:06:21.380</a></span> | <span class="t">They got about 100,000 samples of the stack and stack overflow and then prompted GPT-4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=387" target="_blank">00:06:27.620</a></span> | <span class="t">to determine its educational value for a student whose goal is to learn basic coding concepts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=393" target="_blank">00:06:33.780</a></span> | <span class="t">They then use those annotations to train a random forest classifier that predicts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=398" target="_blank">00:06:38.780</a></span> | <span class="t">the quality of a file using its output embedding, essentially a basic searching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=403" target="_blank">00:06:43.220</a></span> | <span class="t">mechanism to find out which parts of the stack are the most educational.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=407" target="_blank">00:06:47.300</a></span> | <span class="t">But at this point, I want to pause and imagine if they'd used a different prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=411" target="_blank">00:06:51.500</a></span> | <span class="t">Imagine a future paper looking across a different data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=414" target="_blank">00:06:54.980</a></span> | <span class="t">That paper could prompt GPT-4 to annotate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=417" target="_blank">00:06:57.660</a></span> | <span class="t">the educational value for a student whose goal is to learn French.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=421" target="_blank">00:07:01.340</a></span> | <span class="t">Then you could have an amazing French speaking model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=423" target="_blank">00:07:03.980</a></span> | <span class="t">Or maybe they could get it to annotate which examples would be most educational</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=427" target="_blank">00:07:07.980</a></span> | <span class="t">for learning to predict the stock market and then maybe train it on a small synthetic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=432" target="_blank">00:07:12.700</a></span> | <span class="t">textbook of successful previous examples of predicting the stock market.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=436" target="_blank">00:07:16.580</a></span> | <span class="t">I'm just saying this seems to be a model that could be applied elsewhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=440" target="_blank">00:07:20.060</a></span> | <span class="t">And these annotations here were the only times they used GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=444" target="_blank">00:07:24.140</a></span> | <span class="t">The rest was GPT-3.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=446" target="_blank">00:07:26.220</a></span> | <span class="t">And as Ronan says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=447" target="_blank">00:07:27.620</a></span> | <span class="t">GPT-4 is not only great as something we can use directly for better productivity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=452" target="_blank">00:07:32.900</a></span> | <span class="t">but it's also a way to get much better other models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=456" target="_blank">00:07:36.540</a></span> | <span class="t">And that's one thing I want OpenAI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=458" target="_blank">00:07:38.380</a></span> | <span class="t">Anthropic and Google to address the capability of their models to train smaller models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=464" target="_blank">00:07:44.060</a></span> | <span class="t">Here, by the way, is an example of the kind of exercises</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=467" target="_blank">00:07:47.340</a></span> | <span class="t">and solutions that the model was then fine tuned on, created, of course, by GPT-3.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=473" target="_blank">00:07:53.140</a></span> | <span class="t">And the authors note that quite remarkably, the model after fine tuning on those few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=477" target="_blank">00:07:57.820</a></span> | <span class="t">over than 200 million tokens of exercises and solutions also exhibits a substantial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=483" target="_blank">00:08:03.660</a></span> | <span class="t">improvement in executing tasks that are not featured in the fine tuning dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=488" target="_blank">00:08:08.820</a></span> | <span class="t">For example, fine tuning on code exercises unexpectedly improves the model's ability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=493" target="_blank">00:08:13.740</a></span> | <span class="t">to use external libraries such as Pygame, even though our exercises do not contain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=498" target="_blank">00:08:18.780</a></span> | <span class="t">these libraries. This suggests that fine tuning not only improves the tasks we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=503" target="_blank">00:08:23.300</a></span> | <span class="t">targeted, but also makes unrelated tasks easier to distill.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=507" target="_blank">00:08:27.820</a></span> | <span class="t">It's this unexpectedness that I find really interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=510" target="_blank">00:08:30.820</a></span> | <span class="t">For example, before training GPT-4,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=513" target="_blank">00:08:33.060</a></span> | <span class="t">did they expect the emergent ability to do self repair or reflection?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=517" target="_blank">00:08:37.820</a></span> | <span class="t">According to this new paper, that ability is not found in GPT-3.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=522" target="_blank">00:08:42.420</a></span> | <span class="t">Going back to the PHY-1 paper, the authors admit that there remain a number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=526" target="_blank">00:08:46.940</a></span> | <span class="t">of limitations of our model compared to larger models for code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=530" target="_blank">00:08:50.340</a></span> | <span class="t">Firstly, PHY-1 is specialized in Python coding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=533" target="_blank">00:08:53.660</a></span> | <span class="t">which restricts its versatility compared to multi language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=537" target="_blank">00:08:57.620</a></span> | <span class="t">Secondly, PHY-1 lacks the domain specific knowledge of larger models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=541" target="_blank">00:09:01.300</a></span> | <span class="t">such as programming with specific APIs or using less common packages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=545" target="_blank">00:09:05.580</a></span> | <span class="t">It's a bit like the more classical narrow AI, good at only a few things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=549" target="_blank">00:09:09.780</a></span> | <span class="t">Furthermore, due to the structured nature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=551" target="_blank">00:09:11.900</a></span> | <span class="t">of the datasets and the lack of diversity in terms of language and style,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=555" target="_blank">00:09:15.580</a></span> | <span class="t">it's less robust to stylistic variations or errors in the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=559" target="_blank">00:09:19.860</a></span> | <span class="t">It's quite funny if you make a grammatical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=561" target="_blank">00:09:21.820</a></span> | <span class="t">mistake in your prompt, it does a lot worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=564" target="_blank">00:09:24.700</a></span> | <span class="t">But what about this? We also believe that significant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=567" target="_blank">00:09:27.660</a></span> | <span class="t">gains could be achieved by using GPT-4 to generate the synthetic data instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=572" target="_blank">00:09:32.700</a></span> | <span class="t">of GPT-3.5, as we notice that GPT-3.5 data has a high error rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=577" target="_blank">00:09:37.500</a></span> | <span class="t">I asked Ronan about that, speculating that it's because GPT-4 costs more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=581" target="_blank">00:09:41.260</a></span> | <span class="t">And he said, yeah, it costs more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=582" target="_blank">00:09:42.780</a></span> | <span class="t">Also, GPT-4 is much slower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=584" target="_blank">00:09:44.860</a></span> | <span class="t">But another reason is we wanted to demonstrate something here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=588" target="_blank">00:09:48.340</a></span> | <span class="t">that you don't even need a smart model like GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=591" target="_blank">00:09:51.660</a></span> | <span class="t">Even GPT-3.5, which isn't that great at coding, is enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=595" target="_blank">00:09:55.980</a></span> | <span class="t">So there you go. You could get even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=597" target="_blank">00:09:57.660</a></span> | <span class="t">better results on this using GPT-4, but at the moment, GPT-4 is a bit too slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=601" target="_blank">00:10:01.940</a></span> | <span class="t">Before I get to timelines, some of you might have noticed the WizardCoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=605" target="_blank">00:10:05.340</a></span> | <span class="t">results and wondered how that model did so well, despite only being 16 billion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=610" target="_blank">00:10:10.020</a></span> | <span class="t">parameters, which of course is 10 times bigger than PHY1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=612" target="_blank">00:10:12.980</a></span> | <span class="t">Well, of course, I read that paper too,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=614" target="_blank">00:10:14.740</a></span> | <span class="t">as well as almost every paper referenced in the textbook's paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=618" target="_blank">00:10:18.500</a></span> | <span class="t">The secret of WizardCoder seems to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=620" target="_blank">00:10:20.780</a></span> | <span class="t">been increasing the difficulty of the training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=624" target="_blank">00:10:24.380</a></span> | <span class="t">Fine tune the model with more difficult examples, e.g.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=627" target="_blank">00:10:27.660</a></span> | <span class="t">if the original problem can be solved with only a few logical steps,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=630" target="_blank">00:10:30.740</a></span> | <span class="t">please add more reasoning steps, maybe complicate the input or deepen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=635" target="_blank">00:10:35.020</a></span> | <span class="t">the question or increase the reasoning involved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=637" target="_blank">00:10:37.740</a></span> | <span class="t">You can start to see the shared themes of Orca, WizardCoder and PHY1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=642" target="_blank">00:10:42.780</a></span> | <span class="t">This could be what Sarah Constantine was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=644" target="_blank">00:10:44.580</a></span> | <span class="t">pointing to in the Asterisk magazine that I read yesterday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=648" target="_blank">00:10:48.340</a></span> | <span class="t">I'm not sponsored by them, but it was a great issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=650" target="_blank">00:10:50.580</a></span> | <span class="t">So do check out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=651" target="_blank">00:10:51.580</a></span> | <span class="t">She said rather than a refutation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=653" target="_blank">00:10:53.500</a></span> | <span class="t">of scaling laws or an acceleration of their slope, I think this is more like a move</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=657" target="_blank">00:10:57.660</a></span> | <span class="t">in a different direction altogether towards a Cambrian explosion of little AIs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=662" target="_blank">00:11:02.260</a></span> | <span class="t">used for different purposes where getting good performance on a task depends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=666" target="_blank">00:11:06.260</a></span> | <span class="t">on the quality of your task specific dataset like PHY1 for Python.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=670" target="_blank">00:11:10.820</a></span> | <span class="t">That could be consistent with the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=672" target="_blank">00:11:12.380</a></span> | <span class="t">of the art continuing to progress steadily along scaling law lines for quite some time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=676" target="_blank">00:11:16.900</a></span> | <span class="t">But it could also mean the economic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=679" target="_blank">00:11:19.180</a></span> | <span class="t">incentive towards ever bigger models would diminish and would enter an entirely new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=683" target="_blank">00:11:23.340</a></span> | <span class="t">era where AI progress would not be driven primarily by semiconductor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=687" target="_blank">00:11:27.660</a></span> | <span class="t">scaling or Moore's law.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=689" target="_blank">00:11:29.180</a></span> | <span class="t">This relates directly to a tweet from the co-founder of Anthropic, Jack Clark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=693" target="_blank">00:11:33.780</a></span> | <span class="t">He said a world where we can push a button and stop larger compute things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=698" target="_blank">00:11:38.580</a></span> | <span class="t">being built and all focus on safety for a while is good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=701" target="_blank">00:11:41.900</a></span> | <span class="t">That is really interesting to hear from someone at the top of an AGI lab.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=706" target="_blank">00:11:46.220</a></span> | <span class="t">But I do have some questions for this policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=708" target="_blank">00:11:48.420</a></span> | <span class="t">If we freeze compute, wouldn't that incentivize every company just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=712" target="_blank">00:11:52.100</a></span> | <span class="t">to use algorithmic progress to get more out of the compute we do have?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=716" target="_blank">00:11:56.180</a></span> | <span class="t">And so on the safety front,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=717" target="_blank">00:11:57.660</a></span> | <span class="t">I think it's far more effective public messaging to focus on concrete things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=722" target="_blank">00:12:02.380</a></span> | <span class="t">that everyone can understand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=723" target="_blank">00:12:03.780</a></span> | <span class="t">For example, in this paper from Oxford this week,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=726" target="_blank">00:12:06.260</a></span> | <span class="t">LLMs will in particular lower barriers to biological misuse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=730" target="_blank">00:12:10.180</a></span> | <span class="t">Biological design tools will expand the capabilities of sophisticated actors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=735" target="_blank">00:12:15.140</a></span> | <span class="t">Concretely, BDTs may enable the creation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=737" target="_blank">00:12:17.980</a></span> | <span class="t">of pandemic pathogens substantially worse than anything seen to date and could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=742" target="_blank">00:12:22.740</a></span> | <span class="t">enable forms of more predictable and targeted biological weapons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=746" target="_blank">00:12:26.620</a></span> | <span class="t">I think this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=747" target="_blank">00:12:27.660</a></span> | <span class="t">something that everyone can get behind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=749" target="_blank">00:12:29.500</a></span> | <span class="t">And as the paper says, it's been hypothesized that for evolutionary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=753" target="_blank">00:12:33.060</a></span> | <span class="t">reasons, naturally emerging pathogens feature a tradeoff between transmissibility.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=757" target="_blank">00:12:37.740</a></span> | <span class="t">That's how much they spread and virulence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=759" target="_blank">00:12:39.700</a></span> | <span class="t">That's how deadly they are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=760" target="_blank">00:12:40.900</a></span> | <span class="t">AI based BDTs might generate design</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=763" target="_blank">00:12:43.500</a></span> | <span class="t">capabilities that are able to overcome this tradeoff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=766" target="_blank">00:12:46.460</a></span> | <span class="t">Thus, for the first time, humanity might face a security threat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=769" target="_blank">00:12:49.940</a></span> | <span class="t">from pathogens substantially worse than anything nature might create,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=773" target="_blank">00:12:53.500</a></span> | <span class="t">including pathogens capable of posing an existential threat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=777" target="_blank">00:12:57.620</a></span> | <span class="t">To be honest, this is my main safety concern.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=779" target="_blank">00:12:59.380</a></span> | <span class="t">But back to the paper and timelines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=781" target="_blank">00:13:01.780</a></span> | <span class="t">Here is another snippet of my conversation with Ronan.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=784" target="_blank">00:13:04.660</a></span> | <span class="t">I said, I just feel like we are much closer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=786" target="_blank">00:13:06.860</a></span> | <span class="t">to something really transformative than the public has quite realized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=790" target="_blank">00:13:10.620</a></span> | <span class="t">And people like OpenAI puts out that in 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=793" target="_blank">00:13:13.460</a></span> | <span class="t">years we will have something as powerful as a corporation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=796" target="_blank">00:13:16.220</a></span> | <span class="t">I say three to five years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=798" target="_blank">00:13:18.140</a></span> | <span class="t">Ronan replied, that depends on how much resources are actually spent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=802" target="_blank">00:13:22.020</a></span> | <span class="t">into training bigger and bigger models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=804" target="_blank">00:13:24.020</a></span> | <span class="t">I have no idea what OpenAI and Google are doing right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=807" target="_blank">00:13:27.300</a></span> | <span class="t">But definitely if this is our main goal, I think it can easily be five years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=812" target="_blank">00:13:32.140</a></span> | <span class="t">I said, or less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=813" target="_blank">00:13:33.300</a></span> | <span class="t">Ronan replied, or less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=814" target="_blank">00:13:34.820</a></span> | <span class="t">I feel like the bottleneck is maybe the production of GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=818" target="_blank">00:13:38.180</a></span> | <span class="t">And I mean, it's not just to produce the GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=820" target="_blank">00:13:40.820</a></span> | <span class="t">You also have to build the data centers and connect them to electricity, etc, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=825" target="_blank">00:13:45.180</a></span> | <span class="t">I think if you have all that, then, yeah, I don't see the barrier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=828" target="_blank">00:13:48.660</a></span> | <span class="t">With more data, higher quality data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=830" target="_blank">00:13:50.940</a></span> | <span class="t">synthetic data, better and better algorithms and more and better GPUs and TPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=836" target="_blank">00:13:56.900</a></span> | <span class="t">That's what we mean when we say we don't see a barrier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=839" target="_blank">00:13:59.260</a></span> | <span class="t">Of course, everyone has slightly different definitions of AGI, but almost everyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=843" target="_blank">00:14:03.660</a></span> | <span class="t">agrees that the next five to 10 years are going to be the most critical in seeing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=848" target="_blank">00:14:08.620</a></span> | <span class="t">whether more data, better data, better algorithms or just more and more compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=853" target="_blank">00:14:13.380</a></span> | <span class="t">will lead to AGI or superintelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=856" target="_blank">00:14:16.660</a></span> | <span class="t">I loved how Karl Schulman put it on the Dvorkes Patel podcast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=860" target="_blank">00:14:20.700</a></span> | <span class="t">If you generate like close to $10 million a year out of the future version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=866" target="_blank">00:14:26.500</a></span> | <span class="t">of H100, that costs tens of thousands of dollars with a huge profit margin now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=871" target="_blank">00:14:31.860</a></span> | <span class="t">And profit margin could be reduced with like large production.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=875" target="_blank">00:14:35.540</a></span> | <span class="t">That is a big difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=877" target="_blank">00:14:37.660</a></span> | <span class="t">That chip pays for itself almost instantly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=881" target="_blank">00:14:41.100</a></span> | <span class="t">And so you could support paying ten times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=885" target="_blank">00:14:45.100</a></span> | <span class="t">as much to have these fabs constructed more rapidly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=889" target="_blank">00:14:49.180</a></span> | <span class="t">You could have if AI is starting to be able to contribute, could have AI contributing more of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=896" target="_blank">00:14:56.100</a></span> | <span class="t">real technical work that makes it hard for, say, NVIDIA to suddenly find thousands</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=901" target="_blank">00:15:01.140</a></span> | <span class="t">upon thousands of top quality engineering hires if AI can provide that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=905" target="_blank">00:15:05.700</a></span> | <span class="t">Now, if AI hasn't reached that level of performance, then this is how you can have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=910" target="_blank">00:15:10.740</a></span> | <span class="t">things stall out and like a world where AI progress stalls out is one where you go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=915" target="_blank">00:15:15.180</a></span> | <span class="t">to the $100 billion and then over succeeding years, trillion dollar things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=921" target="_blank">00:15:21.020</a></span> | <span class="t">software progress</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=924" target="_blank">00:15:24.460</a></span> | <span class="t">turns out to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=925" target="_blank">00:15:25.700</a></span> | <span class="t">stall.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=926" target="_blank">00:15:26.700</a></span> | <span class="t">You lose the gains that you are getting from moving researchers from other fields.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=931" target="_blank">00:15:31.100</a></span> | <span class="t">Lots of physicists and people from other areas of computer science have been going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=934" target="_blank">00:15:34.620</a></span> | <span class="t">to AI, but you sort of tap out those resources as AI becomes a larger proportion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=940" target="_blank">00:15:40.940</a></span> | <span class="t">of the research field and like, okay, you've put in all of these inputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=944" target="_blank">00:15:44.380</a></span> | <span class="t">but they just haven't yielded AGI yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=946" target="_blank">00:15:46.820</a></span> | <span class="t">I think that set of inputs probably would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=949" target="_blank">00:15:49.900</a></span> | <span class="t">yield the kind of AI capabilities needed for intelligence explosion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=953" target="_blank">00:15:53.620</a></span> | <span class="t">But if it doesn't,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=955" target="_blank">00:15:55.300</a></span> | <span class="t">then you're going to have to wait for the slow grind of things like general economic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=959" target="_blank">00:15:59.620</a></span> | <span class="t">growth, population growth and such, and so things slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=962" target="_blank">00:16:02.900</a></span> | <span class="t">And that results in my credences and this kind of advanced AI happening to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=967" target="_blank">00:16:07.380</a></span> | <span class="t">relatively concentrated, like over the next ten years compared to the rest of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=972" target="_blank">00:16:12.300</a></span> | <span class="t">century, because we just can't we can't keep going with this rapid,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=977" target="_blank">00:16:17.100</a></span> | <span class="t">you know, slow growth of AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=979" target="_blank">00:16:19.100</a></span> | <span class="t">And so I think that's a really important thing to think about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=982" target="_blank">00:16:22.700</a></span> | <span class="t">And I think that's a really important thing to think about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=984" target="_blank">00:16:24.900</a></span> | <span class="t">And I think that's a really important thing to think about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=986" target="_blank">00:16:26.300</a></span> | <span class="t">And I think that's a really important thing to think about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=988" target="_blank">00:16:28.220</a></span> | <span class="t">And I think that's a really important thing to think about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=989" target="_blank">00:16:29.740</a></span> | <span class="t">Thank you so much for learning about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=991" target="_blank">00:16:31.860</a></span> | <span class="t">Phi One with me, and as always, thank you so much for staying all the way to the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7S68y6huEpU&t=996" target="_blank">00:16:36.540</a></span> | <span class="t">Do try to have a wonderful day.</span></div></div></body></html>