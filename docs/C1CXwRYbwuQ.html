<html><head><title>Making Open Models 10x faster and better for Modern Application Innovation: Dmytro (Dima) Dzhulgakov</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Making Open Models 10x faster and better for Modern Application Innovation: Dmytro (Dima) Dzhulgakov</h2><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ"><img src="https://i.ytimg.com/vi_webp/C1CXwRYbwuQ/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./C1CXwRYbwuQ.html">Whisper Transcript</a> | <a href="./transcript_C1CXwRYbwuQ.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=0" target="_blank">00:00:00.640</a></span> | <span class="t">Hello, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=14" target="_blank">00:00:14.920</a></span> | <span class="t">So, my name is Dima.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=17" target="_blank">00:00:17.040</a></span> | <span class="t">As mentioned, unfortunately, my co-founder, Lin, who was on the schedule, couldn't make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=21" target="_blank">00:00:21.960</a></span> | <span class="t">it today because of some personal emergency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=24" target="_blank">00:00:24.320</a></span> | <span class="t">So you got me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=25" target="_blank">00:00:25.840</a></span> | <span class="t">And as you saw, we don't have yet AI to figure out video projection, but we have AI for a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=31" target="_blank">00:00:31.960</a></span> | <span class="t">lot of other things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=32" target="_blank">00:00:32.960</a></span> | <span class="t">So today I'm going to talk about Fireworks AI and generally I'm going to continue the theme</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=36" target="_blank">00:00:36.900</a></span> | <span class="t">which Katrin started about open models and how we basically focus on productionization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=43" target="_blank">00:00:43.080</a></span> | <span class="t">and customization of open source models in inference at Fireworks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=48" target="_blank">00:00:48.600</a></span> | <span class="t">But first, as an introduction, what's our background?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=52" target="_blank">00:00:52.140</a></span> | <span class="t">So the founding team of Fireworks comes from PyTorch leads at Meta and some veterans from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=58" target="_blank">00:00:58.300</a></span> | <span class="t">Google AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=59" target="_blank">00:00:59.640</a></span> | <span class="t">So we combined have, like, probably a decade of experience in productionizing AI in some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=63" target="_blank">00:01:03.640</a></span> | <span class="t">of the biggest companies in the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=66" target="_blank">00:01:06.300</a></span> | <span class="t">And I myself personally have been core maintainer of PyTorch for the past five years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=71" target="_blank">00:01:11.520</a></span> | <span class="t">So topic of open source is really close to my heart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=75" target="_blank">00:01:15.980</a></span> | <span class="t">And since we kind of led this revolution of open source toolchain for deep learning through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=80" target="_blank">00:01:20.500</a></span> | <span class="t">our work sort of on PyTorch and some of the Google technologies, we really believe that open</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=85" target="_blank">00:01:25.680</a></span> | <span class="t">source models are the future also for, like, for gen AI application and our focus at Fireworks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=93" target="_blank">00:01:33.420</a></span> | <span class="t">is precisely on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=96" target="_blank">00:01:36.820</a></span> | <span class="t">So I mean, how many people in the audience actually, like, use GPT and deploy it for production?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=105" target="_blank">00:01:45.420</a></span> | <span class="t">And how many people -- how many folks use open models for the production?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=110" target="_blank">00:01:50.320</a></span> | <span class="t">Oh, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=111" target="_blank">00:01:51.640</a></span> | <span class="t">So I was about to convince you that share of open source models is going to grow over time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=116" target="_blank">00:01:56.300</a></span> | <span class="t">so it looks like in this audience it's already -- already sizable, but nevertheless.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=122" target="_blank">00:02:02.020</a></span> | <span class="t">So why -- why basically this trade-off, why go big or why go small?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=127" target="_blank">00:02:07.700</a></span> | <span class="t">Currently still, like, bulk of production inference is still based on proprietary models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=132" target="_blank">00:02:12.080</a></span> | <span class="t">And the catch is that those are really good models and often frontier in many domains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=137" target="_blank">00:02:17.740</a></span> | <span class="t">However, the catch is that it's one model which is good in many, many sins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=142" target="_blank">00:02:22.520</a></span> | <span class="t">And it's often served in the same way, regardless of the use case, which means that it may be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=147" target="_blank">00:02:27.560</a></span> | <span class="t">if you have batch inference on some narrow domain or you have some super real-time use case where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=153" target="_blank">00:02:33.400</a></span> | <span class="t">you need to -- you need to do, like, voice assistance or something, those are often served</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=158" target="_blank">00:02:38.640</a></span> | <span class="t">from the same infrastructure without customization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=161" target="_blank">00:02:41.160</a></span> | <span class="t">In terms of model capabilities, it also means, yeah, like, GPT-4 is great or Claude is great</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=166" target="_blank">00:02:46.580</a></span> | <span class="t">and can handle a lot of sins, but you are often paying a lot for additional capabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=171" target="_blank">00:02:51.320</a></span> | <span class="t">which are not needed in particular use case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=173" target="_blank">00:02:53.480</a></span> | <span class="t">You don't really need customer support chatbot to know about 150 Pokemons or be able to write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=179" target="_blank">00:02:59.700</a></span> | <span class="t">your poetry, but you really want it to be really good in the particular narrow domain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=186" target="_blank">00:03:06.380</a></span> | <span class="t">So this kind of discrepancy for large models leads to several issues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=191" target="_blank">00:03:11.040</a></span> | <span class="t">One as I mentioned is high latency, because using a big model means longer response times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=197" target="_blank">00:03:17.040</a></span> | <span class="t">which is particularly important for real-time use cases, like, voice systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=202" target="_blank">00:03:22.140</a></span> | <span class="t">It gets more and more important with stuff, because for stuff like, for example, next -- right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=209" target="_blank">00:03:29.420</a></span> | <span class="t">Like, you need to do a lot of steps for, like, something, like, agent-like application to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=214" target="_blank">00:03:34.580</a></span> | <span class="t">reasoning and call the model many times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=216" target="_blank">00:03:36.160</a></span> | <span class="t">So latency is really, really important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=218" target="_blank">00:03:38.220</a></span> | <span class="t">And often you see that you can pick smaller models, like Lama or Gemma, which you just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=223" target="_blank">00:03:43.780</a></span> | <span class="t">talked about, and achieve the -- for the narrow domain, same or better quality while being,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=229" target="_blank">00:03:49.440</a></span> | <span class="t">you know, up to 10 times faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=232" target="_blank">00:03:52.260</a></span> | <span class="t">For example, for some of the function calling use cases, like, externally benchmark from Berkeley,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=237" target="_blank">00:03:57.380</a></span> | <span class="t">yeah, like, the -- you get similar performance from fine-tuned Lama 3 at 10x the speed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=243" target="_blank">00:04:03.440</a></span> | <span class="t">Cost is also -- is also an issue if you're running a big model for -- on a lot of traffic, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=250" target="_blank">00:04:10.000</a></span> | <span class="t">even if you have perhaps, you know, 5K tokens prompt and 10,000 users, and each of them calls</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=255" target="_blank">00:04:15.920</a></span> | <span class="t">SLM 20 times per day, you know, on GPT-4, even on GPT-4-0, it probably adds up to, like, 10K per</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=261" target="_blank">00:04:21.500</a></span> | <span class="t">day or something like several million per month -- or several million per year, which is a sizable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=266" target="_blank">00:04:26.500</a></span> | <span class="t">cost of a startup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=267" target="_blank">00:04:27.500</a></span> | <span class="t">You can easily cut that with much smaller models, and that often we see as a -- as a kind of motivation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=273" target="_blank">00:04:33.960</a></span> | <span class="t">for reaching out for smaller and more customizable models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=279" target="_blank">00:04:39.180</a></span> | <span class="t">But really the -- like, where open models shine is domain adaptability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=284" target="_blank">00:04:44.180</a></span> | <span class="t">And that comes in two aspects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=286" target="_blank">00:04:46.160</a></span> | <span class="t">First, there are so many different fine-tunes and customizations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=289" target="_blank">00:04:49.760</a></span> | <span class="t">I think Caitlin was mentioning about, you know, German built Indian languages adaptations, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=294" target="_blank">00:04:54.980</a></span> | <span class="t">there are models specialized for code or for medicine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=297" target="_blank">00:04:57.480</a></span> | <span class="t">If you had to hide in face, there are, like, tens of thousands of different model variants.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=302" target="_blank">00:05:02.180</a></span> | <span class="t">And because the weights are open, you can always customize to your particular use case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=306" target="_blank">00:05:06.040</a></span> | <span class="t">And tune -- and tune quality specifically for -- for what you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=311" target="_blank">00:05:11.740</a></span> | <span class="t">So open-source models are great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=313" target="_blank">00:05:13.140</a></span> | <span class="t">So what are the challenges?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=314" target="_blank">00:05:14.140</a></span> | <span class="t">The challenges really come from three areas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=317" target="_blank">00:05:17.000</a></span> | <span class="t">First, like, what we usually see when people try to use, you know, open model, something like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=321" target="_blank">00:05:21.420</a></span> | <span class="t">gem or whatever, or LAM might be, you run into complicated setup and maintenance, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=328" target="_blank">00:05:28.120</a></span> | <span class="t">You need to go and find GPUs somewhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=330" target="_blank">00:05:30.120</a></span> | <span class="t">You need to figure out which frameworks to run on those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=333" target="_blank">00:05:33.680</a></span> | <span class="t">You need to, like, download your models, maybe do some performance optimization tuning, and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=337" target="_blank">00:05:37.740</a></span> | <span class="t">kind of have to repeat this process end-to-end every time the model gets updated or new version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=342" target="_blank">00:05:42.420</a></span> | <span class="t">is released, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=344" target="_blank">00:05:44.860</a></span> | <span class="t">On optimization itself, there is -- especially for LLMs, but generally for GNI models, there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=349" target="_blank">00:05:49.940</a></span> | <span class="t">are many attributes and settings which are really dependent on your use case and requirements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=356" target="_blank">00:05:56.880</a></span> | <span class="t">Somebody needs low latency, somebody needs high throughput, prompts can be short, prompts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=360" target="_blank">00:06:00.680</a></span> | <span class="t">can be long, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=361" target="_blank">00:06:01.880</a></span> | <span class="t">And choosing the optimal settings across the stack is actually not trivial.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=366" target="_blank">00:06:06.300</a></span> | <span class="t">And as I show you later, in many cases, you can get multiple X improvements from doing -- from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=370" target="_blank">00:06:10.940</a></span> | <span class="t">doing this efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=372" target="_blank">00:06:12.880</a></span> | <span class="t">And finally, like, just getting the production ready is actually hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=376" target="_blank">00:06:16.420</a></span> | <span class="t">As you kind of go from experimentation to production, even just babysitting GPUs on public clouds is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=383" target="_blank">00:06:23.060</a></span> | <span class="t">not as easy because GPUs are finicky, not always reliable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=386" target="_blank">00:06:26.880</a></span> | <span class="t">But getting to enterprise scale requires, you know, all the scalability technology, telemetry,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=391" target="_blank">00:06:31.680</a></span> | <span class="t">observability, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=393" target="_blank">00:06:33.400</a></span> | <span class="t">So those are things which we focus on solving at Fireworks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=398" target="_blank">00:06:38.100</a></span> | <span class="t">So starting with efficiency, we built our own custom service stack, which we believe is one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=403" target="_blank">00:06:43.680</a></span> | <span class="t">of the fastest, if not the fastest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=406" target="_blank">00:06:46.400</a></span> | <span class="t">We did it -- did it from the ground up, meaning from writing our own, you know, CUDA kernels,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=411" target="_blank">00:06:51.440</a></span> | <span class="t">all the way to customizing how the stuff gets deployed and orchestrated on the service level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=417" target="_blank">00:06:57.520</a></span> | <span class="t">And that brings multiple optimizations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=419" target="_blank">00:06:59.400</a></span> | <span class="t">But most importantly, we really focus on customizing the service stack to your needs, which basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=424" target="_blank">00:07:04.780</a></span> | <span class="t">means for your custom workload and for your custom cost and latency requirements, we can tune</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=431" target="_blank">00:07:11.360</a></span> | <span class="t">it for those settings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=434" target="_blank">00:07:14.240</a></span> | <span class="t">What does it mean in practice?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=435" target="_blank">00:07:15.240</a></span> | <span class="t">And what does customization mean in practice?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=437" target="_blank">00:07:17.700</a></span> | <span class="t">For example, many use cases use reg and use very long prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=442" target="_blank">00:07:22.260</a></span> | <span class="t">So there are many settings you can tune actually on the runtime level at the deployment level to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=446" target="_blank">00:07:26.620</a></span> | <span class="t">optimize for long prompts, which often can be repeatable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=450" target="_blank">00:07:30.220</a></span> | <span class="t">So caching is useful or just tuning settings so the throughput is higher while maintaining latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=455" target="_blank">00:07:35.580</a></span> | <span class="t">So this is independently benchmarkable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=457" target="_blank">00:07:37.680</a></span> | <span class="t">If you go to, you know, artificial analysis and select long prompt, where Fireworks actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=462" target="_blank">00:07:42.280</a></span> | <span class="t">is the fastest, even faster than some of the other providers which are over there at ExpoBooth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=468" target="_blank">00:07:48.580</a></span> | <span class="t">And we don't only focus -- we don't only focus on LLM inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=471" target="_blank">00:07:51.800</a></span> | <span class="t">We focus on many modalities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=474" target="_blank">00:07:54.040</a></span> | <span class="t">As an example, for image generation, we are the fastest providers serving SDXL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=478" target="_blank">00:07:58.160</a></span> | <span class="t">We are also the only providers serving SD3 in stability's new model because their API actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=484" target="_blank">00:08:04.800</a></span> | <span class="t">routes to our servers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=488" target="_blank">00:08:08.460</a></span> | <span class="t">And finally, as I mentioned, like, LLMs, like, especially for LLMs, customization matters a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=494" target="_blank">00:08:14.660</a></span> | <span class="t">One requirement -- like, one paradigm of how to think about performance of LLMs often is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=498" target="_blank">00:08:18.400</a></span> | <span class="t">useful for use cases is to think about maxim -- like, minimizing cost under a particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=502" target="_blank">00:08:22.400</a></span> | <span class="t">latency constraint.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=503" target="_blank">00:08:23.400</a></span> | <span class="t">We often have customers come and say, like, hey, I need to, like -- I have this -- my interactive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=507" target="_blank">00:08:27.240</a></span> | <span class="t">implication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=508" target="_blank">00:08:28.240</a></span> | <span class="t">I need to generate that many tokens under two seconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=511" target="_blank">00:08:31.420</a></span> | <span class="t">And that's where -- that's really where, like, cross-stack optimizations shine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=516" target="_blank">00:08:36.000</a></span> | <span class="t">Whereby tune into particular, like, latency cutoff and change in many settings, you can deliver</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=522" target="_blank">00:08:42.100</a></span> | <span class="t">much higher support, multiple times higher support, which -- higher support basically means fewer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=526" target="_blank">00:08:46.380</a></span> | <span class="t">GPUs and lower cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=531" target="_blank">00:08:51.540</a></span> | <span class="t">In terms of -- in terms of model support, we support -- support best quality open source models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=536" target="_blank">00:08:56.600</a></span> | <span class="t">You know, we heard about Gemma now, obviously, LLMAS, some of the ASR and text-to-speech models pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=543" target="_blank">00:09:03.920</a></span> | <span class="t">much from many providers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=545" target="_blank">00:09:05.560</a></span> | <span class="t">We also work with model developers, for example, for example, in U.S. has also served on fireworks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=552" target="_blank">00:09:12.560</a></span> | <span class="t">launched -- launched last week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=555" target="_blank">00:09:15.940</a></span> | <span class="t">And as a kind of platform capabilities, as I mentioned, we have a lot of open source models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=561" target="_blank">00:09:21.360</a></span> | <span class="t">to get you started or customized ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=564" target="_blank">00:09:24.660</a></span> | <span class="t">We do some of the fine-tuning of those models in-house.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=567" target="_blank">00:09:27.740</a></span> | <span class="t">So I'm going to talk a little bit about function calling specialized models later on, or we do some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=572" target="_blank">00:09:32.840</a></span> | <span class="t">some of the vision language models using ourselves, which we release as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=578" target="_blank">00:09:38.480</a></span> | <span class="t">And of course, the key for open source -- open model development is we can tune for a particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=584" target="_blank">00:09:44.480</a></span> | <span class="t">use case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=585" target="_blank">00:09:45.480</a></span> | <span class="t">So we do provide a platform for fine-tuning, whether you are bringing your dataset collected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=590" target="_blank">00:09:50.480</a></span> | <span class="t">elsewhere or collecting it live with the feedback when served on our platform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=595" target="_blank">00:09:55.480</a></span> | <span class="t">Specifically on customization is like one interesting feature, which a lot of people starting to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=602" target="_blank">00:10:02.120</a></span> | <span class="t">experiment with models find interesting, is if you try to fine-tune and deploy the resulting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=607" target="_blank">00:10:07.000</a></span> | <span class="t">model, how to serve it efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=609" target="_blank">00:10:09.120</a></span> | <span class="t">It turns out if you do Plura fine-tuning, which a lot of folks do, you can do smart tricks and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=616" target="_blank">00:10:16.000</a></span> | <span class="t">deploy multiple Plura models on the same GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=618" target="_blank">00:10:18.120</a></span> | <span class="t">Actually, thousands of them, which means we can give you still serverless inference with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=623" target="_blank">00:10:23.760</a></span> | <span class="t">paying for token even if you have, like, thousands of model variants sitting and deployed there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=628" target="_blank">00:10:28.640</a></span> | <span class="t">without having to pay any fixed cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=633" target="_blank">00:10:33.400</a></span> | <span class="t">Of course, single model is all great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=635" target="_blank">00:10:35.760</a></span> | <span class="t">But what we see increasingly more and more in applications is model is not a product, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=642" target="_blank">00:10:42.400</a></span> | <span class="t">by itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=643" target="_blank">00:10:43.400</a></span> | <span class="t">You need a kind of bigger system in order to solve target application.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=647" target="_blank">00:10:47.720</a></span> | <span class="t">And the reason for that is because models by themselves tend to hallucinate, so you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=651" target="_blank">00:10:51.760</a></span> | <span class="t">some grounding, and that's where, like, access to external knowledge bases comes in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=658" target="_blank">00:10:58.280</a></span> | <span class="t">Also we don't have, you know, yet an industry magical multimodal AI across all the modalities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=664" target="_blank">00:11:04.040</a></span> | <span class="t">so often you have to kind of chain multiple types of models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=666" target="_blank">00:11:06.760</a></span> | <span class="t">And, of course, you have all these, like, external tools and external actions which kind of end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=671" target="_blank">00:11:11.820</a></span> | <span class="t">to end applications might want to do in agentic form.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=676" target="_blank">00:11:16.420</a></span> | <span class="t">So I think the term which I really like, which is, like, popularized by Databricks is, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=680" target="_blank">00:11:20.800</a></span> | <span class="t">compound AI system, but basically increasingly seeing the transition from just the model being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=685" target="_blank">00:11:25.860</a></span> | <span class="t">the product to kind of this combination of maybe, like, rag and function calling and external</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=689" target="_blank">00:11:29.940</a></span> | <span class="t">tools, et cetera, built together as the product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=692" target="_blank">00:11:32.980</a></span> | <span class="t">And that's pretty much direction which we kind of see this field moving along over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=699" target="_blank">00:11:39.700</a></span> | <span class="t">So what does it mean from kind of our perspective what we do in this case?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=705" target="_blank">00:11:45.160</a></span> | <span class="t">So we see kind of as a function calling, like, agent at the core of this emerging architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=711" target="_blank">00:11:51.700</a></span> | <span class="t">which might be connected to either domain-specialized models served on our platform directly or maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=718" target="_blank">00:11:58.920</a></span> | <span class="t">tuned for different needs and connected to external tools.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=721" target="_blank">00:12:01.640</a></span> | <span class="t">So maybe it's a content interpreter, or maybe it's, like, external APIs somewhere,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=725" target="_blank">00:12:05.640</a></span> | <span class="t">with really, like, this kind of central agentic view, kind of central model kind of coordinating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=732" target="_blank">00:12:12.920</a></span> | <span class="t">and trying to triage the user requirements, if it's, for example, a chatbot or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=737" target="_blank">00:12:17.640</a></span> | <span class="t">You've probably all heard about, like, function calling, you know, popularized by OpenAI initially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=744" target="_blank">00:12:24.360</a></span> | <span class="t">That's basically the same idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=746" target="_blank">00:12:26.360</a></span> | <span class="t">So, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=747" target="_blank">00:12:27.360</a></span> | <span class="t">The function calling is really, like, how to -- how to connect LLM to external tools and external</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=754" target="_blank">00:12:34.360</a></span> | <span class="t">elements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=755" target="_blank">00:12:35.360</a></span> | <span class="t">What does it mean in practice?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=756" target="_blank">00:12:36.360</a></span> | <span class="t">So we actually focus on fine-tuning models specifically for function calling, so we release</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=763" target="_blank">00:12:43.080</a></span> | <span class="t">a series of models like that, like, the latest one for function V2 was released two weeks ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=767" target="_blank">00:12:47.960</a></span> | <span class="t">And what you can do with that is -- if I manage to click -- if I manage to click on this button.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=778" target="_blank">00:12:58.520</a></span> | <span class="t">What it means is, like, you can build applications which kind of combine free-form general chat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=785" target="_blank">00:13:05.320</a></span> | <span class="t">capabilities with function calling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=788" target="_blank">00:13:08.200</a></span> | <span class="t">So in this case, this is -- you know, this fire function has some chat capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=793" target="_blank">00:13:13.240</a></span> | <span class="t">So you can see you can, like, ask it what -- what can you do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=795" target="_blank">00:13:15.720</a></span> | <span class="t">And it has, like, some self-reflection to tell you what it can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=798" target="_blank">00:13:18.600</a></span> | <span class="t">It's also connected in this demo app to a bunch of external tools.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=802" target="_blank">00:13:22.440</a></span> | <span class="t">So it can query, like, stock quotes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=806" target="_blank">00:13:26.120</a></span> | <span class="t">It can plot some charts, all those, like, external APIs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=809" target="_blank">00:13:29.000</a></span> | <span class="t">It can also generate images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=813" target="_blank">00:13:33.800</a></span> | <span class="t">But what it really needs to figure out is how to translate user query into -- do complex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=818" target="_blank">00:13:38.360</a></span> | <span class="t">reasoning, translate it into function calls.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=820" target="_blank">00:13:40.520</a></span> | <span class="t">So, for example, if we ask it to generate a bar chart with top three -- like, stocks of top</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=827" target="_blank">00:13:47.320</a></span> | <span class="t">cloud providers, like, the big three, it actually needs to do several steps, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=831" target="_blank">00:13:51.480</a></span> | <span class="t">It needs to understand that, like, top three cloud providers means, you know, AWS, GCP, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=837" target="_blank">00:13:57.240</a></span> | <span class="t">an Azure, right, and Azure is on the Microsoft.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=839" target="_blank">00:13:59.880</a></span> | <span class="t">It needs to then go do function calls querying their stock prices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=844" target="_blank">00:14:04.600</a></span> | <span class="t">And finally, it needs to combine those information and send it to chat plotting API,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=848" target="_blank">00:14:08.920</a></span> | <span class="t">which is what just happened in the background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=851" target="_blank">00:14:11.880</a></span> | <span class="t">Another important aspect which you have to do for, like, efficient kind of function calling chat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=858" target="_blank">00:14:18.200</a></span> | <span class="t">capabilities, you need to have contextual awareness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=860" target="_blank">00:14:20.440</a></span> | <span class="t">So if I ask it to add particular -- if I ask it to add Oracle to this graph, it needs to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=865" target="_blank">00:14:25.640</a></span> | <span class="t">what I'm referring to and, like, still keep the previous context and regenerate the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=869" target="_blank">00:14:29.880</a></span> | <span class="t">And finally, you know, if I switch to a different topic, it kind of needs to drop the previous context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=875" target="_blank">00:14:35.560</a></span> | <span class="t">and understand that, like, hey, this is less -- this historical context is less important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=879" target="_blank">00:14:39.960</a></span> | <span class="t">I'm going to start from scratch so there is no, like, oracle in that cat or whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=883" target="_blank">00:14:43.880</a></span> | <span class="t">So, you know, this particular demo is -- is actually open source.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=888" target="_blank">00:14:48.360</a></span> | <span class="t">You can, like, go to our GitHub and try it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=890" target="_blank">00:14:50.840</a></span> | <span class="t">It's built with Fire Function and built with, like, a few other -- a few other models, including, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=895" target="_blank">00:14:55.320</a></span> | <span class="t">SDXL, which are run on our platform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=897" target="_blank">00:14:57.240</a></span> | <span class="t">The model itself for function calling is actually open source.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=901" target="_blank">00:15:01.400</a></span> | <span class="t">It's on Hagen Face.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=902" target="_blank">00:15:02.760</a></span> | <span class="t">I mean, you can, of course, call it at Fireworks for optimal speeds, but you can also run it locally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=908" target="_blank">00:15:08.600</a></span> | <span class="t">if you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=909" target="_blank">00:15:09.560</a></span> | <span class="t">It uses a bunch of, you know, functionality on our platform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=913" target="_blank">00:15:13.000</a></span> | <span class="t">For example, like, structure generation is, like, with JSON model grammar mode, which I think was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=919" target="_blank">00:15:19.000</a></span> | <span class="t">similar to some of the previous talks from, like, Outline guys, which we were talking here yesterday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=924" target="_blank">00:15:24.040</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=925" target="_blank">00:15:25.480</a></span> | <span class="t">So, finally, try it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=926" target="_blank">00:15:26.680</a></span> | <span class="t">And generally, like, how to get started in Fireworks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=930" target="_blank">00:15:30.200</a></span> | <span class="t">So, if you head out to Fireworks, say, such models, you'll find a lot of open source,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=936" target="_blank">00:15:36.760</a></span> | <span class="t">open base models, which I mentioned about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=938" target="_blank">00:15:38.280</a></span> | <span class="t">They're available in the playground.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=939" target="_blank">00:15:39.880</a></span> | <span class="t">In terms of product offering, we have this kind of range which can take you from early prototyping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=945" target="_blank">00:15:45.800</a></span> | <span class="t">all the way to enterprise scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=947" target="_blank">00:15:47.400</a></span> | <span class="t">So, you can start with serverless inference, which is, you know, not different from, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=951" target="_blank">00:15:51.720</a></span> | <span class="t">getting to open API -- open AI playground or something where you pay per token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=955" target="_blank">00:15:55.560</a></span> | <span class="t">It's a cost and price.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=957" target="_blank">00:15:57.400</a></span> | <span class="t">You don't need to worry about, like, hardware settings or anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=960" target="_blank">00:16:00.440</a></span> | <span class="t">As I mentioned, you can still do fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=962" target="_blank">00:16:02.440</a></span> | <span class="t">So, you can -- you can do hosted fine-tuning on our platform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=964" target="_blank">00:16:04.920</a></span> | <span class="t">You can bring your own lower adapter and still serve it serverless.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=968" target="_blank">00:16:08.120</a></span> | <span class="t">As you kind of graduate, like, maybe, like, a startup and you graduate to a more production</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=972" target="_blank">00:16:12.680</a></span> | <span class="t">scale, you might want to go to on-demand where it's more, like, dedicated hardware with more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=977" target="_blank">00:16:17.560</a></span> | <span class="t">settings and modifications for your use case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=980" target="_blank">00:16:20.200</a></span> | <span class="t">You can bring your own custom model fine-tuned from scratch or do it on our platform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=984" target="_blank">00:16:24.920</a></span> | <span class="t">And finally, if you kind of -- if you scale up to a bigger volume and want to go to enterprise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=991" target="_blank">00:16:31.320</a></span> | <span class="t">level where it's kind of discounted long-term contracts, and we also will help you to kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=996" target="_blank">00:16:36.600</a></span> | <span class="t">of personalize hardware setup into some of those tuning for performance, which I -- which I talked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1001" target="_blank">00:16:41.800</a></span> | <span class="t">about earlier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1002" target="_blank">00:16:42.440</a></span> | <span class="t">And in terms of these cases, I mean, we're running production for many, many companies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1008" target="_blank">00:16:48.360</a></span> | <span class="t">ranging from small startups to big enterprises.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1010" target="_blank">00:16:50.760</a></span> | <span class="t">We're serving, like -- last time I checked, like, more than 150 billion tokens per day.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1014" target="_blank">00:16:54.680</a></span> | <span class="t">So, you know, companies like Quora built chatbots like Paul.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1018" target="_blank">00:16:58.760</a></span> | <span class="t">Source Draft and Courser, which I think Courser had a talk here yesterday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1023" target="_blank">00:17:03.560</a></span> | <span class="t">They used us for, like, some of the code assistant functionality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1026" target="_blank">00:17:06.040</a></span> | <span class="t">And their, like, latency is really important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1027" target="_blank">00:17:07.720</a></span> | <span class="t">As you can imagine, you know, folks, like, upstage and Liner are building, like, different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1033" target="_blank">00:17:13.000</a></span> | <span class="t">assistants and agents on top of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1035" target="_blank">00:17:15.400</a></span> | <span class="t">So, we are definitely production ready.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1038" target="_blank">00:17:18.040</a></span> | <span class="t">Go try it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1039" target="_blank">00:17:19.320</a></span> | <span class="t">Finally, we care a lot about developers, you guys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1044" target="_blank">00:17:24.040</a></span> | <span class="t">So, actually, this is external numbers from, like, last year, land chain, state of AI stuff,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1049" target="_blank">00:17:29.240</a></span> | <span class="t">where it turns out we are one of the -- like, after Hagen phase, the most popular platform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1054" target="_blank">00:17:34.280</a></span> | <span class="t">for where people pull models, which is great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1055" target="_blank">00:17:35.960</a></span> | <span class="t">It was very nice to hear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1058" target="_blank">00:17:38.600</a></span> | <span class="t">And, again, for getting started, just, you know, head out -- head out to our website.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1064" target="_blank">00:17:44.520</a></span> | <span class="t">You can go in the -- go play in the playground right away.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1067" target="_blank">00:17:47.880</a></span> | <span class="t">So, for example, you can run, you know, Lama or Gemma or whatever at the -- at the top speeds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1072" target="_blank">00:17:52.600</a></span> | <span class="t">And kind of go -- start building from there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1075" target="_blank">00:17:55.720</a></span> | <span class="t">I'm really excited to see what you can build with open models or fire function or some stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1080" target="_blank">00:18:00.280</a></span> | <span class="t">which you -- which you can fine tune on -- on your own.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1083" target="_blank">00:18:03.320</a></span> | <span class="t">And, yeah, last point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1085" target="_blank">00:18:05.080</a></span> | <span class="t">We are, as I mentioned, open API compatible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1087" target="_blank">00:18:07.080</a></span> | <span class="t">So you can still use, you know, your favorite tools, the same clients, or you can use frameworks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1093" target="_blank">00:18:13.880</a></span> | <span class="t">like land chain or Lama index or et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1096" target="_blank">00:18:16.600</a></span> | <span class="t">So, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1097" target="_blank">00:18:17.000</a></span> | <span class="t">Really excited to kind of -- to be here and tell a little bit about open source -- open source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1105" target="_blank">00:18:25.320</a></span> | <span class="t">models and how we have fireworks focusing on productionizing that and scaling it up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1110" target="_blank">00:18:30.760</a></span> | <span class="t">Go try it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1111" target="_blank">00:18:31.480</a></span> | <span class="t">And you can also find us at the booth at the Expo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1114" target="_blank">00:18:34.040</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=C1CXwRYbwuQ&t=1117" target="_blank">00:18:37.880</a></span> | <span class="t">Thank you.</span></div></div></body></html>