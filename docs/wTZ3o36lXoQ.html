<html><head><title>Stanford CS25: V1 I DeepMind's Perceiver and Perceiver IO: new data family architecture</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V1 I DeepMind's Perceiver and Perceiver IO: new data family architecture</h2><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ"><img src="https://i.ytimg.com/vi/wTZ3o36lXoQ/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=181">3:1</a> Improving Transformers<br><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=365">6:5</a> Why non-locality?<br><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=520">8:40</a> Scalability vs. generality?<br><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=642">10:42</a> Cross-attention: attention with linear scaling<br><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1230">20:30</a> In contrast: VII<br><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1583">26:23</a> ImageNet classification<br><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1841">30:41</a> Featurizing multimodality<br><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2883">48:3</a> What is optical flow?<br><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3076">51:16</a> Real-world qualitative results<br><br><div style="text-align: left;"><a href="./wTZ3o36lXoQ.html">Whisper Transcript</a> | <a href="./transcript_wTZ3o36lXoQ.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">So today, I'm going to be talking about some recent work that we've been doing at DeepMind,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=9" target="_blank">00:00:09.600</a></span> | <span class="t">developing this line of architectures that we're calling perceivers, and I'll be motivating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=14" target="_blank">00:00:14.880</a></span> | <span class="t">this in terms of a goal that we have, which is to develop general purpose architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=20" target="_blank">00:00:20.280</a></span> | <span class="t">And so just right off the bat, I want to motivate why we care about general purpose architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=25" target="_blank">00:00:25.680</a></span> | <span class="t">And so both of the reasons are fairly pragmatic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=30" target="_blank">00:00:30.200</a></span> | <span class="t">But basically, the idea is, if we're thinking about all of the data that we could possibly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=34" target="_blank">00:00:34.700</a></span> | <span class="t">imagine collecting in the world, a lot of it basically involves what we think of as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=39" target="_blank">00:00:39.800</a></span> | <span class="t">sort of traditional sense modalities, and these things range from touch and proprioception</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=45" target="_blank">00:00:45.160</a></span> | <span class="t">to echolocation to the kind of perception you need to ingest text, however you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=50" target="_blank">00:00:50.320</a></span> | <span class="t">to format that, to more exotic things like event-based cameras, whisker, touching with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=56" target="_blank">00:00:56.680</a></span> | <span class="t">whistler senses, things like smell and depth, and all the way up to the kinds of sense modalities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=62" target="_blank">00:01:02.680</a></span> | <span class="t">that we really think about when we're thinking about scientific perception.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=66" target="_blank">00:01:06.840</a></span> | <span class="t">And so basically, if we think about the full set of data and what it would take to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=71" target="_blank">00:01:11.560</a></span> | <span class="t">model each of these different modalities, it basically looks effectively intractable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=77" target="_blank">00:01:17.000</a></span> | <span class="t">to try to engineer inductive biases that will work for every single one of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=81" target="_blank">00:01:21.120</a></span> | <span class="t">So we don't want to engineer them one by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=84" target="_blank">00:01:24.320</a></span> | <span class="t">This is an approach that's worked, and in some ways, it's maybe a reasonable description</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=87" target="_blank">00:01:27.600</a></span> | <span class="t">of how we think about developing new architectures for different problems, but it's just not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=92" target="_blank">00:01:32.480</a></span> | <span class="t">going to scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=93" target="_blank">00:01:33.480</a></span> | <span class="t">We can't afford, as a community, to hand-design inductive biases that will work for each and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=98" target="_blank">00:01:38.440</a></span> | <span class="t">every one of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=99" target="_blank">00:01:39.440</a></span> | <span class="t">And so rather than doing that, we want to sort of build architectures that, at least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=102" target="_blank">00:01:42.440</a></span> | <span class="t">at first pass, will allow us to handle everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=106" target="_blank">00:01:46.080</a></span> | <span class="t">There's another practical argument for why we should work on general-purpose architectures,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=110" target="_blank">00:01:50.600</a></span> | <span class="t">and that's because it will allow us to build simpler, more unified systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=115" target="_blank">00:01:55.380</a></span> | <span class="t">So if you look at how, in particular, complex multimodal data streams are typically approached</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=121" target="_blank">00:02:01.920</a></span> | <span class="t">in the sensory, computer vision, or pattern recognition literatures, effectively, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=129" target="_blank">00:02:09.080</a></span> | <span class="t">typical way this is done is by using inductive biases that we know hold for the individual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=133" target="_blank">00:02:13.440</a></span> | <span class="t">modalities and then engineer ways of combining those different subsystems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=138" target="_blank">00:02:18.040</a></span> | <span class="t">So this can mean building specific heads, specific input modules for each of these things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=144" target="_blank">00:02:24.480</a></span> | <span class="t">and then trying out the various different ways of combining them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=147" target="_blank">00:02:27.520</a></span> | <span class="t">So this can work, but it gives us systems that, in principle, really will only work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=151" target="_blank">00:02:31.760</a></span> | <span class="t">on one or a small number of domains, and it gives us systems that are very hard to maintain,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=156" target="_blank">00:02:36.200</a></span> | <span class="t">tend to be fragile, tend to depend on specific processing assumptions about the input modalities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=161" target="_blank">00:02:41.680</a></span> | <span class="t">So rather than do that, we sort of want to move in the direction of having unified blackbox</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=166" target="_blank">00:02:46.680</a></span> | <span class="t">architectures that kind of just work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=169" target="_blank">00:02:49.120</a></span> | <span class="t">And the idea here is that if we can get to that point, we can abstract the architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=173" target="_blank">00:02:53.160</a></span> | <span class="t">construction process and really focus on other, more high-level problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=177" target="_blank">00:02:57.080</a></span> | <span class="t">So this is sort of the motivation for this line of work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=181" target="_blank">00:03:01.400</a></span> | <span class="t">And the way that we're going to be doing this is, of course, by working on the most general-purpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=186" target="_blank">00:03:06.800</a></span> | <span class="t">architecture that we have so far, which is basically a transformer, and you'll all be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=190" target="_blank">00:03:10.840</a></span> | <span class="t">very familiar with the basic building blocks of a transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=194" target="_blank">00:03:14.560</a></span> | <span class="t">But just at a very high level, we can think about what they do right, which is they use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=198" target="_blank">00:03:18.520</a></span> | <span class="t">a general-purpose inductive bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=201" target="_blank">00:03:21.160</a></span> | <span class="t">So they're non-local, which means they're not making domain-specific assumptions about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=204" target="_blank">00:03:24.680</a></span> | <span class="t">which points should be compared to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=206" target="_blank">00:03:26.680</a></span> | <span class="t">Rather, they tend to be global in terms of the attentional focus that they have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=211" target="_blank">00:03:31.960</a></span> | <span class="t">They use position as a feature rather than a hard constraint of the architecture, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=216" target="_blank">00:03:36.360</a></span> | <span class="t">this is in contrast to MLP-based architectures or ConvNets in the way that they typically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=222" target="_blank">00:03:42.840</a></span> | <span class="t">work, which use position as an architectural component to constrain how compute is happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=230" target="_blank">00:03:50.380</a></span> | <span class="t">And then, of course, finally, there's extensive weight sharing in the way that they're designed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=236" target="_blank">00:03:56.880</a></span> | <span class="t">And because they focus on matmuls, they tend to be TPU and GPU-friendly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=239" target="_blank">00:03:59.920</a></span> | <span class="t">So these are all very nice things about the way transformers work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=243" target="_blank">00:04:03.800</a></span> | <span class="t">Of course, on the other hand, they have very poor compute memory scaling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=247" target="_blank">00:04:07.520</a></span> | <span class="t">And there are two components to this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=249" target="_blank">00:04:09.200</a></span> | <span class="t">So attention itself scales quadratically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=251" target="_blank">00:04:11.520</a></span> | <span class="t">So there's this big O of M squared L complexity at the heart of transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=258" target="_blank">00:04:18.360</a></span> | <span class="t">And I like writing it this way because it really emphasizes that this is a property</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=262" target="_blank">00:04:22.240</a></span> | <span class="t">of-- that basically, as you make your models bigger, either at the input size or as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=266" target="_blank">00:04:26.840</a></span> | <span class="t">make them deeper, this problem is just going to get worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=271" target="_blank">00:04:31.080</a></span> | <span class="t">And because you have this scaling in depth as well, there's another practical thing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=275" target="_blank">00:04:35.920</a></span> | <span class="t">happens here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=277" target="_blank">00:04:37.160</a></span> | <span class="t">Because the amount of compute that we're doing is proportional to the input size, so there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=281" target="_blank">00:04:41.360</a></span> | <span class="t">no bottleneck in the way that standard transformers work, even the linear scaling becomes a problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=288" target="_blank">00:04:48.600</a></span> | <span class="t">And so in practice, for very, very large transformers, this can often be the bottleneck that really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=293" target="_blank">00:04:53.800</a></span> | <span class="t">matters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=294" target="_blank">00:04:54.800</a></span> | <span class="t">But they're both at play here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=295" target="_blank">00:04:55.800</a></span> | <span class="t">And so we really want to tamp down both of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=298" target="_blank">00:04:58.160</a></span> | <span class="t">And so the perspective here is that to have really general-purpose architectures, we can't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=303" target="_blank">00:05:03.080</a></span> | <span class="t">have ones that are just in principle general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=304" target="_blank">00:05:04.960</a></span> | <span class="t">We have to have ones that you can actually use on the scales and the kinds of data that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=309" target="_blank">00:05:09.640</a></span> | <span class="t">we care about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=313" target="_blank">00:05:13.440</a></span> | <span class="t">And so just to-- this will all be old hat for all of you, but just the way that standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=318" target="_blank">00:05:18.080</a></span> | <span class="t">QKV attention works is basically like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=321" target="_blank">00:05:21.360</a></span> | <span class="t">So it's all matrix multiplication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=323" target="_blank">00:05:23.320</a></span> | <span class="t">So we have some input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=324" target="_blank">00:05:24.520</a></span> | <span class="t">We compute the query keys and values by having a 1D convolution, a one-by-one convolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=329" target="_blank">00:05:29.240</a></span> | <span class="t">that we run over the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=331" target="_blank">00:05:31.280</a></span> | <span class="t">We then compute the attention scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=334" target="_blank">00:05:34.840</a></span> | <span class="t">This is a matrix multiply that has the following-- these sorts of shapes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=338" target="_blank">00:05:38.920</a></span> | <span class="t">We then use the output here to compute the weights, to compute the actual output of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=345" target="_blank">00:05:45.200</a></span> | <span class="t">attention module itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=346" target="_blank">00:05:46.880</a></span> | <span class="t">And then finally, we run this through an additional MLP, which is applied convolutionally, to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=351" target="_blank">00:05:51.360</a></span> | <span class="t">get the outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=353" target="_blank">00:05:53.160</a></span> | <span class="t">So this is the starting point of what we're working on here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=358" target="_blank">00:05:58.040</a></span> | <span class="t">And let me just briefly just reiterate why we would want the advantages that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=363" target="_blank">00:06:03.040</a></span> | <span class="t">with these standard transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=365" target="_blank">00:06:05.180</a></span> | <span class="t">So non-locality is one of the two inductive bias principles that we have here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=369" target="_blank">00:06:09.800</a></span> | <span class="t">It's useful, I think, to contrast this to the effective locality that you get in ConvNets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=374" target="_blank">00:06:14.400</a></span> | <span class="t">and what this actually means.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=376" target="_blank">00:06:16.240</a></span> | <span class="t">So if we look at, basically, as a function of depth, which inputs can see which other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=381" target="_blank">00:06:21.600</a></span> | <span class="t">functions, which means how easily it is to express a function of two input points, let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=386" target="_blank">00:06:26.760</a></span> | <span class="t">say we look at this yellow and purple point here at the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=389" target="_blank">00:06:29.800</a></span> | <span class="t">Now, I've set them as far apart as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=392" target="_blank">00:06:32.800</a></span> | <span class="t">But we might ask, how deep would the effective computation have to be before you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=398" target="_blank">00:06:38.000</a></span> | <span class="t">process these two?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=399" target="_blank">00:06:39.000</a></span> | <span class="t">And if you look at a three-by-three convolution, you're having to look, basically, until the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=405" target="_blank">00:06:45.200</a></span> | <span class="t">very end of the network, until you're processing these things together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=410" target="_blank">00:06:50.440</a></span> | <span class="t">And what this means is that the functions that you can express that actually look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=415" target="_blank">00:06:55.640</a></span> | <span class="t">both of these points end up being quite shallow, because they have to be built on top of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=419" target="_blank">00:06:59.400</a></span> | <span class="t">very, very deep stack that just gives you the locality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=422" target="_blank">00:07:02.900</a></span> | <span class="t">And so in point of fact, if you look at, for example, the way ResNets work, so you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=427" target="_blank">00:07:07.480</a></span> | <span class="t">an initial block, which has a seven-by-seven convolution, and then afterwards, it's three-by-three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=431" target="_blank">00:07:11.120</a></span> | <span class="t">cons all the way up, you need 28 three-by-three cons with that standard processing stack before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=436" target="_blank">00:07:16.700</a></span> | <span class="t">all of the 224 by 224 pixels in an image are looking at each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=441" target="_blank">00:07:21.260</a></span> | <span class="t">And what this means is that in a ResNet-50, the points on the very edge of the pixels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=445" target="_blank">00:07:25.840</a></span> | <span class="t">actually never see each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=447" target="_blank">00:07:27.560</a></span> | <span class="t">And I found this a little bit counterintuitive, but it suggests that we really are constraining</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=453" target="_blank">00:07:33.520</a></span> | <span class="t">quite a lot the functions that are easy to express with these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=456" target="_blank">00:07:36.640</a></span> | <span class="t">And so there are some functions of images you just can't capture with a ResNet-50.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=462" target="_blank">00:07:42.680</a></span> | <span class="t">On the other hand, if you look at an architecture that has global attention over the full input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=468" target="_blank">00:07:48.180</a></span> | <span class="t">so a transformer, if you could scale it that way, or a perceiver, as we're going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=472" target="_blank">00:07:52.080</a></span> | <span class="t">talking about, all of the pixels can interact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=475" target="_blank">00:07:55.480</a></span> | <span class="t">So the model can basically capture these things and express these functions much more easily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=479" target="_blank">00:07:59.800</a></span> | <span class="t">than can be expressed in things that put locality first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=485" target="_blank">00:08:05.520</a></span> | <span class="t">We also-- the other interesting property of these sorts of architectures is that position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=490" target="_blank">00:08:10.440</a></span> | <span class="t">is featurized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=492" target="_blank">00:08:12.120</a></span> | <span class="t">And this basically means that we're no longer sort of encoding the architectural location</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=496" target="_blank">00:08:16.680</a></span> | <span class="t">of something to figure out where it's located with respect to the other ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=501" target="_blank">00:08:21.760</a></span> | <span class="t">And this allows the network to basically use any positional information that it wants but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=507" target="_blank">00:08:27.260</a></span> | <span class="t">can also discard it as it prefers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=510" target="_blank">00:08:30.560</a></span> | <span class="t">And so this is the standard way it's done, of course, in the context of architectures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=514" target="_blank">00:08:34.280</a></span> | <span class="t">that use Fourier or sinusoidal-like features, but there's a lot of flexibility here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=519" target="_blank">00:08:39.400</a></span> | <span class="t">OK, so now just thinking in terms of how ConvNets relate to transformers, sort of at the opposite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=525" target="_blank">00:08:45.320</a></span> | <span class="t">end, it may look like that we have a sort of scalability versus generality trade-off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=532" target="_blank">00:08:52.000</a></span> | <span class="t">And so if we look at ConvNets, the way that they're applied-- so typically, we can think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=536" target="_blank">00:08:56.880</a></span> | <span class="t">about using them on grid-structured data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=538" target="_blank">00:08:58.920</a></span> | <span class="t">There are, of course, generalizations of convolutions that work on data sets with more interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=544" target="_blank">00:09:04.720</a></span> | <span class="t">topology, but typically, we can think of them as operating on grids in some sort of space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=550" target="_blank">00:09:10.920</a></span> | <span class="t">whereas transformers apply to generic sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=553" target="_blank">00:09:13.080</a></span> | <span class="t">So transformers are more general from this point of view.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=556" target="_blank">00:09:16.080</a></span> | <span class="t">On the other hand, they scale much, much worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=558" target="_blank">00:09:18.680</a></span> | <span class="t">So ConvNets are linear, both in the input points, the filter size, and the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=563" target="_blank">00:09:23.840</a></span> | <span class="t">layers of that architecture, whereas transformers have this quadratic scaling, and they're still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=568" target="_blank">00:09:28.940</a></span> | <span class="t">linear in the depth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=571" target="_blank">00:09:31.320</a></span> | <span class="t">So from this point of view, what we're interested in doing in the perceiver line of work was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=575" target="_blank">00:09:35.760</a></span> | <span class="t">to scale transformers, but to keep the generality property.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=579" target="_blank">00:09:39.040</a></span> | <span class="t">So we want something that lives in between these two extremes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=583" target="_blank">00:09:43.120</a></span> | <span class="t">And the way that we do this is by looking at self-attention and sort of modifying it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=588" target="_blank">00:09:48.480</a></span> | <span class="t">in a way that allows us to scale better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=591" target="_blank">00:09:51.880</a></span> | <span class="t">So to walk through what self-attention actually does in sort of standard transformers, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=596" target="_blank">00:09:56.760</a></span> | <span class="t">take our input array, which here is written as the indices, which is the number of tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=602" target="_blank">00:10:02.000</a></span> | <span class="t">or the number of pixels, basically the number of input units, depending on what you're looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=605" target="_blank">00:10:05.840</a></span> | <span class="t">at, and the channels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=608" target="_blank">00:10:08.320</a></span> | <span class="t">We have a 1D convolution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=609" target="_blank">00:10:09.680</a></span> | <span class="t">So this is big O of M with respect to the Q, K, and V. We then compute the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=614" target="_blank">00:10:14.880</a></span> | <span class="t">maps using the output of this operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=617" target="_blank">00:10:17.380</a></span> | <span class="t">This gives us a matrix multiply, which is the source of the quadratic scaling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=621" target="_blank">00:10:21.920</a></span> | <span class="t">And then finally, we compute output features with another matrix multiply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=627" target="_blank">00:10:27.320</a></span> | <span class="t">This is already-- we're already rate-limited here, because for even standard resolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=632" target="_blank">00:10:32.880</a></span> | <span class="t">images, M is quite large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=634" target="_blank">00:10:34.080</a></span> | <span class="t">So it's around 50,000 for standard ImageNet images, which, again, are very small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=638" target="_blank">00:10:38.120</a></span> | <span class="t">So this is something that just isn't going to work if we want deep architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=643" target="_blank">00:10:43.160</a></span> | <span class="t">So what we do is we replace-- at the input to the architecture, we replace the self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=647" target="_blank">00:10:47.920</a></span> | <span class="t">with a cross-attention layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=649" target="_blank">00:10:49.960</a></span> | <span class="t">And we do this using, basically, a learned query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=653" target="_blank">00:10:53.720</a></span> | <span class="t">And so we're replacing only the query from the input here with a learned component.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=659" target="_blank">00:10:59.200</a></span> | <span class="t">And so these indices and channels, you can just think of these as basically working like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=663" target="_blank">00:11:03.000</a></span> | <span class="t">a learned initial state for an RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=665" target="_blank">00:11:05.300</a></span> | <span class="t">There's a variety of names that this idea goes under in the literature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=669" target="_blank">00:11:09.480</a></span> | <span class="t">We refer to them as sort of as latents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=673" target="_blank">00:11:13.240</a></span> | <span class="t">But they're sometimes called inducing points or other things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=677" target="_blank">00:11:17.520</a></span> | <span class="t">So the basic idea is we're learning the input to the query and keeping the key value of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=680" target="_blank">00:11:20.800</a></span> | <span class="t">it the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=682" target="_blank">00:11:22.680</a></span> | <span class="t">The downside-- or the sort of upside of this is that when we compute the attention map</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=687" target="_blank">00:11:27.720</a></span> | <span class="t">after this, now we basically turn this from a square matrix to a rectangular matrix and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=695" target="_blank">00:11:35.160</a></span> | <span class="t">reduces the complexity of the matrix multiply to big O of Mn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=698" target="_blank">00:11:38.120</a></span> | <span class="t">So now it's linear in the input size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=700" target="_blank">00:11:40.740</a></span> | <span class="t">And the second matrix multiply has the exact same property.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=704" target="_blank">00:11:44.220</a></span> | <span class="t">So it becomes-- from quadratic, it becomes linear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=707" target="_blank">00:11:47.840</a></span> | <span class="t">And the quite cool thing about this is that, OK, so the cross-attention is linear in complexity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=712" target="_blank">00:11:52.980</a></span> | <span class="t">But the output is actually smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=714" target="_blank">00:11:54.720</a></span> | <span class="t">And so this, I think, is actually the more important point here is that this allows us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=718" target="_blank">00:11:58.160</a></span> | <span class="t">to map something which is quite large into something that has size that's independent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=722" target="_blank">00:12:02.140</a></span> | <span class="t">of the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=723" target="_blank">00:12:03.140</a></span> | <span class="t">So we have full control over this as a hyperparameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=726" target="_blank">00:12:06.000</a></span> | <span class="t">And this allows us to build deep networks on top of this latent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=730" target="_blank">00:12:10.320</a></span> | <span class="t">So because this is of a small size that we can control, we can afford to have quadratic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=733" target="_blank">00:12:13.980</a></span> | <span class="t">complexity on top of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=735" target="_blank">00:12:15.860</a></span> | <span class="t">And so we use this idea-- yep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=736" target="_blank">00:12:16.860</a></span> | <span class="t">Go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=737" target="_blank">00:12:17.860</a></span> | <span class="t">Oh, sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=738" target="_blank">00:12:18.860</a></span> | <span class="t">I'm still a little bit confused as to how you guys are able to turn this square into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=744" target="_blank">00:12:24.280</a></span> | <span class="t">a rectangle in the second step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=745" target="_blank">00:12:25.800</a></span> | <span class="t">Is it because you replaced the query with a learned something that is significantly smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=751" target="_blank">00:12:31.020</a></span> | <span class="t">compared to the input size in the first step?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=753" target="_blank">00:12:33.560</a></span> | <span class="t">Yeah, that's exactly right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=755" target="_blank">00:12:35.260</a></span> | <span class="t">So if you look at the-- so the underlying matrix multiply here, which is written as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=760" target="_blank">00:12:40.640</a></span> | <span class="t">the QK transpose, so this will basically-- so the outer dimension here has shape n, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=767" target="_blank">00:12:47.240</a></span> | <span class="t">is determined by the query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=768" target="_blank">00:12:48.860</a></span> | <span class="t">And so by shrinking that query, we're just changing the output of the matrix multiply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=772" target="_blank">00:12:52.440</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=773" target="_blank">00:12:53.440</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=774" target="_blank">00:12:54.440</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=775" target="_blank">00:12:55.440</a></span> | <span class="t">So I guess--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=776" target="_blank">00:12:56.440</a></span> | <span class="t">Sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=777" target="_blank">00:12:57.440</a></span> | <span class="t">Go ahead, please.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=778" target="_blank">00:12:58.440</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=779" target="_blank">00:12:59.440</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=780" target="_blank">00:13:00.440</a></span> | <span class="t">So basically, you only do that for the query, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=783" target="_blank">00:13:03.280</a></span> | <span class="t">So key and value remain like the original size matrices, correct?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=788" target="_blank">00:13:08.740</a></span> | <span class="t">That's right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=789" target="_blank">00:13:09.740</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=790" target="_blank">00:13:10.740</a></span> | <span class="t">But so basically-- so I don't know what I'm not understanding, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=797" target="_blank">00:13:17.200</a></span> | <span class="t">So the problem for me is that for a query, now in my head, I'm looking for-- let's say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=803" target="_blank">00:13:23.520</a></span> | <span class="t">I have the if token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=805" target="_blank">00:13:25.380</a></span> | <span class="t">Now there is no if query anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=809" target="_blank">00:13:29.000</a></span> | <span class="t">Doesn't that cause a problem when I'm trying to use it and to compute scores?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=815" target="_blank">00:13:35.520</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=816" target="_blank">00:13:36.820</a></span> | <span class="t">So what's happening here is you'll have a smaller subset of queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=820" target="_blank">00:13:40.660</a></span> | <span class="t">So if you think about this not in terms of the matrix multiplies, but in terms of comparing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=824" target="_blank">00:13:44.260</a></span> | <span class="t">each query to each key.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=826" target="_blank">00:13:46.540</a></span> | <span class="t">So in normal self-attention, we have one query for each key, so every point compares to every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=831" target="_blank">00:13:51.240</a></span> | <span class="t">other point, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=832" target="_blank">00:13:52.240</a></span> | <span class="t">So here, what we've done is instead of comparing every point to every other point, we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=836" target="_blank">00:13:56.000</a></span> | <span class="t">a set of sort of cluster centers you might be able to think about them as.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=839" target="_blank">00:13:59.620</a></span> | <span class="t">So it's a smaller number, and we compare each of those to each of the input points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=843" target="_blank">00:14:03.940</a></span> | <span class="t">But we don't know which tokens technically belong to which clusters, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=851" target="_blank">00:14:11.140</a></span> | <span class="t">That's right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=852" target="_blank">00:14:12.140</a></span> | <span class="t">So it has to be learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=853" target="_blank">00:14:13.140</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=854" target="_blank">00:14:14.140</a></span> | <span class="t">Yeah, exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=855" target="_blank">00:14:15.660</a></span> | <span class="t">So one way to think about this, about what's happening here, is that instead of-- so in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=862" target="_blank">00:14:22.300</a></span> | <span class="t">a normal self-attention transformer, by comparing all to all, we're sort of saying, OK, I know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=868" target="_blank">00:14:28.260</a></span> | <span class="t">what the feature is at this point, and I want it to attend to similar features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=872" target="_blank">00:14:32.940</a></span> | <span class="t">Here what we're saying is we're learning a bunch of supplementary points that should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=878" target="_blank">00:14:38.380</a></span> | <span class="t">be sort of maximally similar to some subset of the inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=881" target="_blank">00:14:41.540</a></span> | <span class="t">So correct me if I'm wrong, but this is essentially doing some sort of hard attention, where you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=886" target="_blank">00:14:46.780</a></span> | <span class="t">saying instead of querying over all the points, let's select some points which we think are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=892" target="_blank">00:14:52.060</a></span> | <span class="t">very similar, and only put self-attention over this hard point, like these points you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=897" target="_blank">00:14:57.420</a></span> | <span class="t">have selected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=898" target="_blank">00:14:58.420</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=899" target="_blank">00:14:59.420</a></span> | <span class="t">Yeah, so they're related.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=901" target="_blank">00:15:01.660</a></span> | <span class="t">That would be one way to think about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=903" target="_blank">00:15:03.980</a></span> | <span class="t">The slight modifier to that idea, though, is that they basically live in an abstract</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=908" target="_blank">00:15:08.420</a></span> | <span class="t">space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=909" target="_blank">00:15:09.420</a></span> | <span class="t">So they're not assigned sort of one-to-one to one of the input queries, or to one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=913" target="_blank">00:15:13.780</a></span> | <span class="t">the input points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=914" target="_blank">00:15:14.780</a></span> | <span class="t">They're sort of learned, so they can be somewhere in the middle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=917" target="_blank">00:15:17.860</a></span> | <span class="t">But I think that's a good way to think about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=919" target="_blank">00:15:19.340</a></span> | <span class="t">That's a good intuition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=921" target="_blank">00:15:21.500</a></span> | <span class="t">But I guess one of the places where I'm a little confused here is you have here indices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=926" target="_blank">00:15:26.900</a></span> | <span class="t">and indices for the two, like the purple and green matrices in the far left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=931" target="_blank">00:15:31.440</a></span> | <span class="t">But those indices are not necessarily corresponding to inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=934" target="_blank">00:15:34.460</a></span> | <span class="t">Like in the NLP space, those would not necessarily be tokens, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=937" target="_blank">00:15:37.380</a></span> | <span class="t">These are just sort of--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=938" target="_blank">00:15:38.580</a></span> | <span class="t">Exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=939" target="_blank">00:15:39.580</a></span> | <span class="t">--indices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=940" target="_blank">00:15:40.580</a></span> | <span class="t">But the--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=941" target="_blank">00:15:41.580</a></span> | <span class="t">That's right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=942" target="_blank">00:15:42.580</a></span> | <span class="t">--index in this case is the result of some kind of mapping from the input tokens to an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=946" target="_blank">00:15:46.180</a></span> | <span class="t">n-by-d matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=947" target="_blank">00:15:47.180</a></span> | <span class="t">Is that right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=948" target="_blank">00:15:48.180</a></span> | <span class="t">No, it's actually-- so it basically acts like-- it's a learned set of weights, is one way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=952" target="_blank">00:15:52.420</a></span> | <span class="t">to think about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=953" target="_blank">00:15:53.520</a></span> | <span class="t">So they function exactly the same way that learned position encodings do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=956" target="_blank">00:15:56.620</a></span> | <span class="t">So it's basically just a-- it's a learned embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=959" target="_blank">00:15:59.660</a></span> | <span class="t">But it's not conditioned on anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=961" target="_blank">00:16:01.620</a></span> | <span class="t">It's just sort of-- it just is-- it's just a set of weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=966" target="_blank">00:16:06.780</a></span> | <span class="t">Oh, OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=967" target="_blank">00:16:07.780</a></span> | <span class="t">That makes more sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=968" target="_blank">00:16:08.780</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=969" target="_blank">00:16:09.780</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=970" target="_blank">00:16:10.780</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=971" target="_blank">00:16:11.780</a></span> | <span class="t">So if there are no more questions, I'm going to keep going.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=976" target="_blank">00:16:16.980</a></span> | <span class="t">But of course, feel free to interrupt me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=980" target="_blank">00:16:20.220</a></span> | <span class="t">So the way that-- given this idea-- so we have this learned latent array, which, again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=986" target="_blank">00:16:26.220</a></span> | <span class="t">it functions sort of like an RNN initial state, or it's a set of weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=990" target="_blank">00:16:30.260</a></span> | <span class="t">We basically randomly initialize that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=992" target="_blank">00:16:32.820</a></span> | <span class="t">And then we use this to attend onto the input byte array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=996" target="_blank">00:16:36.700</a></span> | <span class="t">And so the byte array here is the flattened set of pixels, for example, for ImageNet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1001" target="_blank">00:16:41.340</a></span> | <span class="t">And the output of this is going to live in the same space as-- so the same index space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1005" target="_blank">00:16:45.980</a></span> | <span class="t">as the latent array does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1007" target="_blank">00:16:47.980</a></span> | <span class="t">And there's residual connections in the way that you would normally do in an attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1013" target="_blank">00:16:53.140</a></span> | <span class="t">layer as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1016" target="_blank">00:16:56.040</a></span> | <span class="t">So once we're in the space, we can then build an architecture by taking-- by using a standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1021" target="_blank">00:17:01.500</a></span> | <span class="t">transformer but phrased in the latent space rather than in the input space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1026" target="_blank">00:17:06.300</a></span> | <span class="t">And this is going to allow us to basically end up-- because we've sort of distilled the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1030" target="_blank">00:17:10.520</a></span> | <span class="t">input down to the smaller space, we can still flexibly allow all of these points to interact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1035" target="_blank">00:17:15.300</a></span> | <span class="t">So this should be still as nearly as expressive as the transformer-- as a normal transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1040" target="_blank">00:17:20.220</a></span> | <span class="t">is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1041" target="_blank">00:17:21.220</a></span> | <span class="t">And then each of the modules here now is quadratic in the latent size rather than the input size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1045" target="_blank">00:17:25.420</a></span> | <span class="t">So this is something that we can control quite a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1049" target="_blank">00:17:29.580</a></span> | <span class="t">So in the original version of the perceiver, we found it was very helpful to have additional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1056" target="_blank">00:17:36.020</a></span> | <span class="t">cross-attends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1057" target="_blank">00:17:37.020</a></span> | <span class="t">So this is certainly something that you can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1059" target="_blank">00:17:39.740</a></span> | <span class="t">And the reason-- the intuition behind this is that if this bottleneck is quite severe,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1066" target="_blank">00:17:46.360</a></span> | <span class="t">we can't maintain all of the information from the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1068" target="_blank">00:17:48.860</a></span> | <span class="t">And so we want these queries, which are now sort of conditioned on the past, to be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1072" target="_blank">00:17:52.740</a></span> | <span class="t">to look back at the input point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1075" target="_blank">00:17:55.000</a></span> | <span class="t">And so this is something that we found to be quite helpful when tuning for the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1079" target="_blank">00:17:59.980</a></span> | <span class="t">paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1080" target="_blank">00:18:00.980</a></span> | <span class="t">But the caveat, I will say, is that we're no longer recommending this as best practice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1084" target="_blank">00:18:04.620</a></span> | <span class="t">because these cross-attentions end up being quite heavy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1087" target="_blank">00:18:07.420</a></span> | <span class="t">But this is something that you can explore, certainly, if you want sort of more conditional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1090" target="_blank">00:18:10.500</a></span> | <span class="t">queries or if you want to be able to cross-attend to new inputs that are coming in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1096" target="_blank">00:18:16.020</a></span> | <span class="t">The other thing that we found quite helpful in the context of data sets that have a limited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1100" target="_blank">00:18:20.900</a></span> | <span class="t">amount of data, which for these architectures includes ImageNet, is to allow weight sharing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1106" target="_blank">00:18:26.020</a></span> | <span class="t">in depth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1107" target="_blank">00:18:27.220</a></span> | <span class="t">And so this basically just amounts to tying the weights for the different cross-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1111" target="_blank">00:18:31.160</a></span> | <span class="t">and different self-attention layers as they're repeated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1113" target="_blank">00:18:33.940</a></span> | <span class="t">So this ends up looking like an RNN that's unrolled in depth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1118" target="_blank">00:18:38.860</a></span> | <span class="t">So this is just at a high level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1122" target="_blank">00:18:42.140</a></span> | <span class="t">This gives us an architecture that we can apply to images but doesn't make any assumptions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1125" target="_blank">00:18:45.580</a></span> | <span class="t">about image structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1126" target="_blank">00:18:46.820</a></span> | <span class="t">So it's one that you can use elsewhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1129" target="_blank">00:18:49.300</a></span> | <span class="t">And we give information about the input spatial structure by having positional encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1138" target="_blank">00:18:58.180</a></span> | <span class="t">And here we use a 2D Fourier feature position encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1141" target="_blank">00:19:01.300</a></span> | <span class="t">And just to show you what that looks like here, to give you a sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1144" target="_blank">00:19:04.660</a></span> | <span class="t">So each of the input points is assigned basically-- so you'll be in some position here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1151" target="_blank">00:19:11.340</a></span> | <span class="t">And we have sinusoidal and cosinusoidal features in 2D.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1154" target="_blank">00:19:14.560</a></span> | <span class="t">So this is basically a Fourier decomposition of the position of the 2D input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1159" target="_blank">00:19:19.880</a></span> | <span class="t">And a couple of things that we found were that if we sampled the frequency, that's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1164" target="_blank">00:19:24.540</a></span> | <span class="t">maximum frequency that's used, up to the Nyquist frequency of the signal, we end up doing better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1170" target="_blank">00:19:30.640</a></span> | <span class="t">than if you use a lower version of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1172" target="_blank">00:19:32.320</a></span> | <span class="t">And this basically is because this will allow every other point to be aware of every distinct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1177" target="_blank">00:19:37.960</a></span> | <span class="t">point in the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1178" target="_blank">00:19:38.960</a></span> | <span class="t">Whereas if you sample at a lower frequency, you're going to end up with aliasing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1181" target="_blank">00:19:41.920</a></span> | <span class="t">And so not all points will be legible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1185" target="_blank">00:19:45.480</a></span> | <span class="t">We also found that sampling the spectrum relatively densely tends to help.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1189" target="_blank">00:19:49.400</a></span> | <span class="t">And the contrast here, at the time we were developing, was with respect to NERF.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1194" target="_blank">00:19:54.160</a></span> | <span class="t">So NERF, at least in earlier implementations, used quite a small number of frequency bands.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1199" target="_blank">00:19:59.520</a></span> | <span class="t">We found that the more we added, the better we did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1201" target="_blank">00:20:01.840</a></span> | <span class="t">So in general, this is something to be attentive to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1206" target="_blank">00:20:06.000</a></span> | <span class="t">And then finally, as opposed to language, where you typically have addition of whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1210" target="_blank">00:20:10.560</a></span> | <span class="t">your embedding is with the sinusoidal or position encoding that you use, here we found that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1216" target="_blank">00:20:16.960</a></span> | <span class="t">concatenating them performed consistently better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1220" target="_blank">00:20:20.320</a></span> | <span class="t">And so this may be because the content embedding is not as sparse as it is in language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1225" target="_blank">00:20:25.600</a></span> | <span class="t">We're not totally sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1226" target="_blank">00:20:26.600</a></span> | <span class="t">But this is something that I observed consistently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1230" target="_blank">00:20:30.400</a></span> | <span class="t">And before I move on to results, I just want to contrast this to some other approaches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1234" target="_blank">00:20:34.400</a></span> | <span class="t">for using transformers in the image context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1238" target="_blank">00:20:38.720</a></span> | <span class="t">So the obvious precedent here is visual transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1242" target="_blank">00:20:42.920</a></span> | <span class="t">And I think this is a very-- this line of work is great, especially in the image context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1248" target="_blank">00:20:48.400</a></span> | <span class="t">But there are some caveats about it that make it less suitable for sort of more general</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1253" target="_blank">00:20:53.000</a></span> | <span class="t">purpose use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1254" target="_blank">00:20:54.840</a></span> | <span class="t">So one is that-- so vision transformers do use an input 2D convolution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1259" target="_blank">00:20:59.060</a></span> | <span class="t">So this is often phrased in terms of patches, input patches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1263" target="_blank">00:21:03.040</a></span> | <span class="t">It's a special case of a 2D transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1265" target="_blank">00:21:05.680</a></span> | <span class="t">So it does restrict the class of inputs you can use it for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1270" target="_blank">00:21:10.520</a></span> | <span class="t">And because we're basically building this patching or convolution into it, this means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1276" target="_blank">00:21:16.140</a></span> | <span class="t">that this as an approach really isn't sufficient to get it to work on non-grid data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1280" target="_blank">00:21:20.560</a></span> | <span class="t">There are other ways you could adapt it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1281" target="_blank">00:21:21.720</a></span> | <span class="t">But this is something that you will have to special case for every domain you're looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1284" target="_blank">00:21:24.400</a></span> | <span class="t">at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1285" target="_blank">00:21:25.400</a></span> | <span class="t">And then finally, because we have this sort of input where we're telling the architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1290" target="_blank">00:21:30.640</a></span> | <span class="t">what it should look at first in the initial grouping, this does amount to getting rid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1294" target="_blank">00:21:34.400</a></span> | <span class="t">of the non-locality assumption.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1296" target="_blank">00:21:36.560</a></span> | <span class="t">It's not super clear how much doing this just once will make a difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1300" target="_blank">00:21:40.600</a></span> | <span class="t">But this is something to be aware of when you're thinking about this architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1305" target="_blank">00:21:45.000</a></span> | <span class="t">And then finally, cross-attention itself is used quite broadly in the vision literature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1310" target="_blank">00:21:50.100</a></span> | <span class="t">So just to highlight a couple of examples, Detter, which is an object detection method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1316" target="_blank">00:21:56.520</a></span> | <span class="t">from Facebook, basically has a convolutional backbone that's then used to give an output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1321" target="_blank">00:22:01.960</a></span> | <span class="t">feature map.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1323" target="_blank">00:22:03.120</a></span> | <span class="t">This is then passed into a transformer encoder decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1326" target="_blank">00:22:06.000</a></span> | <span class="t">And of course, whenever you hear encoder decoder, you think cross-attention, because from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1329" target="_blank">00:22:09.520</a></span> | <span class="t">encoder to the decoder, there's a cross-attention step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1332" target="_blank">00:22:12.560</a></span> | <span class="t">And so they're using basically the cross-attention to go from some feature map representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1336" target="_blank">00:22:16.480</a></span> | <span class="t">to something that looks more like the object bounding boxes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1340" target="_blank">00:22:20.880</a></span> | <span class="t">There's also quite nice work on learning self-supervised or unsupervised object segmentation models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1350" target="_blank">00:22:30.280</a></span> | <span class="t">And in this work, they're doing something very similar where they have a convolutional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1353" target="_blank">00:22:33.320</a></span> | <span class="t">backbone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1354" target="_blank">00:22:34.320</a></span> | <span class="t">They then use something like the latents that we introduce here to do-- they call them slots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1362" target="_blank">00:22:42.040</a></span> | <span class="t">here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1363" target="_blank">00:22:43.040</a></span> | <span class="t">But basically, to assign some of the output pixels to different slots so that they sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1368" target="_blank">00:22:48.040</a></span> | <span class="t">of have independent complementary decoding of the slots in the segmentation model here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1373" target="_blank">00:22:53.480</a></span> | <span class="t">And there's a lot of other things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1377" target="_blank">00:22:57.640</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1378" target="_blank">00:22:58.640</a></span> | <span class="t">So first, I just-- so now I'm going to sort of walk you through the results of this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1381" target="_blank">00:23:01.960</a></span> | <span class="t">Hi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1382" target="_blank">00:23:02.960</a></span> | <span class="t">Can I-- oh, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1383" target="_blank">00:23:03.960</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1384" target="_blank">00:23:04.960</a></span> | <span class="t">I'll go after you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1385" target="_blank">00:23:05.960</a></span> | <span class="t">Go for it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1386" target="_blank">00:23:06.960</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1387" target="_blank">00:23:07.960</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1388" target="_blank">00:23:08.960</a></span> | <span class="t">Sorry for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1389" target="_blank">00:23:09.960</a></span> | <span class="t">Can you go back a couple of slides where you had the-- how the inputs flow into, I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1399" target="_blank">00:23:19.520</a></span> | <span class="t">think, one of-- yeah, that one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1402" target="_blank">00:23:22.040</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1403" target="_blank">00:23:23.040</a></span> | <span class="t">So I have two questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1404" target="_blank">00:23:24.040</a></span> | <span class="t">So the latent transformer is basically like a self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1407" target="_blank">00:23:27.080</a></span> | <span class="t">Is that correct?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1408" target="_blank">00:23:28.840</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1409" target="_blank">00:23:29.840</a></span> | <span class="t">So the latent transformer is a fully self-attentional transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1413" target="_blank">00:23:33.400</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1415" target="_blank">00:23:35.200</a></span> | <span class="t">And why-- see, for the key and value, they flow directly into the cross-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1421" target="_blank">00:23:41.520</a></span> | <span class="t">And there is the query also flowing into it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1424" target="_blank">00:23:44.440</a></span> | <span class="t">But the latent array is flowing into the cross-attention in parallel to the query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1429" target="_blank">00:23:49.920</a></span> | <span class="t">Can you explain that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1430" target="_blank">00:23:50.920</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1431" target="_blank">00:23:51.920</a></span> | <span class="t">So this is just--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1432" target="_blank">00:23:52.920</a></span> | <span class="t">[INTERPOSING VOICES]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1433" target="_blank">00:23:53.920</a></span> | <span class="t">--here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1434" target="_blank">00:23:54.920</a></span> | <span class="t">I'm just trying to understand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1435" target="_blank">00:23:55.920</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1436" target="_blank">00:23:56.920</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1437" target="_blank">00:23:57.920</a></span> | <span class="t">So how do you pick the residual connection?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1438" target="_blank">00:23:58.920</a></span> | <span class="t">So the cross-attention-- this is a cross-attention depicted as a cross-attention module.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1444" target="_blank">00:24:04.200</a></span> | <span class="t">And so the cross-attention itself has the attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1446" target="_blank">00:24:06.640</a></span> | <span class="t">It has a residual connection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1447" target="_blank">00:24:07.840</a></span> | <span class="t">And then there's an MLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1449" target="_blank">00:24:09.000</a></span> | <span class="t">So that's what that's meant to indicate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1450" target="_blank">00:24:10.640</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1451" target="_blank">00:24:11.640</a></span> | <span class="t">But it's basically-- the QKV is standard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1454" target="_blank">00:24:14.800</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1456" target="_blank">00:24:16.400</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1457" target="_blank">00:24:17.400</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1458" target="_blank">00:24:18.400</a></span> | <span class="t">Hi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1459" target="_blank">00:24:19.400</a></span> | <span class="t">I had a question that is slightly related to this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1462" target="_blank">00:24:22.320</a></span> | <span class="t">We can just stay off the slide, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1464" target="_blank">00:24:24.160</a></span> | <span class="t">So I think one thing that's interesting about this argument--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1466" target="_blank">00:24:26.400</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1467" target="_blank">00:24:27.400</a></span> | <span class="t">I'm sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1468" target="_blank">00:24:28.400</a></span> | <span class="t">I lost you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1469" target="_blank">00:24:29.400</a></span> | <span class="t">You're cutting off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1470" target="_blank">00:24:30.400</a></span> | <span class="t">It's mostly consisting of attention layers, whether it's self-attention or in image transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1480" target="_blank">00:24:40.080</a></span> | <span class="t">Can you hear me?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1481" target="_blank">00:24:41.080</a></span> | <span class="t">Is that coming through?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1482" target="_blank">00:24:42.080</a></span> | <span class="t">No, it's cutting off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1483" target="_blank">00:24:43.080</a></span> | <span class="t">I think--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1484" target="_blank">00:24:44.080</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1485" target="_blank">00:24:45.080</a></span> | <span class="t">But I think some recent--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1486" target="_blank">00:24:46.080</a></span> | <span class="t">Oh, OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1487" target="_blank">00:24:47.080</a></span> | <span class="t">Should I type it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1488" target="_blank">00:24:48.080</a></span> | <span class="t">Is that-- should I type?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1489" target="_blank">00:24:49.080</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1490" target="_blank">00:24:50.080</a></span> | <span class="t">I think that's a good idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1491" target="_blank">00:24:51.080</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1492" target="_blank">00:24:52.080</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1493" target="_blank">00:24:53.080</a></span> | <span class="t">I'll type it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1494" target="_blank">00:24:54.080</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1495" target="_blank">00:24:55.080</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1496" target="_blank">00:24:56.080</a></span> | <span class="t">It's kind of [INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1498" target="_blank">00:24:58.600</a></span> | <span class="t">Feel free to go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1499" target="_blank">00:24:59.600</a></span> | <span class="t">I'll type it slowly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1500" target="_blank">00:25:00.960</a></span> | <span class="t">And--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1501" target="_blank">00:25:01.960</a></span> | <span class="t">Sounds good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1502" target="_blank">00:25:02.960</a></span> | <span class="t">Sounds good to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1503" target="_blank">00:25:03.960</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1504" target="_blank">00:25:04.960</a></span> | <span class="t">Actually, can I chime in?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1505" target="_blank">00:25:05.960</a></span> | <span class="t">Drew, while you're on that previous slide--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1507" target="_blank">00:25:07.220</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1508" target="_blank">00:25:08.220</a></span> | <span class="t">--on the flow, so these residual connections, I actually didn't know the cross-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1512" target="_blank">00:25:12.400</a></span> | <span class="t">used them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1513" target="_blank">00:25:13.400</a></span> | <span class="t">How reliant are these sequential cross-attention layers on the residual connections?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1518" target="_blank">00:25:18.560</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1520" target="_blank">00:25:20.040</a></span> | <span class="t">So here, in the initial-- so two things I will say is that in the initial cross-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1530" target="_blank">00:25:30.680</a></span> | <span class="t">it doesn't really make a difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1532" target="_blank">00:25:32.080</a></span> | <span class="t">So this is something we've ablated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1534" target="_blank">00:25:34.560</a></span> | <span class="t">When we get to the Perceiver I/O version of this, we also did the same thing in the decoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1538" target="_blank">00:25:38.400</a></span> | <span class="t">cross-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1539" target="_blank">00:25:39.400</a></span> | <span class="t">And it can make some of a difference-- it can make a difference there, depending on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1541" target="_blank">00:25:41.720</a></span> | <span class="t">what you're doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1542" target="_blank">00:25:42.720</a></span> | <span class="t">I think it's actually essential when you're using repeated cross-attention of this way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1547" target="_blank">00:25:47.680</a></span> | <span class="t">so when you have this sort of iterative structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1549" target="_blank">00:25:49.720</a></span> | <span class="t">And the reason for this is that the thing that's actually used to condition the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1554" target="_blank">00:25:54.580</a></span> | <span class="t">is basically all the-- that's your full representation of the state of the architecture so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1560" target="_blank">00:26:00.000</a></span> | <span class="t">And so the skip connection is from the-- it's in, basically, the query channel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1564" target="_blank">00:26:04.600</a></span> | <span class="t">It's in the latent space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1566" target="_blank">00:26:06.660</a></span> | <span class="t">And so this is basically what allows you to end up with this sort of dense and stable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1570" target="_blank">00:26:10.240</a></span> | <span class="t">architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1571" target="_blank">00:26:11.240</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1572" target="_blank">00:26:12.240</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1573" target="_blank">00:26:13.240</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1574" target="_blank">00:26:14.240</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1575" target="_blank">00:26:15.240</a></span> | <span class="t">So to ImageNet-- OK, so in standard ImageNet processing, basically, we compare against</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1588" target="_blank">00:26:28.600</a></span> | <span class="t">a few-- so this is a little bit out of date at this point-- but against a few just sanity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1594" target="_blank">00:26:34.280</a></span> | <span class="t">check baselines here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1596" target="_blank">00:26:36.200</a></span> | <span class="t">So comparing against ResNet-50 and then, at the time, the best vision transformer model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1600" target="_blank">00:26:40.280</a></span> | <span class="t">that was purely on ImageNet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1602" target="_blank">00:26:42.300</a></span> | <span class="t">And we're definitely in the ballpark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1604" target="_blank">00:26:44.100</a></span> | <span class="t">This isn't-- these aren't anywhere near state-of-the-art results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1607" target="_blank">00:26:47.620</a></span> | <span class="t">But this is an architecture that, again, is not using any 2D convolutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1611" target="_blank">00:26:51.580</a></span> | <span class="t">And so the fact that it was able to do this well was, we found, very, very surprising</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1614" target="_blank">00:26:54.760</a></span> | <span class="t">at the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1615" target="_blank">00:26:55.760</a></span> | <span class="t">One of the quite cool things about this is that, because this architecture is not making</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1620" target="_blank">00:27:00.400</a></span> | <span class="t">any assumptions-- the architecture itself isn't making any assumptions about the spatial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1625" target="_blank">00:27:05.760</a></span> | <span class="t">structure of the input images-- we can look at permuted ImageNet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1630" target="_blank">00:27:10.720</a></span> | <span class="t">And in the first version of this, what we do is, basically, we compute the features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1634" target="_blank">00:27:14.960</a></span> | <span class="t">using the 2D position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1636" target="_blank">00:27:16.360</a></span> | <span class="t">So the 2D position is fixed to a position-- to the pixel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1639" target="_blank">00:27:19.580</a></span> | <span class="t">And then we just shuffle them all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1641" target="_blank">00:27:21.040</a></span> | <span class="t">And so this is-- basically, we'll give you a sense of how dependent the baselines are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1645" target="_blank">00:27:25.520</a></span> | <span class="t">on the input image structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1648" target="_blank">00:27:28.360</a></span> | <span class="t">And so if we look at the transformer and perceiver, by construction, they don't change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1653" target="_blank">00:27:33.080</a></span> | <span class="t">So this is not an empirical finding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1654" target="_blank">00:27:34.720</a></span> | <span class="t">This is a property of the models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1656" target="_blank">00:27:36.920</a></span> | <span class="t">But we find that ResNet-50 falls by about-- the performance falls by about half.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1661" target="_blank">00:27:41.920</a></span> | <span class="t">And VIT, which, again, only has one layer where it's relying on the spatial structure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1665" target="_blank">00:27:45.880</a></span> | <span class="t">also has about a 15-point drop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1668" target="_blank">00:27:48.200</a></span> | <span class="t">And so this suggests that it's relying quite a lot on that very first one to give it some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1673" target="_blank">00:27:53.000</a></span> | <span class="t">information about the structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1675" target="_blank">00:27:55.560</a></span> | <span class="t">We can push this a little bit by, instead of relying on 2D Fourier features, learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1680" target="_blank">00:28:00.880</a></span> | <span class="t">completely learned positional encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1683" target="_blank">00:28:03.600</a></span> | <span class="t">And this basically-- this is an architecture now-- this is a model that has absolutely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1687" target="_blank">00:28:07.240</a></span> | <span class="t">no information about the input structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1690" target="_blank">00:28:10.120</a></span> | <span class="t">And so shuffling them and learning them again is absolutely equivalent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1693" target="_blank">00:28:13.620</a></span> | <span class="t">And we find that this architecture also can be pushed above 70%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1696" target="_blank">00:28:16.920</a></span> | <span class="t">And we've gotten slightly better numbers here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1699" target="_blank">00:28:19.020</a></span> | <span class="t">In general, this seems to work worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1701" target="_blank">00:28:21.860</a></span> | <span class="t">So the 2D information is useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1704" target="_blank">00:28:24.300</a></span> | <span class="t">But it's quite cool that you can get what would have been numbers comparable to state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1709" target="_blank">00:28:29.120</a></span> | <span class="t">of the art about five or six years ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1711" target="_blank">00:28:31.080</a></span> | <span class="t">So this is quite cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1712" target="_blank">00:28:32.400</a></span> | <span class="t">Sorry, I'm a little thick here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1715" target="_blank">00:28:35.920</a></span> | <span class="t">You're saying the difference between the last two rows is that the second-to-last row has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1722" target="_blank">00:28:42.360</a></span> | <span class="t">a two-dimensional position embedding, and the last one has a one-dimensional position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1725" target="_blank">00:28:45.680</a></span> | <span class="t">embedding, essentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1726" target="_blank">00:28:46.680</a></span> | <span class="t">Is that right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1727" target="_blank">00:28:47.680</a></span> | <span class="t">So it's learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1728" target="_blank">00:28:48.900</a></span> | <span class="t">So it's basically-- it'll be-- it's, I believe, a 256-dimensional vector that's learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1736" target="_blank">00:28:56.560</a></span> | <span class="t">But it doesn't-- it basically-- it means that the model itself has no information about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1741" target="_blank">00:29:01.360</a></span> | <span class="t">the input spatial structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1744" target="_blank">00:29:04.600</a></span> | <span class="t">So the 2D positional encodings that we're using end up having about 200-- it's 200-some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1749" target="_blank">00:29:09.880</a></span> | <span class="t">features, depending on what you're looking at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1751" target="_blank">00:29:11.960</a></span> | <span class="t">But they're always-- they give you very detailed information about the 2D structure of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1755" target="_blank">00:29:15.440</a></span> | <span class="t">input, because they're based on a Fourier decomposition of the input space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1758" target="_blank">00:29:18.360</a></span> | <span class="t">I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1759" target="_blank">00:29:19.360</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1760" target="_blank">00:29:20.360</a></span> | <span class="t">That makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1761" target="_blank">00:29:21.360</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1762" target="_blank">00:29:22.360</a></span> | <span class="t">Hi, Drew.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1763" target="_blank">00:29:23.360</a></span> | <span class="t">Can I ask a question about frequency you use to generate those sensorial waves?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1770" target="_blank">00:29:30.440</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1771" target="_blank">00:29:31.440</a></span> | <span class="t">So like a couple of slides before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1776" target="_blank">00:29:36.200</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1777" target="_blank">00:29:37.200</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1778" target="_blank">00:29:38.200</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1779" target="_blank">00:29:39.200</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1780" target="_blank">00:29:40.200</a></span> | <span class="t">This slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1781" target="_blank">00:29:41.200</a></span> | <span class="t">So basically, I do have taken some lectures in signal processing, and I know if I want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1788" target="_blank">00:29:48.900</a></span> | <span class="t">to avoid aliasing, I need to sample with at least Nyquist frequency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1794" target="_blank">00:29:54.360</a></span> | <span class="t">So I'm curious to know why do you use frequency starting from 1 to the Nyquist frequency,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1801" target="_blank">00:30:01.000</a></span> | <span class="t">instead of starting from Nyquist frequency to some very high frequency?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1805" target="_blank">00:30:05.760</a></span> | <span class="t">Oh, I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1807" target="_blank">00:30:07.260</a></span> | <span class="t">So basically-- so the maximum frequency that's used is always Nyquist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1813" target="_blank">00:30:13.920</a></span> | <span class="t">So anything about Nyquist is going to be aliased, so you're not actually going to be able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1816" target="_blank">00:30:16.960</a></span> | <span class="t">resolve it, because it's in pixel space, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1820" target="_blank">00:30:20.780</a></span> | <span class="t">So we sample-- one is basically just giving you an oscillation that covers the entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1825" target="_blank">00:30:25.320</a></span> | <span class="t">image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1826" target="_blank">00:30:26.320</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1827" target="_blank">00:30:27.320</a></span> | <span class="t">And so this is basically just a sample of the full range of non-aliased frequencies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1831" target="_blank">00:30:31.320</a></span> | <span class="t">Oh, OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1832" target="_blank">00:30:32.720</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1833" target="_blank">00:30:33.720</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1834" target="_blank">00:30:34.720</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1835" target="_blank">00:30:35.720</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1836" target="_blank">00:30:36.720</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1837" target="_blank">00:30:37.720</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1838" target="_blank">00:30:38.720</a></span> | <span class="t">So after the image results, we wanted to try it on other domains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1848" target="_blank">00:30:48.680</a></span> | <span class="t">And in particular, we were interested in how this could be used to work on sort of multimodal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1852" target="_blank">00:30:52.480</a></span> | <span class="t">domains, so ones combining various different types of input features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1857" target="_blank">00:30:57.480</a></span> | <span class="t">And one challenge or one sort of problem that you encounter in these sorts of spaces is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1862" target="_blank">00:31:02.440</a></span> | <span class="t">that the data from different modalities end up having different features, and they always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1867" target="_blank">00:31:07.100</a></span> | <span class="t">have different semantics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1868" target="_blank">00:31:08.560</a></span> | <span class="t">So if you take the positional encoding plus the RGB for video, you end up with some number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1873" target="_blank">00:31:13.640</a></span> | <span class="t">of channels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1874" target="_blank">00:31:14.640</a></span> | <span class="t">And then if you have audio, that corresponds-- the data may be paired, but it tends to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1878" target="_blank">00:31:18.300</a></span> | <span class="t">fewer features, and it only has a 1D positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1881" target="_blank">00:31:21.940</a></span> | <span class="t">So the way that we handle this is basically by learning modality-specific position encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1887" target="_blank">00:31:27.860</a></span> | <span class="t">And so these are basically embeddings that are special and learned for each of the modalities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1893" target="_blank">00:31:33.100</a></span> | <span class="t">And what this does is basically tags-- ends up tagging the features that come from audio</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1897" target="_blank">00:31:37.300</a></span> | <span class="t">or video with some information that the network can learn that allows it to distinguish which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1901" target="_blank">00:31:41.660</a></span> | <span class="t">one's which.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1902" target="_blank">00:31:42.920</a></span> | <span class="t">But given these padded-- these sort of learned padded feature vectors, we then concatenate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1908" target="_blank">00:31:48.500</a></span> | <span class="t">them all, and that's how we process multimodal data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1911" target="_blank">00:31:51.640</a></span> | <span class="t">So basically, the input to the architecture still looks like just one big array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1915" target="_blank">00:31:55.120</a></span> | <span class="t">It's just that when constructing this, we know that some of those features, some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1918" target="_blank">00:31:58.060</a></span> | <span class="t">the rows in that array, come from video and some come from audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1921" target="_blank">00:32:01.220</a></span> | <span class="t">But the model itself isn't given information about that other than what it learns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1924" target="_blank">00:32:04.460</a></span> | <span class="t">Oh, we also have some questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1928" target="_blank">00:32:08.100</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1929" target="_blank">00:32:09.100</a></span> | <span class="t">You can go first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1930" target="_blank">00:32:10.100</a></span> | <span class="t">Can we turn?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1931" target="_blank">00:32:11.100</a></span> | <span class="t">Oh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1932" target="_blank">00:32:12.100</a></span> | <span class="t">Yeah, sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1933" target="_blank">00:32:13.100</a></span> | <span class="t">I thought it was [INAUDIBLE] but yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1934" target="_blank">00:32:14.100</a></span> | <span class="t">Yeah, sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1935" target="_blank">00:32:15.100</a></span> | <span class="t">If you can hear me, this is just a simple reason.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1936" target="_blank">00:32:16.100</a></span> | <span class="t">I haven't studied a lot of transformer stuff formally, so I just didn't know what a positional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1937" target="_blank">00:32:17.100</a></span> | <span class="t">embedding was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1938" target="_blank">00:32:18.100</a></span> | <span class="t">Oh, so what a positional embedding?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1939" target="_blank">00:32:19.100</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1940" target="_blank">00:32:20.100</a></span> | <span class="t">So basically, a positional embedding is-- it's a feature that says this-- so the simplest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1961" target="_blank">00:32:41.320</a></span> | <span class="t">way to think about it is in text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1963" target="_blank">00:32:43.000</a></span> | <span class="t">So text, the input is 1D, so things live in some 1D sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1966" target="_blank">00:32:46.680</a></span> | <span class="t">And for each point there, you featurize where it's located in that sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1970" target="_blank">00:32:50.540</a></span> | <span class="t">So the simplest thing to do would be if you have negative 1 to 1 is the full range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1975" target="_blank">00:32:55.000</a></span> | <span class="t">It just denotes actually where it's located in that sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1978" target="_blank">00:32:58.440</a></span> | <span class="t">But we typically will add-- we'll want to featurize this to have more dimensions than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1983" target="_blank">00:33:03.980</a></span> | <span class="t">just a single one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1985" target="_blank">00:33:05.240</a></span> | <span class="t">And so the Fourier decomposition is one way to do this to give it privileged information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1990" target="_blank">00:33:10.560</a></span> | <span class="t">about the high frequency structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1992" target="_blank">00:33:12.960</a></span> | <span class="t">But we can also just use the position to index onto some embedding array, which is how we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=1998" target="_blank">00:33:18.960</a></span> | <span class="t">do it when we're learning things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2000" target="_blank">00:33:20.440</a></span> | <span class="t">So basically, it's just a set of weights that are added to the feature for that point that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2004" target="_blank">00:33:24.160</a></span> | <span class="t">give the network information about where it's located in the Groucher sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2012" target="_blank">00:33:32.280</a></span> | <span class="t">You want to go next?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2013" target="_blank">00:33:33.280</a></span> | <span class="t">Sorry, I had to find a mute button-- unmute button.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2021" target="_blank">00:33:41.160</a></span> | <span class="t">OK, so I actually have two questions regarding the Fourier features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2028" target="_blank">00:33:48.640</a></span> | <span class="t">I think-- do you guys sample them uniformly, or are they-- do you learn these?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2038" target="_blank">00:33:58.720</a></span> | <span class="t">Yeah, so basically, we sample them linearly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2043" target="_blank">00:34:03.080</a></span> | <span class="t">So basically, we take the full space, and we sample them linearly with whatever the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2045" target="_blank">00:34:05.800</a></span> | <span class="t">budget is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2048" target="_blank">00:34:08.040</a></span> | <span class="t">There are-- so in various settings, we have actually tried learning these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2052" target="_blank">00:34:12.160</a></span> | <span class="t">So you could actually initialize an array with them and then learn them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2055" target="_blank">00:34:15.560</a></span> | <span class="t">And that does help sometimes, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2058" target="_blank">00:34:18.440</a></span> | <span class="t">And you could potentially learn-- you could try a more sophisticated strategy on this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2062" target="_blank">00:34:22.040</a></span> | <span class="t">too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2063" target="_blank">00:34:23.040</a></span> | <span class="t">OK, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2064" target="_blank">00:34:24.040</a></span> | <span class="t">My follow-up question is that basically, I feel like the selling point of your research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2069" target="_blank">00:34:29.680</a></span> | <span class="t">is that you don't make any structural assumptions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2073" target="_blank">00:34:33.040</a></span> | <span class="t">You can take any type of format.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2075" target="_blank">00:34:35.200</a></span> | <span class="t">However, for the encoding, wouldn't the dimensionality-- so for example, if it's text, it's 1D, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2083" target="_blank">00:34:43.640</a></span> | <span class="t">If it's an image, it will be 2D.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2086" target="_blank">00:34:46.960</a></span> | <span class="t">And if it's a video, it will be 3D.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2089" target="_blank">00:34:49.960</a></span> | <span class="t">You have more-- the positional encoding will have more points, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2096" target="_blank">00:34:56.480</a></span> | <span class="t">Wouldn't that inherently give away the nature of the input?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2103" target="_blank">00:35:03.360</a></span> | <span class="t">Yeah, so it does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2104" target="_blank">00:35:04.960</a></span> | <span class="t">So I completely agree with this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2106" target="_blank">00:35:06.480</a></span> | <span class="t">You're totally right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2108" target="_blank">00:35:08.600</a></span> | <span class="t">The version of this where we have learned position encodings is the most pure from that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2112" target="_blank">00:35:12.000</a></span> | <span class="t">point of view.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2113" target="_blank">00:35:13.000</a></span> | <span class="t">So it's one that gives it basically no information about the ground truth spatial structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2118" target="_blank">00:35:18.260</a></span> | <span class="t">What it does give the model-- so when you do the learned position encoding, it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2122" target="_blank">00:35:22.360</a></span> | <span class="t">say that, for example, there is a correspondence between point k on image 1 and point k on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2128" target="_blank">00:35:28.800</a></span> | <span class="t">image 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2130" target="_blank">00:35:30.060</a></span> | <span class="t">So that's basically the least amount of information you can give it while still allowing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2134" target="_blank">00:35:34.920</a></span> | <span class="t">it to figure out what the structural relationship between the input points is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2138" target="_blank">00:35:38.920</a></span> | <span class="t">So this is the direction that we've been trying to push in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2142" target="_blank">00:35:42.000</a></span> | <span class="t">In general, giving the architecture access to ground truth structural information, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2147" target="_blank">00:35:47.320</a></span> | <span class="t">this lives on this point in 2D, is helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2150" target="_blank">00:35:50.480</a></span> | <span class="t">So there's a couple things here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2153" target="_blank">00:35:53.400</a></span> | <span class="t">There's from a practical point of view, if you want good results, you need to exploit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2157" target="_blank">00:35:57.680</a></span> | <span class="t">these things, or it's helpful to exploit these things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2160" target="_blank">00:36:00.480</a></span> | <span class="t">But we do want to move in the direction where we're relying on these things less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2164" target="_blank">00:36:04.400</a></span> | <span class="t">And so this is basically something we're actively looking into.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2167" target="_blank">00:36:07.440</a></span> | <span class="t">OK, makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2169" target="_blank">00:36:09.440</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2170" target="_blank">00:36:10.440</a></span> | <span class="t">So I think has posted her question on the chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2175" target="_blank">00:36:15.920</a></span> | <span class="t">I also see you have your hand raised.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2178" target="_blank">00:36:18.280</a></span> | <span class="t">So if you want, you can give it a try.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2181" target="_blank">00:36:21.560</a></span> | <span class="t">If not, I'll read out the question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2183" target="_blank">00:36:23.360</a></span> | <span class="t">OK, I'll try.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2184" target="_blank">00:36:24.760</a></span> | <span class="t">Just let me know if it's choppy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2186" target="_blank">00:36:26.560</a></span> | <span class="t">Yeah, so is it good right now?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2189" target="_blank">00:36:29.280</a></span> | <span class="t">So far, so good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2190" target="_blank">00:36:30.280</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2191" target="_blank">00:36:31.280</a></span> | <span class="t">Oh, good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2192" target="_blank">00:36:32.280</a></span> | <span class="t">OK, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2193" target="_blank">00:36:33.280</a></span> | <span class="t">So if you look at the perceiver diagram you had, it's a bunch of attention layers, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2196" target="_blank">00:36:36.320</a></span> | <span class="t">Like cross-attention and self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2198" target="_blank">00:36:38.360</a></span> | <span class="t">And I think there's been this small trend in recent work in vision transformers to try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2204" target="_blank">00:36:44.440</a></span> | <span class="t">to sort of replace the last few layers instead of having attention, like make them be convolutions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2209" target="_blank">00:36:49.200</a></span> | <span class="t">to address this attention scaling problem, right, in a different manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2213" target="_blank">00:36:53.600</a></span> | <span class="t">And so here, the perceiver architecture is trying to make self-attention less expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2218" target="_blank">00:36:58.600</a></span> | <span class="t">And there, they're just trying to replace it, and they kind of just avoid the problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2223" target="_blank">00:37:03.640</a></span> | <span class="t">And so I'm curious.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2224" target="_blank">00:37:04.640</a></span> | <span class="t">And so I've seen papers both ways, like some that try to do things like the ones you cited,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2228" target="_blank">00:37:08.760</a></span> | <span class="t">and then some that are trying to do this as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2231" target="_blank">00:37:11.120</a></span> | <span class="t">And in my mind, everyone always has the good results and stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2234" target="_blank">00:37:14.360</a></span> | <span class="t">So I'm curious if you think there's a reason to do one or the other, or if you think this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2238" target="_blank">00:37:18.560</a></span> | <span class="t">alternative approach is also promising, or is there a reason research should go in one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2243" target="_blank">00:37:23.840</a></span> | <span class="t">direction or the other?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2246" target="_blank">00:37:26.440</a></span> | <span class="t">Yeah, so to my mind, the big trade-off is one between...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2249" target="_blank">00:37:29.040</a></span> | <span class="t">So the vision literature, I think, has just exploded in terms of these sort of hybrids</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2254" target="_blank">00:37:34.040</a></span> | <span class="t">and people trying to find the exact right place on the Pareto curve for the trade-off</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2258" target="_blank">00:37:38.240</a></span> | <span class="t">of speed and performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2261" target="_blank">00:37:41.680</a></span> | <span class="t">But they're basically looking primarily on vision-specific problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2264" target="_blank">00:37:44.900</a></span> | <span class="t">So something that the computer vision community itself typically doesn't regularize itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2269" target="_blank">00:37:49.800</a></span> | <span class="t">away from things that don't work on things that aren't vision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2274" target="_blank">00:37:54.080</a></span> | <span class="t">So you end up with things that are very, very efficient and very performant on vision problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2279" target="_blank">00:37:59.040</a></span> | <span class="t">So I think from that point of view, it's an incredibly important line of work, and that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2283" target="_blank">00:38:03.040</a></span> | <span class="t">probably the right way of doing things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2286" target="_blank">00:38:06.780</a></span> | <span class="t">What we're aiming for is the things that are as general as possible while still being performant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2294" target="_blank">00:38:14.080</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2295" target="_blank">00:38:15.080</a></span> | <span class="t">So this kind of thing is critical...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2296" target="_blank">00:38:16.640</a></span> | <span class="t">Oh, sorry to cut you off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2297" target="_blank">00:38:17.640</a></span> | <span class="t">Go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2298" target="_blank">00:38:18.640</a></span> | <span class="t">No, no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2299" target="_blank">00:38:19.640</a></span> | <span class="t">Please, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2300" target="_blank">00:38:20.640</a></span> | <span class="t">I was going to say, so this kind of thing is important...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2303" target="_blank">00:38:23.380</a></span> | <span class="t">Just to summarize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2304" target="_blank">00:38:24.380</a></span> | <span class="t">So you feel like it's important to focus on attention because that's kind of critical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2306" target="_blank">00:38:26.800</a></span> | <span class="t">for NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2307" target="_blank">00:38:27.800</a></span> | <span class="t">Like you can't just sort of put in a convolution at the end and sort of fix the problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2311" target="_blank">00:38:31.180</a></span> | <span class="t">But in vision, maybe you can and it's fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2313" target="_blank">00:38:33.160</a></span> | <span class="t">Is that a right way of understanding it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2316" target="_blank">00:38:36.280</a></span> | <span class="t">That's part of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2317" target="_blank">00:38:37.280</a></span> | <span class="t">Vision and NLP aren't the only two domains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2319" target="_blank">00:38:39.280</a></span> | <span class="t">And so the thing that we're looking for are really basically...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2322" target="_blank">00:38:42.600</a></span> | <span class="t">So the kinds of problems that we're interested in doing with this include things like event-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2328" target="_blank">00:38:48.000</a></span> | <span class="t">cameras, cell biology, sort of proteins, all of these sorts of things where we may or may</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2334" target="_blank">00:38:54.680</a></span> | <span class="t">not have the right convolutional inductive biases to even know how to build those sorts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2338" target="_blank">00:38:58.700</a></span> | <span class="t">of things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2339" target="_blank">00:38:59.700</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2340" target="_blank">00:39:00.700</a></span> | <span class="t">They end up being whole research programs, like the mesh-based convolution work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2344" target="_blank">00:39:04.260</a></span> | <span class="t">Oh, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2345" target="_blank">00:39:05.260</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2346" target="_blank">00:39:06.260</a></span> | <span class="t">I also had one more question about the architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2349" target="_blank">00:39:09.260</a></span> | <span class="t">So I saw that...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2350" target="_blank">00:39:10.260</a></span> | <span class="t">I'm sorry if you said this and I just missed it, but you had cross-attention and then like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2355" target="_blank">00:39:15.180</a></span> | <span class="t">that tritium transformer and then cross-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2358" target="_blank">00:39:18.020</a></span> | <span class="t">I'm curious what happens if you replace the self-attention in those layers with cross-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2362" target="_blank">00:39:22.500</a></span> | <span class="t">Does it affect your accuracy?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2364" target="_blank">00:39:24.180</a></span> | <span class="t">Is that even feasible?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2365" target="_blank">00:39:25.180</a></span> | <span class="t">Is that a valid question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2368" target="_blank">00:39:28.300</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2369" target="_blank">00:39:29.300</a></span> | <span class="t">So the sort of thing that you could do is you could modify this to make it sort of hierarchical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2372" target="_blank">00:39:32.620</a></span> | <span class="t">so that there are multiple stages of cross-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2375" target="_blank">00:39:35.100</a></span> | <span class="t">We haven't gotten this working yet, but it doesn't mean it's not a good idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2380" target="_blank">00:39:40.820</a></span> | <span class="t">So there might be a right way to do this that we haven't figured out right, but it's something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2384" target="_blank">00:39:44.380</a></span> | <span class="t">we have tried a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2385" target="_blank">00:39:45.380</a></span> | <span class="t">Oh, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2386" target="_blank">00:39:46.380</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2387" target="_blank">00:39:47.380</a></span> | <span class="t">Thank you so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2388" target="_blank">00:39:48.380</a></span> | <span class="t">I appreciate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2389" target="_blank">00:39:49.380</a></span> | <span class="t">Yeah, no problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2390" target="_blank">00:39:50.380</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2391" target="_blank">00:39:51.380</a></span> | <span class="t">Let me...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2392" target="_blank">00:39:52.380</a></span> | <span class="t">We're running short on time, so maybe I'll skip ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2397" target="_blank">00:39:57.380</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2398" target="_blank">00:39:58.380</a></span> | <span class="t">So before we run out of too much time, I want to at least talk about the sort of the modifications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2407" target="_blank">00:40:07.700</a></span> | <span class="t">to this architecture that we've made to make it work sort of even more generally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2412" target="_blank">00:40:12.020</a></span> | <span class="t">So one of the problems of the sort of the first architecture that we looked at here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2416" target="_blank">00:40:16.580</a></span> | <span class="t">the basic perceiver, is that it works basically for arbitrary inputs, but it's designed to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2424" target="_blank">00:40:24.460</a></span> | <span class="t">work only on classification or regression tasks as an output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2428" target="_blank">00:40:28.400</a></span> | <span class="t">And so basically we wanted to see if we could use the same cross-attention strategy for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2432" target="_blank">00:40:32.540</a></span> | <span class="t">decoding and it turns out you can.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2434" target="_blank">00:40:34.420</a></span> | <span class="t">It's something that works pretty well, just kind of out of the box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2437" target="_blank">00:40:37.820</a></span> | <span class="t">So the idea is that we have, if we have our cross-attention input and self-attention sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2443" target="_blank">00:40:43.060</a></span> | <span class="t">of to do the processing, we can introduce a set of additional queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2448" target="_blank">00:40:48.140</a></span> | <span class="t">And these are basically queries that give the semantics of each of the points that you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2451" target="_blank">00:40:51.780</a></span> | <span class="t">trying to decode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2453" target="_blank">00:40:53.860</a></span> | <span class="t">And we pass these as input to another cross-attention layer, which is configured in basically the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2458" target="_blank">00:40:58.860</a></span> | <span class="t">opposite way that the encoder cross-attention is configured.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2462" target="_blank">00:41:02.200</a></span> | <span class="t">So now the queries are going to be something that's potentially large and the keys and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2465" target="_blank">00:41:05.660</a></span> | <span class="t">values are coming from this latent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2467" target="_blank">00:41:07.660</a></span> | <span class="t">And so what this allows us to do basically is to keep all of the nice advantages of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2471" target="_blank">00:41:11.740</a></span> | <span class="t">original perceiver.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2473" target="_blank">00:41:13.020</a></span> | <span class="t">So we have an encoder that scales linearly, we have a processor stage, this sort of latent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2476" target="_blank">00:41:16.980</a></span> | <span class="t">self-attention that scales independently of the input size, and we now have a decoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2481" target="_blank">00:41:21.940</a></span> | <span class="t">that keeps the decoupling, but gives us linear scaling with respect to output size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2487" target="_blank">00:41:27.020</a></span> | <span class="t">And so by doing this, we can now basically apply the same approach to basically dense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2493" target="_blank">00:41:33.820</a></span> | <span class="t">output tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2495" target="_blank">00:41:35.420</a></span> | <span class="t">And so to give you a sense of how this works, just sort of intuitively, if we're doing auto</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2499" target="_blank">00:41:39.940</a></span> | <span class="t">encoding on this image of puppies, basically what we do is we encode process, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2505" target="_blank">00:41:45.940</a></span> | <span class="t">to decode, we take a query that corresponds to each of the points, and then we pass it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2511" target="_blank">00:41:51.100</a></span> | <span class="t">into this decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2512" target="_blank">00:41:52.340</a></span> | <span class="t">So we can query one of the points, we get one pixel, query another one, we get another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2516" target="_blank">00:41:56.380</a></span> | <span class="t">one, and all the way up till we get all 10,000 points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2520" target="_blank">00:42:00.340</a></span> | <span class="t">And that's how we can do reconstruction with this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2522" target="_blank">00:42:02.980</a></span> | <span class="t">And the cool thing about this is that it opens up a bunch of new applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2528" target="_blank">00:42:08.940</a></span> | <span class="t">And we can get different kinds of outputs just by changing how the queries work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2532" target="_blank">00:42:12.460</a></span> | <span class="t">So if we want to do something like multimodal auto encoding, where we have some of the outputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2536" target="_blank">00:42:16.340</a></span> | <span class="t">are videos, we use the same construction trick to get positions, to get queries that have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2542" target="_blank">00:42:22.980</a></span> | <span class="t">the relevant semantics for each of the points that we're decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2545" target="_blank">00:42:25.940</a></span> | <span class="t">And we can do this even though basically the sizes of these different data, so the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2550" target="_blank">00:42:30.540</a></span> | <span class="t">of points they have is quite diverse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2552" target="_blank">00:42:32.820</a></span> | <span class="t">So in the multimodal auto encoding experiments that we have in this paper, we do this for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2557" target="_blank">00:42:37.180</a></span> | <span class="t">video, audio, and labels at the same time, so that all of them are just passed into their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2560" target="_blank">00:42:40.820</a></span> | <span class="t">uniform network, and then decoded one by one in this way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2564" target="_blank">00:42:44.900</a></span> | <span class="t">But we can also do mass language modeling now by conditioning on the position in a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2571" target="_blank">00:42:51.340</a></span> | <span class="t">We can do multitask classification by having basically an index that gives which task you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2576" target="_blank">00:42:56.500</a></span> | <span class="t">querying from the network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2578" target="_blank">00:42:58.940</a></span> | <span class="t">And we can do things like optical flow by passing in input features as well as the positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2583" target="_blank">00:43:03.940</a></span> | <span class="t">And so I'm just going to just quickly skip to a couple of the different-- I can share</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2590" target="_blank">00:43:10.300</a></span> | <span class="t">these slides with you all afterwards to look through them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2594" target="_blank">00:43:14.620</a></span> | <span class="t">Some of these things are quite cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2596" target="_blank">00:43:16.620</a></span> | <span class="t">But just quickly, I want to talk about language and then optical flow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2601" target="_blank">00:43:21.980</a></span> | <span class="t">So for language, basically what we wanted to do with this was to see if we could use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2607" target="_blank">00:43:27.180</a></span> | <span class="t">this to replace tokenization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2610" target="_blank">00:43:30.020</a></span> | <span class="t">And why might we care about getting rid of tokenization?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2613" target="_blank">00:43:33.580</a></span> | <span class="t">So one, we use tokenization primarily because transformers scale poorly with sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2618" target="_blank">00:43:38.980</a></span> | <span class="t">And tokenizing cuts sequence length by about a factor of four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2624" target="_blank">00:43:44.660</a></span> | <span class="t">But there are various problems that arise with this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2626" target="_blank">00:43:46.900</a></span> | <span class="t">And so why might we care about removing tokenizers?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2632" target="_blank">00:43:52.380</a></span> | <span class="t">So for one, tokenizers perform less well on rare words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2638" target="_blank">00:43:58.220</a></span> | <span class="t">So if you compare the sort of the byte-based decomposition, the UTF-8 encoding of an input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2644" target="_blank">00:44:04.620</a></span> | <span class="t">sequence like this, you can see that there's basically a uniform allocation of points in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2650" target="_blank">00:44:10.220</a></span> | <span class="t">memory to each of the input characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2652" target="_blank">00:44:12.260</a></span> | <span class="t">The exception are diacritics, which end up splitting into two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2656" target="_blank">00:44:16.140</a></span> | <span class="t">But if you look at the sentence piece tokenization, so it's learned that pepper is one token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2661" target="_blank">00:44:21.620</a></span> | <span class="t">but jalapeno gets split into five in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2665" target="_blank">00:44:25.520</a></span> | <span class="t">So this basically says the amount of capacity that you allocate depends on how rare the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2671" target="_blank">00:44:31.220</a></span> | <span class="t">word is, which can lead to suboptimal encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2675" target="_blank">00:44:35.780</a></span> | <span class="t">They're also brittle to subtle perturbations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2677" target="_blank">00:44:37.740</a></span> | <span class="t">A famous example of this is that if you've ever played around with GPT-3, you'll notice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2684" target="_blank">00:44:44.500</a></span> | <span class="t">that the output can be quite sensitive to if you add a space or emit a space at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2688" target="_blank">00:44:48.580</a></span> | <span class="t">end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2689" target="_blank">00:44:49.580</a></span> | <span class="t">And that basically is because the space can end up being factorized into different parts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2692" target="_blank">00:44:52.380</a></span> | <span class="t">of the tokenization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2693" target="_blank">00:44:53.380</a></span> | <span class="t">There are other things that can happen there, too, but this is one cause of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2698" target="_blank">00:44:58.780</a></span> | <span class="t">And finally, tokens don't transfer across languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2701" target="_blank">00:45:01.620</a></span> | <span class="t">So if you wanted to have a model that without any tuning could be used on many different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2705" target="_blank">00:45:05.220</a></span> | <span class="t">languages at the same time, tokenizers are a blocker for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2708" target="_blank">00:45:08.620</a></span> | <span class="t">So if we can get rid of them, it'll simplify the pipeline, it'll also make things less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2712" target="_blank">00:45:12.140</a></span> | <span class="t">brittle, and then hopefully lead to more general models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2716" target="_blank">00:45:16.620</a></span> | <span class="t">So the way that we do mass language modeling is the same as the way that I showed in that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2720" target="_blank">00:45:20.260</a></span> | <span class="t">schematic auto-encoding experiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2723" target="_blank">00:45:23.020</a></span> | <span class="t">So we mask some fraction of our inputs, about 15%, is sort of the standard magic number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2728" target="_blank">00:45:28.780</a></span> | <span class="t">We then decode at each of the positions that are masked, and we task the model with decoding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2733" target="_blank">00:45:33.980</a></span> | <span class="t">whatever characters were masked at those locations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2738" target="_blank">00:45:38.420</a></span> | <span class="t">And then once we have this model, so this is what we do for pre-training, we can then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2741" target="_blank">00:45:41.740</a></span> | <span class="t">fine tune it by replacing the decoder with a multitask decoder that takes in the tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2747" target="_blank">00:45:47.180</a></span> | <span class="t">that we're using on the downstream evaluation setting, and training the model to reconstruct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2753" target="_blank">00:45:53.060</a></span> | <span class="t">the logits on a per-task basis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2756" target="_blank">00:45:56.340</a></span> | <span class="t">Okay, so to look at how this model performs, we basically first compare it to BERT base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2764" target="_blank">00:46:04.420</a></span> | <span class="t">So this is just a solid benchmark that we understand very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2767" target="_blank">00:46:07.860</a></span> | <span class="t">And first, by looking at sort of matched, two models that have matched flops, we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2773" target="_blank">00:46:13.420</a></span> | <span class="t">see that Perceiver IO and BERT base work on par.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2778" target="_blank">00:46:18.420</a></span> | <span class="t">You see there's a different trade-off here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2779" target="_blank">00:46:19.980</a></span> | <span class="t">So to get the same number of flops, basically we make Perceiver IO deeper, and this ends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2784" target="_blank">00:46:24.660</a></span> | <span class="t">up giving it more parameters, but on a per-flops basis, it ends up performing about the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2791" target="_blank">00:46:31.900</a></span> | <span class="t">On the other hand, if we remove the tokenizer from BERT and keep the flops the same, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2796" target="_blank">00:46:36.620</a></span> | <span class="t">see that the number of parameters and the depth just drastically fall down, and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2801" target="_blank">00:46:41.540</a></span> | <span class="t">is because BERT scales quite poorly with sequence length, because it uses a normal transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2807" target="_blank">00:46:47.260</a></span> | <span class="t">But if we use a Perceiver without the tokenization, we can see that we only get a slight reduction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2814" target="_blank">00:46:54.140</a></span> | <span class="t">in the number of parameters at the flops count, but the performance performs almost exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2819" target="_blank">00:46:59.220</a></span> | <span class="t">the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2820" target="_blank">00:47:00.500</a></span> | <span class="t">So this means that the Perceiver in this setting is performing basically the same with and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2823" target="_blank">00:47:03.980</a></span> | <span class="t">without the tokenization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2824" target="_blank">00:47:04.980</a></span> | <span class="t">It's learning a different strategy, it's using different parameters, but it basically can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2828" target="_blank">00:47:08.380</a></span> | <span class="t">be brought to the same performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2830" target="_blank">00:47:10.980</a></span> | <span class="t">We can then scale this more by leaning into what happens in the tokenizer-free setting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2835" target="_blank">00:47:15.740</a></span> | <span class="t">and we see that we can get a moderate performance boost as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2840" target="_blank">00:47:20.660</a></span> | <span class="t">I think it's also, in the language setting, it's also useful to look at what the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2844" target="_blank">00:47:24.820</a></span> | <span class="t">maps that are learned, and what's being visualized here are basically, for each of the latents,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2849" target="_blank">00:47:29.740</a></span> | <span class="t">for some subset of the latents, we're looking at where it's attending to in the input sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2855" target="_blank">00:47:35.220</a></span> | <span class="t">and some of these end up being local, so looking at specific points in the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2860" target="_blank">00:47:40.780</a></span> | <span class="t">Some of them are periodic, so they look at recurring points over the sequence, and some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2866" target="_blank">00:47:46.020</a></span> | <span class="t">of them also look like they pick out syntactic features, which is quite nice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2870" target="_blank">00:47:50.260</a></span> | <span class="t">So they pick out basically exclamation points, or capital letters, or other punctuation that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2875" target="_blank">00:47:55.460</a></span> | <span class="t">quite useful and decodable right at the beginning of the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2882" target="_blank">00:48:02.420</a></span> | <span class="t">We can also basically use this exact same architecture on optical flow, and optical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2888" target="_blank">00:48:08.100</a></span> | <span class="t">flow is basically an important classical problem in computer vision, where given a pair of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2893" target="_blank">00:48:13.220</a></span> | <span class="t">frames in a video, we want to basically track all of the points, so figure out the motion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2898" target="_blank">00:48:18.340</a></span> | <span class="t">from every point from one frame to the other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2901" target="_blank">00:48:21.540</a></span> | <span class="t">And so optical flow is usually visualized using these sort of colorized images that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2905" target="_blank">00:48:25.900</a></span> | <span class="t">are shown on the bottom, and what this gives you basically is a per pixel indication of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2910" target="_blank">00:48:30.460</a></span> | <span class="t">the velocity at every single point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2913" target="_blank">00:48:33.420</a></span> | <span class="t">And so you can see that, so the blade that the character here is holding is moving to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2920" target="_blank">00:48:40.060</a></span> | <span class="t">the right, whereas this creature behind her is sort of moving downwards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2926" target="_blank">00:48:46.980</a></span> | <span class="t">So there are a couple of problems with optical flow that make it interesting to sort of approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2932" target="_blank">00:48:52.900</a></span> | <span class="t">So one is it's a dense task, and it basically involves long range correspondences, but the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2939" target="_blank">00:48:59.060</a></span> | <span class="t">standard training protocol, there's basically no large scale realistic training data, just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2942" target="_blank">00:49:02.660</a></span> | <span class="t">because it's incredibly hard to sort of label all of the pixels in a real world scene and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2946" target="_blank">00:49:06.660</a></span> | <span class="t">figure out where they go to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2948" target="_blank">00:49:08.540</a></span> | <span class="t">So the typical way to do this is to train on some synthetic data, and then evaluate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2951" target="_blank">00:49:11.980</a></span> | <span class="t">on more realistic scenes, and optical flow is also interesting, because it's basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2959" target="_blank">00:49:19.620</a></span> | <span class="t">the locus of some of the most complicated visual architectures in the literature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2964" target="_blank">00:49:24.780</a></span> | <span class="t">So the previous state of the art result here is this method called raft, which won the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2968" target="_blank">00:49:28.860</a></span> | <span class="t">best paper award at DCCV last year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2971" target="_blank">00:49:31.540</a></span> | <span class="t">And I'm just highlighting this to give you a sense of how much work people do into sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2974" target="_blank">00:49:34.980</a></span> | <span class="t">of hand engineering these architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2977" target="_blank">00:49:37.580</a></span> | <span class="t">So this is a very, very cleverly designed architecture, and basically it incorporates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2981" target="_blank">00:49:41.580</a></span> | <span class="t">things like global correlation volumes that are explicitly computed at different offsets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2986" target="_blank">00:49:46.340</a></span> | <span class="t">to basically allow the model to reason about how things at different scales are moving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2990" target="_blank">00:49:50.580</a></span> | <span class="t">with respect to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2993" target="_blank">00:49:53.460</a></span> | <span class="t">It also has local neighborhood gather operations, as well as update blocks to keep track of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=2998" target="_blank">00:49:58.980</a></span> | <span class="t">what's happening within each specific correlation block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3002" target="_blank">00:50:02.180</a></span> | <span class="t">And then finally, there's a flow-specific upsampling operators that were developed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3007" target="_blank">00:50:07.560</a></span> | <span class="t">So in contrast to this, we're basically-- we wanted to see how well Perceiver.io would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3013" target="_blank">00:50:13.100</a></span> | <span class="t">do here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3014" target="_blank">00:50:14.240</a></span> | <span class="t">And just to give you a sense of sort of what we were expecting coming into this, we thought,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3017" target="_blank">00:50:17.820</a></span> | <span class="t">well, maybe-- so Perceiver.io was throwing a lot of the structure away, so we were hoping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3021" target="_blank">00:50:21.540</a></span> | <span class="t">that we would get some good results, but it would probably overfit, and there's this sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3025" target="_blank">00:50:25.060</a></span> | <span class="t">of problem of the domain transfer that's happening here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3027" target="_blank">00:50:27.780</a></span> | <span class="t">But on the other hand, self-attention seems to be a reasonable way to match this sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3031" target="_blank">00:50:31.060</a></span> | <span class="t">of correspondence thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3033" target="_blank">00:50:33.260</a></span> | <span class="t">What we actually found was that just by doing the very, very simple preprocessing here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3038" target="_blank">00:50:38.900</a></span> | <span class="t">so extracting basically a patch around each pixel, and then using the standard Perceiver.io</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3045" target="_blank">00:50:45.100</a></span> | <span class="t">architecture, we were able to get state-of-the-art results here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3047" target="_blank">00:50:47.820</a></span> | <span class="t">And so this is basically-- was validation of this general approach of trying to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3055" target="_blank">00:50:55.140</a></span> | <span class="t">general-purpose architectures that would transfer over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3058" target="_blank">00:50:58.460</a></span> | <span class="t">And so basically, with minimal tuning, we were able to get results that would beat both</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3064" target="_blank">00:51:04.380</a></span> | <span class="t">of the sort of compelling benchmarks on both of the Sintel evaluation methods, and to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3071" target="_blank">00:51:11.900</a></span> | <span class="t">comparable results on KITTI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3073" target="_blank">00:51:13.620</a></span> | <span class="t">So these are the standard ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3075" target="_blank">00:51:15.540</a></span> | <span class="t">And we can also sort of visualize what happens when we apply this on real-world data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3080" target="_blank">00:51:20.620</a></span> | <span class="t">So there's no ground truth here, so we can't really compare it, but it's still useful to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3084" target="_blank">00:51:24.780</a></span> | <span class="t">sort of see how it moves around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3087" target="_blank">00:51:27.060</a></span> | <span class="t">And we can see that qualitatively, it's able to capture a lot of the fine structure, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3091" target="_blank">00:51:31.700</a></span> | <span class="t">to sort of get the right motion for the things that are very clearly moving in a specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3097" target="_blank">00:51:37.380</a></span> | <span class="t">direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3099" target="_blank">00:51:39.540</a></span> | <span class="t">We can also sort of-- it's also, I think, informative to look at what happens, how it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3103" target="_blank">00:51:43.780</a></span> | <span class="t">manages to represent sort of small structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3107" target="_blank">00:51:47.620</a></span> | <span class="t">Is this video playing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3108" target="_blank">00:51:48.620</a></span> | <span class="t">Yeah, we can see it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3111" target="_blank">00:51:51.540</a></span> | <span class="t">OK, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3113" target="_blank">00:51:53.300</a></span> | <span class="t">So the thing to look at here is the fine water droplets that are sort of flying through the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3117" target="_blank">00:51:57.200</a></span> | <span class="t">air as that bird flies by.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3119" target="_blank">00:51:59.340</a></span> | <span class="t">And because we're decoding at every single output point, the architecture is able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3124" target="_blank">00:52:04.140</a></span> | <span class="t">represent those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3125" target="_blank">00:52:05.420</a></span> | <span class="t">So it's able to capture very, very fine-scale segmentation that would be difficult to capture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3129" target="_blank">00:52:09.500</a></span> | <span class="t">if you had, for example, a convolutional upsampler here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3132" target="_blank">00:52:12.860</a></span> | <span class="t">OK, so I'm just going to-- oh, the light has gone off in this room.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3140" target="_blank">00:52:20.860</a></span> | <span class="t">I'm also curious if you also try other tasks like depth estimation, because Perseverio</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3146" target="_blank">00:52:26.620</a></span> | <span class="t">looks like it can do also quite well on that modalities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3150" target="_blank">00:52:30.500</a></span> | <span class="t">Yeah, so we haven't published anything, but some internal results suggest that it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3156" target="_blank">00:52:36.420</a></span> | <span class="t">just fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3157" target="_blank">00:52:37.980</a></span> | <span class="t">There basically-- there don't seem to be-- one of the surprising things, the things that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3161" target="_blank">00:52:41.860</a></span> | <span class="t">we were a little bit unsure about, was how much information was going to be contained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3165" target="_blank">00:52:45.180</a></span> | <span class="t">in this latent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3166" target="_blank">00:52:46.700</a></span> | <span class="t">Because basically, you're abstracting quite a lot, and it doesn't have any 2D structure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3170" target="_blank">00:52:50.380</a></span> | <span class="t">intrinsically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3171" target="_blank">00:52:51.900</a></span> | <span class="t">But it does seem like this-- it seems to be able to represent things quite well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3176" target="_blank">00:52:56.820</a></span> | <span class="t">And these sorts of decoding mechanisms do seem to be able to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3180" target="_blank">00:53:00.500</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3181" target="_blank">00:53:01.500</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3182" target="_blank">00:53:02.500</a></span> | <span class="t">So I'm just going to-- just in the interest of time, I'm going to skip ahead to the conclusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3188" target="_blank">00:53:08.420</a></span> | <span class="t">Drew, I had one question with respect to the metrics that you've shared for the optical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3193" target="_blank">00:53:13.380</a></span> | <span class="t">flow, the number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3195" target="_blank">00:53:15.380</a></span> | <span class="t">So in the table, it was like Sintel, Final, Clean, and Kitty.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3200" target="_blank">00:53:20.420</a></span> | <span class="t">Were these different data sets or different metrics?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3203" target="_blank">00:53:23.900</a></span> | <span class="t">Same metric for different data sets, or these are three different metrics?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3207" target="_blank">00:53:27.540</a></span> | <span class="t">Yeah, so these are three different data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3210" target="_blank">00:53:30.140</a></span> | <span class="t">So Sintel Clean and Sintel Final are basically two-- they're two ways of doing the final</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3215" target="_blank">00:53:35.180</a></span> | <span class="t">rendering for Sintel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3217" target="_blank">00:53:37.620</a></span> | <span class="t">In all cases, these methods are trained just on the autoflow data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3221" target="_blank">00:53:41.780</a></span> | <span class="t">So they're trained on this sort of general purpose kind of wacky synthetic motion data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3227" target="_blank">00:53:47.540</a></span> | <span class="t">set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3228" target="_blank">00:53:48.540</a></span> | <span class="t">And then we're evaluating on these different demands without fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3231" target="_blank">00:53:51.060</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3232" target="_blank">00:53:52.060</a></span> | <span class="t">Yeah, the flow has quite-- the data sets are quite small, so it's generally even problematic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3240" target="_blank">00:54:00.140</a></span> | <span class="t">to fine-tune.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3241" target="_blank">00:54:01.140</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3242" target="_blank">00:54:02.580</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3243" target="_blank">00:54:03.580</a></span> | <span class="t">OK, so just to summarize--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3247" target="_blank">00:54:07.540</a></span> | <span class="t">What was the ground truth to find the endpoint error?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3252" target="_blank">00:54:12.740</a></span> | <span class="t">Yeah, so the way this works is Sintel is a computer-- it's basically a relatively high-quality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3260" target="_blank">00:54:20.500</a></span> | <span class="t">CGI movie that was basically open source.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3264" target="_blank">00:54:24.180</a></span> | <span class="t">And so they actually have the ground truth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3265" target="_blank">00:54:25.740</a></span> | <span class="t">So if you know the ground truth 3D state, you can compute the pixel correspondence from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3271" target="_blank">00:54:31.340</a></span> | <span class="t">frame to frame.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3272" target="_blank">00:54:32.420</a></span> | <span class="t">So that's what's used on Sintel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3273" target="_blank">00:54:33.940</a></span> | <span class="t">And then KITTI, they basically have a LiDAR sensor that's used to figure out the depth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3279" target="_blank">00:54:39.740</a></span> | <span class="t">of all points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3280" target="_blank">00:54:40.740</a></span> | <span class="t">And then they compute the correspondences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3283" target="_blank">00:54:43.260</a></span> | <span class="t">So the ground truth is actually the ground truth optical flow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3286" target="_blank">00:54:46.460</a></span> | <span class="t">But in general, it's hard to get dense optical flow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3289" target="_blank">00:54:49.580</a></span> | <span class="t">It's very expensive to collect it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3292" target="_blank">00:54:52.540</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3293" target="_blank">00:54:53.540</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3294" target="_blank">00:54:54.540</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3295" target="_blank">00:54:55.540</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3296" target="_blank">00:54:56.540</a></span> | <span class="t">So basically, just to summarize, so the perceivers are attention-based architectures that scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3303" target="_blank">00:55:03.180</a></span> | <span class="t">linearly and work as drop-in replacements for transformers on a variety of settings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3307" target="_blank">00:55:07.340</a></span> | <span class="t">They also seem to be able to achieve results that are comparable, at least in performance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3313" target="_blank">00:55:13.420</a></span> | <span class="t">with models that rely on 2D convolutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3315" target="_blank">00:55:15.140</a></span> | <span class="t">But of course, there is a trade-off here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3317" target="_blank">00:55:17.260</a></span> | <span class="t">And so it's good to be very aware of this, of generality versus speed in specific domains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3322" target="_blank">00:55:22.620</a></span> | <span class="t">And so as was pointed out, in settings where you can use 2D convolutions, it's certainly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3327" target="_blank">00:55:27.380</a></span> | <span class="t">good to have them in the loop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3330" target="_blank">00:55:30.620</a></span> | <span class="t">It's basically a unified architecture that allows joint modeling of different modalities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3336" target="_blank">00:55:36.340</a></span> | <span class="t">of different sizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3338" target="_blank">00:55:38.820</a></span> | <span class="t">And basically, overall, it seems to be a quite flexible architecture that's able to produce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3343" target="_blank">00:55:43.980</a></span> | <span class="t">a state-of-the-art or near state-of-the-art results on a variety of different domains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3348" target="_blank">00:55:48.460</a></span> | <span class="t">And in the two papers, we look at a number of other domains that I didn't talk about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3353" target="_blank">00:55:53.380</a></span> | <span class="t">including 3D point cloud modeling, replacing the transformer that's used in the StarCraft</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3358" target="_blank">00:55:58.300</a></span> | <span class="t">and the StarCraft behavioral cloning agent, and a couple of others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3363" target="_blank">00:56:03.260</a></span> | <span class="t">So we have a lot of evidence that this general approach seems to work broadly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3367" target="_blank">00:56:07.920</a></span> | <span class="t">And there's a lot of things we still haven't tried.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3369" target="_blank">00:56:09.860</a></span> | <span class="t">So we're very interested in pushing this and always open for suggestions and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3376" target="_blank">00:56:16.420</a></span> | <span class="t">So we're relying on a large body of related work because we're drawing from a lot of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3381" target="_blank">00:56:21.260</a></span> | <span class="t">areas here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3382" target="_blank">00:56:22.260</a></span> | <span class="t">So here are some highlights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3384" target="_blank">00:56:24.140</a></span> | <span class="t">And then I just want to thank my co-authors on this work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3389" target="_blank">00:56:29.420</a></span> | <span class="t">And of course, I'm happy to talk more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3391" target="_blank">00:56:31.700</a></span> | <span class="t">Thanks, Drew.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3393" target="_blank">00:56:33.540</a></span> | <span class="t">Yeah, thanks a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3396" target="_blank">00:56:36.340</a></span> | <span class="t">Thanks, Drew.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3397" target="_blank">00:56:37.340</a></span> | <span class="t">So one question I had is, so what do you think is the future of perceiver models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3404" target="_blank">00:56:44.300</a></span> | <span class="t">Do you think this is going to be used more in the transformer community to replace Conn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3412" target="_blank">00:56:52.220</a></span> | <span class="t">and add to none of this stuff?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3414" target="_blank">00:56:54.580</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3415" target="_blank">00:56:55.580</a></span> | <span class="t">So I think, broadly speaking, I think of perceivers now as sort of-- because we know how to adapt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3422" target="_blank">00:57:02.860</a></span> | <span class="t">them pretty well to sort of domains where we don't have a great idea of the right way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3426" target="_blank">00:57:06.820</a></span> | <span class="t">to structure an architecture, an inductive bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3429" target="_blank">00:57:09.940</a></span> | <span class="t">So I think that's one of the really strong cases for it, so settings in which you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3435" target="_blank">00:57:15.220</a></span> | <span class="t">really know what the right way to structure a problem is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3438" target="_blank">00:57:18.420</a></span> | <span class="t">I also think these kinds of approaches can be used in conjunction with confnets for sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3444" target="_blank">00:57:24.540</a></span> | <span class="t">of things that are as domain agnostic as needed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3448" target="_blank">00:57:28.940</a></span> | <span class="t">But I think multimodal and new domains is really the-- that's where these are obvious</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3455" target="_blank">00:57:35.500</a></span> | <span class="t">choices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3456" target="_blank">00:57:36.500</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3457" target="_blank">00:57:37.500</a></span> | <span class="t">Also, what do you think are the current bottlenecks with this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3459" target="_blank">00:57:39.420</a></span> | <span class="t">And if you don't mind, if you can disclose, what are you working on towards next with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3463" target="_blank">00:57:43.780</a></span> | <span class="t">this stuff?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3465" target="_blank">00:57:45.900</a></span> | <span class="t">So I can't talk about too many details about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3470" target="_blank">00:57:50.080</a></span> | <span class="t">But a couple of domains-- so one, we don't really have a great handle on how to use them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3476" target="_blank">00:57:56.420</a></span> | <span class="t">on sort of small-scale data, so data where you don't have the data to sort of recover</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3484" target="_blank">00:58:04.300</a></span> | <span class="t">the inductive bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3485" target="_blank">00:58:05.540</a></span> | <span class="t">So this is, I think, a really important area.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3488" target="_blank">00:58:08.100</a></span> | <span class="t">The other thing that we haven't sort of talked about here, but you could probably imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3491" target="_blank">00:58:11.940</a></span> | <span class="t">that we'd be thinking about, would be how to train on multiple modalities or sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3495" target="_blank">00:58:15.220</a></span> | <span class="t">multiple things at the same time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3497" target="_blank">00:58:17.780</a></span> | <span class="t">So right now, all of these architectures are sort of trained in isolation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3502" target="_blank">00:58:22.420</a></span> | <span class="t">But there are a lot of opportunities for sort of figuring out how to pose problems together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3506" target="_blank">00:58:26.940</a></span> | <span class="t">and use a single architecture on all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3508" target="_blank">00:58:28.940</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3509" target="_blank">00:58:29.940</a></span> | <span class="t">Also, I'm not sure if you've tried, but can you also use this for tabular data stuff?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3515" target="_blank">00:58:35.860</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3516" target="_blank">00:58:36.860</a></span> | <span class="t">So effectively, the architecture treats any input data as tabular data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3521" target="_blank">00:58:41.340</a></span> | <span class="t">So I think that's exactly the right way to think about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3524" target="_blank">00:58:44.260</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3525" target="_blank">00:58:45.260</a></span> | <span class="t">Sounds good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3526" target="_blank">00:58:46.260</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3527" target="_blank">00:58:47.260</a></span> | <span class="t">Thanks for the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3528" target="_blank">00:58:48.260</a></span> | <span class="t">I will open to make general questions from the students.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3530" target="_blank">00:58:50.900</a></span> | <span class="t">So let's jump to the recording.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3531" target="_blank">00:58:51.900</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3532" target="_blank">00:58:52.900</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3533" target="_blank">00:58:53.900</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3533" target="_blank">00:58:53.900</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wTZ3o36lXoQ&t=3535" target="_blank">00:58:55.960</a></span> | <span class="t">you</span></div></div></body></html>