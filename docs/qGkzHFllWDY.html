<html><head><title>Stanford CS25: V1 I Transformers in Language: The development of GPT Models, GPT3</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V1 I Transformers in Language: The development of GPT Models, GPT3</h2><a href="https://www.youtube.com/watch?v=qGkzHFllWDY"><img src="https://i.ytimg.com/vi/qGkzHFllWDY/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=8">0:8</a> 3-Gram Model (Shannon 1951)<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=27">0:27</a> Recurrent Neural Nets (Sutskever et al 2011)<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=72">1:12</a> Big LSTM (Jozefowicz et al 2016)<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=112">1:52</a> Transformer (Llu and Saleh et al 2018)<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=153">2:33</a> GPT-2: Big Transformer (Radford et al 2019)<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=218">3:38</a> GPT-3: Very Big Transformer (Brown et al 2019)<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=312">5:12</a> GPT-3: Can Humans Detect Generated News Articles?<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=549">9:9</a> Why Unsupervised Learning?<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=638">10:38</a> Is there a Big Trove of Unlabeled Data?<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=671">11:11</a> Why Use Autoregressive Generative Models for Unsupervised Learnin<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=780">13:0</a> Unsupervised Sentiment Neuron (Radford et al 2017)<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=851">14:11</a> Radford et al 2018)<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=921">15:21</a> Zero-Shot Reading Comprehension<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1004">16:44</a> GPT-2: Zero-Shot Translation<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1095">18:15</a> Language Model Metalearning<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1163">19:23</a> GPT-3: Few Shot Arithmetic<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1214">20:14</a> GPT-3: Few Shot Word Unscrambling<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1236">20:36</a> GPT-3: General Few Shot Learning<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1422">23:42</a> IGPT (Chen et al 2020): Can we apply GPT to images?<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1531">25:31</a> IGPT: Completions<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1584">26:24</a> IGPT: Feature Learning<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1940">32:20</a> Isn't Code Just Another Modality?<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2013">33:33</a> The HumanEval Dataset<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2160">36:0</a> The Pass @ K Metric<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2219">36:59</a> Codex: Training Details<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2283">38:3</a> An Easy Human Eval Problem (pass@1 -0.9)<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2316">38:36</a> A Medium HumanEval Problem (pass@1 -0.17)<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2340">39:0</a> A Hard HumanEval Problem (pass@1 -0.005)<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2486">41:26</a> Calibrating Sampling Temperature for Pass@k<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2539">42:19</a> The Unreasonable Effectiveness of Sampling<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2597">43:17</a> Can We Approximate Sampling Against an Oracle?<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2752">45:52</a> Main Figure<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2813">46:53</a> Limitations<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2858">47:38</a> Conclusion<br><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2899">48:19</a> Acknowledgements<br><br><div style="text-align: left;"><a href="./qGkzHFllWDY.html">Whisper Transcript</a> | <a href="./transcript_qGkzHFllWDY.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Great. Okay, perfect. So, a sample from this model looks like this. So, they also point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=11" target="_blank">00:00:11.240</a></span> | <span class="t">to $99.6 billion from 2004063%. It's a bunch of kind of gibberish. So, the sentence isn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=19" target="_blank">00:00:19.840</a></span> | <span class="t">too coherent, but at least the words do seem to be somewhat related, like they come from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=24" target="_blank">00:00:24.400</a></span> | <span class="t">the same space. Now, jumping forward to the beginning of the deep learning boom in 2011,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=31" target="_blank">00:00:31.960</a></span> | <span class="t">we have language modeling with neural networks now, and in particular with recurrent neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=37" target="_blank">00:00:37.040</a></span> | <span class="t">networks. So, we can get rid of this giant lookup table from the n-gram models, and instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=42" target="_blank">00:00:42.600</a></span> | <span class="t">we can have our inputs be these tokens and let this kind of recurrent cell remember some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=49" target="_blank">00:00:49.240</a></span> | <span class="t">state and persist some state. So, if we set up a neural model like this, we get a sample</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=55" target="_blank">00:00:55.680</a></span> | <span class="t">as shown below. So, the meaning of life is the tradition of the ancient human reproduction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=60" target="_blank">00:01:00.280</a></span> | <span class="t">is less favorable to the good boy for when to remove vigor. So, again, this doesn't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=65" target="_blank">00:01:05.780</a></span> | <span class="t">make any sense, but it kind of starts to have the flow of a real sentence. Yeah, so jumping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=73" target="_blank">00:01:13.040</a></span> | <span class="t">forward even more to 2016, we have LSTM models, and of course, LSTMs are an architectural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=80" target="_blank">00:01:20.520</a></span> | <span class="t">innovation on top of RNNs, and they have kind of better gradient flow, so they can better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=86" target="_blank">00:01:26.120</a></span> | <span class="t">model long-term dependencies. And so, with an LSTM model, we get a sample like this with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=92" target="_blank">00:01:32.740</a></span> | <span class="t">even more new technologies coming onto the market quickly. During the past three years,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=97" target="_blank">00:01:37.320</a></span> | <span class="t">an increasing number of companies must tackle the ever-changing and ever-changing environmental</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=102" target="_blank">00:01:42.120</a></span> | <span class="t">challenges online. So, this sentence is starting to make a little bit of sense, though there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=106" target="_blank">00:01:46.120</a></span> | <span class="t">are clear artifacts like the repetition of the phrase ever-changing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=110" target="_blank">00:01:50.640</a></span> | <span class="t">Now, starting in 2018, we have our first autoregressive transformer-based language models, which are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=119" target="_blank">00:01:59.080</a></span> | <span class="t">even better at modeling these very long-term dependencies. And here, what I'm showing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=124" target="_blank">00:02:04.320</a></span> | <span class="t">an example of a completion. So, in a completion, the user supplies the prompt. In this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=131" target="_blank">00:02:11.720</a></span> | <span class="t">this text swings over Kansas, and the model will continue from this prompt. So, you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=138" target="_blank">00:02:18.440</a></span> | <span class="t">see that this completion is coherent across multiple sentences now, though there are notable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=143" target="_blank">00:02:23.840</a></span> | <span class="t">spelling mistakes. So, you see this like a whatever document it is, so it doesn't quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=150" target="_blank">00:02:30.520</a></span> | <span class="t">make sense. And now, we arrive at GPT-2, which is a 1.5-billion-parameter transformer model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=160" target="_blank">00:02:40.280</a></span> | <span class="t">And I copied in what I personally found was the most compelling completion from GPT-2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=165" target="_blank">00:02:45.920</a></span> | <span class="t">And in contrast with the last slide, what this does is it sets up a clearly fake prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=171" target="_blank">00:02:51.920</a></span> | <span class="t">So, we have something about finding unicorns and scientists in South America. And so, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=179" target="_blank">00:02:59.520</a></span> | <span class="t">model's probably not seen this exact prompt before and has to make up something that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=182" target="_blank">00:03:02.760</a></span> | <span class="t">consistent. So, the thing I find most impressive is it does so, and it's coherent across multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=189" target="_blank">00:03:09.760</a></span> | <span class="t">paragraphs. It invents this fictional Dr. Perez, and it persists Perez throughout multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=196" target="_blank">00:03:16.120</a></span> | <span class="t">paragraphs. And I think it's very aptly named. You have him from University of LaPaz. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=203" target="_blank">00:03:23.240</a></span> | <span class="t">yeah, we just have fairly coherent completions at this point. So, it's worth disclosing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=209" target="_blank">00:03:29.360</a></span> | <span class="t">this was the best of 10 samples. So, we still had to sample multiple times to get a sample</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=215" target="_blank">00:03:35.320</a></span> | <span class="t">like this. And finally, to end this session--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=220" target="_blank">00:03:40.840</a></span> | <span class="t">>> Sorry, can I interrupt?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=221" target="_blank">00:03:41.840</a></span> | <span class="t">>> Yeah, yeah, for sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=222" target="_blank">00:03:42.840</a></span> | <span class="t">>> Are you talking about just examples of it failing, the worst of the 10?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=225" target="_blank">00:03:45.840</a></span> | <span class="t">>> I can pull some up, yes. Yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=228" target="_blank">00:03:48.720</a></span> | <span class="t">>> I'm curious to know what the bad looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=230" target="_blank">00:03:50.320</a></span> | <span class="t">>> Yes, yes, yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=231" target="_blank">00:03:51.320</a></span> | <span class="t">>> [inaudible]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=232" target="_blank">00:03:52.320</a></span> | <span class="t">>> Wait, sorry, one last question. When you have these 10, you said we took the best of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=238" target="_blank">00:03:58.840</a></span> | <span class="t">10. That doesn't make sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=239" target="_blank">00:03:59.840</a></span> | <span class="t">>> Yeah. So, this is human judged. And I'll probably expand a little bit on that for today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=244" target="_blank">00:04:04.840</a></span> | <span class="t">So, I want to end this kind of flyby overview with GPT-3. And since GPT-2 already produces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=253" target="_blank">00:04:13.640</a></span> | <span class="t">such coherent text, how do you characterize GPT-3? And I would say that the best way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=259" target="_blank">00:04:19.240</a></span> | <span class="t">do so is that say you took the best out of five or 10 completions from GPT-2, that would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=266" target="_blank">00:04:26.800</a></span> | <span class="t">be kind of your first completion from GPT-3. And of course, best is kind of a personal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=271" target="_blank">00:04:31.960</a></span> | <span class="t">metric here. So, here I'm showing completion from the book Three-Body Problem. And you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=281" target="_blank">00:04:41.360</a></span> | <span class="t">can see that the impressive things about this completion are that it really stays true to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=285" target="_blank">00:04:45.760</a></span> | <span class="t">this style of the novel. I think the second thing that kind of impressed me was just how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=292" target="_blank">00:04:52.040</a></span> | <span class="t">poetic like the metaphors and similes that it produces are. So, you have this stuff like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=296" target="_blank">00:04:56.880</a></span> | <span class="t">blood was seeping through her jacket and a dark red flower was blooming on her chest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=300" target="_blank">00:05:00.720</a></span> | <span class="t">It's kind of like very poetic and stylistic sentences. So, it definitely understands it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=305" target="_blank">00:05:05.960</a></span> | <span class="t">part of a novel and it's trying to generate this kind of prose in the same style.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=311" target="_blank">00:05:11.880</a></span> | <span class="t">So, as generated text becomes more and more coherent, I think one really...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=317" target="_blank">00:05:17.120</a></span> | <span class="t">[inaudible]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=318" target="_blank">00:05:18.120</a></span> | <span class="t">Yeah, yeah. So, it's 175 billion parameters versus GPT-2, which is around one billion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=326" target="_blank">00:05:26.120</a></span> | <span class="t">[inaudible]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=327" target="_blank">00:05:27.120</a></span> | <span class="t">Yeah, yeah. That's a very good question. So, there's kind of... Maybe we can dive into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=341" target="_blank">00:05:41.920</a></span> | <span class="t">it a little bit after, but there is work on kind of neural scaling laws. And so, the idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=346" target="_blank">00:05:46.000</a></span> | <span class="t">is like, can you predict the performance of a larger model from a series of smaller models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=350" target="_blank">00:05:50.760</a></span> | <span class="t">And so, I would rather characterize the increase in performance, not by kind of the small gain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=354" target="_blank">00:05:54.760</a></span> | <span class="t">in perplexity, but like whether it lines up with the projections. And in that sense, GPT-3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=360" target="_blank">00:06:00.120</a></span> | <span class="t">does. So, yeah, that's some intuition for... Yeah. I think personally, at OpenAI, we would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=366" target="_blank">00:06:06.280</a></span> | <span class="t">have stopped the experiment if we did it. So, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=367" target="_blank">00:06:07.280</a></span> | <span class="t">No, I just think it's interesting how... This is like a general thing, so we don't need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=368" target="_blank">00:06:08.280</a></span> | <span class="t">to go into this tangent, but in machine learning, you see people pushing for like an extra,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=369" target="_blank">00:06:09.280</a></span> | <span class="t">you know, 1% to like 0.5% accuracy, but the models are increasing in a scale that's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=382" target="_blank">00:06:22.280</a></span> | <span class="t">functional.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=383" target="_blank">00:06:23.280</a></span> | <span class="t">Right, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=384" target="_blank">00:06:24.280</a></span> | <span class="t">So, I wonder sometimes whether it's worth it and like where you should stop, like [inaudible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=389" target="_blank">00:06:29.440</a></span> | <span class="t">00:47]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=390" target="_blank">00:06:30.440</a></span> | <span class="t">Right. Yeah. I think maybe this side we'll get to it a little bit, but there's also some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=394" target="_blank">00:06:34.440</a></span> | <span class="t">sense in which like as you reach kind of like the entry floor of modeling, like every having</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=400" target="_blank">00:06:40.760</a></span> | <span class="t">kind of gives you like... If you think about accuracy, right, it's not on a linear scale,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=406" target="_blank">00:06:46.760</a></span> | <span class="t">right? Like a 1% early on isn't the same as that last 1%. And so, yeah, those last bits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=413" target="_blank">00:06:53.680</a></span> | <span class="t">really do help you squeeze a little bit out of that, you know, as accuracy, yep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=417" target="_blank">00:06:57.480</a></span> | <span class="t">Sorry, excuse me, but I want to ask this too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=420" target="_blank">00:07:00.480</a></span> | <span class="t">Oh, yes, yes. Sorry, this is accuracy. I will explain this slide. Yep. Cool. So, yep. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=427" target="_blank">00:07:07.520</a></span> | <span class="t">as generated text becomes more and more realistic, I think one very natural question to ask is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=432" target="_blank">00:07:12.080</a></span> | <span class="t">whether humans can still distinguish between real and fake text, right? And so, in here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=437" target="_blank">00:07:17.040</a></span> | <span class="t">we have... This is, of course, like a very set up scenario. It's not... In all cases,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=443" target="_blank">00:07:23.320</a></span> | <span class="t">the model's able to trick humans, but this is for news articles. We kind of presented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=447" target="_blank">00:07:27.680</a></span> | <span class="t">GPT-3 generated samples against real news articles. And you can tell kind of as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=452" target="_blank">00:07:32.920</a></span> | <span class="t">number of parameters increases, the ability of humans to distinguish between the real</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=457" target="_blank">00:07:37.440</a></span> | <span class="t">and fake articles, that that ability goes down to near random chance. And... Oh, yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=464" target="_blank">00:07:44.840</a></span> | <span class="t">How did you generate the news articles? What prompts did you use?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=470" target="_blank">00:07:50.000</a></span> | <span class="t">I'm actually not completely sure. So, I didn't do this work particularly, but I think one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=476" target="_blank">00:07:56.680</a></span> | <span class="t">possible approach would be to prime with a couple of news articles and then just to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=480" target="_blank">00:08:00.840</a></span> | <span class="t">a delimiter and just have it start generating news articles from there. Yeah. Any other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=487" target="_blank">00:08:07.280</a></span> | <span class="t">questions? Great. So, even with all of these impressive results, I think it's worth taking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=496" target="_blank">00:08:16.200</a></span> | <span class="t">a step back at this point and asking, "What do we really care about language modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=500" target="_blank">00:08:20.160</a></span> | <span class="t">for? And what is it actually useful for?" And I think one can make the argument that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=505" target="_blank">00:08:25.800</a></span> | <span class="t">it is actually a fairly narrow capability. Like, why would you just want some system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=509" target="_blank">00:08:29.520</a></span> | <span class="t">that just continues text for you? And you could argue that there's more important tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=514" target="_blank">00:08:34.320</a></span> | <span class="t">to solve, like summarization or translation. And I think most researchers at OpenAI would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=519" target="_blank">00:08:39.400</a></span> | <span class="t">agree with this point of view. And in fact, GPT was not really a project that was focused</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=525" target="_blank">00:08:45.340</a></span> | <span class="t">on language modeling as an end goal, but mostly as a tool to solve a problem called unsupervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=531" target="_blank">00:08:51.480</a></span> | <span class="t">learning, which I'm going to go through in the next couple of slides. So, I want to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=536" target="_blank">00:08:56.240</a></span> | <span class="t">a history of language modeling at OpenAI and hopefully motivate why we ended up at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=541" target="_blank">00:09:01.680</a></span> | <span class="t">GPT series of models and kind of how we arrived there. And hopefully it'll become much more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=546" target="_blank">00:09:06.920</a></span> | <span class="t">intuitive after this session. So, the deep learning boom started in 2012</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=553" target="_blank">00:09:13.440</a></span> | <span class="t">with AlexNet, which was a system that could take images and labels and it could classify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=559" target="_blank">00:09:19.080</a></span> | <span class="t">images to their labels. And what we found with AlexNet was these systems were able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=564" target="_blank">00:09:24.440</a></span> | <span class="t">generalize surprisingly well. Like you could take data sets that weren't necessarily the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=568" target="_blank">00:09:28.080</a></span> | <span class="t">training distribution and you still have pretty good features on. And since then, this kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=573" target="_blank">00:09:33.560</a></span> | <span class="t">of supervised approach has been really, really powerful, right? We've been able to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=577" target="_blank">00:09:37.720</a></span> | <span class="t">models in many different domains to classify very accurately. And you can even have some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=583" target="_blank">00:09:43.680</a></span> | <span class="t">guarantees that supervised learning will work well. So, there's empirical risk minimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=589" target="_blank">00:09:49.800</a></span> | <span class="t">But the problem with supervised learning is that oftentimes the labels are scarce, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=595" target="_blank">00:09:55.000</a></span> | <span class="t">Especially in language tasks, there isn't really that many kind of texts paired with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=598" target="_blank">00:09:58.880</a></span> | <span class="t">their summaries or too many pairs across languages, for instance. So, collecting a lot of data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=605" target="_blank">00:10:05.880</a></span> | <span class="t">can be not too hard, but actually scalably labeling all of that data, it could be very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=610" target="_blank">00:10:10.360</a></span> | <span class="t">time consuming and expensive. So, the main problem with unsupervised learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=615" target="_blank">00:10:15.120</a></span> | <span class="t">is can we also learn from unlabeled data? And this is a lot scarier because all of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=621" target="_blank">00:10:21.240</a></span> | <span class="t">sudden we're starting to optimize an objective, which isn't the one we care about downstream,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=625" target="_blank">00:10:25.320</a></span> | <span class="t">right? So, a lot of the guarantees that we used to have, we no longer have. And we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=632" target="_blank">00:10:32.000</a></span> | <span class="t">only kind of hope that we learn some features that are adaptable to a wide variety of downstream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=636" target="_blank">00:10:36.740</a></span> | <span class="t">tasks. But nevertheless, there's a reason to be very optimistic in language. And the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=644" target="_blank">00:10:44.120</a></span> | <span class="t">reason is that there is a huge trove of unlabeled data and it's called the internet. And so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=649" target="_blank">00:10:49.280</a></span> | <span class="t">the real question is, can we leverage all this unlabeled data from the internet to solve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=654" target="_blank">00:10:54.040</a></span> | <span class="t">language tasks where we don't really have that much data? And the hope is that if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=659" target="_blank">00:10:59.560</a></span> | <span class="t">kind of pre-train this model on the internet, you'll see all of these words used in different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=663" target="_blank">00:11:03.360</a></span> | <span class="t">settings, kind of understand the relationships, and you'll be able to leverage this kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=667" target="_blank">00:11:07.440</a></span> | <span class="t">understanding for any kind of task we do. So, now that we've established why language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=674" target="_blank">00:11:14.120</a></span> | <span class="t">is such a good domain to try unsupervised learning in, let's talk about why use generative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=679" target="_blank">00:11:19.040</a></span> | <span class="t">models for it and also why use autoregressive generative models. And I do want to stress</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=684" target="_blank">00:11:24.360</a></span> | <span class="t">that a lot of the guarantees you have with supervised learning are no longer there for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=688" target="_blank">00:11:28.280</a></span> | <span class="t">unsupervised learning. So, some of these arguments will be a little bit kind of intuitive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=693" target="_blank">00:11:33.000</a></span> | <span class="t">And so, the first argument I want to present is this quote by Richard Feynman, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=698" target="_blank">00:11:38.680</a></span> | <span class="t">pretty widespread. So, what I cannot create, I do not understand. And there's the inverse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=704" target="_blank">00:11:44.080</a></span> | <span class="t">of this idea, which we call analysis by synthesis. And it's what I can create, I can also understand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=710" target="_blank">00:11:50.000</a></span> | <span class="t">And this has been studied by Josh Tenenbaum. There's definitely some kind of biological</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=716" target="_blank">00:11:56.840</a></span> | <span class="t">motivation as well for it. But the idea here is that if you're able to create a language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=724" target="_blank">00:12:04.560</a></span> | <span class="t">model, which can generate diverse samples that are coherent, then it must also build</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=728" target="_blank">00:12:08.760</a></span> | <span class="t">up representations that can help you solve language understanding tasks. And then the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=734" target="_blank">00:12:14.120</a></span> | <span class="t">next question is, why do we use autoregressive models? You might argue that autoregressive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=739" target="_blank">00:12:19.680</a></span> | <span class="t">models are a kind of local objective, right? Like you're just predicting the next words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=743" target="_blank">00:12:23.960</a></span> | <span class="t">You could do really well with kind of some N-gram approximation, right? Why would it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=749" target="_blank">00:12:29.280</a></span> | <span class="t">be good at solving things that allow you to summarize an entire piece of text? And so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=754" target="_blank">00:12:34.320</a></span> | <span class="t">an intuitive argument here could be, say that you wanted to do very well on language modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=759" target="_blank">00:12:39.960</a></span> | <span class="t">for a mystery novel. And there's this grand reveal at the end, like, oh, like the culprit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=765" target="_blank">00:12:45.200</a></span> | <span class="t">was, and then you want to predict that next token. And to do really well at that task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=770" target="_blank">00:12:50.040</a></span> | <span class="t">you really need to have a good understanding of what happened in the story, along with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=773" target="_blank">00:12:53.440</a></span> | <span class="t">all the twists and turns, and maybe even some of this kind of like deductive reasoning built</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=777" target="_blank">00:12:57.440</a></span> | <span class="t">in. So the first sign of life, oh, you got a question? Oh yeah. So the first sign of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=788" target="_blank">00:13:08.960</a></span> | <span class="t">life we had at OpenAI was in the task of predicting whether Amazon reviews were positive or negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=794" target="_blank">00:13:14.880</a></span> | <span class="t">And this was work done in 2017. So instead of training a classifier in the kind of typical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=800" target="_blank">00:13:20.840</a></span> | <span class="t">supervised way, what we did was we trained an LSTM model just to predict the next character</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=806" target="_blank">00:13:26.060</a></span> | <span class="t">in Amazon reviews. And when we trained a linear model on the features from this LSTM, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=812" target="_blank">00:13:32.120</a></span> | <span class="t">we found surprisingly was like one of these cells or one of these neurons was firing in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=817" target="_blank">00:13:37.740</a></span> | <span class="t">terms of predicting sentiment. And positive activations for this neuron corresponded to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=823" target="_blank">00:13:43.780</a></span> | <span class="t">positive reviews and negative activations to negative reviews. And this was despite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=828" target="_blank">00:13:48.680</a></span> | <span class="t">not seeing any of the labels at training time. So you can even track kind of what this neuron</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=835" target="_blank">00:13:55.080</a></span> | <span class="t">value is across a sample. So it's a little bit hard to read, but these are reviews where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=840" target="_blank">00:14:00.080</a></span> | <span class="t">maybe someone says, oh, I really like this film, but I didn't like this part. And you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=843" target="_blank">00:14:03.240</a></span> | <span class="t">can kind of see the sentiment switching as you go from positive to negative. So yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=850" target="_blank">00:14:10.080</a></span> | <span class="t">just predicting the next character resulted in-- oh, yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=853" target="_blank">00:14:13.960</a></span> | <span class="t">Was there any sort of complicated architecture to encourage it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=859" target="_blank">00:14:19.360</a></span> | <span class="t">Oh, yeah. No, no. This was just a pure LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=861" target="_blank">00:14:21.720</a></span> | <span class="t">Oh, yeah. So you basically looked at all the neurons and saw which ones were most--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=865" target="_blank">00:14:25.000</a></span> | <span class="t">Yeah, in the hidden state. Yeah. So you train a linear classifier on top of that, and one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=868" target="_blank">00:14:28.560</a></span> | <span class="t">neuron is firing with, yeah, just outsized predictive power. Yeah. Great. So next, GPT-1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=876" target="_blank">00:14:36.280</a></span> | <span class="t">was one of the first demonstrations that this kind of approach could work broadly for text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=880" target="_blank">00:14:40.800</a></span> | <span class="t">So GPT-1 was trained on the internet, not on Amazon Reviews anymore, and it was fine-tuned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=885" target="_blank">00:14:45.900</a></span> | <span class="t">on a bunch of different downstream tasks. And one thing to stress here was kind of to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=891" target="_blank">00:14:51.920</a></span> | <span class="t">your point that the fine-tuning was very, I guess, minimally kind of-- you're not kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=898" target="_blank">00:14:58.920</a></span> | <span class="t">of bashing the architecture apart and kind of repurposing new modules. And it's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=904" target="_blank">00:15:04.720</a></span> | <span class="t">a new head that classifies for your task. And this showed that you can use this approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=911" target="_blank">00:15:11.700</a></span> | <span class="t">not just for semantic analysis, but also for entailment, semantic similarity, and getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=916" target="_blank">00:15:16.840</a></span> | <span class="t">SODAs on a lot of these benchmarks downstream. So I've already presented GPT-2 from the point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=924" target="_blank">00:15:24.160</a></span> | <span class="t">of view of a very powerful language model. And now, I think it's worth revisiting from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=928" target="_blank">00:15:28.760</a></span> | <span class="t">the viewpoint of unsupervised learning. So like GPT-1, GPT-2 was trained on a large chunk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=934" target="_blank">00:15:34.320</a></span> | <span class="t">of the internet. And it's only trained to predict the next token or word from previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=939" target="_blank">00:15:39.500</a></span> | <span class="t">words. But the key insight of GPT-2 is that many downstream tasks can be expressed naturally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=945" target="_blank">00:15:45.920</a></span> | <span class="t">as a language modeling task. And yeah, so GPT-2 explores how well we can perform on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=951" target="_blank">00:15:51.760</a></span> | <span class="t">downstream tasks simply by using this method without any fine-tuning. So let me start with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=956" target="_blank">00:15:56.760</a></span> | <span class="t">a couple of examples. So let's say you want to solve some reading comprehension benchmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=962" target="_blank">00:16:02.280</a></span> | <span class="t">And this is usually set up as a prompt, which is some passage you have to read, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=965" target="_blank">00:16:05.880</a></span> | <span class="t">a bunch of questions which you have to answer. So you can literally just take the entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=969" target="_blank">00:16:09.760</a></span> | <span class="t">prompting context. You put a question colon. You write out the question, answer colon,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=975" target="_blank">00:16:15.440</a></span> | <span class="t">and then have the model complete from there. And this kind of gives you zero-shot reading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=979" target="_blank">00:16:19.800</a></span> | <span class="t">comprehension. We can also use it for other tasks, like summarization. For instance, here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=986" target="_blank">00:16:26.400</a></span> | <span class="t">like the beginning of a CNN article about some archaeological finding. And you can just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=994" target="_blank">00:16:34.160</a></span> | <span class="t">put TLDR after you see this passage. And the model, hopefully, if it's good enough, will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1000" target="_blank">00:16:40.000</a></span> | <span class="t">produce good summaries. And the final example I want to show is that you can do zero-shot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1007" target="_blank">00:16:47.040</a></span> | <span class="t">translation as well. So the way you would do this is if you wanted to convert, let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1012" target="_blank">00:16:52.480</a></span> | <span class="t">say, a French sentence into English, you could set up a prompt like the sentence-- insert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1016" target="_blank">00:16:56.960</a></span> | <span class="t">the French sentence, translate it from French to English means, and then the model will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1021" target="_blank">00:17:01.000</a></span> | <span class="t">complete. And you can sometimes do this as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1025" target="_blank">00:17:05.240</a></span> | <span class="t">And one kind of critical thing to note here is that here's the chart of performance as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1029" target="_blank">00:17:09.680</a></span> | <span class="t">you increase the number of parameters. And in all these models, they were trained on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1037" target="_blank">00:17:17.000</a></span> | <span class="t">the same data set. So the only kind of compounding variable is scale. And you can see that as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1041" target="_blank">00:17:21.120</a></span> | <span class="t">we scale up the models, these kind of zero-shot capabilities emerge and kind of smoothly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1047" target="_blank">00:17:27.680</a></span> | <span class="t">get better. So the role of scale is important here. And yeah, and I think these are starting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1053" target="_blank">00:17:33.640</a></span> | <span class="t">to approach some-- I guess they're not great benchmarks, but at least respectable benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1059" target="_blank">00:17:39.600</a></span> | <span class="t">Yeah, yeah, yeah, exactly. It's not going to be great in a lot of cases. And to be honest,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1065" target="_blank">00:17:45.960</a></span> | <span class="t">the blue metric used for translation is actually often-- oh, thank you very much. It's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1072" target="_blank">00:17:52.120</a></span> | <span class="t">a great metric. What it does is it takes a reference solution. And basically, it does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1077" target="_blank">00:17:57.920</a></span> | <span class="t">some kind of like n-gram comparison. So it is a big problem to have good translation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1084" target="_blank">00:18:04.240</a></span> | <span class="t">metrics in an LP. And yeah, I think when I talk about code, I'll talk a little bit more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1094" target="_blank">00:18:14.600</a></span> | <span class="t">about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1095" target="_blank">00:18:15.600</a></span> | <span class="t">Right, so let's finally talk about how GPT-3 fits into this picture. So the primary insight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1101" target="_blank">00:18:21.520</a></span> | <span class="t">of GPT-3 is that the training process itself can be interpreted in the context of meta-learn,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1107" target="_blank">00:18:27.200</a></span> | <span class="t">which is kind of like learning over a distribution of tasks. And during training, what the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1112" target="_blank">00:18:32.000</a></span> | <span class="t">is doing is it's developing certain kind of capabilities. It's picking up some set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1118" target="_blank">00:18:38.720</a></span> | <span class="t">skills in terms of modeling certain passages. And during inference time, what it's doing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1125" target="_blank">00:18:45.080</a></span> | <span class="t">it's quickly picking up on what a task is based on what the prompt is so far, and adapting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1130" target="_blank">00:18:50.960</a></span> | <span class="t">to that task to predict the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1133" target="_blank">00:18:53.360</a></span> | <span class="t">So you can view there's this outward loop of all the SGD steps you're doing during training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1138" target="_blank">00:18:58.760</a></span> | <span class="t">this inward loop of picking up on what the task is, and then modeling the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1144" target="_blank">00:19:04.240</a></span> | <span class="t">So you can imagine a lot of tasks being framed in this way. For instance, on the left, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1148" target="_blank">00:19:08.920</a></span> | <span class="t">can have addition. You have a lot of examples of addition in context. And hopefully, that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1154" target="_blank">00:19:14.520</a></span> | <span class="t">would help you with a new addition problem, or you can try to unscramble a word, for instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1160" target="_blank">00:19:20.480</a></span> | <span class="t">And I'll explore results on these two benchmarks in the next slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1165" target="_blank">00:19:25.360</a></span> | <span class="t">So this setting we call a few-shot arithmetic. And just to explain what's going on, you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1170" target="_blank">00:19:30.600</a></span> | <span class="t">taking the entire context slide of your transformer, and you're putting in as many examples as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1175" target="_blank">00:19:35.120</a></span> | <span class="t">will fit. And then finally, you put in the example that you would like to solve. So here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1182" target="_blank">00:19:42.200</a></span> | <span class="t">these examples could be these kind of first three addition problems, and then you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1189" target="_blank">00:19:49.240</a></span> | <span class="t">31 plus 41 equals, and you ask the model to complete.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1192" target="_blank">00:19:52.960</a></span> | <span class="t">So you notice that as the language model gets bigger, it's better able to recognize this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1197" target="_blank">00:19:57.560</a></span> | <span class="t">task. And you can see that performance on addition, subtraction, even some kind of multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1203" target="_blank">00:20:03.560</a></span> | <span class="t">tasks increases sharply as you go towards 200 billion parameters. And there does seem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1208" target="_blank">00:20:08.840</a></span> | <span class="t">to be some step function change right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1212" target="_blank">00:20:12.320</a></span> | <span class="t">And looking at word unscrambling, this is also true. So we have parameters, again, on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1219" target="_blank">00:20:19.120</a></span> | <span class="t">the x-axis. We have accuracy, and each of these is a different kind of unscrambled task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1223" target="_blank">00:20:23.280</a></span> | <span class="t">So this blue line is you do a cyclic shift of the letters, and you want it to uncycle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1228" target="_blank">00:20:28.800</a></span> | <span class="t">And there's a lot of other transforms you can do, like randomly inserting words, for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1234" target="_blank">00:20:34.960</a></span> | <span class="t">instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1235" target="_blank">00:20:35.960</a></span> | <span class="t">So the final point here is that this is a pretty general phenomenon. We didn't just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1241" target="_blank">00:20:41.440</a></span> | <span class="t">test it on these two aforementioned tasks. We tried an array of, I think, 40 plus tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1248" target="_blank">00:20:48.620</a></span> | <span class="t">And here you can see how the zero shot, one shot, and few shot performance increases as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1252" target="_blank">00:20:52.680</a></span> | <span class="t">we scale the models. So of course, they're all smoothly increasing. But one thing to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1257" target="_blank">00:20:57.860</a></span> | <span class="t">be aware of is that the gap between zero shot and few shot is also improving as a function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1262" target="_blank">00:21:02.640</a></span> | <span class="t">of scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1265" target="_blank">00:21:05.600</a></span> | <span class="t">Awesome. So we've just seen that we can pre-train a transform-- oh, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1272" target="_blank">00:21:12.760</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1273" target="_blank">00:21:13.760</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1274" target="_blank">00:21:14.760</a></span> | <span class="t">One is the tasks themselves that we're using. Two is the number of parameters. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1285" target="_blank">00:21:25.360</a></span> | <span class="t">three, my understanding, is also the quantity of data that we've ingested.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1288" target="_blank">00:21:28.060</a></span> | <span class="t">Yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1289" target="_blank">00:21:29.060</a></span> | <span class="t">And I was curious between those three, which ones-- you've shown a lot of-- the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1293" target="_blank">00:21:33.260</a></span> | <span class="t">of parameters definitely helps. I was curious, though, in terms of the degree to which also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1297" target="_blank">00:21:37.900</a></span> | <span class="t">the training tasks and the sophistication of the tasks, as well as the quantity of data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1302" target="_blank">00:21:42.200</a></span> | <span class="t">ingested.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1303" target="_blank">00:21:43.200</a></span> | <span class="t">Yeah, yeah. So I guess I can dive-- maybe it's something to save for after. But yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1309" target="_blank">00:21:49.240</a></span> | <span class="t">let's dig into that after.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1310" target="_blank">00:21:50.240</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1311" target="_blank">00:21:51.240</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1312" target="_blank">00:21:52.240</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1313" target="_blank">00:21:53.240</a></span> | <span class="t">I guess GPT-2 and 3 aren't different. GPT-1 just has an extra classification head for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1321" target="_blank">00:22:01.980</a></span> | <span class="t">the training tasks. Yeah. Yeah. Great, yeah. Good questions. So yeah, we've just seen that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1329" target="_blank">00:22:09.220</a></span> | <span class="t">we can use a transformer in this pre-train and binding setup, where we have a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1334" target="_blank">00:22:14.780</a></span> | <span class="t">unlabeled data in the pre-training setting. And we have just a little bit of data in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1339" target="_blank">00:22:19.660</a></span> | <span class="t">binding setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1340" target="_blank">00:22:20.660</a></span> | <span class="t">And we can solve a lot of language tasks in this way. And I would say this has become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1345" target="_blank">00:22:25.760</a></span> | <span class="t">the dominant paradigm in language over the last couple of years. So there's follow-up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1350" target="_blank">00:22:30.100</a></span> | <span class="t">objectives like BERT and T5, which have done extremely good at pushing the SOTA. But there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1355" target="_blank">00:22:35.420</a></span> | <span class="t">nothing really that says that these transformer models have to be applied to language. The</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1360" target="_blank">00:22:40.380</a></span> | <span class="t">transformer is a sequence model. And as such, it can just ingest any sequence of bytes and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1365" target="_blank">00:22:45.620</a></span> | <span class="t">model them. And when you think about this, all of the data that we consume, like videos</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1370" target="_blank">00:22:50.180</a></span> | <span class="t">or audio, they're represented on our computers as sequences of bytes, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1374" target="_blank">00:22:54.140</a></span> | <span class="t">And so you might think, oh, could this approach be used to just model whatever modality we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1379" target="_blank">00:22:59.740</a></span> | <span class="t">want? And I think this kind of paradigm is very, at least interesting, when we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1388" target="_blank">00:23:08.100</a></span> | <span class="t">really have good inductive biases. Like we don't [INAUDIBLE] data. But one question to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1392" target="_blank">00:23:12.500</a></span> | <span class="t">ask is, does it even work when you do have really strong inductive biases?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1396" target="_blank">00:23:16.660</a></span> | <span class="t">So I'm going to present some work that suggests that the answer is, yes, it still works fairly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1403" target="_blank">00:23:23.280</a></span> | <span class="t">well in this case, in the domain of images, where convolutions are already so popular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1408" target="_blank">00:23:28.660</a></span> | <span class="t">and proven out. And I'm going to show a second result very briefly here, which is DALI, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1414" target="_blank">00:23:34.020</a></span> | <span class="t">shows that it's strong enough to even ingest two different modalities and be able to jointly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1419" target="_blank">00:23:39.260</a></span> | <span class="t">model them. So the first question is, how would you apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1424" target="_blank">00:23:44.340</a></span> | <span class="t">GPTU to images? And there's a few things you have to do. You have to modify this autoregressive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1430" target="_blank">00:23:50.340</a></span> | <span class="t">next word prediction objective. So the natural analog is, you can think of images as a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1436" target="_blank">00:23:56.700</a></span> | <span class="t">strange language, where the words are pixels instead. And instead, you need to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1441" target="_blank">00:24:01.860</a></span> | <span class="t">the next pixel at each point. And so we can just change the objective from next word prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1446" target="_blank">00:24:06.200</a></span> | <span class="t">to next pixel prediction. And of course, we want this kind of large--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1450" target="_blank">00:24:10.520</a></span> | <span class="t">[INAUDIBLE] Oh, yeah. So you just unroll it as a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1457" target="_blank">00:24:17.140</a></span> | <span class="t">It's the same way it's stored on a computer. You just have a sequence of bytes. Yeah. Yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1462" target="_blank">00:24:22.300</a></span> | <span class="t">good question. So in the language setting, we pre-train on this large unlabeled data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1467" target="_blank">00:24:27.020</a></span> | <span class="t">set on the internet, and we fine tune on question answering or these other benchmarks. In images,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1474" target="_blank">00:24:34.420</a></span> | <span class="t">one good analog of this situation is you can pre-train on image net without the labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1478" target="_blank">00:24:38.700</a></span> | <span class="t">You have, let's say, a low resource-- low data, sorry, setting, like CIFAR. And you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1483" target="_blank">00:24:43.020</a></span> | <span class="t">can try to attack CIFAR classification. And of course, in both settings, you can do fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1487" target="_blank">00:24:47.700</a></span> | <span class="t">tuning. In GPT, you can do zero shot. And I would say the standard eval on images is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1492" target="_blank">00:24:52.660</a></span> | <span class="t">you do linear probes. So you take features from your model. The model is frozen. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1497" target="_blank">00:24:57.340</a></span> | <span class="t">pass through CIFAR through the model, get some features, and you see how predictive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1501" target="_blank">00:25:01.860</a></span> | <span class="t">these features are of the CIFAR classes. Is it kind of pixels there, which basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1508" target="_blank">00:25:08.540</a></span> | <span class="t">you ask a model to predict the max pixel given the-- Yeah, yeah. So pixel CNN is an instantiation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1514" target="_blank">00:25:14.660</a></span> | <span class="t">of an autoregressive image prediction model. So what we're asking here is, can we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1519" target="_blank">00:25:19.100</a></span> | <span class="t">take the same transformer architecture that we use in language, don't make any modifications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1523" target="_blank">00:25:23.700</a></span> | <span class="t">at all, and just throw-- so there's no kind of 2D prior. So yeah, I'll call this a model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1534" target="_blank">00:25:34.900</a></span> | <span class="t">that we train image GTC or IGPT for short. And here you can see actually what some completions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1540" target="_blank">00:25:40.180</a></span> | <span class="t">from the model look like. So on the left column, what I'm feeding in is the pixels of the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1545" target="_blank">00:25:45.980</a></span> | <span class="t">half of the image. And the next four columns, what you're seeing is different model-generated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1551" target="_blank">00:25:51.620</a></span> | <span class="t">completions. And the right column here is the original reference image. And you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1557" target="_blank">00:25:57.260</a></span> | <span class="t">actually see that the model is kind of doing some interesting things. If you look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1560" target="_blank">00:26:00.740</a></span> | <span class="t">last two rows, it's not coming up with semantically the same completion every single time. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1565" target="_blank">00:26:05.980</a></span> | <span class="t">like putting these birds in different settings, sometimes adding reflections. It's putting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1570" target="_blank">00:26:10.380</a></span> | <span class="t">this lighthouse in grassy areas and watery areas, for instance. So if you buy into this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1575" target="_blank">00:26:15.980</a></span> | <span class="t">philosophy of analysis by synthesis, we definitely have some hint of the synthesis part. So I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1584" target="_blank">00:26:24.580</a></span> | <span class="t">don't have time to go through all the results with you. But I just want to say that it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1588" target="_blank">00:26:28.580</a></span> | <span class="t">fairly successful in this SIFAR setting where you don't have much labeled data. If you train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1593" target="_blank">00:26:33.820</a></span> | <span class="t">a linear model on top of the features, you get better results than if you do the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1600" target="_blank">00:26:40.380</a></span> | <span class="t">approach with a ResNet trained on ImageNet with labels. So that's like the typical approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1604" target="_blank">00:26:44.620</a></span> | <span class="t">in the field. You train some ResNet on ImageNet, you get the features. Oh yeah. And if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1610" target="_blank">00:26:50.100</a></span> | <span class="t">compare to this approach, a generative model on ImageNet without the labels, take the features,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1616" target="_blank">00:26:56.220</a></span> | <span class="t">it's actually better predictive of synthesis. Yeah. I'm just curious, once the architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1621" target="_blank">00:27:01.620</a></span> | <span class="t">for this is the same as GPT? Oh yeah. Exactly. Yeah, yeah, yeah, yeah. It's the GPT architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1629" target="_blank">00:27:09.500</a></span> | <span class="t">So you can modify GPT to have 2D bias. You can do 2D position embeddings. Well, we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1634" target="_blank">00:27:14.580</a></span> | <span class="t">do that. We just want to see, can you use the same exact approach? So early use of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1640" target="_blank">00:27:20.180</a></span> | <span class="t">data is just sequential. Yeah. But also there's metadata showing about how that sequential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1643" target="_blank">00:27:23.980</a></span> | <span class="t">should be reconstructed. Like what's the weight, for example. Oh, can you explain? So the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1652" target="_blank">00:27:32.340</a></span> | <span class="t">on this stored, but when you want to transform that sequence into an image, you have metadata</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1658" target="_blank">00:27:38.180</a></span> | <span class="t">that will say something like, just like in NumPy arrays, it'll say, here's the strike.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1661" target="_blank">00:27:41.780</a></span> | <span class="t">So here's how to rearrange it. I see. What I'm curious to notice is GPT, before it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1667" target="_blank">00:27:47.280</a></span> | <span class="t">given an image, at least given this metadata. I see, I see. Okay. Yeah, that's an extremely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1672" target="_blank">00:27:52.020</a></span> | <span class="t">good question. I don't know how this problem is solved. Yeah, yeah, yeah. In this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1677" target="_blank">00:27:57.840</a></span> | <span class="t">all the images have the same shape. Oh, okay, okay. Yeah, but we don't tell it the concept</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1685" target="_blank">00:28:05.020</a></span> | <span class="t">of row within the model. Yeah, but all images are the same shape. Yeah, so it needs to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1689" target="_blank">00:28:09.460</a></span> | <span class="t">it from the data, but yeah, the data looks the same. Got it. Yeah. It'll be interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1693" target="_blank">00:28:13.100</a></span> | <span class="t">if it's variable image shapes, then it's going to be interesting to do it. Yeah, yeah. Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1701" target="_blank">00:28:21.660</a></span> | <span class="t">Are there a lot more pixels than there are token sizes in the context there? Yeah, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1706" target="_blank">00:28:26.740</a></span> | <span class="t">this is a pretty low resolution images. Yeah, so we can actually, the models we're comparing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1713" target="_blank">00:28:33.380</a></span> | <span class="t">against are trained on kind of high resolution images. So I think that makes it even more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1717" target="_blank">00:28:37.140</a></span> | <span class="t">impressive. But yeah, we're just training at 32 by 32 resolution. Yeah. Cool. So if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1724" target="_blank">00:28:44.580</a></span> | <span class="t">we fine tune these models for CIFAR classification, we can get 99% accuracy, which matches G-pipe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1730" target="_blank">00:28:50.980</a></span> | <span class="t">And this is G-pipe, for instance, is a system which is pre-trained on ImageNet with labels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1735" target="_blank">00:28:55.860</a></span> | <span class="t">and then also fine tuned with labels. So yeah, it just kind of shows you like even this approach,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1741" target="_blank">00:29:01.620</a></span> | <span class="t">which doesn't really know about convolutions can do well. I think you're going to hear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1745" target="_blank">00:29:05.260</a></span> | <span class="t">more about that next week with Lucas' talk. Cool. So by now, it shouldn't be surprising</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1752" target="_blank">00:29:12.860</a></span> | <span class="t">at all that you can model a lot of different modalities with transformers. So in DALI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1757" target="_blank">00:29:17.940</a></span> | <span class="t">we just asked, what about throwing two different modalities at the model and seeing if it can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1763" target="_blank">00:29:23.180</a></span> | <span class="t">learn kind of how to condition on text to produce an image. And for instance, one thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1769" target="_blank">00:29:29.380</a></span> | <span class="t">you might want it to do is like you provide one of these text captions and you want it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1773" target="_blank">00:29:33.340</a></span> | <span class="t">to generate some image like the one below. And the easy way to do this is just train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1777" target="_blank">00:29:37.740</a></span> | <span class="t">a transformer on the concatenation of a caption and an image. And of course, in a lot of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1783" target="_blank">00:29:43.460</a></span> | <span class="t">situations, the idea is very simple, but the implementation and execution is where the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1788" target="_blank">00:29:48.540</a></span> | <span class="t">difficulty is. And I'm not going to talk too much about that. I think the focus today is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1792" target="_blank">00:29:52.340</a></span> | <span class="t">on language, but you can refer to the paper for a lot of those details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1796" target="_blank">00:29:56.140</a></span> | <span class="t">Oh, yeah. So you have a max caption length and you just kind of cut it off at that length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1808" target="_blank">00:30:08.620</a></span> | <span class="t">and you can pad up to that. Right. So you can see that it can generate fairly good samples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1816" target="_blank">00:30:16.900</a></span> | <span class="t">So if you want like a storefront with the word "OpenAI" on it, it's not perfect, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1821" target="_blank">00:30:21.540</a></span> | <span class="t">it's understood at least it's kind of like reverse OCR problem where you take some text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1826" target="_blank">00:30:26.300</a></span> | <span class="t">and render it. And it's kind of typically rendering it in like office looking places.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1831" target="_blank">00:30:31.140</a></span> | <span class="t">So that's one encouraging sign. But I do think my favorite results here are zero-shot image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1838" target="_blank">00:30:38.380</a></span> | <span class="t">transformation. So what's going on here is, for instance, if your prompt is the exact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1842" target="_blank">00:30:42.940</a></span> | <span class="t">same cat on the top as a sketch on the bottom and you feed in the top half of it, this image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1848" target="_blank">00:30:48.820</a></span> | <span class="t">which is a cat, and you ask it to complete the rest of the image, then it'll render the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1853" target="_blank">00:30:53.780</a></span> | <span class="t">top cat actually as like a sketch. And you can do the same thing with like flipping over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1859" target="_blank">00:30:59.580</a></span> | <span class="t">photos, for instance. You can zoom in to a photo. Of course, they're not perfect, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1865" target="_blank">00:31:05.260</a></span> | <span class="t">it has some understanding of what the text is trying to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1868" target="_blank">00:31:08.500</a></span> | <span class="t">In the captions originally, like the training, in the training set, do they have like wording</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1874" target="_blank">00:31:14.740</a></span> | <span class="t">such as extreme close up view? I think that is the, it probably are some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1880" target="_blank">00:31:20.820</a></span> | <span class="t">examples like that. And that's probably where it's picking up some of this knowledge from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1883" target="_blank">00:31:23.940</a></span> | <span class="t">Though we don't seek out these examples. It's just, yeah, yeah, exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1888" target="_blank">00:31:28.860</a></span> | <span class="t">Okay. Perfect. Yeah. So this is just how we just go and do a massive web script. There's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1896" target="_blank">00:31:36.620</a></span> | <span class="t">no kind of, we're not trying to find examples like this. Right. And so you can also do things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1901" target="_blank">00:31:41.940</a></span> | <span class="t">like colorization, right? You can take the cat color red, and this has to kind of recognize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1906" target="_blank">00:31:46.560</a></span> | <span class="t">that what the object is in the figure. And yeah, and here you can do stuff like semantic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1913" target="_blank">00:31:53.620</a></span> | <span class="t">transformations, like adding sunglasses into the cat, and you can put it on postage, for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1918" target="_blank">00:31:58.980</a></span> | <span class="t">instance. Yeah. So it's remarkable that you can do a lot of these, like transform zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1923" target="_blank">00:32:03.420</a></span> | <span class="t">shot. It wasn't trained to do these things specifically. Cool. So moving on, the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1933" target="_blank">00:32:13.060</a></span> | <span class="t">section of my talk today is on codex, which is our most recently released code writing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1937" target="_blank">00:32:17.380</a></span> | <span class="t">models. And the first question you should rightly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1941" target="_blank">00:32:21.420</a></span> | <span class="t">ask here is why, why train them all on code anyway? Isn't at this point, isn't it just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1947" target="_blank">00:32:27.340</a></span> | <span class="t">another modality and what is the novelty that there is at this point? Right. So let me give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1953" target="_blank">00:32:33.820</a></span> | <span class="t">you a couple of reasons. So the first is that GPT-3, it had a rudimentary ability to write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1959" target="_blank">00:32:39.820</a></span> | <span class="t">Python code already from a doc string or descriptive method name. And we actually didn't train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1965" target="_blank">00:32:45.420</a></span> | <span class="t">it on much code data. Actually, I think there might've been active filtering to get rid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1969" target="_blank">00:32:49.540</a></span> | <span class="t">of code data. And so we were surprised that there was this capability anyway. So that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1973" target="_blank">00:32:53.820</a></span> | <span class="t">you know, like if we actually purpose the model and trained it on the large amount of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1977" target="_blank">00:32:57.620</a></span> | <span class="t">code that we can find, maybe something interesting will happen there. Next, what sets apart code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1983" target="_blank">00:33:03.740</a></span> | <span class="t">from other modalities is that there is a kind of ground truth correctness of a sample and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1989" target="_blank">00:33:09.660</a></span> | <span class="t">functions can be tested with unit tests and an interpreter. So this is very different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1994" target="_blank">00:33:14.140</a></span> | <span class="t">from language, whereas to get a ground truth, you might need a human to come in. And even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=1998" target="_blank">00:33:18.500</a></span> | <span class="t">then sometimes humans won't agree like this, this is the better sample or this isn't the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2002" target="_blank">00:33:22.940</a></span> | <span class="t">better sample. And the last thing is I used to dabble in competitive programming myself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2007" target="_blank">00:33:27.380</a></span> | <span class="t">and yeah, I really wanted to create a model that could solve problems that I could. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2014" target="_blank">00:33:34.660</a></span> | <span class="t">yeah, we wrote a paper on it too. So yeah. I think it's kind of a high-level programming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2030" target="_blank">00:33:50.020</a></span> | <span class="t">language which is similar to our human language. Have you guys ever tried to predict some even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2037" target="_blank">00:33:57.860</a></span> | <span class="t">lower level languages like CPP? Yeah. I think there's, yeah, there's follow-up work where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2046" target="_blank">00:34:06.100</a></span> | <span class="t">we just train on a bunch of different languages and I don't know the metrics off the top of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2050" target="_blank">00:34:10.700</a></span> | <span class="t">my head, but I have seen some assembly writing models. Yeah. Cool. So I guess, yeah, continue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2060" target="_blank">00:34:20.740</a></span> | <span class="t">on the third from before. So we have this setting where we have unit tests and interpreter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2065" target="_blank">00:34:25.860</a></span> | <span class="t">So how do we actually evaluate these models in a way that's kind of aware of these two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2070" target="_blank">00:34:30.660</a></span> | <span class="t">concepts? So the first thing we did was we have a data set, a new data set, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2074" target="_blank">00:34:34.900</a></span> | <span class="t">164 handwritten programming problems. And these kind of have the format shown here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2081" target="_blank">00:34:41.140</a></span> | <span class="t">Like there's a function name, a doc string, there's a solution and there's an average</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2085" target="_blank">00:34:45.980</a></span> | <span class="t">of around eight units per problem. And why is it important that we hand wrote these?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2090" target="_blank">00:34:50.220</a></span> | <span class="t">Well, the thing is we're training on such a large part of GitHub. Like if you said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2094" target="_blank">00:34:54.780</a></span> | <span class="t">okay, I'm going to take like some B code problems and I'm going to turn them into an evaluation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2099" target="_blank">00:34:59.260</a></span> | <span class="t">that's not going to work because there's just so many GitHub repos that are like, oh, here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2102" target="_blank">00:35:02.700</a></span> | <span class="t">the solution to this B code problem. So while this doesn't kind of guarantee that this problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2107" target="_blank">00:35:07.340</a></span> | <span class="t">isn't duplicated, at least someone wrote it without trying to copy it from another source.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2114" target="_blank">00:35:14.780</a></span> | <span class="t">So here's some kind of examples of a unit test that you would evaluate the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2120" target="_blank">00:35:20.060</a></span> | <span class="t">function on. I think it should be fairly clear that we should be using this metric. This</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2126" target="_blank">00:35:26.060</a></span> | <span class="t">is the correct kind of ground truth metric to use. I mean, humans do use unit tests to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2130" target="_blank">00:35:30.260</a></span> | <span class="t">evaluate code. And I would say if you're familiar with competitive programming, you can't manually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2135" target="_blank">00:35:35.420</a></span> | <span class="t">judge all like tens of thousands of submissions that are coming in. You need the unit tests</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2139" target="_blank">00:35:39.860</a></span> | <span class="t">and that is a fairly good filter. So one interesting point here was we had to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2144" target="_blank">00:35:44.980</a></span> | <span class="t">create a sandbox environment to run these kind of generated solutions in. Because when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2150" target="_blank">00:35:50.180</a></span> | <span class="t">you train on GitHub, there's a bunch of malicious code. There's a bunch of kind of insecure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2153" target="_blank">00:35:53.900</a></span> | <span class="t">code. You know, all your models should be sampling that and kind of running that on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2157" target="_blank">00:35:57.060</a></span> | <span class="t">your environment. Cool. So now that we have an evaluation data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2162" target="_blank">00:36:02.620</a></span> | <span class="t">set, let's define a metric. And so the metric we're going to use is called pass at K. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2168" target="_blank">00:36:08.540</a></span> | <span class="t">the definition is the average probability over all the problems that at least one out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2172" target="_blank">00:36:12.980</a></span> | <span class="t">of two samples passes the unit test. So if we evaluate this metric by just taking every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2180" target="_blank">00:36:20.500</a></span> | <span class="t">problem and exactly generating K samples, it's actually not, there's high variance just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2186" target="_blank">00:36:26.980</a></span> | <span class="t">kind of sampling in that way. But you imagine the pass rate of a particular sample is around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2190" target="_blank">00:36:30.780</a></span> | <span class="t">one over K. This is kind of like an all or nothing metric. So what we do instead is we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2196" target="_blank">00:36:36.940</a></span> | <span class="t">generate a much larger set of samples and greater than K. Most of the times it's like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2201" target="_blank">00:36:41.580</a></span> | <span class="t">greater than 5K. And we count the number that are correct and we compute this unbiased estimator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2208" target="_blank">00:36:48.100</a></span> | <span class="t">And it looks more complicated than it actually is. It's just complimentary counting. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2212" target="_blank">00:36:52.620</a></span> | <span class="t">take kind of the number of combos where all of them fail. Cool. So then we train our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2222" target="_blank">00:37:02.580</a></span> | <span class="t">And like I alluded to earlier, there's 160, about 160 gigabytes of code, which is collected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2229" target="_blank">00:37:09.900</a></span> | <span class="t">from 54 million repositories. For efficient training, what we did was we fine tuned from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2235" target="_blank">00:37:15.260</a></span> | <span class="t">GPT-3 models of various sizes. And this isn't actually strictly necessary. We find that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2240" target="_blank">00:37:20.620</a></span> | <span class="t">we can get to roughly the same final loss and performance without this, but it is slower</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2246" target="_blank">00:37:26.020</a></span> | <span class="t">to do it without the super training stuff. And so we already have these models, why not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2250" target="_blank">00:37:30.900</a></span> | <span class="t">just fine tune them? And one extra trick to make training much faster here is in code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2256" target="_blank">00:37:36.660</a></span> | <span class="t">there's a lot of runs of spaces, right? And those don't get compressed efficiently in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2260" target="_blank">00:37:40.940</a></span> | <span class="t">language because you just don't see them very often. So they typically get broken up into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2266" target="_blank">00:37:46.140</a></span> | <span class="t">like many separate tokens. So we introduce additionally some tokens that compress runs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2272" target="_blank">00:37:52.220</a></span> | <span class="t">of that space. And that makes training maybe like 30 or 40% more efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2277" target="_blank">00:37:57.340</a></span> | <span class="t">Yeah, exactly. Yeah. Great. So once we have these models, we can go and revisit the human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2287" target="_blank">00:38:07.620</a></span> | <span class="t">eval data set. And I can share a couple of problems to give you a sense of where the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2292" target="_blank">00:38:12.140</a></span> | <span class="t">models are at and also what kind of difficulty level the problems in the data set are at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2298" target="_blank">00:38:18.440</a></span> | <span class="t">So this is a 12 billion parameter model that passed out 90%, which means that 90% of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2303" target="_blank">00:38:23.700</a></span> | <span class="t">samples will pass the unit test. And this is something like anyone doing a first day</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2309" target="_blank">00:38:29.620</a></span> | <span class="t">of Python would be able to do. So you increment all the elements of a list by one. Here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2316" target="_blank">00:38:36.780</a></span> | <span class="t">a problem where the pass rate is 17%. So this is the problem I gave earlier. So you are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2322" target="_blank">00:38:42.820</a></span> | <span class="t">given a non-empty list of integers. You want to return the sum of all odd elements that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2326" target="_blank">00:38:46.820</a></span> | <span class="t">are in even positions. And this might not sound that much harder to you, but models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2329" target="_blank">00:38:49.820</a></span> | <span class="t">can often get confused about like, "Oh, is odd referring to positions or elements?" And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2335" target="_blank">00:38:55.300</a></span> | <span class="t">so here you can actually see that it's doing the right thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2340" target="_blank">00:39:00.980</a></span> | <span class="t">And finally, this is an example of one of the harder problems in the data set. So the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2345" target="_blank">00:39:05.460</a></span> | <span class="t">pass rate is under 1% here. And so what's going on here is actually there's an encode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2349" target="_blank">00:39:09.500</a></span> | <span class="t">function which takes a string. It chunks it up into groups of three characters and it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2354" target="_blank">00:39:14.460</a></span> | <span class="t">does a cyclic shift on each character. And you have to write a decoder, something that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2358" target="_blank">00:39:18.500</a></span> | <span class="t">reverses this operation. So you can see that the model, this is a real model solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2364" target="_blank">00:39:24.940</a></span> | <span class="t">So it chunks up the characters in the same way. You can see that the cyclic shift is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2369" target="_blank">00:39:29.340</a></span> | <span class="t">the opposite way. So up there, it takes the first element of each group, moves it to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2374" target="_blank">00:39:34.540</a></span> | <span class="t">end, and now it takes the last element of each group, moves it to the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2379" target="_blank">00:39:39.540</a></span> | <span class="t">Okay. So I'm wondering what's the effect of... So you had a couple of examples in the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2385" target="_blank">00:39:45.340</a></span> | <span class="t">slide, but you didn't give us in the comments. So I'm wondering if the model will be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2389" target="_blank">00:39:49.540</a></span> | <span class="t">to extrapolate what it's doing by the examples on its own and not relying on the distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2395" target="_blank">00:39:55.100</a></span> | <span class="t">Right. Yeah. So some of our tasks, there are some examples in the doc string, and some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2399" target="_blank">00:39:59.860</a></span> | <span class="t">of them don't. I think it's just to kind of match the distribution of real kind of tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2404" target="_blank">00:40:04.820</a></span> | <span class="t">we find in the real world. Like in this case, it doesn't have it, but definitely for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2409" target="_blank">00:40:09.340</a></span> | <span class="t">unit tests, none of those appear within...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2411" target="_blank">00:40:11.780</a></span> | <span class="t">I'm just curious if you just give it the examples and not be able to distribute all the tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2417" target="_blank">00:40:17.860</a></span> | <span class="t">Oh, I see. I see. So can it do like pure induction where you don't tell the task at all? Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2423" target="_blank">00:40:23.980</a></span> | <span class="t">I haven't tried it, to be honest. I think it's worth a shot. Yeah. Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2430" target="_blank">00:40:30.460</a></span> | <span class="t">Yep. So yeah, at this point, we've trained codex models, we've evaluated on this metric,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2437" target="_blank">00:40:37.020</a></span> | <span class="t">but the thing is, was it worth all this trouble? You already have these metrics like blue that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2442" target="_blank">00:40:42.460</a></span> | <span class="t">are match-based in language. Couldn't we have just used this to approximate it? We don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2447" target="_blank">00:40:47.420</a></span> | <span class="t">need an interpreter, we don't need to generate so many samples. And it would be great if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2452" target="_blank">00:40:52.140</a></span> | <span class="t">it kind of separated out like this. But what we find is that this is, if you take four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2459" target="_blank">00:40:59.260</a></span> | <span class="t">random problems from human data, and you plot the distribution of blue scores for correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2464" target="_blank">00:41:04.460</a></span> | <span class="t">and wrong solutions, you actually find a lot of distributional overlap, right? It's hard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2469" target="_blank">00:41:09.500</a></span> | <span class="t">to distinguish the green from the blue distribution. And so this suggests that blue actually isn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2476" target="_blank">00:41:16.340</a></span> | <span class="t">a very good metric for gauging functional correctness, and that we actually do need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2480" target="_blank">00:41:20.060</a></span> | <span class="t">this new kind of metric and this new data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2487" target="_blank">00:41:27.140</a></span> | <span class="t">So now let's explore the setting where in PASAC-K, K is greater than one. And so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2493" target="_blank">00:41:33.620</a></span> | <span class="t">first observation we have here is that the temperature that you sample at, it affects</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2499" target="_blank">00:41:39.260</a></span> | <span class="t">your PASAC-K. And just for some intuition, if you do temperature zero sampling, you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2504" target="_blank">00:41:44.500</a></span> | <span class="t">going to get the same sample every single time you're doing hard fact sampling. So it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2508" target="_blank">00:41:48.220</a></span> | <span class="t">doesn't matter how many samples you generate, you're just going to get the same PASAC-K.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2513" target="_blank">00:41:53.460</a></span> | <span class="t">But if you want to generate a hundred samples, you can afford to make some mistakes. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2518" target="_blank">00:41:58.420</a></span> | <span class="t">just want a very diverse set of samples. So you can up the temperature. You can see kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2522" target="_blank">00:42:02.820</a></span> | <span class="t">of as you up the temperature, the slope of the kind of number of samples against pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2527" target="_blank">00:42:07.260</a></span> | <span class="t">rate, it becomes steeper. And so you can kind of take the upper hull of this and you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2532" target="_blank">00:42:12.300</a></span> | <span class="t">find the optimal temperature for each number of samples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2539" target="_blank">00:42:19.180</a></span> | <span class="t">And so this brings me to personally my favorite result of the paper, which I call the unreasonable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2543" target="_blank">00:42:23.940</a></span> | <span class="t">effectiveness of sampling. And so let me explain what's going on here. So this is the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2548" target="_blank">00:42:28.720</a></span> | <span class="t">of parameters in the model, and here you have pass rate at one and a pass rate at a hundred.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2554" target="_blank">00:42:34.100</a></span> | <span class="t">And the reason I use this term unreasonable effectiveness is that I think there's a world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2558" target="_blank">00:42:38.700</a></span> | <span class="t">where if the orange line and the blue line weren't that far apart, I might not be that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2564" target="_blank">00:42:44.260</a></span> | <span class="t">surprised. At these scales, the model, it rarely makes kind of syntactical errors anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2569" target="_blank">00:42:49.020</a></span> | <span class="t">Like if you run it, it'll run and produce some kind of output. So you could imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2573" target="_blank">00:42:53.260</a></span> | <span class="t">a world where basically what you're doing, the model has some approach in mind. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2577" target="_blank">00:42:57.340</a></span> | <span class="t">just repeatedly sampling that approach and it's just either right or wrong. But instead,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2581" target="_blank">00:43:01.580</a></span> | <span class="t">what we find is that the model is actually composing different parts and producing functionally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2586" target="_blank">00:43:06.580</a></span> | <span class="t">different things. And you get this huge boost from under 30% to over 70% just by sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2593" target="_blank">00:43:13.460</a></span> | <span class="t">a lot of samples from the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2598" target="_blank">00:43:18.740</a></span> | <span class="t">So unfortunately, knowing that one of your samples is correct, it isn't that useful if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2603" target="_blank">00:43:23.940</a></span> | <span class="t">you don't have access to the unit tests. And one setting where, practical setting where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2610" target="_blank">00:43:30.100</a></span> | <span class="t">you would care about this is say you're creating an autocomplete tool, right? And you generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2614" target="_blank">00:43:34.460</a></span> | <span class="t">a hundred samples, but you don't want to show your user a hundred samples and have them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2618" target="_blank">00:43:38.300</a></span> | <span class="t">pick one, right? You want to kind of try to pre-filter, but you don't have unit tests.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2623" target="_blank">00:43:43.940</a></span> | <span class="t">So can we kind of approximate this Oracle sampling with some other ranking heuristic?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2630" target="_blank">00:43:50.500</a></span> | <span class="t">So here I'm showing a couple of different heuristics. You can randomly pick one, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2635" target="_blank">00:43:55.780</a></span> | <span class="t">the one that seems most promising is to rank by mean probability. And I know it's kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2643" target="_blank">00:44:03.460</a></span> | <span class="t">of maybe not theoretically well grounded, but in language, this kind of heuristic is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2648" target="_blank">00:44:08.500</a></span> | <span class="t">fairly strong as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2653" target="_blank">00:44:13.540</a></span> | <span class="t">So recall that what we're doing is we have this evaluation set where we have kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2658" target="_blank">00:44:18.260</a></span> | <span class="t">standalone functions. We want to produce solutions to that. But when we're doing training, there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2664" target="_blank">00:44:24.260</a></span> | <span class="t">a lot of code that isn't relevant for this task. For instance, there's a lot of classes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2668" target="_blank">00:44:28.600</a></span> | <span class="t">that we're seeing. There's actually data classes too, which aren't relevant at all. And actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2672" target="_blank">00:44:32.820</a></span> | <span class="t">there's a lot of incorrect code on GitHub too. So we might be modeling incorrect solutions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2677" target="_blank">00:44:37.780</a></span> | <span class="t">as well as correct ones. So one thing we thought was let's fine tune codecs on further on a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2684" target="_blank">00:44:44.780</a></span> | <span class="t">couple of data sets where they are standalone functions and you have kind of more guaranteed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2690" target="_blank">00:44:50.780</a></span> | <span class="t">correct solutions to that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2692" target="_blank">00:44:52.740</a></span> | <span class="t">So what we did was we found these problems from a couple of sources. So one is competitive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2697" target="_blank">00:44:57.820</a></span> | <span class="t">programming problems. You can kind of go on these sites. Oftentimes they'll just give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2701" target="_blank">00:45:01.780</a></span> | <span class="t">you the unit test. Sometimes when they don't give you the unit test, you can submit incorrect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2705" target="_blank">00:45:05.500</a></span> | <span class="t">solutions and they'll tell you the first one you failed on and then you can kind of keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2708" target="_blank">00:45:08.580</a></span> | <span class="t">inserting that in. So you can get a lot of competitive programming problems. And another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2714" target="_blank">00:45:14.740</a></span> | <span class="t">source is projects where continuous integration is enabled. So why are these useful? Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2721" target="_blank">00:45:21.620</a></span> | <span class="t">you can actually kind of do an execution tracing. So when you run the integration tests, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2727" target="_blank">00:45:27.220</a></span> | <span class="t">can get all the inputs to functions that are called and their outputs as well. And so you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2731" target="_blank">00:45:31.540</a></span> | <span class="t">actually have the true function body. You know what the test output is supposed to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2735" target="_blank">00:45:35.140</a></span> | <span class="t">You know, kind of the ground truth inputs and outputs. And these are kind of like two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2739" target="_blank">00:45:39.540</a></span> | <span class="t">orthogonal data sets. One kind of helps you with like algorithmic kind of tasks. And one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2744" target="_blank">00:45:44.420</a></span> | <span class="t">is more kind of like, can I manipulate command line utilities and tasks like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2752" target="_blank">00:45:52.260</a></span> | <span class="t">So this brings us to the main figure of the codecs paper. So really what we're seeing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2757" target="_blank">00:45:57.020</a></span> | <span class="t">is a progression of capability. So with GPT-3 on this human eval data set, the pass rate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2762" target="_blank">00:46:02.380</a></span> | <span class="t">at one is zero. Basically you can generate like one or two lines coherently, never really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2767" target="_blank">00:46:07.940</a></span> | <span class="t">a whole program coherently. Now when you fine tune on code, which is codex, this orange</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2774" target="_blank">00:46:14.340</a></span> | <span class="t">line, you start to see some kind of non-noticeable performance on this data set. When you do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2779" target="_blank">00:46:19.260</a></span> | <span class="t">this additional supervised fine tuning, that's this green line, you get even better pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2784" target="_blank">00:46:24.500</a></span> | <span class="t">rates. And then if you kind of generate a hundred samples from this model, re-rank with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2790" target="_blank">00:46:30.380</a></span> | <span class="t">mean log P, even better pass rates. And finally, of course, if you have access to an Oracle,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2795" target="_blank">00:46:35.180</a></span> | <span class="t">it gives you the best pass rates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2796" target="_blank">00:46:36.740</a></span> | <span class="t">So I have one question here. So can you actually use a re-ranking to like, like for the, to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2801" target="_blank">00:46:41.020</a></span> | <span class="t">the model? Can you use it for like as a backdrop signal?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2803" target="_blank">00:46:43.900</a></span> | <span class="t">Yeah, yeah. So we've explored that. I don't know if I can say too much about these results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2813" target="_blank">00:46:53.420</a></span> | <span class="t">And finally, I don't want to suggest that these, these models are perfect. They have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2816" target="_blank">00:46:56.780</a></span> | <span class="t">a lot of limitations that human programmers don't run into. So one is like, actually all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2822" target="_blank">00:47:02.420</a></span> | <span class="t">generative models are autoregressive generative models, kind of, we have some problems with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2826" target="_blank">00:47:06.780</a></span> | <span class="t">binding. So when there's like a lot of variables going on, like a lot of operations going on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2830" target="_blank">00:47:10.860</a></span> | <span class="t">sometimes it's like hard to figure out which operation is binding to which variable. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2834" target="_blank">00:47:14.620</a></span> | <span class="t">you can kind of see some examples of that on the left. And one other kind of counterintuitive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2839" target="_blank">00:47:19.300</a></span> | <span class="t">behavior is composition. So we can take a bunch of very simple building blocks, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2843" target="_blank">00:47:23.740</a></span> | <span class="t">take a string and reverse it, or, or like delete every third character or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2848" target="_blank">00:47:28.220</a></span> | <span class="t">And assuming like, if you can chain two of these operations, you could probably chain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2851" target="_blank">00:47:31.220</a></span> | <span class="t">10 of them, but our models aren't able to do that yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2857" target="_blank">00:47:37.540</a></span> | <span class="t">Cool. So moving on to the conclusion, we have four main points in today's talk. So first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2865" target="_blank">00:47:45.180</a></span> | <span class="t">progress in neural language modeling has been fairly rapid. And at GPT, it wasn't the result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2870" target="_blank">00:47:50.140</a></span> | <span class="t">of a push on language modeling and more of a result of work on pushing unsupervised learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2875" target="_blank">00:47:55.460</a></span> | <span class="t">in language. The third point is that autoregressive modeling is universal and it can yield strong</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2881" target="_blank">00:48:01.660</a></span> | <span class="t">results even when there are strong adaptive biases, like in images or in text. And finally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2888" target="_blank">00:48:08.180</a></span> | <span class="t">we can produce strong co-generating models by fine tuning GPT3 on code. And sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2893" target="_blank">00:48:13.620</a></span> | <span class="t">is an unreasonably effective way to improve model performance. Cool. And to end with some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2899" target="_blank">00:48:19.700</a></span> | <span class="t">acknowledgments, I want to thank my CodeX primary co-authors, some mentors at OpenAI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2905" target="_blank">00:48:25.700</a></span> | <span class="t">and the algorithms team, which I have worked very closely with. Great. Thank you guys for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2910" target="_blank">00:48:30.580</a></span> | <span class="t">your attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2912" target="_blank">00:48:32.460</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2913" target="_blank">00:48:33.460</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qGkzHFllWDY&t=2913" target="_blank">00:48:33.460</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>