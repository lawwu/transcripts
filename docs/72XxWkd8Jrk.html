<html><head><title>Effective agent design patterns in production — Laurie Voss, LlamaIndex</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Effective agent design patterns in production — Laurie Voss, LlamaIndex</h2><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk"><img src="https://i.ytimg.com/vi_webp/72XxWkd8Jrk/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./72XxWkd8Jrk.html">Whisper Transcript</a> | <a href="./transcript_72XxWkd8Jrk.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi, everybody. You probably met me this morning when I greeted you all to the conference. My name</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=19" target="_blank">00:00:19.820</a></span> | <span class="t">is Laurie. I'm VP at VELP Relations at Llama Index. Today I'm going to be talking about Llama</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=25" target="_blank">00:00:25.200</a></span> | <span class="t">Index and what it is very briefly because I've only got 15 minutes. And then we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=30" target="_blank">00:00:30.580</a></span> | <span class="t">to talk about agents and how they are built plus a very, very brief refresher on RAG and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=36" target="_blank">00:00:36.260</a></span> | <span class="t">why it's necessary for agents. Then we're going to look at some high-level design patterns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=40" target="_blank">00:00:40.780</a></span> | <span class="t">that improve the performance of your agent. And we won't have time to dive into how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=45" target="_blank">00:00:45.640</a></span> | <span class="t">build an agent as well as a multi-agent system because we've only got 15 minutes. So let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=50" target="_blank">00:00:50.180</a></span> | <span class="t">go fast. So what is Llama Index? We are a bunch of things. Start with the most obvious. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=55" target="_blank">00:00:55.040</a></span> | <span class="t">are a framework in Python and TypeScript for building generative AI applications. We are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=60" target="_blank">00:01:00.420</a></span> | <span class="t">particularly good at building agents. We also have a service called Llama Parse. Llama Parse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=64" target="_blank">00:01:04.220</a></span> | <span class="t">will parse complicated document formats for you. So PDFs, Word, PowerPoints, those sorts of things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=69" target="_blank">00:01:09.980</a></span> | <span class="t">These are crucial to building an effective agent is being able to parse your unstructured data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=77" target="_blank">00:01:17.340</a></span> | <span class="t">in an effective way. And Llama Parse will demonstrably improve the quality of the agents you build by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=83" target="_blank">00:01:23.380</a></span> | <span class="t">making the data easier for an LLM to understand than if you just try and feed these things in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=88" target="_blank">00:01:28.760</a></span> | <span class="t">through an open source parser or something like that. We also have an enterprise service</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=95" target="_blank">00:01:35.140</a></span> | <span class="t">called Llama Cloud. If what you want to do is stuff documents into one end and get a retrieval</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=99" target="_blank">00:01:39.500</a></span> | <span class="t">endpoint out of the other, then that is the service for you. Unlike the rest of Llama Index, it costs money.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=105" target="_blank">00:01:45.140</a></span> | <span class="t">It is available as a SaaS at cloud.llamaindex.ai or you can get it deployed onto your own private cloud. We also have a website called Llama Hub, which is a huge registry of open source software that plugs into the framework that integrates with everything. So if you need to get your data out of Notion or out of Slack or out of any database in the world, that is where you find the adapters. If you want to store your data in any vector database that exists, the adapters exist for that. And we also integrate with every LLM that exists.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=135" target="_blank">00:02:15.120</a></span> | <span class="t">So, 400 different models over 80 different LLM providers, including local ones like Llama 3. Oh, and it also has pre-built agent tools. So if you are building an agent, you can just plug in an existing agent tool without having to build one yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=152" target="_blank">00:02:32.120</a></span> | <span class="t">So why should you use Llama Index? Because we will help you go faster. That is the base promise of a framework generally. You have actual business and technology problems to solve and you have limited time. A framework is going to help you get past those by skipping the boilerplate, getting best practices for free, and getting to production faster in general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=175" target="_blank">00:02:55.120</a></span> | <span class="t">So, what can you build in Llama Index? Well, anything, obviously, but there's two things that we are particularly good at. One is retrieval augmented generation, and the other one is agents, both of which you are probably familiar with already from just being at this conference because we won't shut up about agents at this conference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=194" target="_blank">00:03:14.400</a></span> | <span class="t">So, what is an agent? An agent is a dramatically overused term in the industry. Just about everything is an agent in 2025.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=206" target="_blank">00:03:26.560</a></span> | <span class="t">But what I mean when I say an agent is it is a bit of semi-autonomous software that can use tools to achieve a goal without you having to explicitly specify what steps it's going to take to achieve that goal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=217" target="_blank">00:03:37.140</a></span> | <span class="t">And the tools can do anything, which is great. They can take retrieve information or they can take action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=222" target="_blank">00:03:42.440</a></span> | <span class="t">Agents are a dramatic departure from traditional programming.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=226" target="_blank">00:03:46.360</a></span> | <span class="t">LLMs are given decision-making power to decide what tools to use and stuff like that. That makes them extremely flexible and powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=234" target="_blank">00:03:54.600</a></span> | <span class="t">So, the time to build an agent is when that flexibility, that ability to deal with the unexpected or the unknown is going to come in useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=243" target="_blank">00:04:03.980</a></span> | <span class="t">And when that's really useful is when you have a bunch of unstructured data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=247" target="_blank">00:04:07.360</a></span> | <span class="t">LLMs are extremely good at handling messy inputs, and the world is full of messy inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=253" target="_blank">00:04:13.200</a></span> | <span class="t">So, there's a whole lot of applications for LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=257" target="_blank">00:04:17.620</a></span> | <span class="t">In general, I regard a good agent use case as any situation where an LLM is required to turn a large body of text into a smaller body of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=265" target="_blank">00:04:25.440</a></span> | <span class="t">I think that is a key principle of agent design and LLM use in general, is that they are not good at taking a small prompt and turning it into a big body of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=273" target="_blank">00:04:33.320</a></span> | <span class="t">They are very good at summarizing stuff down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=276" target="_blank">00:04:36.060</a></span> | <span class="t">So, interpreting a contract, processing an invoice, applying regulations, summarizing documents, a thing where you need to turn text into less text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=285" target="_blank">00:04:45.440</a></span> | <span class="t">So, a calendar event, a decision, a report, an answer to a question, those are good applications for LLMs and good applications for agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=292" target="_blank">00:04:52.400</a></span> | <span class="t">The most obvious application of LLMs is a chat interface where you give it questions and it answers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=298" target="_blank">00:04:58.200</a></span> | <span class="t">I encourage you to think beyond the chat bot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=300" target="_blank">00:05:00.340</a></span> | <span class="t">Chat bots are a very 2023 way of using an LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=304" target="_blank">00:05:04.980</a></span> | <span class="t">We believe a much greater addressable surface is if you integrate them into existing software.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=311" target="_blank">00:05:11.540</a></span> | <span class="t">So, you use the LLMs capability to handle messy inputs, to handle unstructured data, and turn it into structured data that it can then make decisions about and feed into your regular software.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=324" target="_blank">00:05:24.740</a></span> | <span class="t">That is a really productive and powerful set of use cases, and we think it's much bigger than the market for chat bots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=330" target="_blank">00:05:30.460</a></span> | <span class="t">I've talked a lot about unstructured data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=336" target="_blank">00:05:36.380</a></span> | <span class="t">The reason I talk about it is because that is where LLMs become really useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=340" target="_blank">00:05:40.660</a></span> | <span class="t">Unless you are building something extremely generic, you are not going to be able to get anything useful out of an LLM by just asking it questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=349" target="_blank">00:05:49.020</a></span> | <span class="t">You're going to have to give it contextual data relevant to your company, to your domain, to whatever problem set it is that you are working on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=356" target="_blank">00:05:56.020</a></span> | <span class="t">So, you have to feed the LLM your data, and the problem is that you have tons of data, and this is the use case for RAC.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=365" target="_blank">00:06:05.100</a></span> | <span class="t">You take all of your data, you embed it, which is turning it into vectors that you can search for in a vector search, and you can then take your query, take your questions about your data, and embed them into the same vector database, and they will end up mathematically nearby in vector space to the context that you fed in that is relevant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=388" target="_blank">00:06:28.320</a></span> | <span class="t">So, instead of having to take all of your data and feed it to the LLM every single time, which would be tremendously slow and tremendously expensive, you can just feed in the most relevant context out of your data corpus and answer questions about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=403" target="_blank">00:06:43.240</a></span> | <span class="t">This is why RAC will never die.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=406" target="_blank">00:06:46.880</a></span> | <span class="t">People keep talking about larger and larger contexts and, you know, more and more powerful models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=411" target="_blank">00:06:51.300</a></span> | <span class="t">It's always going to be cheaper and faster to send less data that the LLM has to think about less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=416" target="_blank">00:06:56.560</a></span> | <span class="t">It's always going to be, you're always going to get more answers, better answers, if the context that you have given your LLM is more specific.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=427" target="_blank">00:07:07.280</a></span> | <span class="t">So, agents can use RAG as one of their tools, but also, so agents need RAG, but RAG also needs agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=437" target="_blank">00:07:17.120</a></span> | <span class="t">RAG by itself, naive top K RAG, where you just, you know, throw in a query and retrieve the most relevant context and feed that to the LLM, that's not going to work very well for a variety of situations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=449" target="_blank">00:07:29.040</a></span> | <span class="t">But what we've found through lots of production use cases is that layering an agent on top of your RAG will produce significantly higher quality results, and they're capable of doing things that RAG just can't do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=464" target="_blank">00:07:44.220</a></span> | <span class="t">RAG is a simple question and answer robot, whereas an agent can do stuff like introspection, like could this complicated question be answered more easily if it were a series of simpler questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=475" target="_blank">00:07:55.680</a></span> | <span class="t">Do I need to try extracting that data again because the data that I got out is nonsense?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=479" target="_blank">00:07:59.700</a></span> | <span class="t">Did I just give a sensible answer to your question, or should I try again?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=483" target="_blank">00:08:03.760</a></span> | <span class="t">That is something that an agent can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=485" target="_blank">00:08:05.300</a></span> | <span class="t">It can look at its own responses and improve itself, which RAG by itself obviously doesn't do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=490" target="_blank">00:08:10.280</a></span> | <span class="t">So, agents improve the performance of RAG, both in terms of speed and, crucially, in accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=498" target="_blank">00:08:18.260</a></span> | <span class="t">In December of last year, Anthropic did an excellent post about how to build agents, in which they codified some design patterns about how to build an effective agent that we immediately recognized from our own work, building agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=513" target="_blank">00:08:33.060</a></span> | <span class="t">So, I'm going to go through them very quickly, and given the amount of time I have, that's probably all I'm going to be able to cover.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=518" target="_blank">00:08:38.760</a></span> | <span class="t">They are chaining, routing, parallelization, orchestrated workers, and evaluator optimizers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=525" target="_blank">00:08:45.480</a></span> | <span class="t">The first and most obvious is the chain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=528" target="_blank">00:08:48.660</a></span> | <span class="t">You can use an LLM to do some work, and you pass the output of that to another LLM, and you pass the output of that to another LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=535" target="_blank">00:08:55.660</a></span> | <span class="t">It is trivial to build, especially in LLM index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=538" target="_blank">00:08:58.720</a></span> | <span class="t">This is what a chain looks like in LLM index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=541" target="_blank">00:09:01.680</a></span> | <span class="t">We use an abstraction called workflows, where you define regular Python functions that do whatever you need them to do, and you use event annotations to define how...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=553" target="_blank">00:09:13.780</a></span> | <span class="t">Sorry, you use type annotations to define how events pass from step to step within a workflow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=559" target="_blank">00:09:19.580</a></span> | <span class="t">It is a very simple and flexible pattern that our users like a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=563" target="_blank">00:09:23.200</a></span> | <span class="t">So, LLM index workflows have a built-in visualizer, the output of which you can see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=570" target="_blank">00:09:30.720</a></span> | <span class="t">This is obviously a chain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=572" target="_blank">00:09:32.280</a></span> | <span class="t">There is much, much more to LLM applications than a chain, though, despite what the names of some other frameworks might indicate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=580" target="_blank">00:09:40.000</a></span> | <span class="t">The next pattern-anthropic called out is routing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=584" target="_blank">00:09:44.360</a></span> | <span class="t">In this one, you create several LLM-based tools to solve a problem in different ways or to solve different types of problems, and you give the LLM decision-making power to say, which of these tools should I call?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=596" target="_blank">00:09:56.700</a></span> | <span class="t">Which of these different LLM paths should I follow?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=600" target="_blank">00:10:00.840</a></span> | <span class="t">Again, not that complicated a concept and simple to build in LLM index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=605" target="_blank">00:10:05.360</a></span> | <span class="t">I'm going to spare you the code this time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=607" target="_blank">00:10:07.420</a></span> | <span class="t">You can do it using branches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=609" target="_blank">00:10:09.540</a></span> | <span class="t">You can just decide that you're going to split off into your own chain and do another series of work based on the original decision made by the LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=621" target="_blank">00:10:21.520</a></span> | <span class="t">The next pattern is parallelization, which is where things begin to get interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=625" target="_blank">00:10:25.380</a></span> | <span class="t">Anthropic defines this as running several LLMs in parallel and then aggregating their results, and they define parallelization as having two flavors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=634" target="_blank">00:10:34.040</a></span> | <span class="t">The first is sectioning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=636" target="_blank">00:10:36.260</a></span> | <span class="t">This is where you take the same input and you act on it in completely different ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=641" target="_blank">00:10:41.120</a></span> | <span class="t">The sort of canonical use case of this is guardrails.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=644" target="_blank">00:10:44.100</a></span> | <span class="t">So the user has a query or a piece of input that they want processed, and you use one of your tracks to actually process the data or to answer the query, and you use the second one of your tracks to query, is this an illegal request?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=657" target="_blank">00:10:57.980</a></span> | <span class="t">Is this against my rules?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=659" target="_blank">00:10:59.500</a></span> | <span class="t">Those two questions are related, but they can be answered in parallel, and you can use your guardrails to cut off the answer from your processing if it turns out to be illegal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=670" target="_blank">00:11:10.500</a></span> | <span class="t">You know, illegal or otherwise undesirable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=672" target="_blank">00:11:12.580</a></span> | <span class="t">The other flavor of parallelization is voting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=677" target="_blank">00:11:17.100</a></span> | <span class="t">This is where you take exactly the same query, and you give it to three different tracks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=684" target="_blank">00:11:24.320</a></span> | <span class="t">The tracks can be literally exactly the same LLM because they are non-deterministic, so it might not give the same answer every time, or you could give it to multiple different LLMs, which have different capabilities and different specialties, and then you take the answers, and you allow them, and you see if the tracks came to the same answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=700" target="_blank">00:11:40.180</a></span> | <span class="t">You can take a majority vote, you can take a unanimous vote, and what this does is it allows you to limit the amount of hallucination that is happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=710" target="_blank">00:11:50.960</a></span> | <span class="t">It's a great way of reducing hallucination because if, you know, three different LLMs come to the same conclusion, then it's probably not the LLM just making stuff up, because LLMs hallucinate, but they hallucinate in different ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=722" target="_blank">00:12:02.140</a></span> | <span class="t">So they seldom hallucinate to the same answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=724" target="_blank">00:12:04.520</a></span> | <span class="t">Whichever flavor you use, it's implemented the same way, using concurrency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=730" target="_blank">00:12:10.280</a></span> | <span class="t">We, LL, LL, LLM index workflows allow you to emit multiple events simultaneously, and then collect those events at the other end, so you can do, you can do work concurrently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=745" target="_blank">00:12:25.420</a></span> | <span class="t">And, uh, yeah, and it, I think the visualization here is particularly pretty.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=750" target="_blank">00:12:30.620</a></span> | <span class="t">Um, the next pattern is orchestrator workers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=754" target="_blank">00:12:34.440</a></span> | <span class="t">Uh, you can use an LLM to look at a complex task, like a multi-part question, uh, and split it into several simpler questions and ask each of those questions in parallel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=763" target="_blank">00:12:43.340</a></span> | <span class="t">So this is how deep research works basically, uh, it takes a very complicated question and it says, I'm going to, I'm going to look at all of the possible questions that could come from this deep question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=772" target="_blank">00:12:52.580</a></span> | <span class="t">I'm going to answer them all at the same time, and then I'm going to aggregate all of the answers that I've got and turn them into one single coherent answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=780" target="_blank">00:13:00.060</a></span> | <span class="t">This is a very powerful pattern that is, uh, doing a lot of good in the world right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=786" target="_blank">00:13:06.080</a></span> | <span class="t">Uh, this is also implemented using parallelization, um, and the final pattern that Anthropic called out is the evaluator optimizer, which is also called self-reflection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=796" target="_blank">00:13:16.680</a></span> | <span class="t">Uh, in this pattern, you use the LLM to decide whether or not the LLM has done a good job.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=801" target="_blank">00:13:21.960</a></span> | <span class="t">So, uh, you take your output, you feed it to an LLM and you say, here was the original question, or here was the original input and the goal that I had.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=809" target="_blank">00:13:29.900</a></span> | <span class="t">Have you actually reached the goal that I had, uh, and if not, you can get the LLM to generate feedback and send it back, uh, to the original first step and say, okay, you almost got the answer, but you hallucinated something or, uh, you missed a part of the question or, you know, something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=826" target="_blank">00:13:46.040</a></span> | <span class="t">Um, this is again, easy to do in LLM index and workflows, you just create a loop, uh, and you can send yourself back to this step one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=833" target="_blank">00:13:53.800</a></span> | <span class="t">Um, and the real power here is obviously combining all of these patterns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=839" target="_blank">00:13:59.240</a></span> | <span class="t">You can create arbitrarily complex workflows, uh, to handle any combination of circumstances, um, right near the beginning, uh, when I was defining an agent, I have 60 seconds left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=850" target="_blank">00:14:10.160</a></span> | <span class="t">So I'm just going to go through the syntax super quick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=852" target="_blank">00:14:12.260</a></span> | <span class="t">Uh, I said it was that agents are defined by their ability to use tools in LLM index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=858" target="_blank">00:14:18.260</a></span> | <span class="t">This is what a tool definition looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=859" target="_blank">00:14:19.760</a></span> | <span class="t">It is just a Python function that you have wrapped, uh, in a step wrapper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=863" target="_blank">00:14:23.200</a></span> | <span class="t">Um, and the way that you use your tool function is you just give it to an agent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=868" target="_blank">00:14:28.580</a></span> | <span class="t">And the agent will figure out that it is a tool function and start using it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=872" target="_blank">00:14:32.420</a></span> | <span class="t">Uh, this allows you to create workflows that are multi-agent systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=879" target="_blank">00:14:39.500</a></span> | <span class="t">I do not have time to, uh, explain how multi-agent systems work for, uh, this, but this is how you create a multi-agent system in, in LLM index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=887" target="_blank">00:14:47.360</a></span> | <span class="t">Uh, you create, uh, a function agent which gets, takes a system prompt, it takes an LLM, uh, and it takes, uh, a set of tools and you can feed, uh, an array of agents into a multi-agent system which then just sort of figures it out by itself, passing control back from one agent to another.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=905" target="_blank">00:15:05.140</a></span> | <span class="t">Uh, this is technically one line of code and we're pretty proud of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=908" target="_blank">00:15:08.540</a></span> | <span class="t">Uh, and that is about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=911" target="_blank">00:15:11.980</a></span> | <span class="t">Um, if you want a full agent workflow and workflows tutorial, this was the simplest possible one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=917" target="_blank">00:15:17.580</a></span> | <span class="t">The, it is available, uh, at that, this, this notebook will teach you how to build a deep research of your own.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=922" target="_blank">00:15:22.980</a></span> | <span class="t">Uh, and with that, I am pretty much out of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=925" target="_blank">00:15:25.980</a></span> | <span class="t">If I, if you have any questions, I will be outside in the hallway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=928" target="_blank">00:15:28.920</a></span> | <span class="t">Thank you very much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=929" target="_blank">00:15:29.920</a></span> | <span class="t">Thank you very much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=930" target="_blank">00:15:30.920</a></span> | <span class="t">Thank you very much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=930" target="_blank">00:15:30.920</a></span> | <span class="t">Thank you very much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=931" target="_blank">00:15:31.920</a></span> | <span class="t">Thank you very much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=72XxWkd8Jrk&t=931" target="_blank">00:15:31.920</a></span> | <span class="t">Thank you very much.</span></div></div></body></html>