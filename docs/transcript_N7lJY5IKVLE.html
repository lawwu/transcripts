<html><head><title>[Workshop] AI Engineering 201: Inference</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>[Workshop] AI Engineering 201: Inference</h2><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE" target="_blank"><img src="https://i.ytimg.com/vi_webp/N7lJY5IKVLE/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=0 target="_blank"">0:0</a> Intro & Overview<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=232 target="_blank"">3:52</a> What is Inference?<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=616 target="_blank"">10:16</a> Proprietary Models for Inference<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1282 target="_blank"">21:22</a> Open Models for Inference<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1841 target="_blank"">30:41</a> Will Open or Proprietary Models Win Long-Term?<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2179 target="_blank"">36:19</a> Q&A on Models<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2652 target="_blank"">44:12</a> Inference on End-User Devices<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3872 target="_blank"">64:32</a> Inference-as-a-Service Providers<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4200 target="_blank"">70:0</a> Cloud Inference and Serverless GPUs<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4666 target="_blank"">77:46</a> Rack-and-Stack for Inference<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4812 target="_blank"">80:12</a> Inference Arithmetic for GPUs<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5227 target="_blank"">87:7</a> TPUs and Other Custom Silicon for Inference<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5771 target="_blank"">96:11</a> Containerizing Inference and Inference Services<br><h3>Transcript</h3><div class='max-width'><p>. In the workshop today was everything that comes after once you've, you know, written your thin wrapper around the OpenAI API to make your ChatGPT-powered app. You've acquired $100 million in venture funding at a $4 billion valuation. And now you're like, what am I supposed to do next? So, we're going to split this into two parts so we can take kind of a break.</p><p>It's like a three-hour long workshop. It's a long time. So, in the first half, we're going to talk about inference. So, about what exactly this workload is that we have given over to the inference as a service provider. What's the shape of it? Why do we need these expensive accelerators?</p><p>What are the other options available? And we're going to spend half of our time on that because that's a place where we can actually talk, you know, in engineering terms about constraints, about service level objectives and service level agreements, the kinds of things that lead to robust systems. And then we'll spend 90 minutes on the rest of the OWL, the rest of what it takes to make a successful AI-powered app, just because there's so very little to say just yet about how to engineer these robustly.</p><p>But we'll talk about what the emerging consensus on what to do is so far and what tools are out there and available to start accelerating that process. So, yeah, so for inference, what are we talking about when we're doing inference workloads? How do we decide between using open and proprietary models to do that inference?</p><p>Where do those models live? Do they live on a device? Do they live in a cloud server? And then we'll spend some time talking about what it takes to self-serve inference. For the rest of the OWL, we'll talk about architectures and patterns. So what are the emerging kind of patterns for usage of large language models in AI applications?</p><p>And we'll talk about monitoring, evaluation, and observability, which is how we try and actually improve applications that fit those patterns over time. So, yeah, any high-level questions? My mic's off. The Zoom is not hooked up. No questions? Probably not. Still at a very high level. No? Just stretching. Great.</p><p>Yeah, maybe we should all stretch. Yeah, so who am I? Before we dive in, who am I and why don't you listen to me tell you about any of these things? So my name's Charles. I like to teach people about AI. I've been doing it for a while now.</p><p>I went to Berkeley, studied neural networks back in the 2010s, taught people about how to use them and Bayesian networks, RIP, for data science. And then I worked in developer relations and education for weights and biases, a previous generation MLOps tool, generation times being shorter than more doubling times, I guess, at this point.</p><p>And then for the last two years I've been working with full-stack deep learning, teaching not just things like the math of machine learning or how to do monitoring of an ML application, but how to build an application that uses ML from soup to nuts, from the GPUs up to the user experience.</p><p>All right. So now let's dive into the first half here on inference and specifically what is actually going on. Why does it cost so much to ping OpenAI? And so generative model inference, the kind of inference that's done by the generative AIs, is you see, when you use it via an API, you see a kind of data-to-data function where both sides of that data are human interpretable data.</p><p>So things like text, images, sounds, and outcome back, new generated text, image, and sounds. So for example, this is from the Palm E paper from Google, you might show a picture of a restaurant and the question, if a robot wanted to be useful here, what steps should it take?</p><p>And the output of this language modeling generative API would be clean the table, pick up trash, pick up chairs, wipe chairs, put chairs down, which could then be sent to a cleaning robot. And now you've got something, you know, it's a pretty useful system there. So this is what you see from the outside.</p><p>But as you start digging in a little bit, you'll start hearing about things like tokens and log probs and temperatures and to understand what's going on and weights and networks. And to understand what's going on, you have to realize that this has been broken down into kind of three pieces.</p><p>One part that goes from what humans understand, like text and images and sound, to what neural networks understand. And then one part that goes back and in between the operations of a neural network that operates on arrays and returns arrays. So tokenizers take in text that a human can read and turn it into an array of numbers, a tensor.</p><p>Just due to the kind of physics-y background of people in the ML community, what people would call arrays, I guess, in other software engineering communities, they get called tensors. They also, because they have derivatives attached to them a lot of the time, it's like there's a connection there, but really an array of numbers.</p><p>Those tensors get turned back to things that humans care about. And in between, neural networks map tensors to tensors. This step is the bottleneck. This step is the hard part. There's interesting stuff going on in the sampling process. There's interesting stuff going on in tokenization. There's cursed stuff going on in tokenization.</p><p>But this step is the bottleneck. This is where the vast majority of the engineering time is spent. This is where the vast majority of the compute time, the memory, are spent. And this is the interesting part. And this is the part where the engineering focus needs to be. Or this is the part that you farm out to somebody else.</p><p>So diving in, double clicking on that tensor to tensor arrow at the bottom, that a neural network is kind of a fancy term for a composition of a bunch of tensor to tensor functions. If you have a function that takes in an A and returns an A, you can just stack those one after another.</p><p>And that's what gives neural networks the kind of Lego-y flavor. You can grab bricks from one set and bricks from another set and attach them to each other. So this neural network, this is an ancient neural network, the Inception V3 model from Google that was state of the art in computer vision for a few months in the 2010s.</p><p>And each of those little blocks there takes in a tensor and returns a tensor. And so it starts with a tensor that looks like an image. So it's got a red, green, and blue channel. It comes out something that 8 by 8 by 2048 example there. That's probably somewhere in the middle of the network.</p><p>It's not the final classification output. Anyway, big block of numbers. They get passed into each other. And then each one of those is itself parametrized by a tensor. So it's not just like a map that you could kind of like write down by hand that's like add one to every entry or something like that.</p><p>It's defined by like another big pile of numbers, the weights and biases of the neural network. So this is the weights from the first layer of a computer vision network, the Alex net that kicked off this whole deep learning revolution. And it's these, these are, these are little, this is a visualization of a little block of red, green, blue, uh, uh, like tensor with three color channels in it.</p><p>So humans can actually look at it and interpret it unlike the rest of them and see what's going on, that it's got little things for detecting edges and textures and color differences and things. And so, uh, we want to run a big tensor to tensor map. And we are going to parameterize that with a big pile of tensors.</p><p>And before it's time to actually serve users, those tensors need to be generated by the training process. And back, uh, a couple of years ago, um, or even as recently as 18 months ago, this would be the part where we would stop, talk about gradient descent optimization, statistical learning theory, and, uh, you know, GPU acceleration and all the things that are needed to turn something into, to like get those numbers, to get things like that involving like grabbing, you know, hoovering up a bunch of information from the internet, uh, without consent, um, and then crystallizing it into those piles of numbers.</p><p>But nowadays you don't have to do that anymore. Specialized foundation modeling teams, uh, generate these weights and then they either put them behind proprietary service or they share them with everybody else to use. Um, and so we can skip past all of that stuff and jump into the actual application.</p><p>Um, uh, yeah. So any questions before we start talking about, you know, where those, you know, where those weights come from, uh, or, or rather what the various ways to get a hold of such a set of weights or to be able to use such a set of weights are?</p><p>Um, any questions at the level of what we're trying to do with inference? Probably pretty clear. That's, uh, maybe a reminder from the one-on-one stuff. All right. So now let's start diving a little bit deeper. So you want to run an AI. Uh, you're one of the first choices that you need to make as is, you know, pretty common with, um, with software is a build versus buy question.</p><p>Um, are you going to like, are you going to make this, uh, you know, out of existing open components? Or are you going to, uh, are you going to kick it off to a service? So, um, there's in the proprietary corner, there are a couple of players in the open corner.</p><p>There are a couple of players. Let's walk through what those are and, um, what the sort of dividing lines are and why to choose one or the other. So there's, uh, a number of proprietary modeling services, uh, like Anthropix, uh, from whom we just heard. Uh, and the good thing about these proprietary models is that they are the most capable models out there.</p><p>Uh, so this is from the LM SIS, uh, leaderboard, maybe the, the hugging face hosted version of that leaderboard. If you look at the, the top five models on that are all proprietary models and they're all from open AI or Anthropic. Um, so there are a couple of other players out there and in the future the, you know, they could release really high quality models.</p><p>Um, but for now, this is the, the, the state of things. So if you need the absolute highest level of intelligence in your application, uh, then you want to roll with one of these. Um, it's also often common to start with some of the most highly capable models and then kind of, uh, prove out your application there and then move to doing it.</p><p>Uh, like with a less capable model, that's cheaper, easier to run, uh, the sort of like rewriting it in rust or something. Uh, so the usual concern with using a proprietary service, there's a number of them, including things like vendor lock-in. Um, but one of the, one of the ones that comes up immediately is like, how much is it going to cost me to use a proprietary service?</p><p>And can't I save money by doing it myself? So the fact that the capabilities are higher with the proprietary models is one reason to say, well, you're not going to get exactly the same thing right now, um, using an open model. Um, but then the other kind of kicker here is that the proprietary models are priced very affordably.</p><p>Um, so this is something that's, that has been the case. Uh, this right quote here is from a blog post. I wrote back in like January, um, when the only open large model was GLM 130 B from Tsinghua. Um, and at that time, just like trying to get the thing running, uh, in a day, I got to within an order of magnitude of the cost of open AI, but you know, on the up, on the upper end.</p><p>Um, and then more for a more recent, uh, and more serious example, the folks at Honeycomb, uh, made a natural language sequel, uh, kind of transformation. Maybe not sequel, but query language, uh, transform it transformation, uh, AI product. And like their opinion was that open AI was very inexpensive to run for their task.</p><p>Uh, and so you'll find it seems that like nobody's attempting to, uh, like extract rents from monopoly pricing the way that you can get with some proprietary services. Uh, where they know that you have no choice, but to pay them $10 million for a MATLAB license every year or whatever.</p><p>Um, not to pick on it, sorry for a license for an array programming language. Um, uh, yeah, so, and the costs here, you know, a dollar for a million tokens for Claude instance, $10 for a million tokens for Claude 2, uh, relative to how much we're used to paying like humans for text.</p><p>You know, that's, that's like a pretty decent deal. Anthropic has been slightly more expensive than open AI without like a clear win on, um, capabilities. Uh, but that's the current state of things. They do have longer context windows, which is kind of nice. But, um, uh, but yeah, most people for the pricing reason and the capability reason, uh, choose open AI at this time.</p><p>Um, so the bad thing about proprietary models, besides like, you know, uh, hearing Richard Stallman, screaming in the back of your mind all the time, uh, is that proprietary model models cannot offer you full control by dint of their like very nature as proprietary models. Um, so Alex Gravely created a co-pilot part of the team that created co-pilot GitHub, um, was celebrating that GPT 3.5 turbo instruct a recent release from open AI had brought back log probabilities.</p><p>So you can see what, not just like what texts did the model generate, but what probabilities the model gave each token along the way. And back when the, back when it was just the GPT 3 API and the playground, you could see that information. And that's where a lot of the like early work on sort of like intuition about prompt engineering came from being able to see those numbers.</p><p>and there's all kinds of cool techniques that you can get up to if you have those log props. Um, and if you can manipulate those log props, uh, so yeah, you can read out confidence information. You can use it like during your development process to sort of like more gather more rich information about the system.</p><p>Um, and there's really, you know, you are, you're interacting with a probabilistic model and you can't see the model's probabilities. You're like fundamentally hamstrung. Um, and so that was mid September. And then like a couple of days ago, they turned that off. Um, and the reason why is because if you give somebody that amount of information, they can start to reverse engineer your model pretty quickly.</p><p>Um, and then you are stuck in the situation of IBM creating a personal computer and then a bunch of people with soldering iron irons and oscilloscopes turn around and make clones of your machine like within a year. Uh, so proprietary models are like fundamentally disincentivized from giving you that level of control, despite the fact that it's very critical for, um, for like actually effectively operating the system.</p><p>So there needs to be that capabilities edge, um, that like raw capabilities edge in order to, uh, make up for this fact. Um, then lastly, maybe some people work in an enterprise, uh, in this room. Don't, you don't have to out yourself, but maybe some. Um, and if you're operating in this sort of situation, you can't just ship a ping to an external API out there, uh, into your business.</p><p>If people want to know about governance, people want to know about GDPR compliance. Um, and the, one of the nice things about opening eyes offering probably true about anthropic by this point and definitely true about Google AI soon is there's a nice white glove enterprise tier, uh, uh, around this.</p><p>That gives you like, right underneath, like, you know, launch an artificial intelligence application and achieve your childhood sci-fi dreams is built in security and compliance. Um, we will spend $20 billion on cybersecurity so that you don't have to. Um, so this, like if you are in a situation where you need to like, you know, assuage concerns about data privacy.</p><p>Um, this sort of enterprise tier, um, you know, sock to compliance, et cetera, is, uh, can be really critical for making your life easier. Uh, any questions about proprietary models and such? How much more expensive is it to run to be the API versus the cloud, right? Um, with, uh, with, uh, with Azure?</p><p>Yeah. I want to say the Azure was cheaper at the start. Um, but maybe, uh, I haven't had any reason to use it. Um, so I'm, I'm not sure. And just about questions, like, are there like any, um, trade-offs in general with regards to performance? Um, for a minute consuming the API or-- Ah, so for, you mean for the enterprise tier versus, yeah.</p><p>So the, uh, the enterprise tier also offers like an actual SLA, which the OpenAI API like doesn't. Um, and is like geared, that's, that's, you know, that's maybe another very critical feature besides security and compliance. Like they will promise that you will get a response and not like a 500.</p><p>Um, and they have much more generous, uh, rate limits and things like that. And I think they also offer like a little bit more control over stuff. So you might be able to do some fine tuning that you can't do via the generic OpenAI API. Oh, the fine tuning for, for Azure, uh, limited to just like three models.</p><p>Okay. Yeah. But there's also limits on the public API for fine tuning, right? Yeah. Um, I personally did not find it particularly useful to like fine tuning API, both like hard to use and not clear benefits. Um, I think the example from gradient did show that like, if you want to achieve a style and you don't want to spend money on.</p><p>Like context to set up that style, uh, then maybe you can win with fine tunes, but it's like, it's not really a successful way to inject new information. So you aren't saving on the tokens that you would retrieve. Um, and you have to pay more to inference a fine tuned model.</p><p>And that just goes down to the fundamental, like you're asking them to do more work for you. Um, and they can amortize that cost over fewer users. And so it's just always going to be more expensive. Um, and so, uh, yeah, so that limits the utility of those fine tuning APIs.</p><p>Yeah. Please definitely like ask questions, um, uh, customize to what people are interested in. And also if I don't know about something like, please do, uh, interrupt. Great. Google has vertex. Yeah. Yeah. So, uh, well, so vertex is a little bit different from this. I think, um, I think of vertex, which I was going to talk about later as sort of like a, something that I can launch my own services into as opposed to like, uh, oh, here's a private version of the Palm Bison API, but maybe, maybe is that part of vertex?</p><p>Okay. Yeah. Great. Yeah. Um, so yeah, that's, um, so they're already available for Google AI. Does anybody know of Anthropics AWS like, uh, like enterprise offering is up yet? Yes. Yes. Yes. Great. Yeah. I like refuse to make slides about this stuff more than like 48 hours in advance.</p><p>And I still find myself getting cut. Like, yeah. Yeah. So the question was, uh, between proprietary and open models, which ones are you betting on? Um, gambling is illegal in the state of California. And so, um, uh, we'll get, we'll get there. So, um, let's talk about the open models and then we can answer or, uh, open up that discussion.</p><p>Um, so open models are less capable, but catching up. Um, and their hackability is very powerful. Um, so going back to that leaderboard that I showed, if you look at the next five out of 10, um, four of them are Lama two model, uh, actually. So this column here to be clear is the license.</p><p>Uh, so the top five are proprietary. Like there's no, uh, there is no license for those weights. Um, for the bottom five, they have, uh, special licenses. Um, so these are fine tunes of meta's Lama model series. And this model series has like kind of captured mind share in the free and open source software world.</p><p>So a lot of the people who are like hacking independently, um, and, uh, you know, making public get commits, um, and, you know, funded by the Linux foundation and things like that. These people are working in the main on, uh, adjustments to, or improvements to the Lama model series.</p><p>Um, and this is really critical because the secret to like the success of open source software in general is the ability to do this kind of like highly parallelized development where lots and lots of people are adding tiny little features. And like, you know, going out into the last mile and adding those tiny little things that they need, um, and sort of like making use of all that, uh, work by others.</p><p>Um, so in so far as you're able to do that, you are able to provide useful open source software that can compete with software that's made by, you know, highly remunerated teams, um, you know, in, in Northern California. Um, so the important question, uh, is this actually, uh, open source.</p><p>So you'll notice these licenses here do not have friendly beloved names like LGPL or MIT or Apache. They are, they have a special unique name. Um, and that's because meta's license for the Lama two weights, the Lama one weights for at least under a research only license and were only sent to certain people.</p><p>And then we're immediately like torrented and the license was violated. So they gave up on that, uh, on like fully controlling it, but they did say you cannot use the, uh, data or output to improve other large language models. You can only use it to improve Lama models, um, and also release under the same license, which is pretty typical with open source, um, which, uh, is partly an attempt to sort of capture this.</p><p>Like as people are doing parallel development, they should only be contributing to the development of this, um, this branch. Um, and then also if your products monthly active users in June of 2023 with 700 million users or above, um, you're not allowed to use it. Or sorry, you have to pay for a special license.</p><p>Um, and that, uh, so apologies to anybody, you know, who's, um, you know, if you're running an app with more than 700 million users, um, uh, you'll have to go elsewhere, I guess. But the, um, the key thing is that these are violations of the, like, sort of agreed terms of what makes something an open source license according to the open source initiative, who, you know, who, you know, has some, uh, claim to controlling how that term is used.</p><p>I didn't, they, I don't think they ever ended up getting a trademark, um, but they, uh, you know, they are, got the community aligned around a small number of licenses that, and around a key set of principles that include, like, you can't tell, you can't say who's allowed to use this software, for example, which is included in the Lama license.</p><p>So there's, like, uh, uh, they're, they have opened up a multi-stakeholder process to define open source AI. This is occurring at a time in which, like, the meaning of open source is also being contested in, sort of, like, software as a service. So things are a little, things are a little tense there.</p><p>Um, but hopefully we'll come to, uh, an agreement as a community on what that means. Um, so this is a fast-moving space still, so just because you get Mindshare early on, if, like, things change rapidly, that doesn't mean, like, Lama's locked in forever. Um, so Mistral, for example, dropped a model, like, two weeks ago, um, that at only 7 billion parameters was outperforming, um, like, larger models in the 13 to 30 billion parameter range.</p><p>And those models were outperforming the previous models, um, at their size. So there's, like, except insofar as those things, like, continue to get updated, um, you know, they, uh, yeah, they can be outcompeted. Um, the other thing to watch out for is that there are a lot of people who are very excited about, like, taking on the death star of open AI or whatever and, um, get very excited about these open models.</p><p>There's also some political things about the politics of how ChatGPT likes to respond to questions versus the politics of how people who meet other people in discords like to respond to questions. Um, and that can, that sort of, like, enthusiasm can lead to, like, pretty big errors. So, for example, there was a lot of excitement about these models that were, take, take an open llama model, uh, the, the ways from that, and then grab, like, 10,000 requests from the open AI API or scrape, like, r slash ChatGPT or whatever, and then just fine tune.</p><p>Like, now there's a data set, use it to fine tune the model. And those are the ones that were, like, up there on the arena, uh, uh, on that, uh, like, ELO ranking, um, from the leaderboard that I showed before. Um, and the, there was a claim that these, uh, had, like, 90% of ChatGPT's quality.</p><p>There are only 7 billion parameters. Like, you know, people were, like, very enthusiastic about this. Back in April when I was talking about this, a lot of people were, like, why are you even talking about open AI anymore? We, like, you know, Vicuña's done it, and, like, yeah, uh, so, um, this is a fake screenshot, uh, that's, uh, from a paper, or that I modified from, from a paper about, uh, about this, this topic of how well these models work.</p><p>Um, so, this is one output from a language model, anonymized, um, on answer, uh, how does actor critic improve over reinforce? Um, so, this is an algorithm from reinforcement learning. Um, so, one language model says, uh, actor critic algorithms are an extension of reinforce that combine both policy-based and value-based methods.</p><p>Um, it's got a critic network, it's got advantage estimation, it's got function approximation. So, that's one answer, that's A. Answer B, uh, actor critic algorithms are a type of reinforcement learning algorithm that improves the reinforcement algorithm by combining actor, policy, and critic value components. Actor-critic algorithms use a single critic, reinforce has a separate critic for each action.</p><p>Actor-critic algorithms learn a policy and actor simultaneously, but reinforce learns them separately. Um, so, you know, those might seem fairly similar. So, does anybody have a strong preference for, uh, answer A here? Anybody have a weak preference for answer A over answer B here? Some people raising, like, a, like, a soft hand?</p><p>Maybe, yeah? Um, anybody have a strong preference for answer B? Maybe one, maybe two, and a weak preference for answer B? This one's got, like, they both got these nice numbered lists, you know, which looks very authoritative. It reminds me of a Medium article, which is likely to be true, of course.</p><p>Um, uh, so, the, uh, so, uh, answer B comes from, uh, I want to say this was GPT-4, yeah. Um, answer B comes from GPT-4 and, oh, wait, wait, sorry, answer A comes from GPT-4 and has the advantage of being correct. Um, answer B comes from one of the, uh, fine-tuned models, um, and is, like, gibberish, basically.</p><p>Um, and so, this just, you know, like, just having humans rate the outputs of language models in the way that a lot of those, uh, leaderboards were constructed, um, did not, like, it didn't have any grounding in the actual utility of the answers. It was just a lot of people going, like, looks good to me, like, nice, yes, merge, um, and, um, without, like, knowing whether it was actually right or not.</p><p>Um, and so, uh, there's a nice paper from some folks at, uh, Berkeley about, um, sort of walking through, like, what's going on. Basically, the models are picking up style from a fine tune, which is things like that delightful little split into bullet points and, like, you know, like, a very authoritative and friendly educational style.</p><p>Um, but not, like, actual knowledge, not, like, reasoning capabilities. And, like, a lot of people in the open modeling communities, like, sort of missed this or, like, willfully ignored it. Um, some of the sharpest people were definitely up on this. Like, the Guanico paper, for example, mentions that there's, uh, some issues with evaluation came out before this paper.</p><p>Um, but definitely a lot of people missed it. Um, so the, the immediate question that comes up is, like, between these open models and these, uh, proprietary models, who's going to win long-term? Like, who should I bet on? Um, and in some ways, I think that's a bit of a misguided question.</p><p>Um, so consider operating systems. Uh, like, the first operating system, roughly, was System 360 from IBM on mainframes, extremely closed. Um, in the '80s, there was a rash of operating systems. Most of them closed. The original Xerox Pilot on the Xerox Star, uh, was extremely closed. DOS and Win-DOS was closed.</p><p>Uh, Mac OS at the time was, like, completely closed. Um, and there were Unix operating systems that were kind of, like, mixed. Um, and then over time, the, like, closed versions of Unix lost out to more, like, friendly licensed ones, and in particular to GNU Linux. Um, and there's been a bit of a trend towards open, uh, open operating systems kind of taking more mind and market share over time.</p><p>Like, data centers have more Linux in them now than they did in 2005 and then than they did in 1995. But, uh, from what I can tell, it's still, like, 70% plus Windows, um, for, uh, for operating web servers. In mobile phones, we also have an open operating system and a closed operating system.</p><p>And these things have been able to co-exist and serve different needs for different organizations throughout, like, the history of operating systems. Um, and the same is true of databases. Uh, so, back in the '70s and '80s, it was Oracle and IBM's DBT2, um, which is still around, I found out.</p><p>Um, like, you live too long in, in the, in San Francisco and you forget that there are people who use IBM DBT2. Um, in the '90s, there was some, like, consolidation around more open implementations of, of SQL. Uh, and in the 2000s to 2010s, there was the NoSQL movement, but that was still, like, mostly open source databases.</p><p>Uh, so there's been a lot of, like, movement in the direction of open databases, um, with streaming databases, we have both proprietary and open options. If you look at the top 10, uh, databases as ranked by dbengines.com, which is, you could quibble with the ranking thing, but the, the key point is that there are, like, it's like half and half split between, um, between proprietary databases and, um, and open source databases.</p><p>Uh, and that has been, like, relatively stable over time with, like, a soft, maybe a soft trend in the direction of open, uh, databases. Uh, so with these, like, very, like, language models, foundation models are this very, like, low-level, uh, component of a software stack. More like an, like an operating system or a database, I think, than, like, um, you know, than a SaaS app or a, or a UI.</p><p>Uh, and because of that, they're likely to be subject to some of these same forces that say there's some people who want to work one way, there's some people who want it to work in another, and for some of them, that openness, that hackability is going to be critical for others.</p><p>The, like, reliability, the existence of a white-gloved enterprise version is going to be really critical, um, and those will allow these two things to coexist. Um, and, uh, yeah, and the CEO of HuggingFace liked my tweet when I said that, so it's probably true. Um, but I think a lot of people are, like, well, no, but, like, who's, like, who's gonna win?</p><p>Like, who should I bet on? Um, and I think the closest thing to an answer that I have is that if capabilities requirements saturate, if people no longer want the absolute smartest model out there, they just want a model smart enough for X, Y, Z, then open models will probably catch up and then, like, start to dominate.</p><p>Um, the thing that keeps open models behind, proprietary models, is the extreme expense of maintaining a large resource team and, like, you know, continually constructing new data centers at an increased scale, um, um, to the tune of, like, $500 million in order to hit that next capability level before everybody else.</p><p>But, you know, uh, uh, at a certain point, processors got fast enough that people were not, like, clamoring for the next upgrade as soon as possible. Um, and at that point, we're starting to see, like, a little bit more opening up in the, sort of, like, in the chip space with, like, RISC-V.</p><p>Um, and so, like, at, like, with, in any number of other technological domains, you've seen that when requirements start to saturate, um, then, like, open, uh, like, open versions can catch up. Um, if they are unbounded and it's like, you know, uh, what's a good example? What's a good example?</p><p>Like, RAM? Like, nobody has enough RAM. Everybody wants more RAM. I don't think there are any, like, open, like, attempts to make, like, an open RAM architecture or something. Um, and that's because, and one reason why I think is capabilities requirements there are, remain unbounded. Um, and so, um, if that's the case for, uh, cognition and AI models, then proprietary models should be able to maintain that edge in, in capabilities, which would sort of tilt the balance in favor.</p><p>More people would say, oh, no, I need this, this proprietary thing. Um, so it's the closest to an answer that I have. Um, yeah, any questions on that front before we dive into, uh, um, where we actually run these models? Yeah. I was curious, uh, what's the language support for these language models?</p><p>Like, can anyone, you know, use, like, a language other than English with these models? Yeah, um, so the question was what kind of language support do these models have? Um, and because it's only an API call away, you can, of course, use Python or Node. code or whatever you want.</p><p>Uh, no. So the question was about, like, these are language modeling, like, machines. What languages do they model? Um, and the basic answer is that the more text in that language that is available on the open internet, the better the language models will be on, on that language. So they are, like, I want to say GBD-4 is smarter in, maybe smarter in Malayalam than it is in Mandarin.</p><p>Um, I forget. There's, like, some interesting inversions of, like, number of people who speak the language versus how, uh, how intelligent the language models are. Um, so I think a lot of them release benchmarks that say, like, how multilingual is this language model and for which languages. Um, there is, you run into the fundamental token constraint of, like, you need, uh, you need existing, like, you need examples of that language that you can get a hold of in order to train the model in them.</p><p>Um, and there just are more English tokens. Um, but for a given capacity, you can probably achieve, like, higher quality in a specific model by looking for, um, by looking for a model trained in that language. So there's definitely some, like, good old nationalist European endeavors to make, like, a French-language model that insults you if you ask it for stuff in English.</p><p>Um, which it, of course, picks up just from reading French. Um. Um, but yeah, but the, but the core models, like, they support English really well. The instruction fine-tuning in the ROHF is actually mostly applied to them in English since the annotators who, uh, enforce that policy, uh, through their examples are mostly writing in English.</p><p>Um, so fun fact, you can get ChatGPT and Claude probably to tell you how to build a bomb if you ask in the right low resource level. Um, uh, just fun facts about language models. Um, yeah, so that, that does, that is a problem, and it does sort of, like, uh, it has a multiplying effect on the English languages, kind of, like, cultural hegemony, um, which is a bit unfortunate.</p><p>Yeah. So for the, for the languages that are less represented, is, um, is reading capabilities lower, or, you know, understanding lower, and also, are there arbitrage opportunities in translating first, uh, in the process of translating first, uh, and then... Yeah, I'm unaware of any, like, you know, any benchmarking work on this.</p><p>My gut tells me that translating to English first, doing chain of thought, and then translating back to the original language would work better. Um, you, you kind of, like, wondering whether the lost in translation effect is bigger than the, like, boost of chain of thought in English. A lot of the, like, circuits in language models are very token specific.</p><p>Um, and then, yeah. So, it's like the, like, just one example. If you ask it who Tom Cruise's mother is, then it answers better than if you ask it that woman's name's son. I, I don't know her, her name at all, um, so I can't really do this example effectively.</p><p>Um, JATGPT wins again. Um, but the, uh, there, so that's, like, an example of a very, of a token specific circuit. It's, like, related to Tom Cruise. Uh, and so, you can see, like, it's not reasoning the way that a person would, or that, or that you would guess from, like, you know, how, how you would think about a knowledge graph or something like that.</p><p>Um, and so, that's where you get these unintuitive things. Um, but, yeah. So, I saw you, um, used to or maybe still do deep learning. Oh, yeah. And I was wondering, like, one of the pieces of the whole event is, okay, there is these reasoning as a service APIs now, right, where you can do a lot more things without having your own experience.</p><p>So, if you are aiming for the typical AI engineer, you know, to build this. Does it still make sense to learn some amount of those and learning some amount of things, you know, like, is it, like, what's the video piece of what post? Is it, like, actually being, like, who can?</p><p>Is it, like, actually being made for, like, what would you like to do? Yeah, that's a great question. Um, for individuals, I think it's a matter of your personal interest in understanding the modeling. Like, I guess the analogy I would immediately jump to is, as an individual developer, you can get away with not knowing anything about databases.</p><p>Like, I have done that. I couldn't write a B-tree right now. I don't want to ever learn how to do that, like, and to think about page sizes and, yeah, it makes me ill to think about that. And, whereas I get excited if I wake up in the morning and I can think about Bayesian inference in language models.</p><p>And so, as an individual, I think you can, like, kind of be guided by your, like, what you find most exciting. As a team and as an organization, though, if you have nobody who understands databases in your organization, you're probably going to be in trouble. Um, just, like, it ends up, like, most applications require, like, pretty decent knowledge of databases and when they go down or when they need to be configured.</p><p>Even if you are using Redshift or, you know, you're using some managed service, being able to, like, understand some stuff about them is actually critical for debugging and being able to know when you need to switch managed services or, like, yeah, or how to reconfigure them. So, I think, like, the direction that we're going to go is to evolve there.</p><p>It's a question of whether you want to be a site reliability engineer focused on LLM reliability or, you know, a modeling engineer or whether you want to be, like, more at the, like, application layer. Yeah. So, here's an interesting one. What's your personal opinion on this? There's a handful of companies that have gotten recent funding to build, like, vertical-oriented commercial models in finance, healthcare, et cetera.</p><p>Yeah. Yeah. So, the question was, what about these models that are foundational but, like, less broad? So, it's, like, a foundational model for law, a foundational model for healthcare. For healthcare. Yeah. I -- my experience has been that if you bet that some capability is not going to be available in a language model or in a foundation model, like, you will get -- you will lose that bet.</p><p>So, just as an example, in the deep learning boot camp, we spent a long time trying to make an optical character recognition system. And it's, like, you know, it's the, like, pinnacle of the class where you can finally, like, deploy a web service that does optical character recognition. And that's, like, an accidental side feature of GPT-4v, and it's, like, better at it than the thing that we built.</p><p>Um, and a lot of ML teams have experienced something like that. Um, so, I worry -- I would worry if -- if somebody were, like, offering me that as a job opportunity, for example, I would worry that it's going to get, like, scooped on either side by a hyper-specific model that's, like, 10 times more efficient and isn't, like, a generic healthcare model, but is, like, a, um, uh, ultrasound for the heart model, the one I worked on before.</p><p>Um, yeah, or just send it to the chat GPT API, or to the GPT API. Great questions. Um, so, uh, maybe another reason to think that there might be, like, a little more alpha in -- in actually learning more about the models is, um, inference doesn't have to be executed over a network.</p><p>It doesn't have to be executed, like, in some central server. There are lots of reasons why you might want to execute your, uh, your inference on an end-user device. Um, so we'll talk about the different types of end-user devices and the different constraints that they put on inference and the implications, um, like engineering and strategic, and then also uh, talk about what the options are for doing things over a network.</p><p>Um, so running stuff for end-users is, like, uh, like where the -- sorry, the end-user actually executes it themselves is not quite there yet, but it's, like, uh, it's getting there and maybe a little bit faster than I personally expected. like, I've run llama 2, uh, 13b on this very laptop, um, without it catching on fire.</p><p>Um, so there's, uh, there's some hope, uh, that there will -- that that will continue to get better. Um, and so this, uh, this is critical for latency-sensitive applications. So, like, being able to actually execute the inference at the same place that the user -- at the same place where the user is.</p><p>Um, and the reason why it goes back to this, like, this famous set of numbers every engineer should know from, uh, Peter Norvig and Jeff Dean at Google, um, which is that the time it takes to send a packet -- just one packet, so probably -- this probably isn't even a whole HTTP request.</p><p>I'd have to check again. But let's just say you send information back and forth from, like, here in California to Europe and back. It's 150 milliseconds. Um, and there's, like, a number of kind of made-up numbers in the UX world about, like, how fast you need to be for something to feel interactive.</p><p>Um, so one of them going back to, like, the '70s or '80s is the Doherty threshold, which says the user and the computer can interact with each other in under 400 milliseconds. Then the, like, human won't feel like they're waiting on the computer. And as you're programming things, you won't, like, end up blocked on human input.</p><p>Um, but you'll -- and you'll still have plenty of time for doing stuff in -- in side threads and things like that. Uh, so if you were -- like, if you have to, like, do a network call every single time, you're using up, like, a third of your budget just on, like, waiting for information -- information to come back.</p><p>And now you're going to spend a ton of engineering effort on, like, trying to find ways -- things that you can do asynchronously during that, like, that network call. And, like, you can -- you can work around it, but it is punishing. Um, and that's -- like, there are even tighter, like, reaction time things.</p><p>Like, if you have a self-driving car, um, you can't wait 150 milliseconds, uh, to find out that you need to brake. Um, so, uh, yeah. So, and the nice thing about this, uh, the other benefit to it, besides it being necessary in some places, is that if end users run the computation, then you don't need to pay for it.</p><p>Um, so your inferencing costs can be zero dollars, which would be -- which would be great. Um, so the cost that you pay, um, is control. So, you have less control of the execution environment. Um, your ability to do telemetry and see what is going on is limited. Uh, people don't like it when you, like, carefully observe their activity using software on their machine, but if they -- you put the same software at a URL, you can spy on them as much as you want, and they don't get mad.</p><p>Um, so you lose out on telemetry. You, uh, have to worry about compatibility with different execution environments, and you have to actually support past versions, unlike, uh, if you're running it as a service. Um, or rather, if you, like, you know, control the execution environment. Um, so the things that are unlocked by this are some of the best applications here.</p><p>Uh, like, some of the most exciting ones, especially to me. So, uh, use on smart -- use in smartphones, use in robots, use in wearables. Um, so Google, just in the past couple days, announced that the Pixel Pro 8, um, is going to have, uh, large language models directly on device.</p><p>Um, they mostly showed off stuff that looked, like, kind of, like, summarization and some, like, light image editing, so not, like, full-on, like, you know, like, "Hey Siri, why did the Ottoman Empire fall?" Like, I don't know what you talk about with ChatGPT. Um, but, uh, like, it's not quite that level, but it's a move in that direction, and a trend we can expect to kind of continue getting that inference onto the device.</p><p>Um, and, uh, there was also a recent hardware hack, um, on, like, using, you know, getting this inference on mobile robotics platforms. Um, and so there's -- there was a ton of cool applications there, like, um, yeah, some stuff with, like, three -- like, point cloud rendering from -- for your -- for inside your house, like a Roomba you can control with your voice.</p><p>Very cool stuff. Um, the -- the constraints that appear here, uh, that you'll have to engineer around are, like, very tight hardware constraints. So, um, there -- there's memory limits. Both disk and, like, VRAM and RAM are, like -- are extremely tight. And, like, current language models, you can always trade more -- up to points where you're spending, like, $100,000 on a machine, you can trade more money for smarter models.</p><p>Um, and phones are down at, like, gigabytes, uh, low gigabytes of RAM, uh, like -- yeah. Um, was running a language model on a single board computer, and that had, like, two gigabytes of shared RAM between the CPU and GPU. Not a lot of space. Um, and the, like, real, uh, deep limit is power.</p><p>Um, it's, uh -- or the -- sorry, there's a limit on power, which is, like, an A100, uh, which you might use for inference, draws 300 watts of power, and something like the single board computer, or using the Jetson Nano, that's 10 watts of power. So, a factor of 30.</p><p>Not gonna make that up anytime soon. Um, and underneath both of these is the problem of heat dissipation. Um, there's -- that's, like, a really, like, tough thing to deal with when you are in these, like, small environments, um, and, like, prevents them from just being, like, oh, I'll just, like, make a chip where you can actually move, like, nine petabytes a second.</p><p>Um, like, across an inch. And it's, like, uh, like, you just do some, like, back-of-the-envelope math, and it's, like, that's gonna, like, egress so much heat the thing's gonna catch on fire. Um, so, um, yeah. This is -- we're talking about some hardcore engineering stuff here. Um, all right, so the, like, mobile environments, uh, maybe, like, further out in the future to get, uh, large capabilities onto them, but, um, not impossible.</p><p>What about, um, what about other consumer hardware? Desktops, um, which are a place where you could have video games with actual artificial intelligence in them. Uh, operating system-level assistants, native apps with these, like, kinds of features that we're starting to see in, um, in browser apps. Uh, so you still run -- like, you run into even more heterogeneous hardware, and that's gonna give you different constraints, depending on the system that you're on, and that is gonna require, like, really heterogeneous software to meet those constraints.</p><p>Like, you probably can't assume that everybody has an NVIDIA 30 series or later GPU, even though it would make your life a lot easier. And you probably can't assume that you can use up all the RAM on that, uh, uh, uh, you know, on that chip, even if it would make your life easier.</p><p>Um, I think the long-term, we might be able to expect ecosystems to adjust around the requirements of these workloads, a bit, so, like, kind of, uh, like, make it, uh, like, make cleaner interfaces for using these things, so you don't have to write 15 different versions, um, or write a make file that looks like llama.cpps.</p><p>Don't look at it. Um, uh, very scary. Uh, there's kind of a sweet spot, actually, in what little, like, next-generation video game consoles, because you have total authority to just use up as much of the system as you want. People pay lots of money for them. They often build custom silicon based on what developers want.</p><p>So that could be, if you're thinking about what you want to be doing in, like, five years, seven years in this field, consider that as a possibility. Lots of people would love to have, um, a real, like, human-like intelligence in the, um, uh, in the things they're shooting in their first-person shooter, you know?</p><p>Like, that, I think that would make a lot of money. Yeah. Uh, when it comes to building, especially for mobile hardware, with those constraints you're talking about. Yeah. Can you say a little bit about quantization? Yeah, so the question was, for mobile hardware, like, what are solutions and specifically quantization?</p><p>So, um, when one of the key constraints is memory, like, just trying to make the size of the model smaller and the size of the computation smaller is helpful. So, the, like, people are pushing to try and take the parameters of language models down from being two bytes to one byte to half a byte to, like, a single bit.</p><p>Um, and I think people are kind of stalling out at the, like, half byte level. Um, and often to actually recognize those gains, you need to, like, write a assembler and stuff. It's, like, it can get pretty gnarly. Um, so that's often only, like, highly resourced teams working for a long time that they can actually see those benefits.</p><p>Um, the other thing that people talk about a lot, uh, for, like, making models work on smaller devices is sparsity. Um, and so sparsity means, like, oh, there's this giant weight matrix. Maybe most of them are close to zero. And maybe we can just, like, get rid of those.</p><p>Like, if we were gonna go to one bit, there's zero or one, like, why not? There's zeros. And then zero is, like, a very easy number to work with. Like, you, the number that comes out is multiply at zero, add, you just keep the number. So it's, you don't need, like, a full logic circuit to handle it.</p><p>Um, so there are some things that make use of sparsity. The problem is that the type of sparsity that neural networks need is called unstructured sparsity. You have just had zeros kind of, like, scattered around your matrix multiply. And all the, like, existing, easy to use, has, like, you know, Python API stuff is, um, uh, is in structured sparsity.</p><p>And so you get gains there. And it's, like, you might need, yeah, a lot, like, yeah, hand-tuned CUDA kernels or, yeah, to, like, actually take use, make use of unstructured sparsity. So that's something, you know, if there's a ton of pressure, we could see those developments in five years or so.</p><p>But, um, we haven't seen, people have been thinking about that for almost, for, like, seven years. And it's, like, not made a ton of progress. But, yeah, helps definitely has made it easier. And, like, Google has been able to fit decent amount of language modeling capabilities on a, uh, on a mobile device.</p><p>Um, using distillation and quantization and probably more secrets that I won't share. I had a dumb question. What's the size of these models in terms of memory like it could be? Yeah. Does it only have the whole model in memory? Mm-hmm. Yeah. Um, so the, if somebody tells you a number, like, 50B, you know, like, Lama, Lama, Anthropic 52B, Lama 70B, that's billions of parameters.</p><p>And then the question is, like, how, what, what, how big is a parameter? Like, how many bytes? And the, like, they're trained or where they, the way, the way they come out of the factory is two bytes per parameter. So take the number that somebody gives you, multiply it by two, and then the B is giga.</p><p>So, like, a small Lama model is, like, 14 gigabytes. Seven B times two, 14 gigabytes. So not gonna fit that in phone RAM. Uh, and that does make, yeah, doing this a lot harder. Um, you can do things, you can, like, try and do stuff with paging, like, put stuff on the disk, bring it, bring it into RAM, then, like, execute with it.</p><p>Um, but that, like, slows things down a ton. Um, so in general. Yeah? Hmm? So, people used to train in float 32 and release models in float 32. Maybe, I thought the Lama models were released in float 16. No? Yeah, a lot of, like, a lot of people train in this new, like, Google Brain float, uh, thing.</p><p>And then they, they're doing that because they want to be able to use float 16. And so have two bytes per parameter. But, like, definitely the, so the, like, the default before that was four bytes per, per parameter. And before that, when people were doing scientific computing with graphics, with graphics cards, like, um, people at the national labs, the default was, like, four bytes per parameter.</p><p>Like, um, or, sorry, eight bytes, 64 bits. Um, because they really needed that, like, high precision and high range. Um, and, yeah, but now the trend has been to push them lower. And many model releases are now, like, already two bytes, 16 bits. Yeah. Um, and now, Georgi Gergenov is, like, immediately converting them down to four bits, uh, and three bits.</p><p>Um, which is, like, wild. Like, what does that even mean? Um, like, a non-power of two. It's scary. Unsettling. Um, okay. So, uh, with -- there's another execution target that gets you a lot of the benefits of desktops, which is, like, you have a beefy machine to run on, and it's not yours, so you don't have to pay for it.</p><p>Um, but then you get a more homogeneous execution environment, which is the browser. Um, so this is not, like, a web app where they, like, talk to a model running in a service, but, like, there is a model inside of the browser that runs inside the browser's, like, runtime.</p><p>Uh, and that, like, the homogenization environments would be very huge. Um, right now, this is kind of, like, awaiting some technical improvements in the world of browsers. So, there is a target in WebAssembly that you could compile your programs down to, um, and in principle run them. Uh, the support for, uh, GPUs is very gross.</p><p>Um, there is, uh, a working draft from the WWW Consortium for WebGPU. For WebGPU, which would make it cleaner and easier to use. Um, so that would help the, like, ecosystem, the, like, stack and ecosystem around this for other kinds of web applications is developing. Will, like, maybe lead developments in using this for, uh, delivering inference.</p><p>Um, you have a new constraint distinct from the other ones, which is you now, at least as it stands right now, you would need to deliver weights over the network. And so now it's, like, you're, you're, you have kind of the model size constraints that you might associate with mobile hardware.</p><p>Um, but only during the, like, first load. Um, so, um, there are probably clever ways to get around that, like, progressively delivering them. Um, or, uh, like, browser, uh, companies sort of agreeing to incorporate some foundation models into the actual browser runtime itself. Um, so, like, inside of, uh, like, uh, like, v9, uh, an update to v8 with a foundation model already built into the runtime.</p><p>That would make your life a lot easier. Um, so, this, like, would, uh, yeah, browser assistance, maybe sort of, like, general, uh, like, executing apps inside of a browser that feel more like native apps. Um, that's the potential applications here. But, um, still a little, um, at the edge.</p><p>At the edge. That was a pun, I guess. At the edge. Um, okay. So, because, oh yeah, question. How many gigabytes is a small and a large model right now? A small and a large model. So when I hear small, large language model, first I cringe internally. And then I accept GPS system, ATM machine, whatever.</p><p>Um, so a small, large language model in my mind is something that has, like, kind of limited ability to, like, speak and interact with. And that's what you see at, like, the, like, 13 billion to 30 billion parameter range. Like, the medium size is, like, the 70 billion parameter range, which is, like, the largest open models.</p><p>And then, like, a true large language model, the ones that, like, make people scared about losing their jobs, are generally, like, mixtures of 70 to 100 billion parameter models. Or maybe they are themselves 200 billion, 280 billion parameter models. Yeah. So then, for all of those, take that and multiply it by, we'll call it two, um, to get the number of gigabytes.</p><p>So, like, half a terabyte for the, um, for, like, a, you know, palm, well, a whole terabyte for palm 540b, um, which is one of the larger ones ever trained. I guess the context for, like, if they are pre-loaded in the browser, would it have to be a smaller one, like, still in the tens of gigabytes?</p><p>Um, yeah. It's been a long while since I downloaded a browser to my computer. But I want to say that the package that you download to install a browser is in the, like, couple of gigabytes range, right? No, I think it's, like, hundreds of megs. Hundreds of megs? Yeah.</p><p>They actually download the whole thing in the installer. Uh, yeah. Oh. Uh. Yeah. Wait, so you download an installer, and then you have to download-- anyway. Yeah. So if people want to install stuff that's only a few hundred megabytes, then that's a non-starter. Um, I guess I expect a Linux distro image to be in the, like, couple of gigabytes, like, if I'm playing around with containers.</p><p>So that's, um, uh, that's, like, another anchor point. Um, and also, like, for those things, we're probably talking, like, two, four, eight years before that kind of, like, standardization effort agreement, like, happens. And we can hope that internet speeds will increase in that time to match the increasing needs, uh, uh, of the internet.</p><p>Um, but yeah, that's-- it's a pretty tight constraint, and, like, probably looks a lot more like mobile stuff for a very long time. Um, yeah. Uh, programming that can change any of this map? No. I think, like, right now, I've been kind of assuming that you're doing stuff relatively efficiently.</p><p>And to be honest, like, PyTorch is, like, pretty good at this already. Like, um, the fact that the application layer is written in Python isn't the problem. Um, but yeah, good question. I have a question. I have a question. I was curious, what's the cost of inference? Like, um-- Yeah.</p><p>Inference on CPU, what's this inference on code? How is it, like, distributed through data? Yeah. Um, I think we'll come to that in, uh, once, like, wanted to talk about, um, after we talk about running AI over network, talk about the actual inference workloads. Um, so we'll definitely get to talking about that.</p><p>I don't think-- I'm not going to have a price number to give to you. Um, but you have to take whatever tokens per second you can get, and then, like, however much you're spending on GPUs, um, and then convert that into a dollars per token. Um, and that's going to give you something you can compare to the, like, model providers.</p><p>Um, and until you put some decent optimization into it, you aren't going to match them. Um, you did ask about CPU inference. That is rapidly evolving. I think there are cases where you can kind of, like, compete in price there. But, um, yeah, we'll cover that more later. Uh, it seems like-- let's put a pin in those two things, and we'll come back to them after we talk about, like, really the thing that almost everybody's going to do, like, immediately after they leave is going to be run AI somewhere in a data center.</p><p>Um, but those are important questions long-term. Okay. So, uh, like, uh, the running stuff on end-user devices has a lot of reasons why it's not so great right now, so what do you get when you run AI workloads in a data center? Um, the biggest win is simplicity. Uh, the biggest pain point is latency, as we've already discussed.</p><p>Um, so simplicity, like, you just-- you control the whole environment. It makes your life a lot easier. Yeah? You said that latency is the biggest pain point. Is that really a thing for LLMs compared to, like, vision models? Because, like, anyway, the tokens that you can infer for seconds are quite a bit slower than any of the network latency that you talked about.</p><p>Yeah, so I would say that you can-- let's see. So the question is whether latency is actually a problem. So, um, if you need to do, like, back and forth, like, you need to get something back from the OpenAI API, then possibly, like, call it again with some added context or, like, run some if statements and then send it back, now you're looking at, like, multiple network calls.</p><p>And you could avoid all of that overhead if you were running things locally. So that's an example of the case where people would run into a latency problem. But locally you're gonna get way lower tokens per second anyway, so you're completely dominated by the time it takes to generate tokens.</p><p>So tokens per second is a throughput number, not a latency number, right? So it doesn't matter if your tokens per second is half-- if your tokens per second is half that of what OpenAI is getting, but you only need to generate 30 tokens, right? that-- then, like, the latency number is going to be the larger one, even though your, like, throughput is lower.</p><p>Like, this is definitely something that people have, like, run into when you have, like, highly interactive things. Like, they're definitely-- so to be clear, there are tons of applications in which you don't feel this pain. And, like, ChatGPT, for example, is at this point, like, the latency of the response from the machine is not really the problem.</p><p>So, yeah, it's not guaranteed to be a pain point, I would say, as well. Yeah. So, yeah, and I guess I'm also kind of maybe imagining situations that are closer to the computer vision case in which you need cognition, well, in the computer vision case, you need rapid responses because it's, like, in the motor loop of a system, for example.</p><p>And if we want to use language models as the cognitive component of a moving system, then they would need latencies like that, like in the tens of milliseconds or something. Yeah. And you are never going to be able to achieve that over a network. But, yeah, great question. All right, so inference as a service providers, this makes it super easy to get started.</p><p>It's what, you know, when you're using OpenAI, they are inference as a service provider. Also, all the proprietary models basically live here. There's not some, like, way that they would ship you the model and you could run it and it's proprietary license. That doesn't exist yet. Open models are also available.</p><p>So, like, if you want to bet on the, like, open ecosystem, you can use a service like Replicate that will, like, they'll run open models for you. It's generally, like, easy to get started. It's not that much more expensive than running it yourself in a lot of cases. But you have limited control of the model.</p><p>For proprietary models, we already talked about how you would have less control kind of inherently. So, for even for people who are providing open models, since they don't have IP they want to protect, in order for them to, like, serve it cheaply to you, they need to have, like, and to have, like, an economic win that they can, like, pass on to you and, like, keep a little bit for themselves, they need to do something like amortize costs across many users of models.</p><p>Many more than you have. And that requires some amount of homogeneity of usage. And it's, like, right now it's proven to be, like, pretty hard to give people control while also giving them homogeneity of usage. For something like AWS, they came up with really smart ways to cache pieces of containers so that the fact that everybody's using kind of the same software allows them to amortize while also giving customization.</p><p>But people have not figured out a similar trick for language models or image generation models yet. So, you don't have as much control as you would have yourself. So, new constraints arise. So, there are things like API rate limits. And now this is sort of like a cost management game.</p><p>You look at this as, like, rather than, like, paying up front for compute that you have, and then you think about maxing, the use of that compute. You think the other direction. You try to minimize your use of compute while fitting the rest of your constraints. So, it's a very different feeling, you know, if you ever switch between having your own compute and switching to cloud.</p><p>It's the same idea. Yeah. So, right. Right. So, you could do that inference yourself. So, rather than having somebody else do it for you. And this works pretty well and is getting easier every day. So, cloud, like, this is, like, running stuff on a public cloud is, like, one of the most popular choices for how to run ML workloads.</p><p>And for, like, SaaS in general, there's some specialist cloud providers in this space, like Lambda Labs, that can be, like, very competitive. They're, like, often cheaper than the, like, big three. And it's a nice balance of control with, like, complexity and, like, which things you actually care about having to deal with versus not.</p><p>It can get expensive over time. It's definitely, like, you know, more expensive than, like, over a long period of time than if you bought the stuff yourself. GPUs sometimes feel like second-class citizens and especially a lot of, like, big public clouds. Google Cloud's a bit of a distinction there in that you can just add GPUs to any instance.</p><p>It's kind of nice. But in other public clouds, that's not really the case. And, again, this is, like, a cost management problem. And one of the popular ways to solve the cloud costs is to just agree to a large deal up front. And now you're starting to get some of the illiquidity associated with actually building buying hardware.</p><p>And you start to get some of the, like, vendor lock-in that you would also associate with that. So you have the opportunity to kind of, like, trade those things off. But they are your constraints to work with. I did want to call out that there are some serverless approaches, which gives you some of the, like, usage-based, like, really tightly usage-based pricing associated with inference-as-a-service providers.</p><p>But also the, like, control associated with, like, you know, renting servers in the cloud. And by this, by serverless, I mean anything with, like, scale-to-zero semantics and pricing. That doesn't involve you having to, like, literally manage servers. So, like, thinking about the operating system, for example. And that offers high availability.</p><p>This is, like, a relatively new category in software in general and especially in machine learning. There's a couple of players here. Modal Labs is one that I like quite a bit because it doesn't just do the ML stuff, though it is very good at it. So, Replicate, which also does inference-as-a-service, will do this.</p><p>Hugging face spaces recently changed their endpoints to scale-to-zero. And there's also, yeah, banana.dev and others. The good thing is that it's, like, easier to get started, especially if you're not, like, a cloud ops person. And very inexpensive at low traffic, like, you only have to pay when you have traffic.</p><p>And if you're, like, running a small, if you're running a demo that only needs to be up when you're showing it to investors. Or if you are working on a tiny feature at a large organization, then you might have very low traffic patterns. Oh, my. It's Fleet Week, I think.</p><p>So that might be the Blue Angels. Yeah. Scale-to-zero means that you, when there are no requests, you are not, when your requests go to zero, the amount of resources that you are using and being charged for also goes to zero. Yes. So, yeah, for a while, Hugging Face spaces, endpoints, they changed, there's inference endpoints, and, yeah.</p><p>For a while, it was, like, it could scale down to one, and it would autoscale. So there's, like, having a cloud server with autoscaling built in, and then there's that thing, but then it also scales to zero, and you don't have to think about server management. And that, like, is the combination, like, it's the original, like, AWS definition of serverless that has kind of fallen.</p><p>Not everyone goes by the old ways. Yeah. So inexpensive at low traffic, when nobody's calling your API, you don't pay for anything. If you, like, come up with a feature, it doesn't work, then it doesn't matter. The bad news is that you kind of generally lose, like, tight control over autoscaling behavior that you could have if you were, like, you know, if you have a, you know, Kubernetes team to work with, they can very tightly set it up so that the autoscaling delivers exactly the throughput and latencies, P99s, that you promised.</p><p>And you kind of give all over some of that control to these serverless providers who are themselves probably running Kubernetes, but for a lot of people. And then the thing that has kind of prevented this from being as successful as maybe serverless architectures in many other places is latency.</p><p>So when you're, it shows up as kind of P99 latency, so the 99th percentile of requests that hit a point when you need to do autoscaling, the, you need to get the weights of the model you're using into, not just off of disk and into RAM, but then from there into the RAM of the accelerator.</p><p>And that takes, like, that can take a very long amount of time. And so you're looking at, like, 30-second, one-minute, three-minute cold boots in some cases because you are moving half a terabyte of data around. And so that's a place where people could maybe come up with these, like, clever ways to cache and share.</p><p>But, yeah, it's the memory constraint that you hit in other domains showing up, like, in disguise as latency. And, yeah, so the, like, you still are probably going to be thinking of this in terms of, like, cost management and cost reduction as opposed to, like, resource maximization. Yeah. Yeah.</p><p>So the point was about Cloudflare workers. So I did see that Cloudflare, I didn't include them on the slide, but Cloudflare actually recently released these, like, GPU workers, which is their entry into this. And I haven't had time to play with it, so I don't know that much about it.</p><p>I think if I need to go from not consuming any of your resources to having a terabyte of my own personal bytes, like, in the VRAM of a GPU, I find it hard to believe that they don't have a latency problem there. Like, so I'm curious what you know about the solution.</p><p>Yeah. Yeah, yeah, yeah. Yeah, that's interesting. Yeah, I'd love to hear about that. That's been my experience with the other serverless GPU providers. So I'd love to hear more about the Cloudflare workers. And, yeah, if that goes away, then serverless becomes a much more competitive way of delivering inference.</p><p>So, yeah. So I maintain a page for full-stack deep learning that has information about, like, cloud GPUs and serverless providers pricing and what compute they provide. So you can check that out from the slides later if you're interested. All right. And then last, let's talk, like, actually, what if you actually physically owned the computers that the inference ran on?</p><p>Like, you can do that. And rather than having to, like, actually, you know, construct a building which maybe the largest enterprises could go about, using a co-location facility isn't so bad. And there's more reason to do this than for other kinds of workloads. And in particular, there's actually room to beat a lot of the major public clouds, which is why there's competitive clouds, like, alternatives in this space.</p><p>A lot of data centers that have been, like, around or that were designed before 2021 or so are configured for, like, disk and network heavy workloads rather than power heavy workloads. So even if you can get a hold of, like, 30,000 A100s, you can't just necessarily put them in the same U.S.</p><p>East data center that used to run -- that was designed for, like, running databases. So it's capital intensive but ends up being cheaper in the long run. You have total control if you need it, which is awesome. But it's very hard, very rare skill set because it, like, kind of crosses this, like, the ML stuff and the hardware stuff.</p><p>And all of these people can go and work for OpenAI for, like, a million and a half a year. So good luck holding on to them. And the biggest constraint that shows up is illiquidity. So you're going to make a big bet on what this looks like. For example, that inference is not going to move on to CPU or not going to move on to custom silicon that behaves very differently from graphics cards.</p><p>There's a great talk from Mitesh Agrawal of Lambda Labs about this that goes into kind of detail. I think it's only, like, a year old, if I remember this talk right. And, of course, he makes it sound very hard because he wants you to use their cloud or to pay them to, like, help you build your co-location -- help you actually build it.</p><p>But it is a detailed explanation of everything involved. And, you know, there's not very many of those out there. So let's go ahead. All right. We're at half time. So I plan to take a break when I finish part one, which goes to the rest of self-serve inference, which we'll all say is another 15 minutes.</p><p>So let's do that. And we'll leave an hour for part two after a little break. So we actually haven't talked in great detail about, like, you know, why are we using GPUs in the first place? Like, what is actually going on here? When we run this, like, tensor-to-tensor map with neural networks, like, what actually does that workload turn into?</p><p>We have two tasks. We need to load numbers from memory, and then we need to do math on those numbers. Those are our, like, two basic tasks. And that is the reason why we have -- why we end up using graphics processing units, because memory is slow and math is fast.</p><p>And in most -- in the transformer architecture in particular, but in many, like, sort of most neural network architectures you might write down, you only need a given number from the weights, like, one time per input. So that means you need to do a memory read, like, of this, of, like, a couple of bytes for a particular parameter to use it in a single floating-point operation.</p><p>And the memory read is going to be very slow, and the floating-point operation is going to be basically instant. So in order to do this economically, you need to do a lot of math for each read for memory. You need to, like, load, you know, load the weight out.</p><p>And in particular, we're talking here about, like, getting out of the VRAM and into the place where the -- you know, into the -- like, the -- what is it called? Well, yeah, it's basically like an L1 cache, like, closer to the actual computation. And so you want to do that and then, like, use it multiple times, you know, and that means you want to run on multiple inputs at once.</p><p>And that is memory intensive, single instruction, multiple data, parallel linear algebra, the same thing you need for graphics workloads. So the, like, graphics cards have turned out to be, like, pretty good at solving this problem. And the numbers there in the corner are, like, demonstrate this, like, general fact of, like, memory is slow, logic or math is fast.</p><p>The -- you can do 312 teraflops per second in -- for two-byte numbers, two-byte floating-point numbers in a tensor core in an A100. And you only get one and a half terabytes a second of memory bandwidth. And when you're using, like, you know, optimized existing CUDA kernels, these two things are, like, multiplexed.</p><p>So, like, you load a weight where you start doing math on it and then the next weight gets loaded, like, you know, concurrently. But you do still have this, like, mismatch in the bandwidths that means that you want to be able to, like, amortize a memory load across as many computations as possible.</p><p>And, like, in principle, this could be flipped around and we would have, like, very -- you know, things would look very different. So, you can get, like, very, very large throughput gains by amortizing memory reads where, basically, if you are operating on a very small number of tokens, then you'll see that as you add -- like, if you're running this workload yourself, as you add more tokens, you would think, like, you should expect, like, a slight increase in the amount of time that it takes.</p><p>And you'll see, like, basically a flat curve for a very long time. And then once you hit the point where the -- yeah, so if you look -- batch size at which you'll see that flip. So, for an A100, it's about 200 elements in the batch. And you'll see -- for an H100, you'll see that that ratio -- these numbers both go up, but the ratio becomes more extreme.</p><p>So, you -- there's a great blog post from Carol Chen on, like, inference arithmetic that both goes through in greater detail and then, like, matches that onto some, like, actual experimental results. And it's able to, like, track where did each, you know, microsecond, basically, of inference time come from.</p><p>So, the, like, key takeaway from this is that if you want to get large throughput in, like, an inference system that you're running yourself, you're going to need batching, you're going to need to, like, collect up multiple inputs from multiple users and operate on them at the same time.</p><p>So, this is -- it's challenging to achieve the same thing in an end-user device. You are -- like, if you're only working for one person, then they might not make 10 requests, you know, quickly enough for you to fill up a batch. And that actually might kind of tilt things in the direction of CPUs or of, you know, different architecture with different, you know, memory bandwidth versus -- TensorFlow versus math bandwidth trade-off.</p><p>One of the useful things that came out of this -- I guess this is a restatement of kind of what I just said. When you have a, like, low load API, you will end up with smaller batch sizes, because maybe you have -- you have to deliver with a certain latency, so you can't just wait forever.</p><p>And so you will make different decisions about compute memory trade-offs. For example, like, caching past computations of keys and values is very common when you're doing batch inference, but it's actually not necessarily the right choice when you are already memory bound. Trading off memory for compute isn't a good idea.</p><p>If you're, like, at -- for larger batch sizes, you -- if you're doing something that's an API where, like, serving requests directly, then you would want to -- like, roughly balance being flop bound and memory bound so that you can get quick latency of responses, even though your throughput is less than it would be otherwise.</p><p>Whereas if you're doing, like, a batch job, like an overnight-type job, or that is, like, typical of data science or big data workloads, then you would just go for the largest batch that you can. And that's what people do during training. They go for the absolute largest batch that they can, because there's no latency requirement.</p><p>There's only throughput. I'm assuming all this still applies to the other accelerators. Yeah, yeah. So TPUs end up -- I, like, looked around, and I've never personally used them for anything serious, so I wasn't able to, like, directly map on, like, pull out the equivalent of a tensor core, like, flop bandwidth and VRAM to L1 latency.</p><p>But the results and benchmarks that I've seen is that they're, like, 30% better. Like, they have a -- they do have a slightly different choice of trade-off, and they -- and it's better for neural network workloads than graphics workloads. And that gets what -- from what I have seen, like, 30% improvement, but not, like, a 10 to 50x improvement, which is what you would really like to see if you're making as drastic a decision as, like, going over to a completely different accelerator with a completely different software stack.</p><p>Yeah, I guess -- yeah, I got into a discussion with one of the people on the TPU team about, like, the hardware lottery, and he was, like, the GPU is already, like, is just an excellent machine. Like, it does this workload really, really well. And I think that is borne out in the numbers.</p><p>Yeah? From what I heard, the main advantage of TPUs is operational efficiency and power efficiency, not, like, good performance. Yeah. So, for the folks listening online, repeating the point, one of the benefits of TPUs is power. I totally buy that. Yeah. I think -- yeah, the numbers that tend to get reported in things like Google's Pathways paper and Palm Papers are open -- and, like, what is known about OpenAI.</p><p>It's, like, it's all about, like, flop utilization and total flops and stuff like that, and not things like power that do matter. I'd say, like, people have not gone rushing to try and get a hold of them, which feels like a strong signal, similar with, like, other types of custom silicon, like Cerebris' chip.</p><p>But, you know, who knows? I think this is getting back to a question that got asked earlier, I believe. The, like, having custom chips works really well when workloads stay very fixed in, like, kind of precise detail. Not just this, like, oh, we need to load from memory and then we need to do it.</p><p>But, like, no, we need to do this shaped thing, like, with this many -- like, that works super well. And you can see that in blockchain mining where there is, like, for many chains for a long time, ASICs were the, like, only profitable way to mine. Or, like, the most profitable way to mine.</p><p>And that's the workload is, like, unchangeable except by, like, distributed consensus. And that allows you to, like, very tightly target a specific workload. Whereas, like, neural network architectures actually kind of change reasonably often. And the, like, you know, even the difference -- the difference between a 7 billion parameter model, a 13 billion parameter model, a 170 billion parameter model, that's, like, a bigger difference than you would see between workloads for, like, the same blockchain over time.</p><p>And so that's -- that, like, difference in -- like, that technical difference is, I think, a big reason why we haven't seen much uptake of custom -- custom -- Yeah, I think -- like, they're one of the -- like, they're one of the -- they're one of the providers for -- like, they're one of the startups working on this.</p><p>They come up most frequently. I don't -- it's still, like, fairly experimental, I think. Like, it's not -- it's not the -- it's not, like, generally available in a public cloud or something. So, yeah, I don't have much to say about it, I guess. Yeah. I think the main thing you can see there is the memory that they have is 40 gigabytes.</p><p>Compare that with an Android, which is 80. And for most of Elements, memory is . Mm-hmm? Yeah, yeah, so Cerebus has done some large model training, so it's definitely, like, possible. They -- they have this very fast memory bandwidth, I think, is, like, maybe the big one, 20 petabytes memory bandwidth.</p><p>Now, like, the point that was raised about the bottleneck being the memory capacity is very -- is well taken. You can network multiple chips together, and then you have more storage. But, yeah, and so, like, better balancing of memory bandwidth and math bandwidth would maybe get you more -- like, more efficacy at lower batch sizes.</p><p>But you can't exactly put that chip in a phone. So that -- yeah. But definitely, like, people are really converging on transformer architectures, so that could mean that this custom silicon works better in five years. Yeah, I think I -- yeah, the way that you, like -- the way that you solve this problem is if you have multiple GPUs, these -- the, like, memory per second and flops per second, like, scale at the same amount, right?</p><p>You have -- now you have two GPUs loading, you have two GPUs doing operations. They both scale linearly. If you're really clever, you can, like, actually get that linear scaling in practice. The GPU memory goes up as well. And so you can serve larger batches and eventually hit -- this ratio is staying the same, so you can eventually hit that point.</p><p>And that's what -- and then, like, once you hit the point where you're actually, you know, flops bound, you can just, like, crank it, you know, just crank it up. Just get really big -- really big batches, speed up -- speed up the computation. And that is what people do during -- are, like, used to do -- used to doing during training.</p><p>And so you'll very commonly see people talking about this way of making inference efficient. So if you are running a -- like, a web service, this is going to give you your, sort of, like, pod size, roughly. Like, a pod in Kubernetes is, like, a bunch of services that end up on a single physical machine.</p><p>So this is your, like, unit -- is determined by, like, how many GPUs do you need to hit the, like, batch size for efficiency implied by the, like, this flops memory per second. And you check in to make sure this is also true for your architecture. Yeah. All that's, like, kind of theoretical stuff.</p><p>or sort of, like, trying to be relatively first principles. When you -- when it comes time to actually check whether you're doing things correctly, make sure to use, like, a profiler or a tracer and actually, like, look at these things. Even, like, people who are pretty good at this will, like, miss stuff that shows up on a tracer.</p><p>Like, like, the VLLM implementation was launching a CUDA kernel for, like, every individual element of a batch. And, like, a tiny change made it switch to, like, one CUDA kernel launch. And that saved them, like, increased their throughput by 33%. And it's something that shows up as, like, way more CUDA kernel launches, which would show up as, like, I guess -- if I had the interactive version of this -- would show up as this, like, huge thing of red lines.</p><p>And also just the, like, general utilization shows up as these big, like, patches of gray here, which shows you where you're, like, not actually using your GPU at all. And I found, like, looking directly at these traces, not just at statistical profiles, helps, like, catch those kinds of easy-to-fix 80/20 kind of bugs.</p><p>You can also profile and trace memory. And that's going to be, like, given that memory is this key constraint that's likely to be at least as useful. And these two are examples from PyTorch, these -- the compute trace and the memory trace, both from PyTorch. You might not be using PyTorch, so you might have to fall back on generic, like, GPU profiling or CPU profiling or memory profiling and tracing tools if you're using, like, a custom inference solution.</p><p>Yeah, speaking of which, there's a bunch of specialized LLM inference libraries out there. There's a nice blog post from Hamil Hussain about using these, and this is specifically for, like, batch size one. And so the VLM is -- people are, like, pretty -- that seems to be getting, like, the most community excitement and community contribution.</p><p>But Hamil suggests some reasons why you might be into NLC and C Translate, too, instead for doing your LLM inference. So, actually -- so all that was about, like, just thinking in terms of, like, an individual workload, setting up an entire inference service, which is now something that maybe, like, somebody in the company sets up an inference service or an inference platform.</p><p>And then people can, you know, submit workloads to it. That can be fairly challenging. Containerization for GPU accelerated workloads is less painful now than it used to be. Like, NVIDIA Docker actually works in a way that it did not, like, early on. But containerization is, like, fundamentally, you know, more dubious for these workloads than a lot of other ones.</p><p>So one is that the, like, application layer of the container is probably where the weights are going to live. Like, it doesn't live in the operating system yet. It's not built into the Docker engine. So it's, like, that's up there, and that's, like, maybe half a terabyte in bad situations.</p><p>So now your container images are really large, and that's -- that can be pretty unpleasant. You could try and do the things that people do with databases and, like, move it into remote storage. But, yeah, then the container is not as, like, unitary. And then the, like, the worst problem maybe is that the CUDA driver changes the choices that you need to make kind of at the, like, application level of, like, we were just talking about actually going one level down.</p><p>But, like, which GPU is going to run on changes the point at which you switch from being memory bound to flops bound because that has a different, like, memory bandwidth to math bandwidth ratio. So things at the level of the NVIDIA of the GPU are entangled with things at the application layer, like choices of, like, batching strategy.</p><p>So that's -- and a similar thing can be said for the CUDA driver. I was, like, naive about containers, and I was, like, why is the, like, CUDA driver showing up as 12.0, which is what I have installed in my host operating system, when I specifically downloaded the CUDA 11.2 container, and it's, like, containers can only virtualize so much.</p><p>And so that also changes, like, which kernels are available by default. I guess what kernels would be a layer above. So there are features of CUDA drivers that change the, like, you know, how hugging face transformers or PyTorch will work. And so that will -- and that will also change decisions that are made at the application layer.</p><p>So when you're in these, like, super -- like, this is clearly a performance-limited regime that we're talking about here. Like, the inference is, like, extremely expensive, and we're, like, trying to maximize performance really aggressively, and that, like, starts to reveal the limitations of virtualization and containerization. So that doesn't mean it's impossible.</p><p>It just means it's hard. And, yeah, I don't know if you've ever worked with, like, you know, trying to set up a heterogeneous compute Kubernetes cluster where there's, like, some have GPUs and some have ARM and some have, like, x86. You, like, this -- dealing with this requires a better ops engineer than if you can ignore that.</p><p>Speaking of which, this, like, application serving can be and is, in fact, done with the, like, industry standard for container orchestration in Kubernetes. I would note that now the, like, Nvidia stuff is showing up here again and, like, all the problem -- like, the entangling -- and now Kubernetes is in between them.</p><p>So now there's, like, a lot of opportunity for crosstalk, a lot of opportunity for tiers. And so this is a hard mode version of that problem, which is not notorious for being easy. So you might choose and need to do it by hand. There was a comment on r/moops that was, like, oh, yeah, we look -- you know, I was looking around to see what the opinions are on these tools.</p><p>And an opinion I saw in a couple places was, like, if you care about -- like, you've chosen to do serving of inference yourself and not do, like, just API calls, then maybe you should make mloops a core competency of your company. And you're going to want to do something more, like, building it out of the, like, Kubernetes-affiliated ecosystem of open-source tools or other open-source things, like Q-Ray to do -- to run Ray on Kubernetes for serving, or Selden Core to run that on -- again, on Kubernetes as your container orchestration layer.</p><p>But given how, like, painful that is and the fact that there are so many people who are trying to run a relatively similar workload, you might consider, like, either of two kind of tiers of managed services, either the, like, white-glove end-to-end kind of cloud provider approach, the oldest one, Amazon SageMaker, more recently, Vertex AI from Google.</p><p>I'm sure there's an Azure version that I'm forgetting. There are also, from some startups, a sort of more toolbox-y approach, a less end-to-end approach, in part because it's not, like, integrated all the way at the layer of the, like, actual hardware like the cloud providers are. So, Bento Cloud, Selden, and AnyScale, AnyScale being a managed version of Ray, Selden offers a managed version of Selden Core, Bento Cloud, managed version of Bento ML.</p><p>So, depending on where you want to -- maybe you have a really great, sweet deal with GCP, and so Vertex is the right choice. But, yeah, I think from -- I have had limited experience with these things because I've tried to keep my life simple and happy. But if you're -- if you end up in this space -- I've seen some nice things and played around with RayServe and AnyScale.</p><p>Ray's the tool that a lot of people use for cluster management for training. So, a lot of the teams that train the foundation models use Ray. And so, there's kind of, like, a natural competency for them, both in the open source library and AnyScale as a layer around that.</p><p>So, maybe it'd be a good choice for inference as well. All right. So, rather than take questions, I think I'm going to do a five-minute break in which you can ask me questions up here while we all get a little stretch, maybe a little air. And we'll come back for the rest of the OWL in five minutes.</p></div></div></body></html>