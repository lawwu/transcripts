<html><head><title>AI Breaks Its Silence: OpenAI’s ‘Next 12 Days’, Genie 2, and a Word of Caution</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>AI Breaks Its Silence: OpenAI’s ‘Next 12 Days’, Genie 2, and a Word of Caution</h2><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M"><img src="https://i.ytimg.com/vi_webp/jIm2T7h_a0M/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=43">0:43</a> OpenAI 12 Days, Sora Turbo, o1<br><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=186">3:6</a> Genie 2<br><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=506">8:26</a> Jensen Huang and Altman Hallucination Predictions<br><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=585">9:45</a> Bag of Heuristics Paper<br><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=700">11:40</a> Procedural Knowledge Paper<br><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=782">13:2</a> AssemblyAI Universal 2<br><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=825">13:45</a> SimpleBench QwQ and Chinese Models<br><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=882">14:42</a> Kling Motion Brush<br><br><div style="text-align: left;"><a href="./jIm2T7h_a0M.html">Whisper Transcript</a> | <a href="./transcript_jIm2T7h_a0M.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Did you notice that there hadn't been much interesting AI news in the last few weeks?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=5" target="_blank">00:00:05.600</a></span> | <span class="t">And I know that's news to all the accounts that proclaim huge AI news every three days,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=12" target="_blank">00:00:12.240</a></span> | <span class="t">but I mean actually. And I even had a whole video ready on that hibernation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=17" target="_blank">00:00:17.360</a></span> | <span class="t">going over a bunch of new papers on some of the hurdles ahead as we inch closer to AGI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=23" target="_blank">00:00:23.920</a></span> | <span class="t">But then a couple of announcements came tonight and that video will have to wait. The mini AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=30" target="_blank">00:00:30.240</a></span> | <span class="t">winter, which is more of a cold snap really, might be drawing to an end. But after covering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=35" target="_blank">00:00:35.760</a></span> | <span class="t">those announcements, we're going to see evidence that no matter what AI tech titans tell you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=41" target="_blank">00:00:41.120</a></span> | <span class="t">never leave your hypometer at home. First, what just got announced by Sam Altman and OpenAI?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=48" target="_blank">00:00:48.160</a></span> | <span class="t">Well, it's 12 days of releases. And that's all they tell us. But we can join the dots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=55" target="_blank">00:00:55.680</a></span> | <span class="t">and piece together some reporting to find out a bit more about what's coming.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=59" target="_blank">00:00:59.840</a></span> | <span class="t">What's almost certainly coming in these next 12 days is Sora at long last. In case you've long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=66" target="_blank">00:01:06.240</a></span> | <span class="t">since forgotten what Sora is, it's a text to video generator from OpenAI. It was first showcased</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=72" target="_blank">00:01:12.960</a></span> | <span class="t">back in February of this year. And even though it's been almost a full year, I would argue that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=78" target="_blank">00:01:18.960</a></span> | <span class="t">some of these demo videos are still the best I've seen from a text to video model. A version of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=84" target="_blank">00:01:24.240</a></span> | <span class="t">Sora was leaked by disgruntled artists around a week ago. And that's how some people were able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=89" target="_blank">00:01:29.920</a></span> | <span class="t">to generate these clips. Some of these are obviously state of the art, but then some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=94" target="_blank">00:01:34.720</a></span> | <span class="t">them are less impressive. And what we learn unofficially is that it seems like there might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=99" target="_blank">00:01:39.680</a></span> | <span class="t">be a Sora turbo mode. In short, a model that generates outputs more quickly, but with less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=105" target="_blank">00:01:45.920</a></span> | <span class="t">quality. I'll have more on hallucinations in a minute, but what else might be coming in these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=110" target="_blank">00:01:50.720</a></span> | <span class="t">12 days? Well, very likely their smartest model, which is called O1. One of their employees writing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=117" target="_blank">00:01:57.680</a></span> | <span class="t">under an alias said, first, OpenAI is unbelievably back. That's yesterday. Someone asked, give us full</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=124" target="_blank">00:02:04.960</a></span> | <span class="t">O1. And he said, okay. Indeed, OpenAI senior vice president of research, newly promoted Mark Chen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=131" target="_blank">00:02:11.680</a></span> | <span class="t">wrote, if you know, you know. The full version of O1 simply called O1 as compared to O1 preview</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=138" target="_blank">00:02:18.560</a></span> | <span class="t">looks set to be the smartest model, at least in terms of mathematics and coding. That doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=143" target="_blank">00:02:23.760</a></span> | <span class="t">automatically mean it will become your chosen model. In some areas, it actually slightly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=148" target="_blank">00:02:28.640</a></span> | <span class="t">underperforms the currently available O1 preview. That does though leave one question for me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=153" target="_blank">00:02:33.920</a></span> | <span class="t">which is what are they going to do to fill the other 10 days? According to one of their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=158" target="_blank">00:02:38.640</a></span> | <span class="t">key researchers, they're going to have to ship faster than the goalposts move.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=163" target="_blank">00:02:43.680</a></span> | <span class="t">And now that ChatGPT has surpassed 300 million weekly active users, just three months after it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=170" target="_blank">00:02:50.000</a></span> | <span class="t">surpassed 200 million weekly active users, no one can deny that plenty of people might use anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=175" target="_blank">00:02:55.760</a></span> | <span class="t">they do ship. Of course, none of us have to wait for things like Sora. There are some epic free</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=181" target="_blank">00:03:01.040</a></span> | <span class="t">tools that you can use today that I will show you at the end of this video. But the biggest news of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=185" target="_blank">00:03:05.760</a></span> | <span class="t">the day didn't actually come from OpenAI. It came from Google DeepMind with their presentation on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=191" target="_blank">00:03:11.440</a></span> | <span class="t">Genie 2. In short, it's a model that you can't yet use, but which can turn any image into a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=199" target="_blank">00:03:19.040</a></span> | <span class="t">playable world. And there's something just a little bit ironic about the announcement of Genie 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=204" target="_blank">00:03:24.480</a></span> | <span class="t">tonight. The background is that I covered Genie 1 on this channel and talked about its potential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=210" target="_blank">00:03:30.080</a></span> | <span class="t">moving forward. The playable worlds that Genie 1 could conjure up were decent, but pretty low</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=216" target="_blank">00:03:36.080</a></span> | <span class="t">definition and limited. But I noted from the paper that the architecture could scale gracefully with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=222" target="_blank">00:03:42.480</a></span> | <span class="t">additional computational resources. That's not the irony. The irony is that just a couple of days</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=227" target="_blank">00:03:47.760</a></span> | <span class="t">ago, I interviewed the person who managed and coordinated this Genie 2 project, Tim Roktaschel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=233" target="_blank">00:03:53.760</a></span> | <span class="t">This was for a podcast that's been released on my AI Insiders platform on Patreon. I will,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=238" target="_blank">00:03:58.720</a></span> | <span class="t">of course, go through the Genie 2 announcement, the demo videos, and what they say is coming next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=243" target="_blank">00:04:03.600</a></span> | <span class="t">But I can't help but point out, I directly asked Tim Roktaschel about Genie 2. The paper, it capped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=249" target="_blank">00:04:09.680</a></span> | <span class="t">out, I think, 2.7 billion parameters. I just wonder, you might not be able to say, but surely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=254" target="_blank">00:04:14.320</a></span> | <span class="t">Genie 2, you know, 270 billion parameters or something, is that something that you're working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=259" target="_blank">00:04:19.360</a></span> | <span class="t">on or excited about with more data? Because the paper even talked about how one day we might train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=264" target="_blank">00:04:24.240</a></span> | <span class="t">on a greater scale of internet data, all of YouTube potentially. Is that something you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=267" target="_blank">00:04:27.920</a></span> | <span class="t">working on or could speak about at all? I'm excited about this. You can just look,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=272" target="_blank">00:04:32.640</a></span> | <span class="t">basically, what happened over the last few months since Genie was published. There's, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=278" target="_blank">00:04:38.240</a></span> | <span class="t">the Oasis work that came out a few weeks ago, where people basically learned a neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=283" target="_blank">00:04:43.280</a></span> | <span class="t">to simulate Minecraft. Before that, there was a paper learning to simulate Doom using neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=288" target="_blank">00:04:48.640</a></span> | <span class="t">network. That space is definitely heating up. I think it's exciting. I think maybe at some point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=294" target="_blank">00:04:54.720</a></span> | <span class="t">these simulators, these learned simulators, are getting fast and rich enough so that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=298" target="_blank">00:04:58.640</a></span> | <span class="t">then can also use them to adversarially probe in a body AGI and teach it new capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=304" target="_blank">00:05:04.720</a></span> | <span class="t">The reason I was interviewing him, by the way, is because he is the author of the brand new book,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=310" target="_blank">00:05:10.000</a></span> | <span class="t">AI - 10 Things You Should Know. But what really is Genie 2 and what does it say about what's coming?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=316" target="_blank">00:05:16.000</a></span> | <span class="t">DeepMind call it a foundation world model. Essentially, you give it a single image and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=320" target="_blank">00:05:20.880</a></span> | <span class="t">Genie 2 will turn it into an interactive world. The world might not quite be as high resolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=326" target="_blank">00:05:26.800</a></span> | <span class="t">as the original image, but you can use keyboard actions to control that world. Jump, fly, skip,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=333" target="_blank">00:05:33.200</a></span> | <span class="t">swim, all that kind of thing. I can imagine this being used for dream sequences within games,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=338" target="_blank">00:05:38.080</a></span> | <span class="t">where a character might have a dream of an alternate reality and you can interact with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=342" target="_blank">00:05:42.560</a></span> | <span class="t">that world. Or maybe in the future, websites, instead of having static images in the background</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=347" target="_blank">00:05:47.200</a></span> | <span class="t">or even looping videos, will have interactive environments that you can play like games.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=351" target="_blank">00:05:51.920</a></span> | <span class="t">But just a few quick caveats. These worlds, these generations, on average last 10 to 20 seconds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=357" target="_blank">00:05:57.920</a></span> | <span class="t">or for up to a minute. Next is, even though they seem that way, these example videos aren't quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=364" target="_blank">00:06:04.480</a></span> | <span class="t">real time. As it stands, if you want real time interaction, you'd have to suffer from a reduction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=370" target="_blank">00:06:10.480</a></span> | <span class="t">in quality. And let's be honest, these outputs weren't exactly high resolution to begin with,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=375" target="_blank">00:06:15.600</a></span> | <span class="t">so we're not talking about replacing AAA games anytime soon. Next, the outputs can go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=381" target="_blank">00:06:21.520</a></span> | <span class="t">quite wrong quite quickly with no real explanation. Like in this one, a ghost appears for no reason.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=389" target="_blank">00:06:29.040</a></span> | <span class="t">In this one, the guy started with a snowboard, but then immediately decides just to run the course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=394" target="_blank">00:06:34.320</a></span> | <span class="t">As Google wrote, the character prefers parkour over snowboarding. Yes, by the way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=399" target="_blank">00:06:39.040</a></span> | <span class="t">the initial prompt could be a real world image. And that, of course, is super cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=404" target="_blank">00:06:44.320</a></span> | <span class="t">It can kind of model lighting, although we're not talking ray tracing here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=408" target="_blank">00:06:48.400</a></span> | <span class="t">And it can, it says model gravity, but look at this horse jump on the left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=413" target="_blank">00:06:53.520</a></span> | <span class="t">I wouldn't say that's terribly high accuracy physics. This bit, though, I did find more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=419" target="_blank">00:06:59.120</a></span> | <span class="t">impressive, which is that Genie 2 is capable of remembering parts of the world that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=424" target="_blank">00:07:04.000</a></span> | <span class="t">no longer in view and then rendering them accurately when they become observable again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=428" target="_blank">00:07:08.560</a></span> | <span class="t">So if you look at the characters, they'll look away from something, look back to it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=432" target="_blank">00:07:12.720</a></span> | <span class="t">and it's mostly the same as when they first looked away.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=435" target="_blank">00:07:15.760</a></span> | <span class="t">Interestingly, in the announcement page, which didn't yet come with a paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=439" target="_blank">00:07:19.520</a></span> | <span class="t">they actually pushed a different angle for why this was important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=442" target="_blank">00:07:22.800</a></span> | <span class="t">They said that if we're going to train general embodied agents, in other words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=447" target="_blank">00:07:27.440</a></span> | <span class="t">AI controlling a robot, that's bottlenecked by the availability of sufficiently rich and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=452" target="_blank">00:07:32.080</a></span> | <span class="t">diverse training environments. And they gave an example of how they use Genie 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=456" target="_blank">00:07:36.400</a></span> | <span class="t">to create this interactive world and then told an AI agent to, for example, open the red door.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=462" target="_blank">00:07:42.880</a></span> | <span class="t">The SEMA agent, which I've covered before on the channel, was indeed able to open the red door.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=468" target="_blank">00:07:48.080</a></span> | <span class="t">But I personally would put an asterisk here because it's an AI agent trained on this AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=474" target="_blank">00:07:54.320</a></span> | <span class="t">generated, not particularly realistic world. Because of the stark gap that exists between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=479" target="_blank">00:07:59.520</a></span> | <span class="t">these kind of simulations and our rich, complex reality, I'm not entirely convinced that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=485" target="_blank">00:08:05.440</a></span> | <span class="t">approach will lead to reliable agents. Of course, Google DeepMind could well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=489" target="_blank">00:08:09.520</a></span> | <span class="t">prove me wrong. They say that they believe Genie 2 is the path to solving a structural problem of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=496" target="_blank">00:08:16.720</a></span> | <span class="t">training embodied agents safely while achieving the breadth and generality required to progress</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=502" target="_blank">00:08:22.080</a></span> | <span class="t">towards AGI. Now, of course, my objection would fall away if we had a path for removing these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=507" target="_blank">00:08:27.360</a></span> | <span class="t">creative but not particularly reliable AI hallucinations. But as even the CEO of NVIDIA</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=513" target="_blank">00:08:33.280</a></span> | <span class="t">recently admitted, we are "several years away" from that happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=517" target="_blank">00:08:37.600</a></span> | <span class="t">His solution, as you might expect, is just buy more GPUs. Some of you may say that reliability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=523" target="_blank">00:08:43.360</a></span> | <span class="t">issues and hallucinations, they're just a minor bug. They're going to go soon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=527" target="_blank">00:08:47.040</a></span> | <span class="t">What's the problem with Jensen Huang saying that the solution is still just a few years away?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=531" target="_blank">00:08:51.360</a></span> | <span class="t">Well, I think many people, including AI lab leaders, massively underestimated the hallucination</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=537" target="_blank">00:08:57.200</a></span> | <span class="t">issue. As I covered on this channel in June of 2023, Sam Altman said, and I quote,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=542" target="_blank">00:09:02.640</a></span> | <span class="t">"We won't be talking about hallucinations in one and a half to two years."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=546" target="_blank">00:09:06.640</a></span> | <span class="t">One and a half years from then is like today, and we are talking about hallucinations. And even at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=551" target="_blank">00:09:11.840</a></span> | <span class="t">the upper end, we're talking mid-2025. And I don't think anyone would now vouch for the claim,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=556" target="_blank">00:09:16.720</a></span> | <span class="t">echoed by Mustafa Suleiman, that LLM hallucinations will be largely eliminated by 2025.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=563" target="_blank">00:09:23.040</a></span> | <span class="t">In short, the very thing that makes these models great at generating creative interpolations of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=569" target="_blank">00:09:29.120</a></span> | <span class="t">their data, creative worlds, is the very thing that makes them unreliable when it comes to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=573" target="_blank">00:09:33.840</a></span> | <span class="t">things like physics. Remember that even frontier generative models like SORA,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=578" target="_blank">00:09:38.160</a></span> | <span class="t">when given 10 minutes to produce an output, still produce things where the physics don't make sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=583" target="_blank">00:09:43.680</a></span> | <span class="t">And this links to a recent paper that I was going to analyse for this video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=588" target="_blank">00:09:48.160</a></span> | <span class="t">In mathematics, and plausibly physics, large language models based on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=592" target="_blank">00:09:52.320</a></span> | <span class="t">transformer architecture don't learn robust algorithms. They rely on a bag of heuristics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=598" target="_blank">00:09:58.720</a></span> | <span class="t">or rules of thumb. In other words, they don't learn a single cohesive world model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=603" target="_blank">00:10:03.680</a></span> | <span class="t">They deploy a collection of simpler rules and patterns. That's why, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=607" target="_blank">00:10:07.760</a></span> | <span class="t">with Genie 2 and SORA, you get plausible continuations that, if you look closely,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=612" target="_blank">00:10:12.320</a></span> | <span class="t">don't make too much sense. Imagine SORA or Genie 2 generating a car going off a cliff and all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=618" target="_blank">00:10:18.400</a></span> | <span class="t">resulting physics. You might have hoped that their training data had inculcated Isaac Newton's laws</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=623" target="_blank">00:10:23.920</a></span> | <span class="t">of physics and you'd get a very exact result. But they don't actually have the computational</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=629" target="_blank">00:10:29.360</a></span> | <span class="t">bandwidth to perform those kinds of calculations. Instead, it's a bit more like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=633" target="_blank">00:10:33.520</a></span> | <span class="t">When models are tasked with 226, take away 68, they get this vibe. That kind of feels like an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=640" target="_blank">00:10:40.560</a></span> | <span class="t">answer between 150 and 180. That's one of the heuristics or rules of thumb that these authors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=647" target="_blank">00:10:47.280</a></span> | <span class="t">studied. Patch together enough of these vibes or heuristics and you start getting pretty accurate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=652" target="_blank">00:10:52.240</a></span> | <span class="t">answers most of the time. Each heuristic they learn only slightly boosts the correct answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=658" target="_blank">00:10:58.080</a></span> | <span class="t">logic. But combined, they cause the model to produce the correct answer with high probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=663" target="_blank">00:11:03.840</a></span> | <span class="t">not reliability. Indeed, their results suggest that improving LLM's mathematical abilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=670" target="_blank">00:11:10.240</a></span> | <span class="t">may require fundamental changes to training and architectures. And I totally get it. This is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=675" target="_blank">00:11:15.840</a></span> | <span class="t">same video that I showed you the O1 model getting 83% in an exceptionally hard math competition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=682" target="_blank">00:11:22.400</a></span> | <span class="t">But you may have noticed that none of these models tend to get 100% in anything. For example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=687" target="_blank">00:11:27.360</a></span> | <span class="t">surely if they can solve 93% of PhD level physics problems, why do they only get 81% in AP physics?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=695" target="_blank">00:11:35.920</a></span> | <span class="t">That's the same O1 model that we're set to get in the next 12 days. I almost can't help myself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=700" target="_blank">00:11:40.880</a></span> | <span class="t">I'm starting to cover the two papers that I said I would cover another day. But still,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=705" target="_blank">00:11:45.120</a></span> | <span class="t">I just want to touch on this other paper. The ridiculously compressed TLDR is that they show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=710" target="_blank">00:11:50.000</a></span> | <span class="t">that models do learn procedures rather than memorizing individual answers. The way they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=715" target="_blank">00:11:55.280</a></span> | <span class="t">show this is really complex and does rely on some approximations. Estimating, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=720" target="_blank">00:12:00.720</a></span> | <span class="t">if you remove these 500 tokens, how would that affect the model parameters and therefore the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=725" target="_blank">00:12:05.360</a></span> | <span class="t">likelihood of getting an answer right? You can then in short judge which kind of sources the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=729" target="_blank">00:12:09.840</a></span> | <span class="t">model is relying on for particular types of questions. Again, what the authors are showing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=734" target="_blank">00:12:14.160</a></span> | <span class="t">is that the models aren't memorizing particular answers to reasoning questions. Like when asked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=738" target="_blank">00:12:18.960</a></span> | <span class="t">what is 7-4 in brackets times 7, they're not looking up a source that says the answer of 21.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=745" target="_blank">00:12:25.440</a></span> | <span class="t">They're relying on multiple sources that give the kind of procedures you'd need to answer that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=751" target="_blank">00:12:31.040</a></span> | <span class="t">question. But while that seems really promising for these models developing world models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=755" target="_blank">00:12:35.920</a></span> | <span class="t">and true reasoning, they add this crucial caveat. They don't find evidence for models generalizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=762" target="_blank">00:12:42.080</a></span> | <span class="t">from pre-training data about one type of reasoning to another similar type of reasoning. You could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=767" target="_blank">00:12:47.840</a></span> | <span class="t">kind of think of that like a model getting fairly good at simulating the physics of the moon but not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=774" target="_blank">00:12:54.080</a></span> | <span class="t">then applying that when asked to simulate the physics of Mars. In-distribution generalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=779" target="_blank">00:12:59.600</a></span> | <span class="t">versus out-of-distribution generalization. Anyway, I've definitely gone on too long,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=784" target="_blank">00:13:04.000</a></span> | <span class="t">time to bring us back to the real world. And before I end with that cute turtle and how you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=788" target="_blank">00:13:08.560</a></span> | <span class="t">can move it around, here is another real-world tool that you can use today. This is Assembly AI's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=794" target="_blank">00:13:14.640</a></span> | <span class="t">Universal 2 speech-to-text model and you can see its performance here. As many of you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=799" target="_blank">00:13:19.920</a></span> | <span class="t">I reached out to Assembly AI and they are kindly sponsoring this video. I use their models to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=805" target="_blank">00:13:25.120</a></span> | <span class="t">transcribe my projects and you can see the comparison not just with Universal 1 but with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=810" target="_blank">00:13:30.560</a></span> | <span class="t">other competitive models. One thing I've learned in doing so is don't always focus on word error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=815" target="_blank">00:13:35.680</a></span> | <span class="t">rate. Think about how models perform with proper nouns and alphanumerics. That is at least for me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=821" target="_blank">00:13:41.280</a></span> | <span class="t">what sets the Universal family apart. Now, as we wrap up this video, just for anyone wondering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=826" target="_blank">00:13:46.320</a></span> | <span class="t">about an update to SimpleBench. First, what about the new Gemini experimental models? Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=831" target="_blank">00:13:51.760</a></span> | <span class="t">they are rate-limited. I might soon be getting early access to something else but for now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=837" target="_blank">00:13:57.040</a></span> | <span class="t">we can't run it in full on SimpleBench. What about DeepSeq R1? Well, again, as of today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=842" target="_blank">00:14:02.400</a></span> | <span class="t">not available through the API. What though about Alibaba's QWQ model? That has been getting a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=848" target="_blank">00:14:08.640</a></span> | <span class="t">of hype lately but honestly, what's new? Almost everything gets hyped in AI. Of course, I am</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=854" target="_blank">00:14:14.160</a></span> | <span class="t">following all of the models coming out of China and testing them as much as I possibly can and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=858" target="_blank">00:14:18.960</a></span> | <span class="t">I did read that interview with the founder of DeepSeq. I may cover that in a different video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=863" target="_blank">00:14:23.840</a></span> | <span class="t">but for now, I could actually run QWQ on SimpleBench and unfortunately, it got a score below</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=870" target="_blank">00:14:30.320</a></span> | <span class="t">Claude 3.5 Haiku so it doesn't appear on the list. I'm sorry that I can't do a shocked-faced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=875" target="_blank">00:14:35.920</a></span> | <span class="t">thumbnail and say that AGI has arrived but that's just the result we got. It was around 11%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=881" target="_blank">00:14:41.040</a></span> | <span class="t">I'm going to show you one more tool just very quickly and you can use it for free today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=886" target="_blank">00:14:46.080</a></span> | <span class="t">It's Kling 1.5, actually another model coming out of China and you could argue, in a way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=891" target="_blank">00:14:51.200</a></span> | <span class="t">it's a foretaste of the kind of interactivity that something like Genie 2 will bring. Again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=896" target="_blank">00:14:56.160</a></span> | <span class="t">free to sign up for at least 5 professional generations. Click on the left to upload an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=901" target="_blank">00:15:01.760</a></span> | <span class="t">image. I generated this one with Ideagram. Then go down to Motion Brush. Then I selected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=907" target="_blank">00:15:07.680</a></span> | <span class="t">Auto Segmentation so I could pick out the turtle and then for Tracking, I drew this arrow to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=914" target="_blank">00:15:14.000</a></span> | <span class="t">right. Then Confirm of course. Go to Professional Mode. I only have 2 trial uses left and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=919" target="_blank">00:15:19.840</a></span> | <span class="t">Generate. You control the movement and you can end up with super cute generations like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=jIm2T7h_a0M&t=925" target="_blank">00:15:25.360</a></span> | <span class="t">So whatever you found the most interesting, thank you for watching and have a wonderful day.</span></div></div></body></html>