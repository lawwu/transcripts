<html><head><title>How OpenAI's Sora model is DIFFERENT #ai #openai</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>How OpenAI's Sora model is DIFFERENT #ai #openai</h2><a href="https://www.youtube.com/watch?v=_59FVxZf-nM" target="_blank"><img src="https://i.ytimg.com/vi/_59FVxZf-nM/sd2.jpg?sqp=-oaymwEoCIAFEOAD8quKqQMcGADwAQH4AbYIgAKAD4oCDAgAEAEYWCBcKGUwDw==&rs=AOn4CLBiyIRiIO7h2lkZ9AXsJDkYx2zw3g" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>if you haven't seen it, Sora is the text video model from open AI. And this, if you weren't studying it, would look like a major feature film. The traditional approach for rendering video is you create three dimensional objects. And then you have a rendering engine that renders those objects. And then you have a system that defines where the camera goes. And that's how you get the visual that you use to generate a 2d movie like this. This doesn't do that. This was a trained model. So how would you train a model to do this without having a 3d space, the compute necessary to define each of those objects, place them in 3d space is practically impossible today, my guess is that open AI used a tool like Unreal Engine five and generated tons and tons of video content, tagged it labeled it. And we're then able to use that to train this model that can for whatever reason that we don't understand, do this. You're referring to synthetic training data.</p></div></div></body></html>