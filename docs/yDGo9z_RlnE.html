<html><head><title>Sentiment Analysis on ANY Length of Text With Transformers (Python)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Sentiment Analysis on ANY Length of Text With Transformers (Python)</h2><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE"><img src="https://i.ytimg.com/vi_webp/yDGo9z_RlnE/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=3">0:3</a> apply sentiment analysis to longer pieces of text<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=179">2:59</a> using bert for sequence classification<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=184">3:4</a> import the four sequence classification model<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=230">3:50</a> open the plug-in phase transformers<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=247">4:7</a> filter by text classification<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=338">5:38</a> add special tokens<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=372">6:12</a> add 412 padding tokens<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=589">9:49</a> split each of our tensors<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=598">9:58</a> split those into chunks of length 510<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=658">10:58</a> split our tensor in two batches<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=676">11:16</a> print out the length of each one of our tensors<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=756">12:36</a> pass a list of all the tensors<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=862">14:22</a> add the cls and separator<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=975">16:15</a> check the length of each one<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1114">18:34</a> print out the length of each one of those tenses<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1228">20:28</a> printing out the input id chunks<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1248">20:48</a> stack our input ids and attention mass tensors<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1430">23:50</a> add softmax onto the end<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1489">24:49</a> take a softmax function across each one of these outputs<br><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1585">26:25</a> take the odd maps of the mean<br><br><div style="text-align: left;"><a href="./yDGo9z_RlnE.html">Whisper Transcript</a> | <a href="./transcript_yDGo9z_RlnE.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">In this video, we're going to take a look at how we can apply sentiment analysis to longer pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=7" target="_blank">00:00:07.360</a></span> | <span class="t">of text. So if you've done this sort of thing before, in particular with transformers or even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=12" target="_blank">00:00:12.960</a></span> | <span class="t">LSTMs or any other architecture in NLP, you will find that we have an upper limit on the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=19" target="_blank">00:00:19.840</a></span> | <span class="t">words that we can consider at once. In this tutorial, we're going to be using BERT, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=25" target="_blank">00:00:25.440</a></span> | <span class="t">is a transformer model. And at max, that consumes 512 tokens. Anything beyond that is just truncated,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=33" target="_blank">00:00:33.120</a></span> | <span class="t">so we don't consider anything beyond that limit. Now, in a lot of cases, maybe, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=39" target="_blank">00:00:39.760</a></span> | <span class="t">you're analyzing sentiment tweets. It's not a problem. But when we start to look at maybe news</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=46" target="_blank">00:00:46.800</a></span> | <span class="t">articles or Reddit posts, they can be quite a bit longer than just 512 tokens. And when I say tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=55" target="_blank">00:00:55.200</a></span> | <span class="t">tokens, typically maps to words or punctuation. So what I want to explore in this video is how we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=63" target="_blank">00:01:03.200</a></span> | <span class="t">actually remove that limitation and just consume as many tokens or words as we'd like and still get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=71" target="_blank">00:01:11.280</a></span> | <span class="t">an accurate sentiment score whilst considering the full length of text. And at a high level,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=78" target="_blank">00:01:18.400</a></span> | <span class="t">this is essentially what we are going to be doing. We're going to be taking the original tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=84" target="_blank">00:01:24.560</a></span> | <span class="t">which is the 1361 tokens, and we're going to split into different chunks. So we have chunk one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=90" target="_blank">00:01:30.880</a></span> | <span class="t">chunk two, and chunk three here. Now, we want most of these chunks or all of these chunks in the end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=97" target="_blank">00:01:37.120</a></span> | <span class="t">are going to be 512 tokens long. And you can see with chunk one, chunk two, they are 512 already.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=106" target="_blank">00:01:46.640</a></span> | <span class="t">However, of course, 1361 can't be evenly split into 512. So the final chunk will be shorter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=115" target="_blank">00:01:55.200</a></span> | <span class="t">And once we have split those into chunks, we will need to add padding, we need to add the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=122" target="_blank">00:02:02.960</a></span> | <span class="t">start sequence and separated tokens. If that is new to you, then don't worry,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=127" target="_blank">00:02:07.120</a></span> | <span class="t">we'll explain that very soon. And then we calculate the sentiment for each one of those,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=131" target="_blank">00:02:11.520</a></span> | <span class="t">take the average, and then use that as a sentiment prediction for the entire text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=136" target="_blank">00:02:16.880</a></span> | <span class="t">And that's essentially a high level what we're going to be doing. But that is much easier said</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=142" target="_blank">00:02:22.400</a></span> | <span class="t">than done. So let's just jump straight into the code and I'll show you how we actually do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=147" target="_blank">00:02:27.840</a></span> | <span class="t">Okay, what we have here is a post from the investing subreddit. It's pretty long,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=154" target="_blank">00:02:34.640</a></span> | <span class="t">I think it's something like 1300 tokens when we tokenize it. And obviously, that is far beyond the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=162" target="_blank">00:02:42.240</a></span> | <span class="t">512 token limit that we have with BERT. So if we want to consider the full text, we obviously have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=170" target="_blank">00:02:50.800</a></span> | <span class="t">to do something different. And first thing that I think we want to do is actually initialize our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=178" target="_blank">00:02:58.000</a></span> | <span class="t">model and tokenize it. Because we're using BERT for sequence classification, we will import the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=185" target="_blank">00:03:05.280</a></span> | <span class="t">BERT for sequence classification model or class. And we are importing that from the Transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=192" target="_blank">00:03:12.000</a></span> | <span class="t">library. So that is going to be our model class. And then we also need the tokenizer as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=206" target="_blank">00:03:26.800</a></span> | <span class="t">which is just a generic BERT tokenizer. So those two are our imports. And then we actually need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=213" target="_blank">00:03:33.520</a></span> | <span class="t">initialize the tokenizer and the model. So the BERT tokenizer is pretty straightforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=221" target="_blank">00:03:41.280</a></span> | <span class="t">And then we are going from pre-trained. So we're using a pre-trained model here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=229" target="_blank">00:03:49.840</a></span> | <span class="t">And if we just open the HuggingFace Transformers models page, so HuggingFace.co/models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=238" target="_blank">00:03:58.400</a></span> | <span class="t">And we can head over here and we can actually search for the model that we'd like to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=244" target="_blank">00:04:04.960</a></span> | <span class="t">We're doing text classification, so we head over here and filter by text classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=250" target="_blank">00:04:10.080</a></span> | <span class="t">And then the investing subreddit is basically full of financial advice. So we really want to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=257" target="_blank">00:04:17.520</a></span> | <span class="t">if possible, use a more financially savvy BERT model, which we can find with FinBERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=265" target="_blank">00:04:25.520</a></span> | <span class="t">And we have two options for FinBERT here. I'm going to go with the ProcessAI FinBERT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=270" target="_blank">00:04:30.400</a></span> | <span class="t">And all we actually need is this text here. We go back to our code and we'll just enter it here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=277" target="_blank">00:04:37.520</a></span> | <span class="t">So process, we want slash, and we all just want this on the same line, like that. And we're also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=285" target="_blank">00:04:45.520</a></span> | <span class="t">going to be using the same model for our BERT for sequence classification. So BERT sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=298" target="_blank">00:04:58.400</a></span> | <span class="t">classification. And we do the from pre-trained ProcessAI FinBERT again. And that's all we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=306" target="_blank">00:05:06.560</a></span> | <span class="t">to do to actually initialize our model and tokenizer. And now we're ready to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=311" target="_blank">00:05:11.280</a></span> | <span class="t">tokenize that input text. So when it comes to tokenizing input text, for those of you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=316" target="_blank">00:05:16.960</a></span> | <span class="t">that have worked with transformers before, it typically looks something like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=322" target="_blank">00:05:22.240</a></span> | <span class="t">So we write tokens or whichever variable name you'd like to use. We use tokenizer, encode plus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=334" target="_blank">00:05:34.640</a></span> | <span class="t">We pass our text here. We add special tokens. So this is the CLS,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=343" target="_blank">00:05:43.760</a></span> | <span class="t">separated tokens, padding tokens. So anything from this list here. So all these tokens are used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=355" target="_blank">00:05:55.840</a></span> | <span class="t">specifically within BERT for different purposes. So we have padding token, which we use when a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=361" target="_blank">00:06:01.840</a></span> | <span class="t">sequence is too short. So BERT always requires that we have 512 tokens within our inputs. If we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=370" target="_blank">00:06:10.160</a></span> | <span class="t">are feeding in 100 tokens, then we add 412 padding tokens to fill that empty space. Unknown is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=378" target="_blank">00:06:18.720</a></span> | <span class="t">when a word is unknown to BERT. And then we have the CLS token here. And this appears at the start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=384" target="_blank">00:06:24.640</a></span> | <span class="t">of every sequence. And the token ID for this is 101. So we'll be using this later. So it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=391" target="_blank">00:06:31.040</a></span> | <span class="t">important to remember that number. And then we also have the SEP token, which indicates the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=395" target="_blank">00:06:35.360</a></span> | <span class="t">separator, which indicates the point between our input text and the padding. Or if there is no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=402" target="_blank">00:06:42.800</a></span> | <span class="t">padding, it would just indicate the end of the text. And they're the only ones that we really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=407" target="_blank">00:06:47.600</a></span> | <span class="t">need to be concerned about. So typically, we have those special tokens in there because BERT does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=414" target="_blank">00:06:54.480</a></span> | <span class="t">need them. We specify a max length, which is the 512 tokens that BERT would expect. And then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=422" target="_blank">00:07:02.320</a></span> | <span class="t">say anything beyond that we want to truncate. And anything below that we want to pad up to the max</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=429" target="_blank">00:07:09.280</a></span> | <span class="t">length. And this is typically what our tokens will look like. So now we have, it's a dictionary,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=439" target="_blank">00:07:19.040</a></span> | <span class="t">we have input IDs. We have this token type IDs, which we don't need to worry about. And we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=444" target="_blank">00:07:24.560</a></span> | <span class="t">the attention mass. And that's typically what we would do. But in this case, we are doing things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=451" target="_blank">00:07:31.680</a></span> | <span class="t">slightly different. Because one, we don't want to add those special tokens immediately. Because if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=456" target="_blank">00:07:36.640</a></span> | <span class="t">we add this special token, we have a CLS or start of sentence token. And then we also have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=463" target="_blank">00:07:43.200</a></span> | <span class="t">separate token at the end and start of our tensor. And we don't want that because we're going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=470" target="_blank">00:07:50.880</a></span> | <span class="t">splitting our tens up into three smaller tensors. So we actually don't want to add those yet,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=476" target="_blank">00:07:56.480</a></span> | <span class="t">we're going to add those manually later. And then we also have this maximum truncation and padding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=481" target="_blank">00:08:01.520</a></span> | <span class="t">Obviously, we actually don't want to be using any of these because if we truncate our 1300 token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=489" target="_blank">00:08:09.680</a></span> | <span class="t">text into just 512, then that's just what we would normally do. We're not actually considering the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=495" target="_blank">00:08:15.440</a></span> | <span class="t">whole text, we're just considering the first 512 tokens. So clearly, we also don't want any of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=501" target="_blank">00:08:21.440</a></span> | <span class="t">those variables in there. In our case, we actually do something slightly different. We still use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=507" target="_blank">00:08:27.600</a></span> | <span class="t">ENCODE plus method. So tokenizer, ENCODE plus. We also include text. This time, we want to specify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=523" target="_blank">00:08:43.440</a></span> | <span class="t">that we don't want to add those special tokens. So we set that to false. And that's actually it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=531" target="_blank">00:08:51.920</a></span> | <span class="t">we don't want to include any of those other arguments in there. The only extra parameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=536" target="_blank">00:08:56.800</a></span> | <span class="t">that we do want to add, which we want to add whenever we're working with PyTorch, is we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=543" target="_blank">00:09:03.200</a></span> | <span class="t">to add return tensors equals PT. And this just tells the tokenizer to return PyTorch tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=553" target="_blank">00:09:13.760</a></span> | <span class="t">Whereas here, what we had are actually just simple Python lists. And if we're using TensorFlow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=561" target="_blank">00:09:21.520</a></span> | <span class="t">we switch this over to TF. In our case, using PyTorch. And let's just see what that gives us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=569" target="_blank">00:09:29.840</a></span> | <span class="t">Okay, so here we get a warning about the sequence length. And that's fine, because we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=576" target="_blank">00:09:36.400</a></span> | <span class="t">deal with that later. And then in here, we can see, okay, now we have PyTorch tensors rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=582" target="_blank">00:09:42.720</a></span> | <span class="t">than the list that we had before, which is great, that's what we want. Now we have that, we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=588" target="_blank">00:09:48.720</a></span> | <span class="t">want to split each of our tensors, or the input IDs and the attention mass tensors, we don't need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=595" target="_blank">00:09:55.200</a></span> | <span class="t">to do anything with the token type IDs, we can get rid of those. We want to split those into chunks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=601" target="_blank">00:10:01.520</a></span> | <span class="t">of length 510. So the reason we're using 510, rather than 512, is because at the moment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=609" target="_blank">00:10:09.280</a></span> | <span class="t">we don't have our CLS and separator tokens in there. So once we do add those, that will push</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=615" target="_blank">00:10:15.440</a></span> | <span class="t">the 510 up to 512. So to split those into those chunks, it's actually incredibly easy. So we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=624" target="_blank">00:10:24.640</a></span> | <span class="t">just write input ID chunks. And we need to access our tokens dictionary. So tokens, and then we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=635" target="_blank">00:10:35.600</a></span> | <span class="t">to access the input IDs here. And you'll see here that this is actually a tensor, it's almost like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=642" target="_blank">00:10:42.720</a></span> | <span class="t">a list within a list. So to access that, we want to access a zero index of that. And then we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=649" target="_blank">00:10:49.920</a></span> | <span class="t">just going to split, which is a PyTorch method by 510. And that is literally all we need to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=657" target="_blank">00:10:57.680</a></span> | <span class="t">to split our tensor into batches. And we repeat this again, but for the mask,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=667" target="_blank">00:11:07.520</a></span> | <span class="t">and just changes to attention mask. Again, we don't need token type IDs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=674" target="_blank">00:11:14.560</a></span> | <span class="t">so we can just ignore that. And then let's just print out the length of each one of our tensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=680" target="_blank">00:11:20.160</a></span> | <span class="t">here. So for tensor, and input ID chunks, just print the length of it. So we can check that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=691" target="_blank">00:11:31.600</a></span> | <span class="t">are actually doing this correctly. So we can see we have 510, 510. And the last one is shorter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=700" target="_blank">00:11:40.080</a></span> | <span class="t">of course, like we explained before, at 325. So that's pretty ideal, that's what we want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=708" target="_blank">00:11:48.080</a></span> | <span class="t">And now we can move on to adding in our CLS and separate tokens. I'll just show you how this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=715" target="_blank">00:11:55.920</a></span> | <span class="t">going to work. So I'm going to use a smaller tensor quickly, just as an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=723" target="_blank">00:12:03.520</a></span> | <span class="t">So we just need to also import torch. So we do that here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=739" target="_blank">00:12:19.840</a></span> | <span class="t">Okay, so we have this tensor. And to add a value on either side of that, we can use the torch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=746" target="_blank">00:12:26.560</a></span> | <span class="t">cat method, which is for concatenating multiple tensors. In this case, we'd use torch cat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=754" target="_blank">00:12:34.160</a></span> | <span class="t">And then we just pass a list of all the tensors that we would like to include here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=760" target="_blank">00:12:40.720</a></span> | <span class="t">Now, we don't have a tensor for our token, so we just create it within this list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=768" target="_blank">00:12:48.480</a></span> | <span class="t">And that's very easy, we just use torch tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=771" target="_blank">00:12:51.520</a></span> | <span class="t">And then if you remember before, the CLS token is the equivalent of 101, when it's converted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=780" target="_blank">00:13:00.880</a></span> | <span class="t">to the token ID. So that's going to come at the start of our tensor. And in the middle,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=786" target="_blank">00:13:06.880</a></span> | <span class="t">we have our actual tensor. And at the end, we want to append our 102 tensor, which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=795" target="_blank">00:13:15.280</a></span> | <span class="t">separator token. Okay, and we just print that out, we can see, okay, we've got 101,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=804" target="_blank">00:13:24.320</a></span> | <span class="t">and then we have our sequence and 102 at the end. Then after we add our CLS and separator tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=812" target="_blank">00:13:32.560</a></span> | <span class="t">we will use the same method for our padding as well. But we want to write this logic within a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=819" target="_blank">00:13:39.360</a></span> | <span class="t">for loop, which will iterate through each chunk and process each one individually. So first,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=826" target="_blank">00:13:46.080</a></span> | <span class="t">I'm going to create a variable to define the chunk size, which is going to be 512, which is our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=834" target="_blank">00:13:54.640</a></span> | <span class="t">target size. And we already split our tokens into chunks up here. So we can just iterate through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=844" target="_blank">00:14:04.240</a></span> | <span class="t">each one of those. So we'll just go through a range of the length of the number of chunks that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=852" target="_blank">00:14:12.640</a></span> | <span class="t">we have, this will go 0, 1, and 2. And now we can access each chunk using the I index here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=860" target="_blank">00:14:20.640</a></span> | <span class="t">So first, we want to add the CLS and separator tokens, just like we have above. So to do that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=869" target="_blank">00:14:29.520</a></span> | <span class="t">we go input ID chunks, we get the current index, and then just do torch cat, which is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=881" target="_blank">00:14:41.360</a></span> | <span class="t">concatenate. And then we pass a list just like we did before, which is going to be torch tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=890" target="_blank">00:14:50.560</a></span> | <span class="t">And then in the middle, we have A, we're going to replace that with this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=898" target="_blank">00:14:58.480</a></span> | <span class="t">Okay, and then we want to do the same for our attention mask. But of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=903" target="_blank">00:15:03.040</a></span> | <span class="t">in our attention mask, if we look up here, it's just full of ones. And the only two values that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=909" target="_blank">00:15:09.760</a></span> | <span class="t">we can actually have in our attention mask is either 1 or 0. And the reason for this is whenever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=917" target="_blank">00:15:17.040</a></span> | <span class="t">we have a real token that Bert needs to pay attention to, we have a 1 in this attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=924" target="_blank">00:15:24.320</a></span> | <span class="t">mask. Whereas if you have a padding token, that will correspond to a 0 in this attention mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=930" target="_blank">00:15:30.880</a></span> | <span class="t">And the reason for this is just so Bert doesn't process attention for the padding tokens within</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=938" target="_blank">00:15:38.800</a></span> | <span class="t">our inputs. So it's essentially like telling Bert to just ignore the padding. So in our case here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=945" target="_blank">00:15:45.680</a></span> | <span class="t">both of these are not padding tokens. So both of them should be 1. Okay, and then that gets us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=954" target="_blank">00:15:54.480</a></span> | <span class="t">our sequences with the CLS separator and added attention mask tokens in there. So now we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=963" target="_blank">00:16:03.040</a></span> | <span class="t">to do the padding. And realistically with padding, we're actually only going to do that for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=968" target="_blank">00:16:08.160</a></span> | <span class="t">final tensor. So what we will do to make sure that we don't try and pad the other tensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=975" target="_blank">00:16:15.440</a></span> | <span class="t">is just check the length of each one. First, we'll calculate the required padding length,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=983" target="_blank">00:16:23.120</a></span> | <span class="t">which is just going to be equal to the chunk size minus the input ID chunk. And then we want the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=995" target="_blank">00:16:35.040</a></span> | <span class="t">index shape 0. So this is like taking the length of the tensor. Okay, and for chunks 1 and 2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1003" target="_blank">00:16:43.680</a></span> | <span class="t">this will just be equal to 0. Whereas for the final chunk, it will not, it will be something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1008" target="_blank">00:16:48.480</a></span> | <span class="t">like 150 or 200. So what we want to do is say, if the pad length is greater than 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1018" target="_blank">00:16:58.240</a></span> | <span class="t">then this is where we add our padding tokens. So first, we'll do the input ID chunk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1026" target="_blank">00:17:06.160</a></span> | <span class="t">And again, we're just going to use the torch concatenate method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1033" target="_blank">00:17:13.040</a></span> | <span class="t">This time, we have our input ID chunk at the start. I think it's chunks, not chunk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1045" target="_blank">00:17:25.040</a></span> | <span class="t">And also here, this should be mask chunks. So let's just fix that quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1057" target="_blank">00:17:37.440</a></span> | <span class="t">Okay. And here, we first have this, and then the parts following this need to be our padding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1070" target="_blank">00:17:50.560</a></span> | <span class="t">tokens. And to create those, we are going to do the torch tensor again. And then in here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1079" target="_blank">00:17:59.440</a></span> | <span class="t">we're going to just add one zero in a list. But then we're going to multiply that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1085" target="_blank">00:18:05.200</a></span> | <span class="t">by the pad length. So if the pad length is 100, this will give us a tensor that has 100 zeros</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1095" target="_blank">00:18:15.520</a></span> | <span class="t">inside it, which is exactly what we want. And then we'll copy and paste this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1102" target="_blank">00:18:22.000</a></span> | <span class="t">and do the exact same thing for our masking tensor as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1105" target="_blank">00:18:25.760</a></span> | <span class="t">Okay. So now let's just print out the length of each one of those tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1119" target="_blank">00:18:39.280</a></span> | <span class="t">So for chunk and input ID chunks, print the length of that chunk. And then we'll also just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1131" target="_blank">00:18:51.760</a></span> | <span class="t">print out the final chunk as well, so we can see everything is in the right place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1136" target="_blank">00:18:56.800</a></span> | <span class="t">And here, so just copy. So this here needs to have an S on the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1145" target="_blank">00:19:05.840</a></span> | <span class="t">Oh, and up here. So when we first build these, so if I just print out one of them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1154" target="_blank">00:19:14.640</a></span> | <span class="t">you see that the input ID chunks is actually a tuple containing three of our tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1164" target="_blank">00:19:24.640</a></span> | <span class="t">So what we actually want to do, I'll just close that, is before we start this whole process,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1174" target="_blank">00:19:34.800</a></span> | <span class="t">we just want to convert them into lists so that we can actually change the values inside. Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1181" target="_blank">00:19:41.680</a></span> | <span class="t">otherwise we are trying to change the values of a tuple, which we obviously can't because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1186" target="_blank">00:19:46.640</a></span> | <span class="t">tuples are immutable in Python, which means you can't change the values inside them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1192" target="_blank">00:19:52.560</a></span> | <span class="t">So we just convert those to lists. And then we also need to add an S on here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1201" target="_blank">00:20:01.840</a></span> | <span class="t">And there we go. We finally got there. So now we can see, okay, here we have 514.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1209" target="_blank">00:20:09.360</a></span> | <span class="t">So let me just rerun this bit here. And then rerun this. Okay. So it's because I was running</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1217" target="_blank">00:20:17.840</a></span> | <span class="t">it twice. It was adding these twice. So now we have 512. And then we can see we have our tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1228" target="_blank">00:20:28.080</a></span> | <span class="t">So this is just printing out the input ID chunks. You can see here we have all these values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1233" target="_blank">00:20:33.120</a></span> | <span class="t">And this is just the final one. So you can see at the bottom we have this padding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1239" target="_blank">00:20:39.040</a></span> | <span class="t">If we go up here, we have our starter sequence token 101. And down here we have the end of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1244" target="_blank">00:20:44.800</a></span> | <span class="t">sequence separator. So now what we want to do is stack our input IDs and attention mass tensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1252" target="_blank">00:20:52.240</a></span> | <span class="t">together. So we'll create input IDs. We use torch stack for that. And that's going to be input ID</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1263" target="_blank">00:21:03.040</a></span> | <span class="t">chunks. And then we also have the attention mask that we need to create. So we do the same thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1270" target="_blank">00:21:10.480</a></span> | <span class="t">there. And that is the mask chunks. And then the format that BERT expects us to be feeding in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1281" target="_blank">00:21:21.120</a></span> | <span class="t">data is a dictionary where we have key value pairs. So we have a key input IDs, which will lead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1290" target="_blank">00:21:30.720</a></span> | <span class="t">to our input IDs tensor here. And then another one called attention mask that will have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1296" target="_blank">00:21:36.880</a></span> | <span class="t">attention mask as its value. So we'll just define that here. And this is just the format that BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1307" target="_blank">00:21:47.200</a></span> | <span class="t">expects. So the input IDs. And then we have the input IDs there. And then we also have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1314" target="_blank">00:21:54.560</a></span> | <span class="t">attention mask. We have the attention mask in there. Now, as well as that, BERT expects these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1324" target="_blank">00:22:04.880</a></span> | <span class="t">tensors to be in a particular format. So the input IDs expects it to be in a long format. So we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1331" target="_blank">00:22:11.440</a></span> | <span class="t">add long onto the end of there. And then for the attention mask, we expect integers. So we just add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1337" target="_blank">00:22:17.600</a></span> | <span class="t">int onto the end of there. And then we just print out input dict. So we can see what we are putting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1344" target="_blank">00:22:24.240</a></span> | <span class="t">in there. OK, great. So that is exactly the format that we need. Now we can get our outputs. So we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1352" target="_blank">00:22:32.720</a></span> | <span class="t">pass these into our model as keyword arguments. So we just add these two asterisk symbols.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1359" target="_blank">00:22:39.280</a></span> | <span class="t">That means it's a keyword argument. And then in there, we pass our input dict. And this allows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1367" target="_blank">00:22:47.120</a></span> | <span class="t">the function to read these keywords, take them as variables, and assign these tensors to them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1374" target="_blank">00:22:54.800</a></span> | <span class="t">So there we have our outputs. You can see here that we have these logits. These are our activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1384" target="_blank">00:23:04.880</a></span> | <span class="t">from the final layer of the BERT model. And you see, OK, we have these values. What we want in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1391" target="_blank">00:23:11.840</a></span> | <span class="t">the end is a set of probabilities. And of course, this is not a set of probabilities, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1397" target="_blank">00:23:17.680</a></span> | <span class="t">probabilities we would expect to be between the values of 0 and 1. Here we have negatives. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1402" target="_blank">00:23:22.480</a></span> | <span class="t">have values that are over 1. And that's not really what we would expect. So to convert these into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1410" target="_blank">00:23:30.000</a></span> | <span class="t">probabilities, all we need to do is apply a softmax function to them. Now softmax is essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1419" target="_blank">00:23:39.200</a></span> | <span class="t">sigmoid but applied across a set of categorical or output classes. And to implement that, we just do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1427" target="_blank">00:23:47.760</a></span> | <span class="t">torch and then functional. And then we just add softmax onto the end there. And we need to access</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1435" target="_blank">00:23:55.600</a></span> | <span class="t">the output logits, which is in index 0 of the outputs variable. So that is just accessing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1442" target="_blank">00:24:02.320</a></span> | <span class="t">this tensor here. And then we access dimension minus 1. So the dimension negative 1 is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1451" target="_blank">00:24:11.840</a></span> | <span class="t">accessing the final dimension of our tensor. So in this case, we have a 3D tensor. So this is like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1459" target="_blank">00:24:19.360</a></span> | <span class="t">accessing the second dimension or dimension number 2. Because when we have 3D tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1466" target="_blank">00:24:26.080</a></span> | <span class="t">we have dimensions 0, 1, and 2. Minus 1 of 0 is just the dimension 2, if that makes sense. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1475" target="_blank">00:24:35.040</a></span> | <span class="t">imagine we have 0, 1, and 2 here. If we go here and we take negative 1, we come around here to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1484" target="_blank">00:24:44.000</a></span> | <span class="t">the back of the list. And that is accessing the second dimension. So that is going to take a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1490" target="_blank">00:24:50.560</a></span> | <span class="t">softmax function across each one of these outputs. And then we can print that out. So now we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1501" target="_blank">00:25:01.440</a></span> | <span class="t">our probabilities. So the outputs of the FinBert model, these ones here in the first column are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1509" target="_blank">00:25:09.520</a></span> | <span class="t">all positive. So this is the prediction of the chunks having a positive sentiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1515" target="_blank">00:25:15.200</a></span> | <span class="t">These are all negative. So the prediction of the chunk having a negative sentiment. And these are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1521" target="_blank">00:25:21.440</a></span> | <span class="t">all neutral. So if it has a neutral sentiment. So we see here, the first and second chunks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1528" target="_blank">00:25:28.560</a></span> | <span class="t">are both predicted to have a negative sentiment, particularly the first one. And the final one is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1534" target="_blank">00:25:34.640</a></span> | <span class="t">predicted to have a positive sentiment. Now if we want to get the overall prediction, all we do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1541" target="_blank">00:25:41.520</a></span> | <span class="t">take the mean. So the probabilities. And we just want to take the mean. And we take that in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1547" target="_blank">00:25:47.920</a></span> | <span class="t">0 dimension, which would just go from here down, take the mean of those three, take the mean of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1555" target="_blank">00:25:55.040</a></span> | <span class="t">these three, and take the mean of these three as well. Print it out. And you see here, negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1563" target="_blank">00:26:03.040</a></span> | <span class="t">sentiment is definitely winning here. But only just, it's pretty close to the positive. So it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1568" target="_blank">00:26:08.160</a></span> | <span class="t">reasonably difficult one to understand. And this is because over here, we have mostly negative,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1575" target="_blank">00:26:15.360</a></span> | <span class="t">kind of negative, and most positive. So it's a bit of a difficult one. But negative sentiment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1580" target="_blank">00:26:20.320</a></span> | <span class="t">does win out in the end. Now if you'd like to get the specific category that won, we'll just take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1586" target="_blank">00:26:26.560</a></span> | <span class="t">the arg maps of the mean. And that will give us a tensor. If we want to actually get the value out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1594" target="_blank">00:26:34.480</a></span> | <span class="t">of that tensor, we can just add item onto the end there. And that is it. We have taken the average</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1600" target="_blank">00:26:40.640</a></span> | <span class="t">sentiment of a pretty long piece of text. And of course, we can just use this code and iterate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1607" target="_blank">00:26:47.120</a></span> | <span class="t">through it for multiple long pieces of text. And it doesn't really matter how long those pieces of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1612" target="_blank">00:26:52.960</a></span> | <span class="t">text are. This will still work. So I hope this has been an interesting and useful video for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1620" target="_blank">00:27:00.880</a></span> | <span class="t">I've definitely enjoyed working through this and figuring it all out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yDGo9z_RlnE&t=1625" target="_blank">00:27:05.040</a></span> | <span class="t">So thank you very much for watching. And I will see you again in the next one.</span></div></div></body></html>