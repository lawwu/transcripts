<html><head><title>If an LLM solves this then we'll probably have AGI – Francois Chollet</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>If an LLM solves this then we'll probably have AGI – Francois Chollet</h2><a href="https://www.youtube.com/watch?v=wZ0ToxtXz5g" target="_blank"><img src="https://i.ytimg.com/vi_webp/wZ0ToxtXz5g/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>So suppose that it's the case that in a year, a multimodal model can solve ARC, let's say get 80% or whatever the average human would get, then AGI? Quite possibly, yes. I think if you start, so honestly, what I would like to see is an LLM type model solving ARC at like 80%, but only trained on information that is not explicitly trying to anticipate what's going to be in the ARC test set.</p><p>But isn't the whole point of ARC that you can't sort of, it's a new chart of type of intelligence test every single time? Yes, that is the point. So if ARC were a perfect, flawless benchmark, it would be impossible to anticipate what's in the test set. And, you know, ARC was released more than four years ago, and so far it's been resistant to memorization.</p><p>So I think it has, to some extent, passed the test of time, but I don't think it's perfect. I think if you try to make by hand hundreds of thousands of ARC tasks, and then you try to multiply them by programmatically generating variations, and then you end up with maybe hundreds of millions of tasks, just by brute forcing the task space, there will be enough overlap between what you're trained on and what's in the test set that you can actually score very highly.</p><p>So, you know, with enough scale, you can always cheat. If you can do this for every single thing that supposedly requires intelligence, then what good is intelligence? Apparently, you can just brute force intelligence. If the world, if your life were a static distribution, then sure, you could just brute force the space of possible behaviors.</p><p>Like, you know, the way we think about intelligence, there are several metaphors I like to use, but one of them is you can think of intelligence as a pathfinding algorithm in future situation space. So like, I don't know if you're familiar with game development, like RTS game development, but you have a map, right?</p><p>And you have, it's like a 2D map, and you have partial information about it. Like there is some fog of war on your map, there are areas that you haven't explored yet, you know nothing about them. And then there are areas that you've explored, but you only know how they were like in the past.</p><p>You don't know how they are like today. And now instead of thinking about a 2D map, think about the space of possible future situations that you might encounter and how they are connected to each other. Intelligence is a pathfinding algorithm. So once you set a goal, it will tell you how to get there optimally.</p><p>But of course, it's constrained by the information you have. It cannot pathfind in an area that you know nothing about. It cannot also anticipate changes. And the thing is, if you had complete information about the map, then you could solve the pathfinding problem by simply memorizing every possible path, every mapping from point A to point B.</p><p>You could solve the problem with pure memory. But the reason you cannot do that in real life is because you don't actually know what's going to happen in the future. Life is ever-changing. I feel like you're using words like memorization, which we would never use for human children. If your kid learns to do algebra and then now learns to do calculus, you wouldn't say they've memorized calculus.</p><p>If they can just solve any arbitrary algebraic problem, you wouldn't say they've memorized algebra. You'd say they've learned algebra. Humans are never really doing pure memorization or pure reasoning. But that's only because you're semantically labeling when the human does a skill, it's a memorization, when the exact same skill is done by the LLM, as you can measure by these benchmarks.</p><p>And you can just plug in any sort of math problem. Most humans are doing the exact same as the LLM is doing, which is just, for instance, I know if you learn to add numbers, you're memorizing an algorithm, you're memorizing a program and then you can reapply it. You are not synthesizing on the fly the addition program.</p><p>So obviously at some point, some human had to figure out how to do addition. But the way a kid learns it is not that they sort of figure out from the absence of set theory how to do addition. I think what you learn in school is mostly memorization. Right.</p><p>So my claim is that, listen, these models are vastly under-parameterized relative to how many flops or how many parameters you have in the human brain. And so, yeah, they're not going to be like coming up with new theorems like the smartest humans can. But most humans can't do that either.</p><p>What most humans do, it sounds like it's similar to what you are calling memorization, which is memorizing skills or memorizing, you know, techniques that you've learned. And so it sounds like it's compatible in your, tell me if this is wrong. Is it compatible in your world if like all the remote workers are gone, but they're doing skills which we can potentially make synthetic data of?</p><p>So we record everybody's screen and every single remote worker's screen. We sort of understand the skills they're performing there. And now we've trained a model that can do all this. All the remote workers are unemployed. We're generating trillions of dollars of economic activity from AI remote workers. In that world, are we still in the memorization regime?</p><p>So sure. With memorization, you can automate almost anything. As long as it's a static distribution, as long as you don't have to deal with change.</p></div></div></body></html>