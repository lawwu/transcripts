<html><head><title>System Design for Next-Gen Frontier Models — Dylan Patel, SemiAnalysis</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>System Design for Next-Gen Frontier Models — Dylan Patel, SemiAnalysis</h2><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo"><img src="https://i.ytimg.com/vi_webp/gFyBdBm0AGo/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./gFyBdBm0AGo.html">Whisper Transcript</a> | <a href="./transcript_gFyBdBm0AGo.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">A couple different things, right? Like, you know, people have been talking about stagnation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=16" target="_blank">00:00:16.860</a></span> | <span class="t">and I don't think anyone else, anyone here sees that. But a lot of people have been talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=22" target="_blank">00:00:22.900</a></span> | <span class="t">stagnation of models, and a lot of that has to just do with the fact that we haven't seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=27" target="_blank">00:00:27.820</a></span> | <span class="t">a big capabilities leap in the last bit. But that comes really from models that we're using today are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=36" target="_blank">00:00:36.220</a></span> | <span class="t">largely the same as the models that were trained in 2022, right? GPT-4, 4Turbo, 4.0, those are just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=42" target="_blank">00:00:42.420</a></span> | <span class="t">smaller models that are trained for longer, so similar quality, right? You know, 3.5 Synet came</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=47" target="_blank">00:00:47.420</a></span> | <span class="t">out recently, but again, that's actually smaller than Opus, but it's somehow better because they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=51" target="_blank">00:00:51.760</a></span> | <span class="t">trained it for longer, right? But we haven't seen an extremely large model come out yet, but we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=56" target="_blank">00:00:56.900</a></span> | <span class="t">soon. But one interesting thing, right, is GPT-4 is like 1.8 trillion parameters. It's crazy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=61" target="_blank">00:01:01.720</a></span> | <span class="t">crazy expensive to run, right? 200 billion parameters. Each token requires, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=68" target="_blank">00:01:08.180</a></span> | <span class="t">almost 600 gigaflops, but that's almost going to be considered a last generation model, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=74" target="_blank">00:01:14.480</a></span> | <span class="t">in a year from now. So there's a couple of things that I wanted to talk about regarding that, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=78" target="_blank">00:01:18.900</a></span> | <span class="t">And mostly on the inference side, because I don't think, you know, anyone here is going to try and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=83" target="_blank">00:01:23.120</a></span> | <span class="t">train that kind of next generation model, but definitely we need to be able to run it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=88" target="_blank">00:01:28.120</a></span> | <span class="t">And so, you know, a few things, right? So just going to break down inference in detail,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=93" target="_blank">00:01:33.440</a></span> | <span class="t">right? You know, there's two parts of inference, right? There's pre-fill, there's decode. Pre-fill</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=98" target="_blank">00:01:38.820</a></span> | <span class="t">is the prompt processing, right? And the interesting thing is if you have a 2K prompt, 2K context-length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=105" target="_blank">00:01:45.360</a></span> | <span class="t">prompt, right, 2,000 tokens you input into GPT, that's a petaflop itself, right? And then, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=112" target="_blank">00:01:52.340</a></span> | <span class="t">if you have 32,000 prompt that you enter, it's 20 petaflops, actually. So it's an incredible amount</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=116" target="_blank">00:01:56.840</a></span> | <span class="t">of compute that's required to just process the prompt. And, you know, while pre-fill is very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=124" target="_blank">00:02:04.860</a></span> | <span class="t">compute-intensive, right, it's actually the opposite of decode, right? Decode is actually generating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=129" target="_blank">00:02:09.300</a></span> | <span class="t">each token iteratively, right? So you process the prompt, then you generate a token, you feed it back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=134" target="_blank">00:02:14.720</a></span> | <span class="t">in, and you keep going iteratively, right? And decode is extremely memory bandwidth intensive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=139" target="_blank">00:02:19.860</a></span> | <span class="t">right? You have to load the whole model from the weights, the entire, all the weights into the chip,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=146" target="_blank">00:02:26.800</a></span> | <span class="t">right, or chips for decode. And the big challenge here is that, you know, hey, if you have 1.8 trillion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=154" target="_blank">00:02:34.280</a></span> | <span class="t">parameters, if you're running at a reasonable batch size, you're activating all the experts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=157" target="_blank">00:02:37.420</a></span> | <span class="t">you need to load all 1.8 trillion parameters every single token generation, right? Even if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=164" target="_blank">00:02:44.460</a></span> | <span class="t">serving multiple users at once, that means you're, you need, you know, 1.8, you need terabytes a second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=171" target="_blank">00:02:51.540</a></span> | <span class="t">of memory bandwidth. You want to do 30 tokens per second, I think that's like a minimum bar for most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=175" target="_blank">00:02:55.760</a></span> | <span class="t">people, right? A lot of people want hundreds of tokens per second, but even if you want 30 tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=180" target="_blank">00:03:00.960</a></span> | <span class="t">per second per user, 64 users, you need 60 terabytes a second of memory bandwidth. Even if you look at an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=186" target="_blank">00:03:06.960</a></span> | <span class="t">H100, it has like three, right? So this is a extremely challenging systems problem. More, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=194" target="_blank">00:03:14.200</a></span> | <span class="t">while it is very bandwidth intensive, it's actually quite cheap on the compute, which is why if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=199" target="_blank">00:03:19.220</a></span> | <span class="t">look at like open AI pricing or cloud pricing, you see a three or four to one ratio between pre-fill</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=205" target="_blank">00:03:25.060</a></span> | <span class="t">versus decode pricing, right? So the input tokens cost, you know, one-third that of the output tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=211" target="_blank">00:03:31.940</a></span> | <span class="t">or one-fourth that. So, you know, today the best models, I think, 4.0 and 3.5 Senet are,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=219" target="_blank">00:03:39.140</a></span> | <span class="t">I want to say it's $15 per million tokens, and then it's $5 per million tokens for input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=224" target="_blank">00:03:44.160</a></span> | <span class="t">$15 for output, so $5 for pre-fill, $15 for decode. And soon we're going to have, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=232" target="_blank">00:03:52.000</a></span> | <span class="t">in the open source, you know, so what everyone here can touch is, is Llama 3.405b, right? And that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=237" target="_blank">00:03:57.060</a></span> | <span class="t">going to be a real capability sort of unlock for the, you know, the open source market as well as,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=244" target="_blank">00:04:04.420</a></span> | <span class="t">you know, builders here, right? And I think there's a couple things that people really need to be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=249" target="_blank">00:04:09.380</a></span> | <span class="t">to implement, right? Like, you can't just run Llama CPP on Llama 4.5b, right? Like, it's just not going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=254" target="_blank">00:04:14.840</a></span> | <span class="t">to work. So there's a bunch of stuff that people have to work on, you know, whether it's using, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=260" target="_blank">00:04:20.560</a></span> | <span class="t">know, closed source libraries like TensorRTLLM that only work on NVIDIA, or like VLLM, which is an open</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=266" target="_blank">00:04:26.240</a></span> | <span class="t">source library that works on AMD and Intel and soon other people's chips as well. You know, there's a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=273" target="_blank">00:04:33.860</a></span> | <span class="t">of stuff that people need to figure out. One of those is continuous batching, right? Because you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=278" target="_blank">00:04:38.300</a></span> | <span class="t">going to get, you know, running inference at batch size one is horrendously expensive. You know, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=283" target="_blank">00:04:43.800</a></span> | <span class="t">great to run out if you're running it on your own personal devices, but if you're running it in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=287" target="_blank">00:04:47.560</a></span> | <span class="t">cloud, right, you're renting GPUs, you're running batch size one, you're going to cost yourself 10x more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=292" target="_blank">00:04:52.840</a></span> | <span class="t">You know, 10x is a low bar, right? It's actually could be 10x to 100x more than running at a high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=297" target="_blank">00:04:57.860</a></span> | <span class="t">batch, right? So you have to figure out how to run high batch sizes, batch sizes, how many concurrent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=303" target="_blank">00:05:03.020</a></span> | <span class="t">users you're serving. And so one of those things that makes it difficult is that users requests</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=308" target="_blank">00:05:08.460</a></span> | <span class="t">come in at different times, right? One person might send a request now, and then another person sends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=313" target="_blank">00:05:13.080</a></span> | <span class="t">in a request five seconds later, but the first person's request is not done. So you need to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=317" target="_blank">00:05:17.240</a></span> | <span class="t">able to do continuous batching, i.e. be able to run through the model iteratively every time, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=323" target="_blank">00:05:23.640</a></span> | <span class="t">And bring in new users. So continuous batching is one of the things that you have to have to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=328" target="_blank">00:05:28.620</a></span> | <span class="t">support of. And a lot of software today, like Llama CPP, doesn't have support for that. So either</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=333" target="_blank">00:05:33.200</a></span> | <span class="t">you need to build it yourself or, you know, contribute to an open source project that builds this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=338" target="_blank">00:05:38.680</a></span> | <span class="t">to enable low-cost inference, right, for, you know, models like Llama 405B, right? Another one of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=347" target="_blank">00:05:47.100</a></span> | <span class="t">is disaggregated pre-fill or disaggregated batching, right? It depends on what you call it. But, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=355" target="_blank">00:05:55.560</a></span> | <span class="t">know, if you go back to earlier, I was discussing pre-fill is very compute-intensive, decode is very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=361" target="_blank">00:06:01.580</a></span> | <span class="t">bandwidth-intensive. These are two different workloads, but when you're serving a user, right, whether it's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=367" target="_blank">00:06:07.340</a></span> | <span class="t">you know, in your own app or you're using an API, what have you, right, like these users don't care that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=373" target="_blank">00:06:13.660</a></span> | <span class="t">it's two different workloads, right? It's one workload to them. I get tokens out, right? I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=377" target="_blank">00:06:17.280</a></span> | <span class="t">submit something to you and I get tokens back. But, but for anyone running the infra themselves,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=382" target="_blank">00:06:22.060</a></span> | <span class="t">they need to, they need to be keenly aware that these are two different workloads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=385" target="_blank">00:06:25.580</a></span> | <span class="t">So one thing that a lot of people have started to do, Google's publicly said they're doing it. I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=391" target="_blank">00:06:31.260</a></span> | <span class="t">believe OpenID and Anthropic are also doing it. You know, other firms like Together and Fireworks have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=397" target="_blank">00:06:37.340</a></span> | <span class="t">hinted that they're doing this is disaggregated pre-fill, right? So once your inference volumes are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=403" target="_blank">00:06:43.100</a></span> | <span class="t">high enough, you don't just run inference, you know, you don't just replicate the model across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=407" target="_blank">00:06:47.980</a></span> | <span class="t">however many chips you have, right? Say, say it takes four model, four chips to serve Llama 405B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=413" target="_blank">00:06:53.100</a></span> | <span class="t">right, in the future. You wouldn't just have, you know, if you have so many, if you have enough users,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=418" target="_blank">00:06:58.460</a></span> | <span class="t">you don't just go four and then eight, 16, whatever, right? You don't just replicate that across the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=423" target="_blank">00:07:03.100</a></span> | <span class="t">You actually do this thing called disaggregated pre-fill. You have one set of accelerators</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=427" target="_blank">00:07:07.340</a></span> | <span class="t">do the pre-fill, which is very compute intensive, and then you hand it off to the other set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=431" target="_blank">00:07:11.420</a></span> | <span class="t">accelerators to do decode. Now today, everyone just uses the same accelerator for that, right? H100 or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=436" target="_blank">00:07:16.940</a></span> | <span class="t">A100 or, you know, maybe, maybe L40 or something, but mostly H100. But there's a, there's a reason you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=444" target="_blank">00:07:24.380</a></span> | <span class="t">do this, right? And that big reason is that you have a lot of noisy neighbors, right? So if you've ever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=449" target="_blank">00:07:29.420</a></span> | <span class="t">worked in, like, CPUs or on anything in cloud computing, noisy neighbors are a huge, huge issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=453" target="_blank">00:07:33.900</a></span> | <span class="t">And actually, like, there's, it's very trivial to dramatically slow down most inference providers'</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=459" target="_blank">00:07:39.900</a></span> | <span class="t">services. If you just send queries in a certain way, like in a, in a sort of malicious way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=465" target="_blank">00:07:45.900</a></span> | <span class="t">you can, you can just slow down people's service, right? Whether that's, you know, and that'll,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=471" target="_blank">00:07:51.980</a></span> | <span class="t">that'll impact the user's time to first token, right? And I think that's a huge issue, right? If time to first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=476" target="_blank">00:07:56.780</a></span> | <span class="t">token is too long, people will just quit, right? Using your service. If, you know, the tokens per</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=484" target="_blank">00:08:04.700</a></span> | <span class="t">second varies a lot, right? For a moment, you're getting 100 tokens per second, and then it drops</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=487" target="_blank">00:08:07.980</a></span> | <span class="t">down to like 30, then it drops, it goes back up to 100. That's going to be really annoying to the user.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=492" target="_blank">00:08:12.700</a></span> | <span class="t">So, so there's a lot of things around, you know, SLA and, and reliability and all these things that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=497" target="_blank">00:08:17.980</a></span> | <span class="t">you have to guarantee. And so disaggregated pre-fill is, is one of the techniques to do that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=503" target="_blank">00:08:23.900</a></span> | <span class="t">right? And, and so you don't want to have someone submit, you know, for example, hey, I have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=510" target="_blank">00:08:30.300</a></span> | <span class="t">database and I want to submit, I want to run an LLM query across every single row in that database.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=515" target="_blank">00:08:35.100</a></span> | <span class="t">And I'm just going to submit it to you, my service provider, because you have this cool model or what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=519" target="_blank">00:08:39.660</a></span> | <span class="t">have you that's fine-tuned on some data set and whatever it is, right? If I submit 10,000 rows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=524" target="_blank">00:08:44.700</a></span> | <span class="t">to you at once, that's going to kill everyone else's performance, right? So, so this is one of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=528" target="_blank">00:08:48.300</a></span> | <span class="t">techniques that people have for making it so, you know, that, that person who you definitely want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=533" target="_blank">00:08:53.740</a></span> | <span class="t">serve doesn't impact everyone else's usage. Because once you open up your service to the real world,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=539" target="_blank">00:08:59.980</a></span> | <span class="t">you're not going to be able to control who's submitting what and rate limits are the most annoying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=543" target="_blank">00:09:03.740</a></span> | <span class="t">thing ever. So that's not the correct way to go about it. Another thing is context caching,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=549" target="_blank">00:09:09.660</a></span> | <span class="t">right? So Google launched this recently. They're the only one offering this today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=554" target="_blank">00:09:14.140</a></span> | <span class="t">but I think this is a really big deal. Because when people talk about fine-tuning, right, of models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=558" target="_blank">00:09:18.860</a></span> | <span class="t">that's great. But in reality, the best models are really expensive to fine-tune or impossible to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=564" target="_blank">00:09:24.620</a></span> | <span class="t">fine-tune, right? I can't go fine-tune 3.5s to net. Or fine-tuning Llama 405b is going to take,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=570" target="_blank">00:09:30.700</a></span> | <span class="t">you know, dozens and dozens of GPUs, right? So, so instead of that, the, the, or, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=576" target="_blank">00:09:36.300</a></span> | <span class="t">and, and, and closed-source models generally. So Google only does closed-source models mostly for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=579" target="_blank">00:09:39.660</a></span> | <span class="t">the big ones, right? So Gemini 1.5 Pro, they offered this, they, they brought this recently,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=584" target="_blank">00:09:44.300</a></span> | <span class="t">right? Which is context caching. So instead of, you know, fine-tuning your model, why not, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=589" target="_blank">00:09:49.420</a></span> | <span class="t">just fill out a context length of, you know, they, they offer, I think, two million now today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=593" target="_blank">00:09:53.420</a></span> | <span class="t">right? Two million context length. Why not fill it out with your data there, right? You know, and, and there's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=599" target="_blank">00:09:59.820</a></span> | <span class="t">couple, you know, advantages to that. One is you can use the best models, right? In the case of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=604" target="_blank">00:10:04.620</a></span> | <span class="t">fine-tuned models, you really are focused on like the Llama 7b or Mixtrol or Llama, you know, 70b. It's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=611" target="_blank">00:10:11.420</a></span> | <span class="t">it's kind of look much lower quality models than what's available in the closed-source world. So one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=616" target="_blank">00:10:16.780</a></span> | <span class="t">of the things you can do is you can implement what Google has called context caching. In the, in the open</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=622" target="_blank">00:10:22.220</a></span> | <span class="t">source world, we'll, we'll have super long context models soon enough. But economically, right? You know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=627" target="_blank">00:10:27.100</a></span> | <span class="t">we talked about $15 token per million tokens output and 5 million per million tokens input. If you were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=634" target="_blank">00:10:34.620</a></span> | <span class="t">to have on, on, you know, the best closed-source models today, if you were to submit a prompt of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=640" target="_blank">00:10:40.060</a></span> | <span class="t">like, you know, a million tokens and, and most, most of the times you're looking at a document,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=644" target="_blank">00:10:44.860</a></span> | <span class="t">you get a query back, right? You, your, your output is very small. Almost all of the cost is just sending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=649" target="_blank">00:10:49.420</a></span> | <span class="t">them that document, right? So that's, that's going to really, really hurt you. So for people, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=654" target="_blank">00:10:54.060</a></span> | <span class="t">targeting maybe like a legal AI or like, um, you know, some sort of other contract review AI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=659" target="_blank">00:10:59.180</a></span> | <span class="t">a lot of these enterprise use cases, uh, pre-fill is going to dominate your cost if you're using APIs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=664" target="_blank">00:11:04.140</a></span> | <span class="t">Um, and so Google has this context caching and, and open source will have it. So models you can run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=669" target="_blank">00:11:09.660</a></span> | <span class="t">yourself and, and others will deploy over time. Uh, but basically you don't recompute the KV cache,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=675" target="_blank">00:11:15.900</a></span> | <span class="t">right? The, the context length every single time. Instead you cache it, uh, but the problem is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=681" target="_blank">00:11:21.900</a></span> | <span class="t">save, save that takes an, an incredible amount of memory. Um, so you don't save it in the GPU's memory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=688" target="_blank">00:11:28.060</a></span> | <span class="t">right? You save it on the CPU's memory or storage. Um, and so, uh, VLLM, uh, which is an open source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=695" target="_blank">00:11:35.180</a></span> | <span class="t">library for inference is contributing, is building this currently. So if you're interested in contributing to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=700" target="_blank">00:11:40.380</a></span> | <span class="t">that, uh, check that out. Um, or if you're interested in using it, just start the project,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=705" target="_blank">00:11:45.180</a></span> | <span class="t">right? Um, because, you know, well, most of the models we have in the closed source today are like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=709" target="_blank">00:11:49.500</a></span> | <span class="t">only like 32 or 8K or 4K context length. They're coming with longer. Um, and being able to, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=716" target="_blank">00:11:56.300</a></span> | <span class="t">dramatically reduce your costs, um, by caching the context, um, is, is very, is going to, is going to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=724" target="_blank">00:12:04.140</a></span> | <span class="t">is going to dramatically reduce costs, right? Um, so now I'm just going to talk about like head in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=728" target="_blank">00:12:08.940</a></span> | <span class="t">the cloud stuff instead of like real usable things, which is, um, you know, what's coming down the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=733" target="_blank">00:12:13.660</a></span> | <span class="t">pipeline, right? Which is, you know, GPT-4 was like 20,000 chips for 90 to 100 days. Um, used, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=739" target="_blank">00:12:19.500</a></span> | <span class="t">38 gigawatt hours. Very, very expensive. Cool. Um, but you know, what's, what is, what are they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=744" target="_blank">00:12:24.300</a></span> | <span class="t">building now, right? Uh, OpenAI, XAI, um, Anthropic, many others that are building 100,000 chip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=750" target="_blank">00:12:30.300</a></span> | <span class="t">clusters, right? And it would train GPT-4 in three days, right? So it's kind of irrelevant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=753" target="_blank">00:12:33.980</a></span> | <span class="t">Um, you know, and, and, uh, I'll skip over this part, uh, because it's not really, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=760" target="_blank">00:12:40.300</a></span> | <span class="t">too relevant. Um, but you know, what, what, what's the modern system capable of, right? Like H100 is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=766" target="_blank">00:12:46.140</a></span> | <span class="t">is pretty, uh, pretty fast relative to A100 and, and coming down the pipeline is these, the new NVIDIA</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=771" target="_blank">00:12:51.740</a></span> | <span class="t">chips. But what, what, what's come, you know, what's coming down with these 100,000 GPU clusters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=775" target="_blank">00:12:55.500</a></span> | <span class="t">right? Um, it's not going to be a 1.8 trillion parameter model. It's actually going to be, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=779" target="_blank">00:12:59.100</a></span> | <span class="t">it could be in the tens of trillions of parameters. Um, you know, the, the training flops</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=783" target="_blank">00:13:03.820</a></span> | <span class="t">right? It talked about GPT-4 is, it's roughly two E25 flops, right? Which is, uh, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=791" target="_blank">00:13:11.020</a></span> | <span class="t">a number that's not really relevant or two E25 flop. Um, but with a 100,000 GPU cluster, you can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=796" target="_blank">00:13:16.540</a></span> | <span class="t">10 E26, 10 E27 flops, uh, and to run that model is going to require 200 gigabytes or terabytes a second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=802" target="_blank">00:13:22.380</a></span> | <span class="t">of memory bandwidth, right? Um, but what, what is that like, what does that look like, right? So, so this is a,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=807" target="_blank">00:13:27.420</a></span> | <span class="t">a, on the top right is an image of, uh, Microsoft's data centers in Arizona where they're making</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=814" target="_blank">00:13:34.620</a></span> | <span class="t">GPT-5, right? Um, they have about a hundred thousand GPUs here. Uh, it's 150 megawatts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=820" target="_blank">00:13:40.700</a></span> | <span class="t">right? Like the average home does not consume, you know, that's like, that's like, like tens of thousands,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=825" target="_blank">00:13:45.980</a></span> | <span class="t">if not hundreds of thousands of homes of power consumption, right? It's, it's kind of insane.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=830" target="_blank">00:13:50.060</a></span> | <span class="t">Um, Elon's talked about his next generation cluster. He's building a hundred thousand GPU cluster today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=834" target="_blank">00:13:54.460</a></span> | <span class="t">Uh, but he's talked about his next generation cluster is 300,000 GPUs. That is kind of insane,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=839" target="_blank">00:13:59.580</a></span> | <span class="t">but the, the power cost for that alone would be like $500 million a year, right? So it's like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=845" target="_blank">00:14:05.260</a></span> | <span class="t">you know, people are, people are kind of insane, but it's pretty cool. Um, but, you know, the, the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=851" target="_blank">00:14:11.020</a></span> | <span class="t">the interesting thing here is, you know, on training, we, we, you know, when, when you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=854" target="_blank">00:14:14.540</a></span> | <span class="t">when you try and train a model today, people just talk about fully connected clusters. Uh, every GPU is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=860" target="_blank">00:14:20.060</a></span> | <span class="t">connected to every other GPU at some speed and you, you know, you have to do, you know, all your operations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=864" target="_blank">00:14:24.700</a></span> | <span class="t">but that's not really possible when you go to these super large clusters, right? Um, so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=870" target="_blank">00:14:30.220</a></span> | <span class="t">hundred thousand GPU clusters, those are being built this year and the next year they're planning to build</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=874" target="_blank">00:14:34.140</a></span> | <span class="t">multiple hundred thousand GPU clusters. Already you can see that it exists across multiple buildings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=879" target="_blank">00:14:39.180</a></span> | <span class="t">right? Um, and so there's a lot of complicated networking, uh, going on, right? To connect these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=886" target="_blank">00:14:46.460</a></span> | <span class="t">data centers together. Um, and, and one other thing that, that I think is just like kind of interesting to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=891" target="_blank">00:14:51.340</a></span> | <span class="t">again, head in the clouds just to think about is, um, when you connect these chips together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=895" target="_blank">00:14:55.820</a></span> | <span class="t">there's a lot of optics, right? Uh, you know, you convert from electrical to optical, uh, and then,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=901" target="_blank">00:15:01.180</a></span> | <span class="t">you know, over fiber optics to connect between chips, transceivers, et cetera, right? Uh, these are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=905" target="_blank">00:15:05.500</a></span> | <span class="t">extremely unreliable, right? Uh, they tend to have a failure rate of around five years. Um, and so what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=912" target="_blank">00:15:12.060</a></span> | <span class="t">interesting is if you're talking about a hundred thousand GPU cluster, um, or if you're talking about a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=916" target="_blank">00:15:16.460</a></span> | <span class="t">500,000 GPU cluster, you're going to have something fail like every five minutes, right? Um, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=922" target="_blank">00:15:22.700</a></span> | <span class="t">insane, right? How, how do you even deal with something in your cluster failing every five minutes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=927" target="_blank">00:15:27.020</a></span> | <span class="t">when you're training a model, right? Um, so, you know, this is, this is again more of like a hardware</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=932" target="_blank">00:15:32.940</a></span> | <span class="t">oriented thing, but, uh, you know, the, the other thing that's interesting is like when you get chips,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=938" target="_blank">00:15:38.220</a></span> | <span class="t">they're not all the same speed. You know, an H100 is not an H100. Um, they're stragglers. Uh, so if you get a large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=944" target="_blank">00:15:44.380</a></span> | <span class="t">distribution of chips, um, what we call it in the industry is, it's called the silicon lottery. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=949" target="_blank">00:15:49.740</a></span> | <span class="t">in that like, you know, you, you can buy, for example, a, a gaming GPU and, and compare it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=955" target="_blank">00:15:55.580</a></span> | <span class="t">other people's gaming GPUs on the forums and they're actually like percentages difference in performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=960" target="_blank">00:16:00.300</a></span> | <span class="t">But when you do a massive training cluster, um, you end up with, you know, training is a synchronous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=965" target="_blank">00:16:05.500</a></span> | <span class="t">workload, right? You know, you, you, you update the weights, you, then you pass the gradients around,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=970" target="_blank">00:16:10.620</a></span> | <span class="t">right? Um, and then you, you know, then you again run through a bunch of data, uh, update the weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=976" target="_blank">00:16:16.220</a></span> | <span class="t">or pass the gradients around, update the weights, right? Um, so it's a synchronous workload. So if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=980" target="_blank">00:16:20.300</a></span> | <span class="t">one of them is 10% slower, then everything is 10% slower. And ByteDance had a cool paper where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=985" target="_blank">00:16:25.100</a></span> | <span class="t">actually they saw a 25% decrease in speed just because one random GPU they got, uh, while it did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=991" target="_blank">00:16:31.020</a></span> | <span class="t">technically work, um, and NVIDIA, and, and, and according to NVIDIA, it was fine. It was like 25% slower than,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=997" target="_blank">00:16:37.580</a></span> | <span class="t">uh, what they wanted, right? So they're, you know, this is like, this is on like a 20,000 GPU cluster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1002" target="_blank">00:16:42.300</a></span> | <span class="t">even, right? Um, so, so it's, uh, it's, it's quite interesting that, you know, that that's, these are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1009" target="_blank">00:16:49.020</a></span> | <span class="t">the problems people are running into at scale, right? So they pulled that GPU out, um, and then you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1014" target="_blank">00:16:54.300</a></span> | <span class="t">sort of see their performance dramatically uplifted, right? Um, during, during training. Um, and then again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1020" target="_blank">00:17:00.540</a></span> | <span class="t">this is ByteDance on a 20,000 GPU cluster. So it's, it's, um, it's a, it's a big, big issue. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1027" target="_blank">00:17:07.340</a></span> | <span class="t">and I think, I think some of the other stuff in this presentation is not really relevant. Uh, but I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1031" target="_blank">00:17:11.900</a></span> | <span class="t">think, I think what are these next generation systems look like is a very, um, important question to ask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1039" target="_blank">00:17:19.420</a></span> | <span class="t">yourself, right? Um, you know, and what, what do I, what do I, what do I do when I deal with that, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1044" target="_blank">00:17:24.540</a></span> | <span class="t">Like, I think a lot of the scaffolding that people are building, uh, today for LLMs are dealing with,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1050" target="_blank">00:17:30.620</a></span> | <span class="t">you know, is, is dealing with hallucinations and things like that. And, and the hope that everyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1054" target="_blank">00:17:34.540</a></span> | <span class="t">has, or at least a lot of the AGI people have is that, you know, when I, when I 100X the compute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1059" target="_blank">00:17:39.740</a></span> | <span class="t">um, you know, when I build a cluster that takes $500 million of electricity and I trade a model with it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1064" target="_blank">00:17:44.780</a></span> | <span class="t">it's going to make something that, uh, uh, you know, yearly electricity costs and make a model with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1069" target="_blank">00:17:49.420</a></span> | <span class="t">And then the cluster itself costs over 10 billion, by the way, right? Uh, it's, it's going to get rid of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1073" target="_blank">00:17:53.180</a></span> | <span class="t">a lot of this, um, the hallucinations. It's going to let us do a lot of interesting things. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1079" target="_blank">00:17:59.020</a></span> | <span class="t">yeah, so, so I think that's, that's basically all for the talk. I just wanted to, you know, uh, mention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1084" target="_blank">00:18:04.700</a></span> | <span class="t">you know, sort of a reasonable thing, which is how do you run LLM4 or 5B kind of some strategies that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1089" target="_blank">00:18:09.340</a></span> | <span class="t">people need to implement that aren't necessarily implemented yet, uh, in the open source that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1093" target="_blank">00:18:13.180</a></span> | <span class="t">implemented at the labs. Um, but then also like, you know, what are they doing, right? Because they're not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gFyBdBm0AGo&t=1097" target="_blank">00:18:17.580</a></span> | <span class="t">worried about, you know, LLM4 or 5B capable models.</span></div></div></body></html>