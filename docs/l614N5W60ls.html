<html><head><title>Foundry Local: Cutting-Edge AI experiences on device with ONNX Runtime/Olive — Emma Ning, Microsoft</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Foundry Local: Cutting-Edge AI experiences on device with ONNX Runtime/Olive — Emma Ning, Microsoft</h2><a href="https://www.youtube.com/watch?v=l614N5W60ls"><img src="https://i.ytimg.com/vi_webp/l614N5W60ls/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./l614N5W60ls.html">Whisper Transcript</a> | <a href="./transcript_l614N5W60ls.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello everybody, my name is Aima. I'm a Program Manager at Microsoft. It's a pleasure to talk to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=23" target="_blank">00:00:23.600</a></span> | <span class="t">you today about Foundry Local which enables developers to easily build up cross-platform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=31" target="_blank">00:00:31.120</a></span> | <span class="t">applications powered by local AI. So let's get started. The first question is if the cloud</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=40" target="_blank">00:00:40.080</a></span> | <span class="t">AI is so powerful, why do we need local AI? So here are four key reasons based on our conversations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=50" target="_blank">00:00:50.400</a></span> | <span class="t">and observations with our customers. So first of all, how does cloud AI work in environments with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=59" target="_blank">00:00:59.920</a></span> | <span class="t">low network bandwidth or even offline access? Many of you have experienced the bad internet during this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=69" target="_blank">00:01:09.200</a></span> | <span class="t">conference, right? And a common sentence we have heard from many speakers with live demo is, oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=76" target="_blank">00:01:16.960</a></span> | <span class="t">finger crossed, hopefully the WiFi connection is good. So it's not their fault. It's required by cloud AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=83" target="_blank">00:01:23.920</a></span> | <span class="t">But it's not a concern at all for my session because my live demo runs entirely locally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=91" target="_blank">00:01:31.280</a></span> | <span class="t">So reason number two, privacy and security. Many companies work with very sensitive data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=99" target="_blank">00:01:39.520</a></span> | <span class="t">such as legal documents and patient information. They need to process that data entirely locally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=108" target="_blank">00:01:48.800</a></span> | <span class="t">without anything ever leaving device, right? And reason number three, cost efficiency. Think about game</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=119" target="_blank">00:01:59.440</a></span> | <span class="t">applications, which is deployed to millions of devices, with hundreds of millions of inference calls every day.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=128" target="_blank">00:02:08.080</a></span> | <span class="t">It's not just sustainable. And reason number three, real time latency. So many AI applications need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=137" target="_blank">00:02:17.040</a></span> | <span class="t">respond in real time. And it's not just not possible if we wait on cloud. So that's why we need a local AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=146" target="_blank">00:02:26.880</a></span> | <span class="t">Then the next question, whether local AI is ready now?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=151" target="_blank">00:02:31.040</a></span> | <span class="t">So thanks to the decades of progress in computing, hardware has become more and more powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=160" target="_blank">00:02:40.400</a></span> | <span class="t">So many current devices are equipped with modern GPUs, NPUs, capable of running advanced AI models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=168" target="_blank">00:02:48.960</a></span> | <span class="t">Meanwhile, model companies are keeping publishing more and more models, which are leaner, faster, more optimized for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=179" target="_blank">00:02:59.600</a></span> | <span class="t">local inference, such as 5.4 milli and deep-seek small variants. And we are also seeing more and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=189" target="_blank">00:03:09.120</a></span> | <span class="t">more advanced state-of-art optimization techniques introduced at runtime level. So this convergence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=197" target="_blank">00:03:17.280</a></span> | <span class="t">now makes the local AI a reality. Then how do we build up the solution?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=204" target="_blank">00:03:24.480</a></span> | <span class="t">So Microsoft already has many great assets. Azure AI Foundry introduced last year at Microsoft Eagle Light,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=215" target="_blank">00:03:35.600</a></span> | <span class="t">has been trusted by over 70,000 organizations, with over 1,900 models. And then our cross-platform high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=227" target="_blank">00:03:47.920</a></span> | <span class="t">performance on device inference engine has now seen over 10 million downloads per month. And our customers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=236" target="_blank">00:03:56.720</a></span> | <span class="t">are pretty happy with the significant performance acceleration provided by Onyx runtime compared to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=243" target="_blank">00:04:03.760</a></span> | <span class="t">PyTorch. And lastly, let's not forget Windows. The scale and the reach of Windows on client devices are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=252" target="_blank">00:04:12.880</a></span> | <span class="t">massive. So when we think about democratizing AI, the millions of devices and the millions of customers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=263" target="_blank">00:04:23.440</a></span> | <span class="t">using Windows devices really matter to us. So we are not starting from scratch. We are bringing all those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=272" target="_blank">00:04:32.080</a></span> | <span class="t">advanced assets into Foundry local and optimized end-to-end solution for seamless on-device AI. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=282" target="_blank">00:04:42.480</a></span> | <span class="t">at the bottom, as you can see, it used Onyx runtime to accelerate the performance across various camps of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=291" target="_blank">00:04:51.040</a></span> | <span class="t">hardware. On the top, we are introducing a new Foundry local management service, which hosts and manage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=300" target="_blank">00:05:00.080</a></span> | <span class="t">model on your client device. It also connects to Azure AI Foundry to download open source models on demand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=309" target="_blank">00:05:09.840</a></span> | <span class="t">And for user experience, we provide Foundry local CLI, which allows you to easily explore models on device.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=320" target="_blank">00:05:20.320</a></span> | <span class="t">And we also provide SDK so that developers can easily integrate Foundry local into your own applications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=328" target="_blank">00:05:28.080</a></span> | <span class="t">applications from cloud to local from different hardware. So Foundry local was officially announced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=335" target="_blank">00:05:35.840</a></span> | <span class="t">just one month ago at Microsoft Builder conference. It's available on both Windows and Mac OS. On the Windows,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=345" target="_blank">00:05:45.120</a></span> | <span class="t">it is integrated into the platform, which makes the experience even simpler for Windows AI developer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=355" target="_blank">00:05:55.600</a></span> | <span class="t">As I just mentioned on Foundry local accelerator performance across different kinds of silicon. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=362" target="_blank">00:06:02.640</a></span> | <span class="t">have been closely working with hardware vendors, including Nvidia, Intel, AMD, Qualcomm to integrate their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=371" target="_blank">00:06:11.040</a></span> | <span class="t">hardware accelerators into Foundry local, ensuring the best-in-class performance that you can get on their hardware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=382" target="_blank">00:06:22.240</a></span> | <span class="t">So before our official announcement, over 100 customers joined our private preview. They have shared a valuable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=391" target="_blank">00:06:31.120</a></span> | <span class="t">feedback on how easy Foundry local is to use and how good the performance it is. So let's hear some of their feedback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=406" target="_blank">00:06:46.000</a></span> | <span class="t">Hey there, Savu here, CEO and co-founder at Pieces, where we've been on an ambitious journey to give developers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=412" target="_blank">00:06:52.880</a></span> | <span class="t">artificial long-term memory across the OS. Now, offline-first AI is core to this vision. And in late 2022, we began to explore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=421" target="_blank">00:07:01.520</a></span> | <span class="t">small language models running at the edge on all major platforms. But between rolling our own, Llama C++, and then to O Llama, frustrations around versioning, performance, and end-user experience still remained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=434" target="_blank">00:07:14.320</a></span> | <span class="t">That was until our recent partnership with Microsoft on their new Foundry local platform, an end-to-end AI inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=441" target="_blank">00:07:21.360</a></span> | <span class="t">solution that offers ease of use and high performance across different hardware. In no time, our team went from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=447" target="_blank">00:07:27.440</a></span> | <span class="t">documentation access to a production-ready build with noticeable improvements in memory management, time-to-first token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=454" target="_blank">00:07:34.160</a></span> | <span class="t">and tokens per second. If you're looking to deploy on-device models, you can't go wrong with Foundry local.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=459" target="_blank">00:07:39.680</a></span> | <span class="t">We have been working on AI projects for our customers for several years now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=466" target="_blank">00:07:46.720</a></span> | <span class="t">Some clients want to use the latest AI technologies, but are restricted from using external services</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=473" target="_blank">00:07:53.120</a></span> | <span class="t">when the information they want to process contains sensitive data. Foundry local is a perfect solution for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=478" target="_blank">00:07:58.400</a></span> | <span class="t">these scenarios, as it allows us to easily run Gen AI models locally. Here, we can see a solution that combines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=484" target="_blank">00:08:04.720</a></span> | <span class="t">Foundry local with a speech-to-text service, which also runs locally. One aspect we were really impressed by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=498" target="_blank">00:08:18.160</a></span> | <span class="t">was the simplicity of the installation and the ease of using the models. With Foundry local, we can now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=503" target="_blank">00:08:23.440</a></span> | <span class="t">create hybrid solutions where part of the solution can be run locally. It's been our privilege to work with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=510" target="_blank">00:08:30.480</a></span> | <span class="t">these customers to improve Foundry local together. All right, I will talk in love. Who wants to see live demos?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=521" target="_blank">00:08:41.280</a></span> | <span class="t">Okay, let's do that. So first of all, we can let's see our CLI experience. So on Windows platform,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=533" target="_blank">00:08:53.040</a></span> | <span class="t">you can install Foundry local package using Winget, and on Mac OS, you can use the homebrew commands.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=542" target="_blank">00:09:02.240</a></span> | <span class="t">So I have already installed Foundry local. So first, we want to see what models supported by Foundry local. So we can type Foundry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=563" target="_blank">00:09:23.120</a></span> | <span class="t">Model list. So as you can see, it supports many popular generative AI models. And for each model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=574" target="_blank">00:09:34.160</a></span> | <span class="t">you can get different variants optimized for different hardware. So you can see we have optimization version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=581" target="_blank">00:09:41.840</a></span> | <span class="t">for CPU, for CUDA, for integrated GPU. We also provide NPU variants because my device doesn't contain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=591" target="_blank">00:09:51.040</a></span> | <span class="t">core count NPU. So that variants doesn't show up. Okay, so we want to run some models, right? And if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=600" target="_blank">00:10:00.640</a></span> | <span class="t">haven't downloaded the model before, the Foundry local will download the model from the cloud and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=607" target="_blank">00:10:07.520</a></span> | <span class="t">run the model. It requires internet. But I have already pre-downloaded the model, so we don't need that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=614" target="_blank">00:10:14.960</a></span> | <span class="t">So we're going to see what model I have already downloaded using Foundry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=620" target="_blank">00:10:20.240</a></span> | <span class="t">Foundry cache next. So as you can see, I have downloaded four models here. So I want to, during our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=629" target="_blank">00:10:29.600</a></span> | <span class="t">experiments, we might want to explore different models to see the quality, to see the performance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=636" target="_blank">00:10:36.720</a></span> | <span class="t">then decide which model we want to use to build up our application, right? So firstly, I want to try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=644" target="_blank">00:10:44.560</a></span> | <span class="t">Foundry model run QWIN 2.5, 1.5 billion model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=654" target="_blank">00:10:54.960</a></span> | <span class="t">Since I have already downloaded this model, so the model loading is pretty quick, should be pretty quick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=671" target="_blank">00:11:11.520</a></span> | <span class="t">Okay, the model is set up. You can talk to the model directly. So let's ask a simple question. What was our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=678" target="_blank">00:11:18.880</a></span> | <span class="t">next runtime? Oh, it's pretty quick, right? So I think we may want to see the latency number. So let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=690" target="_blank">00:11:30.720</a></span> | <span class="t">let's exit here and rerun it with verbose mode. And same question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=701" target="_blank">00:11:41.840</a></span> | <span class="t">Okay, so here we get around the 90 tokens per second. We also want to try our different model. So let's do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=714" target="_blank">00:11:54.480</a></span> | <span class="t">We want to try our different model. So we want to try foundry models on this 5.4 mini.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=724" target="_blank">00:12:04.720</a></span> | <span class="t">Also with verbose mode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=735" target="_blank">00:12:15.120</a></span> | <span class="t">So it's loading the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=740" target="_blank">00:12:20.400</a></span> | <span class="t">Okay, model is set up. Same question. What's our next long time?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=753" target="_blank">00:12:33.200</a></span> | <span class="t">So 5.4 mini is more advanced than like Cuban model. The model size is bigger than the Cuban model. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=764" target="_blank">00:12:44.080</a></span> | <span class="t">I would say in terms of the performance, it is a bit smaller than Cuban model. But in terms of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=771" target="_blank">00:12:51.520</a></span> | <span class="t">quality, as you can see, 5.4 mini can provide more detailed information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=777" target="_blank">00:12:57.120</a></span> | <span class="t">All right. So personally, I vote on 5.4 mini. So I want to use this model to build up an application.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=786" target="_blank">00:13:06.720</a></span> | <span class="t">So what application do we want to build? I guess many of you have such experience. Your team moved to a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=793" target="_blank">00:13:13.920</a></span> | <span class="t">new organization, needed to ramp up the existing project very quickly. And there are many long,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=800" target="_blank">00:13:20.960</a></span> | <span class="t">detailed documents you needed to read. And it's very time consuming to read every word, right? So you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=808" target="_blank">00:13:28.640</a></span> | <span class="t">might want some high level summarized version of all of this project. So you can quickly ramp up. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=815" target="_blank">00:13:35.920</a></span> | <span class="t">but this project is an internal project. You cannot upload all those documents to a cloud. And meanwhile,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=822" target="_blank">00:13:42.800</a></span> | <span class="t">you have, you know, some of your team members are using Windows, some of your team members are using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=828" target="_blank">00:13:48.400</a></span> | <span class="t">mic. So you want to build a application cross platform powered by local AI. So let's do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=835" target="_blank">00:13:55.600</a></span> | <span class="t">So I have this application setting up. So what it does, so we can run it first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=849" target="_blank">00:14:09.600</a></span> | <span class="t">And let's quick the existing conversation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=863" target="_blank">00:14:23.600</a></span> | <span class="t">All right. So the app is setting up. Basically, it is used to summarize content. You can give it a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=875" target="_blank">00:14:35.920</a></span> | <span class="t">URL or you give it a local file. So it can do summarization. And it also has a setting tab. You can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=883" target="_blank">00:14:43.520</a></span> | <span class="t">choose the model you want to run. As mentioned before, I prefer 5.4 mini. So because I want to get some more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=892" target="_blank">00:14:52.720</a></span> | <span class="t">more detailed information. So we can put this model ID here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=906" target="_blank">00:15:06.720</a></span> | <span class="t">Then I will pass it with our project document. And that's it to give me some high level information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=919" target="_blank">00:15:19.200</a></span> | <span class="t">And I will hit summarization. So as you can see, the summarized version is coming out. And it says</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=928" target="_blank">00:15:28.880</a></span> | <span class="t">"Foundry local is useful to build up across platform AI applications that run directly on device." That's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=935" target="_blank">00:15:35.600</a></span> | <span class="t">pretty cool. And then let's take a look at the code quickly. So as you can see, we, in terms of SDK, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=946" target="_blank">00:15:46.640</a></span> | <span class="t">provide Python SDK and the JavaScript SDK. So here we use a JavaScript one. So we create the Foundry local manager. And we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=962" target="_blank">00:16:02.400</a></span> | <span class="t">So we initiate the manager with the model name. So as you can see, I passed the 5.4 mini here. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=984" target="_blank">00:16:24.800</a></span> | <span class="t">it uses Foundry local endpoint to create a client. And then you just wait for the chat to be complete,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=993" target="_blank">00:16:33.200</a></span> | <span class="t">to be completed, and output the result. So this is the application running on Windows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1000" target="_blank">00:16:40.320</a></span> | <span class="t">I, somehow, one team member is using Mac. So I want he to use my app as well. So I package the whole project and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1011" target="_blank">00:16:51.680</a></span> | <span class="t">share it to him. And then let's see what his experience is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1016" target="_blank">00:16:56.480</a></span> | <span class="t">He take the project and record a demo for me. So as you can see, the exactly the same code. And he just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1028" target="_blank">00:17:08.560</a></span> | <span class="t">used the same command in npm run start to start this application. And exactly UI, exactly application. And oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1038" target="_blank">00:17:18.880</a></span> | <span class="t">he chose Q1 model. Maybe he likes this model more. And he also used the same documents I used in my previous demo. And hit summarize button.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1055" target="_blank">00:17:35.200</a></span> | <span class="t">So as we see in the previous demo, Q1 model is kind of provide more brief information than 5.4 mini model. So as you can see here, it also shows the summarization is more shorter than what 5.4 mini provides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1075" target="_blank">00:17:55.360</a></span> | <span class="t">Okay, so we build up cross platform applications. Is that all my demo? Of course not. We forgot one important thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1085" target="_blank">00:18:05.680</a></span> | <span class="t">Agent, right? So everybody talks about agent. So we, so do I. So Foundry Local enables you to easily create and build and run a local agent using local model and MCP servers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1103" target="_blank">00:18:23.520</a></span> | <span class="t">This feature is still in private preview. But I want to give you a quick look. So you know how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1112" target="_blank">00:18:32.800</a></span> | <span class="t">So let's back to our CMD. So we can use Foundry Agent List</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1126" target="_blank">00:18:46.880</a></span> | <span class="t">to show all the available agents in Foundry Local. As you can see, we have built up three sample agents here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1135" target="_blank">00:18:55.280</a></span> | <span class="t">So in terms of the concept, an agent in Foundry Local consists of one model and one more MCP servers based on your need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1146" target="_blank">00:19:06.160</a></span> | <span class="t">So you can use one model from the list and pick up whatever MCP server you like to create your own agent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1154" target="_blank">00:19:14.080</a></span> | <span class="t">But here we want to run existing one. So I'm interested in this OCR agent. So let's see. Foundry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1164" target="_blank">00:19:24.880</a></span> | <span class="t">agent info to know what it can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1175" target="_blank">00:19:35.360</a></span> | <span class="t">Okay. Okay. So it can extract the text from images in your local device. And this agent contains one model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1184" target="_blank">00:19:44.560</a></span> | <span class="t">which is the FIFO mini, my favorite one. And the two MCP servers. One is file system MCP server, one OCR mini MCP server.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1193" target="_blank">00:19:53.920</a></span> | <span class="t">So let's run this agent. Foundry agent run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1199" target="_blank">00:19:59.040</a></span> | <span class="t">So this command will check the dependencies of this agent first. If the dependency hasn't been installed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1211" target="_blank">00:20:11.200</a></span> | <span class="t">before, it will be installed with your permission. So I have already installed all those dependencies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1217" target="_blank">00:20:17.920</a></span> | <span class="t">So it just run. Okay. The agent is setting up. It asks for permission to use this MCP server.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1226" target="_blank">00:20:26.240</a></span> | <span class="t">So let's say yes. And he asks directory, you want it to access. I give it the demo folder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1240" target="_blank">00:20:40.320</a></span> | <span class="t">And he also asks permission for the OCR MCP server. I would say yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1245" target="_blank">00:20:45.840</a></span> | <span class="t">So, all right. So from here, you can get all the tools supported by this agent. So literally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1256" target="_blank">00:20:56.640</a></span> | <span class="t">the tools provided in the MCP servers. So you can get some tools related to file system,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1263" target="_blank">00:21:03.120</a></span> | <span class="t">tools related to OCR. So what we want it to do. So here is the use query. I want it to find my receipt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1275" target="_blank">00:21:15.920</a></span> | <span class="t">process it, and get the total amount from it. So let's see whether it can complete this task or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1285" target="_blank">00:21:25.600</a></span> | <span class="t">So it starts to thinking because it needs to figure out which tool to use. Okay. The first tool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1293" target="_blank">00:21:33.040</a></span> | <span class="t">tool to use is search file because it needs to find the receipt. And then he figure out to use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1300" target="_blank">00:21:40.240</a></span> | <span class="t">after search, it'll use the OCR one to extract the text and then get the output, get the total amount.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1311" target="_blank">00:21:51.120</a></span> | <span class="t">Okay. That's cool. So that's all my demo. So finally, I know it's a little bit over time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1318" target="_blank">00:21:58.560</a></span> | <span class="t">but I just quickly run POP. Foundry local enables you to build up applications powered by local AI. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1326" target="_blank">00:22:06.800</a></span> | <span class="t">one best practice. So local model generally are not that capable as cloud model. So you cannot expect it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1341" target="_blank">00:22:21.200</a></span> | <span class="t">do the fancy work that cloud model or cloud agent can do. But it has unlocked a lot of potential. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1349" target="_blank">00:22:29.280</a></span> | <span class="t">I leave that to you guys to explore. If you want to get more information, here's the link. And you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=l614N5W60ls&t=1357" target="_blank">00:22:37.040</a></span> | <span class="t">to try out our agent feature, you can sign up our private preview form. All right. Thanks, everyone.</span></div></div></body></html>