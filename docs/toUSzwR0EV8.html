<html><head><title>Distributed Training with PyTorch: complete tutorial with cloud infrastructure and code</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Distributed Training with PyTorch: complete tutorial with cloud infrastructure and code</h2><a href="https://www.youtube.com/watch?v=toUSzwR0EV8"><img src="https://i.ytimg.com/vi_webp/toUSzwR0EV8/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=163">2:43</a> What is distributed training?<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=284">4:44</a> Data Parallelism vs Model Parallelism<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=385">6:25</a> Gradient accumulation<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1178">19:38</a> Distributed Data Parallel<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1584">26:24</a> Collective Communication Primitives<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1719">28:39</a> Broadcast operator<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1828">30:28</a> Reduce operator<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1959">32:39</a> All-Reduce<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2000">33:20</a> Failover<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2174">36:14</a> Creating the cluster (Paperspace)<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2940">49:0</a> Distributed Training with TorchRun<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3297">54:57</a> LOCAL RANK vs GLOBAL RANK<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3365">56:5</a> Code walkthrough<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4007">66:47</a> No_Sync context<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4128">68:48</a> Computation-Communication overlap<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4250">70:50</a> Bucketing<br><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4331">72:11</a> Conclusion<br><br><div style="text-align: left;"><a href="./toUSzwR0EV8.html">Whisper Transcript</a> | <a href="./transcript_toUSzwR0EV8.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys, welcome back to my channel. Today we are going to talk about the distributed training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4" target="_blank">00:00:04.080</a></span> | <span class="t">of a model with PyTorch. What do I mean? Well imagine you have a model that is very big and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=9" target="_blank">00:00:09.120</a></span> | <span class="t">you have a lot of data upon which you want to train this model but it doesn't, it takes a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=13" target="_blank">00:00:13.520</a></span> | <span class="t">of time to train it because maybe your GPU is not so powerful or maybe you cannot use a large batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=19" target="_blank">00:00:19.840</a></span> | <span class="t">size or just you have a lot of data to train this model and you want to train it in a reasonable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=23" target="_blank">00:00:23.840</a></span> | <span class="t">time. One way to do it is to parallelize the training on using multiple GPUs or even multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=29" target="_blank">00:00:29.200</a></span> | <span class="t">computers, each one having multiple GPUs or even one GPU. In this video I will show you how this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=35" target="_blank">00:00:35.200</a></span> | <span class="t">distributed training works, how to integrate it in your existing project and we will combine theory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=41" target="_blank">00:00:41.280</a></span> | <span class="t">with practice which means that I will give you all the theory behind the distributed training and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=45" target="_blank">00:00:45.280</a></span> | <span class="t">then also show you how to first of all build the cloud infrastructure to do cloud training of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=51" target="_blank">00:00:51.920</a></span> | <span class="t">model and also how to integrate the distributed training into an existing model that you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=56" target="_blank">00:00:56.800</a></span> | <span class="t">already built and it's maybe already training on a single GPU. So let's review the topics of today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=62" target="_blank">00:01:02.160</a></span> | <span class="t">First of all I will give you an introduction to distributed training and show you the two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=65" target="_blank">00:01:05.760</a></span> | <span class="t">ways in which we can do it. One is data parallelism and one in model parallelism. In this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=70" target="_blank">00:01:10.080</a></span> | <span class="t">video I will explore data parallelism. We will see a little bit of the neural networks theory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=75" target="_blank">00:01:15.520</a></span> | <span class="t">because I want to introduce a concept called gradient accumulation which is very important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=79" target="_blank">00:01:19.760</a></span> | <span class="t">for distributed training and later we will see distributed training as it is built in PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=85" target="_blank">00:01:25.040</a></span> | <span class="t">so distributed data parallel. We will see how it works and I will also show you what we mean by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=91" target="_blank">00:01:31.200</a></span> | <span class="t">communication primitives because maybe you have heard terms like all reduce, reduce broadcast,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=96" target="_blank">00:01:36.080</a></span> | <span class="t">all gather etc but you maybe don't know what they are or how they work. In this video I will show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=101" target="_blank">00:01:41.040</a></span> | <span class="t">you the algorithms upon which these operations are built and how they work when we talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=106" target="_blank">00:01:46.000</a></span> | <span class="t">distributed training. I will also show you how to manage failover of a node so imagine you are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=110" target="_blank">00:01:50.400</a></span> | <span class="t">training on multiple nodes and one of the nodes suddenly dies how we manage this situation. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=115" target="_blank">00:01:55.840</a></span> | <span class="t">will do a little coding session I will not write code I will just take an existing code and show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=120" target="_blank">00:02:00.000</a></span> | <span class="t">you how to add the distributed training to an existing model and I will be using a model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=125" target="_blank">00:02:05.200</a></span> | <span class="t">I built in a previous video so it's the video about the how to code a transformer model from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=130" target="_blank">00:02:10.240</a></span> | <span class="t">scratch so if you have already watched my previous video on how to do it it's wonderful if you didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=134" target="_blank">00:02:14.960</a></span> | <span class="t">it's not a problem because anyway the knowledge I will give you today applies to any model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=140" target="_blank">00:02:20.000</a></span> | <span class="t">you have built not only the one wrote by me and I will also show you the underlying mechanism in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=146" target="_blank">00:02:26.800</a></span> | <span class="t">which PyTorch does distributed training so I will introduce you the computation communication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=152" target="_blank">00:02:32.320</a></span> | <span class="t">overlap and the bucketing which are optimizations made in the PyTorch implementation of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=157" target="_blank">00:02:37.920</a></span> | <span class="t">distributed training and so we will explore all these topics. Let's start our journey so let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=163" target="_blank">00:02:43.840</a></span> | <span class="t">start by introducing what is distributed training. Imagine you want to train a language model on a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=169" target="_blank">00:02:49.280</a></span> | <span class="t">very big data set for example the entire content of wikipedia now the data set is quite big because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=174" target="_blank">00:02:54.640</a></span> | <span class="t">it's made up of millions of articles and each article has maybe thousands of tokens to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=180" target="_blank">00:03:00.560</a></span> | <span class="t">this model on a single gpu may be possible but it poses some challenges first of all the model may</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=186" target="_blank">00:03:06.240</a></span> | <span class="t">not fit on a single gpu this happens when the model has many parameters this happens for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=191" target="_blank">00:03:11.600</a></span> | <span class="t">also with the latest llama they have billions of parameters and with so many parameters the ram of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=198" target="_blank">00:03:18.160</a></span> | <span class="t">the gpu may not be enough or you are forced to use a small batch size because a bigger batch size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=204" target="_blank">00:03:24.640</a></span> | <span class="t">leads to the out of error out of memory error on CUDA or the model may take years to train because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=211" target="_blank">00:03:31.360</a></span> | <span class="t">the data set is very big if any of the above points applies to you then you need to scale up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=216" target="_blank">00:03:36.400</a></span> | <span class="t">your training setup and scaling can be done vertically or horizontally let's compare the two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=221" target="_blank">00:03:41.200</a></span> | <span class="t">options. Vertical scaling means that you have a single machine for example a single gpu which has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=227" target="_blank">00:03:47.520</a></span> | <span class="t">some ram and the gpu has some memory for example four gigabyte vertical scaling means that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=232" target="_blank">00:03:52.880</a></span> | <span class="t">just take the you just buy a bigger computer and or a bigger gpu so you take the existing machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=239" target="_blank">00:03:59.920</a></span> | <span class="t">and you upgrade the hardware and this requires no code change because the code that was running on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=245" target="_blank">00:04:05.680</a></span> | <span class="t">the small machine can also run on the big machine you just maybe need to adjust a little bit of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=249" target="_blank">00:04:09.440</a></span> | <span class="t">parameters maybe you can increase the batch size now with horizontal scaling we have we go from a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=255" target="_blank">00:04:15.680</a></span> | <span class="t">single machine to multiple machines each one interconnected with each other that are communicating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=260" target="_blank">00:04:20.800</a></span> | <span class="t">for this parallelization training and each machine may have one or multiple gpus and this requires</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=268" target="_blank">00:04:28.720</a></span> | <span class="t">code change but this code is mean this code change is minimal thanks to pytorch and its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=273" target="_blank">00:04:33.120</a></span> | <span class="t">implementation of the distributed data parallel and in this video we will explore horizontal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=278" target="_blank">00:04:38.800</a></span> | <span class="t">scaling because vertical scaling basically we don't need to do anything the code there is no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=283" target="_blank">00:04:43.040</a></span> | <span class="t">code change so there are two ways of distributing training of a model one is called the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=290" target="_blank">00:04:50.080</a></span> | <span class="t">parallelism and one is called the model parallelism so if the model can fit within a single gpu then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=295" target="_blank">00:04:55.520</a></span> | <span class="t">we can distribute the training on multiple servers each one having one or more gpus with each gpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=302" target="_blank">00:05:02.400</a></span> | <span class="t">processing a subset of the entire data in parallel and synchronizing the gradients during back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=308" target="_blank">00:05:08.160</a></span> | <span class="t">propagation this technique is known as data parallelism so we have one model that can fit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=314" target="_blank">00:05:14.400</a></span> | <span class="t">within a single gpu and we have a lot of data what we do is basically we split this data into subsets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=320" target="_blank">00:05:20.480</a></span> | <span class="t">and we train it on multiple computers in parallel such that each gpu is training on a subset of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=327" target="_blank">00:05:27.520</a></span> | <span class="t">entire data set if the model however cannot fit within a single gpu then we need to break the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=334" target="_blank">00:05:34.400</a></span> | <span class="t">model into smaller pieces into smaller layers and each gpu process a part of the forward and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=340" target="_blank">00:05:40.640</a></span> | <span class="t">the backward step during gradient descent this option is known as model parallelism so imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=346" target="_blank">00:05:46.160</a></span> | <span class="t">this model initial model doesn't fit on a single gpu so we can break it into layers and each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=351" target="_blank">00:05:51.600</a></span> | <span class="t">computer is managing a layer not the entire model and of course we can also combine data parallelism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=359" target="_blank">00:05:59.040</a></span> | <span class="t">with model parallelism to create a hybrid option in this video we will focus on data parallelism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=365" target="_blank">00:06:05.200</a></span> | <span class="t">so we pretend that we have a model that is complex but it can fit on a single gpu but we have a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=371" target="_blank">00:06:11.520</a></span> | <span class="t">of training data and it's taking really forever to train it on a single computer so we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=376" target="_blank">00:06:16.320</a></span> | <span class="t">distribute this training on multiple computers or multiple gpus such that each gpu will train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=382" target="_blank">00:06:22.480</a></span> | <span class="t">on a subset of the data okay we need to review a little bit of the neural networks because i need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=388" target="_blank">00:06:28.960</a></span> | <span class="t">to introduce a concept called gradient accumulation okay imagine you want to train a neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=395" target="_blank">00:06:35.440</a></span> | <span class="t">to predict the price of a house given two variables the number of bedroom in the house and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=401" target="_blank">00:06:41.200</a></span> | <span class="t">we will call this variable x1 and the number of bathrooms in the house and we will call this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=405" target="_blank">00:06:45.920</a></span> | <span class="t">variable x2 we think because of some intuition that the relationship between the input and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=412" target="_blank">00:06:52.880</a></span> | <span class="t">output variable is linear so we can write that the predicted price is equal to x1 multiplied by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=419" target="_blank">00:06:59.280</a></span> | <span class="t">first weight plus x2 multiplied by the second weight plus a bias term our goal is to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=426" target="_blank">00:07:06.080</a></span> | <span class="t">stochastic gradient descent to find the values of the parameters w1 and w2 and bias such that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=432" target="_blank">00:07:12.880</a></span> | <span class="t">the mean squared error loss between the actual house price and the predicted house price is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=438" target="_blank">00:07:18.240</a></span> | <span class="t">minimized in other words we want to find w1 w2 and b such that we minimize this quantity here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=445" target="_blank">00:07:25.600</a></span> | <span class="t">how do we do a training loop with pytorch without gradient accumulation so first of all we run the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=454" target="_blank">00:07:34.720</a></span> | <span class="t">training for a few epochs we take our training data which is made up of x1 x2 and the target</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=461" target="_blank">00:07:41.120</a></span> | <span class="t">price we calculate the output of the model which is basically y pred is equal to x1 multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=467" target="_blank">00:07:47.360</a></span> | <span class="t">w1 plus x2 multiplied by w2 plus the bias then we calculate the loss we do backward on the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=475" target="_blank">00:07:55.120</a></span> | <span class="t">this will calculate the gradient of the loss function with respect to each parameter and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=480" target="_blank">00:08:00.000</a></span> | <span class="t">we update the parameters using the gradient we have calculated i am not using the optimizer i'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=486" target="_blank">00:08:06.320</a></span> | <span class="t">just writing the the update rule by hand so using a learning rate i don't use the momentum just for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=492" target="_blank">00:08:12.880</a></span> | <span class="t">simplicity and this is actually how the training loop works right so this is this part here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=498" target="_blank">00:08:18.560</a></span> | <span class="t">equivalent to calling optimizer.snap and this part here is equivalent to calling optimizer.zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=503" target="_blank">00:08:23.920</a></span> | <span class="t">let's see it graphically what happens when we do a training loop like this so imagine we have our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=510" target="_blank">00:08:30.160</a></span> | <span class="t">expression which represents the model which could in this case it's a very simple expression but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=514" target="_blank">00:08:34.560</a></span> | <span class="t">imagine it's a very complex model okay what pytorch will do it will create a computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=520" target="_blank">00:08:40.320</a></span> | <span class="t">graph so it will take our input it will multiply it by the w parameters each input with its own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=528" target="_blank">00:08:48.080</a></span> | <span class="t">weight then it will combine the sum of the two plus the bias term this will produce an output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=534" target="_blank">00:08:54.560</a></span> | <span class="t">then we have a target the target is compared with the output to produce a loss and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=541" target="_blank">00:09:01.680</a></span> | <span class="t">run back propagation to calculate the gradient of the loss function with respect to the target so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=547" target="_blank">00:09:07.360</a></span> | <span class="t">let's visualize the training process one item at a time using our computation graph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=551" target="_blank">00:09:11.920</a></span> | <span class="t">imagine we are training on the input x1 is equal to 6 x2 is equal to 2 and the target is 15 the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=560" target="_blank">00:09:20.800</a></span> | <span class="t">first thing our model will do is will start from the two input nodes so x1 and x2 it will multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=566" target="_blank">00:09:26.400</a></span> | <span class="t">by the w weights and we also think that the w weights have been initialized with the following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=572" target="_blank">00:09:32.560</a></span> | <span class="t">value so the value of this weight is 0.773 this weight is 0.321 and this weight is 0.067</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=580" target="_blank">00:09:40.240</a></span> | <span class="t">these values could be randomly generated as we usually do actually pytorch will produce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=588" target="_blank">00:09:48.240</a></span> | <span class="t">the output of this node which is multiplying the x1 value with w1 then it will combine the two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=595" target="_blank">00:09:55.200</a></span> | <span class="t">producing the output of the model it will compare it with the target to produce a loss which is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=600" target="_blank">00:10:00.560</a></span> | <span class="t">number but it's also a function so we can calculate the gradient so now usually we call a loss dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=606" target="_blank">00:10:06.880</a></span> | <span class="t">backward to calculate the gradient of the loss function with respect to each parameter so w1 w2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=612" target="_blank">00:10:12.800</a></span> | <span class="t">and b pytorch will also compute the intermediate nodes but i will not show them here for simplicity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=618" target="_blank">00:10:18.640</a></span> | <span class="t">so we run loss dot backward what will loss dot backward do it will calculate the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=624" target="_blank">00:10:24.960</a></span> | <span class="t">of the loss function with respect to each weight how does it do it well the first thing we do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=631" target="_blank">00:10:31.600</a></span> | <span class="t">will do it will calculate the gradient of the loss function with respect to the output which can be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=636" target="_blank">00:10:36.400</a></span> | <span class="t">done like this then it will calculate the gradient of the output with respect to this node i here i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=644" target="_blank">00:10:44.880</a></span> | <span class="t">am showing only the computation of the gradient of the loss function with respect to the w1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=651" target="_blank">00:10:51.280</a></span> | <span class="t">node you can do the other ones for exercise for example so next it will compute the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=658" target="_blank">00:10:58.320</a></span> | <span class="t">of the loss function with respect to the z1 node but in order to do it because of the chain rule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=663" target="_blank">00:11:03.760</a></span> | <span class="t">we need the gradient of the output with respect to the z1 node and then it will compute the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=670" target="_blank">00:11:10.320</a></span> | <span class="t">the gradient of the z1 node with respect to w1 and this allow us to calculate the gradient of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=676" target="_blank">00:11:16.160</a></span> | <span class="t">the loss function with respect to w1 these are all the calculations that you need to do in order to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=681" target="_blank">00:11:21.200</a></span> | <span class="t">gather the gradient of this node here and as you can see we also need to compute the gradient of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=686" target="_blank">00:11:26.560</a></span> | <span class="t">the intermediate node but we are only interested in the in order to arrive to the gradient of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=691" target="_blank">00:11:31.840</a></span> | <span class="t">parameters of the loss function with respect to the parameters and these are the values for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=696" target="_blank">00:11:36.960</a></span> | <span class="t">the gradient that i that we have calculated for each parameter the next thing we do in during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=703" target="_blank">00:11:43.680</a></span> | <span class="t">training is we run optimizer dot step this will update the value of each parameter using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=710" target="_blank">00:11:50.240</a></span> | <span class="t">gradient that we have calculated and how do we do it we say that the new value of the parameter is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=714" target="_blank">00:11:54.960</a></span> | <span class="t">equal to the old value of the parameter minus learning rate multiplied by the gradient why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=722" target="_blank">00:12:02.240</a></span> | <span class="t">minus because the gradient indicates the direction in which the function grows the most but we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=727" target="_blank">00:12:07.600</a></span> | <span class="t">to move against this direction because we want to minimize the the loss so we put a minus sign here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=734" target="_blank">00:12:14.560</a></span> | <span class="t">and this is how the value will be updated using the gradient that we have calculated in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=740" target="_blank">00:12:20.000</a></span> | <span class="t">previous step which was the loss dot backward function the next thing we do is we run optimizer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=747" target="_blank">00:12:27.360</a></span> | <span class="t">dot zero so this will zero out all the gradients of each parameter and also the intermediate nodes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=753" target="_blank">00:12:33.600</a></span> | <span class="t">and then we do the next iteration on the next item so imagine we are training one item at a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=760" target="_blank">00:12:40.160</a></span> | <span class="t">time so batch size is one the next item may have x1 is equal to 5 x2 is equal to 2 and the target</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=766" target="_blank">00:12:46.880</a></span> | <span class="t">may be 12 this will produce this loss and this output the next thing we do is we run loss dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=773" target="_blank">00:12:53.440</a></span> | <span class="t">backward loss dot backward we calculate the gradient of the loss function with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=778" target="_blank">00:12:58.640</a></span> | <span class="t">each weight and you can see the gradient here the next thing we do is we run optimizer dot step which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=784" target="_blank">00:13:04.960</a></span> | <span class="t">will update the value of the parameter using the gradient that we calculated in the previous step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=789" target="_blank">00:13:09.520</a></span> | <span class="t">and the learning rate and finally we run optimizer dot zero and this will reset all the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=795" target="_blank">00:13:15.360</a></span> | <span class="t">for all the weights we may want to visualize this on this process on the loss function so let's see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=802" target="_blank">00:13:22.080</a></span> | <span class="t">what happens to the loss function while we are doing this process we started some with some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=807" target="_blank">00:13:27.440</a></span> | <span class="t">initial weights so remember we started with some randomly initialized weights values then we ran</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=813" target="_blank">00:13:33.280</a></span> | <span class="t">forward step on the first data item this calculated an output and then we run loss dot backward this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=819" target="_blank">00:13:39.920</a></span> | <span class="t">resulted in a gradient being calculated for the loss function with respect to each weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=824" target="_blank">00:13:44.480</a></span> | <span class="t">this indicates a direction in which we should move our weights in order to minimize the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=830" target="_blank">00:13:50.720</a></span> | <span class="t">because we move against the direction of the gradient so the arrow is pointing already in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=835" target="_blank">00:13:55.200</a></span> | <span class="t">the negative direction of the gradient the next thing we do is we do optimizer dot step optimizer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=841" target="_blank">00:14:01.440</a></span> | <span class="t">dot step will take our initial weights and will change the weights value according to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=847" target="_blank">00:14:07.680</a></span> | <span class="t">negative direction of the gradient we then run optimizer dot zero this just resets the value of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=854" target="_blank">00:14:14.080</a></span> | <span class="t">the gradient and then we run forward on the next data item this will then we run loss dot backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=860" target="_blank">00:14:20.480</a></span> | <span class="t">and this will result in a calculation of a gradient which indicates a direction in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=864" target="_blank">00:14:24.320</a></span> | <span class="t">we need to move our weights in order to minimize the loss function and then we actually modify our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=870" target="_blank">00:14:30.240</a></span> | <span class="t">weights using optimizer dot step so the actual update of the parameters values happens when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=876" target="_blank">00:14:36.800</a></span> | <span class="t">call optimizer dot step and finally we run optimizer dot zero and this will reset the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=883" target="_blank">00:14:43.600</a></span> | <span class="t">gradient to zero and this is how gradient descent works without accumulation and without without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=891" target="_blank">00:14:51.600</a></span> | <span class="t">gradient accumulation at every step so every data item because here we pretend we have a batch size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=896" target="_blank">00:14:56.240</a></span> | <span class="t">equal to one we update the parameters of the model at every step and however with gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=902" target="_blank">00:15:02.720</a></span> | <span class="t">accumulation we don't do at every step let's see how it works so the the initial part of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=908" target="_blank">00:15:08.480</a></span> | <span class="t">training loop with gradient accumulation is the same so we have run for a few epochs we extract</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=912" target="_blank">00:15:12.880</a></span> | <span class="t">the data from our training data so x1 x2 and target we calculated the loss just like before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=918" target="_blank">00:15:18.720</a></span> | <span class="t">we run loss dot backward just like before the difference is here that we don't update the value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=925" target="_blank">00:15:25.200</a></span> | <span class="t">of our parameter at every item we train our model upon or every batch we train our model upon but we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=931" target="_blank">00:15:31.840</a></span> | <span class="t">do it at every few items or every few batches in this case i do it in this training loop we do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=939" target="_blank">00:15:39.840</a></span> | <span class="t">every two items because we are extracting one item at a time from our training data and so we update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=948" target="_blank">00:15:48.880</a></span> | <span class="t">the the parameters using the gradient every two items in this case and what happens is that when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=956" target="_blank">00:15:56.960</a></span> | <span class="t">the first item for example we will arrive to loss dot backward we will not run this code here we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=962" target="_blank">00:16:02.000</a></span> | <span class="t">will restart the loop at the next item we will calculate the output we run again loss dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=967" target="_blank">00:16:07.280</a></span> | <span class="t">backward and this loss will produce this loss dot backward will create a gradient but this gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=972" target="_blank">00:16:12.960</a></span> | <span class="t">will not replace the previous the gradient that we calculated in the previous step but it will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=978" target="_blank">00:16:18.240</a></span> | <span class="t">accumulated it will summed up to the previous gradient so let's visualize this process step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=983" target="_blank">00:16:23.280</a></span> | <span class="t">by step imagine we have our first item and this is x1 is equal to 6 x2 is equal to 2 and the target</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=990" target="_blank">00:16:30.560</a></span> | <span class="t">is 15 we run the forward loop using this item this will result in an output being calculated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=997" target="_blank">00:16:37.360</a></span> | <span class="t">and using the target we can calculate a loss we then run the loss dot backward this will result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1003" target="_blank">00:16:43.200</a></span> | <span class="t">in a gradient being calculated for this item then we don't update the parameters we do forward step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1010" target="_blank">00:16:50.720</a></span> | <span class="t">on the next item so note that during this forward step the gradient is not zero because we didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1016" target="_blank">00:16:56.480</a></span> | <span class="t">zero it out in the previous step because in the previous step we didn't run optimizer dot step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1021" target="_blank">00:17:01.280</a></span> | <span class="t">or optimizer dot zero so the gradient is still there from the previous item and okay the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1028" target="_blank">00:17:08.560</a></span> | <span class="t">item which is x1 is equal to 5 x2 is equal to 2 and the target is 12 will result in a loss being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1033" target="_blank">00:17:13.440</a></span> | <span class="t">calculated and an output we then run loss dot backward and the pytorch will not replace this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1040" target="_blank">00:17:20.320</a></span> | <span class="t">new gradient with the previous one we'll sum it up it will be accumulated that's why we call it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1045" target="_blank">00:17:25.760</a></span> | <span class="t">gradient accumulation so the new gradient is accumulated with the old one now that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1051" target="_blank">00:17:31.360</a></span> | <span class="t">reached the batch size we can now finally call the optimizer dot step method this will result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1057" target="_blank">00:17:37.920</a></span> | <span class="t">in the values of the parameter being updated using the accumulative gradient and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1065" target="_blank">00:17:45.760</a></span> | <span class="t">run optimizer dot zero which will reset the gradient to zero and then we can proceed with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1070" target="_blank">00:17:50.640</a></span> | <span class="t">another loop of two items etc let's visualize what happens to our loss function so to our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1078" target="_blank">00:17:58.000</a></span> | <span class="t">parameters and the loss function when we do gradient descent with gradient accumulation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1082" target="_blank">00:18:02.640</a></span> | <span class="t">so our initial weights are here because they are randomly initialized we run the forward loop on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1089" target="_blank">00:18:09.840</a></span> | <span class="t">the first item this will result in an output being calculated by the model then we run loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1097" target="_blank">00:18:17.520</a></span> | <span class="t">loss dot backward this will result in the gradient being calculated with respect to the first data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1103" target="_blank">00:18:23.200</a></span> | <span class="t">item and this gradient will indicate a direction then we run again forward on the second data item</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1110" target="_blank">00:18:30.480</a></span> | <span class="t">and then we run loss dot backward in the second data item this also will result in a gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1115" target="_blank">00:18:35.520</a></span> | <span class="t">being calculated but these two gradients will then be summed up by pytorch and this will result in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1121" target="_blank">00:18:41.760</a></span> | <span class="t">resulting vector that indicates a direction and then we run the optimizer so we do optimizer dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1129" target="_blank">00:18:49.040</a></span> | <span class="t">step which will update the values of our weights according to the direction indicated by the sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1135" target="_blank">00:18:55.440</a></span> | <span class="t">of the two gradients of the two previous data items and then we run optimizer dot zero this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1141" target="_blank">00:19:01.040</a></span> | <span class="t">will result the gradients to zero for all the nodes and so with gradient accumulation we update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1148" target="_blank">00:19:08.080</a></span> | <span class="t">the parameters of the model only after we accumulated the gradient of a batch and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1153" target="_blank">00:19:13.360</a></span> | <span class="t">can decide how much we want this batch to be so gradient accumulation is used not only distributed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1159" target="_blank">00:19:19.680</a></span> | <span class="t">training it's also used for example when you want to accumulate the gradient without increasing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1164" target="_blank">00:19:24.160</a></span> | <span class="t">batch size because maybe a bigger batch size doesn't fit in your gpu so you can accumulate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1169" target="_blank">00:19:29.120</a></span> | <span class="t">the gradient for more than one item and then move the parameters because this will result in a more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1174" target="_blank">00:19:34.240</a></span> | <span class="t">smooth training the loss will oscillate less for example and now that we have seen how gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1181" target="_blank">00:19:41.040</a></span> | <span class="t">accumulation works let's see distributed data parallel training in detail so how does it work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1186" target="_blank">00:19:46.240</a></span> | <span class="t">what we mean by communication primitives and how do we manage the failover okay distributed data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1193" target="_blank">00:19:53.040</a></span> | <span class="t">parallel training so imagine you have a training script that is running on a single computer but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1198" target="_blank">00:19:58.720</a></span> | <span class="t">it's very slow because the data set is very big and you can't use a bigger batch size because it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1203" target="_blank">00:20:03.680</a></span> | <span class="t">will result in an out of memory error on CUDA this distributed data parallel is the solution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1209" target="_blank">00:20:09.360</a></span> | <span class="t">in this case it works in the following three scenarios so imagine you have a model that can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1215" target="_blank">00:20:15.680</a></span> | <span class="t">fit on a single gpu and you have a training loop that works but it's very slow so the first thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1222" target="_blank">00:20:22.880</a></span> | <span class="t">you can do is you want to train the model on multiple servers each one having a single gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1228" target="_blank">00:20:28.480</a></span> | <span class="t">and this can be done with distributed data parallel the second setup that you can use is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1235" target="_blank">00:20:35.280</a></span> | <span class="t">you want to just increase the increase the number of gpus on your existing computer and this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1239" target="_blank">00:20:39.840</a></span> | <span class="t">also can be managed by distributed data parallel training the final setup that you can that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1245" target="_blank">00:20:45.280</a></span> | <span class="t">may want is you want multiple computers and each one having more than one gpu in this video i will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1251" target="_blank">00:20:51.760</a></span> | <span class="t">show you how to implement this setup here because the other two setup are just a particular case of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1258" target="_blank">00:20:58.240</a></span> | <span class="t">this one so this is the most generic setup we can have and i will show you how to create the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1265" target="_blank">00:21:05.360</a></span> | <span class="t">cloud infrastructure for this setup and also how to run the to convert an existing code into a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1271" target="_blank">00:21:11.120</a></span> | <span class="t">distributed training code and also run it on the cluster that we will create so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1277" target="_blank">00:21:17.040</a></span> | <span class="t">from now on i will use the term node and gpu interchangeably if a cluster is made up of two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1284" target="_blank">00:21:24.800</a></span> | <span class="t">computers and each computer has two gpus then we in total we will have four nodes or four gpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1290" target="_blank">00:21:30.880</a></span> | <span class="t">distributed data parallel works in the following way so at the beginning of the training the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1296" target="_blank">00:21:36.480</a></span> | <span class="t">model's weights are initialized on one node so on one gpu and then send to all the other nodes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1302" target="_blank">00:21:42.800</a></span> | <span class="t">using an operation called broadcast each node then trains the model the same model because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1309" target="_blank">00:21:49.920</a></span> | <span class="t">it started from the same initial weights on a subset of the data so maybe this one is maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1316" target="_blank">00:21:56.320</a></span> | <span class="t">one gpu is training it on the batch number one the second one is on the batch number two the third one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1322" target="_blank">00:22:02.480</a></span> | <span class="t">the batch number three etc etc such that there is no overlap between this data and every few batches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1329" target="_blank">00:22:09.200</a></span> | <span class="t">that these nodes get trained this model the gradients of each node are accumulated so summed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1335" target="_blank">00:22:15.600</a></span> | <span class="t">up into one node and then sent the sum is sent back to all the other nodes this operation is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1341" target="_blank">00:22:21.440</a></span> | <span class="t">known as all reduce then each node updates the parameters of its local model using the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1349" target="_blank">00:22:29.520</a></span> | <span class="t">received by the previous step so this the gradient receives are the previous step is the sum of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1354" target="_blank">00:22:34.400</a></span> | <span class="t">gradients of all nodes using also its own optimizer so doing optimizer dot step and then we start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1361" target="_blank">00:22:41.600</a></span> | <span class="t">again from the step number two so we train it for a few batches we have some local gradient we send</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1367" target="_blank">00:22:47.280</a></span> | <span class="t">our local gradient to a central node which will sum it up sum it up and then send it back to all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1372" target="_blank">00:22:52.320</a></span> | <span class="t">the nodes then all the nodes will update their parameters using this accumulated gradient and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1378" target="_blank">00:22:58.480</a></span> | <span class="t">and then we continue again let's see all these steps one by one in detail so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1387" target="_blank">00:23:07.200</a></span> | <span class="t">step number one our model's weight are initialized on one gpu so imagine we have a setup with four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1394" target="_blank">00:23:14.480</a></span> | <span class="t">gpus we have one gpu that will initialize the weight and send it to all the others so now the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1400" target="_blank">00:23:20.400</a></span> | <span class="t">value of the parameters this is the value of the parameter and this is the gradient so the value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1405" target="_blank">00:23:25.760</a></span> | <span class="t">of the parameter imagine is 0.1 we only have one parameter so for simplicity the initial weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1411" target="_blank">00:23:31.680</a></span> | <span class="t">are sent to all the other nodes so now all the nodes have the same weights so all the nodes now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1417" target="_blank">00:23:37.520</a></span> | <span class="t">have the same parameters the all the same value the next thing we do is each node runs a forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1426" target="_blank">00:23:46.000</a></span> | <span class="t">and backward step on one or more batch of data this will result in a local gradient being calculated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1432" target="_blank">00:23:52.720</a></span> | <span class="t">because as we saw before when we run loss dot backward we we have a gradient being calculated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1439" target="_blank">00:23:59.520</a></span> | <span class="t">of the loss function with respect to each parameter this local gradient of course may</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1445" target="_blank">00:24:05.120</a></span> | <span class="t">be different for each node because each one is training on a different subset of the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1449" target="_blank">00:24:09.600</a></span> | <span class="t">and this local gradient may also be the accumulation of one or more batch so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1454" target="_blank">00:24:14.720</a></span> | <span class="t">imagine that this gpu is training on each gpu is training on three batch so we can accumulate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1461" target="_blank">00:24:21.600</a></span> | <span class="t">the gradient for three batch and of course because we are training on a different subset of the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1466" target="_blank">00:24:26.320</a></span> | <span class="t">this cumulative gradient but still local is different for each node the next thing we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1472" target="_blank">00:24:32.640</a></span> | <span class="t">is we send all this gradient to every node sends its gradient to one single node and this node will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1479" target="_blank">00:24:39.680</a></span> | <span class="t">calculate the sum of all the gradient it receives so imagine all the nodes decide to send their own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1486" target="_blank">00:24:46.160</a></span> | <span class="t">gradient to the first gpu and the sum is calculated on the first gpu then this operation is known as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1493" target="_blank">00:24:53.600</a></span> | <span class="t">reduce and later we will see how it works then the gpu that calculated the sum of all the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1501" target="_blank">00:25:01.600</a></span> | <span class="t">will send it back to all the other nodes in a in an operation that is known as broadcast</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1506" target="_blank">00:25:06.400</a></span> | <span class="t">and the sequence of reduce and broadcast because here i show it as a separate operation usually is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1512" target="_blank">00:25:12.560</a></span> | <span class="t">implemented as a single operation known as all reduce and later we will see in detail how it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1517" target="_blank">00:25:17.840</a></span> | <span class="t">works but this is the key idea so we have the models weights that are initialized on a model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1524" target="_blank">00:25:24.320</a></span> | <span class="t">they are sent back to all the other models so now all the other gpus they they have the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1529" target="_blank">00:25:29.600</a></span> | <span class="t">weights initial weights they train it on a different subset of the data so each one has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1534" target="_blank">00:25:34.240</a></span> | <span class="t">a local gradient they send each one its own local gradient to one node which will calculate the sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1539" target="_blank">00:25:39.680</a></span> | <span class="t">of all these gradients and send it back to all the other nodes so now they have all the same sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1544" target="_blank">00:25:44.880</a></span> | <span class="t">then they run optimization steps to update the weights using the same sum so they will produce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1550" target="_blank">00:25:50.560</a></span> | <span class="t">the resulting weights of the models will be the same for all of them as you can see here so now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1556" target="_blank">00:25:56.960</a></span> | <span class="t">each node will update the value of their own parameters using the same sum of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1563" target="_blank">00:26:03.520</a></span> | <span class="t">you can see here so it's 0.3 they use 0.3 to update the value of their local parameters and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1570" target="_blank">00:26:10.000</a></span> | <span class="t">they will end up with the same weights so each node updates the parameter of its local model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1576" target="_blank">00:26:16.480</a></span> | <span class="t">using the gradient received after the updates the gradients are reset to 0 and then we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1581" target="_blank">00:26:21.920</a></span> | <span class="t">start with another loop now let's talk about collective communication primitives so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1587" target="_blank">00:26:27.680</a></span> | <span class="t">operation that we talk i showed before in distributed computing environments a node may</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1593" target="_blank">00:26:33.440</a></span> | <span class="t">need to communicate with other nodes if the communication pattern is similar to a client</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1598" target="_blank">00:26:38.000</a></span> | <span class="t">and the server then we talk about point-to-point communication because we have one client connects</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1603" target="_blank">00:26:43.760</a></span> | <span class="t">to one server in a request response chains of events however there are cases in which one node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1610" target="_blank">00:26:50.080</a></span> | <span class="t">needs to communicate to multiple receivers at once for example in the broadcast scenario right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1615" target="_blank">00:26:55.040</a></span> | <span class="t">we have one node that needs to send its weights to all the other nodes this is the typical case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1621" target="_blank">00:27:01.280</a></span> | <span class="t">of data parallel training in deep learning so one node needs to send its initial weights to all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1626" target="_blank">00:27:06.640</a></span> | <span class="t">other nodes moreover all the other nodes need to send their gradient to one single node to receive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1632" target="_blank">00:27:12.640</a></span> | <span class="t">the cumulative gradient back so collective communication allows to model the communication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1638" target="_blank">00:27:18.480</a></span> | <span class="t">pattern between group of nodes let's visualize the difference between these two modes of communication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1644" target="_blank">00:27:24.320</a></span> | <span class="t">imagine you need to send a file to seven friends with point-to-point communication you would send</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1650" target="_blank">00:27:30.800</a></span> | <span class="t">the file iteratively to each of your friend one by one suppose the internet speed that you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1656" target="_blank">00:27:36.480</a></span> | <span class="t">is one megabyte per second and the file is five megabyte in size what you do is first you send</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1663" target="_blank">00:27:43.040</a></span> | <span class="t">the imagine you are here so you have the file and you send it to your first friend and it will take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1668" target="_blank">00:27:48.240</a></span> | <span class="t">five seconds because you are sending five megabyte with the speed of one megabyte per second then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1673" target="_blank">00:27:53.840</a></span> | <span class="t">send it to the second friend the third the fourth the fifth the sixth and the seventh in total the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1678" target="_blank">00:27:58.960</a></span> | <span class="t">time to send the file to seven friends will be 35 seconds of course you may say okay but why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1685" target="_blank">00:28:05.440</a></span> | <span class="t">not send the file simultaneously to all the seven friends at the same time of course you can do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1691" target="_blank">00:28:11.760</a></span> | <span class="t">but the problem is your internet speed is still one megabyte per second and the internet speed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1698" target="_blank">00:28:18.400</a></span> | <span class="t">of sending the file simultaneously to seven friends will be split among these seven friends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1702" target="_blank">00:28:22.960</a></span> | <span class="t">so each friend will be receiving the file at a speed of 143 kilobyte per second more or less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1709" target="_blank">00:28:29.840</a></span> | <span class="t">so the total time is still 35 seconds to distribute your file to your seven friends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1715" target="_blank">00:28:35.120</a></span> | <span class="t">let's see if there is a better way so with collective communication we introduce the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1722" target="_blank">00:28:42.160</a></span> | <span class="t">first operator is known as broadcast so the operation of sending a data to all the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1726" target="_blank">00:28:46.880</a></span> | <span class="t">known although all the other nodes is known as the broadcast operator collective communication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1732" target="_blank">00:28:52.000</a></span> | <span class="t">libraries like nccl which is pronounced nickel which is a library from nvidia assigns a unique</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1740" target="_blank">00:29:00.000</a></span> | <span class="t">id to each node and this node this unique id is known as rank suppose you want to send five</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1746" target="_blank">00:29:06.720</a></span> | <span class="t">megabyte with an internet speed of one megabyte per second and let's see how the collective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1751" target="_blank">00:29:11.280</a></span> | <span class="t">communication would work in the case of this broadcast operation the first thing you do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1756" target="_blank">00:29:16.480</a></span> | <span class="t">you are here so you have the file you send it not to this friend here but to the rank number four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1762" target="_blank">00:29:22.080</a></span> | <span class="t">at the next step we realize that the rank 0 and the rank 4 have the file so they can send</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1768" target="_blank">00:29:28.080</a></span> | <span class="t">simultaneously to another friend so rank 0 now will send it to rank 2 and rank 4 will send it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1773" target="_blank">00:29:33.680</a></span> | <span class="t">to rank 6 but now we realize that rank 0 rank 2 rank 4 and rank 6 all have the file so they can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1780" target="_blank">00:29:40.000</a></span> | <span class="t">simultaneously send to it another friend so in total it will take 15 seconds to distribute your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1786" target="_blank">00:29:46.560</a></span> | <span class="t">initial file to all the seven friends compared to the 35 second with the point-to-point communication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1792" target="_blank">00:29:52.560</a></span> | <span class="t">this approach is known as divide and conquer with collective communication we exploit the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1798" target="_blank">00:29:58.960</a></span> | <span class="t">interconnectivity between nodes to avoid idle times and reduce the total communication time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1805" target="_blank">00:30:05.440</a></span> | <span class="t">this is how also your gpu sends the initial weight to all the other nodes it's not one by one because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1811" target="_blank">00:30:11.280</a></span> | <span class="t">that would be too slow not it's not even in parallel because anyway the connection speed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1816" target="_blank">00:30:16.000</a></span> | <span class="t">is always the same it would be split among all the receivers what we do is we do this divide and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1820" target="_blank">00:30:20.720</a></span> | <span class="t">conquer approach so our initial weights are sent using this broadcast operation in this manner here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1826" target="_blank">00:30:26.160</a></span> | <span class="t">which is much faster let's see the reduce operation what is the reduce operation reduce operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1832" target="_blank">00:30:32.240</a></span> | <span class="t">means that we apply it during the training when we want to calculate the sum of the gradient of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1838" target="_blank">00:30:38.320</a></span> | <span class="t">all the nodes so each node has a local gradient which was calculated using a subset of the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1843" target="_blank">00:30:43.600</a></span> | <span class="t">but they are all different from each other and what we want is to we want to calculate the sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1849" target="_blank">00:30:49.360</a></span> | <span class="t">of the all these gradients into one single node let's see how it works so the broadcast operator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1854" target="_blank">00:30:54.720</a></span> | <span class="t">is used to send the initial weights to all the other nodes when we start the training loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1860" target="_blank">00:31:00.560</a></span> | <span class="t">at every few batches of data being processed by each node the gradients of all the node needs to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1865" target="_blank">00:31:05.760</a></span> | <span class="t">be sent to one node and accumulated so summed up this operation is known as reduce let's see how it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1871" target="_blank">00:31:11.280</a></span> | <span class="t">works so imagine initially each node has its own gradient because they are training on a subset of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1877" target="_blank">00:31:17.520</a></span> | <span class="t">the data and these gradients is all different from each other what we can do is each node will send</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1884" target="_blank">00:31:24.240</a></span> | <span class="t">the gradient to his adjacent node so and the adjacent node will sum it up with the his own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1890" target="_blank">00:31:30.640</a></span> | <span class="t">gradient so the node 7 for example will send his gradient to nodes number 6 and the node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1896" target="_blank">00:31:36.160</a></span> | <span class="t">number 6 the receiver node will be responsible for calculating the sum and the same with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1901" target="_blank">00:31:41.440</a></span> | <span class="t">rank number 5 and 4 and 3 and 2 and 1 and 0 the next step we have the sum here at rank number 4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1911" target="_blank">00:31:51.920</a></span> | <span class="t">rank number 2 and rank number 0 what we do is we send the rank the sum here from rank number 6 to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1920" target="_blank">00:32:00.000</a></span> | <span class="t">rank number 4 which will calculate the sum of the sum and then also from rank number 2 to rank number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1926" target="_blank">00:32:06.800</a></span> | <span class="t">0 the rank number 0 will calculate the sum of the sum and then we do a final step in which we send</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1932" target="_blank">00:32:12.800</a></span> | <span class="t">the sum that was present at rank number 4 to the rank number 0 and this sum here is the sum of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1939" target="_blank">00:32:19.280</a></span> | <span class="t">the gradients of all nodes and in total it took us only three steps to do it so with only three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1946" target="_blank">00:32:26.160</a></span> | <span class="t">steps we accumulated the gradient of all nodes into one single node and it can be proven that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1951" target="_blank">00:32:31.920</a></span> | <span class="t">the communication time is logarithmic with respect to the number of nodes and this is very typical of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1957" target="_blank">00:32:37.840</a></span> | <span class="t">all the divide and conquer approaches the all reduce operation so what we saw before is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1965" target="_blank">00:32:45.040</a></span> | <span class="t">we are first broadcasting our data so our initial weights then we are reducing the local gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1972" target="_blank">00:32:52.160</a></span> | <span class="t">and then we need to send it back the sequence of reduce and broadcast is known as all reduce and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1978" target="_blank">00:32:58.480</a></span> | <span class="t">is usually implemented as a single operation which is faster than doing the sequence of the two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1983" target="_blank">00:33:03.760</a></span> | <span class="t">operations i will not show the algorithm behind all reduce but you can think of it as logically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1989" target="_blank">00:33:09.840</a></span> | <span class="t">as a sequence of reduce and broadcast operation but remember that in practice it's implemented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=1995" target="_blank">00:33:15.840</a></span> | <span class="t">as a single operation and it's much faster than doing the two operations alone now imagine you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2003" target="_blank">00:33:23.200</a></span> | <span class="t">are training your model on multiple gpus and one node crashes so imagine you are training in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2009" target="_blank">00:33:29.120</a></span> | <span class="t">distributed scenario like the one shown below and suddenly one node is crashed and in this case two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2015" target="_blank">00:33:35.520</a></span> | <span class="t">gpus out of four become unavailable how should the system react well one way would be to restart the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2023" target="_blank">00:33:43.040</a></span> | <span class="t">entire cluster and that's easy however by restarting the cluster the training would restart also from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2028" target="_blank">00:33:48.720</a></span> | <span class="t">zero because as you remember we start from initial weights that are randomly chosen by one node which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2033" target="_blank">00:33:53.440</a></span> | <span class="t">are then sent to all the other nodes but this would mean that we would lose all the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2038" target="_blank">00:33:58.800</a></span> | <span class="t">and computation that we have done so far when this node crashed a better approach is to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2044" target="_blank">00:34:04.320</a></span> | <span class="t">checkpointing so checkpointing means that we save the weights of a model on a shared disk every few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2051" target="_blank">00:34:11.040</a></span> | <span class="t">iteration for example every epoch and then we resume the training from the last checkpoint</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2056" target="_blank">00:34:16.080</a></span> | <span class="t">in case there is a crash so remember the step in which we initialize the weights of the model in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2063" target="_blank">00:34:23.680</a></span> | <span class="t">one node and then send it to all the others well instead of just initializing it randomly we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2068" target="_blank">00:34:28.480</a></span> | <span class="t">can initialize the weights of the model using the latest checkpoint available so that the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2073" target="_blank">00:34:33.360</a></span> | <span class="t">can continue from there we need a shared storage for saving the checkpoints because it's pytorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2083" target="_blank">00:34:43.760</a></span> | <span class="t">that will decide which node will initialize the weights and so every node should have access to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2090" target="_blank">00:34:50.080</a></span> | <span class="t">this shared storage plus it is good rule in distributed system to not have one node that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2094" target="_blank">00:34:54.800</a></span> | <span class="t">is more important than the others because any node can fail at any time so we should not make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2100" target="_blank">00:35:00.560</a></span> | <span class="t">any assumption on which is the node that will initialize the weights its pytorch will choose it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2105" target="_blank">00:35:05.120</a></span> | <span class="t">and actually usually pytorch chooses the rank number zero but we should not make assumption</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2111" target="_blank">00:35:11.520</a></span> | <span class="t">on which node will be the rank number zero so all nodes should be treated equally so they should run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2117" target="_blank">00:35:17.120</a></span> | <span class="t">the same code and they should have access to the same shared storage and who however when we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2124" target="_blank">00:35:24.000</a></span> | <span class="t">a shared storage who should be responsible for saving the checkpoint because if we make a code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2128" target="_blank">00:35:28.160</a></span> | <span class="t">in which everyone is writing the checkpoint they may end up overwriting each other so what we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2134" target="_blank">00:35:34.160</a></span> | <span class="t">is because pytorch will give us will tell us which what is the rank of the current node we will write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2140" target="_blank">00:35:40.640</a></span> | <span class="t">the code in such a way that we check the rank of the current node and if it's the rank number zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2144" target="_blank">00:35:44.640</a></span> | <span class="t">then we save the checkpoint if it's a rank number one two or three we don't do anything so it means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2150" target="_blank">00:35:50.080</a></span> | <span class="t">that only one rank will be responsible for saving the checkpoint but later when we restart the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2155" target="_blank">00:35:55.600</a></span> | <span class="t">training we don't make any assumption on who will become the rank number zero and as the pytorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2161" target="_blank">00:36:01.600</a></span> | <span class="t">documentation says it says that the the rank is not stable means that if you restart the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2169" target="_blank">00:36:09.760</a></span> | <span class="t">it the rank number zero may be assigned to another node okay now that we have seen how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2176" target="_blank">00:36:16.960</a></span> | <span class="t">distributed training works at the theoretical level so we accumulate the gradient that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2183" target="_blank">00:36:23.440</a></span> | <span class="t">calculated locally by multiple nodes and then send it to one single node which then calculates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2188" target="_blank">00:36:28.720</a></span> | <span class="t">the sum and send it back to all the others which then update the parameter using this sum of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2194" target="_blank">00:36:34.320</a></span> | <span class="t">gradients it's time to actually look at the practical training so we will first build the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2201" target="_blank">00:36:41.040</a></span> | <span class="t">infrastructure that is needed to run our training and then i will also show you what are the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2208" target="_blank">00:36:48.160</a></span> | <span class="t">changes that we need to make to our existing training loop to make it distributed and run it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2213" target="_blank">00:36:53.120</a></span> | <span class="t">on the infrastructure that we are going to create i will use paper space as a cloud service mostly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2219" target="_blank">00:36:59.840</a></span> | <span class="t">because it's easy to use and doesn't require much setup time even for beginners i chose it over aws</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2226" target="_blank">00:37:06.240</a></span> | <span class="t">because aws has a lot of other setup that you need to do in order to even do simple operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2231" target="_blank">00:37:11.840</a></span> | <span class="t">and it's easy to get lost so i'm using paper space mostly for this reason so that anyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2236" target="_blank">00:37:16.480</a></span> | <span class="t">without with any kind of level of knowledge can do the same can follow the tutorial very easily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2242" target="_blank">00:37:22.400</a></span> | <span class="t">so let's get started okay the first thing we need to do is to go to my repository called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2249" target="_blank">00:37:29.600</a></span> | <span class="t">pytorch transformer distributed in which you will find the code of the distributed model that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2255" target="_blank">00:37:35.040</a></span> | <span class="t">will be running on this cluster and also the instruction on how to create this cluster on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2259" target="_blank">00:37:39.840</a></span> | <span class="t">paper space so i have already accessed my account on paper space the first thing you will want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2265" target="_blank">00:37:45.600</a></span> | <span class="t">do is to create your ssh key just like you do on github so that you can use this your public ssh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2272" target="_blank">00:37:52.320</a></span> | <span class="t">key to you need to associate your public ssh key to your account here so that you can connect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2279" target="_blank">00:37:59.360</a></span> | <span class="t">the machines that are created on paper space okay the first thing we need to do is to create a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2285" target="_blank">00:38:05.520</a></span> | <span class="t">private network on which we will connect all these two machines so we will create two machines that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2290" target="_blank">00:38:10.000</a></span> | <span class="t">are connected in this private network and also a shared disk that can be accessed by both machines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2295" target="_blank">00:38:15.840</a></span> | <span class="t">we will use a machine that has two gpus so in total we will have four gpus on which we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2300" target="_blank">00:38:20.560</a></span> | <span class="t">run the training so let's create our private network networks here and we create a new network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2308" target="_blank">00:38:28.160</a></span> | <span class="t">remember to choose the same region for your computers and the cluster and we will call it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2312" target="_blank">00:38:32.640</a></span> | <span class="t">distributed training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2317" target="_blank">00:38:37.760</a></span> | <span class="t">okay now we have our subnet you can see here the next step to do is to create two nodes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2327" target="_blank">00:38:47.440</a></span> | <span class="t">of type of we can choose any node actually i i choose this one because i wanted to test two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2332" target="_blank">00:38:52.800</a></span> | <span class="t">machines each one having two gpus and we use ml in a box as the operating system so as the image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2338" target="_blank">00:38:58.400</a></span> | <span class="t">of these machines so we create a new machine ml in a box the machine we will do is multi gpu p4000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2346" target="_blank">00:39:06.800</a></span> | <span class="t">multiplied by two this one the region is the same as the network so new york 2 the disk size 50 gb</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2353" target="_blank">00:39:13.600</a></span> | <span class="t">is enough in total i think last time i created this cluster and i ran it for many epochs i spent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2360" target="_blank">00:39:20.400</a></span> | <span class="t">five dollars so i think you should not spend more than five dollars for running your own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2365" target="_blank">00:39:25.040</a></span> | <span class="t">distributed training it should not be more expensive than five dollars the first machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2371" target="_blank">00:39:31.440</a></span> | <span class="t">we will call it cuda zero the network we need to select the network that we have created before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2377" target="_blank">00:39:37.680</a></span> | <span class="t">and we choose public ip dynamic because otherwise we cannot connect to the machine if without a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2382" target="_blank">00:39:42.080</a></span> | <span class="t">public ip then we never create a snapshot because we don't care about backup for now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2387" target="_blank">00:39:47.120</a></span> | <span class="t">and the price of running each machine should be around one dollar you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2390" target="_blank">00:39:50.560</a></span> | <span class="t">so we created the first machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2394" target="_blank">00:39:54.000</a></span> | <span class="t">and then we create also the second one so ml in a box the machine is this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2405" target="_blank">00:40:05.280</a></span> | <span class="t">new york disk size this one we will call it cuda one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2410" target="_blank">00:40:10.720</a></span> | <span class="t">distributed training dynamic we don't save and create machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2418" target="_blank">00:40:18.880</a></span> | <span class="t">last but not least we need to create a network drive of 250 gigabytes we can create this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2425" target="_blank">00:40:25.840</a></span> | <span class="t">smallest one they have available so that's why i chose 250 gigabytes we will call it model training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2432" target="_blank">00:40:32.720</a></span> | <span class="t">250 new york and distributed train this must belong to the same network as the two machines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2438" target="_blank">00:40:38.320</a></span> | <span class="t">okay create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2442" target="_blank">00:40:42.240</a></span> | <span class="t">okay so they are provisioning and starting the two machines now we will configure the machines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2451" target="_blank">00:40:51.760</a></span> | <span class="t">so we need to install some packages first of all we need to install ifconfig because we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2456" target="_blank">00:40:56.080</a></span> | <span class="t">have the ip address and while installing ifconfig i ran into a error with a seahorse and i show also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2463" target="_blank">00:41:03.120</a></span> | <span class="t">how to solve this error we will then mount the network drive and then we will clone this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2468" target="_blank">00:41:08.160</a></span> | <span class="t">repository install all the packages that we need for running this code and then we initial i also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2475" target="_blank">00:41:15.520</a></span> | <span class="t">recommend using weights and biases for uh keeping tracking keeping track of the loss etc of all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2481" target="_blank">00:41:21.680</a></span> | <span class="t">metrics that you need during the training of this model so i recommend you register on weights and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2487" target="_blank">00:41:27.840</a></span> | <span class="t">biases and then you install it and use it also for this for this code for running this code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2494" target="_blank">00:41:34.320</a></span> | <span class="t">because it will make you make it easy for you to visualize the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2497" target="_blank">00:41:37.360</a></span> | <span class="t">metrics okay the cuda zero is ready we can see some information here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2507" target="_blank">00:41:47.760</a></span> | <span class="t">and connect this will give you us the ip address</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2513" target="_blank">00:41:53.280</a></span> | <span class="t">we connect to it yes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2518" target="_blank">00:41:58.480</a></span> | <span class="t">wonderful so now we are inside the machine the first thing we do is we update all the packages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2532" target="_blank">00:42:12.320</a></span> | <span class="t">okay we do also install the net tools but i remember it will run into an into an error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2539" target="_blank">00:42:19.920</a></span> | <span class="t">but looks like this time it doesn't which is good let's try ifconfig wonderful so now we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2548" target="_blank">00:42:28.320</a></span> | <span class="t">the ip address of the first machine which is this one this is the private address that belongs to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2552" target="_blank">00:42:32.800</a></span> | <span class="t">your subnet that you created here so the one you created here 810 okay now we can do the we we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2565" target="_blank">00:42:45.440</a></span> | <span class="t">to keep track of this ip address because we later we need to modify the host and this is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2569" target="_blank">00:42:49.920</a></span> | <span class="t">needed because i ran into an error with the pytorch when running distributed training because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2574" target="_blank">00:42:54.720</a></span> | <span class="t">it could not um it could not reach the other node so i had to modify the host by mapping the host</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2580" target="_blank">00:43:00.640</a></span> | <span class="t">name of the other nodes into its ip uh okay i let's mount the network drive following just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2588" target="_blank">00:43:08.800</a></span> | <span class="t">the instructions i have written so we install this package let me call this one cuda zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2594" target="_blank">00:43:14.880</a></span> | <span class="t">okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2606" target="_blank">00:43:26.960</a></span> | <span class="t">then we created the directory in which we want to mount this network drive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2611" target="_blank">00:43:31.280</a></span> | <span class="t">and then we have to run this command here you can see it here but we need to replace the ip address</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2619" target="_blank">00:43:39.680</a></span> | <span class="t">and the username and password of the drive so let's first paste it then we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2626" target="_blank">00:43:46.640</a></span> | <span class="t">replace this one with the ip address and the network share name which i'll show you how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2632" target="_blank">00:43:52.240</a></span> | <span class="t">find we go here drives and we can see here the address so we do but we need to replace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2645" target="_blank">00:44:05.440</a></span> | <span class="t">the escape character with the slash slash okay and the network drive username is this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2659" target="_blank">00:44:19.040</a></span> | <span class="t">let's run this command the password is also here we can copy it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2671" target="_blank">00:44:31.840</a></span> | <span class="t">et voila now it should be mounted the first thing we do is we clone our repository so we are in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2678" target="_blank">00:44:38.240</a></span> | <span class="t">home directory of the default user so paper space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2683" target="_blank">00:44:43.280</a></span> | <span class="t">we can clone it directly here no problem we then cd</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2688" target="_blank">00:44:48.560</a></span> | <span class="t">and then we install the requirements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2697" target="_blank">00:44:57.840</a></span> | <span class="t">okay now we have installed the requirements so now we can log in into weights and biases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2703" target="_blank">00:45:03.280</a></span> | <span class="t">using this command here but remember to copy the key from the website of weights and biases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2709" target="_blank">00:45:09.840</a></span> | <span class="t">so which can be found here let's run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2714" target="_blank">00:45:14.560</a></span> | <span class="t">and it should be okay okay now we are ready to run the training command on the first computer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2726" target="_blank">00:45:26.560</a></span> | <span class="t">but we need to also prepare the second one so let me prepare the second one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2730" target="_blank">00:45:30.000</a></span> | <span class="t">of course in a real scenario you would create a docker file and you would run everything using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2739" target="_blank">00:45:39.360</a></span> | <span class="t">kubernetes so it should be done automatically but in this case because most of you maybe are not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2745" target="_blank">00:45:45.200</a></span> | <span class="t">familiar with kubernetes or docker or you just want to run very fast your training to see how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2751" target="_blank">00:45:51.280</a></span> | <span class="t">so i recommend using paper space because it's very easy to configure all the setup</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2770" target="_blank">00:46:10.480</a></span> | <span class="t">okay just like before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2799" target="_blank">00:46:39.200</a></span> | <span class="t">okay now we also clone here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2801" target="_blank">00:46:41.120</a></span> | <span class="t">so we clone the same repository on both computers and then we run the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2808" target="_blank">00:46:48.080</a></span> | <span class="t">same code for training but we will see later how to do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2821" target="_blank">00:47:01.520</a></span> | <span class="t">okay now we do login with weights and biases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2827" target="_blank">00:47:07.200</a></span> | <span class="t">and now we are ready to run the training on both computers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2837" target="_blank">00:47:17.760</a></span> | <span class="t">okay the command that you need to run to in order to run the training on both machine is the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2844" target="_blank">00:47:24.720</a></span> | <span class="t">but we first need to take the ip address of the computer that we will choose so one of the two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2850" target="_blank">00:47:30.160</a></span> | <span class="t">computer will become the rendezvous master it means that all the communication will be managed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2854" target="_blank">00:47:34.800</a></span> | <span class="t">by that node and all the others will adapt now of course to make it more fail safe we need to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2863" target="_blank">00:47:43.360</a></span> | <span class="t">we can use for example a dynamic ip that can be mapped to another machine in case the master</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2869" target="_blank">00:47:49.120</a></span> | <span class="t">crashes so that we can restart the training using always the same ip but in this simple case i will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2875" target="_blank">00:47:55.440</a></span> | <span class="t">configure it by hand otherwise i need an orchestration tool and that will complicate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2880" target="_blank">00:48:00.000</a></span> | <span class="t">all the scenario so in this case i just want to show you how to do distributed training i don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2884" target="_blank">00:48:04.400</a></span> | <span class="t">want to spend too much time creating the perfect infrastructure which you would ideally do in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2889" target="_blank">00:48:09.680</a></span> | <span class="t">real scenario so we take this command here and as you can see in this command we need to tell</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2896" target="_blank">00:48:16.960</a></span> | <span class="t">the ip address of the master node here so how which one we will choose the master in my case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2902" target="_blank">00:48:22.400</a></span> | <span class="t">i will choose cuda0 so the first machine i have created and the other one will be kind of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2906" target="_blank">00:48:26.400</a></span> | <span class="t">slave even if they both perform the same operation so we find the ip of this slave here which is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2915" target="_blank">00:48:35.120</a></span> | <span class="t">one and we put it in the host file of the master and also i need the host name of the slave which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2922" target="_blank">00:48:42.000</a></span> | <span class="t">is this one perfect so okay okay so we need to paste here the ip address of this node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2934" target="_blank">00:48:54.880</a></span> | <span class="t">and also its host name which is this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2939" target="_blank">00:48:59.200</a></span> | <span class="t">and that's pretty much it now we can start the training so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2946" target="_blank">00:49:06.400</a></span> | <span class="t">we take the ip address of the master now so cuda0 is our master which is this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2952" target="_blank">00:49:12.640</a></span> | <span class="t">and we put it in the command in this position here and we cd to the torch and we can run the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2962" target="_blank">00:49:22.240</a></span> | <span class="t">command so in this command what we are saying is torch run is a special command that will create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2967" target="_blank">00:49:27.280</a></span> | <span class="t">the cluster and will manage all the cluster creation and communication it will run our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2972" target="_blank">00:49:32.000</a></span> | <span class="t">training loop you can see here first of all we need to tell how many process we have in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2978" target="_blank">00:49:38.400</a></span> | <span class="t">cluster so we have two computers and how many process we want to create for each node so how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2983" target="_blank">00:49:43.600</a></span> | <span class="t">many compute how many gpus we have for each computer so this is the number of gpus per</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2988" target="_blank">00:49:48.320</a></span> | <span class="t">computer and this is the number of computers so we have two and two this is a unique id that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2993" target="_blank">00:49:53.360</a></span> | <span class="t">indicates this particular cluster it should be unique for each cluster and this is the back-end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=2999" target="_blank">00:49:59.280</a></span> | <span class="t">library that is managing the communication for us all the parameters after the file name that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3004" target="_blank">00:50:04.000</a></span> | <span class="t">we will use for training are the arguments that are passed to this file here so in our case we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3009" target="_blank">00:50:09.200</a></span> | <span class="t">are running with a batch size of 8 we are also telling him that the model folder so where the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3014" target="_blank">00:50:14.000</a></span> | <span class="t">checkpoint should be saved are this is the shared folder that we created before so the mount file</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3020" target="_blank">00:50:20.160</a></span> | <span class="t">of the shared network drive we run the same command on both computers and this will start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3025" target="_blank">00:50:25.280</a></span> | <span class="t">the training so we do it this one and also on the other one as you can see this computer here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3035" target="_blank">00:50:35.520</a></span> | <span class="t">is not proceeding it's waiting for the other so now we run also it here and now it should proceed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3047" target="_blank">00:50:47.840</a></span> | <span class="t">yeah so now both are proceeding oops i forgot to set the host file on this computer here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3057" target="_blank">00:50:57.840</a></span> | <span class="t">so i retrieved the ip of this one so ip ip of this one and also the host name of this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3065" target="_blank">00:51:05.680</a></span> | <span class="t">and we put in the host file of the other computer so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3076" target="_blank">00:51:16.480</a></span> | <span class="t">okay let's run again the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3093" target="_blank">00:51:33.520</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3106" target="_blank">00:51:46.320</a></span> | <span class="t">looks like it's proceeding so they are both doing yeah they are both building the data set now the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3120" target="_blank">00:52:00.720</a></span> | <span class="t">tokenizer so if you have watched my previous video on how to code a transformer model from zero this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3125" target="_blank">00:52:05.760</a></span> | <span class="t">is exactly the same code except that we i have added some things to manage the distributed training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3131" target="_blank">00:52:11.440</a></span> | <span class="t">but it's very minimal code change and later i will show you step by step how to how i done so as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3136" target="_blank">00:52:16.160</a></span> | <span class="t">can see now the training is running in parallel the first thing you will notice is that the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3140" target="_blank">00:52:20.720</a></span> | <span class="t">and biases is only initialized on one node because we cannot send the metrics from multiple nodes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3146" target="_blank">00:52:26.160</a></span> | <span class="t">because it will interfere with each other so we send the metrics only from one node and it's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3151" target="_blank">00:52:31.360</a></span> | <span class="t">node with rank number zero we will see later how we check this information as you can see they are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3156" target="_blank">00:52:36.640</a></span> | <span class="t">both training on a subset of the data so this one is training on 910 batch and this one also 910</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3162" target="_blank">00:52:42.000</a></span> | <span class="t">batch it means that in total we have 1820 batches in total and each one is calculating a local</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3170" target="_blank">00:52:50.880</a></span> | <span class="t">gradient and sending it to the other who will calculate the sum of this gradient actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3176" target="_blank">00:52:56.400</a></span> | <span class="t">we have four nodes here because we have four gpu so each gpu is calculating a local gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3180" target="_blank">00:53:00.880</a></span> | <span class="t">and each each gradient is sent to the rank number zero which will calculate the sum of all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3191" target="_blank">00:53:11.360</a></span> | <span class="t">gradients using the reduce operation actually the old reduce operation because then it will send</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3196" target="_blank">00:53:16.640</a></span> | <span class="t">back the sum to all the other nodes who will then update the parameters using the sum of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3202" target="_blank">00:53:22.320</a></span> | <span class="t">gradients that it has received another thing i made a mistake is that this 910 is not multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3208" target="_blank">00:53:28.240</a></span> | <span class="t">by 2 because it's not the the rank 2 is actually later we will see what is the difference between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3213" target="_blank">00:53:33.280</a></span> | <span class="t">local rank and the global rank but we have four nodes each node is working on 910 batches of data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3220" target="_blank">00:53:40.640</a></span> | <span class="t">so i only show one because tqdm will otherwise if i show the tqdm for both gpus it will interfere</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3228" target="_blank">00:53:48.880</a></span> | <span class="t">with each other this progress bar basically here so i only show one progress bar per computer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3233" target="_blank">00:53:53.760</a></span> | <span class="t">not two because each computer has two gpus so actually i should have two progress bar but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3238" target="_blank">00:53:58.720</a></span> | <span class="t">i only show one otherwise the visualization is very bad so first of all let's navigate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3244" target="_blank">00:54:04.000</a></span> | <span class="t">the code to understand how it works let me open the project</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3253" target="_blank">00:54:13.840</a></span> | <span class="t">okay let's see here let's start from the train file</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3269" target="_blank">00:54:29.840</a></span> | <span class="t">okay the main difference that we have compared to the original code that i built in my previous video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3276" target="_blank">00:54:36.560</a></span> | <span class="t">but it's the same this code with these changes that i'm showing here will apply to any training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3281" target="_blank">00:54:41.440</a></span> | <span class="t">loop that you have built so it's not only for this one this particular code this will apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3285" target="_blank">00:54:45.200</a></span> | <span class="t">to anyone okay the first thing we do is we read the these two variables that are so when we run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3292" target="_blank">00:54:52.800</a></span> | <span class="t">the code with the torch run torch run will insert some environment variables into our environment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3298" target="_blank">00:54:58.560</a></span> | <span class="t">that we can read one is called rank and one is called local rank let's see the difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3303" target="_blank">00:55:03.200</a></span> | <span class="t">okay the local rank basically indicates the number of the gpu in the local computer while</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3312" target="_blank">00:55:12.560</a></span> | <span class="t">the global rank or also called just rank indicates the number unique id of the gpu among all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3320" target="_blank">00:55:20.000</a></span> | <span class="t">cluster so if we have four gpus the rank will be unique among all the cluster while the local rank</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3326" target="_blank">00:55:26.960</a></span> | <span class="t">is not unique but it's unique to the local computer the local rank is useful for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3332" target="_blank">00:55:32.240</a></span> | <span class="t">when we want to print for example only on one gpu per each computer while the global rank is useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3338" target="_blank">00:55:38.800</a></span> | <span class="t">when we want to only one gpu to perform an operation among all the others for example if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3345" target="_blank">00:55:45.360</a></span> | <span class="t">we want to initialize weights and biases or any other service that should be initialized only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3349" target="_blank">00:55:49.600</a></span> | <span class="t">from one node in all the cluster then we use the global rank which is this environment variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3355" target="_blank">00:55:55.360</a></span> | <span class="t">here while if we want to use something for example for printing or only one local gpu should use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3362" target="_blank">00:56:02.160</a></span> | <span class="t">tqdm or the progress bar or any other stuff that is can interfere with each other on the local</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3367" target="_blank">00:56:07.760</a></span> | <span class="t">computer then we use the local rank so the first thing i do is i load these two environment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3373" target="_blank">00:56:13.200</a></span> | <span class="t">variables and save it into the configuration the second thing i do okay i print the configuration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3378" target="_blank">00:56:18.560</a></span> | <span class="t">this first thing we need to do to initialize the cluster and this is where the torch run will stop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3385" target="_blank">00:56:25.200</a></span> | <span class="t">waiting for all the nodes to connect is to call this function here init process group</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3389" target="_blank">00:56:29.360</a></span> | <span class="t">this init process group belongs to a package that i imported here called torch.distributed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3396" target="_blank">00:56:36.960</a></span> | <span class="t">so these are the imports that we need to make in order to use distributed training so torch.utils.data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3403" target="_blank">00:56:43.440</a></span> | <span class="t">distributed we need this distributed sampler this distributed data parallel this init process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3408" target="_blank">00:56:48.640</a></span> | <span class="t">group and destroy process group so this is the first thing we do so we read the environment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3413" target="_blank">00:56:53.360</a></span> | <span class="t">variables we save it somewhere so we can access the local and the global rank we call this function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3418" target="_blank">00:56:58.640</a></span> | <span class="t">here indicating the backend that we want to use i am using cuda so i want to use nickel so which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3424" target="_blank">00:57:04.720</a></span> | <span class="t">nccl we can also use the local rank to tell cuda which is the gpu we will be doing training upon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3431" target="_blank">00:57:11.360</a></span> | <span class="t">so each computer will have the will be given each gpu will be given its local rank with relative to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3437" target="_blank">00:57:17.360</a></span> | <span class="t">the current computer so the first gpu will be assigned local rank zero and the second gpu will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3443" target="_blank">00:57:23.120</a></span> | <span class="t">be assigned the local rank one on each computer so we set here then we run the training loop and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3449" target="_blank">00:57:29.360</a></span> | <span class="t">after the training loop has finished we do destroy process group let's see the detail of this training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3454" target="_blank">00:57:34.080</a></span> | <span class="t">group so when we do the training loop first of all here i disable the training on the cpu because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3460" target="_blank">00:57:40.240</a></span> | <span class="t">because we are using a distributed data parallel so it only works with the backend the nickel it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3464" target="_blank">00:57:44.560</a></span> | <span class="t">only works with cuda so i'm disabling on the cpu the second thing we do is when we create the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3471" target="_blank">00:57:51.920</a></span> | <span class="t">data loader so here so the train data loader and the validation data data loader we need to disable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3478" target="_blank">00:57:58.960</a></span> | <span class="t">the shuffling on the data loader and introduce this parameter sampler sampler and we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3484" target="_blank">00:58:04.640</a></span> | <span class="t">pass an instance of the distributed sampler here in which we tell him what is the data set and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3490" target="_blank">00:58:10.560</a></span> | <span class="t">want to shuffle it so we shuffle it using distributed sampler not the data loader here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3497" target="_blank">00:58:17.120</a></span> | <span class="t">and the next thing we do is we create the code the logic for pre-loading the checkpoint so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3503" target="_blank">00:58:23.920</a></span> | <span class="t">check if we want to preload the checkpoint in my case the default configuration says that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3508" target="_blank">00:58:28.160</a></span> | <span class="t">the we should always load the latest checkpoint you can see it here so the configuration the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3512" target="_blank">00:58:32.960</a></span> | <span class="t">default configuration says that we should load the pre-trained the the latest checkpoint available</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3518" target="_blank">00:58:38.560</a></span> | <span class="t">so the latest checkpoint file is retrieved using the path that we passed using the command so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3525" target="_blank">00:58:45.200</a></span> | <span class="t">remember here we have a path that indicates what is the directory where we save the checkpoints and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3532" target="_blank">00:58:52.240</a></span> | <span class="t">we use this one if there is a latest checkpoint this will be loaded in our model so our model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3539" target="_blank">00:58:59.440</a></span> | <span class="t">is created here so we create an instance of our model which is just basically where you get the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3546" target="_blank">00:59:06.880</a></span> | <span class="t">instance of your model and then you can preload some the state dict here and also the optimizer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3554" target="_blank">00:59:14.000</a></span> | <span class="t">state dict and all the other global variables that should be pre-loaded for for resume the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3560" target="_blank">00:59:20.640</a></span> | <span class="t">if the global config rank is zero we also initialize the services like weights and bias</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3568" target="_blank">00:59:28.240</a></span> | <span class="t">you know in the case of weights and bias we also may want to resume the training from the same run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3572" target="_blank">00:59:32.640</a></span> | <span class="t">in which we last crashed so every time we save the checkpoint we save also the ran the run id of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3578" target="_blank">00:59:38.080</a></span> | <span class="t">weights and bias run and we can restore it if we restore the checkpoint the big part that introduces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3587" target="_blank">00:59:47.920</a></span> | <span class="t">the distributed parallel training is this so now we have our model which could be anything in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3592" target="_blank">00:59:52.960</a></span> | <span class="t">case of my code is the transformer model so it's a special class that i created in my previous video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3598" target="_blank">00:59:58.560</a></span> | <span class="t">this one here it's just nn.model this one here we need to wrap it in an instance of distributed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3606" target="_blank">01:00:06.000</a></span> | <span class="t">data model you can see here and we also indicate what is the device id we will be using so the gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3611" target="_blank">01:00:11.520</a></span> | <span class="t">id which is the local rank we also okay with the global rank zero we can also initialize other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3618" target="_blank">01:00:18.480</a></span> | <span class="t">stuff this in this case is just weights and bias the training loop is the same as the non-parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3624" target="_blank">01:00:24.800</a></span> | <span class="t">code except that instead of running directly the model so if we before do model.forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3633" target="_blank">01:00:33.680</a></span> | <span class="t">we need to do model.module.forward so if we need to access the encode method of our model which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3640" target="_blank">01:00:40.880</a></span> | <span class="t">for example this method here we cannot access model.encode we need to access model.module.encode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3648" target="_blank">01:00:48.080</a></span> | <span class="t">because this model refers to an instance of distributed data parallel if we want to retrieve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3655" target="_blank">01:00:55.760</a></span> | <span class="t">the original model we need to do model.module and every time we run loss.backward on this module</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3663" target="_blank">01:01:03.520</a></span> | <span class="t">PyTorch will intercept this and will calculate the gradient the local gradient and after it has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3670" target="_blank">01:01:10.480</a></span> | <span class="t">calculated the local gradient it will send it to all the other nodes for accumulation and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3674" target="_blank">01:01:14.960</a></span> | <span class="t">receive back the accumulated gradient and then this up this cumulative gradient is used when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3681" target="_blank">01:01:21.120</a></span> | <span class="t">we use optimizer.step and then optimizer.0 will set it to reset it to zero when we want to save</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3690" target="_blank">01:01:30.400</a></span> | <span class="t">the checkpoint which for example we can do every epoch in my case i do it every epoch i save it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3697" target="_blank">01:01:37.920</a></span> | <span class="t">only on the global rank number zero because i don't want to save the checkpoint on all the nodes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3703" target="_blank">01:01:43.440</a></span> | <span class="t">otherwise they will be overwriting each other's checkpoint so i only do it on the global rank zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3708" target="_blank">01:01:48.640</a></span> | <span class="t">as you can see it here and this is pretty much what we need to do i let me show you a template</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3716" target="_blank">01:01:56.080</a></span> | <span class="t">that you can follow for your future projects on how to integrate the distributed data parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3720" target="_blank">01:02:00.640</a></span> | <span class="t">in your existing code so let's see okay this is a template that you should follow for your future</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3728" target="_blank">01:02:08.560</a></span> | <span class="t">vid for future projects the first thing you do is you read these two environment variables called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3733" target="_blank">01:02:13.680</a></span> | <span class="t">the local rank and rank the one the first one indicates the local rank so the number we saw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3738" target="_blank">01:02:18.720</a></span> | <span class="t">before so the number of the gpu relative to the local computer not to the entire cluster the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3744" target="_blank">01:02:24.000</a></span> | <span class="t">second one indicates the global rank which is a unique identifier for this particular gpu among</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3749" target="_blank">01:02:29.120</a></span> | <span class="t">all the gpus of the cluster we then call the function init process group we indicated the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3753" target="_blank">01:02:33.920</a></span> | <span class="t">backend we want to use for in case of cuda which i think is most of the cases is nickel because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3758" target="_blank">01:02:38.960</a></span> | <span class="t">it's the best one and it's made by nvidia so the second thing we do is we set which device</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3764" target="_blank">01:02:44.720</a></span> | <span class="t">cuda should use which is the local rank then we run the training loop the training loop only on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3770" target="_blank">01:02:50.640</a></span> | <span class="t">the global rank number zero we initialize the services like weights and biases we create our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3777" target="_blank">01:02:57.440</a></span> | <span class="t">data loader by telling him that as a sampler it should use the distributed sampler here and using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3782" target="_blank">01:03:02.960</a></span> | <span class="t">shuffle but not shuffle here on the data loader we create an instance of our model which is our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3788" target="_blank">01:03:08.720</a></span> | <span class="t">custom class indicating our model and if there is a latest checkpoint we can load it now after we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3795" target="_blank">01:03:15.280</a></span> | <span class="t">have loaded the latest checkpoint we need to wrap our model into an instance of distributed data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3800" target="_blank">01:03:20.560</a></span> | <span class="t">parallel by also indicating the device id it should use which is the local rank all the rest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3806" target="_blank">01:03:26.320</a></span> | <span class="t">is the same except that on the global rank number zero we can collect some statistics so for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3811" target="_blank">01:03:31.760</a></span> | <span class="t">we can send some statistics to weights and biases and only on the global rank number zero we save</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3818" target="_blank">01:03:38.080</a></span> | <span class="t">the state of the model every epoch for example so that if the training crashes we can resume from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3824" target="_blank">01:03:44.960</a></span> | <span class="t">the latest checkpoint let's try to simulate a crash so let's go here let's see how is the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3833" target="_blank">01:03:53.200</a></span> | <span class="t">going so as you can see now we are already training on the epoch number one so one epoch is already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3838" target="_blank">01:03:58.640</a></span> | <span class="t">done and our code should be in such a way that as you can see the code i built here we are running</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3845" target="_blank">01:04:05.440</a></span> | <span class="t">validation only on the global rank number zero we actually you don't have to run validation during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3853" target="_blank">01:04:13.120</a></span> | <span class="t">this distributed training you can do it asynchronously creating another for example node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3858" target="_blank">01:04:18.400</a></span> | <span class="t">that reads the latest checkpoint and runs the validation asynchronously so that our gpu can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3864" target="_blank">01:04:24.960</a></span> | <span class="t">work directly always on the training and not wasting time for validation because otherwise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3871" target="_blank">01:04:31.840</a></span> | <span class="t">all the nodes are waiting for the node number zero to run validation and this can be expensive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3877" target="_blank">01:04:37.760</a></span> | <span class="t">for bigger models so we can do it asynchronously okay now we can see that the node number zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3886" target="_blank">01:04:46.560</a></span> | <span class="t">the all the nodes are training the epoch number one as you can see here so i will try to simulate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3891" target="_blank">01:04:51.920</a></span> | <span class="t">a crash and and see that the the distributed training will resume from the checkpoint of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3898" target="_blank">01:04:58.960</a></span> | <span class="t">epoch number zero that we have already completed so let's make it crash i will crash the master</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3906" target="_blank">01:05:06.560</a></span> | <span class="t">node directly so we can try the worst case scenario this one has crashed so i just killed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3913" target="_blank">01:05:13.280</a></span> | <span class="t">the node and we see that this one should also be killed after a while</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3917" target="_blank">01:05:17.360</a></span> | <span class="t">okay this one is also killed we can restart using the same command as before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3935" target="_blank">01:05:35.120</a></span> | <span class="t">as you can see they are loading the dataset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3938" target="_blank">01:05:38.800</a></span> | <span class="t">loading the tokenizer and preloading the model as you can see here the model that was previously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3947" target="_blank">01:05:47.600</a></span> | <span class="t">saved by the global rank number zero so now they should resume the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3951" target="_blank">01:05:51.440</a></span> | <span class="t">training from the first epoch not from the zeroth epoch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3960" target="_blank">01:06:00.400</a></span> | <span class="t">and yes they are starting from the epoch number zero one so because the epoch number zero zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3965" target="_blank">01:06:05.200</a></span> | <span class="t">has already been done and this is how failover works so you basically save a checkpoint every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3971" target="_blank">01:06:11.600</a></span> | <span class="t">once in a while then you can restart the training from that checkpoint when you save a checkpoint</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3976" target="_blank">01:06:16.880</a></span> | <span class="t">you should also save all the variables that are relevant to resuming the training so if you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3982" target="_blank">01:06:22.000</a></span> | <span class="t">some state variables like for example a global step counter or you have some other counters that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3988" target="_blank">01:06:28.000</a></span> | <span class="t">you need to keep track of you can save them in the checkpoint and then restore them when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=3991" target="_blank">01:06:31.680</a></span> | <span class="t">load the checkpoint okay let's stop the training here you can try to experiment with yourself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4001" target="_blank">01:06:41.440</a></span> | <span class="t">with the paper space i think it's quite easy and let me know if you have any problem i will try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4005" target="_blank">01:06:45.520</a></span> | <span class="t">help you okay now let's look at how distributed parallel training so distributed data parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4011" target="_blank">01:06:51.920</a></span> | <span class="t">was integrated into pytorch so because there are some clever design choices into this to make it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4017" target="_blank">01:06:57.680</a></span> | <span class="t">very fast let's review them so first of all when does pytorch synchronize the gradients well this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4024" target="_blank">01:07:04.160</a></span> | <span class="t">happens when we call loss.backward because this when we call loss.backward this will lead to each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4030" target="_blank">01:07:10.240</a></span> | <span class="t">node calculating its local gradient which is the derivative of the loss function with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4035" target="_blank">01:07:15.280</a></span> | <span class="t">each node in the computation graph and each node will send its local gradient to a central node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4041" target="_blank">01:07:21.200</a></span> | <span class="t">which is the global rank number zero and receives back the cumulative gradient through an operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4046" target="_blank">01:07:26.240</a></span> | <span class="t">that is all reduce each node will then update its weights so the parameters of the model using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4052" target="_blank">01:07:32.560</a></span> | <span class="t">cumulative gradient and of course the learning rate of the local optimizer we can avoid pytorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4059" target="_blank">01:07:39.120</a></span> | <span class="t">synchronizing the gradient at every backward step and instead let it accumulate it for a few steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4064" target="_blank">01:07:44.800</a></span> | <span class="t">for example for a few batches using the no sync context let's see how to integrate it so usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4071" target="_blank">01:07:51.760</a></span> | <span class="t">you run lost the forward and forward step then you run the backward step this will lead to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4078" target="_blank">01:07:58.800</a></span> | <span class="t">synchronization then you do optimizer.step and then optimizer.zerograd however if we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4086" target="_blank">01:08:06.240</a></span> | <span class="t">do it every few epochs we don't want to run the synchronization at every step we can use the no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4092" target="_blank">01:08:12.400</a></span> | <span class="t">sync context you can see here which is a method provided by distributed data parallel which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4098" target="_blank">01:08:18.080</a></span> | <span class="t">the wrapper of our model and basically we create this context manager this we run the forward step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4104" target="_blank">01:08:24.640</a></span> | <span class="t">and the backward step and we don't call optimizer.step when we are outside of this context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4111" target="_blank">01:08:31.040</a></span> | <span class="t">we still need to do the forward and the backward step then we can run optimizer.step and zerograd</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4115" target="_blank">01:08:35.920</a></span> | <span class="t">basically this no sync will disable the synchronization of the gradient but it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4120" target="_blank">01:08:40.880</a></span> | <span class="t">let it accumulate locally for a while and after a while we can we can synchronize also there is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4129" target="_blank">01:08:49.360</a></span> | <span class="t">another very clever trick in pytorch which is the computation communication overlap so since</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4135" target="_blank">01:08:55.200</a></span> | <span class="t">each gpu needs to send its gradient to a central node for accumulation this can lead to an idle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4141" target="_blank">01:09:01.680</a></span> | <span class="t">time in which the cpu gpus are not working because they're communicating right so each gpu will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4146" target="_blank">01:09:06.400</a></span> | <span class="t">perform the forward step then it will perform the backward step and then there is some communication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4151" target="_blank">01:09:11.680</a></span> | <span class="t">overhead because then gpus need to synchronize the gradients with each other calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4160" target="_blank">01:09:20.000</a></span> | <span class="t">cumulative on one node and receive back the cumulative one if we do this operation sequentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4166" target="_blank">01:09:26.080</a></span> | <span class="t">it will lead to a big delay in the gpu because this time they could do some other meaningful work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4171" target="_blank">01:09:31.520</a></span> | <span class="t">for the training while so what pytorch does it will overlap the communication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4178" target="_blank">01:09:38.720</a></span> | <span class="t">while running the backward step let's see how it works so as you remember we have a computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4186" target="_blank">01:09:46.800</a></span> | <span class="t">graph right so pytorch will communicate the gradient of a node as soon as it is available</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4191" target="_blank">01:09:51.920</a></span> | <span class="t">because as you remember we calculate the gradient of the loss function with respect to each weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4196" target="_blank">01:09:56.960</a></span> | <span class="t">but to calculate the gradient of the loss function with respect to each weight we need to calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4200" target="_blank">01:10:00.800</a></span> | <span class="t">the gradient of the loss function with respect to the intermediate nodes so for example first we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4205" target="_blank">01:10:05.440</a></span> | <span class="t">compute the gradient of the loss function with respect to the output then we compute it of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4211" target="_blank">01:10:11.360</a></span> | <span class="t">loss function with respect to the layer the weights of this layer here which are the weights and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4216" target="_blank">01:10:16.480</a></span> | <span class="t">bias but these are already available so we can already send them to the other nodes and get back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4222" target="_blank">01:10:22.640</a></span> | <span class="t">the cumulative one so this can already be done as already this operation then pytorch will calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4228" target="_blank">01:10:28.880</a></span> | <span class="t">the next layer and this one since it's already available we can already send it to the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4233" target="_blank">01:10:33.760</a></span> | <span class="t">nodes and receive back the cumulative one etc etc etc so while pytorch is computing the backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4239" target="_blank">01:10:39.840</a></span> | <span class="t">step we can already communicate to the other nodes the gradient of the that we have already computed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4245" target="_blank">01:10:45.360</a></span> | <span class="t">and receive back the cumulative one this will result in a very fast speed up and to make the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4252" target="_blank">01:10:52.800</a></span> | <span class="t">communication even faster instead of sending one gradient at a time pytorch basically will create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4258" target="_blank">01:10:58.560</a></span> | <span class="t">buckets of gradients so every time a bucket is available it sends it to the other node and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4263" target="_blank">01:11:03.200</a></span> | <span class="t">receives back the cumulative gradient for that bucket then after the second bucket is available</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4268" target="_blank">01:11:08.400</a></span> | <span class="t">it will send the second bucket and receive back the cumulative if we after the last bucket is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4273" target="_blank">01:11:13.120</a></span> | <span class="t">available it will send it to the other nodes and receive back this way we can as you saw before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4280" target="_blank">01:11:20.800</a></span> | <span class="t">we can overlap the communication overhead with the calculation of the backward step so while we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4287" target="_blank">01:11:27.920</a></span> | <span class="t">calculating the gradients we already send them so that the total time to process the forward the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4294" target="_blank">01:11:34.560</a></span> | <span class="t">backward the communication and the updating it's less because one two steps are overlapped with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4299" target="_blank">01:11:39.920</a></span> | <span class="t">each other pytorch recommends 25 megabyte as the size of the bucket because we don't want it to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4306" target="_blank">01:11:46.800</a></span> | <span class="t">too small because the overhead of the communication of each bucket would be too big so we have a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4312" target="_blank">01:11:52.000</a></span> | <span class="t">of overhead for each bucket so we can spread this overhead on bigger buckets so less in number but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4319" target="_blank">01:11:59.360</a></span> | <span class="t">bigger but we also don't want it to be too big because otherwise the we are we are not using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4327" target="_blank">01:12:07.440</a></span> | <span class="t">communication channel while we are computing the gradient so thank you guys for watching my video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4333" target="_blank">01:12:13.600</a></span> | <span class="t">i hope you learned a lot in this video i introduced the data parallel training and i show you how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4338" target="_blank">01:12:18.960</a></span> | <span class="t">create the first the infrastructure and also how to run this code of distributed training on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4344" target="_blank">01:12:24.320</a></span> | <span class="t">infrastructure that we have created i also show you how pytorch works under the hood so that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4350" target="_blank">01:12:30.560</a></span> | <span class="t">can understand how the gradients is synchronized and also at a mathematical level how does it work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4356" target="_blank">01:12:36.400</a></span> | <span class="t">please like and subscribe my video if you liked it and it was hopeful helpful for you i also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4362" target="_blank">01:12:42.240</a></span> | <span class="t">recommend watching my other videos because i every time i make long videos in which i give a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=toUSzwR0EV8&t=4367" target="_blank">01:12:47.200</a></span> | <span class="t">knowledge and let me know if there is something you don't understand i will try to help all of you</span></div></div></body></html>