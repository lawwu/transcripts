<html><head><title>Stanford XCS224U: Natural Language Understanding I In-context Learning, Pt 1: Origins I Spring 2023</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U: Natural Language Understanding I In-context Learning, Pt 1: Origins I Spring 2023</h2><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI"><img src="https://i.ytimg.com/vi/eyNLkiQ89KI/sddefault.jpg?sqp=-oaymwEmCIAFEOAD8quKqQMa8AEB-AHUBoAC4AOKAgwIABABGGUgZShlMA8=&rs=AOn4CLC2H54fbbIiIfvyUGEULFi2y6oIpw" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=82">1:22</a> Early precedents<br><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=165">2:45</a> Beginnings: Radford et al. 2019 (GPT-2)<br><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=309">5:9</a> Cultural moment: Brown et al. 2020 (GPT-3)<br><br><div style="text-align: left;"><a href="./eyNLkiQ89KI.html">Whisper Transcript</a> | <a href="./transcript_eyNLkiQ89KI.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=5" target="_blank">00:00:05.920</a></span> | <span class="t">This is the first screencast in our series on in-context learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=8" target="_blank">00:00:08.960</a></span> | <span class="t">This series is a kind of companion to the one that we did on information retrieval.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=12" target="_blank">00:00:12.840</a></span> | <span class="t">The two series come together to help you with homework two and bake-off two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=16" target="_blank">00:00:16.560</a></span> | <span class="t">which is focused on few-shot open domain question answering with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=20" target="_blank">00:00:20.520</a></span> | <span class="t">frozen retrievers and frozen large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=24" target="_blank">00:00:24.240</a></span> | <span class="t">To start this series,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=25" target="_blank">00:00:25.480</a></span> | <span class="t">I thought we would just reflect a bit on the origins of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=28" target="_blank">00:00:28.360</a></span> | <span class="t">the idea of in-context learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=30" target="_blank">00:00:30.400</a></span> | <span class="t">which is really a story of how NLP got to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=33" target="_blank">00:00:33.480</a></span> | <span class="t">this strange and exciting and chaotic moment for the field,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=38" target="_blank">00:00:38.160</a></span> | <span class="t">and maybe also for the society more broadly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=41" target="_blank">00:00:41.200</a></span> | <span class="t">All credit to the Chomsky bot for bringing us to this moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=45" target="_blank">00:00:45.420</a></span> | <span class="t">I'm only joking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=46" target="_blank">00:00:46.840</a></span> | <span class="t">The Chomsky bot is a very simple pattern-based language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=52" target="_blank">00:00:52.040</a></span> | <span class="t">It's been around since the '90s, I believe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=55" target="_blank">00:00:55.640</a></span> | <span class="t">With very simple mechanisms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=57" target="_blank">00:00:57.560</a></span> | <span class="t">it produces prose that is roughly in the style of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=60" target="_blank">00:01:00.600</a></span> | <span class="t">the political philosopher and sometime linguist Noam Chomsky.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=64" target="_blank">00:01:04.480</a></span> | <span class="t">It produces prose that delights and maybe informs us,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=68" target="_blank">00:01:08.880</a></span> | <span class="t">and the underlying mechanisms are very simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=71" target="_blank">00:01:11.160</a></span> | <span class="t">I think that's a nice reminder about what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=73" target="_blank">00:01:13.920</a></span> | <span class="t">all of these large language models might be doing even in the present day.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=77" target="_blank">00:01:17.880</a></span> | <span class="t">But I'm only joking,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=79" target="_blank">00:01:19.840</a></span> | <span class="t">although it's only partly a joke.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=81" target="_blank">00:01:21.520</a></span> | <span class="t">I think when we think about precedence for in-context learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=84" target="_blank">00:01:24.680</a></span> | <span class="t">it is worth mentioning that in the pre-deep learning era,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=88" target="_blank">00:01:28.560</a></span> | <span class="t">N-gram-based language models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=90" target="_blank">00:01:30.320</a></span> | <span class="t">very sparse large language models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=92" target="_blank">00:01:32.720</a></span> | <span class="t">were often truly massive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=94" target="_blank">00:01:34.560</a></span> | <span class="t">For example, Brant et al.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=95" target="_blank">00:01:35.880</a></span> | <span class="t">2007 use a 300 billion parameter language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=100" target="_blank">00:01:40.840</a></span> | <span class="t">trained on two trillion tokens of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=102" target="_blank">00:01:42.800</a></span> | <span class="t">text to help with machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=105" target="_blank">00:01:45.680</a></span> | <span class="t">That is a very large and very powerful mechanism with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=108" target="_blank">00:01:48.780</a></span> | <span class="t">a different character from the large language models of today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=112" target="_blank">00:01:52.120</a></span> | <span class="t">But it is nonetheless worth noting that they played</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=114" target="_blank">00:01:54.680</a></span> | <span class="t">an important role in a lot of different fields way back when.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=118" target="_blank">00:01:58.920</a></span> | <span class="t">I think for in-context learning as we know it now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=123" target="_blank">00:02:03.160</a></span> | <span class="t">the earliest paper as far as I know is the DECA NLP paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=127" target="_blank">00:02:07.720</a></span> | <span class="t">This is McCann et al. 2018.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=129" target="_blank">00:02:09.680</a></span> | <span class="t">They do multitask training with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=131" target="_blank">00:02:11.880</a></span> | <span class="t">task instructions that are natural language questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=135" target="_blank">00:02:15.360</a></span> | <span class="t">That does seem like the origin of the idea that with free-form natural language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=139" target="_blank">00:02:19.640</a></span> | <span class="t">instructions we could essentially end up with artifacts that could do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=143" target="_blank">00:02:23.240</a></span> | <span class="t">multiple things guided solely by text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=147" target="_blank">00:02:27.920</a></span> | <span class="t">Then it's worth noting also that in the GPT paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=151" target="_blank">00:02:31.600</a></span> | <span class="t">Radford et al. 2018,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=153" target="_blank">00:02:33.420</a></span> | <span class="t">you can find buried in there some tentative proposals to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=156" target="_blank">00:02:36.640</a></span> | <span class="t">prompt-based experiments with that model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=160" target="_blank">00:02:40.860</a></span> | <span class="t">But the real origins of the ideas, again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=164" target="_blank">00:02:44.420</a></span> | <span class="t">as far as I know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=165" target="_blank">00:02:45.680</a></span> | <span class="t">are Radford et al. 2019.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=167" target="_blank">00:02:47.920</a></span> | <span class="t">This is the GPT-2 paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=170" target="_blank">00:02:50.520</a></span> | <span class="t">Let me just show you some snippets from this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=173" target="_blank">00:02:53.080</a></span> | <span class="t">It's really inspiring how much they did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=175" target="_blank">00:02:55.280</a></span> | <span class="t">They say at the start,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=176" target="_blank">00:02:56.440</a></span> | <span class="t">"We demonstrate language models can perform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=178" target="_blank">00:02:58.520</a></span> | <span class="t">downstream tasks in a zero-shot setting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=181" target="_blank">00:03:01.160</a></span> | <span class="t">without any parameter or architecture modification."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=184" target="_blank">00:03:04.120</a></span> | <span class="t">There you see this idea of using frozen models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=187" target="_blank">00:03:07.440</a></span> | <span class="t">prompting them, and seeing if they will produce interesting behaviors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=191" target="_blank">00:03:11.480</a></span> | <span class="t">They looked at a bunch of different tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=194" target="_blank">00:03:14.160</a></span> | <span class="t">For summarization, they say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=195" target="_blank">00:03:15.640</a></span> | <span class="t">"To induce summarization behavior,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=197" target="_blank">00:03:17.480</a></span> | <span class="t">we add the text TLDR after the article and generate 100 tokens."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=202" target="_blank">00:03:22.080</a></span> | <span class="t">This is mind-blowing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=203" target="_blank">00:03:23.280</a></span> | <span class="t">I remember when I first heard about this idea,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=206" target="_blank">00:03:26.640</a></span> | <span class="t">I had such a cognitive bias against in-context learning of this sort being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=210" target="_blank">00:03:30.920</a></span> | <span class="t">successful that I assumed what they were trying to say to us is that they had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=215" target="_blank">00:03:35.280</a></span> | <span class="t">trained that token in a task-specific way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=219" target="_blank">00:03:39.560</a></span> | <span class="t">do summarization and then just given it a colorful name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=223" target="_blank">00:03:43.120</a></span> | <span class="t">But no, they really meant it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=225" target="_blank">00:03:45.040</a></span> | <span class="t">They simply prompt the model with this token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=227" target="_blank">00:03:47.360</a></span> | <span class="t">and look at what comes out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=229" target="_blank">00:03:49.680</a></span> | <span class="t">For translation, they say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=231" target="_blank">00:03:51.920</a></span> | <span class="t">"We test whether GPT-2 has begun to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=234" target="_blank">00:03:54.040</a></span> | <span class="t">learn how to translate from one language to another.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=236" target="_blank">00:03:56.800</a></span> | <span class="t">In order to help it infer that this is the desired task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=239" target="_blank">00:03:59.720</a></span> | <span class="t">we condition the language model on a context of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=242" target="_blank">00:04:02.040</a></span> | <span class="t">example pairs of the format English sentence equals French sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=246" target="_blank">00:04:06.200</a></span> | <span class="t">Then after a final prompt of English sentence equals,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=249" target="_blank">00:04:09.240</a></span> | <span class="t">we sample from the model with greedy decoding and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=251" target="_blank">00:04:11.680</a></span> | <span class="t">use the first generated sentence as the translation."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=254" target="_blank">00:04:14.560</a></span> | <span class="t">Incredible, and what you see emerging there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=257" target="_blank">00:04:17.080</a></span> | <span class="t">is this idea of demonstrations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=259" target="_blank">00:04:19.120</a></span> | <span class="t">including in the prompt some examples of the behavior that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=261" target="_blank">00:04:21.960</a></span> | <span class="t">want as a way of coaxing the model to do what you would like it to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=266" target="_blank">00:04:26.000</a></span> | <span class="t">Here's a similar example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=267" target="_blank">00:04:27.680</a></span> | <span class="t">They say, "Similar to translation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=269" target="_blank">00:04:29.360</a></span> | <span class="t">the context of the language model is seeded with example question-answer pairs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=273" target="_blank">00:04:33.480</a></span> | <span class="t">which helps the model infer the short answer style of the dataset."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=277" target="_blank">00:04:37.240</a></span> | <span class="t">That's for QA, and again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=278" target="_blank">00:04:38.800</a></span> | <span class="t">they started to see that demonstrations could help</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=281" target="_blank">00:04:41.400</a></span> | <span class="t">the model see what the implicit task instruction was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=285" target="_blank">00:04:45.360</a></span> | <span class="t">They also in the paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=287" target="_blank">00:04:47.360</a></span> | <span class="t">evaluate a bunch of other things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=288" target="_blank">00:04:48.880</a></span> | <span class="t">text completion, Winograd schemas,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=291" target="_blank">00:04:51.440</a></span> | <span class="t">and reading comprehension, and maybe others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=293" target="_blank">00:04:53.800</a></span> | <span class="t">It's a very impressive and thorough exploration,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=295" target="_blank">00:04:55.840</a></span> | <span class="t">very open about the benefits and limitations of the methods,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=299" target="_blank">00:04:59.640</a></span> | <span class="t">a very impressive and creative paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=303" target="_blank">00:05:03.120</a></span> | <span class="t">That was the beginning of the idea in terms of research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=308" target="_blank">00:05:08.880</a></span> | <span class="t">The cultural moment certainly arrives with the GPT-3 paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=313" target="_blank">00:05:13.480</a></span> | <span class="t">Brown et al. 2020,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=315" target="_blank">00:05:15.280</a></span> | <span class="t">which is also impressive in its own ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=317" target="_blank">00:05:17.560</a></span> | <span class="t">Here I'm just going to quote from the abstract and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=319" target="_blank">00:05:19.560</a></span> | <span class="t">can linger a bit over what it says.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=321" target="_blank">00:05:21.920</a></span> | <span class="t">They start, "We show that scaling up language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=324" target="_blank">00:05:24.960</a></span> | <span class="t">greatly improves task agnostic few-shot performance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=328" target="_blank">00:05:28.320</a></span> | <span class="t">sometimes even reaching competitiveness with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=330" target="_blank">00:05:30.920</a></span> | <span class="t">prior state-of-the-art fine-tuning approaches."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=333" target="_blank">00:05:33.840</a></span> | <span class="t">We could quibble with whether or not they actually saw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=336" target="_blank">00:05:36.440</a></span> | <span class="t">competitiveness in that sense,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=338" target="_blank">00:05:38.160</a></span> | <span class="t">but it is absolutely true that they got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=340" target="_blank">00:05:40.720</a></span> | <span class="t">very impressive behaviors out of their model in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=343" target="_blank">00:05:43.160</a></span> | <span class="t">this task agnostic few-shot setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=346" target="_blank">00:05:46.280</a></span> | <span class="t">Specifically, we train GPT-3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=349" target="_blank">00:05:49.440</a></span> | <span class="t">an autoregressive language model with 175 billion parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=353" target="_blank">00:05:53.480</a></span> | <span class="t">10x more than any previous non-sparse language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=357" target="_blank">00:05:57.000</a></span> | <span class="t">and test its performance in the few-shot setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=359" target="_blank">00:05:59.520</a></span> | <span class="t">There are two things I really like about this part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=361" target="_blank">00:06:01.720</a></span> | <span class="t">First, 175 billion parameters is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=364" target="_blank">00:06:04.920</a></span> | <span class="t">indeed incredibly ambitious and impressive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=367" target="_blank">00:06:07.520</a></span> | <span class="t">even today to say nothing of back in 2020.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=370" target="_blank">00:06:10.840</a></span> | <span class="t">I also really love that they mentioned non-sparse language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=375" target="_blank">00:06:15.160</a></span> | <span class="t">a nod to those N-gram based models that I mentioned before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=378" target="_blank">00:06:18.480</a></span> | <span class="t">which were often truly massive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=381" target="_blank">00:06:21.200</a></span> | <span class="t">For all tasks, GPT-3 is applied without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=384" target="_blank">00:06:24.520</a></span> | <span class="t">any gradient updates or fine-tuning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=386" target="_blank">00:06:26.740</a></span> | <span class="t">with tasks and few-shot demonstrations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=388" target="_blank">00:06:28.960</a></span> | <span class="t">specified purely via text interaction with the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=392" target="_blank">00:06:32.120</a></span> | <span class="t">That's nice. You might think in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=394" target="_blank">00:06:34.560</a></span> | <span class="t">retrospect that they're repeating themselves here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=397" target="_blank">00:06:37.440</a></span> | <span class="t">They've already established that these are going to be frozen models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=400" target="_blank">00:06:40.000</a></span> | <span class="t">but I think it's necessary for them to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=402" target="_blank">00:06:42.320</a></span> | <span class="t">that because this was such an unfamiliar idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=404" target="_blank">00:06:44.640</a></span> | <span class="t">I can imagine, again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=406" target="_blank">00:06:46.240</a></span> | <span class="t">being a reader of this paper and assuming that they can't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=409" target="_blank">00:06:49.160</a></span> | <span class="t">really mean they're just using frozen models for all these tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=412" target="_blank">00:06:52.080</a></span> | <span class="t">Surely, there is some fine-tuning somewhere,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=414" target="_blank">00:06:54.040</a></span> | <span class="t">and so they're emphasizing that in fact,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=416" target="_blank">00:06:56.320</a></span> | <span class="t">the model is entirely frozen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=419" target="_blank">00:06:59.000</a></span> | <span class="t">GPT-3 achieves strong performance on many NLP datasets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=423" target="_blank">00:07:03.200</a></span> | <span class="t">including translation, question answering,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=425" target="_blank">00:07:05.440</a></span> | <span class="t">and closed tasks, as well as several tasks that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=428" target="_blank">00:07:08.320</a></span> | <span class="t">require on-the-fly reasoning or domain adaptations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=431" target="_blank">00:07:11.280</a></span> | <span class="t">such as unscrambling words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=433" target="_blank">00:07:13.000</a></span> | <span class="t">using a novel word in a sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=434" target="_blank">00:07:14.680</a></span> | <span class="t">or performing three-digit arithmetic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=436" target="_blank">00:07:16.720</a></span> | <span class="t">I love this. A real diversity of tasks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=439" target="_blank">00:07:19.160</a></span> | <span class="t">and what I think you can see them doing is really trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=442" target="_blank">00:07:22.080</a></span> | <span class="t">push the limits of what would be possible in this mode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=445" target="_blank">00:07:25.840</a></span> | <span class="t">At the same time, we also identify some datasets where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=449" target="_blank">00:07:29.040</a></span> | <span class="t">GPT-3's few-shot learning still struggles,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=452" target="_blank">00:07:32.000</a></span> | <span class="t">as well as some datasets where GPT-3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=454" target="_blank">00:07:34.040</a></span> | <span class="t">faces methodological issues related to training on large web corpora.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=458" target="_blank">00:07:38.360</a></span> | <span class="t">I also love this sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=459" target="_blank">00:07:39.760</a></span> | <span class="t">It's again, very open about what they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=461" target="_blank">00:07:41.560</a></span> | <span class="t">achieved and where the limitations are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=463" target="_blank">00:07:43.560</a></span> | <span class="t">They're acknowledging that they found</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=465" target="_blank">00:07:45.080</a></span> | <span class="t">some tasks that are still hard for the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=467" target="_blank">00:07:47.240</a></span> | <span class="t">and they also acknowledge in the paper that they had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=469" target="_blank">00:07:49.680</a></span> | <span class="t">some minor slip-ups where they intended to make sure they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=473" target="_blank">00:07:53.280</a></span> | <span class="t">hadn't trained on data that was relevant for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=475" target="_blank">00:07:55.800</a></span> | <span class="t">the test task that they were performing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=478" target="_blank">00:07:58.000</a></span> | <span class="t">and in fact, they had not quite gotten that right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=480" target="_blank">00:08:00.760</a></span> | <span class="t">They're being very open about that and exploring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=484" target="_blank">00:08:04.000</a></span> | <span class="t">how hard it is to get that right at the scale that they're operating at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=487" target="_blank">00:08:07.880</a></span> | <span class="t">Just like the GPT-2 paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=490" target="_blank">00:08:10.280</a></span> | <span class="t">a wonderfully open and thorough exploration of the ideas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eyNLkiQ89KI&t=496" target="_blank">00:08:16.080</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>