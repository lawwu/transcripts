<html><head><title>How to Build Trustworthy AI — Allie Howe</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>How to Build Trustworthy AI — Allie Howe</h2><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I"><img src="https://i.ytimg.com/vi/n6wHJDqlS1I/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=28">0:28</a> Who Needs Trustworthy AI<br><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=121">2:1</a> Who is Responsible<br><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=239">3:59</a> Whats the Difference<br><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=564">9:24</a> AI Red teaming<br><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=686">11:26</a> AI Runtime Security<br><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=807">13:27</a> Fortnite Example<br><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=968">16:8</a> Pillar Example<br><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1348">22:28</a> Unlocking Revolutionary Innovation<br><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1399">23:19</a> The Bottom Line<br><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1446">24:6</a> Outro<br><br><div style="text-align: left;"><a href="./n6wHJDqlS1I.html">Whisper Transcript</a> | <a href="./transcript_n6wHJDqlS1I.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1" target="_blank">00:00:01.000</a></span> | <span class="t">Hi, my name is Allie Howe. I am a VCISO for Growth Cyber. We are a business that helps other companies build trustworthy AI. We sit at the intersection of AI security and compliance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=14" target="_blank">00:00:14.620</a></span> | <span class="t">Today we're going to be talking about a variety of different topics, namely what is trustworthy AI, what goes into building trustworthy AI, and why you should care about trustworthy AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=28" target="_blank">00:00:28.720</a></span> | <span class="t">So to start off, who even needs trustworthy AI? Why do we care about it? Well, it's been in the news a lot, whether you realize it or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=36" target="_blank">00:00:36.740</a></span> | <span class="t">All the way back in 2023, we all saw that case with the Chevy Tahoe incident where a user was able to be offered a Chevy Tahoe from a chatbot for a dollar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=49" target="_blank">00:00:49.420</a></span> | <span class="t">So that chatbot did not operate as it was intended to by the company, and it was in a position to be taken advantage of by a user.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=57" target="_blank">00:00:57.440</a></span> | <span class="t">Another instance, in 2024, Slack was able to be tricked into leaking data from private channels via a prompt injection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=65" target="_blank">00:01:05.740</a></span> | <span class="t">So again, that system did not operate as intended and had some pretty strict consequences as well for probably Slack AI, and the companies, whoever, whatever company was, had that data leakage happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=76" target="_blank">00:01:16.900</a></span> | <span class="t">Very recently, a couple weeks ago, we saw Darth Vader, an NPC being released into Fortnite by Epic Labs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=85" target="_blank">00:01:25.340</a></span> | <span class="t">That was really interesting. I think this was the first case of a voice agent being used in a video game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=90" target="_blank">00:01:30.200</a></span> | <span class="t">So users were able to interact within AI Vader and ask it all sorts of crazy questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=94" target="_blank">00:01:34.960</a></span> | <span class="t">You know, at first, Vader exhibited a lot of bad behaviors, sort of reminiscent of the Microsoft Taye chatbot back from 2006.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=102" target="_blank">00:01:42.540</a></span> | <span class="t">It was saying things that were racist, homophobic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=104" target="_blank">00:01:44.860</a></span> | <span class="t">It since has improved dramatically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=108" target="_blank">00:01:48.080</a></span> | <span class="t">That's not what I'm diving into specifically today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=110" target="_blank">00:01:50.600</a></span> | <span class="t">But as you can see, there's many cases in the news where AI was not necessarily trustworthy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=115" target="_blank">00:01:55.220</a></span> | <span class="t">So it's something that is happening quite often and something that needs your attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=123" target="_blank">00:02:03.620</a></span> | <span class="t">There's a lot of debate around who is responsible for trustworthy AI, and it really boils down to you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=130" target="_blank">00:02:10.100</a></span> | <span class="t">You are the one responsible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=131" target="_blank">00:02:11.300</a></span> | <span class="t">There was a lawsuit the other day on May 20th, so very recently, where a radio host was suing OpenAI over false statements that were generated by chat GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=143" target="_blank">00:02:23.440</a></span> | <span class="t">However, chat GPT was able to get the case dismissed, or they won the case, or OpenAI did not end up in trouble, simply because it states that chat GPT can make wrong outputs from time to time, and it's up to the user to understand that and to proceed with caution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=160" target="_blank">00:02:40.540</a></span> | <span class="t">So if you're using AI, it's likely you that will be responsible, both on paper and also just from a brand and reputational standpoint, you are the one that needs to be aware that you are likely responsible and will be taking the fall for anything your AI application does that's incorrect, wrong, or inappropriate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=180" target="_blank">00:03:00.960</a></span> | <span class="t">So when we talk about building trustworthy AI, that's something that both product engineering and security teams are focused on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=188" target="_blank">00:03:08.900</a></span> | <span class="t">Product teams probably care that your AI application is outputting the right topics, it's relevant, it's generally helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=195" target="_blank">00:03:15.360</a></span> | <span class="t">Security teams are probably thinking about, you know, your AI application's not going to be saying anything that's going to be inappropriate or off-topic as well, but they're also looking out for things like prompt injections and jailbreaks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=206" target="_blank">00:03:26.780</a></span> | <span class="t">Engineering is helping out with these as well, probably cross-functioning is also thinking about things like cost and latency as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=212" target="_blank">00:03:32.740</a></span> | <span class="t">So it's a really big team that comes together to be able to put together trustworthy AI specifically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=218" target="_blank">00:03:38.720</a></span> | <span class="t">And the recipe for trustworthy AI is AI security and AI safety.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=224" target="_blank">00:03:44.200</a></span> | <span class="t">So what's the difference?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=225" target="_blank">00:03:45.780</a></span> | <span class="t">AI security is how does the outside world harm my AI application?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=231" target="_blank">00:03:51.500</a></span> | <span class="t">AI safety is how does my AI application harm the world?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=235" target="_blank">00:03:55.480</a></span> | <span class="t">We'll talk into, we'll go into detail about both of those shortly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=239" target="_blank">00:03:59.100</a></span> | <span class="t">So there's this new paradigm out there that AI engineering has introduced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=245" target="_blank">00:04:05.740</a></span> | <span class="t">Traditionally, we had DevSecOps where our scanning tools, security tools like our SaaS tools were integrated within our CI CD pipelines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=255" target="_blank">00:04:15.020</a></span> | <span class="t">They were able to capture most of the vulnerabilities, such as software dependency, supply chain issues, and insecure code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=260" target="_blank">00:04:20.460</a></span> | <span class="t">But now, thanks to AI engineering and, you know, data scientists and machine learning engineers, they don't work in our traditional CI CD platforms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=268" target="_blank">00:04:28.400</a></span> | <span class="t">They work in things like Databricks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=270" target="_blank">00:04:30.000</a></span> | <span class="t">They work in things like Jupyter Notebooks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=271" target="_blank">00:04:31.800</a></span> | <span class="t">So we need a new model for what AI engineering DevSecOps looks like and how we're actually going to build trustworthy AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=280" target="_blank">00:04:40.680</a></span> | <span class="t">Traditionally, there's been a big focus on shifting, but thanks to prompt injections changing rapidly, AI models being deployed and switched out rapidly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=291" target="_blank">00:04:51.040</a></span> | <span class="t">there's now a big focus on shifting right for AI security, which is why runtime security is particularly important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=298" target="_blank">00:04:58.440</a></span> | <span class="t">I mean, all three of these boxes here, build, test, and run are, build, test, and deploy are,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=303" target="_blank">00:05:03.420</a></span> | <span class="t">but with AI security specifically, especially because it's so non-deterministic,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=307" target="_blank">00:05:07.440</a></span> | <span class="t">having something on the rightmost side at the time of runtime is incredibly important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=313" target="_blank">00:05:13.020</a></span> | <span class="t">You know, no longer are we just caring about shift left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=315" target="_blank">00:05:15.860</a></span> | <span class="t">We're really worried about this entire lifecycle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=318" target="_blank">00:05:18.120</a></span> | <span class="t">So on the leftmost side, we've got our build.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=320" target="_blank">00:05:20.660</a></span> | <span class="t">That's where we're going to be doing some sort of like model scanning, thinking about model providence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=324" target="_blank">00:05:24.420</a></span> | <span class="t">looking at AI or ML bombs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=327" target="_blank">00:05:27.100</a></span> | <span class="t">So that's machine learning security operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=329" target="_blank">00:05:29.180</a></span> | <span class="t">It's kind of a play on the term DevSecOps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=331" target="_blank">00:05:31.020</a></span> | <span class="t">That's important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=331" target="_blank">00:05:31.800</a></span> | <span class="t">And then in the middle, we've got AI Red Team.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=334" target="_blank">00:05:34.220</a></span> | <span class="t">We're going to test our AI applications for both AI security and AI safety concerns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=340" target="_blank">00:05:40.460</a></span> | <span class="t">And then on the rightmost side, we've got AI runtime security,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=343" target="_blank">00:05:43.520</a></span> | <span class="t">where we can validate AI inputs and outputs as they come into our AI system and our models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=351" target="_blank">00:05:51.840</a></span> | <span class="t">So DevSecOps is out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=353" target="_blank">00:05:53.800</a></span> | <span class="t">MLSecOps is in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=356" target="_blank">00:05:56.280</a></span> | <span class="t">So basically, MLSecOps is machine learning security operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=360" target="_blank">00:06:00.600</a></span> | <span class="t">As I mentioned before, MLSecOps is able to look into and take into consideration places that traditional DevSecOps does not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=369" target="_blank">00:06:09.220</a></span> | <span class="t">As I mentioned, AI engineers live in Databricks and Jupyter Notebooks, not in traditional CICT pipelines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=374" target="_blank">00:06:14.720</a></span> | <span class="t">So it's important to focus there as well and look for exposed secrets that could be in those Databricks or Jupyter Notebooks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=382" target="_blank">00:06:22.580</a></span> | <span class="t">And also part of MLSecOps is understanding things like model providence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=387" target="_blank">00:06:27.740</a></span> | <span class="t">So, you know, who built this model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=390" target="_blank">00:06:30.600</a></span> | <span class="t">Where did it come from?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=391" target="_blank">00:06:31.460</a></span> | <span class="t">What data was it trained on?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=392" target="_blank">00:06:32.980</a></span> | <span class="t">Those are helpful things to think about for compliance as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=395" target="_blank">00:06:35.760</a></span> | <span class="t">If you have, you know, access to that data the model was trained on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=399" target="_blank">00:06:39.840</a></span> | <span class="t">if you're supposed to be using it or safeguarding it, understanding maybe, you know, maybe a nation state made that model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=406" target="_blank">00:06:46.760</a></span> | <span class="t">What are the implications of that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=408" target="_blank">00:06:48.340</a></span> | <span class="t">Is that something you need to be worried about?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=409" target="_blank">00:06:49.880</a></span> | <span class="t">One of the biggest risks from models that you may be using, especially like open source ones you can get your hands on, are model serialization attacks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=418" target="_blank">00:06:58.640</a></span> | <span class="t">A model serialization attack is when models are, code is saved into the model at runtime, at, sorry, at time of serialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=426" target="_blank">00:07:06.360</a></span> | <span class="t">And then when you go to de-serialize the model, that code will automatically be run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=430" target="_blank">00:07:10.280</a></span> | <span class="t">So you're now talking about arbitrary code execution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=433" target="_blank">00:07:13.500</a></span> | <span class="t">And we see this with the pickle serialization format.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=436" target="_blank">00:07:16.360</a></span> | <span class="t">That's one of the most well-known ones for serializing models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=440" target="_blank">00:07:20.420</a></span> | <span class="t">However, if you look at the pickles documentation, they do say that the module is not secure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=447" target="_blank">00:07:27.600</a></span> | <span class="t">And you're not supposed to unpickle data that you do not inherently trust.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=452" target="_blank">00:07:32.320</a></span> | <span class="t">So you're getting these models, say, from a model repository or a model zoo on the web, and you're just downloading it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=458" target="_blank">00:07:38.300</a></span> | <span class="t">You need to scan those to see if they have any unsafe operators in them, if you're at risk to model serialization attacks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=464" target="_blank">00:07:44.820</a></span> | <span class="t">A lot of serialization attacks run as soon as the model is de-serialized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=467" target="_blank">00:07:47.920</a></span> | <span class="t">So that arbitrary code could be causing data loss, credential loss, or model poisoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=471" target="_blank">00:07:51.820</a></span> | <span class="t">So it's really easy to scan models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=474" target="_blank">00:07:54.280</a></span> | <span class="t">So I really encourage you to do this and be practicing this within your organization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=479" target="_blank">00:07:59.160</a></span> | <span class="t">One example is ModelScan from Protect AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=482" target="_blank">00:08:02.480</a></span> | <span class="t">And Protect AI also created this MLS SecOps community where you can learn more about MLS SecOps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=487" target="_blank">00:08:07.780</a></span> | <span class="t">I've personally learned a lot from it, so I really encourage you to check it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=491" target="_blank">00:08:11.060</a></span> | <span class="t">But I can show you ModelScan really quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=493" target="_blank">00:08:13.120</a></span> | <span class="t">This is the repo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=495" target="_blank">00:08:15.100</a></span> | <span class="t">It's open source, free to use, really easy to download.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=497" target="_blank">00:08:17.960</a></span> | <span class="t">Just pip install it and then run it like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=500" target="_blank">00:08:20.180</a></span> | <span class="t">They also have an example of a model serialization attack within this code base that you can go ahead and check out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=506" target="_blank">00:08:26.040</a></span> | <span class="t">So as you can see here, when the model gets saved, we are adding our unsafe payload in there, which is this command to basically output the AWS Secrets that we have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=516" target="_blank">00:08:36.380</a></span> | <span class="t">And so when this model gets run or loaded, we can now see that this OS Keys has been outputted, which, you know, then we have a credential leak.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=524" target="_blank">00:08:44.080</a></span> | <span class="t">That's not great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=524" target="_blank">00:08:44.800</a></span> | <span class="t">And then if you had used ModelScan to scan your model before, you know, this had happened, you would have seen that there is a critical vulnerability, unsafe operator in use here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=534" target="_blank">00:08:54.320</a></span> | <span class="t">So that's one example of, you know, a potential solution for scanning your models that's open source free that you can use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=540" target="_blank">00:09:00.120</a></span> | <span class="t">There's other ones out there as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=543" target="_blank">00:09:03.200</a></span> | <span class="t">They also have a partnership with HuggingFace.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=545" target="_blank">00:09:05.940</a></span> | <span class="t">So ModelScan is used within HuggingFace to do scans of files and model data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=552" target="_blank">00:09:12.420</a></span> | <span class="t">So if you ever see something that's like unsafe here, you can go learn about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=556" target="_blank">00:09:16.300</a></span> | <span class="t">So keep a lookout for that as well as you're pulling models off of ModelZoes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=560" target="_blank">00:09:20.060</a></span> | <span class="t">So now that we've talked about the left most side, which is MLSecOps, let's now talk about the middle, which is AI red teaming.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=572" target="_blank">00:09:32.620</a></span> | <span class="t">And AI red teaming, we can use that to simulate both adversarial threats and also AI safety concerns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=580" target="_blank">00:09:40.020</a></span> | <span class="t">So during AI red teaming, we can test for things like prompt injections, jailbreaks, but we can also test for AI safety concerns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=587" target="_blank">00:09:47.800</a></span> | <span class="t">Like, so if a user asks, you know, how can I build a bomb?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=590" target="_blank">00:09:50.240</a></span> | <span class="t">How could I create chemical weapons?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=592" target="_blank">00:09:52.060</a></span> | <span class="t">Those are things that your model should not, you know, be answering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=594" target="_blank">00:09:54.960</a></span> | <span class="t">It should be safeguarding against.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=596" target="_blank">00:09:56.200</a></span> | <span class="t">It also should be, you know, it should be biased.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=598" target="_blank">00:09:58.900</a></span> | <span class="t">It shouldn't do anything that's like homophobic or racist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=602" target="_blank">00:10:02.040</a></span> | <span class="t">Like we saw in the Vader example from the very beginning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=604" target="_blank">00:10:04.420</a></span> | <span class="t">These are things we can both test during AI red teaming.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=607" target="_blank">00:10:07.260</a></span> | <span class="t">And we should be continuously testing our models because, you know, as we know, models change as users interact with them, not just with code deploys like traditional software has.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=616" target="_blank">00:10:16.320</a></span> | <span class="t">Another benefit of AI red teaming is you can use them to influence runtime guardrails.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=621" target="_blank">00:10:21.700</a></span> | <span class="t">So if we see that, you know, our model is particularly vulnerable to this type of prompt injection or it's continuously saying like this sort of racist thing, well, we can block certain topics and prompts that would elicit those responses to make sure that our model behaves as expected during runtime.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=638" target="_blank">00:10:38.660</a></span> | <span class="t">Another benefit of AI red teaming is the ability to compare LLMs to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=643" target="_blank">00:10:43.580</a></span> | <span class="t">In some cases, models can have back doors built into them where you're not going to see this during sort of just for testing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=650" target="_blank">00:10:50.940</a></span> | <span class="t">But if you start to compare LLMs to each other, you might start to see differences that might suggest that one model has a back door built into it, whereas another does not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=660" target="_blank">00:11:00.140</a></span> | <span class="t">So if you ask every single model the same question, but one had a really different response, you might be like, OK, maybe this model like one either doesn't work appropriately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=668" target="_blank">00:11:08.060</a></span> | <span class="t">Maybe this was by design.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=669" target="_blank">00:11:09.560</a></span> | <span class="t">There's some sort of backdoor deceptive behavior that was built into this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=673" target="_blank">00:11:13.000</a></span> | <span class="t">So I'm going to use a different model instead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=675" target="_blank">00:11:15.000</a></span> | <span class="t">So there's a lot of reasons to do AI red teaming that can be particularly helpful, both with model selection and helping you influence what runtime guardrails to put in place during runtime.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=687" target="_blank">00:11:27.700</a></span> | <span class="t">So in terms of runtime security, if there was one area to invest in, I would pick runtime security because AI red teaming can be particularly time consuming and also expensive to if you're going to like retrain models based on the results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=701" target="_blank">00:11:41.840</a></span> | <span class="t">It might be particularly difficult or a lot of overhead for your organization if you're starting, you know, an AI security practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=707" target="_blank">00:11:47.560</a></span> | <span class="t">AI runtime security is easy to implement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=710" target="_blank">00:11:50.580</a></span> | <span class="t">Typically, it's done with just including an API or installing another like Python module.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=717" target="_blank">00:11:57.100</a></span> | <span class="t">It's pretty easy to get deployed with a lot of benefits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=719" target="_blank">00:11:59.560</a></span> | <span class="t">So.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=720" target="_blank">00:12:00.340</a></span> | <span class="t">We should really focus on AI security, runtime security, because it's important to shift right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=726" target="_blank">00:12:06.940</a></span> | <span class="t">That's where we're going to see things like the prompt injections being thrown at our models and different types of prompt attacks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=732" target="_blank">00:12:12.940</a></span> | <span class="t">These can be indirect prompt injections where, say, you have a system that is using RAG or calling out and scraping a website.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=740" target="_blank">00:12:20.140</a></span> | <span class="t">Web data can have hidden prompt injections in them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=743" target="_blank">00:12:23.080</a></span> | <span class="t">The same can documents that you've used with your RAG set up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=745" target="_blank">00:12:25.960</a></span> | <span class="t">So you could get something that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=747" target="_blank">00:12:27.220</a></span> | <span class="t">You could also have a direct and prompt injection straight from the user to the chatbot or to the AI agent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=751" target="_blank">00:12:31.900</a></span> | <span class="t">You can also see jailbreaks as well, which in text, those look like things that are semantically strange, pretty chaotic looking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=759" target="_blank">00:12:39.260</a></span> | <span class="t">This is an example here on the slide of a jailbreak from LaCara AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=762" target="_blank">00:12:42.760</a></span> | <span class="t">And also at runtime, your application can see off topic or unsafe prompts, such as ones that might, you know, get your application to output something that is unsafe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=772" target="_blank">00:12:52.800</a></span> | <span class="t">So something that's like, you know, instructions for how to build a bomb or something that's, you know, inappropriate or racist or, you know, something that's bad to say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=780" target="_blank">00:13:00.940</a></span> | <span class="t">You can check that at runtime, both on the input side, looking at the prompts and then either deflecting it and not allowing your model to answer it in the first place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=789" target="_blank">00:13:09.360</a></span> | <span class="t">Or if your model answers it and something that's incorrect or inappropriate comes back, you can go ahead and block that from being ever sent to the user.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=797" target="_blank">00:13:17.860</a></span> | <span class="t">So it's a super nice solution to have in place to make sure that your AI you've deployed is behaving in a trustworthy manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=808" target="_blank">00:13:28.120</a></span> | <span class="t">So let's take a look at that beta solution from our setup from the beginning, where we've got that beta NPC running in Fortnite.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=815" target="_blank">00:13:35.520</a></span> | <span class="t">It was a really interesting setup that I tried to imagine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=818" target="_blank">00:13:38.960</a></span> | <span class="t">So this is just an estimated guess of what their architecture looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=821" target="_blank">00:13:41.920</a></span> | <span class="t">So basically, the users that are using Fortnite are sending proximity audio, user audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=828" target="_blank">00:13:48.240</a></span> | <span class="t">There's like other contacts being sent to the servers likely about like player skins or other player configurations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=835" target="_blank">00:13:55.180</a></span> | <span class="t">And also, of course, the user's audio feed, where we see the, where they're talking to beta go into probably Fortnite servers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=840" target="_blank">00:14:00.940</a></span> | <span class="t">And then it's being passed to 11 labs for voice to text transformation, eventually being passed to Gemini to craft beta's response.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=849" target="_blank">00:14:09.380</a></span> | <span class="t">And then we're going to see beta's response being sent back in text to 11 labs, then sent back as audio to Fortnite servers, and eventually all the way back to the user.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=858" target="_blank">00:14:18.300</a></span> | <span class="t">So there's a lot going on here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=860" target="_blank">00:14:20.820</a></span> | <span class="t">And if I was going to be inserting AI runtime security, I'd probably insert it as close to the model as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=866" target="_blank">00:14:26.840</a></span> | <span class="t">So if we get something that's like unsafe, if the user said something that was maybe against user terms or conditions, or, you know, the user said something inappropriate, maybe we don't want to like allow Vader to answer that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=876" target="_blank">00:14:36.940</a></span> | <span class="t">So we can validate our inputs here at this, at this location.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=880" target="_blank">00:14:40.100</a></span> | <span class="t">So let's say we answer Vader's question or topic, and it did come back as something that was off topic or inappropriate, we could flag it there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=888" target="_blank">00:14:48.020</a></span> | <span class="t">So what I'm talking about right now is AI safety concerns, where the user has said something inappropriate, we don't want Vader to behave in a way that's inappropriate either.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=896" target="_blank">00:14:56.280</a></span> | <span class="t">So we can check for that, but of course, we can also check for, you know, prompt ejections, jailbreaks, other, you know, AI security concerns as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=903" target="_blank">00:15:03.660</a></span> | <span class="t">So AI runtime is both for AI safety and also for AI security.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=907" target="_blank">00:15:07.480</a></span> | <span class="t">But of course, also, you know, include AI runtime security here as well before we even get to 11 labs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=914" target="_blank">00:15:14.500</a></span> | <span class="t">So if the Fortnite servers come back and say, hey, like, you know, this looks like something that's inappropriate, like, I don't even want to do a call to 11 labs, there's another API call, I'm sure it's expensive as well, because they're using other services.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=924" target="_blank">00:15:24.880</a></span> | <span class="t">So you could add it here as well, same here, checking it again after it comes out of 11 labs, checking the audio, making sure that that still looks good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=933" target="_blank">00:15:33.020</a></span> | <span class="t">However, there's tradeoffs at play here, we have to think about cost, latency and accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=938" target="_blank">00:15:38.480</a></span> | <span class="t">That's what a good AI runtime security solution has, it has all of those things, low latency, low cost, or, you know, acceptable cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=945" target="_blank">00:15:45.600</a></span> | <span class="t">It's also like highly accurate as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=947" target="_blank">00:15:47.380</a></span> | <span class="t">A lot of AI runtime security solutions are backed by really well put together and established AI research teams, which are constantly finding new prompt ejections, new jailbreaks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=957" target="_blank">00:15:57.720</a></span> | <span class="t">That's not something you have to worry about when you're building your product, trying to keep up with the latest jailbreaks and prompt ejections, you can rely on your vendor for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=965" target="_blank">00:16:05.580</a></span> | <span class="t">So just to show you an example of AI runtime security at work, this is Pillar, it's an application security lifecycle application, which I have been using to do some of my work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=982" target="_blank">00:16:22.200</a></span> | <span class="t">This is just an example of one, there's also different AI runtime security solutions out there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=986" target="_blank">00:16:26.860</a></span> | <span class="t">But basically, I had an application that I was using for finding patients that would be suitable for ALS clinical trials.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1000" target="_blank">00:16:40.540</a></span> | <span class="t">And as you can see here, this is supposed to be an acceptable use case here, where we are getting patients in our database we would recommend for different studies that we found on the web.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1012" target="_blank">00:16:52.780</a></span> | <span class="t">So basically, the goal of this agent or this multi-agent system is to output a list of patients suitable for each trial.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1019" target="_blank">00:16:59.720</a></span> | <span class="t">It's not supposed to answer questions about individual patients or modify patient data in the database.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1026" target="_blank">00:17:06.280</a></span> | <span class="t">So in this example, I have asked this system to change a patient's FEC percentage in the database to 50.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1034" target="_blank">00:17:14.580</a></span> | <span class="t">The Pillar has blocked this for me, so my agent's not going to answer this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1038" target="_blank">00:17:18.860</a></span> | <span class="t">Because I have configured my guardrails to restrict this topic and keywords related to this so that we can make sure to not change patient data in the database and this application can behave as expected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1052" target="_blank">00:17:32.280</a></span> | <span class="t">So a lot of guardrails are where a lot of guardrails and applications or vendors will allow you to look out for things like PII, making sure that that's not being either input or output, depending on your configuration, making sure we're looking for things like AI safety concerns, toxic responses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1068" target="_blank">00:17:48.960</a></span> | <span class="t">But a really strong advantage of AI runtime security platforms or some of them is the ability to add custom guardrails.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1075" target="_blank">00:17:55.860</a></span> | <span class="t">So like this one in this example, where I've got this very specific functionality where I don't want my system to be able to update database information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1084" target="_blank">00:18:04.140</a></span> | <span class="t">So I'm able to use that for this, which, of course, is a security concern.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1087" target="_blank">00:18:07.960</a></span> | <span class="t">But I could also restrict topics around like, you know, say if I was Tesla and I didn't want to recommend Ford, for example, because that's a competing car company, I have restrict that topic to make sure that my outputs are outputs that are reflecting my business goals, as well as my AI security and AI safety goals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1104" target="_blank">00:18:24.960</a></span> | <span class="t">So runtime solutions can be particularly helpful and impactful in that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1108" target="_blank">00:18:28.760</a></span> | <span class="t">And then once we have an AI security runtime solution, we can go ahead and verify that in different GRC platforms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1121" target="_blank">00:18:41.220</a></span> | <span class="t">So as I mentioned before, I help with compliance as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1123" target="_blank">00:18:43.960</a></span> | <span class="t">So if we're doing all this work to build trustworthy AI, we might as well demonstrate it and we might as well show that to our customers so that they can trust in what we build and also help us with our sales cycles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1134" target="_blank">00:18:54.520</a></span> | <span class="t">So for instance, I have this risk in my risk register, which having a risk assessment is something that is required for SOC 2, at least annually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1144" target="_blank">00:19:04.860</a></span> | <span class="t">So if I have this custom risk of, you know, what if my company or patient service damage to do a harmful or off-topic output from an AI application?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1153" target="_blank">00:19:13.140</a></span> | <span class="t">In Vanta, I made a custom control for validating AI outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1161" target="_blank">00:19:21.380</a></span> | <span class="t">So in this control, I have submitted evidence that I'm going to be validating all of my different AI outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1168" target="_blank">00:19:28.280</a></span> | <span class="t">And so I can go ahead and throw that into a trust center and show that I'm passing controls for validating AI outputs and inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1175" target="_blank">00:19:35.700</a></span> | <span class="t">So if someone comes and wants to, you know, work with my company or buy my solution, they can go ahead and go on my trust center and see not only controls that are applicable to SOC 2, but additional controls that I have created.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1187" target="_blank">00:19:47.220</a></span> | <span class="t">For AI security that shows, hey, we're, you know, I'm using a runtime solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1190" target="_blank">00:19:50.760</a></span> | <span class="t">So, I mean, I'm clearly taking AI security seriously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1193" target="_blank">00:19:53.480</a></span> | <span class="t">And so maybe you want to buy from me, or maybe you want to work with my company, or maybe this, you know, helps you not send me a, you know, 200 question page security questionnaire, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1203" target="_blank">00:20:03.460</a></span> | <span class="t">So something to call out if you're, you know, if you're building with AI and you are taking AI, trustworthy AI seriously, and you're building it, might as well show it, might as well use it as a competitive advantage in your sales cycles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1217" target="_blank">00:20:17.620</a></span> | <span class="t">So if you're not convinced you need to build with trustworthy AI yet, here's some other reasons why you might want to take it seriously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1223" target="_blank">00:20:23.300</a></span> | <span class="t">Cyber security risk and business risk have never been more aligned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1227" target="_blank">00:20:27.580</a></span> | <span class="t">So in the past, you could have built a product that was insecure by design, shipped it, made some revenue with it, and then added security layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1235" target="_blank">00:20:35.740</a></span> | <span class="t">However, with AI, we're not seeing that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1238" target="_blank">00:20:38.200</a></span> | <span class="t">Making sure that AI applications output the correct outputs, that they are aligned, they are safe, they are on topic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1245" target="_blank">00:20:45.900</a></span> | <span class="t">That is as much of a cyber security concern as it is a business concern.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1250" target="_blank">00:20:50.600</a></span> | <span class="t">AI that outputs things that are off topic or relevant, it's not going to be revenue generating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1255" target="_blank">00:20:55.340</a></span> | <span class="t">So getting AI security and trustworthy AI right from the beginning will not only be helpful for your security program, but it will be helpful for revenue as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1263" target="_blank">00:21:03.000</a></span> | <span class="t">AI will also make missing cyber security best practices worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1267" target="_blank">00:21:07.940</a></span> | <span class="t">So you aren't tracking things like, you know, what data you have trained on, where you're getting different models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1273" target="_blank">00:21:13.900</a></span> | <span class="t">how you're taking care of supply chain risk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1276" target="_blank">00:21:16.880</a></span> | <span class="t">AI is only going to magnify that and make that worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1279" target="_blank">00:21:19.660</a></span> | <span class="t">And we're also seeing an increasing regulatory environment for compliance perspective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1285" target="_blank">00:21:25.460</a></span> | <span class="t">We're seeing ISO 42001 crop up, which is the first international standard around AI regulation or a compliance framework, sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1294" target="_blank">00:21:34.600</a></span> | <span class="t">And then the EO AI Act, of course, came out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1297" target="_blank">00:21:37.580</a></span> | <span class="t">And that's what this picture is about on the right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1299" target="_blank">00:21:39.360</a></span> | <span class="t">There was a case where an AI company was putting together a database of faces that they had scraped off the web.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1306" target="_blank">00:21:46.440</a></span> | <span class="t">They ended up getting fined by the EU for about $20 million thanks to the EO AI Act.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1311" target="_blank">00:21:51.240</a></span> | <span class="t">So it's important to take that seriously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1312" target="_blank">00:21:52.620</a></span> | <span class="t">If you are going to accidentally output patient data, if you're building a healthcare AI application, then you might be finding yourself with a HIPAA violation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1324" target="_blank">00:22:04.020</a></span> | <span class="t">So that's important to take into consideration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1325" target="_blank">00:22:05.780</a></span> | <span class="t">But also different guidelines that are specific to whatever you're building, whatever your industry is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1330" target="_blank">00:22:10.300</a></span> | <span class="t">So, for example, even the FDA came out with AI and ML guidelines earlier this year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1334" target="_blank">00:22:14.580</a></span> | <span class="t">So it's important to keep in mind what sort of regulations your company might be subject to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1339" target="_blank">00:22:19.280</a></span> | <span class="t">So it's worth getting ahead of those either regulations that exist now or will exist down the road by building trustworthy AI today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1346" target="_blank">00:22:26.920</a></span> | <span class="t">Trustworthy AI is super important because it's going to unlock a lot of revolutionary innovation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1357" target="_blank">00:22:37.100</a></span> | <span class="t">So, for instance, if we think about the healthcare industry, we can't use solutions that help us identify new proteins that can be able to create entirely new organs for transplant or allow us to use models that portraying on 1.3 million cells and has a lot of, like, very confidential data in them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1374" target="_blank">00:22:54.160</a></span> | <span class="t">We can't take advantage of the amazing technology that we're going to be able to create as a society if what we're building isn't trustworthy to begin with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1382" target="_blank">00:23:02.200</a></span> | <span class="t">These systems need compliance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1383" target="_blank">00:23:03.540</a></span> | <span class="t">They need AI safety.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1384" target="_blank">00:23:04.860</a></span> | <span class="t">They need AI security.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1386" target="_blank">00:23:06.600</a></span> | <span class="t">So if that doesn't motivate you to take trustworthy AI seriously, I hope it does because what will end up happening is we're going to be able to create things we never thought possible, but only if we had trust in place in the beginning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1398" target="_blank">00:23:18.560</a></span> | <span class="t">So the bottom line is you are responsible for building trustworthy AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1406" target="_blank">00:23:26.140</a></span> | <span class="t">We've seen the news headlines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1407" target="_blank">00:23:27.480</a></span> | <span class="t">It happens all the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1408" target="_blank">00:23:28.620</a></span> | <span class="t">We've also seen the lawsuits where it's often the user of the AI that's the one that's responsible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1415" target="_blank">00:23:35.140</a></span> | <span class="t">So, you know, don't wait to get yourself into a lawsuit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1417" target="_blank">00:23:37.600</a></span> | <span class="t">Start building trustworthy AI today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1419" target="_blank">00:23:39.560</a></span> | <span class="t">Trustworthy AI is AI security, which is how does the world harm your AI application, plus AI safety, which is how does your AI application harm the world?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1431" target="_blank">00:23:51.100</a></span> | <span class="t">So you can build trustworthy AI by incorporating ML SecOps practices, by red teaming your AI applications, and by incorporating AI runtime security solution at the time of deployment in your AI system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1444" target="_blank">00:24:04.500</a></span> | <span class="t">So thanks for watching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1448" target="_blank">00:24:08.420</a></span> | <span class="t">If you want to find me, here's my handles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1450" target="_blank">00:24:10.240</a></span> | <span class="t">But it was super awesome to deliver this discussion to you today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1453" target="_blank">00:24:13.940</a></span> | <span class="t">If you've got any questions, please let me know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1455" target="_blank">00:24:15.880</a></span> | <span class="t">Happy to reach out, answer more questions online.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1458" target="_blank">00:24:18.580</a></span> | <span class="t">But thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1459" target="_blank">00:24:19.080</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=n6wHJDqlS1I&t=1460" target="_blank">00:24:20.080</a></span> | <span class="t">Thank you.</span></div></div></body></html>