<html><head><title>Drive Gaze Region Classification in a Tesla</title></head><body>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    <a href="index.html">back to index</a><h2>Drive Gaze Region Classification in a Tesla</h2><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c"><img src="https://i.ytimg.com/vi_webp/ShxbqjnsB8c/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./ShxbqjnsB8c.html">Whisper Transcript</a> | <a href="./transcript_ShxbqjnsB8c.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=0">00:00:00.000</a></span> | <span class="t">We're driving around MIT campus today in this little bit of rain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=3">00:00:03.480</a></span> | <span class="t">In a Tesla we've instrumented with six cameras plus other sensors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=7">00:00:07.880</a></span> | <span class="t">all going into a single board computer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=10">00:00:10.200</a></span> | <span class="t">The reason we're doing that is we're going to give you a demo of driver gaze classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=17">00:00:17.440</a></span> | <span class="t">One of the things we're interested in our group at the MIT Age Lab is developing systems for driver state detection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=26">00:00:26.880</a></span> | <span class="t">So there is the first part is the perception control and planning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=30">00:00:30.800</a></span> | <span class="t">which comes from the external sensors, video cameras, radar, sometimes LiDAR.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=36">00:00:36.200</a></span> | <span class="t">And then there's inward facing sensors like the cameras we have in here that detect the state of the driver.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=42">00:00:42.440</a></span> | <span class="t">This is an important component because the car that's driving itself needs to know when the driver is able to take control back and vice versa.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=51">00:00:51.040</a></span> | <span class="t">And now is a visualization of some of the synchronized data we're capturing, both for real time detection and post-processing analysis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=59">00:00:59.520</a></span> | <span class="t">Top left is the video of the face. Bottom left is the video of the hands, lap and the instrument cluster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=66">00:01:06.120</a></span> | <span class="t">Bottom middle is the cropped video of the center stack display.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=70">00:01:10.720</a></span> | <span class="t">Bottom right is the fish eye video of the instrument cluster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=74">00:01:14.400</a></span> | <span class="t">And the top right is a video of the forward roadway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=78">00:01:18.640</a></span> | <span class="t">And then there are two things being detected. In the top middle is the visualization of the facial landmarks used in the gaze classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=87">00:01:27.760</a></span> | <span class="t">In the bottom right are annotations of the instrument cluster video showing the status of the autopilot based on the automatically detected autopilot icon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=100">00:01:40.200</a></span> | <span class="t">We can think of these two detections as classifying the state of the human and the state of the machine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=105">00:01:45.760</a></span> | <span class="t">And allows us to study the handover of control from the human to the machine and back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=113">00:01:53.040</a></span> | <span class="t">One of the key novel aspects of our approach is instead of looking at gaze estimation as a geometric problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=120">00:02:00.040</a></span> | <span class="t">we treat it as a supervised learning problem in classifying gaze into one of six regions of road, rearview mirror, left, right, center stack and instrument cluster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=132">00:02:12.360</a></span> | <span class="t">This approach allows us to use large semi-automatically annotated data sets to generalize over the edge cases that pop up in the wild.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=141">00:02:21.720</a></span> | <span class="t">And then in addition to the data on the CAN network, there's the automated detection of automation state from the instrument cluster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=151">00:02:31.160</a></span> | <span class="t">This combination of detecting human state and machine state allows us to study the interaction between the two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=158">00:02:38.600</a></span> | <span class="t">[END]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=161">00:02:41.120</a></span> | <span class="t">[AUDIO OUT]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ShxbqjnsB8c&t=163">00:02:43.120</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>