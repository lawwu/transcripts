<html><head><title>Intro to Sentence Embeddings with Transformers</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Intro to Sentence Embeddings with Transformers</h2><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ"><img src="https://i.ytimg.com/vi_webp/WS1uVMGhlWQ/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=58">0:58</a> Machine Translation<br><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=452">7:32</a> Transform Models<br><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=558">9:18</a> CrossEncoders<br><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=932">15:32</a> Softmax Loss Approach<br><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=972">16:12</a> Label Feature<br><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1134">18:54</a> Python Implementation<br><br><div style="text-align: left;"><a href="./WS1uVMGhlWQ.html">Whisper Transcript</a> | <a href="./transcript_WS1uVMGhlWQ.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi, welcome to the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=2" target="_blank">00:00:02.120</a></span> | <span class="t">We're going to explore how we can use sentence transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=5" target="_blank">00:00:05.680</a></span> | <span class="t">and sentence embeddings in NLP for semantic similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=11" target="_blank">00:00:11.160</a></span> | <span class="t">applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=12" target="_blank">00:00:12.560</a></span> | <span class="t">Now, in the video, we're going to have a quick recap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=16" target="_blank">00:00:16.360</a></span> | <span class="t">on transformers and where they came from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=18" target="_blank">00:00:18.400</a></span> | <span class="t">So we're going to have a quick look at recurring neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=20" target="_blank">00:00:20.740</a></span> | <span class="t">networks and the attention mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=22" target="_blank">00:00:22.800</a></span> | <span class="t">And then we're going to move on to trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=25" target="_blank">00:00:25.920</a></span> | <span class="t">to define what is the difference between a transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=29" target="_blank">00:00:29.440</a></span> | <span class="t">and a sentence transformer, and also understanding, OK,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=33" target="_blank">00:00:33.440</a></span> | <span class="t">why are these embeddings that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=35" target="_blank">00:00:35.840</a></span> | <span class="t">produced by transformers or sentence transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=38" target="_blank">00:00:38.640</a></span> | <span class="t">specifically so good?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=40" target="_blank">00:00:40.600</a></span> | <span class="t">And at the end, we're also going to go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=42" target="_blank">00:00:42.960</a></span> | <span class="t">through how we can implement our own sentence transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=46" target="_blank">00:00:46.400</a></span> | <span class="t">in Python as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=48" target="_blank">00:00:48.880</a></span> | <span class="t">So I think we should just jump straight into it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=59" target="_blank">00:00:59.000</a></span> | <span class="t">Before we dive into sentence transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=61" target="_blank">00:01:01.280</a></span> | <span class="t">I think it would make a lot of sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=63" target="_blank">00:01:03.640</a></span> | <span class="t">if we pieced together where transformers come from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=66" target="_blank">00:01:06.840</a></span> | <span class="t">with the intention of trying to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=69" target="_blank">00:01:09.280</a></span> | <span class="t">why we use transformers now rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=72" target="_blank">00:01:12.160</a></span> | <span class="t">than some other architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=74" target="_blank">00:01:14.680</a></span> | <span class="t">And I think it's also very important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=76" target="_blank">00:01:16.520</a></span> | <span class="t">if we try and figure out the difference between a transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=79" target="_blank">00:01:19.280</a></span> | <span class="t">and a sentence transformer as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=81" target="_blank">00:01:21.400</a></span> | <span class="t">So we're going to start with recurring neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=84" target="_blank">00:01:24.300</a></span> | <span class="t">And more specifically, I want to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=86" target="_blank">00:01:26.880</a></span> | <span class="t">a look at machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=88" target="_blank">00:01:28.960</a></span> | <span class="t">So machine translation use something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=92" target="_blank">00:01:32.040</a></span> | <span class="t">called a encoder-decoder network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=94" target="_blank">00:01:34.480</a></span> | <span class="t">where you would have a encoder, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=96" target="_blank">00:01:36.840</a></span> | <span class="t">is a set of, well, recurrent units, usually something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=100" target="_blank">00:01:40.840</a></span> | <span class="t">like LSTMs or GRUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=103" target="_blank">00:01:43.200</a></span> | <span class="t">And information, or that encoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=106" target="_blank">00:01:46.720</a></span> | <span class="t">would encode some sort of input text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=110" target="_blank">00:01:50.120</a></span> | <span class="t">So in, let's say, English, it would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=112" target="_blank">00:01:52.800</a></span> | <span class="t">encode that English text into something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=116" target="_blank">00:01:56.040</a></span> | <span class="t">called a context vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=117" target="_blank">00:01:57.600</a></span> | <span class="t">And then this context vector would be passed along</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=119" target="_blank">00:01:59.720</a></span> | <span class="t">to a decoder network, which, again, is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=123" target="_blank">00:02:03.160</a></span> | <span class="t">another set of LSTM or GRU units.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=127" target="_blank">00:02:07.200</a></span> | <span class="t">And it would decode those into an autologous language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=130" target="_blank">00:02:10.920</a></span> | <span class="t">say, like French, or in this case, actually, Italian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=134" target="_blank">00:02:14.560</a></span> | <span class="t">That is how machine translation worked back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=137" target="_blank">00:02:17.960</a></span> | <span class="t">with recurring neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=139" target="_blank">00:02:19.840</a></span> | <span class="t">The only issue is that we're trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=141" target="_blank">00:02:21.320</a></span> | <span class="t">to pass a lot of information through that single point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=144" target="_blank">00:02:24.880</a></span> | <span class="t">between the encoder and the decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=148" target="_blank">00:02:28.240</a></span> | <span class="t">Now, that creates what is called information bottleneck.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=152" target="_blank">00:02:32.600</a></span> | <span class="t">There's too much information trying to be crammed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=155" target="_blank">00:02:35.280</a></span> | <span class="t">through that single one point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=157" target="_blank">00:02:37.440</a></span> | <span class="t">So what they came up with is something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=161" target="_blank">00:02:41.120</a></span> | <span class="t">called the attention mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=162" target="_blank">00:02:42.760</a></span> | <span class="t">And what the attention mechanism does is for every step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=165" target="_blank">00:02:45.800</a></span> | <span class="t">or every token that is decoded by our decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=169" target="_blank">00:02:49.560</a></span> | <span class="t">that token is sent to the attention mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=173" target="_blank">00:02:53.520</a></span> | <span class="t">And the alignment between the decoder at that time step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=178" target="_blank">00:02:58.880</a></span> | <span class="t">is compared to all of the encoder units</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=182" target="_blank">00:03:02.920</a></span> | <span class="t">or on all of the encoder hidden states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=186" target="_blank">00:03:06.880</a></span> | <span class="t">And what that does is essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=189" target="_blank">00:03:09.280</a></span> | <span class="t">builds this type of attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=192" target="_blank">00:03:12.000</a></span> | <span class="t">So it tells the decoder which tokens from the encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=195" target="_blank">00:03:15.200</a></span> | <span class="t">to focus on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=196" target="_blank">00:03:16.400</a></span> | <span class="t">So it's literally saying, where do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=198" target="_blank">00:03:18.640</a></span> | <span class="t">I need to pay attention to whatever my current unit is?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=203" target="_blank">00:03:23.920</a></span> | <span class="t">And this attention mechanism, what it produces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=206" target="_blank">00:03:26.880</a></span> | <span class="t">is something like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=209" target="_blank">00:03:29.400</a></span> | <span class="t">So this is from another very well-known paper in 2015.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=214" target="_blank">00:03:34.840</a></span> | <span class="t">And what you can see is a matrix of the-- we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=219" target="_blank">00:03:39.520</a></span> | <span class="t">have the French words or the French translation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=222" target="_blank">00:03:42.600</a></span> | <span class="t">on the left, on the y-axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=225" target="_blank">00:03:45.200</a></span> | <span class="t">And then on the top, we have the English translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=228" target="_blank">00:03:48.960</a></span> | <span class="t">And all of these boxes you see are the activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=233" target="_blank">00:03:53.480</a></span> | <span class="t">of the attention mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=235" target="_blank">00:03:55.800</a></span> | <span class="t">So we can see essentially which words are the most aligned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=240" target="_blank">00:04:00.440</a></span> | <span class="t">And that is essentially what the attention mechanism did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=244" target="_blank">00:04:04.280</a></span> | <span class="t">It allowed the decoder to focus on the most relevant words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=248" target="_blank">00:04:08.920</a></span> | <span class="t">in the encoder part of the network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=251" target="_blank">00:04:11.880</a></span> | <span class="t">Now, moving on, in 2017, there was another paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=256" target="_blank">00:04:16.040</a></span> | <span class="t">called "Attention is All You Need."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=257" target="_blank">00:04:17.800</a></span> | <span class="t">And this really marked, I think, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=261" target="_blank">00:04:21.240</a></span> | <span class="t">is a turning point in NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=264" target="_blank">00:04:24.040</a></span> | <span class="t">What was found in this paper is that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=266" target="_blank">00:04:26.520</a></span> | <span class="t">could remove the recurrent part of the encoder decoder network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=271" target="_blank">00:04:31.520</a></span> | <span class="t">and maintain just the attention mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=275" target="_blank">00:04:35.200</a></span> | <span class="t">And what they produced with a few modifications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=279" target="_blank">00:04:39.040</a></span> | <span class="t">to the attention was a high-performing model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=282" target="_blank">00:04:42.160</a></span> | <span class="t">than any of the recurrent neural networks with attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=284" target="_blank">00:04:44.840</a></span> | <span class="t">or without attention that came before it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=287" target="_blank">00:04:47.520</a></span> | <span class="t">And what they named this new model was a transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=293" target="_blank">00:04:53.560</a></span> | <span class="t">So this 2017 paper, "Attention is All You Need,"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=297" target="_blank">00:04:57.600</a></span> | <span class="t">is where transformers came from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=300" target="_blank">00:05:00.840</a></span> | <span class="t">And they actually came from a mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=304" target="_blank">00:05:04.640</a></span> | <span class="t">that was aimed to help improve recurrent neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=310" target="_blank">00:05:10.160</a></span> | <span class="t">Now, of course, like I said, the attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=312" target="_blank">00:05:12.720</a></span> | <span class="t">was not just the same plain attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=316" target="_blank">00:05:16.360</a></span> | <span class="t">that was used before in recurrent neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=319" target="_blank">00:05:19.400</a></span> | <span class="t">It had been modified a little bit as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=322" target="_blank">00:05:22.240</a></span> | <span class="t">And those modifications really came down to three key changes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=328" target="_blank">00:05:28.200</a></span> | <span class="t">And those were positional encoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=331" target="_blank">00:05:31.080</a></span> | <span class="t">which replaced the key advantage of recurrent neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=334" target="_blank">00:05:34.920</a></span> | <span class="t">in NLP, which was the ability to consider</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=337" target="_blank">00:05:37.800</a></span> | <span class="t">the order of a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=339" target="_blank">00:05:39.640</a></span> | <span class="t">Because they were recurrent neural networks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=341" target="_blank">00:05:41.920</a></span> | <span class="t">they considered one word or one time set after the other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=345" target="_blank">00:05:45.880</a></span> | <span class="t">So there was a sense of order to those models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=349" target="_blank">00:05:49.880</a></span> | <span class="t">that does not appear in, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=351" target="_blank">00:05:51.800</a></span> | <span class="t">convolutional neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=353" target="_blank">00:05:53.760</a></span> | <span class="t">And this positional encoding worked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=356" target="_blank">00:05:56.480</a></span> | <span class="t">by adding a set of varying sine wave activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=360" target="_blank">00:06:00.160</a></span> | <span class="t">to each input embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=361" target="_blank">00:06:01.880</a></span> | <span class="t">And these activations varied based on the position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=366" target="_blank">00:06:06.720</a></span> | <span class="t">of the word or token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=367" target="_blank">00:06:07.840</a></span> | <span class="t">So what you have there is a way for the network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=372" target="_blank">00:06:12.480</a></span> | <span class="t">to identify the order of the tokens or the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=376" target="_blank">00:06:16.280</a></span> | <span class="t">activations or embeddings that are being processed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=380" target="_blank">00:06:20.080</a></span> | <span class="t">The next change was self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=382" target="_blank">00:06:22.920</a></span> | <span class="t">Now, self-attention is where the attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=386" target="_blank">00:06:26.280</a></span> | <span class="t">is applied between a word and all of the other words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=389" target="_blank">00:06:29.760</a></span> | <span class="t">in its own context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=392" target="_blank">00:06:32.160</a></span> | <span class="t">So the sentence or the paragraph that it belongs in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=395" target="_blank">00:06:35.400</a></span> | <span class="t">Now, we saw with the encoder/decoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=397" target="_blank">00:06:37.520</a></span> | <span class="t">that attention was being applied between the decoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=401" target="_blank">00:06:41.280</a></span> | <span class="t">and the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=402" target="_blank">00:06:42.880</a></span> | <span class="t">This is like applying attention to the encoder and the encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=407" target="_blank">00:06:47.280</a></span> | <span class="t">again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=408" target="_blank">00:06:48.160</a></span> | <span class="t">And what this did is, rather than just embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=412" target="_blank">00:06:52.280</a></span> | <span class="t">the meaning of a word, it also embeds the context of a word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=416" target="_blank">00:06:56.480</a></span> | <span class="t">into its vector or word representations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=421" target="_blank">00:07:01.040</a></span> | <span class="t">which obviously greatly enriched the amount of information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=424" target="_blank">00:07:04.480</a></span> | <span class="t">that you have within that embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=427" target="_blank">00:07:07.560</a></span> | <span class="t">And then the third and final change that they made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=431" target="_blank">00:07:11.120</a></span> | <span class="t">was the addition of multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=434" target="_blank">00:07:14.640</a></span> | <span class="t">And we can see multi-head attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=437" target="_blank">00:07:17.480</a></span> | <span class="t">as several parallel attention mechanisms working together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=442" target="_blank">00:07:22.880</a></span> | <span class="t">And using these multiple attention heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=446" target="_blank">00:07:26.600</a></span> | <span class="t">allowed the representation of several sets of relationships</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=450" target="_blank">00:07:30.640</a></span> | <span class="t">rather than just a single set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=452" target="_blank">00:07:32.840</a></span> | <span class="t">Now, these new transform models also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=456" target="_blank">00:07:36.520</a></span> | <span class="t">had the benefit of generalizing very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=459" target="_blank">00:07:39.520</a></span> | <span class="t">So what we find with transform models--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=461" target="_blank">00:07:41.720</a></span> | <span class="t">and of course, you could do this to an extent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=464" target="_blank">00:07:44.120</a></span> | <span class="t">with your current neural networks as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=466" target="_blank">00:07:46.760</a></span> | <span class="t">But it was far less effective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=470" target="_blank">00:07:50.640</a></span> | <span class="t">So with transform models, we take the core of the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=474" target="_blank">00:07:54.000</a></span> | <span class="t">which has been trained using a significant amount of time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=478" target="_blank">00:07:58.360</a></span> | <span class="t">and computing power by the likes of Google and OpenAI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=484" target="_blank">00:08:04.120</a></span> | <span class="t">We just take that core model, add a few layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=486" target="_blank">00:08:06.960</a></span> | <span class="t">onto the end of it that are designed in a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=489" target="_blank">00:08:09.600</a></span> | <span class="t">for our specific use case, and train it a little bit more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=493" target="_blank">00:08:13.680</a></span> | <span class="t">So we fine-tune it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=495" target="_blank">00:08:15.960</a></span> | <span class="t">And I think one of the most widely known of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=500" target="_blank">00:08:20.480</a></span> | <span class="t">or the most popular of these models is probably BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=505" target="_blank">00:08:25.080</a></span> | <span class="t">And of course, there are other models as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=508" target="_blank">00:08:28.400</a></span> | <span class="t">Later in this video, we're going to have a look at one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=510" target="_blank">00:08:30.880</a></span> | <span class="t">called the MPNet model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=513" target="_blank">00:08:33.920</a></span> | <span class="t">But BERT is certainly one of the, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=516" target="_blank">00:08:36.280</a></span> | <span class="t">most popular of those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=518" target="_blank">00:08:38.200</a></span> | <span class="t">Now, so far, we've explained that transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=522" target="_blank">00:08:42.520</a></span> | <span class="t">have much richer embeddings, or word or token embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=527" target="_blank">00:08:47.800</a></span> | <span class="t">than anything that came before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=530" target="_blank">00:08:50.240</a></span> | <span class="t">And that's good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=531" target="_blank">00:08:51.360</a></span> | <span class="t">But we're interested in not word or token-level embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=536" target="_blank">00:08:56.280</a></span> | <span class="t">but sentence embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=537" target="_blank">00:08:57.600</a></span> | <span class="t">Because we want to do semantic similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=541" target="_blank">00:09:01.160</a></span> | <span class="t">between sentences or paragraphs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=544" target="_blank">00:09:04.360</a></span> | <span class="t">And of course, transformers, they work--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=548" target="_blank">00:09:08.160</a></span> | <span class="t">the inside of the transformer works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=550" target="_blank">00:09:10.240</a></span> | <span class="t">based on word-level or token-level embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=554" target="_blank">00:09:14.760</a></span> | <span class="t">So that doesn't really help us that much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=558" target="_blank">00:09:18.680</a></span> | <span class="t">So what would happen before sentence transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=562" target="_blank">00:09:22.480</a></span> | <span class="t">were introduced is we would use something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=565" target="_blank">00:09:25.480</a></span> | <span class="t">called a cross-encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=567" target="_blank">00:09:27.000</a></span> | <span class="t">So we would have a BERT cross-encoder model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=569" target="_blank">00:09:29.560</a></span> | <span class="t">And like I said before, we have the core BERT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=572" target="_blank">00:09:32.760</a></span> | <span class="t">And then we just add a couple of layers onto the end of it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=575" target="_blank">00:09:35.360</a></span> | <span class="t">and fine-tune it for our specific use case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=578" target="_blank">00:09:38.200</a></span> | <span class="t">And that's what cross-encoder is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=579" target="_blank">00:09:39.640</a></span> | <span class="t">It's the core BERT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=582" target="_blank">00:09:42.160</a></span> | <span class="t">As you can see on the screen right now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=584" target="_blank">00:09:44.520</a></span> | <span class="t">it's a core BERT model followed by a feedforward neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=587" target="_blank">00:09:47.880</a></span> | <span class="t">network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=588" target="_blank">00:09:48.720</a></span> | <span class="t">We pass two sentences into the BERT model at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=592" target="_blank">00:09:52.480</a></span> | <span class="t">The BERT model embeds them with very rich word and token-level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=598" target="_blank">00:09:58.000</a></span> | <span class="t">embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=598" target="_blank">00:09:58.840</a></span> | <span class="t">The feedforward network takes those and decides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=601" target="_blank">00:10:01.520</a></span> | <span class="t">how similar those two sentences are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=606" target="_blank">00:10:06.200</a></span> | <span class="t">Now, this is fine, and it is actually accurate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=609" target="_blank">00:10:09.360</a></span> | <span class="t">It works well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=611" target="_blank">00:10:11.200</a></span> | <span class="t">But it's not really scalable, because essentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=614" target="_blank">00:10:14.160</a></span> | <span class="t">every time you want to compare a pair of sentences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=617" target="_blank">00:10:17.760</a></span> | <span class="t">you have to run a full inference computation on BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=622" target="_blank">00:10:22.720</a></span> | <span class="t">So let's say we wanted to form a semantic similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=626" target="_blank">00:10:26.040</a></span> | <span class="t">search across just 100,000 sentences, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=629" target="_blank">00:10:29.240</a></span> | <span class="t">is a reasonably small data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=632" target="_blank">00:10:32.360</a></span> | <span class="t">We would have to run the BERT inference computation 100,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=637" target="_blank">00:10:37.880</a></span> | <span class="t">times to actually go through and identify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=642" target="_blank">00:10:42.160</a></span> | <span class="t">the similarity between all of those sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=644" target="_blank">00:10:44.640</a></span> | <span class="t">And that's going to take a lot of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=646" target="_blank">00:10:46.720</a></span> | <span class="t">And it gets worse as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=648" target="_blank">00:10:48.120</a></span> | <span class="t">I mean, if you consider clustering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=650" target="_blank">00:10:50.400</a></span> | <span class="t">all of those 100,000 sentences, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=655" target="_blank">00:10:55.520</a></span> | <span class="t">would end up with just under 500 million comparisons there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=661" target="_blank">00:11:01.320</a></span> | <span class="t">So running a full BERT inference prediction 500 million times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=667" target="_blank">00:11:07.800</a></span> | <span class="t">just to cluster 100,000 sentences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=671" target="_blank">00:11:11.280</a></span> | <span class="t">obviously, that's not scalable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=674" target="_blank">00:11:14.040</a></span> | <span class="t">That will take a very, very long time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=676" target="_blank">00:11:16.480</a></span> | <span class="t">So ideally, what we need is something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=679" target="_blank">00:11:19.400</a></span> | <span class="t">like word or token embeddings, but for sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=683" target="_blank">00:11:23.080</a></span> | <span class="t">Now, with the original BERT, we could actually produce these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=688" target="_blank">00:11:28.360</a></span> | <span class="t">They just were not very good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=689" target="_blank">00:11:29.760</a></span> | <span class="t">So what we could do is take the mean value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=693" target="_blank">00:11:33.720</a></span> | <span class="t">across all of our word embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=696" target="_blank">00:11:36.000</a></span> | <span class="t">Typically, BERT is 512, those being output by the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=700" target="_blank">00:11:40.200</a></span> | <span class="t">We could take the average across all of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=702" target="_blank">00:11:42.240</a></span> | <span class="t">and take that average as what we call sentence embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=705" target="_blank">00:11:45.880</a></span> | <span class="t">Now, there were other methods of doing this as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=709" target="_blank">00:11:49.280</a></span> | <span class="t">That was probably the most popular and effective one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=712" target="_blank">00:11:52.520</a></span> | <span class="t">And we could take that sentence embedding, store it somewhere,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=715" target="_blank">00:11:55.760</a></span> | <span class="t">and then we could just compare it to other sentence embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=718" target="_blank">00:11:58.240</a></span> | <span class="t">using something that's a bit simpler</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=719" target="_blank">00:11:59.720</a></span> | <span class="t">than a full BERT computation, like cosine similarity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=724" target="_blank">00:12:04.640</a></span> | <span class="t">And that is much faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=727" target="_blank">00:12:07.320</a></span> | <span class="t">That's fast enough for us, but it's just not that accurate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=732" target="_blank">00:12:12.880</a></span> | <span class="t">And it was actually found that comparing average glove</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=736" target="_blank">00:12:16.440</a></span> | <span class="t">embeddings, which were produced in 2014,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=740" target="_blank">00:12:20.320</a></span> | <span class="t">were actually more accurate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=743" target="_blank">00:12:23.920</a></span> | <span class="t">So we can't really do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=745" target="_blank">00:12:25.720</a></span> | <span class="t">We can't use what is called a mean pooling approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=752" target="_blank">00:12:32.040</a></span> | <span class="t">Or we can't use it in its current form.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=755" target="_blank">00:12:35.120</a></span> | <span class="t">Now, the solution to this problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=756" target="_blank">00:12:36.680</a></span> | <span class="t">was introduced by two people in 2019.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=760" target="_blank">00:12:40.440</a></span> | <span class="t">Nils Reimers and Irina Gurevich.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=764" target="_blank">00:12:44.120</a></span> | <span class="t">They introduced what is the first sentence transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=768" target="_blank">00:12:48.440</a></span> | <span class="t">or sentence BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=770" target="_blank">00:12:50.040</a></span> | <span class="t">And it was found that sentence BERT or SBERT outperformed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=774" target="_blank">00:12:54.480</a></span> | <span class="t">all of the previous state-of-the-art models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=777" target="_blank">00:12:57.920</a></span> | <span class="t">on pretty much all benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=779" target="_blank">00:12:59.880</a></span> | <span class="t">Not all of them, but most of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=782" target="_blank">00:13:02.280</a></span> | <span class="t">And it did it in a very quick time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=787" target="_blank">00:13:07.520</a></span> | <span class="t">So if we compare it to BERT, if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=790" target="_blank">00:13:10.240</a></span> | <span class="t">wanted to find the most similar sentence pair from 10,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=794" target="_blank">00:13:14.120</a></span> | <span class="t">sentences, in that 2019 paper, they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=797" target="_blank">00:13:17.200</a></span> | <span class="t">found that with BERT, that took 65 hours.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=801" target="_blank">00:13:21.160</a></span> | <span class="t">With SBERT embeddings, they could create all the embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=805" target="_blank">00:13:25.120</a></span> | <span class="t">in just around 5 seconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=808" target="_blank">00:13:28.080</a></span> | <span class="t">And then they could compare all of those with cosine similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=811" target="_blank">00:13:31.360</a></span> | <span class="t">in 0.01 seconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=813" target="_blank">00:13:33.680</a></span> | <span class="t">So it's a lot faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=816" target="_blank">00:13:36.120</a></span> | <span class="t">We go from 65 hours to just over 5 seconds,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=819" target="_blank">00:13:39.920</a></span> | <span class="t">which is, I think, pretty incredible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=822" target="_blank">00:13:42.920</a></span> | <span class="t">Now, I think that's pretty much all the context we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=825" target="_blank">00:13:45.920</a></span> | <span class="t">behind sentence transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=827" target="_blank">00:13:47.960</a></span> | <span class="t">And what we'll do now is dive into a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=830" target="_blank">00:13:50.680</a></span> | <span class="t">of how they actually work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=832" target="_blank">00:13:52.680</a></span> | <span class="t">Now, we said before we have the core transform models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=837" target="_blank">00:13:57.280</a></span> | <span class="t">And what SBERT does is fine-tunes on sentence pairs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=842" target="_blank">00:14:02.840</a></span> | <span class="t">using what is called a Siamese architecture or Siamese</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=846" target="_blank">00:14:06.560</a></span> | <span class="t">network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=848" target="_blank">00:14:08.360</a></span> | <span class="t">What we mean by a Siamese network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=850" target="_blank">00:14:10.520</a></span> | <span class="t">is that we have what we can see, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=852" target="_blank">00:14:12.760</a></span> | <span class="t">can view as two BERT models that are identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=858" target="_blank">00:14:18.240</a></span> | <span class="t">And the weights between those two models are tied.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=860" target="_blank">00:14:20.680</a></span> | <span class="t">Now, in reality, when we're implementing this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=863" target="_blank">00:14:23.640</a></span> | <span class="t">we just use a single BERT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=865" target="_blank">00:14:25.440</a></span> | <span class="t">And what we do is we process one sentence, sentence A,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=869" target="_blank">00:14:29.920</a></span> | <span class="t">through the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=871" target="_blank">00:14:31.040</a></span> | <span class="t">And then we process another sentence, sentence B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=873" target="_blank">00:14:33.760</a></span> | <span class="t">through the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=874" target="_blank">00:14:34.560</a></span> | <span class="t">And that's the sentence pair.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=876" target="_blank">00:14:36.240</a></span> | <span class="t">So with our cross-encoder, we were processing the sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=879" target="_blank">00:14:39.360</a></span> | <span class="t">pair together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=880" target="_blank">00:14:40.040</a></span> | <span class="t">We were putting them both together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=881" target="_blank">00:14:41.500</a></span> | <span class="t">processing them all at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=883" target="_blank">00:14:43.560</a></span> | <span class="t">This time, we processed them separately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=885" target="_blank">00:14:45.760</a></span> | <span class="t">And during training, what happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=888" target="_blank">00:14:48.400</a></span> | <span class="t">is the weights within BERT are optimized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=892" target="_blank">00:14:52.160</a></span> | <span class="t">to reduce the difference between two vector embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=896" target="_blank">00:14:56.320</a></span> | <span class="t">or two sentence embeddings that are produced for sentence A</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=899" target="_blank">00:14:59.800</a></span> | <span class="t">and sentence B. And those sentence embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=903" target="_blank">00:15:03.880</a></span> | <span class="t">are called U and V.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=905" target="_blank">00:15:05.600</a></span> | <span class="t">Now, to actually create those sentence embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=907" target="_blank">00:15:07.960</a></span> | <span class="t">we do what we did before with BERT, where we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=910" target="_blank">00:15:10.600</a></span> | <span class="t">do the mean pooling operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=912" target="_blank">00:15:12.680</a></span> | <span class="t">Now, the reason that this works better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=914" target="_blank">00:15:14.920</a></span> | <span class="t">is because we're fine-tuning it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=916" target="_blank">00:15:16.560</a></span> | <span class="t">So with BERT, we didn't fine-tune it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=918" target="_blank">00:15:18.720</a></span> | <span class="t">This time, we are fine-tuning it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=920" target="_blank">00:15:20.680</a></span> | <span class="t">Now, there are several different ways of training SBIRT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=924" target="_blank">00:15:24.960</a></span> | <span class="t">But the one that was covered most prominently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=928" target="_blank">00:15:28.640</a></span> | <span class="t">in the original paper is called the Softmax Loss Approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=932" target="_blank">00:15:32.880</a></span> | <span class="t">And that's what we are going to be describing here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=936" target="_blank">00:15:36.400</a></span> | <span class="t">Now, for the Softmax Loss Approach,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=939" target="_blank">00:15:39.680</a></span> | <span class="t">we can train on natural language inference data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=943" target="_blank">00:15:43.520</a></span> | <span class="t">Now, the 2019 paper used two of those, the Stanford Natural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=947" target="_blank">00:15:47.640</a></span> | <span class="t">Language Inference Corpus and the Multi-Genre Natural Language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=951" target="_blank">00:15:51.920</a></span> | <span class="t">Inference Corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=953" target="_blank">00:15:53.800</a></span> | <span class="t">Now, both of these were merged together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=957" target="_blank">00:15:57.720</a></span> | <span class="t">And what we have inside there are sentence pairs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=962" target="_blank">00:16:02.200</a></span> | <span class="t">One is a premise, which suggests a certain hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=967" target="_blank">00:16:07.720</a></span> | <span class="t">So those two sentence pairs are, in some cases, related.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=972" target="_blank">00:16:12.400</a></span> | <span class="t">And we can tell whether they're related or not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=974" target="_blank">00:16:14.560</a></span> | <span class="t">using the label feature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=976" target="_blank">00:16:16.920</a></span> | <span class="t">Now, the label feature contains three different classes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=982" target="_blank">00:16:22.080</a></span> | <span class="t">We have 0, which is called entailment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=985" target="_blank">00:16:25.480</a></span> | <span class="t">And what this is, is it indicates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=989" target="_blank">00:16:29.360</a></span> | <span class="t">that the premise sentence suggests the hypothesis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=993" target="_blank">00:16:33.480</a></span> | <span class="t">sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=994" target="_blank">00:16:34.720</a></span> | <span class="t">Then we have class 1, or label 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=997" target="_blank">00:16:37.680</a></span> | <span class="t">That means the two sentences are neutral.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1001" target="_blank">00:16:41.200</a></span> | <span class="t">So they could both be true, but they are not necessarily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1003" target="_blank">00:16:43.760</a></span> | <span class="t">related.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1004" target="_blank">00:16:44.960</a></span> | <span class="t">And then we have number 2, which is contradiction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1007" target="_blank">00:16:47.840</a></span> | <span class="t">which means the premise and hypothesis sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1011" target="_blank">00:16:51.520</a></span> | <span class="t">contradict each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1013" target="_blank">00:16:53.320</a></span> | <span class="t">Now, given this data, we feed sentence A</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1016" target="_blank">00:16:56.880</a></span> | <span class="t">into BERT first for our fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1021" target="_blank">00:17:01.400</a></span> | <span class="t">And then we feed sentence B into BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1025" target="_blank">00:17:05.080</a></span> | <span class="t">So that's our premise and hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1029" target="_blank">00:17:09.280</a></span> | <span class="t">The Siamese BERT, or BERT, outputs two sentence embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1035" target="_blank">00:17:15.280</a></span> | <span class="t">U and V, from this process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1038" target="_blank">00:17:18.720</a></span> | <span class="t">And what we do is concatenate those two sentence embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1043" target="_blank">00:17:23.440</a></span> | <span class="t">Now, the paper explained a few different ways of doing that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1047" target="_blank">00:17:27.760</a></span> | <span class="t">But the most effective was to take U and V.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1051" target="_blank">00:17:31.080</a></span> | <span class="t">And what we do is we take the absolute value of U minus V,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1056" target="_blank">00:17:36.800</a></span> | <span class="t">which is an element-wise operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1058" target="_blank">00:17:38.520</a></span> | <span class="t">So we're basically finding the difference between U and V.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1061" target="_blank">00:17:41.800</a></span> | <span class="t">And that produces this other vector, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1064" target="_blank">00:17:44.040</a></span> | <span class="t">is the bar U minus V bar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1067" target="_blank">00:17:47.760</a></span> | <span class="t">And then we concatenate all of those together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1070" target="_blank">00:17:50.960</a></span> | <span class="t">So they all get concatenated together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1074" target="_blank">00:17:54.000</a></span> | <span class="t">And then they are passed into a feed-forward neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1078" target="_blank">00:17:58.880</a></span> | <span class="t">Now, this feed-forward neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1082" target="_blank">00:18:02.120</a></span> | <span class="t">takes the concatenated vector length, or sentence embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1086" target="_blank">00:18:06.080</a></span> | <span class="t">length, as its input and outputs just three activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1091" target="_blank">00:18:11.680</a></span> | <span class="t">Now, those three activations align with our three labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1096" target="_blank">00:18:16.280</a></span> | <span class="t">So what we do from there, we have those three activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1099" target="_blank">00:18:19.520</a></span> | <span class="t">We then need to calculate the softmax loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1101" target="_blank">00:18:21.480</a></span> | <span class="t">between those predicted labels and the true labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1106" target="_blank">00:18:26.160</a></span> | <span class="t">Now, softmax loss is nothing more than a cross-entropy loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1112" target="_blank">00:18:32.000</a></span> | <span class="t">function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1113" target="_blank">00:18:33.040</a></span> | <span class="t">And that's really all there is to training one of these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1117" target="_blank">00:18:37.680</a></span> | <span class="t">Now, we are going to cover all of this in full.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1120" target="_blank">00:18:40.920</a></span> | <span class="t">We're going to go through the code and everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1122" target="_blank">00:18:42.920</a></span> | <span class="t">and train our own sentence button model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1125" target="_blank">00:18:45.920</a></span> | <span class="t">But for now, we're just describing the process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1128" target="_blank">00:18:48.800</a></span> | <span class="t">And we'll cover that in another--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1131" target="_blank">00:18:51.000</a></span> | <span class="t">well, probably in the next video and article.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1134" target="_blank">00:18:54.200</a></span> | <span class="t">Now, that's really, I think, everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1138" target="_blank">00:18:58.240</a></span> | <span class="t">we need to know for now on how they work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1140" target="_blank">00:19:00.920</a></span> | <span class="t">and where sentence transformers and transformers come from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1144" target="_blank">00:19:04.880</a></span> | <span class="t">So let's jump into Python.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1148" target="_blank">00:19:08.040</a></span> | <span class="t">And what we'll do is actually implement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1149" target="_blank">00:19:09.960</a></span> | <span class="t">some of these models using the sentence transformers library,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1152" target="_blank">00:19:12.840</a></span> | <span class="t">which was built by the same people who designed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1157" target="_blank">00:19:17.680</a></span> | <span class="t">the first sentence transformer, SBIRT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1160" target="_blank">00:19:20.240</a></span> | <span class="t">So let's go ahead and do that now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1162" target="_blank">00:19:22.600</a></span> | <span class="t">So the first thing that you will need to do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1164" target="_blank">00:19:24.600</a></span> | <span class="t">if you do not already have sentence transformers installed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1169" target="_blank">00:19:29.240</a></span> | <span class="t">is just pip install sentence transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1173" target="_blank">00:19:33.600</a></span> | <span class="t">So I already have it installed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1175" target="_blank">00:19:35.160</a></span> | <span class="t">So I'm not going to go ahead and run that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1178" target="_blank">00:19:38.640</a></span> | <span class="t">So all I'm going to do now is, from sentence transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1184" target="_blank">00:19:44.160</a></span> | <span class="t">I'm going to import the sentence transformer class object.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1192" target="_blank">00:19:52.920</a></span> | <span class="t">And then from there, we can initialize a sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1196" target="_blank">00:19:56.360</a></span> | <span class="t">transformer, super easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1197" target="_blank">00:19:57.720</a></span> | <span class="t">So all we need to write is model equals sentence transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1204" target="_blank">00:20:04.480</a></span> | <span class="t">And then in here, we just need to write our model name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1208" target="_blank">00:20:08.160</a></span> | <span class="t">Now, if you go to this website, SBIRT.net,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1216" target="_blank">00:20:16.000</a></span> | <span class="t">you will find a load of different models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1219" target="_blank">00:20:19.600</a></span> | <span class="t">Now, the one that we will be using is this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1221" target="_blank">00:20:21.920</a></span> | <span class="t">So this is the original SBIRT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1225" target="_blank">00:20:25.000</a></span> | <span class="t">And if we just come down here and print out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1227" target="_blank">00:20:27.080</a></span> | <span class="t">what that will return to us, we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1231" target="_blank">00:20:31.080</a></span> | <span class="t">see a few different components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1233" target="_blank">00:20:33.200</a></span> | <span class="t">So you can see that we have two components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1240" target="_blank">00:20:40.200</a></span> | <span class="t">We have the transform model, and we have the pooling layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1243" target="_blank">00:20:43.560</a></span> | <span class="t">which is the mean pooling that I mentioned before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1246" target="_blank">00:20:46.160</a></span> | <span class="t">Now, the transformer, we see that the max sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1248" target="_blank">00:20:48.760</a></span> | <span class="t">length, so the maximum number of tokens that we can input there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1251" target="_blank">00:20:51.400</a></span> | <span class="t">is 128.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1252" target="_blank">00:20:52.840</a></span> | <span class="t">And we see that we're using the base model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1255" target="_blank">00:20:55.360</a></span> | <span class="t">It's a BIRT model from the Hugging Face library.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1260" target="_blank">00:21:00.320</a></span> | <span class="t">Then in pooling, we can see the output dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1264" target="_blank">00:21:04.160</a></span> | <span class="t">of our sentence vector, which is 768.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1267" target="_blank">00:21:07.800</a></span> | <span class="t">And we can also see the way that the tokens have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1271" target="_blank">00:21:11.120</a></span> | <span class="t">been pooled to create the sentence embedding, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1273" target="_blank">00:21:13.400</a></span> | <span class="t">is just the mean pooling approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1277" target="_blank">00:21:17.120</a></span> | <span class="t">Now, given these sentences here, I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1281" target="_blank">00:21:21.680</a></span> | <span class="t">going to run that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1283" target="_blank">00:21:23.800</a></span> | <span class="t">All we need to do to actually encode those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1286" target="_blank">00:21:26.640</a></span> | <span class="t">is we write model.encode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1289" target="_blank">00:21:29.720</a></span> | <span class="t">And then we just pass sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1293" target="_blank">00:21:33.200</a></span> | <span class="t">And that will create our embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1294" target="_blank">00:21:34.960</a></span> | <span class="t">So I'm just going to call this embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1298" target="_blank">00:21:38.240</a></span> | <span class="t">And let's see what they look like, so embeddings again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1301" target="_blank">00:21:41.200</a></span> | <span class="t">And we see that we have these, which are our embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1308" target="_blank">00:21:48.560</a></span> | <span class="t">So each one of these is for a single sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1311" target="_blank">00:21:51.520</a></span> | <span class="t">So here we have the first sentence here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1314" target="_blank">00:21:54.640</a></span> | <span class="t">And these are each of dimensionality 768.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1321" target="_blank">00:22:01.640</a></span> | <span class="t">So we can check the shape of that if we want as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1324" target="_blank">00:22:04.360</a></span> | <span class="t">just to confirm that is true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1327" target="_blank">00:22:07.200</a></span> | <span class="t">So you see that we have five embeddings, five sentences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1331" target="_blank">00:22:11.080</a></span> | <span class="t">each one 768 dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1333" target="_blank">00:22:13.960</a></span> | <span class="t">And now that we have our sentence embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1337" target="_blank">00:22:17.200</a></span> | <span class="t">we can use these to quickly compare sentence similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1340" target="_blank">00:22:20.920</a></span> | <span class="t">for quite a few different use cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1344" target="_blank">00:22:24.600</a></span> | <span class="t">The most popular are semantic textual similarity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1349" target="_blank">00:22:29.120</a></span> | <span class="t">which is a comparison of sentence pairs, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1351" target="_blank">00:22:31.480</a></span> | <span class="t">is what we are going to do here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1353" target="_blank">00:22:33.880</a></span> | <span class="t">And generally, this is probably most often</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1357" target="_blank">00:22:37.160</a></span> | <span class="t">used for benchmarking these kinds of models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1361" target="_blank">00:22:41.680</a></span> | <span class="t">Then we have semantic search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1363" target="_blank">00:22:43.120</a></span> | <span class="t">Now, we've covered semantic search a lot already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1366" target="_blank">00:22:46.800</a></span> | <span class="t">in other articles and videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1369" target="_blank">00:22:49.960</a></span> | <span class="t">And this is information retrieval</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1372" target="_blank">00:22:52.240</a></span> | <span class="t">using semantic meaning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1374" target="_blank">00:22:54.840</a></span> | <span class="t">So given a set of sentences, we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1377" target="_blank">00:22:57.600</a></span> | <span class="t">search using a query sentence and identify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1381" target="_blank">00:23:01.240</a></span> | <span class="t">the most similar records.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1383" target="_blank">00:23:03.200</a></span> | <span class="t">So this enables us to search based on concepts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1387" target="_blank">00:23:07.160</a></span> | <span class="t">rather than specific words, which is pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1391" target="_blank">00:23:11.520</a></span> | <span class="t">Now, we also have clustering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1392" target="_blank">00:23:12.760</a></span> | <span class="t">So we can cluster our sentences, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1394" target="_blank">00:23:14.680</a></span> | <span class="t">is obviously useful for things like topic modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1397" target="_blank">00:23:17.560</a></span> | <span class="t">Now, we can put together a very fast STS,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1401" target="_blank">00:23:21.400</a></span> | <span class="t">so the semantic textual similarity example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1406" target="_blank">00:23:26.240</a></span> | <span class="t">using nothing more than a cosine similarity function and NumPy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1410" target="_blank">00:23:30.320</a></span> | <span class="t">So we just want to import NumPy as np.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1413" target="_blank">00:23:33.720</a></span> | <span class="t">And we also are going to use the sentence transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1420" target="_blank">00:23:40.200</a></span> | <span class="t">cosine similarity function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1423" target="_blank">00:23:43.280</a></span> | <span class="t">So write sentence transformers util import cosim.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1430" target="_blank">00:23:50.260</a></span> | <span class="t">OK, so from there, what I'm going to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1432" target="_blank">00:23:52.680</a></span> | <span class="t">is I'm going to initialize a empty array of zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1439" target="_blank">00:23:59.040</a></span> | <span class="t">And I want that to be the length of our sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1441" target="_blank">00:24:01.760</a></span> | <span class="t">So I want it to be a 5 by 5 array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1445" target="_blank">00:24:05.560</a></span> | <span class="t">And what we're going to do is loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1449" target="_blank">00:24:09.600</a></span> | <span class="t">through all of these sentence embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1451" target="_blank">00:24:11.260</a></span> | <span class="t">we produced from SBIRT and compare those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1453" target="_blank">00:24:13.920</a></span> | <span class="t">with the cosine similarity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1456" target="_blank">00:24:16.320</a></span> | <span class="t">So we just want to write for i in a range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1459" target="_blank">00:24:19.640</a></span> | <span class="t">And we'll delete the sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1461" target="_blank">00:24:21.160</a></span> | <span class="t">And we just want to say the similarity function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1468" target="_blank">00:24:28.040</a></span> | <span class="t">or the similarity array from i to the end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1474" target="_blank">00:24:34.320</a></span> | <span class="t">on this specific column is going to equal to cosine sim.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1481" target="_blank">00:24:41.040</a></span> | <span class="t">And then here, we want to put our embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1482" target="_blank">00:24:42.880</a></span> | <span class="t">We just want the current session followed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1486" target="_blank">00:24:46.000</a></span> | <span class="t">by the other embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1492" target="_blank">00:24:52.020</a></span> | <span class="t">And that will populate our similarity array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1496" target="_blank">00:24:56.120</a></span> | <span class="t">So we can print that out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1497" target="_blank">00:24:57.780</a></span> | <span class="t">And we'll see, OK, so down the middle here--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1501" target="_blank">00:25:01.320</a></span> | <span class="t">so I've not populated these because these--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1506" target="_blank">00:25:06.680</a></span> | <span class="t">well, we've already got those pairs on this side of the array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1511" target="_blank">00:25:11.400</a></span> | <span class="t">You'll see we're going to visualize it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1513" target="_blank">00:25:13.160</a></span> | <span class="t">So that'll probably make more sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1515" target="_blank">00:25:15.400</a></span> | <span class="t">So let's do that now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1516" target="_blank">00:25:16.880</a></span> | <span class="t">So we're going to import matplotlib.pyplot as PLT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1522" target="_blank">00:25:22.800</a></span> | <span class="t">And we're also going to import seaborne.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1524" target="_blank">00:25:24.600</a></span> | <span class="t">This makes it a little bit easier and nicer as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1527" target="_blank">00:25:27.480</a></span> | <span class="t">And I'm just going to write sns.heatmap in here, sim.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1532" target="_blank">00:25:32.640</a></span> | <span class="t">And we want annotations set to true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1536" target="_blank">00:25:36.360</a></span> | <span class="t">OK, so this is just a visual of that array</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1543" target="_blank">00:25:43.440</a></span> | <span class="t">that we produced just now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1545" target="_blank">00:25:45.240</a></span> | <span class="t">And we can see here, so we have the sentence values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1551" target="_blank">00:25:51.040</a></span> | <span class="t">or sentence positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1553" target="_blank">00:25:53.120</a></span> | <span class="t">So sentence zero, if we go here--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1556" target="_blank">00:25:56.520</a></span> | <span class="t">actually, let me print them out here instead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1558" target="_blank">00:25:58.520</a></span> | <span class="t">That makes more sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1560" target="_blank">00:26:00.480</a></span> | <span class="t">So if I print sentences, maybe it's better if I--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1565" target="_blank">00:26:05.320</a></span> | <span class="t">so I'll put them like this, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1569" target="_blank">00:26:09.080</a></span> | <span class="t">So we have number zero is obviously this first sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1573" target="_blank">00:26:13.480</a></span> | <span class="t">And then we have number 1, 2, 3, and 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1578" target="_blank">00:26:18.720</a></span> | <span class="t">And that correlates to 0 to 4 and 0 to 4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1583" target="_blank">00:26:23.080</a></span> | <span class="t">here on the two axes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1585" target="_blank">00:26:25.200</a></span> | <span class="t">So if we want to look at the most similar pair according</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1589" target="_blank">00:26:29.800</a></span> | <span class="t">to our SBIRT model, it's this 4 and 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1592" target="_blank">00:26:32.960</a></span> | <span class="t">which gets a cosine similarity of 0.64.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1597" target="_blank">00:26:37.280</a></span> | <span class="t">And if we have a look, 3 and 4 are these two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1602" target="_blank">00:26:42.520</a></span> | <span class="t">So these two are the only ones that kind of mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1604" target="_blank">00:26:44.600</a></span> | <span class="t">the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1605" target="_blank">00:26:45.720</a></span> | <span class="t">And I've written these so that they basically carry--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1613" target="_blank">00:26:53.080</a></span> | <span class="t">they share none of the same descriptive words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1617" target="_blank">00:26:57.040</a></span> | <span class="t">So for dentists, we have dental specialists.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1620" target="_blank">00:27:00.280</a></span> | <span class="t">Chewing bricks, I put flossing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1622" target="_blank">00:27:02.800</a></span> | <span class="t">So not even the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1624" target="_blank">00:27:04.840</a></span> | <span class="t">And construction materials as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1626" target="_blank">00:27:06.600</a></span> | <span class="t">It's not even the same thing there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1628" target="_blank">00:27:08.680</a></span> | <span class="t">But very similar sort of concepts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1631" target="_blank">00:27:11.200</a></span> | <span class="t">that we're talking about there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1632" target="_blank">00:27:12.520</a></span> | <span class="t">So you can see that it's identifying those two as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1634" target="_blank">00:27:14.560</a></span> | <span class="t">most similar concept, which is pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1636" target="_blank">00:27:16.480</a></span> | <span class="t">And then we get some other similarity scores,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1638" target="_blank">00:27:18.880</a></span> | <span class="t">which are kind of high here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1640" target="_blank">00:27:20.080</a></span> | <span class="t">And they're not really related.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1641" target="_blank">00:27:21.480</a></span> | <span class="t">So we have 3 and 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1644" target="_blank">00:27:24.520</a></span> | <span class="t">It's talking about eggplants and mannequin heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1647" target="_blank">00:27:27.080</a></span> | <span class="t">So it's pretty different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1649" target="_blank">00:27:29.280</a></span> | <span class="t">I suppose, in reality, someone being an eggplant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1652" target="_blank">00:27:32.040</a></span> | <span class="t">and this sort of thing is both kind of weird, strange things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1657" target="_blank">00:27:37.280</a></span> | <span class="t">to happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1658" target="_blank">00:27:38.080</a></span> | <span class="t">So maybe that's why it's capturing similarity there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1661" target="_blank">00:27:41.040</a></span> | <span class="t">But it's not obviously similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1666" target="_blank">00:27:46.480</a></span> | <span class="t">So generally, I think it's good that it identifies this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1671" target="_blank">00:27:51.400</a></span> | <span class="t">as being the most similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1673" target="_blank">00:27:53.560</a></span> | <span class="t">But it could be better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1674" target="_blank">00:27:54.640</a></span> | <span class="t">And we do find that with the more recent models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1677" target="_blank">00:27:57.840</a></span> | <span class="t">it is, in fact, better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1680" target="_blank">00:28:00.000</a></span> | <span class="t">So what we can do is I'm going to get this other model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1683" target="_blank">00:28:03.960</a></span> | <span class="t">We'll just call it model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1685" target="_blank">00:28:05.120</a></span> | <span class="t">And we're going to do sentence transformer again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1690" target="_blank">00:28:10.360</a></span> | <span class="t">And this time, we are using the MPNet base model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1695" target="_blank">00:28:15.680</a></span> | <span class="t">Now, this is basically the highest performing model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1700" target="_blank">00:28:20.720</a></span> | <span class="t">at the moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1702" target="_blank">00:28:22.040</a></span> | <span class="t">Although I was told on the channel's Discord by Ashraq</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1710" target="_blank">00:28:30.120</a></span> | <span class="t">that there is actually, when they were training this MPNet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1713" target="_blank">00:28:33.760</a></span> | <span class="t">model, they also trained a Roberta model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1718" target="_blank">00:28:38.320</a></span> | <span class="t">And although the Roberta model is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1721" target="_blank">00:28:41.280</a></span> | <span class="t">shown on the Sentence Transformer's home page,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1724" target="_blank">00:28:44.800</a></span> | <span class="t">you can see in the competition where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1726" target="_blank">00:28:46.600</a></span> | <span class="t">they trained both these models that they did also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1728" target="_blank">00:28:48.960</a></span> | <span class="t">train that model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1730" target="_blank">00:28:50.400</a></span> | <span class="t">And it does have a slightly higher performance as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1734" target="_blank">00:28:54.520</a></span> | <span class="t">It might be slower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1735" target="_blank">00:28:55.680</a></span> | <span class="t">I'm not sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1736" target="_blank">00:28:56.200</a></span> | <span class="t">Probably because it's Roberta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1738" target="_blank">00:28:58.280</a></span> | <span class="t">But the performance of that is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1740" target="_blank">00:29:00.800</a></span> | <span class="t">higher than the MPNet version of that model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1744" target="_blank">00:29:04.920</a></span> | <span class="t">which is pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1746" target="_blank">00:29:06.200</a></span> | <span class="t">Now, here, we can see a few things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1749" target="_blank">00:29:09.760</a></span> | <span class="t">that are slightly different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1750" target="_blank">00:29:10.880</a></span> | <span class="t">For starters, the max sequence length is three times as long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1753" target="_blank">00:29:13.640</a></span> | <span class="t">as it was with the BERT model using the MPNet base model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1758" target="_blank">00:29:18.120</a></span> | <span class="t">And we also have this additional normalization function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1763" target="_blank">00:29:23.200</a></span> | <span class="t">Now, let's just take what we wrote up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1767" target="_blank">00:29:27.440</a></span> | <span class="t">So I'm just going to take this, bring it down here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1772" target="_blank">00:29:32.840</a></span> | <span class="t">And let's just use the heat map straight away.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1775" target="_blank">00:29:35.800</a></span> | <span class="t">So SNS heat map sim and annot equals true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1785" target="_blank">00:29:45.240</a></span> | <span class="t">And we'll see something slightly different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1787" target="_blank">00:29:47.480</a></span> | <span class="t">Or we should do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1789" target="_blank">00:29:49.880</a></span> | <span class="t">So I haven't processed the similarity yet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1793" target="_blank">00:29:53.680</a></span> | <span class="t">or the embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1794" target="_blank">00:29:54.920</a></span> | <span class="t">So let's write embeddings equals model.encode sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1801" target="_blank">00:30:01.920</a></span> | <span class="t">Now, if we run it, we'll see that the similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1806" target="_blank">00:30:06.400</a></span> | <span class="t">of these ones, of these other sentence pairs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1810" target="_blank">00:30:10.320</a></span> | <span class="t">is now a lot lower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1812" target="_blank">00:30:12.760</a></span> | <span class="t">But it's still identifying 4 and 3 as pretty similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1816" target="_blank">00:30:16.360</a></span> | <span class="t">So we can see straight away there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1817" target="_blank">00:30:17.960</a></span> | <span class="t">a decent performance increase from this MPNet model, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1821" target="_blank">00:30:21.800</a></span> | <span class="t">is the most recent model, and the original SBERT model here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1827" target="_blank">00:30:27.960</a></span> | <span class="t">So I think that's pretty cool to see as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1830" target="_blank">00:30:30.600</a></span> | <span class="t">Now, that's it for this model introducing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1833" target="_blank">00:30:33.120</a></span> | <span class="t">sentence embeddings and the Sentence Transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1836" target="_blank">00:30:36.520</a></span> | <span class="t">library and models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1838" target="_blank">00:30:38.000</a></span> | <span class="t">Now, going forward, obviously, this is a series of videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1841" target="_blank">00:30:41.720</a></span> | <span class="t">So we're going to be covering a lot more than just Sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1844" target="_blank">00:30:44.120</a></span> | <span class="t">Transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1845" target="_blank">00:30:45.120</a></span> | <span class="t">But next, we are actually going to cover</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1848" target="_blank">00:30:48.280</a></span> | <span class="t">how we can train a sentence BERT model, an SBERT model, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1854" target="_blank">00:30:54.200</a></span> | <span class="t">I think will be pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1856" target="_blank">00:30:56.400</a></span> | <span class="t">So until then, that's it for now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1859" target="_blank">00:30:59.840</a></span> | <span class="t">So thank you very much for watching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1861" target="_blank">00:31:01.800</a></span> | <span class="t">And I will see you in the next one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=WS1uVMGhlWQ&t=1864" target="_blank">00:31:04.000</a></span> | <span class="t">Bye.</span></div></div></body></html>