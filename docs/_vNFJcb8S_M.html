<html><head><title>Open Reasoning LLMS: Magistral + SmolLM 3</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Open Reasoning LLMS: Magistral + SmolLM 3</h2><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M"><img src="https://i.ytimg.com/vi/_vNFJcb8S_M/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./_vNFJcb8S_M.html">Whisper Transcript</a> | <a href="./transcript__vNFJcb8S_M.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">I actually want to share my entire screen for a second. So Magistral is kind of Mistral's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=7" target="_blank">00:00:07.520</a></span> | <span class="t">reasoning model. The interesting thing they do here is they don't want to do any distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=13" target="_blank">00:00:13.440</a></span> | <span class="t">Basically, they want to see how far can they get with like native RL without distilling from a big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=19" target="_blank">00:00:19.200</a></span> | <span class="t">model. And then they also do a little bit of distillation. So they train one, which is Magistral</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=24" target="_blank">00:00:24.400</a></span> | <span class="t">Medium. It's trained on Mistral Medium as the base model and they just do pure RL and they kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=31" target="_blank">00:00:31.600</a></span> | <span class="t">talk about how they do it, how they get the data set. The very interesting thing here is this is like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=37" target="_blank">00:00:37.040</a></span> | <span class="t">a very large scale RL run. So what we don't see is how do you actually efficiently do large scale RL?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=45" target="_blank">00:00:45.360</a></span> | <span class="t">So what are the challenges in that? And how do you set up your compute and data center to do this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=50" target="_blank">00:00:50.640</a></span> | <span class="t">Because with RL, you need like two instances of the model. You need to generate rollouts of your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=55" target="_blank">00:00:55.840</a></span> | <span class="t">thinking. You need to verify them, which takes some time. They'll have different leads, right? Like when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=61" target="_blank">00:01:01.360</a></span> | <span class="t">you have a rollout that's like 3000 tokens versus like 30,000, you kind of have a delta as they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=67" target="_blank">00:01:07.920</a></span> | <span class="t">being generated. Do you just waste time and waste GPU hours? And then as you like batch up, do your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=75" target="_blank">00:01:15.440</a></span> | <span class="t">gradient, um, as you do like changes in weights, um, how do you resync this stuff? So Mistral is like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=83" target="_blank">00:01:23.040</a></span> | <span class="t">you know, we actually have to be a real research lab since we've raised billions of dollars. Uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=88" target="_blank">00:01:28.480</a></span> | <span class="t">so let's do some research instead of just basic shit. And then they kind of outline like, okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=92" target="_blank">00:01:32.960</a></span> | <span class="t">here's how we do a really, really good training setup for all this stuff. So that's Magistral.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=99" target="_blank">00:01:39.520</a></span> | <span class="t">The other paper is SmallLM3. SmallLM3 just came out yesterday, I think, from HuggingFace.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=106" target="_blank">00:01:46.720</a></span> | <span class="t">It's kind of the opposite. It's like OML style, fully open, um, where they give you everything,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=113" target="_blank">00:01:53.440</a></span> | <span class="t">training code, full repo, dataset, um, model weights, checkpoints, base model, and struct model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=121" target="_blank">00:02:01.280</a></span> | <span class="t">And they kind of talk about distillation. So their, their dataset is distilled, but it's a small model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=128" target="_blank">00:02:08.320</a></span> | <span class="t">that they get to reason. And then of course they have their fun charts, you know, they're always in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=132" target="_blank">00:02:12.240</a></span> | <span class="t">the top left or top right corner where they want to be. And the interesting thing here is two cool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=138" target="_blank">00:02:18.960</a></span> | <span class="t">takeaways. One is this is a paper that talks about hybrid reasoning. So they have thinking and non-thinking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=146" target="_blank">00:02:26.160</a></span> | <span class="t">mode. And then there is a section on the bottom, which I thought was interesting where they talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=153" target="_blank">00:02:33.120</a></span> | <span class="t">about model merging. You don't see much about model merging, but, uh, spoiler when they, when they did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=160" target="_blank">00:02:40.080</a></span> | <span class="t">their model merging, what did they do this for again? So they, they have this kind of preference optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=165" target="_blank">00:02:45.920</a></span> | <span class="t">for reasoning and non-reasoning. And then as they start to do this training, uh, turns out that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=172" target="_blank">00:02:52.480</a></span> | <span class="t">kind of nerf out the long context capability. So what they did is they took some of this reasoning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=179" target="_blank">00:02:59.040</a></span> | <span class="t">non-reasoning checkpoint, and then they took some of the long context checkpoint that didn't get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=183" target="_blank">00:03:03.440</a></span> | <span class="t">screwed. And then boom, they just, uh, combined them together. And now we've solved both solutions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=189" target="_blank">00:03:09.040</a></span> | <span class="t">which is pretty crazy. I didn't know. I didn't know model merging can just do shit like that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=192" target="_blank">00:03:12.800</a></span> | <span class="t">but that's pretty cool. Um, so they do that, but then they talk about everything that goes into it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=199" target="_blank">00:03:19.040</a></span> | <span class="t">Um, also they're pre-training, post-training, but they use a lot of other data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=203" target="_blank">00:03:23.360</a></span> | <span class="t">Some stuff that I found very weird here was like when they generate synthetic SFT data, right? So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=210" target="_blank">00:03:30.640</a></span> | <span class="t">they're generating reasoning data sets. Uh, they, they, they use QEN 332B in reasoning mode. If anyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=220" target="_blank">00:03:40.800</a></span> | <span class="t">has intuition as to why they would do this instead of, my zoom is overlaying my tabs instead of, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=228" target="_blank">00:03:48.000</a></span> | <span class="t">instead of using the big QEN 3, I would be curious to know, cause you know, in QEN 3, we also have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=237" target="_blank">00:03:57.200</a></span> | <span class="t">this big 235B. So why not use 235B with active? Uh, they do use some deep seek, but yeah, interestingly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=247" target="_blank">00:04:07.760</a></span> | <span class="t">enough in their synthetic data gen they use, they use, um, 132B, but both very good papers. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=254" target="_blank">00:04:14.160</a></span> | <span class="t">they, they, this is actually a pretty, pretty expensive paper. If the answer is compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=260" target="_blank">00:04:20.720</a></span> | <span class="t">So in their blog post, they talk about how many GPU hours this is. So, uh, they're, they're pretty like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=268" target="_blank">00:04:28.240</a></span> | <span class="t">chunky. This is not a little research preview. Like this model is trained on 48 nodes of eight H100s for 24 days.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=276" target="_blank">00:04:36.160</a></span> | <span class="t">That's a 220k GPU hours. If you assume like $2 to $3 per GPU hour, um, that's roughly a million dollar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=286" target="_blank">00:04:46.000</a></span> | <span class="t">train run. Not to mention a bunch of ablations, testing, synthetic data gen, salaries, like resources</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=293" target="_blank">00:04:53.440</a></span> | <span class="t">that go into this. But the training alone, like this was trained on trillions of tokens, generating those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=299" target="_blank">00:04:59.200</a></span> | <span class="t">tokens. But the final train run, like what is this, uh, 220,000 H100 hours. That's, that's over a million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=307" target="_blank">00:05:07.280</a></span> | <span class="t">dollar just train run alone. So not, not cheap, not cheap. Um, but yeah, cool. It's, it's a small LM that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=313" target="_blank">00:05:13.360</a></span> | <span class="t">a hybrid reasoner. So this paper is about hybrid reasoning. Um, this paper is about native reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=319" target="_blank">00:05:19.920</a></span> | <span class="t">without distillation. Okay. I think I'll do this one first since it's, uh, there's more substance to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=325" target="_blank">00:05:25.440</a></span> | <span class="t">The other one you can kind of just read through. It's kind of a blog post. There's not much to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=329" target="_blank">00:05:29.280</a></span> | <span class="t">Um, okay. I will get started. If there's any questions, anything, you know, pop in. We'll just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=335" target="_blank">00:05:35.600</a></span> | <span class="t">interrupt me whenever it's not that deep. Okay. So Magistral, their, um, RL, they, they kind of really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=341" target="_blank">00:05:41.680</a></span> | <span class="t">just lay out their RL pipeline. Uh, the, one of them is open source. It's Apache 2, but they don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=349" target="_blank">00:05:49.040</a></span> | <span class="t">give us as much. They don't give us training data. They give us like numbers around how much data they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=354" target="_blank">00:05:54.480</a></span> | <span class="t">use, but they obviously don't give it all to us. But you know, this is still a pretty good paper for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=358" target="_blank">00:05:58.720</a></span> | <span class="t">Mistral. Like I'm, I'm pretty, I think they're going in the right direction, learning how to do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=363" target="_blank">00:06:03.760</a></span> | <span class="t">from scratch as opposed to some of the other work, but you know, they are bigger. So what do we expect?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=369" target="_blank">00:06:09.040</a></span> | <span class="t">Um, basically there's two things. So they want to show, oh, also the other takeaway of this is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=374" target="_blank">00:06:14.480</a></span> | <span class="t">RL on text alone works pretty well. So it's still like all this stuff generalizes. They do good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=381" target="_blank">00:06:21.440</a></span> | <span class="t">ablations in this. They text, they test, uh, if we just do code, does it work on code and math? If we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=386" target="_blank">00:06:26.240</a></span> | <span class="t">just do math, does it work on math and code? Um, but can we do pure RL on text without distillation on a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=392" target="_blank">00:06:32.240</a></span> | <span class="t">small model? Yes, we can. It's interesting what we think of small models, right? Because Mistral calls</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=397" target="_blank">00:06:37.200</a></span> | <span class="t">small models 24B. Uh, five, four called small models 14B. I guess they had a little 3B section</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=403" target="_blank">00:06:43.760</a></span> | <span class="t">in there with SFT and, um, hugging face calls small models, um, three or 4B, three or 4B. Okay. Uh, let me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=412" target="_blank">00:06:52.880</a></span> | <span class="t">make sure my screen share is sharing the right screen. It is. Okay. So, uh, basically they, they want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=420" target="_blank">00:07:00.960</a></span> | <span class="t">detail these few things. So one, you can do no distillation from existing models. Uh, they talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=427" target="_blank">00:07:07.520</a></span> | <span class="t">about the infrastructure, their design choices, which is kind of an interesting section, right? You don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=431" target="_blank">00:07:11.600</a></span> | <span class="t">really get anyone giving you this much detail about it. Hugging face gave us a little, but honestly, this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=436" target="_blank">00:07:16.480</a></span> | <span class="t">gave us a much better recipe. Um, then, uh, effective way to make it multilingual. So this was another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=443" target="_blank">00:07:23.840</a></span> | <span class="t">interesting little tidbit where they noticed that originally, uh, normally the model would start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=450" target="_blank">00:07:30.720</a></span> | <span class="t">reasoning or outputting its response in different languages. Uh, interestingly enough, as much as this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=456" target="_blank">00:07:36.800</a></span> | <span class="t">is Mistral, a French company, they, they didn't have French output. They had a lot of Chinese, Russian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=463" target="_blank">00:07:43.680</a></span> | <span class="t">and stuff, but they can basically adjust their RL policy. They, they basically do GRPO with some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=470" target="_blank">00:07:50.560</a></span> | <span class="t">modifications. One of those modifications is giving a slight reward increase for models that have, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=477" target="_blank">00:07:57.760</a></span> | <span class="t">that keep their reasoning and output in, in the same language that it's guided. And then they even add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=484" target="_blank">00:08:04.800</a></span> | <span class="t">this in the system prompt. It's like pretty interesting how you can just solve this problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=489" target="_blank">00:08:09.360</a></span> | <span class="t">of reasoning and outputting different languages by a little 0.1 increase in RL, uh, reward. Like it just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=496" target="_blank">00:08:16.160</a></span> | <span class="t">shows how strong RL is, right? You just give slight, slight reward if you keep your language consistent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=500" target="_blank">00:08:20.720</a></span> | <span class="t">and guess what? Language is now consistent, but, um, you know, interesting note there. Um, other stuff in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=506" target="_blank">00:08:26.960</a></span> | <span class="t">the, that are like little takeaways before we go deep that I noticed, um, towards the end of this paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=514" target="_blank">00:08:34.320</a></span> | <span class="t">they, they noticed something about multimodal that we might not get time to cover. So, um, eating the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=522" target="_blank">00:08:42.000</a></span> | <span class="t">multimodal free lunch, basically, uh, all they did was they trained on a pre-trained model that has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=529" target="_blank">00:08:49.360</a></span> | <span class="t">multimodality, right? So mistral three small and medium, they're multimodal models that have vision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=534" target="_blank">00:08:54.720</a></span> | <span class="t">encoders, right? So they encode images into the same embedding latent space as text. Then during the RL,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=542" target="_blank">00:09:02.560</a></span> | <span class="t">they're only trained on text data. So they're like, our intuition is that one might expect that multimodal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=548" target="_blank">00:09:08.320</a></span> | <span class="t">performance would degrade, right? But no, actually, uh, it didn't degrade multimodal actually got even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=554" target="_blank">00:09:14.480</a></span> | <span class="t">better. So testing on stuff like MMMU and MMMU Pro, there's no regression in most benchmarks and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=563" target="_blank">00:09:23.120</a></span> | <span class="t">notable improvements in some. So, um, just since like the models were natively multimodal, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=571" target="_blank">00:09:31.360</a></span> | <span class="t">this reasoning sort of transferred over the model transfers, it's extended thinking across all, all,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=577" target="_blank">00:09:37.120</a></span> | <span class="t">um, modalities. Very interesting. And then this is part of their future work directions. Uh, some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=583" target="_blank">00:09:43.520</a></span> | <span class="t">the questions that they should work on later, uh, looking ahead, we're also excited to push the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=589" target="_blank">00:09:49.120</a></span> | <span class="t">boundaries of RL across whole range of applications, including tool use, um, integrated multimodality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=595" target="_blank">00:09:55.840</a></span> | <span class="t">and agents. So they want to do multimodal reasoning down the line, um, and agents and tool use. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=601" target="_blank">00:10:01.840</a></span> | <span class="t">interesting little takeaway, right? Without, um, without any multimodal data, they were able to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=608" target="_blank">00:10:08.720</a></span> | <span class="t">better multimodal preference just by having, um, text reasoning because it's all in the same latent space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=616" target="_blank">00:10:16.400</a></span> | <span class="t">in the end of the day. Then of course, um, they, they give, um, weights small model as open source.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=623" target="_blank">00:10:23.920</a></span> | <span class="t">Then they, there was basically like prior to this, there was this kind of notion of for small models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=630" target="_blank">00:10:30.080</a></span> | <span class="t">it's not worth doing RL. You would want to always have a big model and distill and they want to challenge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=636" target="_blank">00:10:36.720</a></span> | <span class="t">that claim. So they want to see whether RL can improve on distillation, SFT baseline of small models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=643" target="_blank">00:10:43.760</a></span> | <span class="t">So basically we show that, um, yes, RL is very competitive in some cases better, but harder to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=650" target="_blank">00:10:50.720</a></span> | <span class="t">do. Um, for those that don't stick around through the whole thing, I guess the other takeaway with this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=655" target="_blank">00:10:55.760</a></span> | <span class="t">model is how much data filtration they do. So like a distinction between the two papers, um, small LLM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=662" target="_blank">00:11:02.720</a></span> | <span class="t">uses a lot of preexisting data sets. Mistral basically parses through and data filters everything,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=669" target="_blank">00:11:09.520</a></span> | <span class="t">everything. So I only have, um, purple highlighting for data set stuff, but basically for math and code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=678" target="_blank">00:11:18.960</a></span> | <span class="t">is this just math? Yeah. So for the, for the math data set, they start with 700,000 samples. They have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=685" target="_blank">00:11:25.600</a></span> | <span class="t">like two stages of filtering we're getting to. Getting rid of formatting stuff, they cut down 100,000. In the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=691" target="_blank">00:11:31.440</a></span> | <span class="t">end with their difficulty filtering, they only have 5% of the data left. So they cut from 700,000 to 38,000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=698" target="_blank">00:11:38.400</a></span> | <span class="t">They cut 95% out. Same thing with code. Um, how much do they start with? I don't know how much they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=703" target="_blank">00:11:43.840</a></span> | <span class="t">start with, but they cut a lot. They only use 35k code samples. So Mistral basically like, I don't know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=709" target="_blank">00:11:49.200</a></span> | <span class="t">if all this stuff transfers over like their claims or it's just, Hey, we can do really, really good data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=715" target="_blank">00:11:55.600</a></span> | <span class="t">set filtration, but okay. Enough, enough background yapping. Let's get into these papers. Um, I don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=721" target="_blank">00:12:01.840</a></span> | <span class="t">really like covering benchmarks in my paper yaps. Cause you know, you can read a number always go up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=727" target="_blank">00:12:07.920</a></span> | <span class="t">Benchmarks are a scam. Um, interesting stuff compared to DeepSeq. They don't need to do a cold start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=734" target="_blank">00:12:14.720</a></span> | <span class="t">Cold start is basically before your RL, you take a reasoning chain of thought traces, you do an SFT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=741" target="_blank">00:12:21.040</a></span> | <span class="t">So the model starts to understand reasoning structure. Then you do RL. Um, forget that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=745" target="_blank">00:12:25.760</a></span> | <span class="t">We just do, we just do straight RL. It works. Um, isn't QN3, 4B fully open source to wait? Yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=754" target="_blank">00:12:34.160</a></span> | <span class="t">quite an open source, but we don't have training code. We don't have the data set. We don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=758" target="_blank">00:12:38.560</a></span> | <span class="t">all the ablations. There's a lot of that stuff. You know, uh, we need an LLM to show benchmarks for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=763" target="_blank">00:12:43.280</a></span> | <span class="t">models that are not shown because they don't look favorable. Yes. I think that's interesting. So like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=769" target="_blank">00:12:49.920</a></span> | <span class="t">my takeaway here as well, um, they, they, they look at like three Bs, four Bs, but you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=777" target="_blank">00:12:57.600</a></span> | <span class="t">I want to see some more, I want to see seven Bs, eight Bs. I want to see the other ones that aren't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=781" target="_blank">00:13:01.520</a></span> | <span class="t">on here. I want to see the 0.5 Bs. Um, yeah, that's, that's always useful. Okay. So RL, infra,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=790" target="_blank">00:13:10.320</a></span> | <span class="t">performance. This is basically all the sections. So guess what? RL is back. They do GRPO. Uh, GRPO for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=797" target="_blank">00:13:17.600</a></span> | <span class="t">those that don't know is group relative policy optimization. It's basically what deep seat said</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=803" target="_blank">00:13:23.520</a></span> | <span class="t">we can do instead of all this fancy, um, reward modeling, like policy modeling training, like good,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=810" target="_blank">00:13:30.400</a></span> | <span class="t">all that stuff. What we basically do is we just generate a bunch of samples and then we kind of, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=816" target="_blank">00:13:36.560</a></span> | <span class="t">reward based on what does best out of this like goal of optimization. So use the average award for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=822" target="_blank">00:13:42.400</a></span> | <span class="t">multiple generations per prompt from the policy to compute a baseline for advantage calculation. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=827" target="_blank">00:13:47.680</a></span> | <span class="t">generate a bunch of samples, group, uh, group relative. So what's relatively the better outcome</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=834" target="_blank">00:13:54.240</a></span> | <span class="t">in that group and then reward for that. Here's GRPO in fancy math definitions. I think it's pretty useful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=841" target="_blank">00:14:01.520</a></span> | <span class="t">like send this section and like screenshot this into chat GPT or Claude, I guess, uh, I used to GPT and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=848" target="_blank">00:14:08.160</a></span> | <span class="t">just have it explained through math formula. It's kind of useful to follow through. All these are like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=853" target="_blank">00:14:13.280</a></span> | <span class="t">pretty easy functions. Once you understand what they all do, there's KL divergence penalties,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=858" target="_blank">00:14:18.480</a></span> | <span class="t">all that stuff. They kind of get rid of that. So here's what they did. Here are the modifications to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=863" target="_blank">00:14:23.200</a></span> | <span class="t">GRPO that they do. One, get rid of KL divergence. Um, KL divergence penalty constrains it from deviating too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=871" target="_blank">00:14:31.120</a></span> | <span class="t">far from a reference policy. So basically we have KL divergence, so we don't, we don't like venture too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=877" target="_blank">00:14:37.120</a></span> | <span class="t">far and kind of just generate noise. They find that with, um, GRPO this kind of happens anyway. So they get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=884" target="_blank">00:14:44.880</a></span> | <span class="t">rid of it because KL computation incurs compute costs that's unjustified, right? Um, get rid of that for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=891" target="_blank">00:14:51.840</a></span> | <span class="t">inference like training efficiency, not training efficiency, just efficiency of GPU usage. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=898" target="_blank">00:14:58.160</a></span> | <span class="t">Uh, loss norm to avoid length bias. They have a loss normalization. So basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=905" target="_blank">00:15:05.680</a></span> | <span class="t">they normalize basically on an average length generation. So you're not penalized for being too short or too long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=913" target="_blank">00:15:13.280</a></span> | <span class="t">Uh, makes sense. Advantage normalization as you would expect. Okay. Uh, relaxing the trust region upper bound.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=921" target="_blank">00:15:21.760</a></span> | <span class="t">This is basically allowing again for more, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=924" target="_blank">00:15:24.800</a></span> | <span class="t">more variability and allowing for more exploration. They do a lot of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=930" target="_blank">00:15:30.480</a></span> | <span class="t">Eliminating non-diverse groups. So, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=934" target="_blank">00:15:34.240</a></span> | <span class="t">we filter out all groups with zero advantage. Basically if there's outputs where there's like no significant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=941" target="_blank">00:15:41.760</a></span> | <span class="t">difference. If all answers are correct or all are wrong, we just filter them out and then net the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=947" target="_blank">00:15:47.360</a></span> | <span class="t">reward to zero. Uh, reward shaping during training, dah, dah, dah, dah. Oh, okay. So that's, that's kind of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=955" target="_blank">00:15:55.040</a></span> | <span class="t">those are the four or five main changes to GRPO. Um, then it comes to reward shipping. How do they do this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=961" target="_blank">00:16:01.520</a></span> | <span class="t">Okay. So we have, uh, formatting. So choosing the appropriate reward is very crucial when you do RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=969" target="_blank">00:16:09.360</a></span> | <span class="t">So we want our stuff to be verifiable, right? All the, all the training they do is with verifiable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=974" target="_blank">00:16:14.880</a></span> | <span class="t">reward. So how do we verify that the output is correct? One, we have think tags and they make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=980" target="_blank">00:16:20.240</a></span> | <span class="t">sure that there's exactly one set of think tags and then they look at the thinking in the middle. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=985" target="_blank">00:16:25.280</a></span> | <span class="t">your thinking should be between think and close think, and you must start with a think tag for math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=991" target="_blank">00:16:31.360</a></span> | <span class="t">You must end your answer in a box. Um, this follows the think tag for code. Uh, you must have one bark</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=999" target="_blank">00:16:39.040</a></span> | <span class="t">down markdown block followed by the language specification. And then, um, you know, if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1005" target="_blank">00:16:45.520</a></span> | <span class="t">don't do any of these, so if you're formatting is wrong, instant reward zero, you're cooked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1009" target="_blank">00:16:49.920</a></span> | <span class="t">Otherwise you get slight reward preference. So you learn to use it. Okay. Um, that's formatting. They</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1015" target="_blank">00:16:55.600</a></span> | <span class="t">give the whole system prompt and they talk about it quite a bit. Okay. Correctness. So math correctness,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1021" target="_blank">00:17:01.280</a></span> | <span class="t">they use a rule-based verifier reward of 0.9 is given. If the answer is correct, making the total</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1026" target="_blank">00:17:06.960</a></span> | <span class="t">reward one, uh, as you would expect, this is the biggest portion of the reward, right? If your answer is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1032" target="_blank">00:17:12.800</a></span> | <span class="t">correct, you're good. Uh, if your answer is in the right format, you'll have slightly more reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1038" target="_blank">00:17:18.880</a></span> | <span class="t">If it's in the wrong format, you're completely cooked it to zero. Code correctness, same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1043" target="_blank">00:17:23.360</a></span> | <span class="t">Uh, they, they check if it can compile with a timeout of 10 seconds. They, they randomly, so they,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1049" target="_blank">00:17:29.440</a></span> | <span class="t">they talk about this later in the dataset selection. Uh, they want code that has tests. If it doesn't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1056" target="_blank">00:17:36.240</a></span> | <span class="t">tests, they make tests. If it has tests that seem wrong, they fix the test. They randomly select,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1062" target="_blank">00:17:42.800</a></span> | <span class="t">um, tests that are available to test it. And then, you know, if correct, you get reward. Basic RL,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1069" target="_blank">00:17:49.840</a></span> | <span class="t">right? Um, if you're, so, so far we're at two things. Uh, formatting is a non-negotiable. If your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1076" target="_blank">00:17:56.560</a></span> | <span class="t">formatting is wrong, your reward is directly zero. If it's correct, you get a slight reward. Answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1081" target="_blank">00:18:01.440</a></span> | <span class="t">correctness. If your answer is correct, you get big reward. This is kind of the main thing. Okay. Length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1086" target="_blank">00:18:06.240</a></span> | <span class="t">penalty. They have slow, uh, they have soft length penalties. And as you would expect,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1090" target="_blank">00:18:10.800</a></span> | <span class="t">there are penalties. You're out of distribution with length, you're cooked. Okay. Language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1095" target="_blank">00:18:15.200</a></span> | <span class="t">consistency. This is one of those key points that they wanted to mention, right? So, um, one of the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1101" target="_blank">00:18:21.200</a></span> | <span class="t">one of the main things they did here is we present a simple yet effective strategy to make the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1106" target="_blank">00:18:26.240</a></span> | <span class="t">multilingual, where both the chain of thought and the final response are in the user language. So, how do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1110" target="_blank">00:18:30.640</a></span> | <span class="t">they do this? Um, duh, duh, duh, duh, duh. Okay. So, a core design principle is for to reason in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1117" target="_blank">00:18:37.280</a></span> | <span class="t">same language as the user. We frequently observed outputs mixed in English, Chinese, and Russian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1122" target="_blank">00:18:42.800</a></span> | <span class="t">Uh, they were coherent, but they're not what we want, right? They're undesirable. If I ask something in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1128" target="_blank">00:18:48.560</a></span> | <span class="t">Chinese, I don't want it to think in Russian. If I ask for English, I still want the thought traces,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1133" target="_blank">00:18:53.680</a></span> | <span class="t">and I want to be able to read them, and I want them in English, right? I don't want them in Chinese.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1137" target="_blank">00:18:57.280</a></span> | <span class="t">So, to prevent this, um, they translated their problems into different languages. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1144" target="_blank">00:19:04.080</a></span> | <span class="t">they take 10% of the data set, translate it into French, Spanish, Italian, German, Chinese, Russian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1149" target="_blank">00:19:09.680</a></span> | <span class="t">Then they calculate the, when calculating the reward, they, they have a classifier that basically checks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1156" target="_blank">00:19:16.080</a></span> | <span class="t">if all three parts are, um, in the same language. Three parts being problem, thought, and answer. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1163" target="_blank">00:19:23.600</a></span> | <span class="t">does your problem, thought, and answer match its same language? They check this by, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1168" target="_blank">00:19:28.560</a></span> | <span class="t">doing, you know, normalization or moving latex code blocks. Then they do a fast text classifier. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1174" target="_blank">00:19:34.400</a></span> | <span class="t">oh, shit. Um, they just check, you know, is all, are all three in the correct language? If they are,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1180" target="_blank">00:19:40.640</a></span> | <span class="t">you get like a 0.1 reward bonus, I think. Let's double check that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1188" target="_blank">00:19:48.320</a></span> | <span class="t">da, da, da, da, da. Classifier. Where is it? Yeah. So, um, if the classifier indicates that all three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1194" target="_blank">00:19:54.560</a></span> | <span class="t">parts are in the same language, so the problem, the thinking, and the answer, you get a little bit more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1198" target="_blank">00:19:58.880</a></span> | <span class="t">reward. Similar to if your answer is boxed, you got a little bit more reward. Now, this is not as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1204" target="_blank">00:20:04.080</a></span> | <span class="t">impactful as getting the right answer, but it is slight reward. And with enough data, um, the model starts to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1210" target="_blank">00:20:10.640</a></span> | <span class="t">understand that, you know, I'll get rewarded for staying consistent and that just solves the problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1215" target="_blank">00:20:15.840</a></span> | <span class="t">So simple solution solves reasoning and thinking in the right language. Um, little note here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1221" target="_blank">00:20:21.120</a></span> | <span class="t">they only translated English problems. They didn't translate like French problems to English. They</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1226" target="_blank">00:20:26.640</a></span> | <span class="t">didn't do a crazy mixture. Very simple solution. Just take 10% of your English, translate it to a few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1230" target="_blank">00:20:30.800</a></span> | <span class="t">languages. The model has generalized that if it stays consistent, it gets rewarded. Now it stays consistent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1237" target="_blank">00:20:37.040</a></span> | <span class="t">Very straightforward, simple solution, but elegant and works. And this is kind of power of RL, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1242" target="_blank">00:20:42.240</a></span> | <span class="t">Little reward for doing what you want and you don't need to go crazy and it just works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1246" target="_blank">00:20:46.080</a></span> | <span class="t">If this was like traditional pre-training or SFT, you would have to do all the languages and all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1252" target="_blank">00:20:52.240</a></span> | <span class="t">mixtures and keep it consistent. And like, you know, traditional ML, you have to like have good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1257" target="_blank">00:20:57.760</a></span> | <span class="t">distribution spread, but no, it's RL. Just give little reward for consistency and it generalizes to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1263" target="_blank">00:21:03.520</a></span> | <span class="t">be inconsistent. RL very strong. Okay. System prompt. Um, they, they know, does this cause a drop in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1270" target="_blank">00:21:10.880</a></span> | <span class="t">reasoning performance like R1? No, it doesn't. Um, reasoning performance stays, uh, performance stays</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1276" target="_blank">00:21:16.560</a></span> | <span class="t">consistent. I think they have this in section six in the ablation. I'll, I'll bring it up later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1282" target="_blank">00:21:22.720</a></span> | <span class="t">if you can remind me. Okay. Next section, system prompt. Um, they find that RL is quite sensitive to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1288" target="_blank">00:21:28.720</a></span> | <span class="t">their system prompt. And then I think their system prompt is a little too overfit, but what do I know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1293" target="_blank">00:21:33.920</a></span> | <span class="t">I'm not mistral. So they add stuff like be as casual as you want. This increases entropy, meaning it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1301" target="_blank">00:21:41.360</a></span> | <span class="t">allowed to explore a little bit more and it proves, yeah, it improves exploration. Here is the system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1306" target="_blank">00:21:46.720</a></span> | <span class="t">prompt. So, um, you know, your thinking process must follow the, the think, the template, think your thoughts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1313" target="_blank">00:21:53.680</a></span> | <span class="t">then provide a concise summary of your reasoning and the final answer. Uh, oh, B is casual or causal?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1322" target="_blank">00:22:02.640</a></span> | <span class="t">Oh, causal. I see. I see. I see. I see. Um, okay. You're so a user. It's casual. It's casual. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1328" target="_blank">00:22:08.960</a></span> | <span class="t">casual. You're actually right. I just, I couldn't believe it. I think causal is better. Casual just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1334" target="_blank">00:22:14.480</a></span> | <span class="t">means be chill bro. You know, be chill bro. But yeah, be chill made it reason better. But like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1340" target="_blank">00:22:20.240</a></span> | <span class="t">it's interesting if that's what you want and not even just that, like here, they even ask you, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1345" target="_blank">00:22:25.760</a></span> | <span class="t">they, they remind it to, what is it? Write your thoughts in the same language. Right? So we thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1351" target="_blank">00:22:31.840</a></span> | <span class="t">RL was enough to tell it to just like, okay, stick in reasoning, stick in your language. No, we must also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1359" target="_blank">00:22:39.120</a></span> | <span class="t">prompt it again. Uh, you know, keep your thoughts in the same language and then yeah, be casual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1364" target="_blank">00:22:44.400</a></span> | <span class="t">That's cool. So, okay. Um, a user will ask you to solve a task. You should first draft your thinking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1371" target="_blank">00:22:51.120</a></span> | <span class="t">process, your inner monologue. It tells it that it's an inner monologue. I think, uh, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1375" target="_blank">00:22:55.920</a></span> | <span class="t">in the long scheme of things on billions of tokens, do we need to remind it that thinking is an inner</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1381" target="_blank">00:23:01.520</a></span> | <span class="t">monologue? I think not, but I'm clearly not a prompt engineer like them. Uh, until you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1387" target="_blank">00:23:07.200</a></span> | <span class="t">drawn the final answer afterwards, write a summary of your thoughts. So yeah, then, you know, I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1392" target="_blank">00:23:12.640</a></span> | <span class="t">we should actually read the whole thing. I shouldn't skip through this. You should use markdown and latex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1396" target="_blank">00:23:16.560</a></span> | <span class="t">to format your response. Your thinking should be in this think tags, your thoughts or draft. Also, I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1402" target="_blank">00:23:22.160</a></span> | <span class="t">thought this is interesting. I don't know if this is just European. Uh, normally we see and slash or not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1407" target="_blank">00:23:27.520</a></span> | <span class="t">or slash and, but you know, this model has seen or slash and much more. So interesting little stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1414" target="_blank">00:23:34.400</a></span> | <span class="t">B is casual and as long as you want until you are confident to generate a correct answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1419" target="_blank">00:23:39.840</a></span> | <span class="t">I thought this is a bit much, but what do I know? Okay. After thinking. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1426" target="_blank">00:23:46.800</a></span> | <span class="t">Yeah. I, I think like the way they put problem call in and then stuff, I find it like weird. Shouldn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1434" target="_blank">00:23:54.320</a></span> | <span class="t">the model learn that the problem comes after the tag for the user role?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1438" target="_blank">00:23:58.000</a></span> | <span class="t">Student. Uh, I mean, this is just pretty common. I don't, I don't think it's crazy. Most, most system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1445" target="_blank">00:24:05.520</a></span> | <span class="t">prompts have something like this. Um, I guess it could not be problem. It could be like useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1450" target="_blank">00:24:10.240</a></span> | <span class="t">assistant, user call in great, but I don't know. This is what they do. The other one uses chat ML, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1455" target="_blank">00:24:15.440</a></span> | <span class="t">more standard prompt format, but I didn't dwell as much there. If anyone else has thoughts, let's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1462" target="_blank">00:24:22.560</a></span> | <span class="t">let's discuss or just take it to chat. Okay. Um, sorry, I'm going to go a little quick because this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1468" target="_blank">00:24:28.080</a></span> | <span class="t">is quite a long paper. There's still, there's still fun stuff in here. Uh, same, same system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1472" target="_blank">00:24:32.560</a></span> | <span class="t">prompt for math and coding. Uh, the other interesting thing note here is their reasoning is only math</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1477" target="_blank">00:24:37.200</a></span> | <span class="t">and coding. Um, yeah, that's, that's what it is. Um, but it's all text. Okay. So infrastructure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1486" target="_blank">00:24:46.160</a></span> | <span class="t">this is a very fun section that I don't think any other paper really covers. Here's how to do like very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1491" target="_blank">00:24:51.360</a></span> | <span class="t">large scale, um, RL on a lot of GPUs. Sorry. I've clicked a citation. Let me come back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1499" target="_blank">00:24:59.520</a></span> | <span class="t">Okay. RL. So, um, we adopt the distributed RL training similar to prior works. You should read</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1509" target="_blank">00:25:09.520</a></span> | <span class="t">these works if you're interested. That coordinates three types of workers. So trainers, trainers maintain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1514" target="_blank">00:25:14.800</a></span> | <span class="t">copy of all the model weights and perform gradient updates. Generators are kind of doing these rollouts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1520" target="_blank">00:25:20.320</a></span> | <span class="t">that use the latest policy to return completions with log probs from the training prompt. So basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1526" target="_blank">00:25:26.080</a></span> | <span class="t">prompt generators are doing a bunch of rollouts for GRPO. Here's, you know, 10 outputs. Generators</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1532" target="_blank">00:25:32.800</a></span> | <span class="t">are doing inference. Trainers are doing, uh, they're, they're keeping the model weights and doing, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1538" target="_blank">00:25:38.080</a></span> | <span class="t">gradient outputs. Verifiers. Verifiers are the ones verifying the output. So this is also not like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1544" target="_blank">00:25:44.000</a></span> | <span class="t">you know, this is like that, um, the classifier that checks for output, checks for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1549" target="_blank">00:25:49.280</a></span> | <span class="t">consistency, checks if code compiles, checks if test pass, checks if the output is boxed. There are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1555" target="_blank">00:25:55.040</a></span> | <span class="t">verifiers as well. So these are kind of the three kinds of workers that they need all in sync. And this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1560" target="_blank">00:26:00.320</a></span> | <span class="t">is not like little, you know, okay, I'm running this on like one node. I'm okay with inefficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1564" target="_blank">00:26:04.880</a></span> | <span class="t">This is like, I'm training on tens of billions of tokens. And I don't want to waste like multiple,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1571" target="_blank">00:26:11.360</a></span> | <span class="t">multiple nodes of, um, very expensive GPUs. So how do we optimize all this? It's very interesting. They're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1577" target="_blank">00:26:17.600</a></span> | <span class="t">completely fine with stuff being like out of sync and shit just generalizes. Okay. So challenges with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1583" target="_blank">00:26:23.040</a></span> | <span class="t">distributed RL. Generators are a significant part of the total compute and the part that's unique to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1588" target="_blank">00:26:28.320</a></span> | <span class="t">online RL, right? Online RL being that the model is kind of doing this self-play, right? It's generating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1593" target="_blank">00:26:33.520</a></span> | <span class="t">and then it's being, it's being graded on the output. So you're actually doing a lot of inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1599" target="_blank">00:26:39.760</a></span> | <span class="t">Generators are kind of like the inference boxes, right? Uh, their workload is highly heterogeneous and hard to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1606" target="_blank">00:26:46.080</a></span> | <span class="t">predict as the distribution of sequence lengths is highly skewed, right? Some outputs might be short and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1612" target="_blank">00:26:52.480</a></span> | <span class="t">concise. Some generators might be very long and then they do have stuff in RL policy that, um, has like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1619" target="_blank">00:26:59.120</a></span> | <span class="t">length penalty. They have stuff to keep it within a certain distribution, but then you also want it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1626" target="_blank">00:27:06.640</a></span> | <span class="t">explore long lengths. But point being, um, these, these things are skewed, right? So if you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1633" target="_blank">00:27:13.840</a></span> | <span class="t">some stuff that's waiting for an extra 20,000 tokens to be generated, are you just stalling the generator or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1640" target="_blank">00:27:20.000</a></span> | <span class="t">are you throwing this in some batch? How are you verifying stuff as it comes in? Um, so one of the main</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1646" target="_blank">00:27:26.640</a></span> | <span class="t">components inter is to introduce no bias on the sequence length, right? You can't do this all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1651" target="_blank">00:27:31.920</a></span> | <span class="t">sequentially because you'll have so much delay. So how do we do this? Um, a more competing goal is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1658" target="_blank">00:27:38.320</a></span> | <span class="t">update the generator weights as soon as possible, right? So as soon as one inference is done, just do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1663" target="_blank">00:27:43.680</a></span> | <span class="t">another one. And we want the generators to be as on policy as possible, but we want them to operate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1669" target="_blank">00:27:49.040</a></span> | <span class="t">without waiting for trainers. So you don't want to wait. You want to be on, on policy kind of weird</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1674" target="_blank">00:27:54.880</a></span> | <span class="t">since you're keep, so it's like this kind of lag thing. I think I should have drawn a diagram for this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1679" target="_blank">00:27:59.520</a></span> | <span class="t">but GG. Um, so what they do is async generators. Basically we process batches sequentially. You start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1688" target="_blank">00:28:08.000</a></span> | <span class="t">the generators on a batch, wait for all sequences to complete, update the model weights. No, sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1694" target="_blank">00:28:14.080</a></span> | <span class="t">This is not what they do. This is what you could do. And this is why it's slow. So what you would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1698" target="_blank">00:28:18.640</a></span> | <span class="t">expect is, okay, basically you batch out all inferences. So do all your rollouts, wait for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1707" target="_blank">00:28:27.200</a></span> | <span class="t">them all to complete, update all model weights on both the trainers and generators, and then repeat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1713" target="_blank">00:28:33.200</a></span> | <span class="t">So here's kind of the lag. As the first generator is done, as your first rollout that short finishes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1719" target="_blank">00:28:39.280</a></span> | <span class="t">you're now waiting for, that's kind of sitting idle for the other rollouts to complete. Once they're all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1724" target="_blank">00:28:44.560</a></span> | <span class="t">done, we send them all to verifiers to verify the output. Generators are all sitting idle. And of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1732" target="_blank">00:28:52.000</a></span> | <span class="t">course, the trainers are all just storing weights, doing nothing. Once the verification is done, then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1737" target="_blank">00:28:57.120</a></span> | <span class="t">do all the back prop, check what might weights to change. Then we update the trainers. After trainers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1741" target="_blank">00:29:01.920</a></span> | <span class="t">are updated, then we have to change those weights back to generators, even more lag. So we have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1747" target="_blank">00:29:07.600</a></span> | <span class="t">re-update the weights on generators. Then we do all this again sequentially. Very, very bad. You have idle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1754" target="_blank">00:29:14.400</a></span> | <span class="t">generators, low pipeline efficiency. This is fine for a little bit of post training. It's not that deep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1759" target="_blank">00:29:19.680</a></span> | <span class="t">So for an entire train run, you're cooked. You're like, it's not efficient. They want to be efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1765" target="_blank">00:29:25.760</a></span> | <span class="t">So we operate generators continuously at maximum throughput without even waiting for trainers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1771" target="_blank">00:29:31.200</a></span> | <span class="t">Basically, this means you're always doing rollouts and you'll always gather small groups from generators,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1777" target="_blank">00:29:37.760</a></span> | <span class="t">verify them, update trainers. After these updates, trainers send new weights via NCCL. This is like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1785" target="_blank">00:29:45.280</a></span> | <span class="t">interconnect InfiniBand GPU to GPU weight updates and they don't even disregard in-flight sequences that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1793" target="_blank">00:29:53.760</a></span> | <span class="t">are being generated. So there's like this push of always keep rollouts happening and then always update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1800" target="_blank">00:30:00.640</a></span> | <span class="t">those weights. Even if you're off policy and you're using old weights, just keep using them and you might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1806" target="_blank">00:30:06.640</a></span> | <span class="t">be slightly off. Your KVs, your KV caches, so like your old previously generated tokens will be on old</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1813" target="_blank">00:30:13.440</a></span> | <span class="t">policy, but it's okay. It doesn't matter. Just use it. It doesn't make much of a difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1818" target="_blank">00:30:18.320</a></span> | <span class="t">I think everyone should like really read into this section quite a bit. If you care about distributed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1824" target="_blank">00:30:24.640</a></span> | <span class="t">like large-scale RL training, it's a very elegant solution that just works that they tried.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1829" target="_blank">00:30:29.600</a></span> | <span class="t">Yeah. So they just kind of keep stuff always happening. Here's kind of a, here's kind of a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1835" target="_blank">00:30:35.520</a></span> | <span class="t">diagram of what's happening here. So generators are rollouts, right? So you generate a bunch of sequences of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1841" target="_blank">00:30:41.040</a></span> | <span class="t">different lengths. This one very long, this one very short. Do they open source the code? No,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1847" target="_blank">00:30:47.360</a></span> | <span class="t">they don't. They don't open source shit. They don't even tell us what data this is trained on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1851" target="_blank">00:30:51.280</a></span> | <span class="t">but small LLM does, but they don't do this same fancy stuff. Okay. And they actually don't do native</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1855" target="_blank">00:30:55.840</a></span> | <span class="t">RL from scratch. They're doing distribution. They're doing distillation, but actually that's not as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1862" target="_blank">00:31:02.000</a></span> | <span class="t">relevant. Anyway, so long output, short output. Generators just keep shitting out generations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1868" target="_blank">00:31:08.160</a></span> | <span class="t">Verifiers check when they can. Here are kind of batches that happen and you compute your back prop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1875" target="_blank">00:31:15.120</a></span> | <span class="t">and your weight updates. You do this on the fly as soon as these little mini batches get filled. Then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1881" target="_blank">00:31:21.920</a></span> | <span class="t">with Interconnect and FiniBand, you update these generator weights. And since they're still chunking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1887" target="_blank">00:31:27.360</a></span> | <span class="t">out stuff, you might have stuff that's a little bit off policy from previous steps, previous weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1892" target="_blank">00:31:32.320</a></span> | <span class="t">but it's okay. Just keep it running and it'll all work out in the end. And very efficient,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1897" target="_blank">00:31:37.760</a></span> | <span class="t">fast pipeline of one to four. Let's read through this again, since I think this is interesting. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1904" target="_blank">00:31:44.000</a></span> | <span class="t">generators continuously output completions from prompts. So your generators are always doing rollouts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1910" target="_blank">00:31:50.880</a></span> | <span class="t">When a completion is finished, it's sent to a verifier. So check if it's box, check the output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1919" target="_blank">00:31:59.600</a></span> | <span class="t">run a code compile test. If it passes or if it doesn't, you do your step three, send it to a batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1927" target="_blank">00:32:07.920</a></span> | <span class="t">for updates. Each sequence is sent to a different data parallel group using all this stuff. Then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1935" target="_blank">00:32:15.040</a></span> | <span class="t">do gradient steps. So you do your GRPO. You check what's best on policy. Then this is very interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1942" target="_blank">00:32:22.320</a></span> | <span class="t">Wait to replace mid-generation, which means that in-flight generations continue with a slightly outdated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1950" target="_blank">00:32:30.080</a></span> | <span class="t">KV cache and we do not refresh it. If I could just keep going. Sense model resides on both GPUs and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1957" target="_blank">00:32:37.440</a></span> | <span class="t">trainers and generators. You basically have all the weights. You have to double up the weights, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1962" target="_blank">00:32:42.640</a></span> | <span class="t">So are all very expensive. It's not just like hold weights, do next token prediction. You need two sets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1968" target="_blank">00:32:48.080</a></span> | <span class="t">of weights actually held in memory. So expensive GPU shit. But since you have all the weights on your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1974" target="_blank">00:32:54.400</a></span> | <span class="t">trainers and your generators, you can use NCCL, which is like CUDA InfiniBand GPU to GPU transfer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1982" target="_blank">00:33:02.320</a></span> | <span class="t">which is much faster than like most other data transfer shits. You can use that. It's very fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1990" target="_blank">00:33:10.640</a></span> | <span class="t">That works. Okay. As a solution is generated for a single prompt, it may experience multiple updates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=1997" target="_blank">00:33:17.280</a></span> | <span class="t">from the weights. So even in a single prompt, as you're doing next token generation, you'll have multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2002" target="_blank">00:33:22.720</a></span> | <span class="t">weight updates to that model reflecting latest time improvements. Very, very, very, very fast stuff. I think I'm too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2010" target="_blank">00:33:30.400</a></span> | <span class="t">hyped on this training pipeline. So I'll just continue in case people are getting lost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2015" target="_blank">00:33:35.280</a></span> | <span class="t">Yup. That's that section. I think if you care more, read section three of this paper, follow up in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2023" target="_blank">00:33:43.680</a></span> | <span class="t">Discord. Okay. Data curation. They only want to use verifiable solutions. Oh, actually, I'll take a pause.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2030" target="_blank">00:33:50.560</a></span> | <span class="t">Any questions on this stuff? Or should we move on? Okay. No questions. I hope people followed. I like that section.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2039" target="_blank">00:33:59.120</a></span> | <span class="t">Never mind. No question. Okay. Data curation. So data curation, we, they only want to use verifiable solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2055" target="_blank">00:34:15.280</a></span> | <span class="t">Oh, people, sorry. Can I ask a quick question? Yes, yes, yes. Did they mention what the, like the sort of utilization level they got on their GPUs? No, no, no. They don't talk about any of this. I think actually there's, I don't know if I'm blanking. There might be,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2058" target="_blank">00:34:18.480</a></span> | <span class="t">uh, a note that I'm not remembering it, but I'm like 80% sure they don't. But, uh, it seems like the generators are a hundred percent utilization. Verifiers will never be full utilization. And a lot of that doesn't have to even be on GPU. Cause you have to wait for sequences, even with your batching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2076" target="_blank">00:34:36.480</a></span> | <span class="t">Yeah. Um, trainers doing backprop. That's just efficiency of how well they updated this GRPO stuff. Um, but no, they, they don't talk about this level of efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2096" target="_blank">00:34:56.480</a></span> | <span class="t">Somebody in the chat also asked about learning rate, uh, or open source code for the distributed RL or the two questions I think related to this section.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2104" target="_blank">00:35:04.480</a></span> | <span class="t">Yeah. I answered the, the code. No, Mistral doesn't give any of it, but the small LLM paper does. They give it all. They give all the checkpoints, all the, not all the checkpoints. They give some checkpoints. They give base model, instruct model, training data, all that stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2119" target="_blank">00:35:19.480</a></span> | <span class="t">Good resource to learn a more distributed RL training. Yeah. I have a, I have a link of papers I would recommend. I'll, I'll share it in discord. Just remind me cause I'll probably forget.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2129" target="_blank">00:35:29.480</a></span> | <span class="t">But, um, that's the fun thing about this paper, but let's, let's just spend the next five minutes covering the rest of it pretty quick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2137" target="_blank">00:35:37.480</a></span> | <span class="t">Okay. So, um, they want verifiable stuff. They basically want math and code. Ooh, I didn't know I could do this. Very fun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2144" target="_blank">00:35:44.480</a></span> | <span class="t">Um, they do a lot of filtration. Here's how they filter. So this is something I brought up earlier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2150" target="_blank">00:35:50.480</a></span> | <span class="t">Math started with 700,000 samples. Uh, they cut 95%. This is actually pretty interesting. They train a model with RL to do filtration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2160" target="_blank">00:36:00.480</a></span> | <span class="t">That's the TDR of this. So as you would expect, they get rid of the basic shit, right? So filter out stuff that has wrong answers filter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2168" target="_blank">00:36:08.480</a></span> | <span class="t">Uh, they do a little bit of rewriting. So, uh, final answers are verifiable with a rule based system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2174" target="_blank">00:36:14.480</a></span> | <span class="t">They want to filter proof based multi-part problems where it's difficult to verify, uh, correctness. So get rid of shit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2180" target="_blank">00:36:20.480</a></span> | <span class="t">That's hard to verify. They reformulate multiple choice into statement based problems for more robust verification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2186" target="_blank">00:36:26.480</a></span> | <span class="t">So some rewriting, uh, two stage of filtration process. So first, um, they want stuff that's like Goldilocks difficulty, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2195" target="_blank">00:36:35.480</a></span> | <span class="t">Not too easy, not too hard. So if Mr. Large can answer all of it, um, throw it away. If it can't, can't answer any of it, throw it away again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2204" target="_blank">00:36:44.480</a></span> | <span class="t">So sample 16 problems, 16 solutions for each problem, removing the ones that are either never solved or solved with high success rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2213" target="_blank">00:36:53.480</a></span> | <span class="t">Um, then this initial set they use to train a 24 B model with this RL pipeline, getting a small model that's better than Mistral too large, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2223" target="_blank">00:37:03.480</a></span> | <span class="t">So step one, use your best model, filter out shit. Step two, do a little bit of RL on a model to make it good at math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2232" target="_blank">00:37:12.480</a></span> | <span class="t">Then in the second stage, use this model to once again, answer the shit that was hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2237" target="_blank">00:37:17.480</a></span> | <span class="t">Uh, well answer everything filter, just do inference, right? Get rid of the easy stuff again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2242" target="_blank">00:37:22.480</a></span> | <span class="t">Then we filter out potentially incorrect problems that have a majority of samples with the same final answer of a degree on the ground of a disagree.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2250" target="_blank">00:37:30.480</a></span> | <span class="t">Um, so you can kind of bring back some stuff that was hard, but not hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2255" target="_blank">00:37:35.480</a></span> | <span class="t">This two stage is good because, um, a single pass with the initial weaker model would have been insufficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2263" target="_blank">00:37:43.480</a></span> | <span class="t">The reasoning capabilities would have caused it to discard many difficult problems that were incorrect, but this new middle model can kind of solve them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2273" target="_blank">00:37:53.480</a></span> | <span class="t">So kind of interesting, right? Use your big model, get rid of everything easy and hard. Train a reasoning model, use the reasoning model again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2280" target="_blank">00:38:00.480</a></span> | <span class="t">You can bring back some hard stuff that your hard model couldn't solve. Now you have more data, but holy shit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2286" target="_blank">00:38:06.480</a></span> | <span class="t">They, they filtered a lot. They got rid of 95% of the stuff. Uh, code same, same beans, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2292" target="_blank">00:38:12.480</a></span> | <span class="t">So we want them to be, um, a large number of correct tests. First remove stuff without solutions and without enough tests.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2300" target="_blank">00:38:20.480</a></span> | <span class="t">This was kind of the interesting thing. Each solution is then tested. Uh, we disregard tests with insufficient, we disregard tests with insufficient agreement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2310" target="_blank">00:38:30.480</a></span> | <span class="t">Um, then, uh, yeah, this two paragraph is a ton of work, right? You have to train a whole middle model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2317" target="_blank">00:38:37.480</a></span> | <span class="t">And I mean, you get rid of 95% of your data, kind of crazy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2321" target="_blank">00:38:41.480</a></span> | <span class="t">Um, on code, we want the, okay, what do we do for tests with sufficient agreement, but no successful solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2329" target="_blank">00:38:49.480</a></span> | <span class="t">We assume that the test is incorrect and updated. That's just some like next level shit, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2334" target="_blank">00:38:54.480</a></span> | <span class="t">I don't know if it's that next level, but basically your test is wrong. Fuck you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2338" target="_blank">00:38:58.480</a></span> | <span class="t">We'll make our, we'll rewrite our test. Uh, in cases where the code lacks tests, we just generate tests.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2344" target="_blank">00:39:04.480</a></span> | <span class="t">If we are confident we can generate tests. This is like, this paragraph is also a good bit of work, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2349" target="_blank">00:39:09.480</a></span> | <span class="t">Uh, finally, we, where applicable problem statements are duplicated to Python and C++.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2354" target="_blank">00:39:14.480</a></span> | <span class="t">Then we have 35 K sets. Here's where they're like, Hey, we can't tell you everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2358" target="_blank">00:39:18.480</a></span> | <span class="t">We don't tell you how many we started with. Uh, we just end with, um, with 35 K.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2364" target="_blank">00:39:24.480</a></span> | <span class="t">Okay. Experimentation. Uh, that's, that's data section. I think it's pretty interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2368" target="_blank">00:39:28.480</a></span> | <span class="t">The math stuff is pretty cool. Okay. How far can one get with pure RL given a strong teacher model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2374" target="_blank">00:39:34.480</a></span> | <span class="t">Um, very basic diagram, right? So data filtering, uh, for math, phase one, phase two, phase three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2383" target="_blank">00:39:43.480</a></span> | <span class="t">This is kind of useless for code. Just do that training overview.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2387" target="_blank">00:39:47.480</a></span> | <span class="t">So for MISTO3 medium for magistral medium, it's just pure RL. For MISTO3 small, you do some SFT on rollout.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2395" target="_blank">00:39:55.480</a></span> | <span class="t">Then you do RL and then you've got magic wand over here. Here's kind of their RL stages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2400" target="_blank">00:40:00.480</a></span> | <span class="t">Um, length plateaus, increased completion length in the middle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2404" target="_blank">00:40:04.480</a></span> | <span class="t">So this is like more into that RL policy itself, right? They do multi-stages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2408" target="_blank">00:40:08.480</a></span> | <span class="t">So, uh, start with low length, add more length, add more challenging data, um, as you start to hit the limit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2415" target="_blank">00:40:15.480</a></span> | <span class="t">So it's like similar to pre-training, post-training where the analogy you can think of is for context length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2421" target="_blank">00:40:21.480</a></span> | <span class="t">First you do the majority of your training at like 4K context, then you do higher context, then you do higher context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2427" target="_blank">00:40:27.480</a></span> | <span class="t">Or for post-training, you do the majority of your SFT on regular data. Then you do a little bit of hard math and reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2435" target="_blank">00:40:35.480</a></span> | <span class="t">Then you do SFT like the last 5% on all of your like super hard data. They do the same shit with RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2442" target="_blank">00:40:42.480</a></span> | <span class="t">Um, duh, duh, duh, duh, duh. We, oh, for all their eval, um, they do temperature 0.7.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2449" target="_blank">00:40:49.480</a></span> | <span class="t">Top-P of one, which is basically include all output distribution for math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2454" target="_blank">00:40:54.480</a></span> | <span class="t">And then for math, top-P of one. So include all your outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2458" target="_blank">00:40:58.480</a></span> | <span class="t">For GPQA, top-P of 0.95, which means you only use the, the like top 500 roughly, uh, output tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2468" target="_blank">00:41:08.480</a></span> | <span class="t">Um, maximum length for 4DK evals are good. I guess this, maybe I read a little bit more into this multi-stage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2475" target="_blank">00:41:15.480</a></span> | <span class="t">I don't know if they talked about it. I thought they did, but oh, here they do, they do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2478" target="_blank">00:41:18.480</a></span> | <span class="t">Um, okay. Evals. I don't care about evals on this, uh, model evals are fake news.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2484" target="_blank">00:41:24.480</a></span> | <span class="t">You should really care about your, um, your other, your other evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2489" target="_blank">00:41:29.480</a></span> | <span class="t">Okay. Your, your actual system evals. Okay. Training a model without cold start problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2494" target="_blank">00:41:34.480</a></span> | <span class="t">This is basically what every, this is what DC DeepSeek said. You need cold start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2498" target="_blank">00:41:38.480</a></span> | <span class="t">Turns out that's fake news. Um, Fi showed that you can, you can do other stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2503" target="_blank">00:41:43.480</a></span> | <span class="t">This also shows you cannot do other stuff. So, uh, as model performance increases,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2508" target="_blank">00:41:48.480</a></span> | <span class="t">we add harder data. Crazy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2510" target="_blank">00:41:50.480</a></span> | <span class="t">Harder data splits are in, are constructed by one more complicated data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2515" target="_blank">00:41:55.480</a></span> | <span class="t">which was filtered out earlier stage. So we saw that two stage of math, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2519" target="_blank">00:41:59.480</a></span> | <span class="t">Basically that second stage where they use their smart model to generate stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2524" target="_blank">00:42:04.480</a></span> | <span class="t">that the Mr. Large couldn't solve, but RL model could solve that's used in the second stage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2529" target="_blank">00:42:09.480</a></span> | <span class="t">or just completely removing, um, or completely remove stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2533" target="_blank">00:42:13.480</a></span> | <span class="t">Uh, length doesn't stop growing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2535" target="_blank">00:42:15.480</a></span> | <span class="t">We increase allowed completion length and maximal completion length over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2539" target="_blank">00:42:19.480</a></span> | <span class="t">Uh, this is in their RL penalty.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2542" target="_blank">00:42:22.480</a></span> | <span class="t">So they have a RL penalty for length skewing of being too far out of distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2547" target="_blank">00:42:27.480</a></span> | <span class="t">As training progresses, they start to, um, they start to increase this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2552" target="_blank">00:42:32.480</a></span> | <span class="t">This is all on magistral medium. So just pure RL from, uh, SFT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2556" target="_blank">00:42:36.480</a></span> | <span class="t">So basically from a useful assistant that went from next token predictor to useful chat bot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2562" target="_blank">00:42:42.480</a></span> | <span class="t">um, pure RL, they, they start to add harder and harder data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2567" target="_blank">00:42:47.480</a></span> | <span class="t">They start to allow for longer output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2569" target="_blank">00:42:49.480</a></span> | <span class="t">Um, generally as generation length increases, the memory associated with KV cache increases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2575" target="_blank">00:42:55.480</a></span> | <span class="t">So we scale down the total number of concurrent requests.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2578" target="_blank">00:42:58.480</a></span> | <span class="t">Uh, this is something about RJ or like efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2581" target="_blank">00:43:01.480</a></span> | <span class="t">They, they need to do better, um, better RL stuff, but they don't talk much about the training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2587" target="_blank">00:43:07.480</a></span> | <span class="t">I think I'm just inferring a lot from their stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2589" target="_blank">00:43:09.480</a></span> | <span class="t">I could be completely fake using this and being wrong, but that's my interpretation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2593" target="_blank">00:43:13.480</a></span> | <span class="t">Then the fun one, the Apache 2, um, RL with SFT bootstrapping.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2597" target="_blank">00:43:17.480</a></span> | <span class="t">So, um, they do a cold start on SFT traces from magistral medium.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2601" target="_blank">00:43:21.480</a></span> | <span class="t">So they take their big model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2603" target="_blank">00:43:23.480</a></span> | <span class="t">That's a reasoner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2604" target="_blank">00:43:24.480</a></span> | <span class="t">They don't use deep seek.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2605" target="_blank">00:43:25.480</a></span> | <span class="t">They, they, they pass through, um, they basically pass through traces of, um, problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2615" target="_blank">00:43:35.480</a></span> | <span class="t">They keep a mix of regular, uh, difficulty, dah, dah, dah, dah, avoid biasing towards easier problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2623" target="_blank">00:43:43.480</a></span> | <span class="t">So mix of easy, hard, um, early chain of thought.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2627" target="_blank">00:43:47.480</a></span> | <span class="t">Then they, they do SFT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2628" target="_blank">00:43:48.480</a></span> | <span class="t">So generate responses from reasoning model on a diverse set of prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2632" target="_blank">00:43:52.480</a></span> | <span class="t">They use, um, open data sets here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2634" target="_blank">00:43:54.480</a></span> | <span class="t">They use open thoughts and a code subset of open R1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2638" target="_blank">00:43:58.480</a></span> | <span class="t">Uh, this gives us a reasoning data set with mixed difficulty.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2642" target="_blank">00:44:02.480</a></span> | <span class="t">We also include 10% data points of general instruction tuning to preserve non-reasoning capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2648" target="_blank">00:44:08.480</a></span> | <span class="t">They do four epochs of training of how many samples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2653" target="_blank">00:44:13.480</a></span> | <span class="t">They tell us how many samples and that's somewhere here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2655" target="_blank">00:44:15.480</a></span> | <span class="t">Then we use this SFT checkpoint with RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2660" target="_blank">00:44:20.480</a></span> | <span class="t">They do their RL thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2662" target="_blank">00:44:22.480</a></span> | <span class="t">Now that knows how to kind of do, um, do this reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2667" target="_blank">00:44:27.480</a></span> | <span class="t">Where'd it go?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2668" target="_blank">00:44:28.480</a></span> | <span class="t">Uh, temperature of this, dah, dah, dah, dah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2671" target="_blank">00:44:31.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2672" target="_blank">00:44:32.480</a></span> | <span class="t">Benchmarks of bullshit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2673" target="_blank">00:44:33.480</a></span> | <span class="t">Um, benchmarks exist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2675" target="_blank">00:44:35.480</a></span> | <span class="t">Multilingual benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2676" target="_blank">00:44:36.480</a></span> | <span class="t">This was my, uh, what is this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2680" target="_blank">00:44:40.480</a></span> | <span class="t">We see that the model performs lower on multilingual compared to English.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2684" target="_blank">00:44:44.480</a></span> | <span class="t">Probably because we constrained English in the language of reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2689" target="_blank">00:44:49.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2690" target="_blank">00:44:50.480</a></span> | <span class="t">It's similar to that of a base model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2692" target="_blank">00:44:52.480</a></span> | <span class="t">I guess someone did bring up multi, multi link.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2694" target="_blank">00:44:54.480</a></span> | <span class="t">No, someone brought up multimodal, not multilingual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2696" target="_blank">00:44:56.480</a></span> | <span class="t">But yeah, slightly worse multimodal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2698" target="_blank">00:44:58.480</a></span> | <span class="t">It's good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2699" target="_blank">00:44:59.480</a></span> | <span class="t">Uh, multilingual slightly cooked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2700" target="_blank">00:45:00.480</a></span> | <span class="t">Uh, I don't want to spend much time on benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2703" target="_blank">00:45:03.480</a></span> | <span class="t">So I will skip this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2704" target="_blank">00:45:04.480</a></span> | <span class="t">I don't think many people use these models, but very good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2708" target="_blank">00:45:08.480</a></span> | <span class="t">Very good paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2709" target="_blank">00:45:09.480</a></span> | <span class="t">Uh, sorry for skipping, but it's good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2711" target="_blank">00:45:11.480</a></span> | <span class="t">Abilation is pretty fun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2713" target="_blank">00:45:13.480</a></span> | <span class="t">Um, cross domain generalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2716" target="_blank">00:45:16.480</a></span> | <span class="t">I talked about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2717" target="_blank">00:45:17.480</a></span> | <span class="t">So if you only do RL on math, can it do good on code?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2721" target="_blank">00:45:21.480</a></span> | <span class="t">Yes, it can.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2722" target="_blank">00:45:22.480</a></span> | <span class="t">If you only do RL on, um, code, can it do good on math?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2728" target="_blank">00:45:28.480</a></span> | <span class="t">Yes, it can.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2729" target="_blank">00:45:29.480</a></span> | <span class="t">So baseline was 32 on math, 22 on code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2732" target="_blank">00:45:32.480</a></span> | <span class="t">Only do RL on math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2734" target="_blank">00:45:34.480</a></span> | <span class="t">Guess what?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2735" target="_blank">00:45:35.480</a></span> | <span class="t">Code goes up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2736" target="_blank">00:45:36.480</a></span> | <span class="t">Uh, baseline was, I'm cooked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2739" target="_blank">00:45:39.480</a></span> | <span class="t">Baseline was 32 on math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2741" target="_blank">00:45:41.480</a></span> | <span class="t">Only do RL on code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2743" target="_blank">00:45:43.480</a></span> | <span class="t">Guess what?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2744" target="_blank">00:45:44.480</a></span> | <span class="t">Math goes up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2745" target="_blank">00:45:45.480</a></span> | <span class="t">Um, so shit kind of generalizes, but we only have two styles of reasoning verifiable data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2751" target="_blank">00:45:51.480</a></span> | <span class="t">So I don't know how much this matters, but it's cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2755" target="_blank">00:45:55.480</a></span> | <span class="t">I guess it matters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2756" target="_blank">00:45:56.480</a></span> | <span class="t">I disagree.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2757" target="_blank">00:45:57.480</a></span> | <span class="t">It matters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2758" target="_blank">00:45:58.480</a></span> | <span class="t">Um, what else?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2760" target="_blank">00:46:00.480</a></span> | <span class="t">Are Mistral 3 small with pure RL achieve similar performance?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2764" target="_blank">00:46:04.480</a></span> | <span class="t">Uh, as the distilled version suggests that benefits of RL are not exclusive to larger base models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2770" target="_blank">00:46:10.480</a></span> | <span class="t">This is one of those four points that they make.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2772" target="_blank">00:46:12.480</a></span> | <span class="t">Um, we contribute insights to contradict existing formulas that, um, are,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2780" target="_blank">00:46:20.480</a></span> | <span class="t">whether RL can improve upon distillation of SFT baseline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2785" target="_blank">00:46:25.480</a></span> | <span class="t">So if you remember for DeepSeq, what they also put out was distills.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2789" target="_blank">00:46:29.480</a></span> | <span class="t">They used like actual logic based distillation laws and they distilled SFT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2796" target="_blank">00:46:36.480</a></span> | <span class="t">Um, they, they distilled the models on reasoning rollouts with SFT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2804" target="_blank">00:46:44.480</a></span> | <span class="t">And it had very, very good performance updates, uh, performance benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2809" target="_blank">00:46:49.480</a></span> | <span class="t">They're like, nah, fuck that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2810" target="_blank">00:46:50.480</a></span> | <span class="t">Our RL actually works pretty good too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2812" target="_blank">00:46:52.480</a></span> | <span class="t">And this shows that they did a Mistral 3 small with pure RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2816" target="_blank">00:46:56.480</a></span> | <span class="t">So everything we talked about here was Mistral 3 small with SFT as a cold start then RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2825" target="_blank">00:47:05.480</a></span> | <span class="t">But no, they actually trained one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2826" target="_blank">00:47:06.480</a></span> | <span class="t">That's also pure RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2828" target="_blank">00:47:08.480</a></span> | <span class="t">It's pretty good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2829" target="_blank">00:47:09.480</a></span> | <span class="t">Um, actually, yeah, I think you're right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2831" target="_blank">00:47:11.480</a></span> | <span class="t">I don't think DeepSeq was logic based distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2834" target="_blank">00:47:14.480</a></span> | <span class="t">I think, uh, I don't think they could do that since that's not the same class family.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2838" target="_blank">00:47:18.480</a></span> | <span class="t">It was just SFT based distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2840" target="_blank">00:47:20.480</a></span> | <span class="t">My bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2841" target="_blank">00:47:21.480</a></span> | <span class="t">Um, but, uh, they're probably talking about the Quen, Quen logic based distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2847" target="_blank">00:47:27.480</a></span> | <span class="t">So same, same point, just Quen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2850" target="_blank">00:47:30.480</a></span> | <span class="t">Uh, DeepSeq was just SFT rollouts, but Quen should be logic based because Quen has same family.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2856" target="_blank">00:47:36.480</a></span> | <span class="t">But I could also be wrong there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2858" target="_blank">00:47:38.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2859" target="_blank">00:47:39.480</a></span> | <span class="t">Uh, batching stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2861" target="_blank">00:47:41.480</a></span> | <span class="t">Skip it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2862" target="_blank">00:47:42.480</a></span> | <span class="t">Not enough time, but if you're interested, read it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2864" target="_blank">00:47:44.480</a></span> | <span class="t">Analysis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2865" target="_blank">00:47:45.480</a></span> | <span class="t">Um, increasing completion length is the main resource that improves performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2870" target="_blank">00:47:50.480</a></span> | <span class="t">This is kind of interesting, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2872" target="_blank">00:47:52.480</a></span> | <span class="t">So you must have longer, uh, completion length over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2876" target="_blank">00:47:56.480</a></span> | <span class="t">Uh, the multimodal thing that I talked about for those that missed it was very interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2880" target="_blank">00:48:00.480</a></span> | <span class="t">That kind of, is this, did I skip this free lunch?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2883" target="_blank">00:48:03.480</a></span> | <span class="t">Free lunch on multimodal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2884" target="_blank">00:48:04.480</a></span> | <span class="t">Um, yeah, that was interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2888" target="_blank">00:48:08.480</a></span> | <span class="t">Uh, low dimension space, uh, more fine grade rewards and code tasks, completion, dah, dah, dah, dah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2894" target="_blank">00:48:14.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2895" target="_blank">00:48:15.480</a></span> | <span class="t">Um, a length direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2897" target="_blank">00:48:17.480</a></span> | <span class="t">So they do some PCA on outputs and find that length wants to go up, which is good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2903" target="_blank">00:48:23.480</a></span> | <span class="t">I think you can read that on your own.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2905" target="_blank">00:48:25.480</a></span> | <span class="t">Uh, multimodal, I'll bring this up again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2907" target="_blank">00:48:27.480</a></span> | <span class="t">So models are natively multimodal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2910" target="_blank">00:48:30.480</a></span> | <span class="t">The base models that they train on, they have a vision encoder that encodes images in the same latent space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2918" target="_blank">00:48:38.480</a></span> | <span class="t">Reasoning happens only on text, but it's all in the same latent space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2922" target="_blank">00:48:42.480</a></span> | <span class="t">So one might expect that the performance of multimodal would degrade, but no, not only does it remain good, but it actually gets better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2929" target="_blank">00:48:49.480</a></span> | <span class="t">So mmMU, mmMU pro, uh, provision, all these go up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2935" target="_blank">00:48:55.480</a></span> | <span class="t">Very, very cool stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2937" target="_blank">00:48:57.480</a></span> | <span class="t">Um, what other impacts?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2939" target="_blank">00:48:59.480</a></span> | <span class="t">Uh, function calling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2940" target="_blank">00:49:00.480</a></span> | <span class="t">The thing maintains and improves tool calling and instruction use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2944" target="_blank">00:49:04.480</a></span> | <span class="t">Um, partial reward for code data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2947" target="_blank">00:49:07.480</a></span> | <span class="t">This was an interesting one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2948" target="_blank">00:49:08.480</a></span> | <span class="t">So unsuccessful approaches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2950" target="_blank">00:49:10.480</a></span> | <span class="t">Uh, basically you need verification on output, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2953" target="_blank">00:49:13.480</a></span> | <span class="t">So they thought, okay, what if, uh, we do partial stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2957" target="_blank">00:49:17.480</a></span> | <span class="t">So what if we can't verify the output and we want to give it a slight reward bonus for stuff where it's on the right path, but it gets the wrong output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2965" target="_blank">00:49:25.480</a></span> | <span class="t">So basically if you start thinking, uh, you fuck up at the end, but you're on the right path, can we give you a little bit of reward?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2972" target="_blank">00:49:32.480</a></span> | <span class="t">No, we can't because that starts to give it false signals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2976" target="_blank">00:49:36.480</a></span> | <span class="t">Basically like, let's say you're thinking about how to answer a problem, like a math question, and you have good reasoning traces and you're following the stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2984" target="_blank">00:49:44.480</a></span> | <span class="t">But if you like, forget a theorem, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2986" target="_blank">00:49:46.480</a></span> | <span class="t">Like let's say you forget central limit theorem and you just go down this complete opposite path, or you forget you can't divide by zero, or you forget like how to plot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2994" target="_blank">00:49:54.480</a></span> | <span class="t">how to plot an exponential and you're, you're doing everything right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=2997" target="_blank">00:49:57.480</a></span> | <span class="t">You're thinking your reasoning, but you're on the complete wrong path.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3000" target="_blank">00:50:00.480</a></span> | <span class="t">You're giving false signals and these are incorrect solutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3003" target="_blank">00:50:03.480</a></span> | <span class="t">And, uh, yeah, it's actually worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3005" target="_blank">00:50:05.480</a></span> | <span class="t">So for little proportional rewards, which would be great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3008" target="_blank">00:50:08.480</a></span> | <span class="t">That, that gives us a bunch more training data, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3011" target="_blank">00:50:11.480</a></span> | <span class="t">If I don't have to use verifiable data, if I can still get little outputs from proportional rewards, uh, that would be huge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3017" target="_blank">00:50:17.480</a></span> | <span class="t">Cause we already have to cut 95% of our data, but it turns out this didn't work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3021" target="_blank">00:50:21.480</a></span> | <span class="t">I'm a little bit more bullish on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3023" target="_blank">00:50:23.480</a></span> | <span class="t">I think that their approach to GRPO and how much reward they gave and the filtration, like, I think you need a better verifier on this and you can squeeze out some more performance, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3035" target="_blank">00:50:35.480</a></span> | <span class="t">Uh, but then you need a good verifier on this and that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3039" target="_blank">00:50:39.480</a></span> | <span class="t">So I think there's still stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3040" target="_blank">00:50:40.480</a></span> | <span class="t">I think in the future we'll have like partial output verification stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3045" target="_blank">00:50:45.480</a></span> | <span class="t">You can quote me on this, uh, soon, but you need very good voter verifiers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3049" target="_blank">00:50:49.480</a></span> | <span class="t">Uh, I don't think that the, but like, it's the right approach, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3052" target="_blank">00:50:52.480</a></span> | <span class="t">You, you basically gave it a false signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3054" target="_blank">00:50:54.480</a></span> | <span class="t">You're cooked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3055" target="_blank">00:50:55.480</a></span> | <span class="t">Uh, entropy targeting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3056" target="_blank">00:50:56.480</a></span> | <span class="t">You want it to be exploratory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3058" target="_blank">00:50:58.480</a></span> | <span class="t">Um, RL model using open source reasoning traces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3063" target="_blank">00:51:03.480</a></span> | <span class="t">So they use, um, open thoughts and code subset, both improve prompt generalization, generations, da, da, da, da, da.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3073" target="_blank">00:51:13.480</a></span> | <span class="t">Applying RL is good over SFT checkpoint.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3076" target="_blank">00:51:16.480</a></span> | <span class="t">Good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3077" target="_blank">00:51:17.480</a></span> | <span class="t">Good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3078" target="_blank">00:51:18.480</a></span> | <span class="t">Good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3078" target="_blank">00:51:18.480</a></span> | <span class="t">You can do pure RL without SFT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3080" target="_blank">00:51:20.480</a></span> | <span class="t">What else?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3081" target="_blank">00:51:21.480</a></span> | <span class="t">Conclusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3082" target="_blank">00:51:22.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3083" target="_blank">00:51:23.480</a></span> | <span class="t">Uh, we look forward to the next research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3085" target="_blank">00:51:25.480</a></span> | <span class="t">What loss and optimization algorithms are most appropriate?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3088" target="_blank">00:51:28.480</a></span> | <span class="t">How much gain can be unlocked by bootstrapping with its own reasoning traces or how to scale the next generation, next order of magnitude of compute?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3096" target="_blank">00:51:36.480</a></span> | <span class="t">How do we scale this up?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3097" target="_blank">00:51:37.480</a></span> | <span class="t">So basically I really liked that, um, that section on infrastructure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3103" target="_blank">00:51:43.480</a></span> | <span class="t">How do we scale this up another organ, uh, another order of magnitude?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3108" target="_blank">00:51:48.480</a></span> | <span class="t">That's another follow up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3109" target="_blank">00:51:49.480</a></span> | <span class="t">That's another open question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3111" target="_blank">00:51:51.480</a></span> | <span class="t">Then the fun one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3112" target="_blank">00:51:52.480</a></span> | <span class="t">Uh, we want to push the boundaries of RL across a whole range of applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3117" target="_blank">00:51:57.480</a></span> | <span class="t">They want to do RL with tool use integrated multimodality and agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3121" target="_blank">00:52:01.480</a></span> | <span class="t">Uh, multimodality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3122" target="_blank">00:52:02.480</a></span> | <span class="t">Think about verification of images and like questions like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3127" target="_blank">00:52:07.480</a></span> | <span class="t">And then agents, agents is a feature where we're hyped.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3132" target="_blank">00:52:12.480</a></span> | <span class="t">So that's conclusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3134" target="_blank">00:52:14.480</a></span> | <span class="t">Um, damn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3135" target="_blank">00:52:15.480</a></span> | <span class="t">I really want to go over second paper, but I was too hyped on this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3139" target="_blank">00:52:19.480</a></span> | <span class="t">When I was setting programming, dah, dah, dah, dah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3142" target="_blank">00:52:22.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3143" target="_blank">00:52:23.480</a></span> | <span class="t">Any questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3144" target="_blank">00:52:24.480</a></span> | <span class="t">Um, what questions do we have?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3147" target="_blank">00:52:27.480</a></span> | <span class="t">Someone interrupt me for their questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3149" target="_blank">00:52:29.480</a></span> | <span class="t">Otherwise I'm going to do a four minute, unjustice to small lm3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3153" target="_blank">00:52:33.480</a></span> | <span class="t">No questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3154" target="_blank">00:52:34.480</a></span> | <span class="t">Small lm3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3155" target="_blank">00:52:35.480</a></span> | <span class="t">No questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3156" target="_blank">00:52:36.480</a></span> | <span class="t">Small lm3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3157" target="_blank">00:52:37.480</a></span> | <span class="t">This came out of Hugging Face yesterday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3159" target="_blank">00:52:39.480</a></span> | <span class="t">Chonky, chunky dataset model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3161" target="_blank">00:52:41.480</a></span> | <span class="t">Uh, but you know, very good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3162" target="_blank">00:52:42.480</a></span> | <span class="t">I must shout out Hugging Face.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3163" target="_blank">00:52:43.480</a></span> | <span class="t">It's actual research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3164" target="_blank">00:52:44.480</a></span> | <span class="t">Um, they did a lot of experiments, ablations, a million dollar train run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3170" target="_blank">00:52:50.480</a></span> | <span class="t">They train a three B that's a hybrid reasoner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3173" target="_blank">00:52:53.480</a></span> | <span class="t">First thing that shows us how to do good hybrid reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3175" target="_blank">00:52:55.480</a></span> | <span class="t">Uh, pre-training is all done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3177" target="_blank">00:52:57.480</a></span> | <span class="t">Post-training is all done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3179" target="_blank">00:52:59.480</a></span> | <span class="t">11 trillion tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3180" target="_blank">00:53:00.480</a></span> | <span class="t">Soda at three B scale competitive with four B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3183" target="_blank">00:53:03.480</a></span> | <span class="t">I used to think this is stupid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3184" target="_blank">00:53:04.480</a></span> | <span class="t">Just run a four B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3185" target="_blank">00:53:05.480</a></span> | <span class="t">But, uh, I guess, you know, you're, you're shaving off a lot of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3189" target="_blank">00:53:09.480</a></span> | <span class="t">You're shaving 30% of, um, your weight, which is important for edge stuff, but really uses edge stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3196" target="_blank">00:53:16.480</a></span> | <span class="t">Uh, instruction model with dual mode reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3199" target="_blank">00:53:19.480</a></span> | <span class="t">That's new.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3200" target="_blank">00:53:20.480</a></span> | <span class="t">Uh, basically to do this, uh, they do a mixture of thinking, no thinking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3205" target="_blank">00:53:25.480</a></span> | <span class="t">Is there a good paper on agentic behavior URL?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3208" target="_blank">00:53:28.480</a></span> | <span class="t">I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3209" target="_blank">00:53:29.480</a></span> | <span class="t">Multilingual, long context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3211" target="_blank">00:53:31.480</a></span> | <span class="t">Great paper, great paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3212" target="_blank">00:53:32.480</a></span> | <span class="t">Well, not really a paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3213" target="_blank">00:53:33.480</a></span> | <span class="t">It's just a blog post.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3214" target="_blank">00:53:34.480</a></span> | <span class="t">I expect better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3215" target="_blank">00:53:35.480</a></span> | <span class="t">Um, group query attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3217" target="_blank">00:53:37.480</a></span> | <span class="t">Nope.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3218" target="_blank">00:53:38.480</a></span> | <span class="t">Intra document masking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3219" target="_blank">00:53:39.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3220" target="_blank">00:53:40.480</a></span> | <span class="t">Forget all that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3221" target="_blank">00:53:41.480</a></span> | <span class="t">Here's their fun nodes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3222" target="_blank">00:53:42.480</a></span> | <span class="t">They use a whole lot of GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3224" target="_blank">00:53:44.480</a></span> | <span class="t">48 nodes of H one hundreds for 220,000 GPU hours.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3228" target="_blank">00:53:48.480</a></span> | <span class="t">Uh, distributed was, oh no, I've opened stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3233" target="_blank">00:53:53.480</a></span> | <span class="t">Um, yeah, distributed data parallel and tensor parallel checkpoint saving.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3240" target="_blank">00:54:00.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3241" target="_blank">00:54:01.480</a></span> | <span class="t">Uh, data mixture three phases of training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3244" target="_blank">00:54:04.480</a></span> | <span class="t">First phase, all web, then more math and code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3246" target="_blank">00:54:06.480</a></span> | <span class="t">Then even more math and code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3247" target="_blank">00:54:07.480</a></span> | <span class="t">Crazy, crazy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3248" target="_blank">00:54:08.480</a></span> | <span class="t">Uh, what data sets go into here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3250" target="_blank">00:54:10.480</a></span> | <span class="t">They talk about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3251" target="_blank">00:54:11.480</a></span> | <span class="t">Uh, mid training, this concept of mid training context length extension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3255" target="_blank">00:54:15.480</a></span> | <span class="t">I think the more fun stuff is reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3257" target="_blank">00:54:17.480</a></span> | <span class="t">When reasoning comes in and mid training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3259" target="_blank">00:54:19.480</a></span> | <span class="t">Um, so after extending context length, we do, we do mid training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3264" target="_blank">00:54:24.480</a></span> | <span class="t">They use other data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3266" target="_blank">00:54:26.480</a></span> | <span class="t">So open thoughts and Nvidia Nemo Tron post training reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3269" target="_blank">00:54:29.480</a></span> | <span class="t">Uh, this is distilled from R1 open thoughts as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3273" target="_blank">00:54:33.480</a></span> | <span class="t">35 billion tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3274" target="_blank">00:54:34.480</a></span> | <span class="t">We use chat ML template.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3276" target="_blank">00:54:36.480</a></span> | <span class="t">Da, da, da, da, da.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3277" target="_blank">00:54:37.480</a></span> | <span class="t">Post training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3278" target="_blank">00:54:38.480</a></span> | <span class="t">Um, da, da, da, da.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3280" target="_blank">00:54:40.480</a></span> | <span class="t">Mid training, post training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3282" target="_blank">00:54:42.480</a></span> | <span class="t">APO.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3283" target="_blank">00:54:43.480</a></span> | <span class="t">APO was interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3284" target="_blank">00:54:44.480</a></span> | <span class="t">Building the chat template.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3285" target="_blank">00:54:45.480</a></span> | <span class="t">Do SFT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3286" target="_blank">00:54:46.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3287" target="_blank">00:54:47.480</a></span> | <span class="t">I don't have time for this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3288" target="_blank">00:54:48.480</a></span> | <span class="t">This is too long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3289" target="_blank">00:54:49.480</a></span> | <span class="t">Model merging.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3290" target="_blank">00:54:50.480</a></span> | <span class="t">I talked about model merging.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3291" target="_blank">00:54:51.480</a></span> | <span class="t">This was a fun one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3292" target="_blank">00:54:52.480</a></span> | <span class="t">APO is a way to kind of better DPO.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3297" target="_blank">00:54:57.480</a></span> | <span class="t">It's like a play on DPO that lets it do good RL reasoning plus, um, context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3303" target="_blank">00:55:03.480</a></span> | <span class="t">Then we model merge by context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3305" target="_blank">00:55:05.480</a></span> | <span class="t">Performance is stupid, but I wanna, I wanna talk about this dual instruct reasoning model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3310" target="_blank">00:55:10.480</a></span> | <span class="t">This is very interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3311" target="_blank">00:55:11.480</a></span> | <span class="t">Um, I think, I think I shouldn't do this paper harm by doing it one minute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3316" target="_blank">00:55:16.480</a></span> | <span class="t">Maybe we get someone from hugging face to do it next time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3319" target="_blank">00:55:19.480</a></span> | <span class="t">Or, or we just do this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3321" target="_blank">00:55:21.480</a></span> | <span class="t">It's kind of short, but yeah, you should read this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3323" target="_blank">00:55:23.480</a></span> | <span class="t">Um, good paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3324" target="_blank">00:55:24.480</a></span> | <span class="t">Good paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3325" target="_blank">00:55:25.480</a></span> | <span class="t">Ooh, small LLM pod.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3328" target="_blank">00:55:28.480</a></span> | <span class="t">Nevermind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3329" target="_blank">00:55:29.480</a></span> | <span class="t">Why do paper club when we have podcast?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3331" target="_blank">00:55:31.480</a></span> | <span class="t">Um, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3332" target="_blank">00:55:32.480</a></span> | <span class="t">I guess they're coming on podcast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3334" target="_blank">00:55:34.480</a></span> | <span class="t">Coming paper club important for learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3336" target="_blank">00:55:36.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3337" target="_blank">00:55:37.480</a></span> | <span class="t">Well, maybe next time I'll do 20 minutes on this or we'll have someone join us and talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3343" target="_blank">00:55:43.480</a></span> | <span class="t">about this too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3344" target="_blank">00:55:44.480</a></span> | <span class="t">TBD.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3345" target="_blank">00:55:45.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3346" target="_blank">00:55:46.480</a></span> | <span class="t">I hope my ramble of why I thought this infrastructure was cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3348" target="_blank">00:55:48.480</a></span> | <span class="t">It's very interesting to me actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3350" target="_blank">00:55:50.480</a></span> | <span class="t">I'm not much of an info guy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3352" target="_blank">00:55:52.480</a></span> | <span class="t">I don't really care about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3353" target="_blank">00:55:53.480</a></span> | <span class="t">But then once I, um, I think it's, it's different when you do pre and post training and you just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3358" target="_blank">00:55:58.480</a></span> | <span class="t">have GPU go bird.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3359" target="_blank">00:55:59.480</a></span> | <span class="t">This is not GPU go bird.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3361" target="_blank">00:56:01.480</a></span> | <span class="t">This is, uh, oh shit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3362" target="_blank">00:56:02.480</a></span> | <span class="t">We'll have a lot of like over, overhang and dead GPU inefficiency if we do stuff sequentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3369" target="_blank">00:56:09.480</a></span> | <span class="t">So here's how they, here's how we do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3371" target="_blank">00:56:11.480</a></span> | <span class="t">Next time we can peek into the training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3373" target="_blank">00:56:13.480</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3374" target="_blank">00:56:14.480</a></span> | <span class="t">I think training data is one aspect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3375" target="_blank">00:56:15.480</a></span> | <span class="t">Uh, these data sets are open.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3377" target="_blank">00:56:17.480</a></span> | <span class="t">People have already done a lot of exploratory analysis and breakdown of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3381" target="_blank">00:56:21.480</a></span> | <span class="t">But the fun thing that we can actually dig into is the, um, the training code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3387" target="_blank">00:56:27.480</a></span> | <span class="t">So, you know, uh, all, all this stuff is open.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3390" target="_blank">00:56:30.480</a></span> | <span class="t">I will probably, oh no, we're cooked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3393" target="_blank">00:56:33.480</a></span> | <span class="t">I'll probably just, uh, throw all this in cloud code and we can, we can yap around with it if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3398" target="_blank">00:56:38.480</a></span> | <span class="t">we want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3399" target="_blank">00:56:39.480</a></span> | <span class="t">But yeah, I don't want to, I don't want to keep you guys, um, for too long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3403" target="_blank">00:56:43.480</a></span> | <span class="t">If there's any, if there's any fun questions or anything, let me know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3410" target="_blank">00:56:50.480</a></span> | <span class="t">Um, congrats people on, on a fantastic presentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3414" target="_blank">00:56:54.480</a></span> | <span class="t">Um, always love these like deep dyes, especially when it's like a really good paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3418" target="_blank">00:56:58.480</a></span> | <span class="t">Like, um, I actually didn't read it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3420" target="_blank">00:57:00.480</a></span> | <span class="t">I don't know why I didn't read it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3421" target="_blank">00:57:01.480</a></span> | <span class="t">I just was busy that day.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3422" target="_blank">00:57:02.480</a></span> | <span class="t">So thank you for highlighting it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3424" target="_blank">00:57:04.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3425" target="_blank">00:57:05.480</a></span> | <span class="t">Thanks guys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3426" target="_blank">00:57:06.480</a></span> | <span class="t">See you next week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3427" target="_blank">00:57:07.480</a></span> | <span class="t">Thanks everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3428" target="_blank">00:57:08.480</a></span> | <span class="t">I need volunteers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3429" target="_blank">00:57:09.480</a></span> | <span class="t">Volunteer, volunteer volunteer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3430" target="_blank">00:57:10.480</a></span> | <span class="t">If someone wants to volunteer for a paper, let us know, uh, potentially small LM3 next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3437" target="_blank">00:57:17.480</a></span> | <span class="t">week, but any, any time in the future, if anyone wants to volunteer, let me know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3440" target="_blank">00:57:20.480</a></span> | <span class="t">Grok 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3441" target="_blank">00:57:21.480</a></span> | <span class="t">Grok 4 will not be a paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3443" target="_blank">00:57:23.480</a></span> | <span class="t">For, for, for fun context, for those that don't know, I, I used to run a very aggressive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3449" target="_blank">00:57:29.480</a></span> | <span class="t">shitposting alt on Twitter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3451" target="_blank">00:57:31.480</a></span> | <span class="t">And, um, when Grok 3 came out, I poked, I poked the bear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3455" target="_blank">00:57:35.480</a></span> | <span class="t">I really shat on their charts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3457" target="_blank">00:57:37.480</a></span> | <span class="t">I got into Twitter beef.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3458" target="_blank">00:57:38.480</a></span> | <span class="t">Elon got involved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3459" target="_blank">00:57:39.480</a></span> | <span class="t">And then the next day my account was terminated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3462" target="_blank">00:57:42.480</a></span> | <span class="t">So if you shitpost on Twitter, don't, don't go get Grok.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3466" target="_blank">00:57:46.480</a></span> | <span class="t">I learned my lesson, but, uh, yeah, Grok 4 today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3469" target="_blank">00:57:49.480</a></span> | <span class="t">Um, Grok, Grok 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3472" target="_blank">00:57:52.480</a></span> | <span class="t">So interestingly enough, they open sourced Grok 1 when Grok 2 came out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3477" target="_blank">00:57:57.480</a></span> | <span class="t">They didn't open source Grok 2 when Grok 3 came out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3479" target="_blank">00:57:59.480</a></span> | <span class="t">Maybe they open source Grok 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3480" target="_blank">00:58:00.480</a></span> | <span class="t">Who knows?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3481" target="_blank">00:58:01.480</a></span> | <span class="t">Um, we'll maybe do a watch party.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3485" target="_blank">00:58:05.480</a></span> | <span class="t">Uh, who knows?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3486" target="_blank">00:58:06.480</a></span> | <span class="t">Paper highlights and Zotero?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3487" target="_blank">00:58:07.480</a></span> | <span class="t">No, I'm not a Zotero guy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3488" target="_blank">00:58:08.480</a></span> | <span class="t">I don't know how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3489" target="_blank">00:58:09.480</a></span> | <span class="t">Too hard, too hard for me, but I'll share my paper highlights and maybe someone can throw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3493" target="_blank">00:58:13.480</a></span> | <span class="t">in Zotero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3494" target="_blank">00:58:14.480</a></span> | <span class="t">Um, for Timeless Paper Club, I think I'll share a post later this week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3498" target="_blank">00:58:18.480</a></span> | <span class="t">I have a bunch of topics papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3500" target="_blank">00:58:20.480</a></span> | <span class="t">There's some domains where I'll cover them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3503" target="_blank">00:58:23.480</a></span> | <span class="t">If anyone wants to volunteer, that would be useful too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3506" target="_blank">00:58:26.480</a></span> | <span class="t">So, um, some stuff, like if you think you're good, like if you know diffusion pretty well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3512" target="_blank">00:58:32.480</a></span> | <span class="t">if you know optimizers, if you know, like inference optimization, like flash attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3516" target="_blank">00:58:36.480</a></span> | <span class="t">if there's shit that you're passionate about, if there's stuff that you want accountability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3521" target="_blank">00:58:41.480</a></span> | <span class="t">to learn and, and read over, maybe we do it together or you take it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3525" target="_blank">00:58:45.480</a></span> | <span class="t">And, uh, yeah, that'll be fun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3526" target="_blank">00:58:46.480</a></span> | <span class="t">I'll give you a list of papers or, you know, feel free to add some.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3529" target="_blank">00:58:49.480</a></span> | <span class="t">And then, and then we'll, we'll go through second paper clip.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3532" target="_blank">00:58:52.480</a></span> | <span class="t">Uh, launch event for OpenAI's open source model next week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3536" target="_blank">00:58:56.480</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3536" target="_blank">00:58:56.480</a></span> | <span class="t">Uh, someone said that it's, uh, next week, but I heard a date that's slightly later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3543" target="_blank">00:59:03.480</a></span> | <span class="t">Where is this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3544" target="_blank">00:59:04.480</a></span> | <span class="t">Where is this new open source model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3546" target="_blank">00:59:06.480</a></span> | <span class="t">So someone has said that it comes out, um, next Thursday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3552" target="_blank">00:59:12.480</a></span> | <span class="t">I heard later, but we'll see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3555" target="_blank">00:59:15.480</a></span> | <span class="t">We'll see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3556" target="_blank">00:59:16.480</a></span> | <span class="t">Um, I dunno, maybe we'll do an event.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3559" target="_blank">00:59:19.480</a></span> | <span class="t">Maybe, maybe we won't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3560" target="_blank">00:59:20.480</a></span> | <span class="t">It's not dev day in November.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3563" target="_blank">00:59:23.480</a></span> | <span class="t">It's, it's this month, but maybe not next week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3566" target="_blank">00:59:26.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3567" target="_blank">00:59:27.480</a></span> | <span class="t">Anyway, enough yap.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3569" target="_blank">00:59:29.480</a></span> | <span class="t">Thanks for listening to my yap guys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3573" target="_blank">00:59:33.480</a></span> | <span class="t">Two weeks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3575" target="_blank">00:59:35.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=_vNFJcb8S_M&t=3576" target="_blank">00:59:36.480</a></span> | <span class="t">Not next week, two weeks, but we'll see.</span></div></div></body></html>