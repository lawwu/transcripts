<html><head><title>Lesson 10: Deep Learning Part 2 2018 - NLP Classification and Translation</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Lesson 10: Deep Learning Part 2 2018 - NLP Classification and Translation</h2><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo"><img src="https://i.ytimg.com/vi_webp/h5Tz7gZT9Fo/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=18">0:18</a> Review of Last Week<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1092">18:12</a> Segmentation<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1114">18:34</a> Feature Pyramids<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1137">18:57</a> Nlp<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1288">21:28</a> Basic Paths for Nlp<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1642">27:22</a> Train Test Split<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1685">28:5</a> Tokenization<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2625">43:45</a> Building a Language Model on Wikipedia<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2887">48:7</a> Create an Embedding Matrix<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3684">61:24</a> Averaging the Weights of Embeddings<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4296">71:36</a> Language Model<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4609">76:49</a> Edit Encoder<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4687">78:7</a> Regularizing and Optimizing Lsdm Language Models<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4750">79:10</a> Tie Weights<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4916">81:56</a> Measure Accuracy<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5126">85:26</a> What Is Your Ratio of Paper Reading versus Coding in a Week<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5339">88:59</a> Universal Sentence Encoder<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5978">99:38</a> Add More than One Hidden Layer<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6588">109:48</a> Learning Rate<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6717">111:57</a> Concat Pooling<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7286">121:26</a> Trick Number Two Is To Create Python Scripts<br><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7375">122:55</a> Imdb Scripts<br><br><div style="text-align: left;"><a href="./h5Tz7gZT9Fo.html">Whisper Transcript</a> | <a href="./transcript_h5Tz7gZT9Fo.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">So, welcome to lesson 10, or as somebody on the forum described it, lesson 10, mod 7,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7" target="_blank">00:00:07.600</a></span> | <span class="t">which is probably a clearer way to think about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=11" target="_blank">00:00:11.300</a></span> | <span class="t">We're going to be talking about NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=15" target="_blank">00:00:15.860</a></span> | <span class="t">Before we do, let's do a quick review of last week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=21" target="_blank">00:00:21.900</a></span> | <span class="t">Last week, there's quite a few people who have flown here to San Francisco for this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=26" target="_blank">00:00:26.320</a></span> | <span class="t">in-person course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=27" target="_blank">00:00:27.320</a></span> | <span class="t">I'm seeing them pretty much every day, they're working full-time on this, and quite a few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=31" target="_blank">00:00:31.880</a></span> | <span class="t">of them are still struggling to understand the material from last week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=35" target="_blank">00:00:35.280</a></span> | <span class="t">So if you're finding it difficult, that's fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=37" target="_blank">00:00:37.640</a></span> | <span class="t">One of the reasons I kind of put it up there up front is so that we've got something to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=42" target="_blank">00:00:42.920</a></span> | <span class="t">concentrate about and think about and gradually work towards so that by lesson 14, mod 7,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=50" target="_blank">00:00:50.220</a></span> | <span class="t">you'll get a second crack at it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=52" target="_blank">00:00:52.960</a></span> | <span class="t">But there's so many pieces, so hopefully you can keep developing better understanding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=58" target="_blank">00:00:58.360</a></span> | <span class="t">To understand the pieces, you'll need to understand the shapes of convolutional layer outputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=63" target="_blank">00:01:03.960</a></span> | <span class="t">and receptive fields, and loss functions, and everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=68" target="_blank">00:01:08.520</a></span> | <span class="t">So it's all stuff that you need to understand for all of your deep learning studies anyway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=75" target="_blank">00:01:15.640</a></span> | <span class="t">So everything you do to develop an understanding of last week's lesson is going to help you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=79" target="_blank">00:01:19.640</a></span> | <span class="t">with everything else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=82" target="_blank">00:01:22.480</a></span> | <span class="t">One key thing I wanted to mention is we started out with something which is really pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=86" target="_blank">00:01:26.440</a></span> | <span class="t">simple, which is single object classifier, single object bounding box without a classifier,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=94" target="_blank">00:01:34.040</a></span> | <span class="t">and then single object classifier and bounding box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=98" target="_blank">00:01:38.080</a></span> | <span class="t">And anybody who's spent some time studying since lesson 8, mod 7, has got to the point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=106" target="_blank">00:01:46.840</a></span> | <span class="t">where they understand this bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=109" target="_blank">00:01:49.680</a></span> | <span class="t">Now the reason I mention this is because the bit where we go to multiple objects is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=115" target="_blank">00:01:55.940</a></span> | <span class="t">almost identical to this, except we first have to solve the matching problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=120" target="_blank">00:02:00.800</a></span> | <span class="t">We end up creating far more activations than we need for our number of bounding boxes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=127" target="_blank">00:02:07.280</a></span> | <span class="t">ground truth bounding boxes, so we match each ground truth object to a subset of those activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=132" target="_blank">00:02:12.920</a></span> | <span class="t">And once we've done that, the loss function that we then do to each matched pair is almost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=138" target="_blank">00:02:18.080</a></span> | <span class="t">identical to this loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=140" target="_blank">00:02:20.800</a></span> | <span class="t">So if you're feeling stuck, go back to lesson 8 and make sure you understand the data set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=150" target="_blank">00:02:30.040</a></span> | <span class="t">the data loader, and most importantly the loss function from the end of lesson 8 or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=155" target="_blank">00:02:35.640</a></span> | <span class="t">the start of lesson 9.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=160" target="_blank">00:02:40.880</a></span> | <span class="t">So once we've got this thing which can predict the class and bounding box for one object,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=167" target="_blank">00:02:47.240</a></span> | <span class="t">we went to multiple objects by creating more activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=171" target="_blank">00:02:51.800</a></span> | <span class="t">We had to then deal with the matching problem, we then basically moved each of those anchor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=178" target="_blank">00:02:58.640</a></span> | <span class="t">boxes in and out a little bit and around a little bit, so they tried to line up with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=185" target="_blank">00:03:05.160</a></span> | <span class="t">particular ground truth objects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=187" target="_blank">00:03:07.720</a></span> | <span class="t">And we talked about how we took advantage of the convolutional nature of the network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=193" target="_blank">00:03:13.920</a></span> | <span class="t">to try to have activations that had a receptive field that was similar to the ground truth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=201" target="_blank">00:03:21.560</a></span> | <span class="t">object we were predicting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=203" target="_blank">00:03:23.360</a></span> | <span class="t">And Chloe Sultan provided this fantastic picture, I guess for her own notes, but she shared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=209" target="_blank">00:03:29.800</a></span> | <span class="t">it with everybody, which is lovely, to talk about what does SSD multi-head forward do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=216" target="_blank">00:03:36.320</a></span> | <span class="t">line by line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=217" target="_blank">00:03:37.880</a></span> | <span class="t">And I partly wanted to show this to help you with your revision, but I also partly wanted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=221" target="_blank">00:03:41.680</a></span> | <span class="t">to show this to kind of say, doing this kind of stuff is very useful for you to do, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=228" target="_blank">00:03:48.840</a></span> | <span class="t">walk through and in whatever way helps you make sure you understand something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=233" target="_blank">00:03:53.120</a></span> | <span class="t">You can see what Chloe's done here is she's focused particularly on the dimensions of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=239" target="_blank">00:03:59.760</a></span> | <span class="t">the tensor at each point in the path as we kind of gradually down-sampling using these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=246" target="_blank">00:04:06.440</a></span> | <span class="t">tragedy convolutions, making sure she understands why those grid sizes happen, and then understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=253" target="_blank">00:04:13.000</a></span> | <span class="t">how the outputs come out of those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=256" target="_blank">00:04:16.680</a></span> | <span class="t">And so one thing you might be wondering is how did Chloe calculate these numbers?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=263" target="_blank">00:04:23.480</a></span> | <span class="t">So I don't know the answer I haven't spoken to her, but obviously one approach would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=267" target="_blank">00:04:27.840</a></span> | <span class="t">like from first principles just thinking through it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=270" target="_blank">00:04:30.360</a></span> | <span class="t">But then you want to know what am I right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=273" target="_blank">00:04:33.040</a></span> | <span class="t">And so this is where you've got to remember this pdb.settrace idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=278" target="_blank">00:04:38.560</a></span> | <span class="t">So I just went in just before class and went into SSD multi-head.forward and entered pdb.settrace,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=287" target="_blank">00:04:47.040</a></span> | <span class="t">and then I ran a single batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=290" target="_blank">00:04:50.040</a></span> | <span class="t">And so I put the trace at the end, and then I could just print out the size of all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=296" target="_blank">00:04:56.440</a></span> | <span class="t">these guys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=298" target="_blank">00:04:58.520</a></span> | <span class="t">So which reminds me, last week there may have been a point where I said 21 + 4 = 26, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=311" target="_blank">00:05:11.920</a></span> | <span class="t">is not true in most universes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=321" target="_blank">00:05:21.840</a></span> | <span class="t">And by the way, when I code I do that stuff, that's the kind of thing I do all the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=325" target="_blank">00:05:25.920</a></span> | <span class="t">So that's why we have debuggers and know how to check things and do things in small little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=330" target="_blank">00:05:30.760</a></span> | <span class="t">bits along the way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=332" target="_blank">00:05:32.500</a></span> | <span class="t">So this idea of putting a debugger inside your forward function and printing out the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=336" target="_blank">00:05:36.240</a></span> | <span class="t">sizes is something which is damn super helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=340" target="_blank">00:05:40.440</a></span> | <span class="t">Or you could just put a print statement here as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=344" target="_blank">00:05:44.720</a></span> | <span class="t">So I actually don't know if that's how Chloe figured it out, but that's how I would if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=348" target="_blank">00:05:48.360</a></span> | <span class="t">I was her.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=350" target="_blank">00:05:50.560</a></span> | <span class="t">And then we talked about increasing k, which is the number of anchor boxes for each convolutional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=355" target="_blank">00:05:55.360</a></span> | <span class="t">grid cell, which we can do with different zooms and different aspect ratios.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=359" target="_blank">00:05:59.760</a></span> | <span class="t">And so that gives us a plethora of activations, and therefore predicted bounding boxes, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=367" target="_blank">00:06:07.400</a></span> | <span class="t">we then went down to a small number using non-maximum suppression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=375" target="_blank">00:06:15.920</a></span> | <span class="t">And I'll try to remember to put a link -- there's a really interesting paper that one of our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=378" target="_blank">00:06:18.960</a></span> | <span class="t">students told me about that I hadn't heard about, which is attempting to -- you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=384" target="_blank">00:06:24.120</a></span> | <span class="t">I've mentioned non-maximum suppression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=386" target="_blank">00:06:26.320</a></span> | <span class="t">It's kind of hacky, kind of ugly, it's totally heuristic, I didn't even talk about the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=392" target="_blank">00:06:32.120</a></span> | <span class="t">because it seems kind of hideous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=394" target="_blank">00:06:34.960</a></span> | <span class="t">So somebody actually came up with a paper recently which attempts to do an end-to-end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=398" target="_blank">00:06:38.960</a></span> | <span class="t">ConvNet to replace that NMS piece.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=402" target="_blank">00:06:42.840</a></span> | <span class="t">So I'll put that paper up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=405" target="_blank">00:06:45.320</a></span> | <span class="t">Nobody's created a PyTorch implementation yet, so it would be an interesting project</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=410" target="_blank">00:06:50.880</a></span> | <span class="t">if anyone wanted to try that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=415" target="_blank">00:06:55.800</a></span> | <span class="t">One thing I've noticed in our study groups during the week is not enough people reading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=420" target="_blank">00:07:00.800</a></span> | <span class="t">papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=423" target="_blank">00:07:03.200</a></span> | <span class="t">What we are doing in class now is implementing papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=426" target="_blank">00:07:06.520</a></span> | <span class="t">The papers are the real ground truth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=430" target="_blank">00:07:10.940</a></span> | <span class="t">And I think from talking to people, a lot of the reason people aren't reading papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=434" target="_blank">00:07:14.360</a></span> | <span class="t">is because a lot of people don't think they're capable of reading papers, they don't think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=438" target="_blank">00:07:18.560</a></span> | <span class="t">they're the kind of people that read papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=441" target="_blank">00:07:21.400</a></span> | <span class="t">But you are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=442" target="_blank">00:07:22.640</a></span> | <span class="t">You're here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=444" target="_blank">00:07:24.160</a></span> | <span class="t">And we started looking at a paper last week and we read the words that were in English</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=449" target="_blank">00:07:29.120</a></span> | <span class="t">and we largely understood them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=452" target="_blank">00:07:32.120</a></span> | <span class="t">So if you actually look through this picture from SSD, carefully you'll realize that SSD</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=460" target="_blank">00:07:40.800</a></span> | <span class="t">multi-head dot forward is not doing the same as this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=464" target="_blank">00:07:44.880</a></span> | <span class="t">And then you might think, oh, I wonder if this is better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=469" target="_blank">00:07:49.240</a></span> | <span class="t">And my answer is probably, because SSD multi-head dot forward was the first thing I tried just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=476" target="_blank">00:07:56.540</a></span> | <span class="t">to get something out there, but between this and the YOLO version, there are probably much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=484" target="_blank">00:08:04.240</a></span> | <span class="t">better ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=485" target="_blank">00:08:05.660</a></span> | <span class="t">One thing you'll notice in particular is they use a smaller K, but they have a lot more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=490" target="_blank">00:08:10.600</a></span> | <span class="t">sets of grids, 1x1, 3x3, 5x5, 10x10, 19x19 and 38x38, 8,700 per plus, so a lot more than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=502" target="_blank">00:08:22.080</a></span> | <span class="t">we had, so that'd be an interesting thing to experiment with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=505" target="_blank">00:08:25.600</a></span> | <span class="t">Another thing I noticed is that we had 4x4, 2x2, 1x1, which means there's a lot of overlap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=513" target="_blank">00:08:33.840</a></span> | <span class="t">like every set fits within every other set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=517" target="_blank">00:08:37.360</a></span> | <span class="t">In this case where you've got 1, 3, 5, you don't have that overlap, so it might actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=522" target="_blank">00:08:42.960</a></span> | <span class="t">make it easier to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=524" target="_blank">00:08:44.120</a></span> | <span class="t">So there's lots of interesting things you can play with based on stuff that's either</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=528" target="_blank">00:08:48.920</a></span> | <span class="t">trying to make it closer to the paper or think about other things you could try that aren't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=533" target="_blank">00:08:53.000</a></span> | <span class="t">in the paper or whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=535" target="_blank">00:08:55.720</a></span> | <span class="t">Perhaps the most important thing I would recommend is to put the code and the equations next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=542" target="_blank">00:09:02.520</a></span> | <span class="t">to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=543" target="_blank">00:09:03.520</a></span> | <span class="t">Yes, Rachel?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=544" target="_blank">00:09:04.520</a></span> | <span class="t">There was a question of whether you could speak about the use cyclic learning rate argument</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=548" target="_blank">00:09:08.800</a></span> | <span class="t">and the fit function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=549" target="_blank">00:09:09.800</a></span> | <span class="t">We will get there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=553" target="_blank">00:09:13.920</a></span> | <span class="t">So put the code and the equations from the paper next to each other and draw in one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=561" target="_blank">00:09:21.560</a></span> | <span class="t">two groups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=562" target="_blank">00:09:22.560</a></span> | <span class="t">You're either a code person like me who's not that happy about math, in which case I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=569" target="_blank">00:09:29.480</a></span> | <span class="t">start with the code and then I look at the math and I learn about how the math maps to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=574" target="_blank">00:09:34.560</a></span> | <span class="t">the code and end up eventually understanding the math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=578" target="_blank">00:09:38.520</a></span> | <span class="t">All your PhD in Stochastic Differential Equations like Rachel, whatever that means, in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=587" target="_blank">00:09:47.400</a></span> | <span class="t">case you can look at the math and then learn about how the code completes the math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=592" target="_blank">00:09:52.400</a></span> | <span class="t">But either way, unless you're one of those rare people who is equally comfortable in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=596" target="_blank">00:09:56.800</a></span> | <span class="t">either world, you'll learn about one or the other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=602" target="_blank">00:10:02.740</a></span> | <span class="t">Now learning about code is pretty easy because there's documentation and we know how to look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=607" target="_blank">00:10:07.760</a></span> | <span class="t">it up and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=609" target="_blank">00:10:09.320</a></span> | <span class="t">Sometimes learning the math is hard because the notation might seem hard to look up, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=613" target="_blank">00:10:13.760</a></span> | <span class="t">there's actually a lot of resources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=615" target="_blank">00:10:15.720</a></span> | <span class="t">For example, a list of mathematical symbols on Wikipedia is amazingly great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=621" target="_blank">00:10:21.560</a></span> | <span class="t">It has examples of them, explanations of what they mean, and tells you what to search for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=627" target="_blank">00:10:27.280</a></span> | <span class="t">to find out more about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=630" target="_blank">00:10:30.880</a></span> | <span class="t">Really terrific.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=631" target="_blank">00:10:31.880</a></span> | <span class="t">And if you Google for math notation cheat sheet, you'll find more of these kinds of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=637" target="_blank">00:10:37.680</a></span> | <span class="t">terrific resources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=640" target="_blank">00:10:40.200</a></span> | <span class="t">So over time, you do need to learn the notation, but as you'll see from the Wikipedia page,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=646" target="_blank">00:10:46.560</a></span> | <span class="t">there's not actually that much of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=649" target="_blank">00:10:49.600</a></span> | <span class="t">Obviously there's a lot of concepts behind it, but once you know the notation you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=653" target="_blank">00:10:53.800</a></span> | <span class="t">then quickly look up the concept as it pertains to the particular thing you're studying.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=659" target="_blank">00:10:59.160</a></span> | <span class="t">Nobody learns all of math and then starts machine learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=665" target="_blank">00:11:05.160</a></span> | <span class="t">Everybody, even top researchers I know, when they're reading a new paper will very often</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=670" target="_blank">00:11:10.800</a></span> | <span class="t">come to bits of math they haven't seen before and they'll have to go away and learn that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=675" target="_blank">00:11:15.160</a></span> | <span class="t">bit of math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=678" target="_blank">00:11:18.800</a></span> | <span class="t">Another thing you should try doing is to recreate things that you see in the papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=684" target="_blank">00:11:24.560</a></span> | <span class="t">So here was the key most important figure 1 from the focal loss paper, the Retlinet paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=692" target="_blank">00:11:32.400</a></span> | <span class="t">So recreate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=695" target="_blank">00:11:35.040</a></span> | <span class="t">And very often I put these challenges up on the forums, so keep an eye on the lesson threads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=701" target="_blank">00:11:41.760</a></span> | <span class="t">during the forums, and so I put this challenge up there and within about 3 minutes Serada</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=706" target="_blank">00:11:46.840</a></span> | <span class="t">had said "done it" in Microsoft Excel naturally along with actually a lot more information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=713" target="_blank">00:11:53.880</a></span> | <span class="t">than in the original paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=715" target="_blank">00:11:55.280</a></span> | <span class="t">A nice thing here is that she was actually able to draw a line showing at a 0.5 ground</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=720" target="_blank">00:12:00.600</a></span> | <span class="t">truth probability what's the loss for different amounts of gamma, which is kind of cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=727" target="_blank">00:12:07.360</a></span> | <span class="t">And if you want to cheat, she's also provided Python code on the forum too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=735" target="_blank">00:12:15.760</a></span> | <span class="t">I did discover a minor bug in my code last week, the way that I was flattening out the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=740" target="_blank">00:12:20.920</a></span> | <span class="t">convolutional activations did not line up with how I was using them in the loss function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=746" target="_blank">00:12:26.800</a></span> | <span class="t">and fixing that actually made it quite a bit better, so my motorbikes and cows are actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=752" target="_blank">00:12:32.160</a></span> | <span class="t">in the right place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=753" target="_blank">00:12:33.160</a></span> | <span class="t">So when you go back to the notebook, you'll see it's a little less bad than it was last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=759" target="_blank">00:12:39.680</a></span> | <span class="t">time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=761" target="_blank">00:12:41.160</a></span> | <span class="t">So there's some quick coverage of what's gone before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=768" target="_blank">00:12:48.200</a></span> | <span class="t">Yes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=769" target="_blank">00:12:49.200</a></span> | <span class="t">>> Quick question, are you going to put the PowerPoint on GitHub?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=773" target="_blank">00:12:53.820</a></span> | <span class="t">>> I'll put a subset of it on GitHub.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=778" target="_blank">00:12:58.200</a></span> | <span class="t">>> And then secondly, usually when we down sample, we increase the number of filters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=782" target="_blank">00:13:02.900</a></span> | <span class="t">or depth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=783" target="_blank">00:13:03.900</a></span> | <span class="t">When we're doing sampling from 77 to 44, why are we decreasing the number from 512 to 256?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=790" target="_blank">00:13:10.400</a></span> | <span class="t">Why not decrease dimension in SSD head?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=794" target="_blank">00:13:14.200</a></span> | <span class="t">Is it performance related?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=796" target="_blank">00:13:16.240</a></span> | <span class="t">>> 77 to 44?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=799" target="_blank">00:13:19.760</a></span> | <span class="t">Oh, 7 by 7 to 4 by 4?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=803" target="_blank">00:13:23.920</a></span> | <span class="t">I guess they've got the stars and the colors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=812" target="_blank">00:13:32.200</a></span> | <span class="t">>> Oh yes, that's right, they're weird italics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=815" target="_blank">00:13:35.200</a></span> | <span class="t">>> It's because -- well, largely it's because that's kind of what the papers tend to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=818" target="_blank">00:13:38.640</a></span> | <span class="t">We've got a number of -- well, we have a number of out paths and we kind of want each one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=824" target="_blank">00:13:44.720</a></span> | <span class="t">to be the same, so we don't want each one to have a different number of filters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=831" target="_blank">00:13:51.120</a></span> | <span class="t">And also this is what the papers did, so I was trying to match up with that, having these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=836" target="_blank">00:13:56.760</a></span> | <span class="t">256.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=837" target="_blank">00:13:57.760</a></span> | <span class="t">It's a different concept because we're taking advantage of not just the last layer, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=842" target="_blank">00:14:02.880</a></span> | <span class="t">the layers before that as well. Life's easier if we make them more consistent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=852" target="_blank">00:14:12.080</a></span> | <span class="t">So we're now going to move to NLP, and so let me kind of lay out where we're going here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=863" target="_blank">00:14:23.560</a></span> | <span class="t">We've seen a couple of times now this idea of taking a pre-trained model, in fact we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=868" target="_blank">00:14:28.400</a></span> | <span class="t">seen it in every lesson. Take a pre-trained model, rip off some stuff on the top, replace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=873" target="_blank">00:14:33.280</a></span> | <span class="t">it with some new stuff, get it to do something similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=882" target="_blank">00:14:42.000</a></span> | <span class="t">And so what we're going to do -- and so we've kind of dived in a little bit deeper to that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=888" target="_blank">00:14:48.440</a></span> | <span class="t">to say like okay, with conv_learner.pre_trained, it had a standard way of sticking stuff on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=896" target="_blank">00:14:56.960</a></span> | <span class="t">the top which does a particular thing which was classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=901" target="_blank">00:15:01.600</a></span> | <span class="t">And then we learned actually we can stick any PyTorch module we like on the end and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=907" target="_blank">00:15:07.640</a></span> | <span class="t">have it do anything we like with a custom head. And so suddenly you discover, wow, there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=916" target="_blank">00:15:16.280</a></span> | <span class="t">some really interesting things we can do. In fact, that reminds me, Yang Lu said, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=937" target="_blank">00:15:37.960</a></span> | <span class="t">what if we did a different kind of custom head? And so the different custom head was,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=942" target="_blank">00:15:42.200</a></span> | <span class="t">well, let's take the original pictures and rotate them and then make our dependent variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=951" target="_blank">00:15:51.480</a></span> | <span class="t">the opposite of that rotation basically and see if it can learn to unrotate it. And this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=957" target="_blank">00:15:57.680</a></span> | <span class="t">is like a super useful thing, obviously. In fact, I think Google Photos nowadays has this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=963" target="_blank">00:16:03.160</a></span> | <span class="t">option that it will actually automatically rotate your photos for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=969" target="_blank">00:16:09.200</a></span> | <span class="t">But the cool thing is, as Yang Lu shows here, you can build that network right now by doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=975" target="_blank">00:16:15.000</a></span> | <span class="t">exactly the same as our previous lesson, but your custom head is one that spits out a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=981" target="_blank">00:16:21.320</a></span> | <span class="t">number which is how much to rotate by, and your dataset has a dependent variable which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=987" target="_blank">00:16:27.120</a></span> | <span class="t">is how much did you rotate by. So you suddenly realize with this idea of a backbone plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=994" target="_blank">00:16:34.040</a></span> | <span class="t">a custom head, you can do almost anything you can think about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1001" target="_blank">00:16:41.920</a></span> | <span class="t">So today we're going to look at the same idea and say, okay, how does that apply to NLP?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1009" target="_blank">00:16:49.640</a></span> | <span class="t">And then in the next lesson, we're going to go further and say, well, if NLP and computer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1016" target="_blank">00:16:56.600</a></span> | <span class="t">vision kind of let you do the same basic ideas, how do we combine them two? And we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1022" target="_blank">00:17:02.120</a></span> | <span class="t">to learn about a model that can actually learn to find word structures from images, or images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1031" target="_blank">00:17:11.840</a></span> | <span class="t">from word structures, or images from images. And that will form the basis, if you wanted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1037" target="_blank">00:17:17.860</a></span> | <span class="t">to go further, of doing things like going from an image to a sentence, it's called image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1043" target="_blank">00:17:23.480</a></span> | <span class="t">captioning, or going from a sentence to an image, which will start to do a phrased image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1051" target="_blank">00:17:31.120</a></span> | <span class="t">And so from there, we're going to go deeper then into computer vision to think about what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1059" target="_blank">00:17:39.800</a></span> | <span class="t">other kinds of things we can do with this idea of a pre-trained network plus a custom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1064" target="_blank">00:17:44.400</a></span> | <span class="t">head. And so we'll look at various kinds of image enhancement, like increasing the resolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1069" target="_blank">00:17:49.940</a></span> | <span class="t">of a low-res photo to guess what was missing, or adding artistic filters on top of photos,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1078" target="_blank">00:17:58.200</a></span> | <span class="t">or changing photos of forces into photos of zebras and stuff like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1084" target="_blank">00:18:04.280</a></span> | <span class="t">And then finally, that's going to bring us all the way back to bounding boxes again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1091" target="_blank">00:18:11.240</a></span> | <span class="t">And so to get there, we're going to first of all learn about segmentation, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1094" target="_blank">00:18:14.480</a></span> | <span class="t">not just figuring out where a bounding box is, but figuring out what every single pixel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1099" target="_blank">00:18:19.640</a></span> | <span class="t">in an image is part of. So this pixel is part of a person, this pixel is part of a car.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1105" target="_blank">00:18:25.640</a></span> | <span class="t">And then we're going to use that idea, particularly an idea called unet, and it turns out that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1110" target="_blank">00:18:30.640</a></span> | <span class="t">this idea from unet, we can apply to bounding boxes where it's called feature pyramids.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1116" target="_blank">00:18:36.280</a></span> | <span class="t">Everything has to have a different name in every slightly different area. And we'll use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1121" target="_blank">00:18:41.480</a></span> | <span class="t">that to hopefully get some really good results with bounding boxes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1128" target="_blank">00:18:48.240</a></span> | <span class="t">So that's kind of our path from here. So it's all going to build on each other, but take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1134" target="_blank">00:18:54.120</a></span> | <span class="t">us into lots of different areas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1137" target="_blank">00:18:57.480</a></span> | <span class="t">Now for NLP last part, we relied on a pretty great library called TorchText. But as pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1146" target="_blank">00:19:06.200</a></span> | <span class="t">great as it was, I've since then found the limitations of it too problematic to keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1153" target="_blank">00:19:13.000</a></span> | <span class="t">using it. As a lot of you complained on the forums, it's pretty damn slow. Partly because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1161" target="_blank">00:19:21.160</a></span> | <span class="t">it's not doing parallel processing, and partly it's because it doesn't remember what you did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1169" target="_blank">00:19:29.440</a></span> | <span class="t">last time and it does it all over again from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1174" target="_blank">00:19:34.920</a></span> | <span class="t">And then it's kind of hard to do fairly simple things, like a lot of you were trying to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1178" target="_blank">00:19:38.840</a></span> | <span class="t">into the toxic comment competition on Kaggle, which was a multi-label problem, and trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1184" target="_blank">00:19:44.520</a></span> | <span class="t">to do that with TorchText. I eventually got it working, but it took me like a week of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1189" target="_blank">00:19:49.600</a></span> | <span class="t">hacking away, which is kind of ridiculous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1193" target="_blank">00:19:53.000</a></span> | <span class="t">So to fix all these problems, we've created a new library called FastAI.Text. FastAI.Text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1199" target="_blank">00:19:59.800</a></span> | <span class="t">is a replacement for the combination of TorchText and FastAI.NLP. So don't use FastAI.NLP anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1210" target="_blank">00:20:10.940</a></span> | <span class="t">That's obsolete. It's slower, it's more confusing, it's less good in every way, but there's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1218" target="_blank">00:20:18.880</a></span> | <span class="t">lot of overlaps. Intentionally, a lot of the classes have the same names, a lot of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1224" target="_blank">00:20:24.280</a></span> | <span class="t">functions have the same names, but this is the non-TorchText version.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1233" target="_blank">00:20:33.480</a></span> | <span class="t">So we're going to work with IMDB again. For those of you who have forgotten, go back and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1238" target="_blank">00:20:38.440</a></span> | <span class="t">check out lesson 4. Basically this is a data set of movie reviews, and you remember we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1246" target="_blank">00:20:46.840</a></span> | <span class="t">used it to find out whether we might enjoy some Begeddon or not, and we thought probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1252" target="_blank">00:20:52.440</a></span> | <span class="t">my kind of thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1256" target="_blank">00:20:56.460</a></span> | <span class="t">So we're going to use the same data set, and by default it calls itself ACLIMDB, so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1263" target="_blank">00:21:03.080</a></span> | <span class="t">is just the raw data set that you can download. And as you can see, I'm doing from FastAI.Text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1273" target="_blank">00:21:13.560</a></span> | <span class="t">import star. There's no TorchText, and I'm not using FastAI.NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1281" target="_blank">00:21:21.160</a></span> | <span class="t">I'm going to use Pathlib as per usual. We're going to learn about what these tags are later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1287" target="_blank">00:21:27.720</a></span> | <span class="t">So you might remember the basic path for NLP is that we have to take sentences and turn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1297" target="_blank">00:21:37.360</a></span> | <span class="t">them into numbers, and there's a couple of steps to get there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1304" target="_blank">00:21:44.280</a></span> | <span class="t">So at the moment, somewhat intentionally, FastAI.Text doesn't provide that many helper functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1314" target="_blank">00:21:54.240</a></span> | <span class="t">It's really designed more to let you handle things in a fairly flexible way. So as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1320" target="_blank">00:22:00.200</a></span> | <span class="t">can see here, I wrote something called Get Texts, which goes through each thing in classes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1328" target="_blank">00:22:08.360</a></span> | <span class="t">and these are the three classes that they have in IMDB. Negative, positive, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1333" target="_blank">00:22:13.760</a></span> | <span class="t">there's another folder, unsupervised. That's stuff they haven't gotten around for labeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1337" target="_blank">00:22:17.360</a></span> | <span class="t">yet. So I'm just going to call that a class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1340" target="_blank">00:22:20.400</a></span> | <span class="t">And so I just go through each one of those classes, and then I just find every file in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1347" target="_blank">00:22:27.800</a></span> | <span class="t">that folder with that name, and I open it up and read it and chuck it into the end of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1353" target="_blank">00:22:33.080</a></span> | <span class="t">this array. And as you can see, with Pathlib it's super easy to grab stuff and pull it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1361" target="_blank">00:22:41.080</a></span> | <span class="t">in, and then the label is just whatever class I'm up to so far. So I'll go ahead and do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1369" target="_blank">00:22:49.120</a></span> | <span class="t">for the train bit, and I'll go ahead and do that for the test bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1374" target="_blank">00:22:54.200</a></span> | <span class="t">So there's 70,000 in train, 25,000 in test, 50,000 of the train ones are unsupervised.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1380" target="_blank">00:23:00.280</a></span> | <span class="t">We won't actually be able to use them when we get to the classification piece. So I actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1385" target="_blank">00:23:05.320</a></span> | <span class="t">find this much easier than the torch text approach of having lots of layers and wrappers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1392" target="_blank">00:23:12.440</a></span> | <span class="t">and stuff, because in the end reading text files is not that hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1400" target="_blank">00:23:20.480</a></span> | <span class="t">One thing that's always a good idea is to sort things randomly. It's useful to know this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1408" target="_blank">00:23:28.640</a></span> | <span class="t">simple trick for sorting things randomly, particularly when you've got multiple things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1412" target="_blank">00:23:32.000</a></span> | <span class="t">you have to sort the same way, in this case I've got labels and texts. np.random.permutation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1418" target="_blank">00:23:38.040</a></span> | <span class="t">if you give it an integer, it gives you a random list from 0 up to and not including</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1424" target="_blank">00:23:44.720</a></span> | <span class="t">the number you give it in some random order. So you can then just pass that in as an indexer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1433" target="_blank">00:23:53.480</a></span> | <span class="t">to give you a list that's sorted in that random order. So in this case it's going to sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1438" target="_blank">00:23:58.240</a></span> | <span class="t">train texts and train labels in the same random way. So it's a useful little idiom to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1448" target="_blank">00:24:08.120</a></span> | <span class="t">So now I've got my texts and my labels sorted. I can go ahead and create a data frame from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1453" target="_blank">00:24:13.240</a></span> | <span class="t">them. Why am I doing this? The reason I'm doing this is because there is a somewhat standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1462" target="_blank">00:24:22.360</a></span> | <span class="t">approach starting to appear for text classification datasets, which is to have your training set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1471" target="_blank">00:24:31.480</a></span> | <span class="t">as a CSV file with the labels first and the text of the NLP document second in a train.csv</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1485" target="_blank">00:24:45.280</a></span> | <span class="t">and a test.csv. So basically it looks like this. You've got your labels and your texts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1490" target="_blank">00:24:50.880</a></span> | <span class="t">And then a file called classes.text, which just lists the classes. I think it's somewhat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1497" target="_blank">00:24:57.920</a></span> | <span class="t">standard. In a reasonably recent academic paper, Yann LeCun and a team of researchers looked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1505" target="_blank">00:25:05.920</a></span> | <span class="t">at quite a few datasets and they used this format for all of them. And so that's what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1512" target="_blank">00:25:12.960</a></span> | <span class="t">I've started using as well for my recent paper. So what I've done is you'll find that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1520" target="_blank">00:25:20.360</a></span> | <span class="t">notebook, if you put your data into this format, the whole notebook will work every time. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1529" target="_blank">00:25:29.400</a></span> | <span class="t">rather than having a thousand different classes or formats and readers and writers and whatever,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1535" target="_blank">00:25:35.360</a></span> | <span class="t">I've just said let's just pick a standard format and your job, your code is, you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1539" target="_blank">00:25:39.880</a></span> | <span class="t">do it perfectly well, is to put it in that format which is the CSV file. The CSV files</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1546" target="_blank">00:25:46.920</a></span> | <span class="t">have no header by default.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1552" target="_blank">00:25:52.560</a></span> | <span class="t">Now you'll notice at the start here that I had two different paths. One was the classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1558" target="_blank">00:25:58.040</a></span> | <span class="t">path, one was the language model path. In NLP, you'll see LM all the time. LM means language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1565" target="_blank">00:26:05.640</a></span> | <span class="t">model in NLP. So the classification path is going to contain the information that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1574" target="_blank">00:26:14.160</a></span> | <span class="t">going to use to create a sentiment analysis model. The language model path is going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1579" target="_blank">00:26:19.000</a></span> | <span class="t">contain the information we need to create a language model. So they're a little bit different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1583" target="_blank">00:26:23.880</a></span> | <span class="t">One thing that's different is that when we create the train.csv and the classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1590" target="_blank">00:26:30.360</a></span> | <span class="t">path, we remove everything that has a label of 2 because label of 2 is unsupervised. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1600" target="_blank">00:26:40.640</a></span> | <span class="t">when we remove the unsupervised data from the classifier, we can't use it. So that means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1607" target="_blank">00:26:47.320</a></span> | <span class="t">this is going to have actually 25,000 positive, 25,000 negative. The second difference is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1612" target="_blank">00:26:52.720</a></span> | <span class="t">the labels. For the classification path, the labels are the actual labels. But for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1619" target="_blank">00:26:59.760</a></span> | <span class="t">language model, there are no labels, so we just use a bunch of zeroes. That just makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1624" target="_blank">00:27:04.920</a></span> | <span class="t">it a little bit easier because we can use a consistent data frame format or CSV format.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1632" target="_blank">00:27:12.900</a></span> | <span class="t">Now the language model, we can create our own validation set. So you've probably come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1639" target="_blank">00:27:19.620</a></span> | <span class="t">across by now sklearn.modelSelection.trainTestSplit, which is a really simple little function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1646" target="_blank">00:27:26.480</a></span> | <span class="t">that grabs a data set and randomly splits it into a training set and a validation set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1652" target="_blank">00:27:32.200</a></span> | <span class="t">according to whatever proportion you specify. So in this case, I can catenate my classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1658" target="_blank">00:27:38.680</a></span> | <span class="t">training and validation together. So it's going to be 100,000 altogether, split it by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1664" target="_blank">00:27:44.000</a></span> | <span class="t">10%, so now I've got 90,000 training, 10,000 validation for my language model. So go ahead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1671" target="_blank">00:27:51.320</a></span> | <span class="t">and save that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1672" target="_blank">00:27:52.680</a></span> | <span class="t">So that's my basic get the data in a standard format for my language model and my classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1684" target="_blank">00:28:04.240</a></span> | <span class="t">So the next thing we need to do is tokenization. Tokenization means at this stage we've got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1692" target="_blank">00:28:12.400</a></span> | <span class="t">for a document, for a movie review, we've got a big long string, and we want to put it into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1697" target="_blank">00:28:17.460</a></span> | <span class="t">a list of tokens, which are kind of a list of words, but not quite. For example, 'don't',</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1706" target="_blank">00:28:26.480</a></span> | <span class="t">we want to be 'don't', we probably want 'full stop' to be a token. So tokenization is something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1715" target="_blank">00:28:35.680</a></span> | <span class="t">that we passed off to a terrific library called Spacey, partly terrific because an Australian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1723" target="_blank">00:28:43.080</a></span> | <span class="t">wrote it and partly terrific because it's good at what it does. We've put a bit of stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1730" target="_blank">00:28:50.760</a></span> | <span class="t">on top of Spacey, but the vast majority of the work is being done by Spacey.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1735" target="_blank">00:28:55.560</a></span> | <span class="t">Before we pass it to Spacey, I've written this simple fixup function, which is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1743" target="_blank">00:29:03.440</a></span> | <span class="t">each time I looked at a different dataset, and I've looked at about a dozen in building</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1746" target="_blank">00:29:06.880</a></span> | <span class="t">this, everyone had different weird things that needed to be replaced. Here are all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1755" target="_blank">00:29:15.440</a></span> | <span class="t">ones I've come up with so far. Hopefully this will help you out as well. So I HTML and escape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1764" target="_blank">00:29:24.320</a></span> | <span class="t">all the entities, and then there's a bunch more things I replace. Have a look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1769" target="_blank">00:29:29.320</a></span> | <span class="t">result of running this on text that you put in and make sure there's not more weird tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1774" target="_blank">00:29:34.720</a></span> | <span class="t">in there. It's amazing how many weird things people do to text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1781" target="_blank">00:29:41.120</a></span> | <span class="t">So basically I've got this function called getAll, which is going to go ahead and call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1787" target="_blank">00:29:47.800</a></span> | <span class="t">getTexts, and text is going to go ahead and do a few things, one of which is to apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1792" target="_blank">00:29:52.520</a></span> | <span class="t">that fixup that we just mentioned. So let's kind of look through this because there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1798" target="_blank">00:29:58.840</a></span> | <span class="t">some interesting things to point out. So I've got to use pandas to open our train.csv from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1805" target="_blank">00:30:05.000</a></span> | <span class="t">the language model path, but I'm passing in an extra parameter you may not have seen before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1809" target="_blank">00:30:09.840</a></span> | <span class="t">called chunksites. Python and pandas can both be pretty inefficient when it comes to storing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1819" target="_blank">00:30:19.080</a></span> | <span class="t">and using text data. And so you'll see that very few people in NLP are working with large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1831" target="_blank">00:30:31.000</a></span> | <span class="t">corpuses and I think part of the reason is that traditional tools have just made it really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1836" target="_blank">00:30:36.960</a></span> | <span class="t">difficult - you run out of memory all the time. So this process I'm showing you today</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1843" target="_blank">00:30:43.680</a></span> | <span class="t">I have used on corpuses of over a billion words successfully using this exact code. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1850" target="_blank">00:30:50.720</a></span> | <span class="t">one of the simple tricks is to use this thing called chunksites with pandas. What that means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1855" target="_blank">00:30:55.740</a></span> | <span class="t">is that pandas does not return a data frame, but it returns an iterator that we can iterate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1861" target="_blank">00:31:01.760</a></span> | <span class="t">through chunks of a data frame. And so that's why I don't say "tok_train = get_texts" but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1875" target="_blank">00:31:15.920</a></span> | <span class="t">instead I call "get_all" which loops through the data frame. But actually what it's really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1881" target="_blank">00:31:21.240</a></span> | <span class="t">doing is it's looping through chunks of the data frame. So each of those chunks is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1887" target="_blank">00:31:27.280</a></span> | <span class="t">a data frame representing a subset of the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1891" target="_blank">00:31:31.940</a></span> | <span class="t">"When I'm working with NLP data, many times I come across data with foreign text or characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1897" target="_blank">00:31:37.900</a></span> | <span class="t">Is it better to discard them or keep them?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1900" target="_blank">00:31:40.000</a></span> | <span class="t">No, no, definitely keep them. And this whole process is Unicode, and I've actually used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1905" target="_blank">00:31:45.800</a></span> | <span class="t">this on Chinese text. This is designed to work on pretty much anything. In general,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1915" target="_blank">00:31:55.920</a></span> | <span class="t">most of the time it's not a good idea to remove anything. Old-fashioned NLP approaches tend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1922" target="_blank">00:32:02.640</a></span> | <span class="t">to do all this lemmatization and all these normalization steps to get rid of lowercase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1928" target="_blank">00:32:08.640</a></span> | <span class="t">everything blah blah blah. But that's throwing away information which you don't know ahead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1934" target="_blank">00:32:14.480</a></span> | <span class="t">of time whether it's useful or not. So don't throw away information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1940" target="_blank">00:32:20.720</a></span> | <span class="t">So we go through each chunk, each of which is a data frame, and we call get_texts. get_texts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1946" target="_blank">00:32:26.560</a></span> | <span class="t">is going to grab the labels and make them into ints. It's going to grab then the texts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1957" target="_blank">00:32:37.960</a></span> | <span class="t">And I'll point out a couple of things. The first is that before we include the text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1962" target="_blank">00:32:42.320</a></span> | <span class="t">we have this beginning of stream token, which you might remember we used way back up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1969" target="_blank">00:32:49.800</a></span> | <span class="t">There's nothing special about these particular strings of letters, they're just ones I figured</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1973" target="_blank">00:32:53.520</a></span> | <span class="t">don't appear in normal texts very often. So every text is going to start with XBOS. Why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1981" target="_blank">00:33:01.940</a></span> | <span class="t">is that? Because it's often really useful for your model to know when a new text is starting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1988" target="_blank">00:33:08.980</a></span> | <span class="t">For example, if it's a language model, you're going to concatenate all the text together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1994" target="_blank">00:33:14.280</a></span> | <span class="t">and so it'd be really helpful for it to know this article is finished and a new one started,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=1998" target="_blank">00:33:18.200</a></span> | <span class="t">so I should probably forget some of that context now. Ditto is quite often texts have multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2007" target="_blank">00:33:27.880</a></span> | <span class="t">fields like a title, an abstract, and then the main document. And so by the same token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2012" target="_blank">00:33:32.880</a></span> | <span class="t">I've got this thing here which lets us actually have multiple fields in our CSP. So this process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2020" target="_blank">00:33:40.080</a></span> | <span class="t">is designed to be very flexible. And again, at the start of each one, we put a special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2024" target="_blank">00:33:44.200</a></span> | <span class="t">field starts here token, followed by the number of the field that's starting here for as many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2029" target="_blank">00:33:49.920</a></span> | <span class="t">fields as we have. Then we apply our fix up to it, and then most importantly we tokenize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2036" target="_blank">00:33:56.400</a></span> | <span class="t">it and we tokenize it by doing a process or multiprocessor. So tokenizing tends to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2050" target="_blank">00:34:10.440</a></span> | <span class="t">pretty slow, but we've all got multiple cores in our machines now and some of the better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2055" target="_blank">00:34:15.280</a></span> | <span class="t">machines on AWS and stuff can have dozens of cores. Here on our university computer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2061" target="_blank">00:34:21.200</a></span> | <span class="t">we've got 56 cores. So spaCy is not very amenable to multiprocessing, but I finally figured</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2071" target="_blank">00:34:31.800</a></span> | <span class="t">out how to get it to work. And the good news is it's all wrapped up in this one function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2076" target="_blank">00:34:36.500</a></span> | <span class="t">now. And so all you need to pass to that function is a list of things to tokenize, which each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2083" target="_blank">00:34:43.320</a></span> | <span class="t">part of that list will be tokenized on a different core. And so I've also created this function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2088" target="_blank">00:34:48.520</a></span> | <span class="t">called partition by cores, which takes a list and splits it into sub-lists. The number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2094" target="_blank">00:34:54.200</a></span> | <span class="t">sub-lists is the number of cores that you have in your computer. So on my machine, without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2103" target="_blank">00:35:03.280</a></span> | <span class="t">multiprocessing, this takes about an hour and a half, and with multiprocessing it takes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2109" target="_blank">00:35:09.280</a></span> | <span class="t">about two minutes. So it's a really handy thing to have. And now that this code's here, feel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2116" target="_blank">00:35:16.080</a></span> | <span class="t">free to look inside it and take advantage of it through your own stuff. Remember, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2121" target="_blank">00:35:21.880</a></span> | <span class="t">all have multiple cores even in our laptops, and very few things in Python take advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2129" target="_blank">00:35:29.040</a></span> | <span class="t">of it unless you make a bit of an effort to make it work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2135" target="_blank">00:35:35.040</a></span> | <span class="t">So there's a couple of tricks to get things working quickly and reliably. As it runs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2139" target="_blank">00:35:39.560</a></span> | <span class="t">it prints out how it's going. And so here's the result of the end. Beginning of stream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2147" target="_blank">00:35:47.080</a></span> | <span class="t">token, beginning of field number one token, here's the tokenized text. You'll see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2153" target="_blank">00:35:53.120</a></span> | <span class="t">the punctuation is on the whole, now a separate token. You'll see there's a few interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2162" target="_blank">00:36:02.240</a></span> | <span class="t">little things. One is this. What's this? T-up, MGM. Well, MGM was originally capitalized,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2171" target="_blank">00:36:11.960</a></span> | <span class="t">but the interesting thing is that normally people either lowercase everything or they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2178" target="_blank">00:36:18.120</a></span> | <span class="t">leave the case as is. Now if you leave the case as is, then screw you, or caps, and screw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2186" target="_blank">00:36:26.920</a></span> | <span class="t">you, lowercase, are two totally different sets of tokens that have to be learned from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2192" target="_blank">00:36:32.280</a></span> | <span class="t">scratch. Or if you lowercase them all, then there's no difference at all between screw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2197" target="_blank">00:36:37.720</a></span> | <span class="t">you and screw you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2201" target="_blank">00:36:41.200</a></span> | <span class="t">So how do you fix this so that you both get the semantic impact of "I'm shouting now!"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2210" target="_blank">00:36:50.240</a></span> | <span class="t">but not have every single word have to learn the shouted version versus the normal version.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2215" target="_blank">00:36:55.040</a></span> | <span class="t">And so the idea I came up with, and I'm sure other people have done this too, is to come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2219" target="_blank">00:36:59.440</a></span> | <span class="t">up with a unique token to mean the next thing is all uppercase. So then I lowercase it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2226" target="_blank">00:37:06.600</a></span> | <span class="t">so now whatever used to be uppercase is now lowercase, it's just one token, and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2230" target="_blank">00:37:10.240</a></span> | <span class="t">can learn the semantic meaning of all uppercase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2234" target="_blank">00:37:14.480</a></span> | <span class="t">And so I've done a similar thing. If you've got 29 exclamation marks in a row, we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2239" target="_blank">00:37:19.280</a></span> | <span class="t">learn a separate token for 29 exclamation marks. Instead I put in a special token for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2244" target="_blank">00:37:24.840</a></span> | <span class="t">the next thing repeats lots of times, and then I put the number 29, and then I put the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2250" target="_blank">00:37:30.120</a></span> | <span class="t">exclamation mark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2252" target="_blank">00:37:32.000</a></span> | <span class="t">And so there's a few little tricks like that, and if you're interested in LP, have a look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2256" target="_blank">00:37:36.120</a></span> | <span class="t">at the code for Tokenizer for these little tricks that I've added in because some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2260" target="_blank">00:37:40.560</a></span> | <span class="t">them are kind of fun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2265" target="_blank">00:37:45.440</a></span> | <span class="t">So the nice thing with doing things this way is we can now just np.save that and load it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2272" target="_blank">00:37:52.160</a></span> | <span class="t">back up later. We don't have to recalculate all this stuff each time like we tend to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2277" target="_blank">00:37:57.600</a></span> | <span class="t">to do with TorchText or a lot of other libraries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2282" target="_blank">00:38:02.160</a></span> | <span class="t">So we've now got it tokenized. The next thing we need to do is turn it into numbers, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2288" target="_blank">00:38:08.600</a></span> | <span class="t">we call numericalizing it. And the way we numericalize it is very simple. We make a list of all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2294" target="_blank">00:38:14.560</a></span> | <span class="t">words that appear in some order, and then we replace every word with its index into that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2300" target="_blank">00:38:20.000</a></span> | <span class="t">list. The list of all the tokens that appear, we call the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2309" target="_blank">00:38:29.160</a></span> | <span class="t">So here's an example of some of the vocabulary. The counter class in Python is very handy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2314" target="_blank">00:38:34.200</a></span> | <span class="t">for this. It basically gives us a list of unique items and their counts. So here are the 25</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2322" target="_blank">00:38:42.960</a></span> | <span class="t">most common things in the vocabulary. You can see there are things like apostrophe s and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2328" target="_blank">00:38:48.480</a></span> | <span class="t">double quote and end of paragraph, and also stuff like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2334" target="_blank">00:38:54.720</a></span> | <span class="t">Generally speaking, we don't want every unique token in our vocabulary. If it doesn't appear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2341" target="_blank">00:39:01.320</a></span> | <span class="t">at least two times, then it might just be a spelling mistake or a word. We can't learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2346" target="_blank">00:39:06.560</a></span> | <span class="t">anything about it if it doesn't appear that often. Also the stuff that we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2351" target="_blank">00:39:11.120</a></span> | <span class="t">be learning about at least so far on this part gets a bit clunky once you've got a vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2356" target="_blank">00:39:16.680</a></span> | <span class="t">bigger than 60,000. Time permitting, we may look at some work I've been doing recently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2362" target="_blank">00:39:22.520</a></span> | <span class="t">on handling larger vocabularies, otherwise that might have to come in a future course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2368" target="_blank">00:39:28.600</a></span> | <span class="t">But actually for classification, I've discovered that doing more than about 60,000 words doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2372" target="_blank">00:39:32.920</a></span> | <span class="t">seem to help anyway. So we're going to limit our vocabulary to 60,000 words, things that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2378" target="_blank">00:39:38.560</a></span> | <span class="t">appear at least twice. So here's a simple way to do that. Use that dot most common, pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2385" target="_blank">00:39:45.080</a></span> | <span class="t">in the max_vocab size. That'll sort it by the frequency, by the way. And if it appears</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2392" target="_blank">00:39:52.360</a></span> | <span class="t">less often than a minimum frequency, then don't bother with it at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2397" target="_blank">00:39:57.000</a></span> | <span class="t">So that gives us i to s. That's the same name that torch text used. Remember it means int</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2402" target="_blank">00:40:02.920</a></span> | <span class="t">to string. So this is just the list of the unique tokens in the vocab. I'm going to insert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2410" target="_blank">00:40:10.200</a></span> | <span class="t">two more tokens, a token for unknown, a vocab item for unknown, and a vocab item for padding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2419" target="_blank">00:40:19.960</a></span> | <span class="t">Then we can create the dictionary which goes in the opposite direction, so string to int.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2426" target="_blank">00:40:26.760</a></span> | <span class="t">And that won't cover everything because we intentionally truncated it down to 60,000 words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2433" target="_blank">00:40:33.440</a></span> | <span class="t">And so if we come across something that's not in the dictionary, we want to replace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2436" target="_blank">00:40:36.960</a></span> | <span class="t">it with 0 for unknown, so we can use a default dict for that, with a lambda function that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2443" target="_blank">00:40:43.600</a></span> | <span class="t">always returns 0. So you can see all these things we're using that keep coming back up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2451" target="_blank">00:40:51.480</a></span> | <span class="t">So now that we've got our s to i dictionary defined, we can then just call that for every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2456" target="_blank">00:40:56.480</a></span> | <span class="t">word for every sentence. And so there's our numericalized version, and there it is. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2466" target="_blank">00:41:06.920</a></span> | <span class="t">so of course the nice thing is again, we can save that step as well. So each time we get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2472" target="_blank">00:41:12.840</a></span> | <span class="t">to another step, we can save it. And these are not very big files. Compared to what you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2477" target="_blank">00:41:17.880</a></span> | <span class="t">get used to with images, text is generally pretty small. Very important to also save</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2488" target="_blank">00:41:28.560</a></span> | <span class="t">that vocabulary. Because this list of numbers means nothing, unless you know what each number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2496" target="_blank">00:41:36.160</a></span> | <span class="t">refers to, and that's what I2S tells you. So you save those three things, and then later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2502" target="_blank">00:41:42.840</a></span> | <span class="t">on you can load them back up. So now our vocab size is 60,002, and our training language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2512" target="_blank">00:41:52.800</a></span> | <span class="t">model has 90,000 documents in it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2520" target="_blank">00:42:00.120</a></span> | <span class="t">So that's the preprocessing you do. We can probably wrap a little bit more of that in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2525" target="_blank">00:42:05.520</a></span> | <span class="t">little utility functions if we want to, but it's all pretty straightforward, and basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2530" target="_blank">00:42:10.400</a></span> | <span class="t">that exact code will work for any dataset you have once you've got it in that CSV format.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2538" target="_blank">00:42:18.240</a></span> | <span class="t">So here is a kind of a new insight that's not new at all, which is that we'd like to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2551" target="_blank">00:42:31.280</a></span> | <span class="t">pre-train something. Like we know from lesson 4 that if we pre-train our classifier by first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2559" target="_blank">00:42:39.680</a></span> | <span class="t">creating a language model, and then fine-tuning that as a classifier, that was helpful. Remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2565" target="_blank">00:42:45.520</a></span> | <span class="t">it actually got us a new state-of-the-art result. We got the best IMDB classifier result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2570" target="_blank">00:42:50.720</a></span> | <span class="t">that had ever been published. But quite a bit. Well, we're not going far enough though,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2578" target="_blank">00:42:58.040</a></span> | <span class="t">because IMDB movie reviews are not that different to any other English document compared to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2592" target="_blank">00:43:12.000</a></span> | <span class="t">how different they are to a random string or even to a Chinese document. So just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2599" target="_blank">00:43:19.940</a></span> | <span class="t">ImageNet allowed us to train things that recognize stuff that kind of looks like pictures, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2606" target="_blank">00:43:26.760</a></span> | <span class="t">we could use it on stuff that was nothing to do with ImageNet, like satellite images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2610" target="_blank">00:43:30.680</a></span> | <span class="t">Why don't we train a language model that's just like good at English, and then fine-tune</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2617" target="_blank">00:43:37.400</a></span> | <span class="t">it to be good at movie reviews? So this basic insight led me to try building a language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2627" target="_blank">00:43:47.800</a></span> | <span class="t">model on Wikipedia. So my friend Stephen Meridy has already processed Wikipedia, found a subset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2638" target="_blank">00:43:58.920</a></span> | <span class="t">of nearly the most of it, but throwing away the stupid little articles, and he calls that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2648" target="_blank">00:44:08.240</a></span> | <span class="t">Wikitex 103. So I grabbed Wikitex 103 and I trained a language model on it. I used exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2656" target="_blank">00:44:16.640</a></span> | <span class="t">the same approach I'm about to show you for training an IMDB language model, but instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2661" target="_blank">00:44:21.760</a></span> | <span class="t">I trained a Wikitex 103 language model. And then I saved it and I've made it available</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2669" target="_blank">00:44:29.640</a></span> | <span class="t">for anybody who wants to use it at this URL. So this is not a URL for Wikitex 103, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2676" target="_blank">00:44:36.920</a></span> | <span class="t">documents, this is the Wikitex 103, the language model. So the idea now is let's train an IMDB</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2686" target="_blank">00:44:46.160</a></span> | <span class="t">language model which starts with these words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2690" target="_blank">00:44:50.600</a></span> | <span class="t">Now hopefully to you folks, this is an extremely obvious, extremely non-controversial idea because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2698" target="_blank">00:44:58.720</a></span> | <span class="t">it's basically what we've done in nearly every class so far. But when I first mentioned this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2709" target="_blank">00:45:09.560</a></span> | <span class="t">to people in the NLP community, I guess June/July of last year, there couldn't have been less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2718" target="_blank">00:45:18.920</a></span> | <span class="t">interest. I asked on Twitter, where a lot of the top Twitter researchers are people that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2724" target="_blank">00:45:24.960</a></span> | <span class="t">I follow and they follow me back, I was like "hey, what if we pre-trained a general language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2729" target="_blank">00:45:29.800</a></span> | <span class="t">model?" and they're like "no, all language is different, you can't do that" or "I don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2736" target="_blank">00:45:36.080</a></span> | <span class="t">know why you would bother anyway, I've talked to people at conferences and I'm pretty sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2743" target="_blank">00:45:43.280</a></span> | <span class="t">people have tried that and it's stupid." There was just this weird straight past. I guess</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2756" target="_blank">00:45:56.000</a></span> | <span class="t">because I am arrogant and I ignored them even though they know much more about NLP than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2763" target="_blank">00:46:03.960</a></span> | <span class="t">I do and just tried it anyway and let me show you what happened.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2770" target="_blank">00:46:10.400</a></span> | <span class="t">So here's how we do it. Grab the wiki text models, and if you use wget -r it'll actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2781" target="_blank">00:46:21.280</a></span> | <span class="t">recursively grab the whole directory, it's got a few things in it. We need to make sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2787" target="_blank">00:46:27.480</a></span> | <span class="t">that our language model has exactly the same embedding size, number of hidden and number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2792" target="_blank">00:46:32.900</a></span> | <span class="t">of layers as my wiki text one did, otherwise you can't load the weights in. So here's our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2801" target="_blank">00:46:41.800</a></span> | <span class="t">pre-trained path, here's our pre-trained language model path, let's go ahead and torch.load in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2808" target="_blank">00:46:48.400</a></span> | <span class="t">those weights from the forward wiki text 103 model. We don't normally use torch.load, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2818" target="_blank">00:46:58.440</a></span> | <span class="t">that's the PyTorch way of grabbing a file. And it basically gives you a dictionary containing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2827" target="_blank">00:47:07.080</a></span> | <span class="t">the name of the layer and a tensor of those weights or an array of those weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2834" target="_blank">00:47:14.760</a></span> | <span class="t">Now here's the problem, that wiki text language model was built with a certain vocabulary which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2841" target="_blank">00:47:21.720</a></span> | <span class="t">was not the same as this one was built on. So my number 40 was not the same as wiki text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2847" target="_blank">00:47:27.680</a></span> | <span class="t">103 models number 40. So we need to map one to the other. That's very, very simple because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2855" target="_blank">00:47:35.120</a></span> | <span class="t">luckily I saved the i2s for the wiki text vocab. So here's the list of what each word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2864" target="_blank">00:47:44.280</a></span> | <span class="t">is when I trained the wiki text 103 model, and so we can do the same default dict trick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2870" target="_blank">00:47:50.520</a></span> | <span class="t">to map it in reverse, and I'm going to use -1 to mean that it's not in the wiki text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2876" target="_blank">00:47:56.520</a></span> | <span class="t">dictionary. And so now I can just say my new set of weights is just a whole bunch of zeros</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2885" target="_blank">00:48:05.000</a></span> | <span class="t">with vocab size by embedding size, so we're going to create an embedding matrix. I'm then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2890" target="_blank">00:48:10.480</a></span> | <span class="t">going to go through every one of the words in my IMDB vocabulary. I'm going to look it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2897" target="_blank">00:48:17.200</a></span> | <span class="t">up in S to i2, so string to int for the wiki text 103 vocabulary, and see if that's words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2904" target="_blank">00:48:24.280</a></span> | <span class="t">there. And if that is word there, then I'm not going to get this -1, so r will be greater</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2911" target="_blank">00:48:31.520</a></span> | <span class="t">than or equal to 0. So in that case I will just set that row of the embedding matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2916" target="_blank">00:48:36.800</a></span> | <span class="t">to the weight that I just looked at, which was stored inside this named element. So these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2925" target="_blank">00:48:45.520</a></span> | <span class="t">names, you can just look at this dictionary and it's pretty obvious what each name corresponds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2931" target="_blank">00:48:51.360</a></span> | <span class="t">to because it looks very similar to the names that you gave it when you set up your module.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2935" target="_blank">00:48:55.440</a></span> | <span class="t">So here are the encoder weights. So grab it from the encoder weights. If I don't find it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2945" target="_blank">00:49:05.400</a></span> | <span class="t">then I will use the row mean. In other words, here is the average embedding weight across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2952" target="_blank">00:49:12.400</a></span> | <span class="t">all of the wiki text 103 things. So that's pretty simple, so I'm going to end up with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2958" target="_blank">00:49:18.560</a></span> | <span class="t">an embedding matrix for every word that's in both my vocabulary for IMDB and the wiki</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2964" target="_blank">00:49:24.040</a></span> | <span class="t">text 103 vocabulary. I will use the wiki text 103's embedding matrix weights for anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2970" target="_blank">00:49:30.240</a></span> | <span class="t">else. I will just use whatever was the average weight from the wiki text 103 embedding matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2976" target="_blank">00:49:36.080</a></span> | <span class="t">And then I'll go ahead and I will replace the encoder weights with that turn into a tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2983" target="_blank">00:49:43.600</a></span> | <span class="t">We haven't talked much about weight tying, we might do so later, but basically the decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2988" target="_blank">00:49:48.500</a></span> | <span class="t">so the thing that turns the final prediction back into a word, uses exactly the same weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=2996" target="_blank">00:49:56.380</a></span> | <span class="t">so I pop it there as well. And then there's a bit of a weird thing with how we do embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3001" target="_blank">00:50:01.600</a></span> | <span class="t">dropout that ends up with a whole separate copy of them for a reason that doesn't matter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3006" target="_blank">00:50:06.360</a></span> | <span class="t">much. So we just pop the weights back where they need to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3009" target="_blank">00:50:09.960</a></span> | <span class="t">So this is now something that a dictionary we can now, or a set of torch state which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3016" target="_blank">00:50:16.920</a></span> | <span class="t">we can load in. So let's go ahead and create our language model. And so the basic approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3025" target="_blank">00:50:25.240</a></span> | <span class="t">we're going to use, and I'm going to look at this in more detail in a moment, but the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3027" target="_blank">00:50:27.920</a></span> | <span class="t">basic approach we're going to use is I'm going to concatenate all of the documents together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3038" target="_blank">00:50:38.000</a></span> | <span class="t">into a single list of tokens of length 24.998 million.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3047" target="_blank">00:50:47.260</a></span> | <span class="t">So that's going to be what I pass in as my training set. So the language model, we basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3054" target="_blank">00:50:54.720</a></span> | <span class="t">just take all our documents and just concatenate them back to back. And we're going to be continuously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3059" target="_blank">00:50:59.280</a></span> | <span class="t">trying to predict what's the next word after these words. And we'll look at these details</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3066" target="_blank">00:51:06.600</a></span> | <span class="t">in a moment. I'm going to set up a whole bunch of dropout. We'll look at that in detail in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3071" target="_blank">00:51:11.280</a></span> | <span class="t">a moment. Once we've got a model data object, we can then grab the model from it. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3077" target="_blank">00:51:17.680</a></span> | <span class="t">going to give us a learner. And then as per usual, we can call learner.fit. So we first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3087" target="_blank">00:51:27.280</a></span> | <span class="t">of all, as per usual, just do a single epoch on the last layer just to get that okay. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3094" target="_blank">00:51:34.320</a></span> | <span class="t">the way I've set it up is the last layer is actually the embedding weights. Because that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3098" target="_blank">00:51:38.680</a></span> | <span class="t">obviously the thing that's going to be the most wrong, because a lot of those embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3102" target="_blank">00:51:42.020</a></span> | <span class="t">weights didn't even exist in the vocab, so we're just going to train a single epoch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3107" target="_blank">00:51:47.200</a></span> | <span class="t">just the embedding weights. And then we'll start doing a few epochs of the full model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3114" target="_blank">00:51:54.200</a></span> | <span class="t">And so how is that looking? Well here's lesson 4, which was our academic world's best ever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3122" target="_blank">00:52:02.920</a></span> | <span class="t">result. And after 14 epochs we had a 4.23 loss. Here after 1 epoch we have a 4.12 loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3139" target="_blank">00:52:19.800</a></span> | <span class="t">So by pre-training on Wikitext 103, in fact let's go and have a look, we kept training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3146" target="_blank">00:52:26.720</a></span> | <span class="t">and training at a different rate. Eventually we got to 4.16. So by pre-training on Wikitext</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3152" target="_blank">00:52:32.400</a></span> | <span class="t">103 we have a better loss after 1 epoch than the best loss we got for the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3158" target="_blank">00:52:38.560</a></span> | <span class="t">otherwise. Yes, Rachel?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3162" target="_blank">00:52:42.200</a></span> | <span class="t">What is the Wikitext 103 model? Is it AWD LSTM again?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3167" target="_blank">00:52:47.320</a></span> | <span class="t">Yeah and we're about to dig into that. The way I trained it was literally the same lines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3174" target="_blank">00:52:54.120</a></span> | <span class="t">of code that you see here, but without pre-training it on Wikitext 103.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3180" target="_blank">00:53:00.760</a></span> | <span class="t">So let's take a 10-minute break, come back at 7.40 and we'll dig in and have a look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3186" target="_blank">00:53:06.760</a></span> | <span class="t">these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3188" target="_blank">00:53:08.720</a></span> | <span class="t">Ok welcome back. Before we go back into language models and NLP classifiers, a quick discussion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3197" target="_blank">00:53:17.280</a></span> | <span class="t">about something pretty new at the moment which is the FastAI doc project. So the goal of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3203" target="_blank">00:53:23.200</a></span> | <span class="t">the FastAI doc project is to create documentation that makes readers say "Wow, that's the most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3210" target="_blank">00:53:30.320</a></span> | <span class="t">fantastic documentation I've ever read." And so we have some specific ideas about how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3217" target="_blank">00:53:37.440</a></span> | <span class="t">do that, but it's the same kind of idea of top-down, thoughtful, take-full advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3225" target="_blank">00:53:45.800</a></span> | <span class="t">of the medium approach, interactive, experimental code first that we're all familiar with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3234" target="_blank">00:53:54.040</a></span> | <span class="t">If you're interested in getting involved, the basic approach you can see in the docs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3241" target="_blank">00:54:01.180</a></span> | <span class="t">directory. So this is the readme in the docs directory. In there there is, amongst other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3249" target="_blank">00:54:09.600</a></span> | <span class="t">things, a transforms_template.adoc. What the hell is adoc? Adoc is ASCII doc. How many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3257" target="_blank">00:54:17.600</a></span> | <span class="t">people here have come across ASCII doc? That's awesome. People are laughing because there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3265" target="_blank">00:54:25.280</a></span> | <span class="t">one hand up and it's somebody who was in our study group today who talked to me about ASCII</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3269" target="_blank">00:54:29.560</a></span> | <span class="t">doc. ASCII doc is the most amazing project. It's like Markdown, but it's like what Markdown</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3276" target="_blank">00:54:36.280</a></span> | <span class="t">needs to be to create actual books, and a lot of actual books are written in ASCII doc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3283" target="_blank">00:54:43.280</a></span> | <span class="t">And so it's as easy to use as Markdown, but there's way more cool stuff you can do with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3288" target="_blank">00:54:48.440</a></span> | <span class="t">it. In fact, here is an ASCII doc file here, and as you'll see it looks very normal. There's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3293" target="_blank">00:54:53.720</a></span> | <span class="t">headings and this is pre-formatted text, and there's lists and whatever else. It looks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3305" target="_blank">00:55:05.800</a></span> | <span class="t">pretty standard, and actually I'll show you a more complete ASCII doc thing, a more standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3313" target="_blank">00:55:13.840</a></span> | <span class="t">ASCII doc thing. But you can do stuff like say put a table of contents here please. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3320" target="_blank">00:55:20.880</a></span> | <span class="t">can say colon colon means put a definition list here please. Plus means this is a continuation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3328" target="_blank">00:55:28.780</a></span> | <span class="t">of the previous list item. So there's just little things that you can do which are super</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3334" target="_blank">00:55:34.600</a></span> | <span class="t">handy or make it slightly smaller than everything else. So it's like turbocharged Markdown.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3343" target="_blank">00:55:43.800</a></span> | <span class="t">And so this ASCII doc creates this HTML. And I didn't add any CSS or do anything myself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3352" target="_blank">00:55:52.280</a></span> | <span class="t">We literally started this project like 4 hours ago. So this is like just an example basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3358" target="_blank">00:55:58.480</a></span> | <span class="t">And so you can see we've got a table of contents, we can jump straight to here, we've got a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3365" target="_blank">00:56:05.920</a></span> | <span class="t">cross-reference we can click on to jump straight to the cross-reference. Each method comes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3371" target="_blank">00:56:11.960</a></span> | <span class="t">along with its details and so on and so forth. And to make things even easier, rather than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3378" target="_blank">00:56:18.380</a></span> | <span class="t">having to know that the argument list is meant to be smaller than the main part, or how do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3385" target="_blank">00:56:25.980</a></span> | <span class="t">you create a cross-reference, or how are you meant to format the arguments to the method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3392" target="_blank">00:56:32.280</a></span> | <span class="t">name and list out each one of its arguments, we've created a special template where you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3398" target="_blank">00:56:38.880</a></span> | <span class="t">can just write various stuff in curly brackets like "please put the arguments here, and here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3403" target="_blank">00:56:43.800</a></span> | <span class="t">is an example of one argument, and here is a cross-reference, and here is a method," and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3409" target="_blank">00:56:49.400</a></span> | <span class="t">so forth. So we're in the process of documenting the documentation template that there's basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3415" target="_blank">00:56:55.440</a></span> | <span class="t">like 5 or 6 of these little curly bracket things you'll need to learn. But for you to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3419" target="_blank">00:56:59.760</a></span> | <span class="t">create a documentation of a class or a method, you can just copy one that's already there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3425" target="_blank">00:57:05.680</a></span> | <span class="t">and so the idea is we're going to have, it'll almost be like a book. There'll be tables</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3432" target="_blank">00:57:12.320</a></span> | <span class="t">and pictures and little video segments and hyperlink throughout and all that stuff. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3441" target="_blank">00:57:21.320</a></span> | <span class="t">might be wondering what about docstrings, but actually I don't know if you've noticed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3445" target="_blank">00:57:25.760</a></span> | <span class="t">but if you look at the Python standard library and look at the docstring for example for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3451" target="_blank">00:57:31.320</a></span> | <span class="t">regex compile, it's a single line. Nearly every docstring in Python is a single line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3458" target="_blank">00:57:38.040</a></span> | <span class="t">And Python then does exactly this. They have a website containing the documentation that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3463" target="_blank">00:57:43.080</a></span> | <span class="t">says like "Hey, this is what regular expressions are and this is what you need to know about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3466" target="_blank">00:57:46.940</a></span> | <span class="t">them and if you want them to go faster, you'll need to use compile and here's lots of information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3471" target="_blank">00:57:51.040</a></span> | <span class="t">about compile and here's the examples." It's not in the docstring. And that's how we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3475" target="_blank">00:57:55.840</a></span> | <span class="t">doing it as well. Our docstrings will be one line unless you need two sometimes. It's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3483" target="_blank">00:58:03.640</a></span> | <span class="t">to be very similar to Python, but even better. So everybody is welcome to help contribute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3491" target="_blank">00:58:11.920</a></span> | <span class="t">to the documentation and hopefully by the time you're watching this on the MOOC, it'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3496" target="_blank">00:58:16.640</a></span> | <span class="t">be recently fleshed out and we'll try to keep a list of things to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3506" target="_blank">00:58:26.560</a></span> | <span class="t">So I'm going to do one first. So one question that came up in the break was how does this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3515" target="_blank">00:58:35.440</a></span> | <span class="t">compare to Word2Vec? And this is actually a great thing for you to spend time thinking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3521" target="_blank">00:58:41.440</a></span> | <span class="t">about during the week is how does this compare to Word2Vec. I'll give you the summary now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3526" target="_blank">00:58:46.360</a></span> | <span class="t">but it's a very important conceptual difference. The main conceptual difference is, what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3531" target="_blank">00:58:51.320</a></span> | <span class="t">Word2Vec? Word2Vec is a single embedding matrix. Each word has a vector and that's it. So in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3540" target="_blank">00:59:00.520</a></span> | <span class="t">other words, it's a single layer from a pre-trained model and specifically that layer is the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3548" target="_blank">00:59:08.440</a></span> | <span class="t">layer. And also specifically that pre-trained model is a linear model that is pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3556" target="_blank">00:59:16.960</a></span> | <span class="t">on something called a co-occurrence matrix. So we have no particular reason to believe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3562" target="_blank">00:59:22.600</a></span> | <span class="t">that this model has learned anything much about the English language or that it has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3567" target="_blank">00:59:27.040</a></span> | <span class="t">any particular capabilities because it's just a single linear layer and that's it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3574" target="_blank">00:59:34.320</a></span> | <span class="t">So what's this Wikitex 103 model? It's a language model. It has a 400-dimensional embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3585" target="_blank">00:59:45.200</a></span> | <span class="t">matrix, 3 hidden layers with 1,150 activations per layer, and regularization and all of that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3597" target="_blank">00:59:57.560</a></span> | <span class="t">stuff. Tired input output, matrixes, it's basically a state-of-the-art AWD. So what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3605" target="_blank">01:00:05.920</a></span> | <span class="t">the difference between a single layer of a single linear model versus a three-layer recurrent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3614" target="_blank">01:00:14.800</a></span> | <span class="t">neural network? Everything. They're very different levels of capability. And so you'll see when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3622" target="_blank">01:00:22.200</a></span> | <span class="t">you try using a pre-trained language model versus a Word2vec layer, you'll get very,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3629" target="_blank">01:00:29.240</a></span> | <span class="t">very different results for the vast majority of tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3633" target="_blank">01:00:33.360</a></span> | <span class="t">What if the NumPy array does not fit in memory? Is it possible to write a PyTorch data loader</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3638" target="_blank">01:00:38.180</a></span> | <span class="t">directly from a large CSV file?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3642" target="_blank">01:00:42.440</a></span> | <span class="t">It almost certainly won't come up, so I'm not going to spend time on it. These things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3646" target="_blank">01:00:46.200</a></span> | <span class="t">are tiny. They're just ints. Think about how many ints you would need to run out of memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3652" target="_blank">01:00:52.680</a></span> | <span class="t">It's not going to happen. They don't have to fit in GPU memory, just in your memory. I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3657" target="_blank">01:00:57.880</a></span> | <span class="t">actually done another Wikipedia model, which I called GigaWiki, which was on all of Wikipedia,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3666" target="_blank">01:01:06.680</a></span> | <span class="t">and even that easily fits in memory. The reason I'm not using it is because it turned out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3670" target="_blank">01:01:10.480</a></span> | <span class="t">not to really help very much versus Wikitex 103, but I've built a bigger model than anybody</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3677" target="_blank">01:01:17.800</a></span> | <span class="t">else I found in the academic literature pretty much, and it fits in memory on a single machine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3684" target="_blank">01:01:24.600</a></span> | <span class="t">What is the idea behind averaging the weights of embeddings?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3687" target="_blank">01:01:27.720</a></span> | <span class="t">They've got to be set to something. There are words that weren't there, so other options</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3694" target="_blank">01:01:34.560</a></span> | <span class="t">is we could leave them at 0, but that seems like a very extreme thing to do. 0 is a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3698" target="_blank">01:01:38.880</a></span> | <span class="t">extreme number. Why would it be 0? We could set it equal to some random numbers, but if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3706" target="_blank">01:01:46.160</a></span> | <span class="t">so, what would be the mean and standard deviation of those random numbers, or should it be uniform?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3710" target="_blank">01:01:50.960</a></span> | <span class="t">If we just average the rest of the embeddings, then we have something that's a reasonable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3716" target="_blank">01:01:56.800</a></span> | <span class="t">scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3717" target="_blank">01:01:57.800</a></span> | <span class="t">Just to clarify, this is how you're initializing words that didn't appear in the training corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3722" target="_blank">01:02:02.040</a></span> | <span class="t">Thanks, Rachel, that's right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3723" target="_blank">01:02:03.040</a></span> | <span class="t">I think you've pretty much just answered this one, but someone had asked if there's a specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3729" target="_blank">01:02:09.320</a></span> | <span class="t">advantage to creating our own pre-trained embedding over using glob or Word2Vec.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3734" target="_blank">01:02:14.520</a></span> | <span class="t">I think I have. We're not creating a pre-trained embedding; we're creating a pre-trained model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3743" target="_blank">01:02:23.120</a></span> | <span class="t">Let's talk a little bit more. This is a ton of stuff we've seen before, but it's changed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3746" target="_blank">01:02:26.880</a></span> | <span class="t">a little bit. It's actually a lot easier than it was in Part 1, but I want to go a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3750" target="_blank">01:02:30.800</a></span> | <span class="t">bit deeper into the language model loader.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3758" target="_blank">01:02:38.000</a></span> | <span class="t">So this is the language model loader, and I really hope that by now you've learned in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3761" target="_blank">01:02:41.280</a></span> | <span class="t">your editor or IDE how to jump to symbols. I don't want it to be a burden for you to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3768" target="_blank">01:02:48.920</a></span> | <span class="t">find out what the source code of a language model loader is. And if it's still a burden,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3774" target="_blank">01:02:54.000</a></span> | <span class="t">please go back and try and learn those keyboard shortcuts in VS Code. If your editor does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3780" target="_blank">01:03:00.760</a></span> | <span class="t">not make it easy, don't use that editor anymore. There's lots of good free editors that make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3785" target="_blank">01:03:05.520</a></span> | <span class="t">this easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3790" target="_blank">01:03:10.360</a></span> | <span class="t">So here's the source code for language model loader. It's interesting to notice that it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3798" target="_blank">01:03:18.720</a></span> | <span class="t">not doing anything particularly tricky. It's not deriving from anything at all. What makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3810" target="_blank">01:03:30.400</a></span> | <span class="t">it something that's capable of being a data loader is it's something you can iterate over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3816" target="_blank">01:03:36.640</a></span> | <span class="t">And so specifically, here's the fit function inside fastai.model. This is where everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3827" target="_blank">01:03:47.680</a></span> | <span class="t">ends up eventually, which goes through each epoch, and then it creates an iterator from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3832" target="_blank">01:03:52.960</a></span> | <span class="t">the data loader, and it just does a for loop through it. So anything you can do a for loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3837" target="_blank">01:03:57.480</a></span> | <span class="t">through can be a data loader. And specifically, it needs to return tuples of many batches,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3845" target="_blank">01:04:05.800</a></span> | <span class="t">an independent and dependent variable for many batches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3849" target="_blank">01:04:09.320</a></span> | <span class="t">So anything with a dunder-eater method is something that can act as an iterator. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3857" target="_blank">01:04:17.600</a></span> | <span class="t">yield is a neat little Python keyword you probably should learn about if you don't already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3862" target="_blank">01:04:22.520</a></span> | <span class="t">know it, but it basically spits out a thing and waits for you to ask for another thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3867" target="_blank">01:04:27.800</a></span> | <span class="t">normally in a for loop or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3870" target="_blank">01:04:30.720</a></span> | <span class="t">So in this case, we start by initializing the language model, passing it in the numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3878" target="_blank">01:04:38.600</a></span> | <span class="t">So this is a numericalized, big, long list of all of our documents concatenated together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3886" target="_blank">01:04:46.060</a></span> | <span class="t">And the first thing we do is to batchify it. And this is the thing which quite a few of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3892" target="_blank">01:04:52.280</a></span> | <span class="t">you got confused about last time. If our batch size is 64 and we have 25 million numbers in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3905" target="_blank">01:05:05.320</a></span> | <span class="t">our list, we are not creating items of length 64. We're not doing that. We're creating 64</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3915" target="_blank">01:05:15.080</a></span> | <span class="t">items in total. So each of them is of size t/64, which is 390,000. So that's what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3927" target="_blank">01:05:27.000</a></span> | <span class="t">do here when we reshape it so that this axis here is of length 64, and then this -1 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3936" target="_blank">01:05:36.400</a></span> | <span class="t">everything else. So that's 390,000 long. And then we transpose it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3944" target="_blank">01:05:44.560</a></span> | <span class="t">So that means that we now have 64 columns, 390,000 rows, and then what we do each time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3952" target="_blank">01:05:52.560</a></span> | <span class="t">we do an iterate is we grab one batch of some sequence length, we'll look at that in a moment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3960" target="_blank">01:06:00.120</a></span> | <span class="t">but basically it's approximately equal to bptt, which we set to 70, stands for backprop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3970" target="_blank">01:06:10.160</a></span> | <span class="t">through time, and we just grab that many rows. So from i to i plus 70 rows, and then we try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3983" target="_blank">01:06:23.800</a></span> | <span class="t">to predict that plus 1. So we've got 64 columns, and each of those is 1/64 of our 25 million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=3995" target="_blank">01:06:35.880</a></span> | <span class="t">or whatever it was, tokens, hundreds of thousands long, and we just grab 70 at a time. So each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4005" target="_blank">01:06:45.160</a></span> | <span class="t">of those columns each time we grab it is going to hook up to the previous column. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4011" target="_blank">01:06:51.600</a></span> | <span class="t">why we get this consistency, this language model. It's stateful, just really important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4019" target="_blank">01:06:59.600</a></span> | <span class="t">Pretty much all the cool stuff in the language model is stolen from Stephen Merrity's AWD</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4026" target="_blank">01:07:06.640</a></span> | <span class="t">LSTM, including this little trick here, which is if we always grab 70 at a time and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4035" target="_blank">01:07:15.200</a></span> | <span class="t">we go back and do a new epoch, we're going to grab exactly the same batches every time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4040" target="_blank">01:07:20.480</a></span> | <span class="t">There's no randomness. Now normally we shuffle our data every time we do an epoch, or every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4046" target="_blank">01:07:26.000</a></span> | <span class="t">time we grab some data we grab it at random. You can't do that with a language model because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4050" target="_blank">01:07:30.660</a></span> | <span class="t">this set has to join up to the previous set because it's trying to learn the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4058" target="_blank">01:07:38.120</a></span> | <span class="t">If you suddenly jump somewhere else, then that doesn't make any sense as a sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4063" target="_blank">01:07:43.400</a></span> | <span class="t">So Stephen's idea is to say, since we can't shuffle the order, let's instead randomly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4071" target="_blank">01:07:51.380</a></span> | <span class="t">change the size, the sequence length. So basically he says, 95% of the time we'll use bptt, 70,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4082" target="_blank">01:08:02.020</a></span> | <span class="t">but 5% of the time we'll use half that. And then he says, you know what, I'm not even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4088" target="_blank">01:08:08.640</a></span> | <span class="t">going to make that the sequence length, I'm going to create a normally distributed random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4093" target="_blank">01:08:13.320</a></span> | <span class="t">number with that average and a standard deviation of 5, and I'll make that the sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4100" target="_blank">01:08:20.080</a></span> | <span class="t">So the sequence length is 70ish, and that means every time we go through we're getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4106" target="_blank">01:08:26.600</a></span> | <span class="t">slightly different batches. So we've got that little bit of extra randomness. I asked Stephen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4114" target="_blank">01:08:34.420</a></span> | <span class="t">Meridy where he came up with this idea. Did he think of it? He was like, I think I thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4120" target="_blank">01:08:40.840</a></span> | <span class="t">of it, but it seemed so obvious that I bet I didn't think of it, which is true of every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4126" target="_blank">01:08:46.280</a></span> | <span class="t">time I come up with an idea in deep learning, it always seems so obvious that you assume</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4129" target="_blank">01:08:49.640</a></span> | <span class="t">somebody else has thought of it, but I think he thought of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4134" target="_blank">01:08:54.860</a></span> | <span class="t">So this is a nice thing to look at if you're trying to do something a bit unusual with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4141" target="_blank">01:09:01.600</a></span> | <span class="t">a data loader. It's like, okay, here's a simple kind of role model you can use as to creating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4147" target="_blank">01:09:07.840</a></span> | <span class="t">a data loader from scratch, something that spits out batches of data. So our language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4156" target="_blank">01:09:16.200</a></span> | <span class="t">model loader just took in all of the documents concatenated together along with the batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4160" target="_blank">01:09:20.900</a></span> | <span class="t">size and the BPTT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4163" target="_blank">01:09:23.960</a></span> | <span class="t">Now generally speaking, we want to create a learner, and the way we normally do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4168" target="_blank">01:09:28.700</a></span> | <span class="t">is by getting a model data object and by calling some kind of method which have various names,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4174" target="_blank">01:09:34.360</a></span> | <span class="t">but often we call that method getModel. And so the idea is that the model data object</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4179" target="_blank">01:09:39.920</a></span> | <span class="t">has enough information to know what kind of model to give you. So we have to create that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4185" target="_blank">01:09:45.720</a></span> | <span class="t">model data object, which means we need that class, and so that's very easy to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4195" target="_blank">01:09:55.860</a></span> | <span class="t">So here are all of the pieces. We're going to create a custom learner, a custom model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4199" target="_blank">01:09:59.900</a></span> | <span class="t">data class and a custom model class. So a model data class, again, this one doesn't inherit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4207" target="_blank">01:10:07.040</a></span> | <span class="t">from anything, so you really see there's almost nothing to do. You need to tell it most importantly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4214" target="_blank">01:10:14.440</a></span> | <span class="t">what's your training set, give it a data loader, what's the validation set, give it a data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4219" target="_blank">01:10:19.680</a></span> | <span class="t">loader, and optionally give it a test set, plus anything else it needs to know. So it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4229" target="_blank">01:10:29.040</a></span> | <span class="t">might need to know the VPTT, it needs to know the number of tokens, that's the vocab size,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4238" target="_blank">01:10:38.240</a></span> | <span class="t">it needs to know what is the padding index, and so that it can save temporary files and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4245" target="_blank">01:10:45.360</a></span> | <span class="t">models, model data always needs to know the path. And so we just grab all that stuff and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4250" target="_blank">01:10:50.000</a></span> | <span class="t">we dump it. And that's it, that's the entire initializer, there's no logic there at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4255" target="_blank">01:10:55.920</a></span> | <span class="t">So then all of the work happens inside get_model. And so get_model calls something we'll look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4263" target="_blank">01:11:03.120</a></span> | <span class="t">at later which just grabs a normal PyTorch NN.module architecture. And jux it on the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4274" target="_blank">01:11:14.440</a></span> | <span class="t">Note with PyTorch normally we would say .cuda. With fast.ai, it's better to say to GPU. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4281" target="_blank">01:11:21.040</a></span> | <span class="t">the reason is that if you don't have a GPU, it will leave it on the CPU, and it also provides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4287" target="_blank">01:11:27.440</a></span> | <span class="t">a global variable you can set to choose whether it goes on the GPU or not. So it's a better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4293" target="_blank">01:11:33.360</a></span> | <span class="t">approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4295" target="_blank">01:11:35.520</a></span> | <span class="t">So we wrap the model in a language model. And the language model is this. Basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4300" target="_blank">01:11:40.840</a></span> | <span class="t">a language model is a subclass of basic model. It basically almost does nothing except it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4308" target="_blank">01:11:48.820</a></span> | <span class="t">defines layer groups. And so remember how when we do discriminative learning rates where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4314" target="_blank">01:11:54.660</a></span> | <span class="t">different layers have different learning rates, or we freeze different amounts, we don't provide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4323" target="_blank">01:12:03.300</a></span> | <span class="t">a different learning rate for every layer because there can be like a thousand layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4327" target="_blank">01:12:07.680</a></span> | <span class="t">We provide a different learning rate for every layer group. So when you create a custom model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4333" target="_blank">01:12:13.300</a></span> | <span class="t">you just have to override this one thing which returns a list of all of your layer groups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4341" target="_blank">01:12:21.840</a></span> | <span class="t">So in this case, my last layer group contains the last part of the model and one bit of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4348" target="_blank">01:12:28.680</a></span> | <span class="t">dropout, and the rest of it, this star here, means pull this apart. So this is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4354" target="_blank">01:12:34.120</a></span> | <span class="t">going to be one layer per RNN layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4360" target="_blank">01:12:40.200</a></span> | <span class="t">So that's all that is. And then finally, turn that into a learner. And so a learner you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4367" target="_blank">01:12:47.520</a></span> | <span class="t">just pass in the model and it turns it into a learner. In this case we have overridden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4372" target="_blank">01:12:52.480</a></span> | <span class="t">learner and the only thing we've done is to say I want the default loss function to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4379" target="_blank">01:12:59.160</a></span> | <span class="t">cross-entropy. So this entire set of custom model, custom model data, custom learner all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4387" target="_blank">01:13:07.960</a></span> | <span class="t">fits on a single screen, and they always basically look like this. So that's a kind of little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4395" target="_blank">01:13:15.040</a></span> | <span class="t">dig inside this pretty boring part of the code base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4399" target="_blank">01:13:19.200</a></span> | <span class="t">So the interesting part of this code base is getLanguageModel. GetLanguageModel is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4404" target="_blank">01:13:24.200</a></span> | <span class="t">the thing that gives us our awdlstm. And it actually contains the big idea, the big, incredibly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4415" target="_blank">01:13:35.440</a></span> | <span class="t">simple idea that everybody else here thinks it's really obvious, that everybody in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4420" target="_blank">01:13:40.280</a></span> | <span class="t">NLP community I spoke to thought was insane, which is basically every model can be thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4427" target="_blank">01:13:47.720</a></span> | <span class="t">of as a backbone plus a head, and if you pre-train the backbone and stick on a random head, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4435" target="_blank">01:13:55.960</a></span> | <span class="t">can do fine-tuning and that's a good idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4440" target="_blank">01:14:00.120</a></span> | <span class="t">And so these two bits of the code are literally right next to each other. There is this bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4448" target="_blank">01:14:08.520</a></span> | <span class="t">of fastai.lm_rnn. Here's getLanguageModel. Here's getClassifier. getLanguageModel creates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4458" target="_blank">01:14:18.000</a></span> | <span class="t">an RNN encoder and then creates a sequential model that sticks on top of that a linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4464" target="_blank">01:14:24.200</a></span> | <span class="t">decoder. Classifier creates an RNN encoder and then a sequential model that sticks on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4470" target="_blank">01:14:30.160</a></span> | <span class="t">top of that a pooling linear classifier. We'll see what these differences are in a moment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4475" target="_blank">01:14:35.440</a></span> | <span class="t">but you get the basic idea. They're basically doing pretty much the same thing. They've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4479" target="_blank">01:14:39.880</a></span> | <span class="t">got this head and then they're sticking on a simple linear layer on top.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4486" target="_blank">01:14:46.280</a></span> | <span class="t">So it's worth digging in a little bit deeper and seeing what's going on here. Yes, Rich?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4492" target="_blank">01:14:52.240</a></span> | <span class="t">>> There was a question earlier about whether any of this translates to other languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4499" target="_blank">01:14:59.080</a></span> | <span class="t">>> Yeah, this whole thing works in any language you like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4502" target="_blank">01:15:02.800</a></span> | <span class="t">>> I mean, would you have to retrain your language model on a corpus from that language?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4510" target="_blank">01:15:10.920</a></span> | <span class="t">>> Absolutely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4511" target="_blank">01:15:11.920</a></span> | <span class="t">>> Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4512" target="_blank">01:15:12.920</a></span> | <span class="t">>> So the wikitext-103-pre-trained-language-model knows English. You could use it maybe as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4522" target="_blank">01:15:22.080</a></span> | <span class="t">a pre-trained start for a French or German model. Start by retraining the embedding layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4527" target="_blank">01:15:27.840</a></span> | <span class="t">from scratch. Might be helpful. Chinese, maybe not so much. But given that a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4535" target="_blank">01:15:35.560</a></span> | <span class="t">can be trained from any unlabeled documents at all, you'd never have to do that. Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4542" target="_blank">01:15:42.280</a></span> | <span class="t">almost every language in the world has plenty of documents. You can grab newspapers, web</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4551" target="_blank">01:15:51.120</a></span> | <span class="t">pages, parliamentary records, whatever. As long as you've got a few thousand documents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4559" target="_blank">01:15:59.520</a></span> | <span class="t">showing somewhat normal usage of that language, you can create a language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4564" target="_blank">01:16:04.640</a></span> | <span class="t">And so I know some of our students, one of our students, whose name I'll have to look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4569" target="_blank">01:16:09.280</a></span> | <span class="t">after in a week, very embarrassing, tried this approach for Thai. He said the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4576" target="_blank">01:16:16.600</a></span> | <span class="t">model he built easily beat the previous day of the entire classifier. For those of you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4584" target="_blank">01:16:24.160</a></span> | <span class="t">that are international fellows, this is an easy way for you to whip out a paper in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4591" target="_blank">01:16:31.440</a></span> | <span class="t">you either create the first ever classifier in your language or beat everybody else's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4596" target="_blank">01:16:36.160</a></span> | <span class="t">classifier in your language and then you can tell them that you've been a student of deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4601" target="_blank">01:16:41.080</a></span> | <span class="t">learning for six months and piss off all the academics in your country.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4607" target="_blank">01:16:47.160</a></span> | <span class="t">So here's our edit encoder. It's just a standard edit module. Most of the text in it is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4617" target="_blank">01:16:57.280</a></span> | <span class="t">just documentation, as you can see. It looks like there's more going on in it than there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4623" target="_blank">01:17:03.280</a></span> | <span class="t">actually is, but really all there is is we create an embedding layer, we create an LSTM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4629" target="_blank">01:17:09.640</a></span> | <span class="t">for each layer that's been asked for, and that's it. Everything else in it is dropout.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4639" target="_blank">01:17:19.520</a></span> | <span class="t">Basically all of the interesting stuff in the AWED LSTM paper is all of the places you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4645" target="_blank">01:17:25.320</a></span> | <span class="t">can put dropout. And then the forward is basically the same thing, right? It's call the embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4655" target="_blank">01:17:35.240</a></span> | <span class="t">layer, add some dropout, go through each layer, call that RNN layer, append it to our list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4664" target="_blank">01:17:44.960</a></span> | <span class="t">of outputs, add dropout, and that's about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4674" target="_blank">01:17:54.320</a></span> | <span class="t">So it's really pretty straightforward. The paper you want to be reading, as I've mentioned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4685" target="_blank">01:18:05.020</a></span> | <span class="t">is the AWD LSTM paper, which is this one here, regularizing and optimizing LSTM language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4690" target="_blank">01:18:10.440</a></span> | <span class="t">models, and it's well-written and pretty accessible and entirely implemented inside FastAI as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4700" target="_blank">01:18:20.920</a></span> | <span class="t">well, so you can see all of the code for that paper. And like a lot of the code is shamelessly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4709" target="_blank">01:18:29.240</a></span> | <span class="t">plagiarized with Stephen's permission from his excellent GitHub repo, AWD LSTM, and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4716" target="_blank">01:18:36.880</a></span> | <span class="t">process of which I picked some of his bugs as well. I even told him about them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4726" target="_blank">01:18:46.920</a></span> | <span class="t">So I'm talking increasingly about "please read the papers", so here's the paper, "please</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4731" target="_blank">01:18:51.320</a></span> | <span class="t">read this paper", and it refers to other papers. So for things like why is it that the encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4740" target="_blank">01:19:00.960</a></span> | <span class="t">weight and the decoder weight are the same? Well, it's because there's this thing called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4750" target="_blank">01:19:10.720</a></span> | <span class="t">"tie_weights", this is inside that get_language model, there's a thing called "tie_weights",</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4761" target="_blank">01:19:21.040</a></span> | <span class="t">it defaults to true, and if it's true then we literally use the same weight matrix for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4772" target="_blank">01:19:32.280</a></span> | <span class="t">the encoder and the decoder. So they're literally pointing at the same block of memory. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4779" target="_blank">01:19:39.160</a></span> | <span class="t">so why is that? What's the result of it? That's one of the citations in Stephen's paper, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4784" target="_blank">01:19:44.920</a></span> | <span class="t">is also a well-written paper, you can go and look up and learn about work time. So there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4789" target="_blank">01:19:49.440</a></span> | <span class="t">a lot of cool stuff in there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4793" target="_blank">01:19:53.040</a></span> | <span class="t">So we have basically a standard RNN, the only way it's not standard is it's just got lots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4797" target="_blank">01:19:57.960</a></span> | <span class="t">more types of dropout in it, and then a sequential model, on top of that we stick a linear decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4806" target="_blank">01:20:06.600</a></span> | <span class="t">which is literally half the screen of code. It's got a single linear layer, we initialize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4815" target="_blank">01:20:15.320</a></span> | <span class="t">the weights to some range, we add some dropout, and that's it. So we've got an RNN, on top</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4825" target="_blank">01:20:25.040</a></span> | <span class="t">of that we stick a linear layer with dropout and we're finished. So that's the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4829" target="_blank">01:20:29.880</a></span> | <span class="t">model. So what dropout you choose matters a lot, and through a lot of experimentation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4846" target="_blank">01:20:46.000</a></span> | <span class="t">I found a bunch of dropouts -- you can see here we've got each of these corresponds to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4851" target="_blank">01:20:51.840</a></span> | <span class="t">a particular argument -- a bunch of dropouts that tend to work pretty well for language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4856" target="_blank">01:20:56.480</a></span> | <span class="t">models. But if you have less data for your language model, you'll need more dropout. If</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4866" target="_blank">01:21:06.680</a></span> | <span class="t">you have more data, you can benefit from less dropout, you don't want to regularize more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4871" target="_blank">01:21:11.200</a></span> | <span class="t">than you have to. Rather than having to tune every one of these 5 things, my claim is they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4879" target="_blank">01:21:19.000</a></span> | <span class="t">already pretty good ratios to each other, so just tune this number. I just multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4884" target="_blank">01:21:24.000</a></span> | <span class="t">it all by something. So there's really just one number you have to tune. If you're overfitting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4893" target="_blank">01:21:33.480</a></span> | <span class="t">then you'll need to increase this number. If you're underfitting, you'll need to decrease</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4897" target="_blank">01:21:37.040</a></span> | <span class="t">this number. Other than that, these ratios actually seem pretty good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4905" target="_blank">01:21:45.640</a></span> | <span class="t">So one important idea which may seem pretty minor, but again it's incredibly controversial,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4915" target="_blank">01:21:55.000</a></span> | <span class="t">is that we should measure accuracy when we look at a language model. So normally in language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4921" target="_blank">01:22:01.500</a></span> | <span class="t">models we look at this loss value, which is just cross-entropy loss, but specifically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4928" target="_blank">01:22:08.680</a></span> | <span class="t">where you nearly always take e^ of that, which the NLP community calls perplexity. Perplexity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4936" target="_blank">01:22:16.120</a></span> | <span class="t">is just e^ of cross-entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4942" target="_blank">01:22:22.240</a></span> | <span class="t">There's a lot of problems with comparing things based on cross-entropy loss. I'm not sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4949" target="_blank">01:22:29.120</a></span> | <span class="t">I've got time to go into it in detail now, but the basic problem is that it's kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4955" target="_blank">01:22:35.400</a></span> | <span class="t">like that thing we learned about focal loss. Cross-entropy loss, if you're right, it wants</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4960" target="_blank">01:22:40.240</a></span> | <span class="t">you to be really confident that you're right. So it really penalizes a model that doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4966" target="_blank">01:22:46.720</a></span> | <span class="t">kind of say, I'm so sure this is wrong, whereas accuracy doesn't care at all about how confident</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4972" target="_blank">01:22:52.360</a></span> | <span class="t">you are, it just cares about whether you're right. And this is much more often the thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4976" target="_blank">01:22:56.520</a></span> | <span class="t">which you care about in real life. So this accuracy is how often do we guess the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4982" target="_blank">01:23:02.760</a></span> | <span class="t">word correctly. And I just find that a much more stable number to keep track of. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4989" target="_blank">01:23:09.920</a></span> | <span class="t">a simple little thing that I do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=4994" target="_blank">01:23:14.720</a></span> | <span class="t">So we trained for a while, and we get down to a 3.9 cross-entropy loss, and if you go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5012" target="_blank">01:23:32.160</a></span> | <span class="t">e^, that kind of gives you a sense of what's happened with language models. If you look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5025" target="_blank">01:23:45.840</a></span> | <span class="t">at academic papers from about 18 months ago, you'll see them talking about state-of-the-art</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5034" target="_blank">01:23:54.760</a></span> | <span class="t">complexities of over 100. The rate at which our ability to kind of understand language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5044" target="_blank">01:24:04.440</a></span> | <span class="t">and I think measuring language model accuracy or complexity is not a terrible proxy for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5051" target="_blank">01:24:11.440</a></span> | <span class="t">understanding language. If I can guess what you're going to say next, I pretty much need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5056" target="_blank">01:24:16.640</a></span> | <span class="t">to understand language pretty well, and also the kind of things you might talk about pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5060" target="_blank">01:24:20.480</a></span> | <span class="t">well. So this number has just come down so much. It's been amazing. NLP in the last 12</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5069" target="_blank">01:24:29.160</a></span> | <span class="t">to 18 months. And it's going to come down a lot more. It really feels like 2011-2012 computer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5075" target="_blank">01:24:35.960</a></span> | <span class="t">vision. We're just starting to understand transfer learning and fine-tuning, and these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5081" target="_blank">01:24:41.240</a></span> | <span class="t">basic models are getting so much better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5084" target="_blank">01:24:44.880</a></span> | <span class="t">So everything you thought about what NLP can and can't do is very rapidly going out of date.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5093" target="_blank">01:24:53.920</a></span> | <span class="t">But there's still lots of stuff NLP is not good at, to be clear. Just like in 2012 there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5098" target="_blank">01:24:58.560</a></span> | <span class="t">was lots of stuff computer vision wasn't good at. But it's changing incredibly rapidly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5103" target="_blank">01:25:03.420</a></span> | <span class="t">and now is a very, very good time to be getting very, very good at NLP or starting start-ups</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5110" target="_blank">01:25:10.340</a></span> | <span class="t">based on NLP because there's a whole bunch of stuff which computers were absolutely shit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5115" target="_blank">01:25:15.120</a></span> | <span class="t">at two years ago, and now are not quite as good at people, and then next year they'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5122" target="_blank">01:25:22.320</a></span> | <span class="t">be much better at people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5125" target="_blank">01:25:25.140</a></span> | <span class="t">Two questions. One, what is your ratio of paper reading versus coding in a week?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5135" target="_blank">01:25:35.000</a></span> | <span class="t">What do you think, Rachel? You see me. I mean, it's a lot more coding, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5139" target="_blank">01:25:39.000</a></span> | <span class="t">It's a lot more coding. I feel like it also really varies from week to week. I feel like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5142" target="_blank">01:25:42.560</a></span> | <span class="t">they're...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5144" target="_blank">01:25:44.320</a></span> | <span class="t">Like with that bounding box stuff, there was all these papers and no map through them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5154" target="_blank">01:25:54.040</a></span> | <span class="t">and so I didn't even know which one to read first, and then I'd read the citations and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5158" target="_blank">01:25:58.200</a></span> | <span class="t">didn't understand any of them. So there was a few weeks of just kind of reading papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5162" target="_blank">01:26:02.600</a></span> | <span class="t">before I even knew what to start coding. That's unusual though. Most of the time, I don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5170" target="_blank">01:26:10.560</a></span> | <span class="t">know, any time I start reading a paper, I'm always convinced that I'm not smart enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5175" target="_blank">01:26:15.120</a></span> | <span class="t">to understand it, always, regardless of the paper, and somehow eventually I do. But yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5183" target="_blank">01:26:23.480</a></span> | <span class="t">I try to spend as much time as I can coding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5186" target="_blank">01:26:26.880</a></span> | <span class="t">And then the second question, is your dropout rate the same through the training or do you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5192" target="_blank">01:26:32.360</a></span> | <span class="t">adjust it and the weights accordingly?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5194" target="_blank">01:26:34.680</a></span> | <span class="t">I'll just say one more thing about the last bit, which is very often, like the vast majority,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5202" target="_blank">01:26:42.080</a></span> | <span class="t">nearly always, after I've read a paper, even after I've read the bit that says this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5209" target="_blank">01:26:49.920</a></span> | <span class="t">the problem I'm trying to solve, I'll kind of stop there and try to implement something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5214" target="_blank">01:26:54.080</a></span> | <span class="t">that I think might solve that problem, and then I'll go back and read the paper and I'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5217" target="_blank">01:26:57.600</a></span> | <span class="t">read little bits about how I solve these problem bits, and I'll be like, oh that's a good idea,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5222" target="_blank">01:27:02.240</a></span> | <span class="t">and then I'll try to implement those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5224" target="_blank">01:27:04.120</a></span> | <span class="t">And so that's why, for example, I didn't actually implement SSD. My custom head is not the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5231" target="_blank">01:27:11.320</a></span> | <span class="t">as their head. It's because I kind of read the gist of it and then I tried to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5235" target="_blank">01:27:15.560</a></span> | <span class="t">something best as I could and then go back to the papers and try to see why. So by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5240" target="_blank">01:27:20.960</a></span> | <span class="t">time I got to the focal loss paper, I was driving myself crazy with how come I can't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5248" target="_blank">01:27:28.520</a></span> | <span class="t">find small objects, how come it's always predicting background, and I read the focal loss paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5253" target="_blank">01:27:33.600</a></span> | <span class="t">and I was like, that's why! It's so much better when you deeply understand the problem they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5262" target="_blank">01:27:42.480</a></span> | <span class="t">trying to solve. And I do find the vast majority of the time, by the time I read that bit of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5266" target="_blank">01:27:46.800</a></span> | <span class="t">the paper which is like solving the problem, I'm then like, yeah but these three ideas I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5271" target="_blank">01:27:51.720</a></span> | <span class="t">came up with, they didn't try. And you suddenly realize that you've got new ideas. Or else</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5277" target="_blank">01:27:57.040</a></span> | <span class="t">if you just implement the paper mindlessly, you tend not to have these insights about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5284" target="_blank">01:28:04.840</a></span> | <span class="t">better ways to do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5290" target="_blank">01:28:10.120</a></span> | <span class="t">Varying dropout is really interesting and there are some recent papers actually that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5295" target="_blank">01:28:15.080</a></span> | <span class="t">suggest gradually changing dropout and it was either a good idea to gradually make it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5301" target="_blank">01:28:21.600</a></span> | <span class="t">smaller or to gradually make it bigger. I'm not sure which. Maybe one of us can try and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5309" target="_blank">01:28:29.200</a></span> | <span class="t">find it during the week. I haven't seen it widely used. I tried it a little bit with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5314" target="_blank">01:28:34.280</a></span> | <span class="t">the most recent paper I wrote and I had some good results. I think I was gradually making</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5322" target="_blank">01:28:42.680</a></span> | <span class="t">it smaller but I can't remember.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5325" target="_blank">01:28:45.720</a></span> | <span class="t">And then the next question is, "Am I correct in thinking that this language model is built</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5330" target="_blank">01:28:50.000</a></span> | <span class="t">on word embeddings? Would it be valuable to try this with phrase or sentence embeddings?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5336" target="_blank">01:28:56.120</a></span> | <span class="t">I asked this because I saw from Google the other day universal sentence encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5342" target="_blank">01:29:02.360</a></span> | <span class="t">Yeah, this is much better than that. Do you see what I mean? This is not just an embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5347" target="_blank">01:29:07.480</a></span> | <span class="t">of a sentence, this is an entire model. An embedding by definition is like a fixed thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5356" target="_blank">01:29:16.920</a></span> | <span class="t">I think they're asking, they're saying that this language, well the first question is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5361" target="_blank">01:29:21.920</a></span> | <span class="t">is this language model built on word embeddings?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5364" target="_blank">01:29:24.480</a></span> | <span class="t">Right, but it's not saying, a sentence or a phrase embedding is always a model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5372" target="_blank">01:29:32.160</a></span> | <span class="t">creates that. We've got a model that's like trying to understand language, it's not just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5379" target="_blank">01:29:39.000</a></span> | <span class="t">a phrase, it's not just a sentence, it's a document in the end and it's not just an embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5385" target="_blank">01:29:45.200</a></span> | <span class="t">we're training through the whole thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5386" target="_blank">01:29:46.960</a></span> | <span class="t">So this has been a huge problem with NLP for years now is this attachment they have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5394" target="_blank">01:29:54.120</a></span> | <span class="t">embeddings. So even the paper that the community has been most excited about recently from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5400" target="_blank">01:30:00.280</a></span> | <span class="t">AI2, the Allen Institute, called ELMO, and they found much better results across lots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5407" target="_blank">01:30:07.840</a></span> | <span class="t">of models. But again, it was an embedding. They took a fixed model and created a fixed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5412" target="_blank">01:30:12.720</a></span> | <span class="t">set of numbers which they then fed into a model. But in computer vision, we've known</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5419" target="_blank">01:30:19.080</a></span> | <span class="t">for years that that approach of having a fixed set of features, they're called hypercolons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5426" target="_blank">01:30:26.800</a></span> | <span class="t">in computer vision. People stopped using them like 3 or 4 years ago because fine-tuning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5433" target="_blank">01:30:33.920</a></span> | <span class="t">the entire model works much better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5437" target="_blank">01:30:37.640</a></span> | <span class="t">So for those of you that have spent quite a lot of time with NLP and not much time with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5442" target="_blank">01:30:42.040</a></span> | <span class="t">computer vision, you're going to have to start relearning. All that stuff you have been told</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5448" target="_blank">01:30:48.600</a></span> | <span class="t">about this idea that there are these things called embeddings and that you learn them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5453" target="_blank">01:30:53.800</a></span> | <span class="t">ahead of time, and then you apply these fixed things, whether it be word level or phrase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5460" target="_blank">01:31:00.120</a></span> | <span class="t">level or whatever level, don't do that. You want to actually create a pre-trained model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5466" target="_blank">01:31:06.840</a></span> | <span class="t">and fine-tune it end to end. You'll see some specific results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5476" target="_blank">01:31:16.800</a></span> | <span class="t">For using accuracy instead of perplexity as a metric for the model, could we work that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5486" target="_blank">01:31:26.920</a></span> | <span class="t">into the loss function rather than just use it as a metric?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5490" target="_blank">01:31:30.080</a></span> | <span class="t">No, you never want to do that whether it be computer vision or NLP or whatever. It's too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5494" target="_blank">01:31:34.120</a></span> | <span class="t">bumpy. So cross-entropy is fine as a loss function. And I'm not saying instead of I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5501" target="_blank">01:31:41.040</a></span> | <span class="t">use it in addition, I think it's good to look at the accuracy and to look at the cross-entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5507" target="_blank">01:31:47.480</a></span> | <span class="t">But for your loss function, you need something nice and smooth. Accuracy doesn't work very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5513" target="_blank">01:31:53.480</a></span> | <span class="t">well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5514" target="_blank">01:31:54.480</a></span> | <span class="t">You'll see there's two different versions of save. There's save and save encoder. Save</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5520" target="_blank">01:32:00.040</a></span> | <span class="t">saves the whole model as per usual. Save encoder saves just that bit. In other words, in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5531" target="_blank">01:32:11.520</a></span> | <span class="t">sequential model, it saves just that bit and not that bit. In other words, this bit, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5538" target="_blank">01:32:18.340</a></span> | <span class="t">is the bit that actually makes it into a language model, we don't care about in the classifier,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5543" target="_blank">01:32:23.520</a></span> | <span class="t">we just care about that bit. So let's now create the classifier. I'm going to go through this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5554" target="_blank">01:32:34.280</a></span> | <span class="t">bit pretty quickly because it's the same. But when you go back during the week and look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5558" target="_blank">01:32:38.120</a></span> | <span class="t">at the code, convince yourself it's the same. We do getAllPD, read_csv again, juxize again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5563" target="_blank">01:32:43.880</a></span> | <span class="t">getAll again, save those tokens again. We don't create a new I2S vocabulary. We obviously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5572" target="_blank">01:32:52.900</a></span> | <span class="t">want to use the same vocabulary we had in the language model because we're about to reload</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5578" target="_blank">01:32:58.540</a></span> | <span class="t">the same encoder. Same default dict, same way of creating our numericalized list, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5588" target="_blank">01:33:08.060</a></span> | <span class="t">as per before we can save. So that's all the same. Later on we can reload those rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5594" target="_blank">01:33:14.000</a></span> | <span class="t">than having to rebuild them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5597" target="_blank">01:33:17.000</a></span> | <span class="t">So all of our hyperparameters are the same. We can change the dropout. Optimize a function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5611" target="_blank">01:33:31.120</a></span> | <span class="t">Pick a batch size that is as big as you can that doesn't run out of memory. This bit's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5618" target="_blank">01:33:38.760</a></span> | <span class="t">a bit interesting. There's some fun stuff going on here. The basic idea here is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5630" target="_blank">01:33:50.000</a></span> | <span class="t">for the classifier we do really want to look at a document. We need to say is this document</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5637" target="_blank">01:33:57.040</a></span> | <span class="t">positive or negative. So we do want to shuffle the documents because we like to shuffle things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5645" target="_blank">01:34:05.480</a></span> | <span class="t">But those documents are different lengths, so if we stick them all into one batch -- this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5651" target="_blank">01:34:11.960</a></span> | <span class="t">is a handy thing that fastAI does for you -- you can stick things at different lengths</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5655" target="_blank">01:34:15.360</a></span> | <span class="t">into a batch and it will automatically pad them, so you don't have to worry about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5660" target="_blank">01:34:20.920</a></span> | <span class="t">But if they're wildly different lengths, then you're going to be wasting a lot of computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5665" target="_blank">01:34:25.160</a></span> | <span class="t">times. There might be one thing there that's 2,000 words long and everything else is 50</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5669" target="_blank">01:34:29.240</a></span> | <span class="t">words long and that means you end up with a 2,000-wide tensor. That's pretty annoying.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5676" target="_blank">01:34:36.000</a></span> | <span class="t">So James Bradbury, who's actually one of Stephen Meridy's colleagues and the guy who came up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5681" target="_blank">01:34:41.480</a></span> | <span class="t">with TorchText, came up with an idea which was let's sort the dataset by length-ish.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5695" target="_blank">01:34:55.120</a></span> | <span class="t">So kind of make it so the first things in the list are on the whole, shorter than the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5703" target="_blank">01:35:03.160</a></span> | <span class="t">things at the end, but a little bit random as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5709" target="_blank">01:35:09.360</a></span> | <span class="t">And so I'll show you how I implemented that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5714" target="_blank">01:35:14.820</a></span> | <span class="t">So the first thing we need is a dataset. So we have a dataset passing in the documents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5724" target="_blank">01:35:24.880</a></span> | <span class="t">and their labels. And so here's a text dataset and it inherits from dataset. Here is dataset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5731" target="_blank">01:35:31.800</a></span> | <span class="t">from PyTorch. And actually, dataset doesn't do anything at all. It says you need to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5738" target="_blank">01:35:38.820</a></span> | <span class="t">item if you don't have one, you're going to get an error, you need a length if you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5742" target="_blank">01:35:42.560</a></span> | <span class="t">have one, you're going to get an error. So this is an abstract class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5748" target="_blank">01:35:48.640</a></span> | <span class="t">So we're going to pass in our x, we're going to pass in our y, and getItem is going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5754" target="_blank">01:35:54.640</a></span> | <span class="t">grab the x and grab the y and return them. It couldn't be much simpler. Optionally, it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5762" target="_blank">01:36:02.920</a></span> | <span class="t">could reverse it. Optionally it could stick an end of stream at the end. Optionally it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5766" target="_blank">01:36:06.400</a></span> | <span class="t">could stick a start of stream at the beginning. We're not doing any of those things. So literally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5769" target="_blank">01:36:09.640</a></span> | <span class="t">all we're doing is putting in an x, putting in a y, and then grab an item, we're returning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5774" target="_blank">01:36:14.100</a></span> | <span class="t">the x and the y as a tuple. And the length is how long the x array is. So that's all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5782" target="_blank">01:36:22.200</a></span> | <span class="t">the dataset is. Something with a length that you can index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5787" target="_blank">01:36:27.920</a></span> | <span class="t">So to turn it into a data loader, you simply pass the dataset to the data loader constructor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5794" target="_blank">01:36:34.300</a></span> | <span class="t">and it's now going to go ahead and give you a batch of that at a time. Normally you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5799" target="_blank">01:36:39.560</a></span> | <span class="t">say shuffle=true or shuffle=false, it will decide whether to randomize it for you. In</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5804" target="_blank">01:36:44.920</a></span> | <span class="t">this case though, we're actually going to pass in a sampler parameter. The sampler is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5810" target="_blank">01:36:50.920</a></span> | <span class="t">a class we're going to define that tells the data loader how to shuffle. So for the validation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5819" target="_blank">01:36:59.120</a></span> | <span class="t">set, we're going to define something that actually just sorts it. It just deterministically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5824" target="_blank">01:37:04.440</a></span> | <span class="t">sorts it so all the shortest documents will be at the start, all the longest documents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5829" target="_blank">01:37:09.840</a></span> | <span class="t">will be at the end, and that's going to minimize the amount of padding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5833" target="_blank">01:37:13.720</a></span> | <span class="t">For the training sampler, we're going to create this thing I call a sort-ish sampler, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5842" target="_blank">01:37:22.000</a></span> | <span class="t">also sorts-ish. So this is where I really like PyTorch is that they came up with this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5851" target="_blank">01:37:31.600</a></span> | <span class="t">idea for an API for their data loader where we can hook in new classes to make it behave</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5858" target="_blank">01:37:38.280</a></span> | <span class="t">in different ways. So here's a sort-sampler, it's simply something which again has a length,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5866" target="_blank">01:37:46.320</a></span> | <span class="t">which is the length of the data source, and it has an iterator, which is simply an iterator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5872" target="_blank">01:37:52.160</a></span> | <span class="t">which goes through the data source sorted by length of the key, and I pass in as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5882" target="_blank">01:38:02.080</a></span> | <span class="t">key lambda function which returns the length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5890" target="_blank">01:38:10.280</a></span> | <span class="t">And so for the sort-ish sampler, I won't go through the details, but it basically does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5896" target="_blank">01:38:16.040</a></span> | <span class="t">the same thing with a little bit of randomness. So it's just another of these beautiful little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5904" target="_blank">01:38:24.960</a></span> | <span class="t">design things in PyTorch that I discovered. I could take James Bradbury's ideas, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5911" target="_blank">01:38:31.760</a></span> | <span class="t">he had written a whole new set of classes around, and I could actually just use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5917" target="_blank">01:38:37.760</a></span> | <span class="t">inbuilt hooks inside PyTorch. You will notice that it's not actually PyTorch's data loader,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5926" target="_blank">01:38:46.700</a></span> | <span class="t">it's actually FastAI's data loader, but it's basically almost entirely plagiarized from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5931" target="_blank">01:38:51.600</a></span> | <span class="t">PyTorch but customized in some ways to make it faster, mainly by using multithreading instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5937" target="_blank">01:38:57.520</a></span> | <span class="t">of multiprocessing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5938" target="_blank">01:38:58.520</a></span> | <span class="t">Does the pre-trained LSTM depth and BBTT need to match with the new one we are training?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5947" target="_blank">01:39:07.520</a></span> | <span class="t">No, the BBTT doesn't need to match at all. That's just like how many things do we look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5951" target="_blank">01:39:11.620</a></span> | <span class="t">at at a time, it's got nothing to do with the architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5956" target="_blank">01:39:16.640</a></span> | <span class="t">So now we can call that function we just saw before, getRNNClassifier. It's going to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5962" target="_blank">01:39:22.200</a></span> | <span class="t">exactly the same encoder, more or less, and we're going to pass in the same architectural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5968" target="_blank">01:39:28.720</a></span> | <span class="t">details as before. But this time, the head that we add on, you've got a few more things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5977" target="_blank">01:39:37.200</a></span> | <span class="t">you can do. One is you can add more than one hidden layer. So this layer here says this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5983" target="_blank">01:39:43.800</a></span> | <span class="t">is what the input to my classifier section, my head, is going to be. This is the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5991" target="_blank">01:39:51.440</a></span> | <span class="t">of the first layer, this is the output of the second layer, and you can add as many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=5995" target="_blank">01:39:55.720</a></span> | <span class="t">as you like. So you can basically create a little multi-layered neural net classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6000" target="_blank">01:40:00.240</a></span> | <span class="t">at the end. And so ditto, these are the dropouts to go after each of these layers. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6008" target="_blank">01:40:08.200</a></span> | <span class="t">here are all of the AWD LSTM dropouts, which we're going to basically plagiarize that idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6013" target="_blank">01:40:13.780</a></span> | <span class="t">for our classifier. We're going to use the RNN learner, just like before. We're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6021" target="_blank">01:40:21.860</a></span> | <span class="t">to use discriminative learning rates for different layers. You can try using weight decay or not,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6031" target="_blank">01:40:31.640</a></span> | <span class="t">I've been fiddling around a bit with that to see what happens. And so we start out just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6037" target="_blank">01:40:37.240</a></span> | <span class="t">training the last layer and we get 92.9% accuracy, then we unfreeze one more layer, get 93.3 accuracy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6047" target="_blank">01:40:47.760</a></span> | <span class="t">and then we fine-tune the whole thing. And after 3 epochs, so this was kind of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6067" target="_blank">01:41:07.120</a></span> | <span class="t">main attempt before our paper came along at using a pre-trained model. And what they did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6074" target="_blank">01:41:14.800</a></span> | <span class="t">is they used a pre-trained translation model. But they didn't fine-tune the whole thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6085" target="_blank">01:41:25.460</a></span> | <span class="t">they just took the activations of the translation model. And when they tried IMDB, they got 91.8%</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6107" target="_blank">01:41:47.220</a></span> | <span class="t">which we beat easily after only fine-tuning one layer. They weren't state-of-the-art there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6117" target="_blank">01:41:57.700</a></span> | <span class="t">the state-of-the-art is 94.1, which we beat after fine-tuning the whole thing for 3 epochs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6127" target="_blank">01:42:07.300</a></span> | <span class="t">And so by the end, we're at 94.8, which is obviously a huge difference because in terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6135" target="_blank">01:42:15.460</a></span> | <span class="t">of error rate, that's gone down from 5.9, and then I'll tell you a simple little trick. Go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6142" target="_blank">01:42:22.400</a></span> | <span class="t">back to the start of this notebook, and reverse the order of all of the documents, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6151" target="_blank">01:42:31.280</a></span> | <span class="t">rerun the whole thing. And when you get to the bit that says wt103, replace this fwd</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6161" target="_blank">01:42:41.220</a></span> | <span class="t">for forward with bwd for backward. That's a backward English language model that learns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6167" target="_blank">01:42:47.220</a></span> | <span class="t">to read English backwards. So if you redo this whole thing, put all the documents in reverse,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6174" target="_blank">01:42:54.420</a></span> | <span class="t">and change this to backward, you now have a second classifier which classifies things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6179" target="_blank">01:42:59.300</a></span> | <span class="t">by positive or negative sentiment based on the reverse document. If you then take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6187" target="_blank">01:43:07.740</a></span> | <span class="t">two predictions and take the average of them, you basically have a bidirectional model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6193" target="_blank">01:43:13.020</a></span> | <span class="t">you've trained each bit separately. That gets you to 95.4% accuracy. So we basically load</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6199" target="_blank">01:43:19.820</a></span> | <span class="t">it from 5.9 to 4.6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6202" target="_blank">01:43:22.900</a></span> | <span class="t">So this kind of 20% change in the state-of-the-art is almost unheard of. You have to go back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6212" target="_blank">01:43:32.020</a></span> | <span class="t">to Jeffrey Hinton's ImageNet computer vision thing where they chop 30% off the state-of-the-art.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6219" target="_blank">01:43:39.380</a></span> | <span class="t">It doesn't happen very often. So you can see this idea of just use transfer learning is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6227" target="_blank">01:43:47.880</a></span> | <span class="t">ridiculously powerful, but every new field thinks their new field is too special and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6235" target="_blank">01:43:55.140</a></span> | <span class="t">you can't do it. So it's a big opportunity for all of us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6242" target="_blank">01:44:02.980</a></span> | <span class="t">So we turned this into a paper, and when I say we, I did it with this guy, Sebastian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6247" target="_blank">01:44:07.420</a></span> | <span class="t">Reuter. You might remember his name because in lesson 5 I told you that I actually had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6254" target="_blank">01:44:14.180</a></span> | <span class="t">shared lesson 4 with Sebastian because I think he's an awesome researcher who I thought might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6260" target="_blank">01:44:20.060</a></span> | <span class="t">like it. I didn't know him personally at all. And much to my surprise, he actually watched</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6267" target="_blank">01:44:27.100</a></span> | <span class="t">the damn video. I was like, what NLP researcher is going to watch some beginner's video? He</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6273" target="_blank">01:44:33.900</a></span> | <span class="t">watched the whole video and he was like, that's actually quite fantastic. Well, thank you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6278" target="_blank">01:44:38.740</a></span> | <span class="t">very much, that's awesome coming from you. And he said, hey, we should turn this into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6284" target="_blank">01:44:44.580</a></span> | <span class="t">a paper. And I said, I don't write papers, I don't care about papers, I'm not interested</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6290" target="_blank">01:44:50.700</a></span> | <span class="t">in papers, that sounds really boring. And he said, okay, how about I write the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6298" target="_blank">01:44:58.100</a></span> | <span class="t">for you? And I said, you can't really write a paper about this yet because you'd have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6304" target="_blank">01:45:04.780</a></span> | <span class="t">to do studies to compare it to other things, they're called ablation studies to see which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6308" target="_blank">01:45:08.500</a></span> | <span class="t">bits actually work. There's no rigor here, I just put in everything that came in my head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6312" target="_blank">01:45:12.780</a></span> | <span class="t">and chucked it all together and it happened to work. And it's like, okay, what if I write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6317" target="_blank">01:45:17.380</a></span> | <span class="t">all the paper and do all the ablation studies, then can we write the paper? And I said, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6323" target="_blank">01:45:23.740</a></span> | <span class="t">it's like a whole library that I haven't documented and I'm not going to yet and you don't know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6331" target="_blank">01:45:31.060</a></span> | <span class="t">how it all works. He said, okay, if I write the paper and do the ablation studies and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6335" target="_blank">01:45:35.300</a></span> | <span class="t">figure out from scratch how the code works without bothering you, then can we write the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6338" target="_blank">01:45:38.860</a></span> | <span class="t">paper? I was like, yeah, if you did all those things, you can write the paper. And he was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6348" target="_blank">01:45:48.740</a></span> | <span class="t">like, okay. And so then two days later he comes back and he says, okay, I've done a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6351" target="_blank">01:45:51.580</a></span> | <span class="t">draft with the paper. So I share this story to say like, if you're some student in Ireland</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6362" target="_blank">01:46:02.700</a></span> | <span class="t">and you want to do good work, don't let anybody stop you. I did not encourage him to say the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6370" target="_blank">01:46:10.940</a></span> | <span class="t">least. But in the end he was like, look, I want to do this work, I think it's going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6376" target="_blank">01:46:16.300</a></span> | <span class="t">be good and I'll figure it out. And he wrote a fantastic paper and he did the ablation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6382" target="_blank">01:46:22.420</a></span> | <span class="t">studies and he figured out how fast AI works and now we're planning to write another paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6387" target="_blank">01:46:27.420</a></span> | <span class="t">together. You've got to be a bit careful because sometimes I get messages from random people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6396" target="_blank">01:46:36.300</a></span> | <span class="t">saying like, I've got lots of good ideas, can we have coffee? I can have coffee at my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6403" target="_blank">01:46:43.980</a></span> | <span class="t">office any time, thank you. But it's very different to say like, hey, I took your ideas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6409" target="_blank">01:46:49.660</a></span> | <span class="t">and I wrote a paper and I did a bunch of experiments and I figured out how your code works. I added</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6413" target="_blank">01:46:53.700</a></span> | <span class="t">documentation to it, should we submit this to a conference? Do you see what I mean? There's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6422" target="_blank">01:47:02.300</a></span> | <span class="t">nothing to stop you doing amazing work and if you do amazing work that helps somebody</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6428" target="_blank">01:47:08.660</a></span> | <span class="t">else, like in this case, I'm happy that we have a paper. I don't deeply care about papers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6435" target="_blank">01:47:15.700</a></span> | <span class="t">but I think it's cool that these ideas now have this rigorous study. Let me show you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6440" target="_blank">01:47:20.220</a></span> | <span class="t">what he did. He took all my code, so I'd already done all the fast AI.txt and stuff like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6449" target="_blank">01:47:29.580</a></span> | <span class="t">As you've seen, it lets us work with large corpuses. Sebastian is fantastically well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6456" target="_blank">01:47:36.660</a></span> | <span class="t">read and he said here's a paper that Jan Lekudins and guys just came out with where they tried</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6461" target="_blank">01:47:41.620</a></span> | <span class="t">lots of different classification data sets, so I'm going to try running your code on all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6466" target="_blank">01:47:46.500</a></span> | <span class="t">these data sets. These are the data sets. Some of them had many, many hundreds of thousands</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6472" target="_blank">01:47:52.940</a></span> | <span class="t">of documents and they were far bigger than anything I had tried, but I thought it should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6477" target="_blank">01:47:57.620</a></span> | <span class="t">work. He had a few good little ideas as we went along and so you should totally make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6487" target="_blank">01:48:07.980</a></span> | <span class="t">sure you read the paper. He said this thing that you called in the lessons differential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6498" target="_blank">01:48:18.100</a></span> | <span class="t">learning rates, differential means something else. Maybe we should rename it. It's now called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6505" target="_blank">01:48:25.100</a></span> | <span class="t">discriminative learning rates. This idea that we had from Part 1 where we used different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6509" target="_blank">01:48:29.620</a></span> | <span class="t">learning rates for different layers, after doing some literature research, it does seem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6514" target="_blank">01:48:34.940</a></span> | <span class="t">like that hasn't been done before so it's now officially a thing, discriminative learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6520" target="_blank">01:48:40.540</a></span> | <span class="t">rates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6521" target="_blank">01:48:41.540</a></span> | <span class="t">So all these ideas, this is something we learned in Lesson 1. It now has an equation with Greek</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6526" target="_blank">01:48:46.740</a></span> | <span class="t">and everything. When you see an equation with Greek and everything, that doesn't necessarily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6532" target="_blank">01:48:52.300</a></span> | <span class="t">mean it's more complex than anything we did in Lesson 1 because this one isn't. Again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6537" target="_blank">01:48:57.420</a></span> | <span class="t">that idea of unfreezing a layer at a time also seems to have never been done before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6543" target="_blank">01:49:03.540</a></span> | <span class="t">so it's now a thing and it's got the very clever name gradual unfreezing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6551" target="_blank">01:49:11.180</a></span> | <span class="t">So then, long promised, we're going to look at this, slanted triangular learning rates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6559" target="_blank">01:49:19.860</a></span> | <span class="t">So this actually was not my idea. Leslie Smith, one of my favorite researchers who you all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6565" target="_blank">01:49:25.780</a></span> | <span class="t">now know about, emailed me a while ago and said I'm so over a circle called learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6571" target="_blank">01:49:31.780</a></span> | <span class="t">rates, I don't do that anymore, I now do a slightly different version where I have one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6575" target="_blank">01:49:35.280</a></span> | <span class="t">cycle which goes up quickly at the start and then slows it down afterwards. And he said</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6580" target="_blank">01:49:40.900</a></span> | <span class="t">I often find it works better, I tried going back over all of my old data sets and it worked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6585" target="_blank">01:49:45.020</a></span> | <span class="t">better for all of them, everyone I tried.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6588" target="_blank">01:49:48.060</a></span> | <span class="t">So this is what the learning rate looks like. You can use it in fastAI just by adding UCLR</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6593" target="_blank">01:49:53.540</a></span> | <span class="t">equals to your fit. This first number is the ratio between the highest learning rate and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6601" target="_blank">01:50:01.100</a></span> | <span class="t">the lowest learning rate. So here this is 1/32 of that. The second number is the ratio</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6607" target="_blank">01:50:07.880</a></span> | <span class="t">between the first peak and the last peak. And so the basic idea is if you're doing a cycle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6615" target="_blank">01:50:15.340</a></span> | <span class="t">length 10 and you want the first epoch to be the upward bit and the other 9 epochs to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6623" target="_blank">01:50:23.700</a></span> | <span class="t">be the downward bit, then you would use 10. And I find that works pretty well, that was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6628" target="_blank">01:50:28.660</a></span> | <span class="t">also Leslie's suggestion, make about 1/10 of it the upward bit and about 9/10 the downward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6635" target="_blank">01:50:35.500</a></span> | <span class="t">bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6636" target="_blank">01:50:36.940</a></span> | <span class="t">Since he told me about it, maybe two days ago, he wrote this amazing paper, a disciplined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6643" target="_blank">01:50:43.880</a></span> | <span class="t">approach to neural network hyperparameters, in which he described something very slightly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6649" target="_blank">01:50:49.440</a></span> | <span class="t">different to this again, but the same basic idea. This is a must-read paper. It's got all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6657" target="_blank">01:50:57.220</a></span> | <span class="t">the kinds of ideas that fastAI talks about a lot in great depth, and nobody else is talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6665" target="_blank">01:51:05.100</a></span> | <span class="t">about this stuff. It's kind of a slog, unfortunately Leslie had to go away on a trip before he</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6672" target="_blank">01:51:12.020</a></span> | <span class="t">really had time to edit it properly, so it's a little bit slow reading, but don't let that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6677" target="_blank">01:51:17.140</a></span> | <span class="t">stop you, it's amazing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6679" target="_blank">01:51:19.740</a></span> | <span class="t">So this triangle, this is the equation from my paper with Sebastian. Sebastian was like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6684" target="_blank">01:51:24.220</a></span> | <span class="t">"Jeremy, can you send me the math equation behind that code you wrote?" And I was like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6689" target="_blank">01:51:29.100</a></span> | <span class="t">"No, I just wrote the code, I could not turn it into math." So he figured out the math</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6693" target="_blank">01:51:33.340</a></span> | <span class="t">for it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6697" target="_blank">01:51:37.140</a></span> | <span class="t">So you might have noticed the first layer of our classifier was equal to embedding size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6707" target="_blank">01:51:47.820</a></span> | <span class="t">times 3. Why times 3? Times 3 because, and again this seems to be something which people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6714" target="_blank">01:51:54.960</a></span> | <span class="t">haven't done before, so a new idea, concat pooling, which is that we take the average</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6724" target="_blank">01:52:04.460</a></span> | <span class="t">pooling over the sequence of the activations, the max pooling of the sequence over the activations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6730" target="_blank">01:52:10.940</a></span> | <span class="t">and the final set of activations and just concatenate them all together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6734" target="_blank">01:52:14.820</a></span> | <span class="t">Again, this is something which we talked about in Part 1, but it doesn't seem to be in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6740" target="_blank">01:52:20.940</a></span> | <span class="t">literature before, so it's now called concat pooling, and again it's now got an equation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6745" target="_blank">01:52:25.940</a></span> | <span class="t">and everything, but this is the entirety of the implementation. Pool with average, pool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6752" target="_blank">01:52:32.580</a></span> | <span class="t">with max, concatenate those two along with the final sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6758" target="_blank">01:52:38.460</a></span> | <span class="t">So you can go through this paper and see how the fastai code implements each piece.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6767" target="_blank">01:52:47.100</a></span> | <span class="t">So then, to me one of the kind of interesting pieces is the difference between RNN encoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6775" target="_blank">01:52:55.180</a></span> | <span class="t">which you've already seen, and multibatch RNN encoder. So what's the difference there?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6780" target="_blank">01:53:00.780</a></span> | <span class="t">So the key difference is that the normal RNN encoder for the language model, we could just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6785" target="_blank">01:53:05.900</a></span> | <span class="t">do bptt chunk at a time, but no problem, and predict the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6796" target="_blank">01:53:16.420</a></span> | <span class="t">But for the classifier, we need to do the whole document. We need to do the whole movie</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6802" target="_blank">01:53:22.300</a></span> | <span class="t">review before we decide if it's positive or negative. And the whole movie review can easily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6806" target="_blank">01:53:26.780</a></span> | <span class="t">be 2000 words long, and I can't fit 2000 words worth of gradients in my GPU memory for every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6817" target="_blank">01:53:37.700</a></span> | <span class="t">single one of my activations -- sorry, for every one of my weights. So what do I do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6824" target="_blank">01:53:44.700</a></span> | <span class="t">And so the idea was very simple, which is I go through my whole sequence length one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6832" target="_blank">01:53:52.140</a></span> | <span class="t">batch of bptt at a time, and I call super.forward, so in other words the RNN encoder, to grab</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6844" target="_blank">01:54:04.940</a></span> | <span class="t">its outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6847" target="_blank">01:54:07.060</a></span> | <span class="t">And then I've got this maximum sequence length parameter where it says, okay, as long as you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6857" target="_blank">01:54:17.260</a></span> | <span class="t">doing no more than that sequence length, then start appending it to my list of outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6865" target="_blank">01:54:25.380</a></span> | <span class="t">So in other words, the thing that it sends back to this pooling is only as many activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6877" target="_blank">01:54:37.740</a></span> | <span class="t">as we've asked it to keep. And so that way you can basically figure out how much, what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6885" target="_blank">01:54:45.540</a></span> | <span class="t">maxsec do you, can your particular GPU handle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6891" target="_blank">01:54:51.940</a></span> | <span class="t">So it's still using the whole document, but let's say maxsec is 1000 words, and your longest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6899" target="_blank">01:54:59.540</a></span> | <span class="t">document length is 2000 words. Then it's still going through the RNN creating state for those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6905" target="_blank">01:55:05.700</a></span> | <span class="t">first 1000 words, but it's not actually going to store the activations for the backprop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6914" target="_blank">01:55:14.180</a></span> | <span class="t">the first 1000, it's only going to keep the last 1000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6917" target="_blank">01:55:17.500</a></span> | <span class="t">So that means that it can't backprop the loss back to any state that was created in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6926" target="_blank">01:55:26.460</a></span> | <span class="t">first 1000 words. Basically that's now gone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6931" target="_blank">01:55:31.680</a></span> | <span class="t">So it's a really simple piece of code, and honestly when I wrote it, I didn't spend much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6939" target="_blank">01:55:39.500</a></span> | <span class="t">time thinking about it, it seems so obviously the only way that this could possibly work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6944" target="_blank">01:55:44.500</a></span> | <span class="t">But again, it seems to be a new thing, so we now have backprop through time for text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6949" target="_blank">01:55:49.420</a></span> | <span class="t">classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6950" target="_blank">01:55:50.420</a></span> | <span class="t">So you can see there's lots of little pieces in this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6956" target="_blank">01:55:56.460</a></span> | <span class="t">So what was the result?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6959" target="_blank">01:55:59.020</a></span> | <span class="t">So the result was on every single dataset we tried, we got a better result than any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6966" target="_blank">01:56:06.140</a></span> | <span class="t">previous academic for text classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6971" target="_blank">01:56:11.460</a></span> | <span class="t">So IMDB, Trek 6, AG News, DBpedia, Yelp, all different types.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6980" target="_blank">01:56:20.820</a></span> | <span class="t">And honestly IMDB was the only one I spent any time trying to optimize the model, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6985" target="_blank">01:56:25.660</a></span> | <span class="t">like most of them we just did it like whatever came out first, so if we actually spent time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6989" target="_blank">01:56:29.900</a></span> | <span class="t">on it I think these would be a lot better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=6993" target="_blank">01:56:33.380</a></span> | <span class="t">And the things that these are comparing to, most of them are, you'll see they're different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7000" target="_blank">01:56:40.180</a></span> | <span class="t">on each table because they're optimized, these are like customized algorithms on the whole.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7005" target="_blank">01:56:45.500</a></span> | <span class="t">So this is saying one simple fine-tuning algorithm can beat these really customized algorithms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7016" target="_blank">01:56:56.420</a></span> | <span class="t">And so here's one of the really cool things that Sebastian did with his ablation studies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7022" target="_blank">01:57:02.580</a></span> | <span class="t">which is I was really keen that if we were going to publish a paper we had to say why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7026" target="_blank">01:57:06.580</a></span> | <span class="t">does it work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7028" target="_blank">01:57:08.980</a></span> | <span class="t">So Sebastian went through and tried removing all of those different contributions I mentioned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7037" target="_blank">01:57:17.540</a></span> | <span class="t">So what if we don't use gradual freezing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7042" target="_blank">01:57:22.340</a></span> | <span class="t">What if we don't use discriminative learning rates?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7044" target="_blank">01:57:24.860</a></span> | <span class="t">What if instead of discriminative learning rates we use cosine annealing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7048" target="_blank">01:57:28.900</a></span> | <span class="t">What if we don't do any pre-training with Wikipedia?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7057" target="_blank">01:57:37.340</a></span> | <span class="t">What if we don't do any fine-tuning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7060" target="_blank">01:57:40.580</a></span> | <span class="t">And the really interesting one to me was what's the validation error rate on IMDB if we only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7066" target="_blank">01:57:46.980</a></span> | <span class="t">use 100 training examples versus 200 versus 500?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7070" target="_blank">01:57:50.940</a></span> | <span class="t">And you can see, very interestingly, the full version of this approach is nearly as accurate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7081" target="_blank">01:58:01.140</a></span> | <span class="t">on just 100 training examples, like it's still very accurate versus 20,000 training examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7089" target="_blank">01:58:09.460</a></span> | <span class="t">Whereas if you're training from scratch on 100, it's almost random.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7094" target="_blank">01:58:14.540</a></span> | <span class="t">So it's what I expected, kind of set to Sebastian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7098" target="_blank">01:58:18.660</a></span> | <span class="t">I really think this is most beneficial when you don't have much data, and this is like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7103" target="_blank">01:58:23.940</a></span> | <span class="t">where FastAI is most interested in contributing, small data regimes, small compute regimes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7108" target="_blank">01:58:28.900</a></span> | <span class="t">and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7109" target="_blank">01:58:29.900</a></span> | <span class="t">So he did these studies to check.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7113" target="_blank">01:58:33.100</a></span> | <span class="t">So I want to show you a couple of tricks as to how you can run these kinds of studies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7122" target="_blank">01:58:42.940</a></span> | <span class="t">The first trick is something which I know you're all going to find really handy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7129" target="_blank">01:58:49.060</a></span> | <span class="t">I know you've all been annoyed when you're running something in a Jupyter notebook and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7132" target="_blank">01:58:52.620</a></span> | <span class="t">you lose your internet connection for long enough that it decides you've gone away and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7137" target="_blank">01:58:57.740</a></span> | <span class="t">then your session disappears and you have to start it again from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7142" target="_blank">01:59:02.700</a></span> | <span class="t">So what do you do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7145" target="_blank">01:59:05.780</a></span> | <span class="t">There's a very simple cool thing called VNC, where basically you can install on your AWS</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7153" target="_blank">01:59:13.460</a></span> | <span class="t">instance or paper space or whatever, xWindows, a lightweight window manager, a VNC server,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7162" target="_blank">01:59:22.540</a></span> | <span class="t">Firefox, a terminal, and some fonts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7168" target="_blank">01:59:28.500</a></span> | <span class="t">Track these lines at the end of your VNC xstartup configuration file, and then run this command.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7178" target="_blank">01:59:38.700</a></span> | <span class="t">It's now running a server where you can then run the type VNC viewer on your computer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7194" target="_blank">01:59:54.500</a></span> | <span class="t">and you point it at your server.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7199" target="_blank">01:59:59.180</a></span> | <span class="t">Specifically, what you do is you use SSH port forwarding to port 4913 to localhost 5913.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7213" target="_blank">02:00:13.740</a></span> | <span class="t">And so then you connect to port 5913 on localhost, send it off to port 5913 on your server, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7225" target="_blank">02:00:25.460</a></span> | <span class="t">is the VNC port because you said colon 13 here, and it will display an xWindows desktop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7232" target="_blank">02:00:32.460</a></span> | <span class="t">And then you can click on the Linux start like button and click on Firefox, and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7237" target="_blank">02:00:37.420</a></span> | <span class="t">now have Firefox, and you'll see here in Firefox it says localhost because this Firefox is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7244" target="_blank">02:00:44.700</a></span> | <span class="t">running on my AWS server.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7247" target="_blank">02:00:47.780</a></span> | <span class="t">So you now run Firefox, you start your thing running, and then you close your VNC viewer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7253" target="_blank">02:00:53.700</a></span> | <span class="t">remembering that Firefox is like displaying on this virtual VNC display, not in a real</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7259" target="_blank">02:00:59.660</a></span> | <span class="t">display.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7260" target="_blank">02:01:00.660</a></span> | <span class="t">And so then later on that day, you log back into VNC viewer and it pops up again, so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7265" target="_blank">02:01:05.460</a></span> | <span class="t">like a persistent desktop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7268" target="_blank">02:01:08.300</a></span> | <span class="t">And it's shockingly fast, it works really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7271" target="_blank">02:01:11.380</a></span> | <span class="t">So there's trick number 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7274" target="_blank">02:01:14.020</a></span> | <span class="t">And there's lots of different VNC servers and clients and whatever, but this one worked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7278" target="_blank">02:01:18.860</a></span> | <span class="t">fine for me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7279" target="_blank">02:01:19.860</a></span> | <span class="t">So you can see here I connect to localhost 5913.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7287" target="_blank">02:01:27.980</a></span> | <span class="t">Trick number 2 is to create Python scripts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7293" target="_blank">02:01:33.020</a></span> | <span class="t">This is what we ended up doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7294" target="_blank">02:01:34.960</a></span> | <span class="t">So I ended up creating a little Python script for Sebastian to say this is the basic steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7299" target="_blank">02:01:39.960</a></span> | <span class="t">you need to do, and now you need to create different versions for everything else, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7303" target="_blank">02:01:43.300</a></span> | <span class="t">I suggested to him that he tried using this thing called Google Fire.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7307" target="_blank">02:01:47.140</a></span> | <span class="t">What Google Fire does is you create a function with shitloads of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7313" target="_blank">02:01:53.100</a></span> | <span class="t">And so these are all the things that Sebastian wanted to try doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7316" target="_blank">02:01:56.100</a></span> | <span class="t">Different dropout amounts, different learning rates, do I use pre-training or not, do I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7320" target="_blank">02:02:00.380</a></span> | <span class="t">use CLI or not, do I use discriminative learning rate or not, do I go backwards or not, blah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7325" target="_blank">02:02:05.940</a></span> | <span class="t">blah blah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7326" target="_blank">02:02:06.940</a></span> | <span class="t">So you create a function, and then you add something saying if name equals main, fire.fire,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7331" target="_blank">02:02:11.900</a></span> | <span class="t">and the function name, you do nothing else at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7334" target="_blank">02:02:14.580</a></span> | <span class="t">You don't have to add any metadata, any docstrings, anything at all, and you then call that script</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7340" target="_blank">02:02:20.460</a></span> | <span class="t">and automatically you now have a command line interface, and that's it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7347" target="_blank">02:02:27.060</a></span> | <span class="t">So that's a super fantastic easy way to run lots of different variations in a terminal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7354" target="_blank">02:02:34.700</a></span> | <span class="t">And this ends up being easier if you want to do lots of variations than using a notebook</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7360" target="_blank">02:02:40.500</a></span> | <span class="t">because you can just have a bash script that tries all of them and spits them all out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7368" target="_blank">02:02:48.180</a></span> | <span class="t">You'll find inside the dl2-course directory, there's now something called imdb-scripts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7378" target="_blank">02:02:58.040</a></span> | <span class="t">and I've put there all of the scripts that Sebastian and I used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7382" target="_blank">02:03:02.780</a></span> | <span class="t">So you'll see because we needed to tokenize every single dataset, we had to turn every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7390" target="_blank">02:03:10.460</a></span> | <span class="t">dataset and numericalize every dataset, we had to train a language model on every dataset,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7395" target="_blank">02:03:15.220</a></span> | <span class="t">we had to train and classify every dataset, we had to do all of those things in a variety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7398" target="_blank">02:03:18.420</a></span> | <span class="t">of different ways to compare them, we had a script for all of those things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7401" target="_blank">02:03:21.940</a></span> | <span class="t">So you can check out and see all of the scripts that we used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7412" target="_blank">02:03:32.460</a></span> | <span class="t">When you're doing a lot of scripts and stuff, you've got different code all over the place,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7417" target="_blank">02:03:37.420</a></span> | <span class="t">eventually it might get frustrating that you don't want to symlink your fastai library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7423" target="_blank">02:03:43.340</a></span> | <span class="t">again and again, but you probably don't want to pip-install it because that version tends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7428" target="_blank">02:03:48.460</a></span> | <span class="t">to be a little bit old, we move so fast you want to use the current version in git.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7434" target="_blank">02:03:54.120</a></span> | <span class="t">If you say pip-install-a. from the fastai-repo base, it does something quite neat which basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7443" target="_blank">02:04:03.780</a></span> | <span class="t">creates a symlink to the fastai library inside your site packages directory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7455" target="_blank">02:04:15.540</a></span> | <span class="t">Your site packages directory is like your main Python library.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7460" target="_blank">02:04:20.900</a></span> | <span class="t">And so if you do this, you can then access fastai from anywhere, but every time you do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7468" target="_blank">02:04:28.060</a></span> | <span class="t">git pull, you've got the most recent version. One downside of this is that it installs any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7474" target="_blank">02:04:34.980</a></span> | <span class="t">updated versions of packages from pip which can confuse conda a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7482" target="_blank">02:04:42.120</a></span> | <span class="t">So another alternative here is just to symlink the fastai library to your site packages library.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7490" target="_blank">02:04:50.980</a></span> | <span class="t">That works just as well. And then you can use fastai again from anywhere, and it's quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7497" target="_blank">02:04:57.740</a></span> | <span class="t">handy when you want to run scripts that use fastai from different directories on your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7505" target="_blank">02:05:05.700</a></span> | <span class="t">system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7507" target="_blank">02:05:07.420</a></span> | <span class="t">So one more thing before we go, which is something you can try if you like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7517" target="_blank">02:05:17.660</a></span> | <span class="t">You don't have to tokenize words. Instead of tokenizing words, you can tokenize what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7526" target="_blank">02:05:26.060</a></span> | <span class="t">are called subword units.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7529" target="_blank">02:05:29.140</a></span> | <span class="t">And so for example, unsupervised could be tokenized as unsupervised. Tokenizer could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7540" target="_blank">02:05:40.820</a></span> | <span class="t">be tokenized as tokenizer. And then you can do the same thing, the language model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7547" target="_blank">02:05:47.780</a></span> | <span class="t">works on subword units, the classifier that works on subword units, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7555" target="_blank">02:05:55.740</a></span> | <span class="t">So how well does that work? I started playing with it and with not too much playing, I was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7564" target="_blank">02:06:04.060</a></span> | <span class="t">getting classification results that were nearly as good as using word-level tokenization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7570" target="_blank">02:06:10.260</a></span> | <span class="t">Not quite as good, but nearly as good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7574" target="_blank">02:06:14.860</a></span> | <span class="t">I suspect with more careful thinking and playing around, maybe I could have got as good or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7581" target="_blank">02:06:21.060</a></span> | <span class="t">better. But even if I couldn't, if you create a subword unit wiki text model, then IMDB model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7594" target="_blank">02:06:34.060</a></span> | <span class="t">language model, and then classifier forwards and backwards for subword units, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7599" target="_blank">02:06:39.340</a></span> | <span class="t">ensemble it with the forwards and backwards word-level ones, you should be able to beat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7604" target="_blank">02:06:44.340</a></span> | <span class="t">us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7606" target="_blank">02:06:46.220</a></span> | <span class="t">So here's an approach you may be able to beat our state-of-the-art result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7612" target="_blank">02:06:52.340</a></span> | <span class="t">Google has, as Sebastian told me about this particular project, Google has a project called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7617" target="_blank">02:06:57.780</a></span> | <span class="t">Sentence Piece, which actually uses a neural net to figure out the optimal splitting up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7625" target="_blank">02:07:05.900</a></span> | <span class="t">of words, and so you end up with a vocabulary of subword units. In my playing around, I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7632" target="_blank">02:07:12.700</a></span> | <span class="t">found that creating a vocabulary of about 30,000 subword units seems to be about optimal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7639" target="_blank">02:07:19.940</a></span> | <span class="t">So if you're interested, there's something you can try. It's a bit of a pain to install.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7645" target="_blank">02:07:25.300</a></span> | <span class="t">It's C++. It doesn't have great error messages. But it will work. There is a Python library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7651" target="_blank">02:07:31.780</a></span> | <span class="t">for it, and if anybody tries this, I'm happy to help them get it working. There's been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7658" target="_blank">02:07:38.540</a></span> | <span class="t">little if any experiments with ensembling subword and word-level stuff classification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7666" target="_blank">02:07:46.060</a></span> | <span class="t">and I do think it should be the best approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7668" target="_blank">02:07:48.620</a></span> | <span class="t">Alright, thanks everybody. Have a great week and see you next Monday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=h5Tz7gZT9Fo&t=7672" target="_blank">02:07:52.460</a></span> | <span class="t">[APPLAUSE]</span></div></div></body></html>