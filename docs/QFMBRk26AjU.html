<html><head><title>Stanford XCS224U: NLU I Contextual Word Representations, Part 7: ELECTRA I Spring 2023</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U: NLU I Contextual Word Representations, Part 7: ELECTRA I Spring 2023</h2><a href="https://www.youtube.com/watch?v=QFMBRk26AjU"><img src="https://i.ytimg.com/vi/QFMBRk26AjU/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./QFMBRk26AjU.html">Whisper Transcript</a> | <a href="./transcript_QFMBRk26AjU.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome back everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=6" target="_blank">00:00:06.000</a></span> | <span class="t">This is part seven in our series on contextual representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=9" target="_blank">00:00:09.360</a></span> | <span class="t">We're going to talk about the Electra model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=12" target="_blank">00:00:12.520</a></span> | <span class="t">Recall that I finished the BERT screencast by listing out some known limitations of that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=17" target="_blank">00:00:17.000</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=18" target="_blank">00:00:18.000</a></span> | <span class="t">Roberta addressed item one on that list and we can think of Electra as keying into items</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=22" target="_blank">00:00:22.340</a></span> | <span class="t">two and three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=24" target="_blank">00:00:24.080</a></span> | <span class="t">Item two is about the mask token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=25" target="_blank">00:00:25.680</a></span> | <span class="t">The BERT team observed that they had created a mismatch between the pre-training and fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=30" target="_blank">00:00:30.160</a></span> | <span class="t">tuning vocabularies because the mask token is never seen during fine tuning, only during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=35" target="_blank">00:00:35.560</a></span> | <span class="t">training and you could think that that mismatch might reduce the effectiveness of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=41" target="_blank">00:00:41.660</a></span> | <span class="t">Item three is about efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=43" target="_blank">00:00:43.240</a></span> | <span class="t">The BERT team observed that the MLM objective means that they only use around 15% of tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=50" target="_blank">00:00:50.520</a></span> | <span class="t">when they are training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=52" target="_blank">00:00:52.240</a></span> | <span class="t">Only 15% of them even contribute to the MLM objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=56" target="_blank">00:00:56.240</a></span> | <span class="t">We have to do all this work of processing every item in the sequence, but we get very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=60" target="_blank">00:01:00.420</a></span> | <span class="t">few learning signals from that process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=63" target="_blank">00:01:03.240</a></span> | <span class="t">And that's certainly data inefficient and we might think about finding ways to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=66" target="_blank">00:01:06.760</a></span> | <span class="t">more use of the available data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=70" target="_blank">00:01:10.680</a></span> | <span class="t">Electra is going to make progress on both these fronts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=72" target="_blank">00:01:12.800</a></span> | <span class="t">Let's explore the core model structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=76" target="_blank">00:01:16.160</a></span> | <span class="t">For our example, we have this input sequence X, the chef cooked the meal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=81" target="_blank">00:01:21.000</a></span> | <span class="t">The first thing we do is create X masked, which is a masked version of that input sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=86" target="_blank">00:01:26.200</a></span> | <span class="t">And we could do that using the same protocol as they use for BERT by masking out, say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=91" target="_blank">00:01:31.040</a></span> | <span class="t">15% of the tokens at random.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=94" target="_blank">00:01:34.280</a></span> | <span class="t">Then we have our generator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=95" target="_blank">00:01:35.560</a></span> | <span class="t">This is a small BERT-like model that processes that input and produces what we call X corrupt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=102" target="_blank">00:01:42.880</a></span> | <span class="t">This is an output sequence predicted by the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=105" target="_blank">00:01:45.800</a></span> | <span class="t">And the twist here is that we're going to replace some of those tokens not with their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=110" target="_blank">00:01:50.780</a></span> | <span class="t">original inputs, but rather with tokens that come out with probabilities proportional to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=116" target="_blank">00:01:56.120</a></span> | <span class="t">the probability generators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=118" target="_blank">00:01:58.160</a></span> | <span class="t">And what that means is that sometimes we'll replace with the actual input token and sometimes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=123" target="_blank">00:02:03.120</a></span> | <span class="t">with a different token, like in this case of cooked coming in, being replaced by eight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=129" target="_blank">00:02:09.200</a></span> | <span class="t">That is where Electra, the discriminator, takes over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=133" target="_blank">00:02:13.040</a></span> | <span class="t">The job of the discriminator, which is really the heart of the Electra model, is to figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=137" target="_blank">00:02:17.800</a></span> | <span class="t">out which of those tokens in X corrupt is an original and which was replaced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=144" target="_blank">00:02:24.600</a></span> | <span class="t">So we train this model jointly with the generator and a weighted version of the discriminator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=149" target="_blank">00:02:29.400</a></span> | <span class="t">or Electra objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=151" target="_blank">00:02:31.960</a></span> | <span class="t">And then, essentially, we can allow the generator to drop away and focus on the discriminator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=157" target="_blank">00:02:37.200</a></span> | <span class="t">as the primary pre-trained artifact from this process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=162" target="_blank">00:02:42.280</a></span> | <span class="t">One thing that I really love about the Electra paper is that it includes very rich studies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=166" target="_blank">00:02:46.800</a></span> | <span class="t">of how best to set up the Electra model itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=170" target="_blank">00:02:50.720</a></span> | <span class="t">I'll review some of that evidence here, starting with the relationship that they uncover between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=175" target="_blank">00:02:55.360</a></span> | <span class="t">the generator and the discriminator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=178" target="_blank">00:02:58.280</a></span> | <span class="t">First thing is an observation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=179" target="_blank">00:02:59.680</a></span> | <span class="t">Where the generator and discriminator are the same size, they could, in principle, share</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=184" target="_blank">00:03:04.440</a></span> | <span class="t">their transformer parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=186" target="_blank">00:03:06.640</a></span> | <span class="t">And the team found that more sharing is indeed better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=189" target="_blank">00:03:09.800</a></span> | <span class="t">However, the best results come from having a generator that is small compared to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=195" target="_blank">00:03:15.640</a></span> | <span class="t">discriminator, which means less sharing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=198" target="_blank">00:03:18.880</a></span> | <span class="t">Here's a chart summarizing their evidence for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=201" target="_blank">00:03:21.560</a></span> | <span class="t">Along the x-axis, I have the generator size going up to 1024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=206" target="_blank">00:03:26.760</a></span> | <span class="t">And along the y-axis, we have GLU score, which will be our proxy for overall quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=212" target="_blank">00:03:32.880</a></span> | <span class="t">The blue line up here is the discriminator at size 768.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=217" target="_blank">00:03:37.000</a></span> | <span class="t">And we're tracking different generator sizes, as I said.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=219" target="_blank">00:03:39.720</a></span> | <span class="t">And you see this characteristic reverse U-shape, where, for example, the best discriminator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=224" target="_blank">00:03:44.940</a></span> | <span class="t">at size 768 corresponds to a generator of size 256.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=230" target="_blank">00:03:50.320</a></span> | <span class="t">And indeed, as the generator gets larger and even gets larger than the discriminator, performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=235" target="_blank">00:03:55.800</a></span> | <span class="t">drops off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=237" target="_blank">00:03:57.220</a></span> | <span class="t">And that U-shape is repeated for all these different discriminator sizes, suggesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=242" target="_blank">00:04:02.000</a></span> | <span class="t">a real finding about the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=244" target="_blank">00:04:04.000</a></span> | <span class="t">I think the intuition here is that it's kind of good to have a small and relatively weak</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=248" target="_blank">00:04:08.680</a></span> | <span class="t">generator so that the discriminator has a lot of interesting work to do, because after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=253" target="_blank">00:04:13.560</a></span> | <span class="t">all, the discriminator is our focus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=258" target="_blank">00:04:18.160</a></span> | <span class="t">The paper also includes a lot of efficiency studies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=260" target="_blank">00:04:20.880</a></span> | <span class="t">And those, too, are really illuminating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=262" target="_blank">00:04:22.780</a></span> | <span class="t">This is a summary of some of their evidence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=265" target="_blank">00:04:25.200</a></span> | <span class="t">Along the x-axis, we have pre-trained flops, which you can think of as a raw amount of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=270" target="_blank">00:04:30.000</a></span> | <span class="t">overall compute needed for training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=272" target="_blank">00:04:32.880</a></span> | <span class="t">And along the y-axis, again, we have the GLUE score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=276" target="_blank">00:04:36.200</a></span> | <span class="t">The blue line at the top here is the full Elektra model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=279" target="_blank">00:04:39.040</a></span> | <span class="t">And the core result here is that for any compute budget you have, that is any point along the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=283" target="_blank">00:04:43.480</a></span> | <span class="t">x-axis, Elektra is the best model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=287" target="_blank">00:04:47.140</a></span> | <span class="t">It looks like in second place is adversarial Elektra.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=290" target="_blank">00:04:50.400</a></span> | <span class="t">That's an intriguing variation of the model, where the generator is actually trained to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=295" target="_blank">00:04:55.000</a></span> | <span class="t">try to fool the discriminator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=297" target="_blank">00:04:57.180</a></span> | <span class="t">That's a clear intuition that turns out to be slightly less good than the more cooperative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=301" target="_blank">00:05:01.680</a></span> | <span class="t">objective that I presented before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=304" target="_blank">00:05:04.860</a></span> | <span class="t">And then the green lines are intriguing as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=306" target="_blank">00:05:06.720</a></span> | <span class="t">So for the green lines, we begin by training just in a standard BERT fashion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=312" target="_blank">00:05:12.400</a></span> | <span class="t">And then at a certain point, we switch over to the full Elektra model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=316" target="_blank">00:05:16.320</a></span> | <span class="t">And what you see there is that in switching over to full Elektra, you get a gain in performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=321" target="_blank">00:05:21.720</a></span> | <span class="t">for any compute budget relative to the standard BERT training continuing as before, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=326" target="_blank">00:05:26.880</a></span> | <span class="t">is the lowest line in the chart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=329" target="_blank">00:05:29.940</a></span> | <span class="t">So a clear win for Elektra relative to these interesting competitors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=335" target="_blank">00:05:35.220</a></span> | <span class="t">And they did further efficiency analyses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=338" target="_blank">00:05:38.400</a></span> | <span class="t">Let me review some of what they found there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=340" target="_blank">00:05:40.360</a></span> | <span class="t">This is the full Elektra model as I presented it before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=344" target="_blank">00:05:44.340</a></span> | <span class="t">We could also think about Elektra 15%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=347" target="_blank">00:05:47.440</a></span> | <span class="t">And this is the case where for the discriminator, instead of having it make predictions about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=352" target="_blank">00:05:52.160</a></span> | <span class="t">all of the input tokens, we just zoom in on the tokens that were part of this x corrupt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=358" target="_blank">00:05:58.240</a></span> | <span class="t">sequence, ignoring all the rest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=359" target="_blank">00:05:59.920</a></span> | <span class="t">That's a very BERT-like intuition where the ones that matter were these ones that got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=364" target="_blank">00:06:04.360</a></span> | <span class="t">masked down here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=366" target="_blank">00:06:06.400</a></span> | <span class="t">That makes fewer predictions for the discriminator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=370" target="_blank">00:06:10.360</a></span> | <span class="t">Replace MLM is where we use the generator with no discriminator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=376" target="_blank">00:06:16.040</a></span> | <span class="t">This is a kind of ablation of BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=378" target="_blank">00:06:18.160</a></span> | <span class="t">And then all tokens MLM is a kind of variant of BERT where instead of turning off the objective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=383" target="_blank">00:06:23.360</a></span> | <span class="t">for some of the items, we make predictions about all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=387" target="_blank">00:06:27.840</a></span> | <span class="t">And here's a summary of the evidence that they found in favor of Elektra.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=391" target="_blank">00:06:31.680</a></span> | <span class="t">That's at the top here, according to the Glue score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=394" target="_blank">00:06:34.480</a></span> | <span class="t">All tokens MLM and replace MLM, those BERT variants are just behind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=399" target="_blank">00:06:39.360</a></span> | <span class="t">And that's sort of intriguing because it shows that even if we stick to the BERT architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=404" target="_blank">00:06:44.220</a></span> | <span class="t">we could have done better simply by making more predictions than BERT was making initially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=411" target="_blank">00:06:51.560</a></span> | <span class="t">Behind those is Elektra 15%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=413" target="_blank">00:06:53.820</a></span> | <span class="t">And that shows that on the discriminator side, again, it pays to make more predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=418" target="_blank">00:06:58.840</a></span> | <span class="t">If we retreat to the more BERT-like mode where we predict only for the corrupted elements,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=423" target="_blank">00:07:03.740</a></span> | <span class="t">we find that performance degrades.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=426" target="_blank">00:07:06.240</a></span> | <span class="t">And then at the bottom of this list is the original BERT model showing a clear win overall</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=431" target="_blank">00:07:11.600</a></span> | <span class="t">for Elektra according to this Glue benchmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=436" target="_blank">00:07:16.380</a></span> | <span class="t">The Elektra team released three models initially, small, base, and large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=441" target="_blank">00:07:21.020</a></span> | <span class="t">Base and large kind of correspond roughly to BERT releases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=444" target="_blank">00:07:24.100</a></span> | <span class="t">And small is a tiny one that they say is designed to be quickly trained on a single GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=449" target="_blank">00:07:29.300</a></span> | <span class="t">Again, another nod toward increasing emphasis on efficiency for compute as an important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=455" target="_blank">00:07:35.560</a></span> | <span class="t">ingredient in research in this space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=457" target="_blank">00:07:37.720</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=458" target="_blank">00:07:38.720</a></span> | <span class="t">[END OF TRANSCRIPT]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QFMBRk26AjU&t=458" target="_blank">00:07:38.740</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>